{
  "date": "2026-01-15",
  "name": "trending",
  "repositories": [
    {
      "id": 975734319,
      "name": "opencode",
      "full_name": "anomalyco/opencode",
      "description": "The open source coding agent.",
      "html_url": "https://github.com/anomalyco/opencode",
      "stars": 69321,
      "forks": 6015,
      "language": "TypeScript",
      "topics": [],
      "created_at": "2025-04-30T20:08:00Z",
      "updated_at": "2026-01-15T01:03:43Z",
      "pushed_at": "2026-01-14T23:44:20Z",
      "open_issues": 2883,
      "owner": {
        "login": "anomalyco",
        "avatar_url": "https://avatars.githubusercontent.com/u/66570915?v=4"
      },
      "readme": "<p align=\"center\">\n  <a href=\"https://opencode.ai\">\n    <picture>\n      <source srcset=\"packages/console/app/src/asset/logo-ornate-dark.svg\" media=\"(prefers-color-scheme: dark)\">\n      <source srcset=\"packages/console/app/src/asset/logo-ornate-light.svg\" media=\"(prefers-color-scheme: light)\">\n      <img src=\"packages/console/app/src/asset/logo-ornate-light.svg\" alt=\"OpenCode logo\">\n    </picture>\n  </a>\n</p>\n<p align=\"center\">The open source AI coding agent.</p>\n<p align=\"center\">\n  <a href=\"https://opencode.ai/discord\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1391832426048651334?style=flat-square&label=discord\" /></a>\n  <a href=\"https://www.npmjs.com/package/opencode-ai\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/opencode-ai?style=flat-square\" /></a>\n  <a href=\"https://github.com/anomalyco/opencode/actions/workflows/publish.yml\"><img alt=\"Build status\" src=\"https://img.shields.io/github/actions/workflow/status/anomalyco/opencode/publish.yml?style=flat-square&branch=dev\" /></a>\n</p>\n\n[![OpenCode Terminal UI](packages/web/src/assets/lander/screenshot.png)](https://opencode.ai)\n\n---\n\n### Installation\n\n```bash\n# YOLO\ncurl -fsSL https://opencode.ai/install | bash\n\n# Package managers\nnpm i -g opencode-ai@latest        # or bun/pnpm/yarn\nscoop bucket add extras; scoop install extras/opencode  # Windows\nchoco install opencode             # Windows\nbrew install anomalyco/tap/opencode # macOS and Linux (recommended, always up to date)\nbrew install opencode              # macOS and Linux (official brew formula, updated less)\nparu -S opencode-bin               # Arch Linux\nmise use -g opencode               # Any OS\nnix run nixpkgs#opencode           # or github:anomalyco/opencode for latest dev branch\n```\n\n> [!TIP]\n> Remove versions older than 0.1.x before installing.\n\n### Desktop App (BETA)\n\nOpenCode is also available as a desktop application. Download directly from the [releases page](https://github.com/anomalyco/opencode/releases) or [opencode.ai/download](https://opencode.ai/download).\n\n| Platform              | Download                              |\n| --------------------- | ------------------------------------- |\n| macOS (Apple Silicon) | `opencode-desktop-darwin-aarch64.dmg` |\n| macOS (Intel)         | `opencode-desktop-darwin-x64.dmg`     |\n| Windows               | `opencode-desktop-windows-x64.exe`    |\n| Linux                 | `.deb`, `.rpm`, or AppImage           |\n\n```bash\n# macOS (Homebrew)\nbrew install --cask opencode-desktop\n```\n\n#### Installation Directory\n\nThe install script respects the following priority order for the installation path:\n\n1. `$OPENCODE_INSTALL_DIR` - Custom installation directory\n2. `$XDG_BIN_DIR` - XDG Base Directory Specification compliant path\n3. `$HOME/bin` - Standard user binary directory (if exists or can be created)\n4. `$HOME/.opencode/bin` - Default fallback\n\n```bash\n# Examples\nOPENCODE_INSTALL_DIR=/usr/local/bin curl -fsSL https://opencode.ai/install | bash\nXDG_BIN_DIR=$HOME/.local/bin curl -fsSL https://opencode.ai/install | bash\n```\n\n### Agents\n\nOpenCode includes two built-in agents you can switch between with the `Tab` key.\n\n- **build** - Default, full access agent for development work\n- **plan** - Read-only agent for analysis and code exploration\n  - Denies file edits by default\n  - Asks permission before running bash commands\n  - Ideal for exploring unfamiliar codebases or planning changes\n\nAlso, included is a **general** subagent for complex searches and multistep tasks.\nThis is used internally and can be invoked using `@general` in messages.\n\nLearn more about [agents](https://opencode.ai/docs/agents).\n\n### Documentation\n\nFor more info on how to configure OpenCode [**head over to our docs**](https://opencode.ai/docs).\n\n### Contributing\n\nIf you're interested in contributing to OpenCode, please read our [contributing docs](./CONTRIBUTING.md) before submitting a pull request.\n\n### Building on OpenCode\n\nIf you are working on a project that's related to OpenCode and is using \"opencode\" as a part of its name; for example, \"opencode-dashboard\" or \"opencode-mobile\", please add a note to your README to clarify that it is not built by the OpenCode team and is not affiliated with us in any way.\n\n### FAQ\n\n#### How is this different from Claude Code?\n\nIt's very similar to Claude Code in terms of capability. Here are the key differences:\n\n- 100% open source\n- Not coupled to any provider. Although we recommend the models we provide through [OpenCode Zen](https://opencode.ai/zen); OpenCode can be used with Claude, OpenAI, Google or even local models. As models evolve the gaps between them will close and pricing will drop so being provider-agnostic is important.\n- Out of the box LSP support\n- A focus on TUI. OpenCode is built by neovim users and the creators of [terminal.shop](https://terminal.shop); we are going to push the limits of what's possible in the terminal.\n- A client/server architecture. This for example can allow OpenCode to run on your computer, while you can drive it remotely from a mobile app. Meaning that the TUI frontend is just one of the possible clients.\n\n---\n\n**Join our community** [Discord](https://discord.gg/opencode) | [X.com](https://x.com/opencode)\n",
      "stars_today": 2396
    },
    {
      "id": 943950290,
      "name": "datahaven",
      "full_name": "datahaven-xyz/datahaven",
      "description": "An EVM compatible Substrate chain, powered by StorageHub and secured by EigenLayer",
      "html_url": "https://github.com/datahaven-xyz/datahaven",
      "stars": 3073,
      "forks": 58,
      "language": "Rust",
      "topics": [],
      "created_at": "2025-03-06T14:35:59Z",
      "updated_at": "2026-01-15T01:01:57Z",
      "pushed_at": "2026-01-14T20:05:00Z",
      "open_issues": 7,
      "owner": {
        "login": "datahaven-xyz",
        "avatar_url": "https://avatars.githubusercontent.com/u/207200851?v=4"
      },
      "readme": "# DataHaven ğŸ«\n\nAI-First Decentralized Storage secured by EigenLayer â€” a verifiable storage network for AI training data, machine learning models, and Web3 applications.\n\n## Overview\n\nDataHaven is a decentralized storage and retrieval network designed for applications that need verifiable, production-scale data storage. Built on [StorageHub](https://github.com/Moonsong-Labs/storage-hub) and secured by EigenLayer's restaking protocol, DataHaven separates storage from verification: providers store data off-chain while cryptographic commitments are anchored on-chain for tamper-evident verification.\n\n**Core Capabilities:**\n\n- **Verifiable Storage**: Files are chunked, hashed into Merkle trees, and committed on-chain â€” enabling cryptographic proof that data hasn't been tampered with\n- **Provider Network**: Main Storage Providers (MSPs) serve data with competitive offerings, while Backup Storage Providers (BSPs) ensure redundancy through decentralized replication with on-chain slashing for failed proof challenges\n- **EigenLayer Security**: Validator set secured by Ethereum restaking â€” DataHaven validators register as EigenLayer operators with slashing for misbehavior\n- **EVM Compatibility**: Full Ethereum support via Frontier pallets for smart contracts and familiar Web3 tooling\n- **Cross-chain Bridge**: Native, trustless bridging with Ethereum via Snowbridge for tokens and messages\n\n## Architecture\n\nDataHaven combines EigenLayer's shared security with StorageHub's decentralized storage infrastructure:\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                              Ethereum (L1)                                  â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚  EigenLayer AVS Contracts                                             â”‚  â”‚\nâ”‚  â”‚  â€¢ DataHavenServiceManager (validator lifecycle & slashing)           â”‚  â”‚\nâ”‚  â”‚  â€¢ RewardsRegistry (validator performance & rewards)                  â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                    â†•                                        â”‚\nâ”‚                          Snowbridge Protocol                                â”‚\nâ”‚                    (trustless cross-chain messaging)                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                     â†•\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                          DataHaven (Substrate)                              â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚  StorageHub Pallets                     DataHaven Pallets             â”‚  â”‚\nâ”‚  â”‚  â€¢ file-system (file operations)        â€¢ External Validators         â”‚  â”‚\nâ”‚  â”‚  â€¢ providers (MSP/BSP registry)         â€¢ Native Transfer             â”‚  â”‚\nâ”‚  â”‚  â€¢ proofs-dealer (challenge/verify)     â€¢ Rewards                     â”‚  â”‚\nâ”‚  â”‚  â€¢ payment-streams (storage payments)   â€¢ Frontier (EVM)              â”‚  â”‚\nâ”‚  â”‚  â€¢ bucket-nfts (bucket ownership)                                     â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                     â†•\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        Storage Provider Network                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\nâ”‚  â”‚  Main Storage Providers     â”‚    â”‚  Backup Storage Providers   â”‚        â”‚\nâ”‚  â”‚  (MSP)                      â”‚    â”‚  (BSP)                      â”‚        â”‚\nâ”‚  â”‚  â€¢ User-selected            â”‚    â”‚  â€¢ Network-assigned         â”‚        â”‚\nâ”‚  â”‚  â€¢ Serve read requests      â”‚    â”‚  â€¢ Replicate data           â”‚        â”‚\nâ”‚  â”‚  â€¢ Anchor bucket roots      â”‚    â”‚  â€¢ Proof challenges         â”‚        â”‚\nâ”‚  â”‚  â€¢ MSP Backend service      â”‚    â”‚  â€¢ On-chain slashing        â”‚        â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\nâ”‚  â”‚  Indexer                    â”‚    â”‚  Fisherman                  â”‚        â”‚\nâ”‚  â”‚  â€¢ Index on-chain events    â”‚    â”‚  â€¢ Audit storage proofs     â”‚        â”‚\nâ”‚  â”‚  â€¢ Query storage metadata   â”‚    â”‚  â€¢ Trigger challenges       â”‚        â”‚\nâ”‚  â”‚  â€¢ PostgreSQL backend       â”‚    â”‚  â€¢ Detect misbehavior       â”‚        â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### How Storage Works\n\n1. **Upload**: User selects an MSP, creates a bucket, and uploads files. Files are chunked (8KB default), hashed into Merkle trees, and the root is anchored on-chain.\n2. **Replication**: The MSP coordinates with BSPs to replicate data across the network based on the bucket's replication policy.\n3. **Retrieval**: MSP returns files with Merkle proofs that users verify against on-chain commitments.\n4. **Verification**: BSPs face periodic proof challenges â€” failure to prove data custody results in on-chain slashing via StorageHub pallets.\n\n## Repository Structure\n\n```\ndatahaven/\nâ”œâ”€â”€ contracts/      # EigenLayer AVS smart contracts\nâ”‚   â”œâ”€â”€ src/       # Service Manager, Rewards Registry, Slasher\nâ”‚   â”œâ”€â”€ script/    # Deployment scripts\nâ”‚   â””â”€â”€ test/      # Foundry test suites\nâ”œâ”€â”€ operator/       # Substrate-based DataHaven node\nâ”‚   â”œâ”€â”€ node/      # Node implementation & chain spec\nâ”‚   â”œâ”€â”€ pallets/   # Custom pallets (validators, rewards, transfers)\nâ”‚   â””â”€â”€ runtime/   # Runtime configurations (mainnet/stagenet/testnet)\nâ”œâ”€â”€ test/           # E2E testing framework\nâ”‚   â”œâ”€â”€ suites/    # Integration test scenarios\nâ”‚   â”œâ”€â”€ framework/ # Test utilities and helpers\nâ”‚   â””â”€â”€ launcher/  # Network deployment automation\nâ”œâ”€â”€ deploy/         # Kubernetes deployment charts\nâ”‚   â”œâ”€â”€ charts/    # Helm charts for nodes and relayers\nâ”‚   â””â”€â”€ environments/ # Environment-specific configurations\nâ”œâ”€â”€ tools/          # GitHub automation and release scripts\nâ””â”€â”€ .github/        # CI/CD workflows\n```\n\nEach directory contains its own README with detailed information. See:\n- [contracts/README.md](contracts/README.md) - Smart contract development\n- [operator/README.md](operator/README.md) - Node building and runtime development\n- [test/README.md](test/README.md) - E2E testing and network deployment\n- [deploy/README.md](deploy/README.md) - Kubernetes deployment\n- [tools/README.md](tools/README.md) - Development tools\n\n## Quick Start\n\n### Prerequisites\n\n- [Kurtosis](https://docs.kurtosis.com/install) - Network orchestration\n- [Bun](https://bun.sh/) v1.3.2+ - TypeScript runtime\n- [Docker](https://www.docker.com/) - Container management\n- [Foundry](https://getfoundry.sh/) - Solidity toolkit\n- [Rust](https://www.rust-lang.org/tools/install) - For building the operator\n- [Helm](https://helm.sh/) - Kubernetes deployments (optional)\n- [Zig](https://ziglang.org/) - For macOS cross-compilation (macOS only)\n\n### Launch Local Network\n\nThe fastest way to get started is with the interactive CLI:\n\n```bash\ncd test\nbun i                    # Install dependencies\nbun cli launch           # Interactive launcher with prompts\n```\n\nThis deploys a complete environment including:\n- **Ethereum network**: 2x EL clients (reth), 2x CL clients (lodestar)\n- **Block explorers**: Blockscout (optional), Dora consensus explorer\n- **DataHaven node**: Single validator with fast block times\n- **Storage providers**: MSP and BSP nodes for decentralized storage\n- **AVS contracts**: Deployed and configured on Ethereum\n- **Snowbridge relayers**: Bidirectional message passing\n\nFor more options and detailed instructions, see the [test README](./test/README.md).\n\n### Run Tests\n\n```bash\ncd test\nbun test:e2e              # Run all integration tests\nbun test:e2e:parallel     # Run with limited concurrency\n```\n\nNOTES: Adding the environment variable `INJECT_CONTRACTS=true` will inject the contracts when starting the tests to speed up setup.\n\n### Development Workflows\n\n**Smart Contract Development**:\n```bash\ncd contracts\nforge build               # Compile contracts\nforge test                # Run contract tests\n```\n\n**Node Development**:\n```bash\ncd operator\ncargo build --release --features fast-runtime\ncargo test\n./scripts/run-benchmarks.sh\n```\n\n**After Making Changes**:\n```bash\ncd test\nbun generate:wagmi        # Regenerate contract bindings\nbun generate:types        # Regenerate runtime types\n```\n\n## Key Features\n\n### Verifiable Decentralized Storage\nProduction-scale storage with cryptographic guarantees:\n- **Buckets**: User-created containers managed by an MSP, summarized by a Merkle-Patricia trie root on-chain\n- **Files**: Deterministically chunked, hashed into Merkle trees, with roots serving as immutable fingerprints\n- **Proofs**: Merkle proofs enable verification of data integrity without trusting intermediaries\n- **Audits**: BSPs prove ongoing data custody via randomized proof challenges\n\n### Storage Provider Network\nTwo-tier provider model balancing performance and reliability:\n- **MSPs**: User-selected providers offering data retrieval with competitive service offerings\n- **BSPs**: Network-assigned backup providers ensuring data redundancy and availability, with on-chain slashing for failed proof challenges\n- **Fisherman**: Auditing service that monitors proofs and triggers challenges for misbehavior\n- **Indexer**: Indexes on-chain storage events for efficient querying\n\n### EigenLayer Security\nDataHaven validators secured through Ethereum restaking:\n- Validators register as operators via `DataHavenServiceManager` contract\n- Economic security through ETH restaking\n- Slashing for validator misbehavior (separate from BSP slashing which is on-chain)\n- Performance-based validator rewards through `RewardsRegistry`\n\n### EVM Compatibility\nFull Ethereum Virtual Machine support via Frontier pallets:\n- Deploy Solidity smart contracts\n- Use existing Ethereum tooling (MetaMask, Hardhat, etc.)\n- Compatible with ERC-20, ERC-721, and other standards\n\n### Cross-chain Communication\nTrustless bridging via Snowbridge:\n- Native token transfers between Ethereum â†” DataHaven\n- Cross-chain message passing\n- Finality proofs via BEEFY consensus\n- Three specialized relayers (beacon, BEEFY, execution)\n\n## Use Cases\n\nDataHaven is designed for applications requiring verifiable, tamper-proof data storage:\n\n- **AI & Machine Learning**: Store training datasets, model weights, and agent configurations with cryptographic proofs of integrity â€” enabling federated learning and verifiable AI pipelines\n- **DePIN (Decentralized Physical Infrastructure)**: Persistent storage for IoT sensor data, device configurations, and operational logs with provable data lineage\n- **Real World Assets (RWAs)**: Immutable storage for asset documentation, ownership records, and compliance data with on-chain verification\n\n## Docker Images\n\nProduction images published to [DockerHub](https://hub.docker.com/r/datahavenxyz/datahaven).\n\n**Build optimizations**:\n- [sccache](https://github.com/mozilla/sccache) - Rust compilation caching\n- [cargo-chef](https://lpalmieri.com/posts/fast-rust-docker-builds/) - Dependency layer caching\n- [BuildKit cache mounts](https://docs.docker.com/build/cache/optimize/#use-cache-mounts) - External cache restoration\n\n**Build locally**:\n```bash\ncd test\nbun build:docker:operator    # Creates datahavenxyz/datahaven:local\n```\n\n## Development Environment\n\n### VS Code Configuration\n\nIDE configurations are excluded from version control for personalization, but these settings are recommended for optimal developer experience. Add to your `.vscode/settings.json`:\n\n**Rust Analyzer**:\n```json\n{\n  \"rust-analyzer.linkedProjects\": [\"./operator/Cargo.toml\"],\n  \"rust-analyzer.cargo.allTargets\": true,\n  \"rust-analyzer.procMacro.enable\": false,\n  \"rust-analyzer.server.extraEnv\": {\n    \"CARGO_TARGET_DIR\": \"target/.rust-analyzer\",\n    \"SKIP_WASM_BUILD\": 1\n  },\n  \"rust-analyzer.diagnostics.disabled\": [\"unresolved-macro-call\"],\n  \"rust-analyzer.cargo.buildScripts.enable\": false\n}\n```\n\nOptimizations:\n- Links `operator/` directory as the primary Rust project\n- Disables proc macros and build scripts for faster analysis (Substrate macros are slow)\n- Uses dedicated target directory to avoid conflicts\n- Skips WASM builds during development\n\n**Solidity** ([Juan Blanco's extension](https://marketplace.visualstudio.com/items?itemName=JuanBlanco.solidity)):\n```json\n{\n  \"solidity.formatter\": \"forge\",\n  \"solidity.compileUsingRemoteVersion\": \"v0.8.28+commit.7893614a\",\n  \"[solidity]\": {\n    \"editor.defaultFormatter\": \"JuanBlanco.solidity\"\n  }\n}\n```\n\nNote: Solidity version must match [foundry.toml](./contracts/foundry.toml)\n\n**TypeScript** ([Biome](https://github.com/biomejs/biome)):\n```json\n{\n  \"biome.lsp.bin\": \"test/node_modules/.bin/biome\",\n  \"[typescript]\": {\n    \"editor.defaultFormatter\": \"biomejs.biome\",\n    \"editor.codeActionsOnSave\": {\n      \"source.organizeImports.biome\": \"always\"\n    }\n  }\n}\n```\n\n## CI/CD\n\n### Local CI Testing\n\nRun GitHub Actions workflows locally using [act](https://github.com/nektos/act):\n\n```bash\n# Run E2E workflow\nact -W .github/workflows/e2e.yml -s GITHUB_TOKEN=\"$(gh auth token)\"\n\n# Run specific job\nact -W .github/workflows/e2e.yml -j test-job-name\n```\n\n### Automated Workflows\n\nThe repository includes GitHub Actions for:\n- **E2E Testing**: Full integration tests on PR and main branch\n- **Contract Testing**: Foundry test suites for smart contracts\n- **Rust Testing**: Unit and integration tests for operator\n- **Docker Builds**: Multi-platform image builds with caching\n- **Release Automation**: Version tagging and changelog generation\n\nSee `.github/workflows/` for workflow definitions.\n\n## Contributing\n\n### Development Cycle\n\n1. **Make Changes**: Edit contracts, runtime, or tests\n2. **Run Tests**: Component-specific tests (`forge test`, `cargo test`)\n3. **Regenerate Types**: Update bindings if contracts/runtime changed\n4. **Integration Test**: Run E2E tests to verify cross-component behavior\n5. **Code Quality**: Format and lint (`cargo fmt`, `forge fmt`, `bun fmt:fix`)\n\n### Common Pitfalls\n\n- **Type mismatches**: Regenerate with `bun generate:types` after runtime changes\n- **Contract changes not reflected**: Run `bun generate:wagmi` after modifications\n- **Kurtosis issues**: Ensure Docker is running and Kurtosis engine is started\n- **Slow development**: Use `--features fast-runtime` for shorter epochs/eras (block time stays 6s)\n- **Network launch hangs**: Check Blockscout - forge output can appear frozen\n\nSee [CLAUDE.md](./CLAUDE.md) for detailed development guidance.\n\n## License\n\nGPL-3.0 - See LICENSE file for details\n\n## Links\n\n- [DataHaven Website](https://datahaven.xyz/)\n- [DataHaven Documentation](https://docs.datahaven.xyz/)\n- [StorageHub Repository](https://github.com/Moonsong-Labs/storage-hub)\n- [EigenLayer Documentation](https://docs.eigenlayer.xyz/)\n- [Substrate Documentation](https://docs.substrate.io/)\n- [Snowbridge Documentation](https://docs.snowbridge.network/)\n- [Foundry Book](https://book.getfoundry.sh/)\n- [Polkadot-API Documentation](https://papi.how/)\n",
      "stars_today": 1034
    },
    {
      "id": 820087727,
      "name": "onlook",
      "full_name": "onlook-dev/onlook",
      "description": "The Cursor for Designers â€¢ An Open-Source AI-First Design tool â€¢ Visually build, style, and edit your React App with AI",
      "html_url": "https://github.com/onlook-dev/onlook",
      "stars": 24355,
      "forks": 1792,
      "language": "TypeScript",
      "topics": [
        "ai",
        "cursor",
        "cursor-ai",
        "design",
        "design-to-code",
        "drizzle",
        "editor",
        "figma",
        "frontend",
        "ide",
        "low-code",
        "nextjs",
        "react",
        "supabase",
        "tailwindcss",
        "typescript",
        "ui",
        "vibe-coding",
        "vibecoding"
      ],
      "created_at": "2024-06-25T19:16:02Z",
      "updated_at": "2026-01-15T00:39:09Z",
      "pushed_at": "2025-12-29T02:49:53Z",
      "open_issues": 352,
      "owner": {
        "login": "onlook-dev",
        "avatar_url": "https://avatars.githubusercontent.com/u/157326433?v=4"
      },
      "readme": "<!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 -->\n\n<div align=\"center\">\n<img width=\"800\" alt=\"header image\" src=\"assets/web-preview.png\">\n<h3 align=\"center\">Onlook</h3>\n  <p align=\"center\">\n    Cursor for Designers\n    <br />\n    <a href=\"https://docs.onlook.com\"><strong>Explore the docs Â»</strong></a>\n    <br />\n  </p>\n  <p align=\"center\">\n    ğŸ‘¨â€ğŸ’»ğŸ‘©â€ğŸ’»ğŸ‘¨â€ğŸ’»\n    <a href=\"https://www.ycombinator.com/companies/onlook/jobs/e4gHv1n-founding-engineer-fullstack\">We're hiring engineers in SF!</a>\n    ğŸ‘©â€ğŸ’»ğŸ‘¨â€ğŸ’»ğŸ‘©â€ğŸ’»\n  </p>\n    <br />\n    <a href=\"https://youtu.be/RSX_3EaO5eU?feature=shared\">View Demo</a>\n    Â·\n    <a href=\"https://github.com/onlook-dev/onlook/issues/new?labels=bug&template=bug-report---.md\">Report Bug</a>\n    Â·\n    <a href=\"https://github.com/onlook-dev/onlook/issues/new?labels=enhancement&template=feature-request---.md\">Request Feature</a>\n  </p>\n  <!-- PROJECT SHIELDS -->\n<!--\n*** I'm using markdown \"reference style\" links for readability.\n*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).\n*** See the bottom of this document for the declaration of the reference variables\n*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.\n*** https://www.markdownguide.org/basic-syntax/#reference-style-links\n-->\n<!-- [![Contributors][contributors-shield]][contributors-url]\n[![Forks][forks-shield]][forks-url]\n[![Stargazers][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n[![Apache License][license-shield]][license-url] -->\n\n[![Discord][discord-shield]][discord-url]\n[![LinkedIn][linkedin-shield]][linkedin-url]\n[![Twitter][twitter-shield]][twitter-url]\n\n[ä¸­æ–‡](https://www.readme-i18n.com/onlook-dev/onlook?lang=zh) |\n[EspaÃ±ol](https://www.readme-i18n.com/onlook-dev/onlook?lang=es) |\n[Deutsch](https://www.readme-i18n.com/onlook-dev/onlook?lang=de) |\n[franÃ§ais](https://www.readme-i18n.com/onlook-dev/onlook?lang=fr) |\n[PortuguÃªs](https://www.readme-i18n.com/onlook-dev/onlook?lang=pt) |\n[Ğ ÑƒÑÑĞºĞ¸Ğ¹](https://www.readme-i18n.com/onlook-dev/onlook?lang=ru) |\n[æ—¥æœ¬èª](https://www.readme-i18n.com/onlook-dev/onlook?lang=ja) |\n[í•œêµ­ì–´](https://www.readme-i18n.com/onlook-dev/onlook?lang=ko)\n\n</div>\n\n# An Open-Source, Visual-First Code Editor\n\nCraft websites, prototypes, and designs with AI in Next.js + TailwindCSS. Make\nedits directly in the browser DOM with a visual editor. Design in realtime with\ncode. An open-source alternative to Bolt.new, Lovable, V0, Replit Agent, Figma\nMake, Webflow, etc.\n\n### ğŸš§ ğŸš§ ğŸš§ Onlook is still under development ğŸš§ ğŸš§ ğŸš§\n\nWe're actively looking for contributors to help make Onlook for Web an\nincredible prompt-to-build experience. Check the\n[open issues](https://github.com/onlook-dev/onlook/issues) for a full list of\nproposed features (and known issues), and join our\n[Discord](https://discord.gg/hERDfFZCsH) to collaborate with hundreds of other\nbuilders.\n\n## What you can do with Onlook:\n\n- [x] Create Next.js app in seconds\n  - [x] Start from text or image\n  - [x] Use prebuilt templates\n  - [ ] Import from Figma\n  - [ ] Import from GitHub repo\n  - [ ] Make a PR to a GitHub repo\n- [x] Visually edit your app\n  - [x] Use Figma-like UI\n  - [x] Preview your app in real-time\n  - [x] Manage brand assets and tokens\n  - [x] Create and navigate to Pages\n  - [x] Browse layers\n  - [x] Manage project Images\n  - [x] Detect and use Components â€“ _Previously in\n        [Onlook Desktop](https://github.com/onlook-dev/desktop)_\n  - [ ] Drag-and-drop Components Panel\n  - [x] Use Branching to experiment with designs\n- [x] Development Tools\n  - [x] Real-time code editor\n  - [x] Save and restore from checkpoints\n  - [x] Run commands via CLI\n  - [x] Connect with app marketplace\n- [x] Deploy your app in seconds\n  - [x] Generate sharable links\n  - [x] Link your custom domain    \n- [ ] Collaborate with your team\n  - [x] Real-time editing\n  - [ ] Leave comments\n- [ ] Advanced AI capabilities\n  - [x] Queue multiple messages at once\n  - [ ] Use Images as references and as assets in a project\n  - [ ] Setup and use MCPs in projects\n  - [ ] Allow Onlook to use itself as a toolcall for branch creation and iteration\n- [ ] Advanced project support\n  - [ ] Support non-NextJS projects\n  - [ ] Support non-Tailwind projects\n\n![Onlook-GitHub-Example](https://github.com/user-attachments/assets/642de37a-72cc-4056-8eb7-8eb42714cdc4)\n\n## Getting Started\n\nUse our [hosted app](https://onlook.com) or\n[run locally](https://docs.onlook.com/developers/running-locally).\n\n### Usage\n\nOnlook will run on any Next.js + TailwindCSS project, import your project into\nOnlook or start from scratch within the editor.\n\nUse the AI chat to create or edit a project you're working on. At any time, you\ncan always right-click an element to open up the exact location of the element\nin code.\n\n<img width=\"600\" alt=\"image\" src=\"https://github.com/user-attachments/assets/4ad9f411-b172-4430-81ef-650f4f314666\" />\n\n<br>\n\nDraw-in new divs and re-arrange them within their parent containers by\ndragging-and-dropping.\n\n<img width=\"600\" alt=\"image\" src=\"assets/insert-div.png\">\n\n<br>\n\nPreview the code side-by-side with your site design.\n\n<img width=\"600\" alt=\"image\" src=\"assets/code-connect.png\">\n\n<br>\n\nUse Onlook's editor toolbar to adjust Tailwind styles, directly manipulate\nobjects, and experiment with layouts.\n\n<img width=\"600\" alt=\"image\" src=\"assets/text-styling.png\" />\n\n## Documentation\n\nFor full documentation, visit [docs.onlook.com](https://docs.onlook.com)\n\nTo see how to Contribute, visit\n[Contributing to Onlook](https://docs.onlook.com/developers) in our docs.\n\n## How it works\n\n<img width=\"676\" alt=\"architecture\" src=\"assets/architecture.png\">\n\n1. When you create an app, we load the code into a web container\n2. The container runs and serves the code\n3. Our editor receives the preview link and displays it in an iFrame\n4. Our editor reads and indexes the code from the container\n5. We instrument the code in order to map elements to their place in code\n6. When the element is edited, we edit the element in our iFrame, then in code\n7. Our AI chat also has code access and tools to understand and edit the code\n\nThis architecture can theoretically scale to any language or framework that\ndisplays DOM elements declaratively (e.g. jsx/tsx/html). We are focused on\nmaking it work well with Next.js and TailwindCSS for now.\n\nFor a full walkthrough, check out our\n[Architecture Docs](https://docs.onlook.com/developers/architecture).\n\n### Our Tech Stack\n\n#### Front-end\n\n- [Next.js](https://nextjs.org/) - Full stack\n- [TailwindCSS](https://tailwindcss.com/) - Styling\n- [tRPC](https://trpc.io/) - Server interface\n\n#### Database\n\n- [Supabase](https://supabase.com/) - Auth, Database, Storage\n- [Drizzle](https://orm.drizzle.team/) - ORM\n\n#### AI\n\n- [AI SDK](https://ai-sdk.dev/) - LLM client\n- [OpenRouter](https://openrouter.ai/) - LLM model provider\n- [Morph Fast Apply](https://morphllm.com) - Fast apply model provider\n- [Relace](https://relace.ai) - Fast apply model provider\n\n#### Sandbox and hosting\n\n- [CodeSandboxSDK](https://codesandbox.io/docs/sdk) - Dev sandbox\n- [Freestyle](https://www.freestyle.sh/) - Hosting\n\n#### Runtime\n\n- [Bun](https://bun.sh/) - Monorepo, runtime, bundler\n- [Docker](https://www.docker.com/) - Container management\n\n## Contributing\n\n![image](https://github.com/user-attachments/assets/ecc94303-df23-46ae-87dc-66b040396e0b)\n\nIf you have a suggestion that would make this better, please fork the repo and\ncreate a pull request. You can also\n[open issues](https://github.com/onlook-dev/onlook/issues).\n\nSee the [CONTRIBUTING.md](CONTRIBUTING.md) for instructions and code of conduct.\n\n#### Contributors\n\n<a href=\"https://github.com/onlook-dev/onlook/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=onlook-dev/onlook\" />\n</a>\n\n## Contact\n\n![image](https://github.com/user-attachments/assets/60684b68-1925-4550-8efd-51a1509fc953)\n\n- Team: [Discord](https://discord.gg/hERDfFZCsH) -\n  [Twitter](https://twitter.com/onlookdev) -\n  [LinkedIn](https://www.linkedin.com/company/onlook-dev/) -\n  [Email](mailto:contact@onlook.com)\n- Project:\n  [https://github.com/onlook-dev/onlook](https://github.com/onlook-dev/onlook)\n- Website: [https://onlook.com](https://onlook.com)\n\n## License\n\nDistributed under the Apache 2.0 License. See [LICENSE.md](LICENSE.md) for more\ninformation.\n\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n\n[contributors-shield]: https://img.shields.io/github/contributors/onlook-dev/studio.svg?style=for-the-badge\n[contributors-url]: https://github.com/onlook-dev/onlook/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/onlook-dev/studio.svg?style=for-the-badge\n[forks-url]: https://github.com/onlook-dev/onlook/network/members\n[stars-shield]: https://img.shields.io/github/stars/onlook-dev/studio.svg?style=for-the-badge\n[stars-url]: https://github.com/onlook-dev/onlook/stargazers\n[issues-shield]: https://img.shields.io/github/issues/onlook-dev/studio.svg?style=for-the-badge\n[issues-url]: https://github.com/onlook-dev/onlook/issues\n[license-shield]: https://img.shields.io/github/license/onlook-dev/studio.svg?style=for-the-badge\n[license-url]: https://github.com/onlook-dev/onlook/blob/master/LICENSE.txt\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?logo=linkedin&colorB=555\n[linkedin-url]: https://www.linkedin.com/company/onlook-dev\n[twitter-shield]: https://img.shields.io/badge/-Twitter-black?logo=x&colorB=555\n[twitter-url]: https://x.com/onlookdev\n[discord-shield]: https://img.shields.io/badge/-Discord-black?logo=discord&colorB=555\n[discord-url]: https://discord.gg/hERDfFZCsH\n[React.js]: https://img.shields.io/badge/react-%2320232a.svg?logo=react&logoColor=%2361DAFB\n[React-url]: https://reactjs.org/\n[TailwindCSS]: https://img.shields.io/badge/tailwindcss-%2338B2AC.svg?logo=tailwind-css&logoColor=white\n[Tailwind-url]: https://tailwindcss.com/\n[Electron.js]: https://img.shields.io/badge/Electron-191970?logo=Electron&logoColor=white\n[Electron-url]: https://www.electronjs.org/\n[Vite.js]: https://img.shields.io/badge/vite-%23646CFF.svg?logo=vite&logoColor=white\n[Vite-url]: https://vitejs.dev/\n[product-screenshot]: assets/brand.png\n[weave-shield]: https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_pWcXBHJo3Li2Te2Y4WkCPA33%2F820087727&cacheSeconds=3600&labelColor=#131313\n[weave-url]: https://app.workweave.ai/reports/repository/org_pWcXBHJo3Li2Te2Y4WkCPA33/820087727\n",
      "stars_today": 366
    },
    {
      "id": 167694194,
      "name": "frigate",
      "full_name": "blakeblackshear/frigate",
      "description": "NVR with realtime local object detection for IP cameras",
      "html_url": "https://github.com/blakeblackshear/frigate",
      "stars": 29190,
      "forks": 2723,
      "language": "TypeScript",
      "topics": [
        "ai",
        "camera",
        "google-coral",
        "home-assistant",
        "home-automation",
        "homeautomation",
        "mqtt",
        "nvr",
        "object-detection",
        "realtime",
        "rtsp",
        "tensorflow"
      ],
      "created_at": "2019-01-26T13:52:38Z",
      "updated_at": "2026-01-15T00:54:40Z",
      "pushed_at": "2026-01-14T23:10:26Z",
      "open_issues": 147,
      "owner": {
        "login": "blakeblackshear",
        "avatar_url": "https://avatars.githubusercontent.com/u/569905?v=4"
      },
      "readme": "<p align=\"center\">\n  <img align=\"center\" alt=\"logo\" src=\"docs/static/img/branding/frigate.png\">\n</p>\n\n# Frigate NVRâ„¢ - Realtime Object Detection for IP Cameras\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n<a href=\"https://hosted.weblate.org/engage/frigate-nvr/\">\n<img src=\"https://hosted.weblate.org/widget/frigate-nvr/language-badge.svg\" alt=\"Translation status\" />\n</a>\n\n\\[English\\] | [ç®€ä½“ä¸­æ–‡](https://github.com/blakeblackshear/frigate/blob/dev/README_CN.md)\n\nA complete and local NVR designed for [Home Assistant](https://www.home-assistant.io) with AI object detection. Uses OpenCV and Tensorflow to perform realtime object detection locally for IP cameras.\n\nUse of a GPU or AI accelerator is highly recommended. AI accelerators will outperform even the best CPUs with very little overhead. See Frigate's supported [object detectors](https://docs.frigate.video/configuration/object_detectors/).\n\n- Tight integration with Home Assistant via a [custom component](https://github.com/blakeblackshear/frigate-hass-integration)\n- Designed to minimize resource use and maximize performance by only looking for objects when and where it is necessary\n- Leverages multiprocessing heavily with an emphasis on realtime over processing every frame\n- Uses a very low overhead motion detection to determine where to run object detection\n- Object detection with TensorFlow runs in separate processes for maximum FPS\n- Communicates over MQTT for easy integration into other systems\n- Records video with retention settings based on detected objects\n- 24/7 recording\n- Re-streaming via RTSP to reduce the number of connections to your camera\n- WebRTC & MSE support for low-latency live view\n\n## Documentation\n\nView the documentation at https://docs.frigate.video\n\n## Donations\n\nIf you would like to make a donation to support development, please use [Github Sponsors](https://github.com/sponsors/blakeblackshear).\n\n## License\n\nThis project is licensed under the **MIT License**.\n\n- **Code:** The source code, configuration files, and documentation in this repository are available under the [MIT License](LICENSE). You are free to use, modify, and distribute the code as long as you include the original copyright notice.\n- **Trademarks:** The \"Frigate\" name, the \"Frigate NVR\" brand, and the Frigate logo are **trademarks of Frigate, Inc.** and are **not** covered by the MIT License.\n\nPlease see our [Trademark Policy](TRADEMARK.md) for details on acceptable use of our brand assets.\n\n## Screenshots\n\n### Live dashboard\n\n<div>\n<img width=\"800\" alt=\"Live dashboard\" src=\"https://github.com/blakeblackshear/frigate/assets/569905/5e713cb9-9db5-41dc-947a-6937c3bc376e\">\n</div>\n\n### Streamlined review workflow\n\n<div>\n<img width=\"800\" alt=\"Streamlined review workflow\" src=\"https://github.com/blakeblackshear/frigate/assets/569905/6fed96e8-3b18-40e5-9ddc-31e6f3c9f2ff\">\n</div>\n\n### Multi-camera scrubbing\n\n<div>\n<img width=\"800\" alt=\"Multi-camera scrubbing\" src=\"https://github.com/blakeblackshear/frigate/assets/569905/d6788a15-0eeb-4427-a8d4-80b93cae3d74\">\n</div>\n\n### Built-in mask and zone editor\n\n<div>\n<img width=\"800\" alt=\"Built-in mask and zone editor\" src=\"https://github.com/blakeblackshear/frigate/assets/569905/d7885fc3-bfe6-452f-b7d0-d957cb3e31f5\">\n</div>\n\n## Translations\n\nWe use [Weblate](https://hosted.weblate.org/projects/frigate-nvr/) to support language translations. Contributions are always welcome.\n\n<a href=\"https://hosted.weblate.org/engage/frigate-nvr/\">\n<img src=\"https://hosted.weblate.org/widget/frigate-nvr/multi-auto.svg\" alt=\"Translation status\" />\n</a>\n\n---\n\n**Copyright Â© 2026 Frigate, Inc.**\n",
      "stars_today": 340
    },
    {
      "id": 1031912724,
      "name": "cc-switch",
      "full_name": "farion1231/cc-switch",
      "description": "A cross-platform desktop All-in-One assistant tool for Claude Code, Codex & Gemini CLI.",
      "html_url": "https://github.com/farion1231/cc-switch",
      "stars": 11256,
      "forks": 740,
      "language": "Rust",
      "topics": [
        "ai-tools",
        "claude-code",
        "codex",
        "deepseek-v3",
        "desktop-app",
        "kimi-k2-thiking",
        "mcp",
        "minimax",
        "open-source",
        "provider-management",
        "qwen-coder",
        "rust",
        "tauri",
        "typescript",
        "wsl-support"
      ],
      "created_at": "2025-08-04T14:16:16Z",
      "updated_at": "2026-01-15T01:01:28Z",
      "pushed_at": "2026-01-14T17:10:44Z",
      "open_issues": 113,
      "owner": {
        "login": "farion1231",
        "avatar_url": "https://avatars.githubusercontent.com/u/44939412?v=4"
      },
      "readme": "<div align=\"center\">\n\n# All-in-One Assistant for Claude Code, Codex & Gemini CLI\n\n[![Version](https://img.shields.io/badge/version-3.9.1-blue.svg)](https://github.com/farion1231/cc-switch/releases)\n[![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20macOS%20%7C%20Linux-lightgrey.svg)](https://github.com/farion1231/cc-switch/releases)\n[![Built with Tauri](https://img.shields.io/badge/built%20with-Tauri%202-orange.svg)](https://tauri.app/)\n[![Downloads](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/farion1231/cc-switch/total)](https://github.com/farion1231/cc-switch/releases/latest)\n\n<a href=\"https://trendshift.io/repositories/15372\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/15372\" alt=\"farion1231%2Fcc-switch | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\nEnglish | [ä¸­æ–‡](README_ZH.md) | [æ—¥æœ¬èª](README_JA.md) | [Changelog](CHANGELOG.md)\n\n</div>\n\n## â¤ï¸Sponsor\n\n[![Zhipu GLM](assets/partners/banners/glm-en.jpg)](https://z.ai/subscribe?ic=8JVLJQFSKB)\n\nThis project is sponsored by Z.ai, supporting us with their GLM CODING PLAN.GLM CODING PLAN is a subscription service designed for AI coding, starting at just $3/month. It provides access to their flagship GLM-4.6 model across 10+ popular AI coding tools (Claude Code, Cline, Roo Code, etc.), offering developers top-tier, fast, and stable coding experiences.Get 10% OFF the GLM CODING PLAN with [this link](https://z.ai/subscribe?ic=8JVLJQFSKB)!\n\n---\n\n<table>\n<tr>\n<td width=\"180\"><a href=\"https://www.packyapi.com/register?aff=cc-switch\"><img src=\"assets/partners/logos/packycode.png\" alt=\"PackyCode\" width=\"150\"></a></td>\n<td>Thanks to PackyCode for sponsoring this project! PackyCode is a reliable and efficient API relay service provider, offering relay services for Claude Code, Codex, Gemini, and more. PackyCode provides special discounts for our software users: register using <a href=\"https://www.packyapi.com/register?aff=cc-switch\">this link</a> and enter the \"cc-switch\" promo code during recharge to get 10% off.</td>\n</tr>\n\n<tr>\n<td width=\"180\"><a href=\"https://aigocode.com/invite/CC-SWITCH\"><img src=\"assets/partners/logos/aigocode.png\" alt=\"AIGoCode\" width=\"150\"></a></td>\n<td>Thanks to AIGoCode for sponsoring this project! AIGoCode is an all-in-one platform that integrates Claude Code, Codex, and the latest Gemini models, providing you with stable, efficient, and highly cost-effective AI coding services. The platform offers flexible subscription plans, zero risk of account suspension, direct access with no VPN required, and lightning-fast responses. AIGoCode has prepared a special benefit for CC Switch users: if you register via <a href=\"https://aigocode.com/invite/CC-SWITCH\">this link</a>, you'll receive an extra 10% bonus credit on your first top-up!</td>\n</tr>\n\n<tr>\n<td width=\"180\"><a href=\"https://www.dmxapi.cn/register?aff=bUHu\"><img src=\"assets/partners/logos/dmx-en.jpg\" alt=\"DMXAPI\" width=\"150\"></a></td>\n<td>Thanks to DMXAPI for sponsoring this project! DMXAPI provides global large model API services to 200+ enterprise users. One API key for all global models. Features include: instant invoicing, unlimited concurrency, starting from $0.15, 24/7 technical support. GPT/Claude/Gemini all at 32% off, domestic models 20-50% off, Claude Code exclusive models at 66% off! <a href=\"https://www.dmxapi.cn/register?aff=bUHu\">Register here</a></td>\n</tr>\n\n<tr>\n<td width=\"180\"><a href=\"https://cubence.com/signup?code=CCSWITCH&source=ccs\"><img src=\"assets/partners/logos/cubence.png\" alt=\"Cubence\" width=\"150\"></a></td>\n<td>Thanks to Cubence for sponsoring this project! Cubence is a reliable and efficient API relay service provider, offering relay services for Claude Code, Codex, Gemini, and more with flexible billing options including pay-as-you-go and monthly plans. Cubence provides special discounts for CC Switch users: register using <a href=\"https://cubence.com/signup?code=CCSWITCH&source=ccs\">this link</a> and enter the \"CCSWITCH\" promo code during recharge to get 10% off every top-up!</td>\n</tr>\n\n</table>\n\n## Screenshots\n\n|                  Main Interface                   |                  Add Provider                  |\n| :-----------------------------------------------: | :--------------------------------------------: |\n| ![Main Interface](assets/screenshots/main-en.png) | ![Add Provider](assets/screenshots/add-en.png) |\n\n## Features\n\n### Current Version: v3.9.1 | [Full Changelog](CHANGELOG.md) | [Release Notes](docs/release-note-v3.9.0-en.md)\n\n**v3.8.0 Major Update (2025-11-28)**\n\n**Persistence Architecture Upgrade & Brand New UI**\n\n- **SQLite + JSON Dual-layer Architecture**\n  - Migrated from JSON file storage to SQLite + JSON dual-layer structure\n  - Syncable data (providers, MCP, Prompts, Skills) stored in SQLite\n  - Device-level data (window state, local paths) stored in JSON\n  - Lays the foundation for future cloud sync functionality\n  - Schema version management for database migrations\n\n- **Brand New User Interface**\n  - Completely redesigned interface layout\n  - Unified component styles and smoother animations\n  - Optimized visual hierarchy\n  - Tailwind CSS downgraded from v4 to v3.4 for better browser compatibility\n\n- **Japanese Language Support**\n  - Added Japanese interface support (now supports Chinese/English/Japanese)\n\n- **Auto Launch on Startup**\n  - One-click enable/disable in settings\n  - Platform-native APIs (Registry/LaunchAgent/XDG autostart)\n\n- **Skills Recursive Scanning**\n  - Support for multi-level directory structures\n  - Allow same-named skills from different repositories\n\n- **Critical Bug Fixes**\n  - Fixed custom endpoints lost when updating providers\n  - Fixed Gemini configuration write issues\n  - Fixed Linux WebKitGTK rendering issues\n\n**v3.7.0 Highlights**\n\n**Six Core Features, 18,000+ Lines of New Code**\n\n- **Gemini CLI Integration**\n  - Third supported AI CLI (Claude Code / Codex / Gemini)\n  - Dual-file configuration support (`.env` + `settings.json`)\n  - Complete MCP server management\n  - Presets: Google Official (OAuth) / PackyCode / Custom\n\n- **Claude Skills Management System**\n  - Auto-scan skills from GitHub repositories (3 pre-configured curated repos)\n  - One-click install/uninstall to `~/.claude/skills/`\n  - Custom repository support + subdirectory scanning\n  - Complete lifecycle management (discover/install/update)\n\n- **Prompts Management System**\n  - Multi-preset system prompt management (unlimited presets, quick switching)\n  - Cross-app support (Claude: `CLAUDE.md` / Codex: `AGENTS.md` / Gemini: `GEMINI.md`)\n  - Markdown editor (CodeMirror 6 + real-time preview)\n  - Smart backfill protection, preserves manual modifications\n\n- **MCP v3.7.0 Unified Architecture**\n  - Single panel manages MCP servers across three applications\n  - New SSE (Server-Sent Events) transport type\n  - Smart JSON parser + Codex TOML format auto-correction\n  - Unified import/export + bidirectional sync\n\n- **Deep Link Protocol**\n  - `ccswitch://` protocol registration (all platforms)\n  - One-click import provider configs via shared links\n  - Security validation + lifecycle integration\n\n- **Environment Variable Conflict Detection**\n  - Auto-detect cross-app configuration conflicts (Claude/Codex/Gemini/MCP)\n  - Visual conflict indicators + resolution suggestions\n  - Override warnings + backup before changes\n\n**Core Capabilities**\n\n- **Provider Management**: One-click switching between Claude Code, Codex, and Gemini API configurations\n- **Speed Testing**: Measure API endpoint latency with visual quality indicators\n- **Import/Export**: Backup and restore configs with auto-rotation (keep 10 most recent)\n- **i18n Support**: Complete Chinese/English localization (UI, errors, tray)\n- **Claude Plugin Sync**: One-click apply/restore Claude plugin configurations\n\n**v3.6 Highlights**\n\n- Provider duplication & drag-and-drop sorting\n- Multi-endpoint management & custom config directory (cloud sync ready)\n- Granular model configuration (4-tier: Haiku/Sonnet/Opus/Custom)\n- WSL environment support with auto-sync on directory change\n- 100% hooks test coverage & complete architecture refactoring\n\n**System Features**\n\n- System tray with quick switching\n- Single instance daemon\n- Built-in auto-updater\n- Atomic writes with rollback protection\n\n## Download & Installation\n\n### System Requirements\n\n- **Windows**: Windows 10 and above\n- **macOS**: macOS 10.15 (Catalina) and above\n- **Linux**: Ubuntu 22.04+ / Debian 11+ / Fedora 34+ and other mainstream distributions\n\n### Windows Users\n\nDownload the latest `CC-Switch-v{version}-Windows.msi` installer or `CC-Switch-v{version}-Windows-Portable.zip` portable version from the [Releases](../../releases) page.\n\n### macOS Users\n\n**Method 1: Install via Homebrew (Recommended)**\n\n```bash\nbrew tap farion1231/ccswitch\nbrew install --cask cc-switch\n```\n\nUpdate:\n\n```bash\nbrew upgrade --cask cc-switch\n```\n\n**Method 2: Manual Download**\n\nDownload `CC-Switch-v{version}-macOS.zip` from the [Releases](../../releases) page and extract to use.\n\n> **Note**: Since the author doesn't have an Apple Developer account, you may see an \"unidentified developer\" warning on first launch. Please close it first, then go to \"System Settings\" â†’ \"Privacy & Security\" â†’ click \"Open Anyway\", and you'll be able to open it normally afterwards.\n\n### ArchLinux ç”¨æˆ·\n\n**Install via paru (Recommended)**\n\n```bash\nparu -S cc-switch-bin\n```\n\n### Linux Users\n\nDownload the latest Linux build from the [Releases](../../releases) page:\n\n- `CC-Switch-v{version}-Linux.deb` (Debian/Ubuntu)\n- `CC-Switch-v{version}-Linux.rpm` (Fedora/RHEL/openSUSE)\n- `CC-Switch-v{version}-Linux.AppImage` (Universal)\n- `CC-Switch-v{version}-Linux.flatpak` (Flatpak)\n\nFlatpak install & run:\n\n```bash\nflatpak install --user ./CC-Switch-v{version}-Linux.flatpak\nflatpak run com.ccswitch.desktop\n```\n\n## Quick Start\n\n### Basic Usage\n\n1. **Add Provider**: Click \"Add Provider\" â†’ Choose preset or create custom configuration\n2. **Switch Provider**:\n   - Main UI: Select provider â†’ Click \"Enable\"\n   - System Tray: Click provider name directly (instant effect)\n3. **Takes Effect**: Restart your terminal or Claude Code / Codex / Gemini clients to apply changes\n4. **Back to Official**: Select the \"Official Login\" preset (Claude/Codex) or \"Google Official\" preset (Gemini), restart the corresponding client, then follow its login/OAuth flow\n\n### MCP Management\n\n- **Location**: Click \"MCP\" button in top-right corner\n- **Add Server**:\n  - Use built-in templates (mcp-fetch, mcp-filesystem, etc.)\n  - Support stdio / http / sse transport types\n  - Configure independent MCP servers for different apps\n- **Enable/Disable**: Toggle switches to control which servers sync to live config\n- **Sync**: Enabled servers auto-sync to each app's live files\n- **Import/Export**: Import existing MCP servers from Claude/Codex/Gemini config files\n\n### Skills Management (v3.7.0 New)\n\n- **Location**: Click \"Skills\" button in top-right corner\n- **Discover Skills**:\n  - Auto-scan pre-configured GitHub repositories (Anthropic official, ComposioHQ, community, etc.)\n  - Add custom repositories (supports subdirectory scanning)\n- **Install Skills**: Click \"Install\" to one-click install to `~/.claude/skills/`\n- **Uninstall Skills**: Click \"Uninstall\" to safely remove and clean up state\n- **Manage Repositories**: Add/remove custom GitHub repositories\n\n### Prompts Management (v3.7.0 New)\n\n- **Location**: Click \"Prompts\" button in top-right corner\n- **Create Presets**:\n  - Create unlimited system prompt presets\n  - Use Markdown editor to write prompts (syntax highlighting + real-time preview)\n- **Switch Presets**: Select preset â†’ Click \"Activate\" to apply immediately\n- **Sync Mechanism**:\n  - Claude: `~/.claude/CLAUDE.md`\n  - Codex: `~/.codex/AGENTS.md`\n  - Gemini: `~/.gemini/GEMINI.md`\n- **Protection Mechanism**: Auto-save current prompt content before switching, preserves manual modifications\n\n### Configuration Files\n\n**Claude Code**\n\n- Live config: `~/.claude/settings.json` (or `claude.json`)\n- API key field: `env.ANTHROPIC_AUTH_TOKEN` or `env.ANTHROPIC_API_KEY`\n- MCP servers: `~/.claude.json` â†’ `mcpServers`\n\n**Codex**\n\n- Live config: `~/.codex/auth.json` (required) + `config.toml` (optional)\n- API key field: `OPENAI_API_KEY` in `auth.json`\n- MCP servers: `~/.codex/config.toml` â†’ `[mcp_servers]` tables\n\n**Gemini**\n\n- Live config: `~/.gemini/.env` (API key) + `~/.gemini/settings.json` (auth mode)\n- API key field: `GEMINI_API_KEY` or `GOOGLE_GEMINI_API_KEY` in `.env`\n- Environment variables: Support `GOOGLE_GEMINI_BASE_URL`, `GEMINI_MODEL`, etc.\n- MCP servers: `~/.gemini/settings.json` â†’ `mcpServers`\n- Tray quick switch: Each provider switch rewrites `~/.gemini/.env`, no need to restart Gemini CLI\n\n**CC Switch Storage (v3.8.0 New Architecture)**\n\n- Database (SSOT): `~/.cc-switch/cc-switch.db` (SQLite, stores providers, MCP, Prompts, Skills)\n- Local settings: `~/.cc-switch/settings.json` (device-level settings)\n- Backups: `~/.cc-switch/backups/` (auto-rotate, keep 10)\n\n### Cloud Sync Setup\n\n1. Go to Settings â†’ \"Custom Configuration Directory\"\n2. Choose your cloud sync folder (Dropbox, OneDrive, iCloud, etc.)\n3. Restart app to apply\n4. Repeat on other devices to enable cross-device sync\n\n> **Note**: First launch auto-imports existing Claude/Codex configs as default provider.\n\n## Architecture Overview\n\n### Design Principles\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Frontend (React + TS)                    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚ Components  â”‚  â”‚    Hooks     â”‚  â”‚  TanStack Query  â”‚    â”‚\nâ”‚  â”‚   (UI)      â”‚â”€â”€â”‚ (Bus. Logic) â”‚â”€â”€â”‚   (Cache/Sync)   â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                         â”‚ Tauri IPC\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                  Backend (Tauri + Rust)                     â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚  Commands   â”‚  â”‚   Services   â”‚  â”‚  Models/Config   â”‚    â”‚\nâ”‚  â”‚ (API Layer) â”‚â”€â”€â”‚ (Bus. Layer) â”‚â”€â”€â”‚     (Data)       â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Core Design Patterns**\n\n- **SSOT** (Single Source of Truth): All data stored in `~/.cc-switch/cc-switch.db` (SQLite)\n- **Dual-layer Storage**: SQLite for syncable data, JSON for device-level settings\n- **Dual-way Sync**: Write to live files on switch, backfill from live when editing active provider\n- **Atomic Writes**: Temp file + rename pattern prevents config corruption\n- **Concurrency Safe**: Mutex-protected database connection avoids race conditions\n- **Layered Architecture**: Clear separation (Commands â†’ Services â†’ DAO â†’ Database)\n\n**Key Components**\n\n- **ProviderService**: Provider CRUD, switching, backfill, sorting\n- **McpService**: MCP server management, import/export, live file sync\n- **ConfigService**: Config import/export, backup rotation\n- **SpeedtestService**: API endpoint latency measurement\n\n**v3.6 Refactoring**\n\n- Backend: 5-phase refactoring (error handling â†’ command split â†’ tests â†’ services â†’ concurrency)\n- Frontend: 4-stage refactoring (test infra â†’ hooks â†’ components â†’ cleanup)\n- Testing: 100% hooks coverage + integration tests (vitest + MSW)\n\n## Development\n\n### Environment Requirements\n\n- Node.js 18+\n- pnpm 8+\n- Rust 1.85+\n- Tauri CLI 2.8+\n\n### Development Commands\n\n```bash\n# Install dependencies\npnpm install\n\n# Dev mode (hot reload)\npnpm dev\n\n# Type check\npnpm typecheck\n\n# Format code\npnpm format\n\n# Check code format\npnpm format:check\n\n# Run frontend unit tests\npnpm test:unit\n\n# Run tests in watch mode (recommended for development)\npnpm test:unit:watch\n\n# Build application\npnpm build\n\n# Build debug version\npnpm tauri build --debug\n```\n\n### Rust Backend Development\n\n```bash\ncd src-tauri\n\n# Format Rust code\ncargo fmt\n\n# Run clippy checks\ncargo clippy\n\n# Run backend tests\ncargo test\n\n# Run specific tests\ncargo test test_name\n\n# Run tests with test-hooks feature\ncargo test --features test-hooks\n```\n\n### Testing Guide (v3.6 New)\n\n**Frontend Testing**:\n\n- Uses **vitest** as test framework\n- Uses **MSW (Mock Service Worker)** to mock Tauri API calls\n- Uses **@testing-library/react** for component testing\n\n**Test Coverage**:\n\n- Hooks unit tests (100% coverage)\n  - `useProviderActions` - Provider operations\n  - `useMcpActions` - MCP management\n  - `useSettings` series - Settings management\n  - `useImportExport` - Import/export\n- Integration tests\n  - App main application flow\n  - SettingsDialog complete interaction\n  - MCP panel functionality\n\n**Running Tests**:\n\n```bash\n# Run all tests\npnpm test:unit\n\n# Watch mode (auto re-run)\npnpm test:unit:watch\n\n# With coverage report\npnpm test:unit --coverage\n```\n\n## Tech Stack\n\n**Frontend**: React 18 Â· TypeScript Â· Vite Â· TailwindCSS 4 Â· TanStack Query v5 Â· react-i18next Â· react-hook-form Â· zod Â· shadcn/ui Â· @dnd-kit\n\n**Backend**: Tauri 2.8 Â· Rust Â· serde Â· tokio Â· thiserror Â· tauri-plugin-updater/process/dialog/store/log\n\n**Testing**: vitest Â· MSW Â· @testing-library/react\n\n## Project Structure\n\n```\nâ”œâ”€â”€ src/                      # Frontend (React + TypeScript)\nâ”‚   â”œâ”€â”€ components/           # UI components (providers/settings/mcp/ui)\nâ”‚   â”œâ”€â”€ hooks/                # Custom hooks (business logic)\nâ”‚   â”œâ”€â”€ lib/\nâ”‚   â”‚   â”œâ”€â”€ api/              # Tauri API wrapper (type-safe)\nâ”‚   â”‚   â””â”€â”€ query/            # TanStack Query config\nâ”‚   â”œâ”€â”€ i18n/locales/         # Translations (zh/en)\nâ”‚   â”œâ”€â”€ config/               # Presets (providers/mcp)\nâ”‚   â””â”€â”€ types/                # TypeScript definitions\nâ”œâ”€â”€ src-tauri/                # Backend (Rust)\nâ”‚   â””â”€â”€ src/\nâ”‚       â”œâ”€â”€ commands/         # Tauri command layer (by domain)\nâ”‚       â”œâ”€â”€ services/         # Business logic layer\nâ”‚       â”œâ”€â”€ app_config.rs     # Config data models\nâ”‚       â”œâ”€â”€ provider.rs       # Provider domain models\nâ”‚       â”œâ”€â”€ mcp.rs            # MCP sync & validation\nâ”‚       â””â”€â”€ lib.rs            # App entry & tray menu\nâ”œâ”€â”€ tests/                    # Frontend tests\nâ”‚   â”œâ”€â”€ hooks/                # Unit tests\nâ”‚   â””â”€â”€ components/           # Integration tests\nâ””â”€â”€ assets/                   # Screenshots & partner resources\n```\n\n## Changelog\n\nSee [CHANGELOG.md](CHANGELOG.md) for version update details.\n\n## Legacy Electron Version\n\n[Releases](../../releases) retains v2.0.3 legacy Electron version\n\nIf you need legacy Electron code, you can pull the electron-legacy branch\n\n## Contributing\n\nIssues and suggestions are welcome!\n\nBefore submitting PRs, please ensure:\n\n- Pass type check: `pnpm typecheck`\n- Pass format check: `pnpm format:check`\n- Pass unit tests: `pnpm test:unit`\n- ğŸ’¡ For new features, please open an issue for discussion before submitting a PR\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=farion1231/cc-switch&type=Date)](https://www.star-history.com/#farion1231/cc-switch&Date)\n\n## License\n\nMIT Â© Jason Young\n",
      "stars_today": 244
    },
    {
      "id": 436297812,
      "name": "memos",
      "full_name": "usememos/memos",
      "description": "An open-source, self-hosted note-taking service. Your thoughts, your data, your control â€” no tracking, no ads, no subscription fees.",
      "html_url": "https://github.com/usememos/memos",
      "stars": 54124,
      "forks": 3887,
      "language": "Go",
      "topics": [
        "docker",
        "foss",
        "go",
        "markdown",
        "memo",
        "microblog",
        "note-taking",
        "notecard",
        "react",
        "self-hosted",
        "social-network",
        "sqlite"
      ],
      "created_at": "2021-12-08T15:30:18Z",
      "updated_at": "2026-01-15T01:01:11Z",
      "pushed_at": "2026-01-14T15:56:27Z",
      "open_issues": 54,
      "owner": {
        "login": "usememos",
        "avatar_url": "https://avatars.githubusercontent.com/u/95764151?v=4"
      },
      "readme": "# Memos\n\n<img align=\"right\" height=\"96px\" src=\"https://raw.githubusercontent.com/usememos/.github/refs/heads/main/assets/logo-rounded.png\" alt=\"Memos\" />\n\nAn open-source, self-hosted note-taking service. Your thoughts, your data, your control â€” no tracking, no ads, no subscription fees.\n\n[![Home](https://img.shields.io/badge/ğŸ -usememos.com-blue?style=flat-square)](https://usememos.com)\n[![Live Demo](https://img.shields.io/badge/âœ¨-Try%20Demo-orange?style=flat-square)](https://demo.usememos.com/)\n[![Docs](https://img.shields.io/badge/ğŸ“š-Documentation-green?style=flat-square)](https://usememos.com/docs)\n[![Discord](https://img.shields.io/badge/ğŸ’¬-Discord-5865f2?style=flat-square&logo=discord&logoColor=white)](https://discord.gg/tfPJa4UmAv)\n[![Docker Pulls](https://img.shields.io/docker/pulls/neosmemo/memos?style=flat-square&logo=docker)](https://hub.docker.com/r/neosmemo/memos)\n\n<img src=\"https://raw.githubusercontent.com/usememos/.github/refs/heads/main/assets/demo.png\" alt=\"Memos Demo Screenshot\" height=\"512\" />\n\n### ğŸ’ Featured Sponsors\n\n[**Warp** â€” The AI-powered terminal built for speed and collaboration](https://go.warp.dev/memos)\n\n<a href=\"https://go.warp.dev/memos\" target=\"_blank\" rel=\"noopener\">\n  <img src=\"https://raw.githubusercontent.com/warpdotdev/brand-assets/main/Github/Sponsor/Warp-Github-LG-02.png\" alt=\"Warp - The AI-powered terminal built for speed and collaboration\" width=\"512\" />\n</a>\n\n---\n\n[**LambdaTest** - Cross-browser testing cloud](https://www.lambdatest.com/?utm_source=memos&utm_medium=sponsor)\n  \n<a href=\"https://www.lambdatest.com/?utm_source=memos&utm_medium=sponsor\" target=\"_blank\" rel=\"noopener\">\n  <img src=\"https://www.lambdatest.com/blue-logo.png\" alt=\"LambdaTest - Cross-browser testing cloud\" height=\"50\" />\n</a>\n\n## Overview\n\nMemos is a privacy-first, self-hosted knowledge base that works seamlessly for personal notes, team wikis, and knowledge management. Built with Go and React, it offers lightning-fast performance without compromising on features or usability.\n\n**Why choose Memos over cloud services?**\n\n| Feature           | Memos                          | Cloud Services                |\n| ----------------- | ------------------------------ | ----------------------------- |\n| **Privacy**       | âœ… Self-hosted, zero telemetry | âŒ Your data on their servers |\n| **Cost**          | âœ… Free forever, MIT license   | âŒ Subscription fees          |\n| **Performance**   | âœ… Instant load, no latency    | âš ï¸ Depends on internet        |\n| **Ownership**     | âœ… Full control & export       | âŒ Vendor lock-in             |\n| **API Access**    | âœ… Full REST + gRPC APIs       | âš ï¸ Limited or paid            |\n| **Customization** | âœ… Open source, forkable       | âŒ Closed ecosystem           |\n\n## Features\n\n- **ğŸ”’ Privacy-First Architecture**\n\n  - Self-hosted on your infrastructure with zero telemetry\n  - Complete data ownership and export capabilities\n  - No tracking, no ads, no vendor lock-in\n\n- **ğŸ“ Markdown Native**\n\n  - Full markdown support\n  - Plain text storage â€” take your data anywhere\n\n- **âš¡ Blazing Fast**\n\n  - Built with Go backend and React frontend\n  - Optimized for performance at any scale\n\n- **ğŸ³ Simple Deployment**\n\n  - One-line Docker installation\n  - Supports SQLite, MySQL, and PostgreSQL\n\n- **ğŸ”— Developer-Friendly**\n\n  - Full REST and gRPC APIs\n  - Easy integration with existing workflows\n\n- **ğŸ¨ Beautiful Interface**\n  - Clean, minimal design and dark mode support\n  - Mobile-responsive layout\n\n## Quick Start\n\n### Docker (Recommended)\n\n```bash\ndocker run -d \\\n  --name memos \\\n  -p 5230:5230 \\\n  -v ~/.memos:/var/opt/memos \\\n  neosmemo/memos:stable\n```\n\nOpen `http://localhost:5230` and start writing!\n\n### Try the Live Demo\n\nDon't want to install yet? Try our [live demo](https://demo.usememos.com/) first!\n\n### Other Installation Methods\n\n- **Docker Compose** - Recommended for production deployments\n- **Pre-built Binaries** - Available for Linux, macOS, and Windows\n- **Kubernetes** - Helm charts and manifests available\n- **Build from Source** - For development and customization\n\nSee our [installation guide](https://usememos.com/docs/installation) for detailed instructions.\n\n## Contributing\n\nWe welcome contributions of all kinds! Whether you're fixing bugs, adding features, improving documentation, or helping with translations â€” every contribution matters.\n\n**Ways to contribute:**\n\n- ğŸ› [Report bugs](https://github.com/usememos/memos/issues/new?template=bug_report.md)\n- ğŸ’¡ [Suggest features](https://github.com/usememos/memos/issues/new?template=feature_request.md)\n- ğŸ”§ [Submit pull requests](https://github.com/usememos/memos/pulls)\n- ğŸ“– [Improve documentation](https://github.com/usememos/memos/tree/main/docs)\n- ğŸŒ [Help with translations](https://github.com/usememos/memos/tree/main/web/src/locales)\n\n## Sponsors\n\nLove Memos? [Sponsor us on GitHub](https://github.com/sponsors/usememos) to help keep the project growing!\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=usememos/memos&type=Date)](https://star-history.com/#usememos/memos&Date)\n\n## License\n\nMemos is open-source software licensed under the [MIT License](LICENSE).\n\n## Privacy Policy\n\nMemos is built with privacy as a core principle. As a self-hosted application, all your data stays on your infrastructure. There is no telemetry, no tracking, and no data collection. See our [Privacy Policy](https://usememos.com/privacy) for details.\n\n---\n\n**[Website](https://usememos.com)** â€¢ **[Documentation](https://usememos.com/docs)** â€¢ **[Demo](https://demo.usememos.com/)** â€¢ **[Discord](https://discord.gg/tfPJa4UmAv)** â€¢ **[X/Twitter](https://x.com/usememos)**\n\n<a href=\"https://vercel.com/oss\">\n  <img alt=\"Vercel OSS Program\" src=\"https://vercel.com/oss/program-badge.svg\" />\n</a>\n",
      "stars_today": 242
    },
    {
      "id": 1000362065,
      "name": "awesome-copilot",
      "full_name": "github/awesome-copilot",
      "description": "Community-contributed instructions, prompts, and configurations to help you make the most of GitHub Copilot.",
      "html_url": "https://github.com/github/awesome-copilot",
      "stars": 17564,
      "forks": 2014,
      "language": "JavaScript",
      "topics": [
        "ai",
        "github-copilot",
        "hacktoberfest",
        "prompt-engineering"
      ],
      "created_at": "2025-06-11T16:57:39Z",
      "updated_at": "2026-01-15T01:03:04Z",
      "pushed_at": "2026-01-14T21:50:21Z",
      "open_issues": 14,
      "owner": {
        "login": "github",
        "avatar_url": "https://avatars.githubusercontent.com/u/9919?v=4"
      },
      "readme": "# ğŸ¤– Awesome GitHub Copilot Customizations\n\n[![Powered by Awesome Copilot](https://img.shields.io/badge/Powered_by-Awesome_Copilot-blue?logo=githubcopilot)](https://aka.ms/awesome-github-copilot)\n<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->\n[![All Contributors](https://img.shields.io/badge/all_contributors-93-orange.svg?style=flat-square)](#contributors-)\n<!-- ALL-CONTRIBUTORS-BADGE:END -->\n\nA community created collection of custom agents, prompts, and instructions to supercharge your GitHub Copilot experience across different domains, languages, and use cases.\n\n## ğŸš€ What is Awesome GitHub Copilot?\n\nThis repository provides a comprehensive toolkit for enhancing GitHub Copilot with specialized:\n\n- **ğŸ‘‰ [Awesome Agents](docs/README.agents.md)** - Specialized GitHub Copilot agents that integrate with MCP servers to provide enhanced capabilities for specific workflows and tools\n- **ğŸ‘‰ [Awesome Prompts](docs/README.prompts.md)** - Focused, task-specific prompts for generating code, documentation, and solving specific problems\n- **ğŸ‘‰ [Awesome Instructions](docs/README.instructions.md)** - Comprehensive coding standards and best practices that apply to specific file patterns or entire projects\n- **ğŸ‘‰ [Awesome Skills](docs/README.skills.md)** - Self-contained folders with instructions and bundled resources that enhance AI capabilities for specialized tasks\n- **ğŸ‘‰ [Awesome Collections](docs/README.collections.md)** - Curated collections of related prompts, instructions, and chat modes organized around specific themes and workflows\n\n## ğŸŒŸ Featured Collections\n\nDiscover our curated collections of prompts, instructions, and agents organized around specific themes and workflows.\n\n| Name | Description | Items | Tags |\n| ---- | ----------- | ----- | ---- |\n| [Awesome Copilot](collections/awesome-copilot.md) | Meta prompts that help you discover and generate curated GitHub Copilot chat modes, collections, instructions, prompts, and agents. | 6 items | github-copilot, discovery, meta, prompt-engineering, agents |\n| [Partners](collections/partners.md) | Custom agents that have been created by GitHub partners | 20 items | devops, security, database, cloud, infrastructure, observability, feature-flags, cicd, migration, performance |\n\n\n## MCP Server\n\nTo make it easy to add these customizations to your editor, we have created a [MCP Server](https://developer.microsoft.com/blog/announcing-awesome-copilot-mcp-server) that provides a prompt for searching and installing prompts, instructions, and chat modes directly from this repository. You'll need to have Docker installed and running to run the server.\n\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install-0098FF?logo=visualstudiocode&logoColor=white)](https://aka.ms/awesome-copilot/mcp/vscode) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install-24bfa5?logo=visualstudiocode&logoColor=white)](https://aka.ms/awesome-copilot/mcp/vscode-insiders) [![Install in Visual Studio](https://img.shields.io/badge/Visual_Studio-Install-C16FDE?logo=visualstudio&logoColor=white)](https://aka.ms/awesome-copilot/mcp/vs)\n\n<details>\n<summary>Show MCP Server JSON configuration</summary>\n\n```json\n{\n  \"servers\": {\n    \"awesome-copilot\": {\n      \"type\": \"stdio\",\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"ghcr.io/microsoft/mcp-dotnet-samples/awesome-copilot:latest\"\n      ]\n    }\n  }\n}\n```\n\n</details>\n\n## ğŸ”§ How to Use\n\n### ğŸ¤– Custom Agents\n\nCustom agents can be used in Copilot coding agent (CCA), VS Code, and Copilot CLI (coming soon). For CCA, when assigning an issue to Copilot, select the custom agent from the provided list. In VS Code, you can activate the custom agent in the agents session, alongside built-in agents like Plan and Agent.\n\n### ğŸ¯ Prompts\n\nUse the `/` command in GitHub Copilot Chat to access prompts:\n\n```plaintext\n/awesome-copilot create-readme\n```\n\n### ğŸ“‹ Instructions\n\nInstructions automatically apply to files based on their patterns and provide contextual guidance for coding standards, frameworks, and best practices.\n\n## ğŸ¯ Why Use Awesome GitHub Copilot?\n\n- **Productivity**: Pre-built agents, prompts and instructions save time and provide consistent results.\n- **Best Practices**: Benefit from community-curated coding standards and patterns.\n- **Specialized Assistance**: Access expert-level guidance through specialized custom agents.\n- **Continuous Learning**: Stay updated with the latest patterns and practices across technologies.\n\n## ğŸ¤ Contributing\n\nWe welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details on how to:\n\n- Add new prompts, instructions, or chat modes\n- Improve existing content\n- Report issues or suggest enhancements\n\nFor AI coding agents working with this project, refer to [AGENTS.md](AGENTS.md) for detailed technical guidance on development workflows, setup commands, and contribution standards.\n\n### Quick Contribution Guide\n\n1. Follow our file naming conventions and frontmatter requirements\n2. Test your contributions thoroughly\n3. Update the appropriate README tables\n4. Submit a pull request with a clear description\n\n## ğŸ“– Repository Structure\n\n```plaintext\nâ”œâ”€â”€ prompts/          # Task-specific prompts (.prompt.md)\nâ”œâ”€â”€ instructions/     # Coding standards and best practices (.instructions.md)\nâ”œâ”€â”€ agents/           # AI personas and specialized modes (.agent.md)\nâ”œâ”€â”€ collections/      # Curated collections of related items (.collection.yml)\nâ”œâ”€â”€ scripts/          # Utility scripts for maintenance\nâ””â”€â”€ skills/           # AI capabilities for specialized tasks\n```\n\n## ğŸ“„ License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## ğŸ›¡ï¸ Security & Support\n\n- **Security Issues**: Please see our [Security Policy](SECURITY.md)\n- **Support**: Check our [Support Guide](SUPPORT.md) for getting help\n- **Code of Conduct**: We follow the [Contributor Covenant](CODE_OF_CONDUCT.md)\n\n## â„¹ï¸ Disclaimer\n\nThe customizations in this repository are sourced from and created by third-party developers. GitHub does not verify, endorse, or guarantee the functionality or security of these agents. Please carefully inspect any agent and its documentation before installing to understand permissions it may require and actions it may perform.\n\n---\n\n**Ready to supercharge your coding experience?** Start exploring our [prompts](docs/README.prompts.md), [instructions](docs/README.instructions.md), and [custom agents](docs/README.agents.md)!\n\n## Contributors âœ¨\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.aaron-powell.com/\"><img src=\"https://avatars.githubusercontent.com/u/434140?v=4?s=100\" width=\"100px;\" alt=\"Aaron Powell\"/><br /><sub><b>Aaron Powell</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=aaronpowell\" title=\"Code\">ğŸ’»</a> <a href=\"#maintenance-aaronpowell\" title=\"Maintenance\">ğŸš§</a> <a href=\"#projectManagement-aaronpowell\" title=\"Project Management\">ğŸ“†</a> <a href=\"#promotion-aaronpowell\" title=\"Promotion\">ğŸ“£</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://mubaidr.js.org/\"><img src=\"https://avatars.githubusercontent.com/u/2222702?v=4?s=100\" width=\"100px;\" alt=\"Muhammad Ubaid Raza\"/><br /><sub><b>Muhammad Ubaid Raza</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=mubaidr\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://digitarald.de/\"><img src=\"https://avatars.githubusercontent.com/u/8599?v=4?s=100\" width=\"100px;\" alt=\"Harald Kirschner\"/><br /><sub><b>Harald Kirschner</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=digitarald\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mbianchidev\"><img src=\"https://avatars.githubusercontent.com/u/37507190?v=4?s=100\" width=\"100px;\" alt=\"Matteo Bianchi\"/><br /><sub><b>Matteo Bianchi</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=mbianchidev\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/AungMyoKyaw\"><img src=\"https://avatars.githubusercontent.com/u/9404824?v=4?s=100\" width=\"100px;\" alt=\"Aung Myo Kyaw\"/><br /><sub><b>Aung Myo Kyaw</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=AungMyoKyaw\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://danielscottraynsford.com/\"><img src=\"https://avatars.githubusercontent.com/u/7589164?v=4?s=100\" width=\"100px;\" alt=\"Daniel Scott-Raynsford\"/><br /><sub><b>Daniel Scott-Raynsford</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=PlagueHO\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/burkeholland\"><img src=\"https://avatars.githubusercontent.com/u/686963?v=4?s=100\" width=\"100px;\" alt=\"Burke Holland\"/><br /><sub><b>Burke Holland</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=burkeholland\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://calva.io/\"><img src=\"https://avatars.githubusercontent.com/u/30010?v=4?s=100\" width=\"100px;\" alt=\"Peter StrÃ¶mberg\"/><br /><sub><b>Peter StrÃ¶mberg</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=PEZ\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.devprodlogs.com/\"><img src=\"https://avatars.githubusercontent.com/u/51440732?v=4?s=100\" width=\"100px;\" alt=\"Daniel Meppiel\"/><br /><sub><b>Daniel Meppiel</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=danielmeppiel\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://montemagno.com/\"><img src=\"https://avatars.githubusercontent.com/u/1676321?v=4?s=100\" width=\"100px;\" alt=\"James Montemagno\"/><br /><sub><b>James Montemagno</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=jamesmontemagno\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/VamshiVerma\"><img src=\"https://avatars.githubusercontent.com/u/21999324?v=4?s=100\" width=\"100px;\" alt=\"Vamshi Verma\"/><br /><sub><b>Vamshi Verma</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=VamshiVerma\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sinedied\"><img src=\"https://avatars.githubusercontent.com/u/593151?v=4?s=100\" width=\"100px;\" alt=\"Yohan Lasorsa\"/><br /><sub><b>Yohan Lasorsa</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=sinedied\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/OrenMe\"><img src=\"https://avatars.githubusercontent.com/u/5461862?v=4?s=100\" width=\"100px;\" alt=\"Oren Me\"/><br /><sub><b>Oren Me</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=OrenMe\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mjrousos\"><img src=\"https://avatars.githubusercontent.com/u/10077254?v=4?s=100\" width=\"100px;\" alt=\"Mike Rousos\"/><br /><sub><b>Mike Rousos</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=mjrousos\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/guiopen\"><img src=\"https://avatars.githubusercontent.com/u/94094527?v=4?s=100\" width=\"100px;\" alt=\"Guilherme do Amaral Alves \"/><br /><sub><b>Guilherme do Amaral Alves </b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=guiopen\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.buymeacoffee.com/troystaylor\"><img src=\"https://avatars.githubusercontent.com/u/44444967?v=4?s=100\" width=\"100px;\" alt=\"Troy Simeon Taylor\"/><br /><sub><b>Troy Simeon Taylor</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=troystaylor\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/ambilykk/\"><img src=\"https://avatars.githubusercontent.com/u/10282550?v=4?s=100\" width=\"100px;\" alt=\"Ambily\"/><br /><sub><b>Ambily</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=ambilykk\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://tgrall.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/541250?v=4?s=100\" width=\"100px;\" alt=\"Tugdual Grall\"/><br /><sub><b>Tugdual Grall</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=tgrall\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/TianqiZhang\"><img src=\"https://avatars.githubusercontent.com/u/5326582?v=4?s=100\" width=\"100px;\" alt=\"Tianqi Zhang\"/><br /><sub><b>Tianqi Zhang</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=TianqiZhang\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/shubham070\"><img src=\"https://avatars.githubusercontent.com/u/5480589?v=4?s=100\" width=\"100px;\" alt=\"Shubham Gaikwad\"/><br /><sub><b>Shubham Gaikwad</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=shubham070\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sdolgin\"><img src=\"https://avatars.githubusercontent.com/u/576449?v=4?s=100\" width=\"100px;\" alt=\"Saul Dolgin\"/><br /><sub><b>Saul Dolgin</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=sdolgin\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nullchimp\"><img src=\"https://avatars.githubusercontent.com/u/58362593?v=4?s=100\" width=\"100px;\" alt=\"NULLchimp\"/><br /><sub><b>NULLchimp</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=nullchimp\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/MattVevang\"><img src=\"https://avatars.githubusercontent.com/u/20714898?v=4?s=100\" width=\"100px;\" alt=\"Matt Vevang\"/><br /><sub><b>Matt Vevang</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=MattVevang\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://devkimchi.com/\"><img src=\"https://avatars.githubusercontent.com/u/1538528?v=4?s=100\" width=\"100px;\" alt=\"Justin Yoo\"/><br /><sub><b>Justin Yoo</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=justinyoo\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://hachyderm.io/@0gis0\"><img src=\"https://avatars.githubusercontent.com/u/175379?v=4?s=100\" width=\"100px;\" alt=\"Gisela Torres\"/><br /><sub><b>Gisela Torres</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=0GiS0\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://debbie.codes/\"><img src=\"https://avatars.githubusercontent.com/u/13063165?v=4?s=100\" width=\"100px;\" alt=\"Debbie O'Brien\"/><br /><sub><b>Debbie O'Brien</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=debs-obrien\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/agreaves-ms\"><img src=\"https://avatars.githubusercontent.com/u/111466195?v=4?s=100\" width=\"100px;\" alt=\"Allen Greaves\"/><br /><sub><b>Allen Greaves</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=agreaves-ms\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/AmeliaRose802\"><img src=\"https://avatars.githubusercontent.com/u/26167931?v=4?s=100\" width=\"100px;\" alt=\"Amelia Payne\"/><br /><sub><b>Amelia Payne</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=AmeliaRose802\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/SebastienDegodez\"><img src=\"https://avatars.githubusercontent.com/u/2349146?v=4?s=100\" width=\"100px;\" alt=\"Sebastien DEGODEZ\"/><br /><sub><b>Sebastien DEGODEZ</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=SebastienDegodez\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://graef.io/\"><img src=\"https://avatars.githubusercontent.com/u/19261257?v=4?s=100\" width=\"100px;\" alt=\"Sebastian GrÃ¤f\"/><br /><sub><b>Sebastian GrÃ¤f</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=segraef\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://9ssi7.dev/\"><img src=\"https://avatars.githubusercontent.com/u/76786120?v=4?s=100\" width=\"100px;\" alt=\"Salih Ä°brahimbaÅŸ\"/><br /><sub><b>Salih Ä°brahimbaÅŸ</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=9ssi7\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/inquinity\"><img src=\"https://avatars.githubusercontent.com/u/406234?v=4?s=100\" width=\"100px;\" alt=\"Robert Altman\"/><br /><sub><b>Robert Altman</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=inquinity\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/pertrai1\"><img src=\"https://avatars.githubusercontent.com/u/442374?v=4?s=100\" width=\"100px;\" alt=\"Rob Simpson\"/><br /><sub><b>Rob Simpson</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=pertrai1\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://ricksm.it/\"><img src=\"https://avatars.githubusercontent.com/u/7207783?v=4?s=100\" width=\"100px;\" alt=\"Rick Smit\"/><br /><sub><b>Rick Smit</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=ricksmit3000\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://dotneteers.net/\"><img src=\"https://avatars.githubusercontent.com/u/28162552?v=4?s=100\" width=\"100px;\" alt=\"Peter Smulovics\"/><br /><sub><b>Peter Smulovics</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=psmulovics\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/pelikhan\"><img src=\"https://avatars.githubusercontent.com/u/4175913?v=4?s=100\" width=\"100px;\" alt=\"Peli de Halleux\"/><br /><sub><b>Peli de Halleux</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=pelikhan\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.paulomorgado.net/\"><img src=\"https://avatars.githubusercontent.com/u/470455?v=4?s=100\" width=\"100px;\" alt=\"Paulo Morgado\"/><br /><sub><b>Paulo Morgado</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=paulomorgado\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://nickyt.co/\"><img src=\"https://avatars.githubusercontent.com/u/833231?v=4?s=100\" width=\"100px;\" alt=\"Nick Taylor\"/><br /><sub><b>Nick Taylor</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=nickytonline\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mikeparker104\"><img src=\"https://avatars.githubusercontent.com/u/12763221?v=4?s=100\" width=\"100px;\" alt=\"Mike Parker\"/><br /><sub><b>Mike Parker</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=mikeparker104\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mikekistler\"><img src=\"https://avatars.githubusercontent.com/u/85643503?v=4?s=100\" width=\"100px;\" alt=\"Mike Kistler\"/><br /><sub><b>Mike Kistler</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=mikekistler\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://a11ysupport.io/\"><img src=\"https://avatars.githubusercontent.com/u/498678?v=4?s=100\" width=\"100px;\" alt=\"Michael Fairchild\"/><br /><sub><b>Michael Fairchild</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=mfairchild365\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/michael-volz/\"><img src=\"https://avatars.githubusercontent.com/u/129928?v=4?s=100\" width=\"100px;\" alt=\"Michael A. Volz (Flynn)\"/><br /><sub><b>Michael A. Volz (Flynn)</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=michaelvolz\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/4regab\"><img src=\"https://avatars.githubusercontent.com/u/178603515?v=4?s=100\" width=\"100px;\" alt=\"4regab\"/><br /><sub><b>4regab</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=4regab\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/TheovanKraay\"><img src=\"https://avatars.githubusercontent.com/u/24420698?v=4?s=100\" width=\"100px;\" alt=\"Theo van Kraay\"/><br /><sub><b>Theo van Kraay</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=TheovanKraay\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://glsauto.com/\"><img src=\"https://avatars.githubusercontent.com/u/132710946?v=4?s=100\" width=\"100px;\" alt=\"Troy Witthoeft (glsauto)\"/><br /><sub><b>Troy Witthoeft (glsauto)</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=twitthoeft-gls\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/iletai\"><img src=\"https://avatars.githubusercontent.com/u/26614687?v=4?s=100\" width=\"100px;\" alt=\"TÃ i LÃª\"/><br /><sub><b>TÃ i LÃª</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=iletai\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://tinyurl.com/3p5j9mwe\"><img src=\"https://avatars.githubusercontent.com/u/9591887?v=4?s=100\" width=\"100px;\" alt=\"Udaya Veeramreddygari\"/><br /><sub><b>Udaya Veeramreddygari</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=udayakumarreddyv\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://bio.warengonzaga.com/\"><img src=\"https://avatars.githubusercontent.com/u/15052701?v=4?s=100\" width=\"100px;\" alt=\"Waren Gonzaga\"/><br /><sub><b>Waren Gonzaga</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=warengonzaga\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://blog.miniasp.com/\"><img src=\"https://avatars.githubusercontent.com/u/88981?v=4?s=100\" width=\"100px;\" alt=\"Will ä¿å“¥\"/><br /><sub><b>Will ä¿å“¥</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=doggy8088\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/yukiomoto\"><img src=\"https://avatars.githubusercontent.com/u/38450410?v=4?s=100\" width=\"100px;\" alt=\"Yuki Omoto\"/><br /><sub><b>Yuki Omoto</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=yukiomoto\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/hueanmy\"><img src=\"https://avatars.githubusercontent.com/u/20430626?v=4?s=100\" width=\"100px;\" alt=\"Meii\"/><br /><sub><b>Meii</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=hueanmy\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/samqbush\"><img src=\"https://avatars.githubusercontent.com/u/74389839?v=4?s=100\" width=\"100px;\" alt=\"samqbush\"/><br /><sub><b>samqbush</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=samqbush\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sdanzo-hrb\"><img src=\"https://avatars.githubusercontent.com/u/136493100?v=4?s=100\" width=\"100px;\" alt=\"sdanzo-hrb\"/><br /><sub><b>sdanzo-hrb</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=sdanzo-hrb\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/voidfnc\"><img src=\"https://avatars.githubusercontent.com/u/194750710?v=4?s=100\" width=\"100px;\" alt=\"voidfnc\"/><br /><sub><b>voidfnc</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=voidfnc\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/webreidi\"><img src=\"https://avatars.githubusercontent.com/u/55603905?v=4?s=100\" width=\"100px;\" alt=\"Wendy Breiding\"/><br /><sub><b>Wendy Breiding</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=webreidi\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/zooav\"><img src=\"https://avatars.githubusercontent.com/u/12625412?v=4?s=100\" width=\"100px;\" alt=\"Ankur Sharma\"/><br /><sub><b>Ankur Sharma</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=zooav\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jianminhuang.cc/\"><img src=\"https://avatars.githubusercontent.com/u/6296280?v=4?s=100\" width=\"100px;\" alt=\"é»ƒå¥æ—» Vincent Huang\"/><br /><sub><b>é»ƒå¥æ—» Vincent Huang</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=Jian-Min-Huang\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dgh06175\"><img src=\"https://avatars.githubusercontent.com/u/77305722?v=4?s=100\" width=\"100px;\" alt=\"ì´ìƒí˜„\"/><br /><sub><b>ì´ìƒí˜„</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=dgh06175\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/abdidaudpropel\"><img src=\"https://avatars.githubusercontent.com/u/51310019?v=4?s=100\" width=\"100px;\" alt=\"Abdi Daud\"/><br /><sub><b>Abdi Daud</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=abdidaudpropel\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.senseof.tech/\"><img src=\"https://avatars.githubusercontent.com/u/50712277?v=4?s=100\" width=\"100px;\" alt=\"Adrien Clerbois\"/><br /><sub><b>Adrien Clerbois</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=AClerbois\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.qreate.it/\"><img src=\"https://avatars.githubusercontent.com/u/1868590?v=4?s=100\" width=\"100px;\" alt=\"Alan Sprecacenere\"/><br /><sub><b>Alan Sprecacenere</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=tegola\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://asilva.dev/\"><img src=\"https://avatars.githubusercontent.com/u/2493377?v=4?s=100\" width=\"100px;\" alt=\"AndrÃ© Silva\"/><br /><sub><b>AndrÃ© Silva</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=askpt\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://javaetmoi.com/\"><img src=\"https://avatars.githubusercontent.com/u/838318?v=4?s=100\" width=\"100px;\" alt=\"Antoine Rey\"/><br /><sub><b>Antoine Rey</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=arey\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/artemsaveliev\"><img src=\"https://avatars.githubusercontent.com/u/15679218?v=4?s=100\" width=\"100px;\" alt=\"Artem Saveliev\"/><br /><sub><b>Artem Saveliev</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=artemsaveliev\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://brunoborges.io/\"><img src=\"https://avatars.githubusercontent.com/u/129743?v=4?s=100\" width=\"100px;\" alt=\"Bruno Borges\"/><br /><sub><b>Bruno Borges</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=brunoborges\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.peug.net/\"><img src=\"https://avatars.githubusercontent.com/u/3845786?v=4?s=100\" width=\"100px;\" alt=\"Christophe Peugnet\"/><br /><sub><b>Christophe Peugnet</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=tossnet\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.movinglive.ca/\"><img src=\"https://avatars.githubusercontent.com/u/14792628?v=4?s=100\" width=\"100px;\" alt=\"Chtive\"/><br /><sub><b>Chtive</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=MovingLive\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/craigbekker\"><img src=\"https://avatars.githubusercontent.com/u/1115912?v=4?s=100\" width=\"100px;\" alt=\"Craig Bekker\"/><br /><sub><b>Craig Bekker</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=craigbekker\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/breakid\"><img src=\"https://avatars.githubusercontent.com/u/1446918?v=4?s=100\" width=\"100px;\" alt=\"Dan\"/><br /><sub><b>Dan</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=breakid\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ewega\"><img src=\"https://avatars.githubusercontent.com/u/26189114?v=4?s=100\" width=\"100px;\" alt=\"Eldrick Wega\"/><br /><sub><b>Eldrick Wega</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=ewega\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.felixarjuna.dev/\"><img src=\"https://avatars.githubusercontent.com/u/79026094?v=4?s=100\" width=\"100px;\" alt=\"Felix Arjuna\"/><br /><sub><b>Felix Arjuna</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=felixarjuna\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/feapaydin\"><img src=\"https://avatars.githubusercontent.com/u/19946639?v=4?s=100\" width=\"100px;\" alt=\"Furkan Enes\"/><br /><sub><b>Furkan Enes</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=feapaydin\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://learn.microsoft.com/dotnet\"><img src=\"https://avatars.githubusercontent.com/u/24882762?v=4?s=100\" width=\"100px;\" alt=\"Genevieve Warren\"/><br /><sub><b>Genevieve Warren</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=gewarren\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/geoder101\"><img src=\"https://avatars.githubusercontent.com/u/145904?v=4?s=100\" width=\"100px;\" alt=\"George Dernikos\"/><br /><sub><b>George Dernikos</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=geoder101\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/giomartinsdev\"><img src=\"https://avatars.githubusercontent.com/u/125399281?v=4?s=100\" width=\"100px;\" alt=\"Giovanni de Almeida Martins\"/><br /><sub><b>Giovanni de Almeida Martins</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=giomartinsdev\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Ioana37\"><img src=\"https://avatars.githubusercontent.com/u/69301842?v=4?s=100\" width=\"100px;\" alt=\"Ioana A\"/><br /><sub><b>Ioana A</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=Ioana37\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nohwnd\"><img src=\"https://avatars.githubusercontent.com/u/5735905?v=4?s=100\" width=\"100px;\" alt=\"Jakub JareÅ¡\"/><br /><sub><b>Jakub JareÅ¡</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=nohwnd\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://joe-watkins.io/\"><img src=\"https://avatars.githubusercontent.com/u/3695795?v=4?s=100\" width=\"100px;\" alt=\"Joe Watkins\"/><br /><sub><b>Joe Watkins</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=joe-watkins\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://johnpapa.net/\"><img src=\"https://avatars.githubusercontent.com/u/1202528?v=4?s=100\" width=\"100px;\" alt=\"John Papa\"/><br /><sub><b>John Papa</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=johnpapa\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.sugbo4j.co.nz/\"><img src=\"https://avatars.githubusercontent.com/u/15100839?v=4?s=100\" width=\"100px;\" alt=\"Joseph Gonzales\"/><br /><sub><b>Joseph Gonzales</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=josephgonzales01\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://digio.es/\"><img src=\"https://avatars.githubusercontent.com/u/173672918?v=4?s=100\" width=\"100px;\" alt=\"JosÃ© Antonio Garrido\"/><br /><sub><b>JosÃ© Antonio Garrido</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=josegarridodigio\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Ranrar\"><img src=\"https://avatars.githubusercontent.com/u/95967772?v=4?s=100\" width=\"100px;\" alt=\"Kim Skov Rasmussen\"/><br /><sub><b>Kim Skov Rasmussen</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=Ranrar\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/whiteken\"><img src=\"https://avatars.githubusercontent.com/u/20211937?v=4?s=100\" width=\"100px;\" alt=\"Kenny White\"/><br /><sub><b>Kenny White</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=whiteken\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/LouellaCreemers\"><img src=\"https://avatars.githubusercontent.com/u/46204894?v=4?s=100\" width=\"100px;\" alt=\"Louella Creemers\"/><br /><sub><b>Louella Creemers</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=LouellaCreemers\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://linktr.ee/lukemurray\"><img src=\"https://avatars.githubusercontent.com/u/24467442?v=4?s=100\" width=\"100px;\" alt=\"Luke Murray\"/><br /><sub><b>Luke Murray</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=lukemurraynz\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://marknoble.com/\"><img src=\"https://avatars.githubusercontent.com/u/3819700?v=4?s=100\" width=\"100px;\" alt=\"Mark Noble\"/><br /><sub><b>Mark Noble</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=marknoble\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://soderlind.no\"><img src=\"https://avatars.githubusercontent.com/u/1649452?v=4?s=100\" width=\"100px;\" alt=\"Per SÃ¸derlind\"/><br /><sub><b>Per SÃ¸derlind</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=soderlind\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/riqueufmg\"><img src=\"https://avatars.githubusercontent.com/u/108551585?v=4?s=100\" width=\"100px;\" alt=\"Henrique Nunes\"/><br /><sub><b>Henrique Nunes</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=riqueufmg\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jeremiah-snee-openx\"><img src=\"https://avatars.githubusercontent.com/u/113928685?v=4?s=100\" width=\"100px;\" alt=\"Jeremiah Snee\"/><br /><sub><b>Jeremiah Snee</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=jeremiah-snee-openx\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/spectatora\"><img src=\"https://avatars.githubusercontent.com/u/1385755?v=4?s=100\" width=\"100px;\" alt=\"spectatora\"/><br /><sub><b>spectatora</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=spectatora\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Mike-Hanna\"><img src=\"https://avatars.githubusercontent.com/u/50142889?v=4?s=100\" width=\"100px;\" alt=\"Michael\"/><br /><sub><b>Michael</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=Mike-Hanna\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/lechnerc77\"><img src=\"https://avatars.githubusercontent.com/u/22294087?v=4?s=100\" width=\"100px;\" alt=\"Christian Lechner\"/><br /><sub><b>Christian Lechner</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=lechnerc77\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jan-v.nl\"><img src=\"https://avatars.githubusercontent.com/u/462356?v=4?s=100\" width=\"100px;\" alt=\"Jan de Vries\"/><br /><sub><b>Jan de Vries</b></sub></a><br /><a href=\"https://github.com/github/awesome-copilot/commits?author=Jandev\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n  </tbody>\n  <tfoot>\n    <tr>\n      <td align=\"center\" size=\"13px\" colspan=\"7\">\n        <img src=\"https://raw.githubusercontent.com/all-contributors/all-contributors-cli/1b8533af435da9854653492b1327a23a4dbd0a10/assets/logo-small.svg\">\n          <a href=\"https://all-contributors.js.org/docs/en/bot/usage\">Add your contributions</a>\n        </img>\n      </td>\n    </tr>\n  </tfoot>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!\n\n## ğŸ“š Additional Resources\n\n- [VS Code Copilot Customization Documentation](https://code.visualstudio.com/docs/copilot/copilot-customization) - Official Microsoft documentation\n- [GitHub Copilot Chat Documentation](https://code.visualstudio.com/docs/copilot/chat/copilot-chat) - Complete chat feature guide\n- [Custom Chat Modes](https://code.visualstudio.com/docs/copilot/chat/chat-modes) - Advanced chat configuration\n- [VS Code Settings](https://code.visualstudio.com/docs/getstarted/settings) - General VS Code configuration guide\n\n## â„¢ï¸ Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n",
      "stars_today": 180
    },
    {
      "id": 1011253718,
      "name": "FossFLOW",
      "full_name": "stan-smith/FossFLOW",
      "description": "Make beautiful isometric infrastructure diagrams",
      "html_url": "https://github.com/stan-smith/FossFLOW",
      "stars": 16313,
      "forks": 1051,
      "language": "TypeScript",
      "topics": [
        "devops",
        "infra",
        "infrastructure"
      ],
      "created_at": "2025-06-30T14:31:21Z",
      "updated_at": "2026-01-15T00:55:37Z",
      "pushed_at": "2026-01-14T19:15:45Z",
      "open_issues": 22,
      "owner": {
        "login": "stan-smith",
        "avatar_url": "https://avatars.githubusercontent.com/u/37673863?v=4"
      },
      "readme": "# FossFLOW - Isometric Diagramming Tool <img width=\"30\" height=\"30\" alt=\"fossflow\" src=\"https://github.com/user-attachments/assets/56d78887-601c-4336-ab87-76f8ee4cde96\" />\n\n<p align=\"center\">\n <a href=\"README.md\">English</a> | <a href=\"docs/README.cn.md\">ç®€ä½“ä¸­æ–‡</a> | <a href=\"docs/README.es.md\">EspaÃ±ol</a> | <a href=\"docs/README.pt.md\">PortuguÃªs</a> | <a href=\"docs/README.fr.md\">FranÃ§ais</a> | <a href=\"docs/README.hi.md\">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> | <a href=\"docs/README.bn.md\">à¦¬à¦¾à¦‚à¦²à¦¾</a> | <a href=\"docs/README.ru.md\">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a> | <a href=\"docs/README.id.md\">Bahasa Indonesia</a> | <a href=\"docs/README.de.md\">Deutsch</a>\n</p>\n\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/15118\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/15118\" alt=\"stan-smith%2FFossFLOW | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n\n<b>Hey!</b> Stan here, if you've used FossFLOW and it's helped you, <b>I'd really appreciate if you could donate something small :)</b> I work full time, and finding the time to work on this project is challenging enough.\nIf you've had a feature that I've implemented for you, or fixed a bug it'd be great if you could :) if not, that's not a problem, this software will always remain free!\n\n\n<b>Also!</b> If you haven't yet, please check out the underlying library this is built on by <a href=\"https://github.com/markmanx/isoflow\">@markmanx</a> I truly stand on the shoulders of a giant here ğŸ«¡\n\n[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/P5P61KBXA3)\n\n<a href=\"https://www.buymeacoffee.com/stan.smith\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/default-orange.png\" alt=\"Buy Me A Coffee\" height=\"41\" width=\"174\"></a>\n\nThanks,\n\n-Stan\n\n## Try it online\n<p align=\"center\">\nGo to  <b> --> https://stan-smith.github.io/FossFLOW/ <-- </b>\n</p>\n<p align=\"center\">\n\n <a href=\"https://github.com/stan-smith/SlingShot\">\n  Check out my latest project: <b>SlingShot</b> - Dead easy video streaming over QUIC\n </a>\n</p>\n\n------------------------------------------------------------------------------------------------------------------------------\nFossFLOW is a powerful, open-source Progressive Web App (PWA) for creating beautiful isometric diagrams. Built with React and the <a href=\"https://github.com/markmanx/isoflow\">Isoflow</a> (Now forked and published to NPM as fossflow) library, it runs entirely in your browser with offline support.\n\n![Screenshot_20250630_160954](https://github.com/user-attachments/assets/e7f254ad-625f-4b8a-8efc-5293b5be9d55)\n\n- **ğŸ“ [FOSSFLOW_TODO.md](https://github.com/stan-smith/FossFLOW/blob/master/FOSSFLOW_TODO.md)** - Current issues and roadmap with codebase mappings, most gripes are with the isoflow library itself.\n- **ğŸ¤ [CONTRIBUTING.md](https://github.com/stan-smith/FossFLOW/blob/master/CONTRIBUTING.md)** - How to contribute to the project.\n\n### Performance updates\n - **Reduced frame refresh delay, should look much smoother now**\n\n### Multilingual Support\n- **9 Languages Supported** - Full interface translation in English, Chinese (Simplified), Spanish, Portuguese (Brazilian), French, Hindi, Bengali, Russian, and Indonesian\n- **Language Selector** - Easy-to-use language switcher in the app header\n- **Complete Translation** - All menus, dialogs, settings, tooltips, and help content translated\n- **Locale-Aware** - Automatically detects and remembers your language preference\n\n### Improved Connector Tool\n- **Click-based Creation** - New default mode: click first node, then second node to connect\n- **Drag Mode Option** - Original drag-and-drop still available via settings\n- **Mode Selection** - Switch between click and drag modes in Settings â†’ Connectors tab\n- **Better Reliability** - Click mode provides more predictable connection creation\n\n\n## ğŸ³ Quick Deploy with Docker\n\n```bash\n# Using Docker Compose (recommended - includes persistent storage)\ndocker compose up\n\n# Or run directly from Docker Hub with persistent storage\ndocker run -p 80:80 -v $(pwd)/diagrams:/data/diagrams stnsmith/fossflow:latest\n```\n\nServer storage is enabled by default in Docker. Your diagrams will be saved to `./diagrams` on the host.\n\nTo disable server storage, set `ENABLE_SERVER_STORAGE=false`:\n```bash\ndocker run -p 80:80 -e ENABLE_SERVER_STORAGE=false stnsmith/fossflow:latest\n```\n\n## Quick Start (Local Development)\n\n```bash\n# Clone the repository\ngit clone https://github.com/stan-smith/FossFLOW\ncd FossFLOW\n\n# Install dependencies\nnpm install\n\n# Build the library (required first time)\nnpm run build:lib\n\n# Start development server\nnpm run dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) in your browser.\n\n## Monorepo Structure\n\nThis is a monorepo containing two packages:\n\n- `packages/fossflow-lib` - React component library for drawing network diagrams (built with Webpack)\n- `packages/fossflow-app` - Progressive Web App which wraps the lib and presents it (built with RSBuild)\n\n### Development Commands\n\n```bash\n# Development\nnpm run dev          # Start app development server\nnpm run dev:lib      # Watch mode for library development\n\n# Building\nnpm run build        # Build both library and app\nnpm run build:lib    # Build library only\nnpm run build:app    # Build app only\n\n# Testing & Linting\nnpm test             # Run unit tests\nnpm run lint         # Check for linting errors\n\n# E2E Tests (Selenium)\ncd e2e-tests\n./run-tests.sh       # Run end-to-end tests (requires Docker & Python)\n\n# Publishing\nnpm run publish:lib  # Publish library to npm\n```\n\n## How to Use\n\n### Creating Diagrams\n\n1. **Add Items**:\n   - Press the \"+\" button on the top right menu, the library of components will appear on the left\n   - Drag and drop components from the library onto the canvas\n   - Or right-click on the grid and select \"Add node\"\n\n2. **Connect Items**: \n   - Select the Connector tool (press 'C' or click connector icon)\n   - **Click mode** (default): Click first node, then click second node\n   - **Drag mode** (optional): Click and drag from first to second node\n   - Switch modes in Settings â†’ Connectors tab\n\n3. **Save Your Work**:\n   - **Quick Save** - Saves to browser session\n   - **Export** - Download as JSON file\n   - **Import** - Load from JSON file\n\n### Storage Options\n\n- **Session Storage**: Temporary saves cleared when browser closes\n- **Export/Import**: Permanent storage as JSON files\n- **Auto-Save**: Automatically saves changes every 5 seconds to session\n\n## Contributing\n\nWe welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n## Documentation\n\n- [FOSSFLOW_ENCYCLOPEDIA.md](FOSSFLOW_ENCYCLOPEDIA.md) - Comprehensive guide to the codebase\n- [FOSSFLOW_TODO.md](FOSSFLOW_TODO.md) - Current issues and roadmap\n- [CONTRIBUTING.md](CONTRIBUTING.md) - Contributing guidelines\n\n## License\n\nMIT\n",
      "stars_today": 167
    },
    {
      "id": 676377723,
      "name": "DevOps-Projects",
      "full_name": "NotHarshhaa/DevOps-Projects",
      "description": "ğ‘«ğ’†ğ’—ğ‘¶ğ’‘ğ’” ğ‘¹ğ’†ğ’‚ğ’ ğ‘¾ğ’ğ’“ğ’ğ’… ğ‘·ğ’“ğ’ğ’‹ğ’†ğ’„ğ’•ğ’” ğ’‡ğ’ğ’“ ğ‘¨ğ’”ğ’‘ğ’Šğ’“ğ’Šğ’ğ’ˆ ğ‘«ğ’†ğ’—ğ‘¶ğ’‘ğ’” ğ‘¬ğ’ğ’ˆğ’Šğ’ğ’†ğ’†ğ’“ğ’” [ğ‘©ğ’†ğ’ˆğ’Šğ’ğ’ğ’†ğ’“ ğ’•ğ’ ğ‘¨ğ’…ğ’—ğ’‚ğ’ğ’„ğ’†ğ’…]",
      "html_url": "https://github.com/NotHarshhaa/DevOps-Projects",
      "stars": 3204,
      "forks": 3352,
      "language": "Java",
      "topics": [
        "devops",
        "devops-learning",
        "devops-poc",
        "devops-project",
        "devops-realtime",
        "devops-tools",
        "practical-devops",
        "projects-list",
        "realtime-devops-projects",
        "realtimeprojects"
      ],
      "created_at": "2023-08-09T04:19:38Z",
      "updated_at": "2026-01-15T00:40:34Z",
      "pushed_at": "2025-12-21T07:30:47Z",
      "open_issues": 15,
      "owner": {
        "login": "NotHarshhaa",
        "avatar_url": "https://avatars.githubusercontent.com/u/112948305?v=4"
      },
      "readme": "# **Real-World DevOps/Cloud Projects For Learning from Beginner to Advanced** â™\n\n<p align=\"center\">\n  <a href=\"https://trendshift.io/repositories/13316\" target=\"_blank\" rel=\"noopener noreferrer\">\n    <img src=\"https://trendshift.io/api/badge/repositories/13316\" alt=\"NotHarshhaa/DevOps-Projects | Trendshift\" style=\"width:350px;height:80px;max-width:100%;\" />\n  </a>\n</p>\n\n[![Forks][forks-shield]][forks-url]\n[![Stars][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n[![Last Commit][commit-shield]][commit-url]\n[![Code of Conduct][coc-shield]][coc-url]\n[![Contributing][contrib-shield]][contrib-url]\n\n<!-- MARKDOWN LINKS & IMAGES -->\n[forks-shield]: https://img.shields.io/github/forks/NotHarshhaa/DevOps-Projects?style=for-the-badge&logo=github&logoColor=white&color=orange\n[forks-url]: https://github.com/NotHarshhaa/DevOps-Projects/network/members\n\n[stars-shield]: https://img.shields.io/github/stars/NotHarshhaa/DevOps-Projects.svg?style=for-the-badge&logo=github&logoColor=white&color=brightgreen\n[stars-url]: https://github.com/NotHarshhaa/DevOps-Projects/stargazers\n\n[issues-shield]: https://img.shields.io/github/issues/NotHarshhaa/DevOps-Projects?style=for-the-badge&logo=github&logoColor=white&color=blue\n[issues-url]: https://github.com/NotHarshhaa/DevOps-Projects/issues\n\n[commit-shield]: https://img.shields.io/github/last-commit/NotHarshhaa/DevOps-Projects?style=for-the-badge&logo=git&logoColor=white&color=ff69b4\n[commit-url]: https://github.com/NotHarshhaa/DevOps-Projects/commits/master\n\n[coc-shield]: https://img.shields.io/badge/Code%20of%20Conduct-Enforced-blueviolet?style=for-the-badge&logo=handshake&logoColor=white\n[coc-url]: https://github.com/NotHarshhaa/DevOps-Projects/blob/master/CODE_OF_CONDUCT.md\n\n[contrib-shield]: https://img.shields.io/badge/Contributions-Welcome-ff69b4?style=for-the-badge&logo=gitbook&logoColor=white\n[contrib-url]: https://github.com/NotHarshhaa/DevOps-Projects/blob/master/CONTRIBUTING.md\n\n![DevOps-Projects](https://imgur.com/tlMOmn0.png)\n\n## ğŸ‘¥ **Project Ownership**\n\n<a href=\"https://github.com/NotHarshhaa/DevOps-Projects/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=NotHarshhaa/DevOps-Projects\" />\n</a>\n\n---\n\n## ğŸŒŸ **Star History**\n\n[![Star History Chart](https://api.star-history.com/svg?repos=NotHarshhaa/DevOps-Projects&type=Date)](https://www.star-history.com/#NotHarshhaa/DevOps-Projects&Date)\n\n---\n\n_*Welcome to the ultimate resource for **learning DevOps through hands-on projects!** This repository is designed to cater to aspiring **DevOps engineers** of all skill levels, from beginners taking their first steps in the field to advanced users looking to deepen their knowledge and expertise.*_\n\n![Welcome Badge](https://img.shields.io/badge/ğŸš€%20Welcome-Learn%20DevOps%20Through%20Hands--On%20Projects-10b981?style=for-the-badge&logo=opsgenie&logoColor=white)\n\n---\n\n## ğŸ§  **Purpose of the Repository**\n\nThis repository serves as a comprehensive resource for aspiring DevOps engineers to learn and implement real-world DevOps projects. It includes guides and solutions for deploying scalable systems, such as deploying a Java application on AWS using a 3-tier architecture and setting up scalable VPC architectures in the cloud.\n\nThe README files provide detailed instructions for implementing these projects, emphasizing practical deployment steps, pre-requisites, and validation processes. For example, one project focuses on deploying a Java-based login application integrated with a MySQL database, while another covers creating modular VPC network setups leveraging AWS services.\n\n![Purpose of Repository](https://img.shields.io/badge/ğŸ¯%20Purpose-Comprehensive%20DevOps%20Learning%20Hub-8b5cf6?style=for-the-badge&logo=target&logoColor=white)\n\n---\n\n## ğŸ” **Analysis of Features and Technologies**\n\nThe repository demonstrates extensive use of DevOps concepts and tools, focusing on AWS cloud infrastructure and automation. It features technologies such as:\n\n- **EC2, RDS, VPC, Auto Scaling, IAM roles**\n- **Maven, SonarCloud, JFrog Artifactory**\n- **Monitoring via CloudWatch**\n- **Custom AMIs, automation scripts**\n\nThese elements showcase a robust implementation of scalable, secure, and automated systems aligned with real-world DevOps practices.\n\n![Analysis of Features](https://img.shields.io/badge/ğŸ› ï¸%20Features%20&%20Technologies-AWS%2C%20CI%2FCD%2C%20Automation-10b981?style=for-the-badge&logo=amazonaws&logoColor=white)\n\n---\n\n## ğŸŒ **Real-time DevOps/Cloud Projects Showcase**\n\nTo improve readability and accessibility for users, this repository is also available as a modern and responsive web interface.\n\nA website showcasing a curated list of major real-time DevOps and Cloud projects, ranging from beginner to advanced levels. Built using **Next.js** and styled with **Tailwind CSS**, this project leverages a modern starter template for fast and responsive development. Perfect for learning and exploring hands-on DevOps and Cloud concepts!\n\nğŸ”— **Explore the site**: [projects.prodevopsguytech.com](https://projects.prodevopsguytech.com)\n\n![Showcase Website](https://img.shields.io/badge/ğŸŒ%20Project%20Showcase-Next.js%20+%20Tailwind%20UI-0ea5e9?style=for-the-badge&logo=vercel&logoColor=white)\n\n---\n\n## ğŸ”— **Related AWS Projects Repository**\n\nFor comprehensive AWS-specific projects and hands-on learning experiences, visit our dedicated AWS Projects repository:\n\n**AWS Projects Repository Highlights:**\n\n* **Real-world AWS Projects** from beginner to advanced levels\n* **AWS DevOps Focus** with practical implementation guides\n* **Hands-on Learning** with AWS services and best practices\n* **Industry-Relevant** projects covering EC2, VPC, RDS, Lambda, and more\n* **Community Driven** with active contributions and AWS expertise\n\nğŸ”— **Visit the AWS repository**: [AWS-Projects](https://github.com/NotHarshhaa/AWS-Projects)\n\n![AWS Projects](https://img.shields.io/badge/â˜ï¸%20AWS%20Projects-Dedicated%20AWS%20Learning%20Hub-ff9900?style=for-the-badge&logo=amazonaws&logoColor=white)\n\n---\n\n## **Repository Contents for DevOps Projects from Beginner to Advanced Levels**\n\n> [!IMPORTANT]\n>\n> _This repository contains a comprehensive collection of DevOps projects, each meticulously crafted to provide a hands-on learning experience. The projects are categorized into different skill levels to ensure that everyone, regardless of their current expertise, can find a suitable starting point and progressively enhance their skills._\n>\n> - **Beginner Projects:** Simple, foundational projects that introduce basic DevOps concepts and tools.\n> - **Intermediate Projects:** More complex projects that require a good understanding of DevOps fundamentals.\n> - **Advanced Projects:** Challenging projects designed to push your limits and deepen your understanding of sophisticated DevOps practices.\n\n![DevOps Levels](https://img.shields.io/badge/ğŸ“‚%20DevOps%20Projects-Beginner%20to%20Advanced-blueviolet?style=for-the-badge&logo=github&logoColor=white)\n\n---\n\n## **Integration of DevOps Technology with Other Technologies**\n\n> [!NOTE]\n> _In the modern tech landscape, DevOps doesn't exist in isolation. It intersects with a variety of other technologies, enhancing and being enhanced by them. This repository includes projects that integrate DevOps with several key technologies, allowing you to see how these integrations work in real-world scenarios._\n>\n> - **Machine Learning:** Implement DevOps practices to manage and deploy machine learning models efficiently.\n> - **Version Control with Git & GitHub:** Learn how to manage code versions and collaborate with others using Git and GitHub.\n> - **CI/CD Pipelines:** Set up continuous integration and continuous deployment pipelines to automate the testing and deployment of your applications.\n> - **Cloud Platforms (AWS, Azure, GCP):** Deploy applications on cloud platforms and leverage their services for scalability and reliability.\n> - **Containerization (Docker, Kubernetes):** Use container technologies to ensure that your applications run consistently across different environments.\n\n![Integration](https://img.shields.io/badge/ğŸ”—%20DevOps%20+%20Other%20Technologies-Seamless%20Integration-green?style=for-the-badge&logo=git&logoColor=white)\n\n---\n\n## **Project Scope**\n\n> [!IMPORTANT]\n> The projects span a wide array of topics within the DevOps domain, each designed to provide practical experience and insights into real-world scenarios. Hereâ€™s a detailed look at the areas covered:\n>\n> - **Automated Deployment:** Learn how to automate the deployment process to ensure that your applications are deployed quickly and reliably.\n> - **Continuous Integration & Continuous Deployment (CI/CD):** Understand how to set up and manage CI/CD pipelines to automate the testing and deployment of your code.\n> - **Infrastructure as Code (IaC):** Use tools like Terraform and CloudFormation to manage your infrastructure through code, ensuring consistency and scalability.\n> - **Monitoring & Logging:** Implement monitoring and logging solutions to keep track of your applicationsâ€™ performance and troubleshoot issues.\n> - **Security & Compliance:** Learn how to incorporate security practices into your DevOps workflows to ensure that your applications are secure and compliant with regulations.\n> - **Scalability & Performance Optimization:** Understand how to scale your applications and optimize their performance to handle increasing loads.\n\n![Project Scope](https://img.shields.io/badge/ğŸ› ï¸%20Project%20Scope-Hands--on%20DevOps%20Coverage-blue?style=for-the-badge&logo=vercel&logoColor=white)\n\n---\n\n## **Why Explore This Repository?**\n\n> [!NOTE]\n> This repository is a treasure trove of learning opportunities, tailored to help you grow in the DevOps field. Here's why you should dive in:\n>\n> - **Hands-on Experience:** Each project is designed to provide you with practical, hands-on experience. You'll work through real-world challenges and gain the skills you need to succeed in the industry.\n> - **Skill Enhancement:** Whether you're just starting or looking to build on existing skills, the projects are structured to guide you through a learning path that will enhance your capabilities.\n> - **Industry Relevance:** Stay up-to-date with the latest trends and technologies in DevOps. The projects reflect current industry practices, ensuring that what you learn is relevant and applicable.\n> - **Community Engagement:** Join a community of like-minded learners and professionals. Share your projects, seek feedback, and collaborate on exciting DevOps initiatives.\n\n![Why Explore This Repository](https://img.shields.io/badge/ğŸ“š%20Why%20Explore%20This%20Repository%3F-Unlock%20DevOps%20Mastery-brightgreen?style=for-the-badge&logo=bookstack&logoColor=white)\n\n---\n\n## **Code of Conduct**\n\n> [!CAUTION]\n>\n> We are committed to fostering a welcoming and respectful environment for all contributors. Please take a moment to review our [Code of Conduct](./CODE_OF_CONDUCT.md) before participating in this community.\n\n[![Code of Conduct](https://img.shields.io/badge/Code%20of%20Conduct-Enforced-blueviolet?style=for-the-badge&logo=handshake&logoColor=white)](https://github.com/NotHarshhaa/DevOps-Projects/blob/master/CODE_OF_CONDUCT.md)\n\n---\n\n## **Contribute and Collaborate**\n\n> [!TIP]\n> This repository thrives on community contributions and collaboration. Hereâ€™s how you can get involved:\n>\n> - **Fork the Repository:** Create your own copy of the repository to work on.\n> - **Submit Pull Requests:** Contribute your projects or improvements to existing projects by submitting pull requests.\n> - **Engage with Others:** Participate in discussions, provide feedback on othersâ€™ projects, and collaborate to create better solutions.\n> - **Share Your Knowledge:** If youâ€™ve developed a new project or learned something valuable, share it with the community. Your contributions can help others in their learning journey.\n>\n> **We follow best practices for contribution.**\n\n[![Contributing](https://img.shields.io/badge/Contribute-Guide-ff69b4?style=for-the-badge&logo=gitbook&logoColor=white)](https://github.com/NotHarshhaa/DevOps-Projects/blob/master/CONTRIBUTING.md)\n\n---\n\n## ğŸŒ **Join the Community**\n\n> [!IMPORTANT]\n> Be a part of our active DevOps community:\n\n[![Join Telegram](https://img.shields.io/badge/Join%20Us%20on-Telegram-26A5E4?style=for-the-badge&logo=telegram&logoColor=white)](https://t.me/prodevopsguy) \n[![Follow on GitHub](https://img.shields.io/badge/Follow%20me%20on-GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/NotHarshhaa)\n\n---\n\n## â­ **Hit the Star!**\n\n**If you find this helpful, donâ€™t forget to give this repository a star. Your support matters!** â­\n\n![Star Badge](https://img.shields.io/badge/â­%20Support-Give%20a%20Star%20if%20You%20Like%20It-ffd700?style=for-the-badge&logo=github&logoColor=white)\n\n---\n\n## ğŸ› ï¸ **Author & Community**\n\nThis project is crafted by **[Harshhaa](https://github.com/NotHarshhaa)** ğŸ’¡  \nIâ€™d love to hear your feedback! Feel free to share your thoughts.  \n\n![Author Badge](https://img.shields.io/badge/ğŸ› ï¸%20Author%20&%20Community-Crafted%20with%20Passion%20by%20Harshhaa-8a2be2?style=for-the-badge&logo=github&logoColor=white)\n\n---\n\n## ğŸ“§ **Connect with me:**\n\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-%230077B5.svg?style=for-the-badge&logo=linkedin&logoColor=white)](https://linkedin.com/in/harshhaa-vardhan-reddy) [![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/NotHarshhaa) [![Telegram](https://img.shields.io/badge/Telegram-26A5E4?style=for-the-badge&logo=telegram&logoColor=white)](https://t.me/prodevopsguy) [![Dev.to](https://img.shields.io/badge/Dev.to-0A0A0A?style=for-the-badge&logo=dev.to&logoColor=white)](https://dev.to/notharshhaa) [![Hashnode](https://img.shields.io/badge/Hashnode-2962FF?style=for-the-badge&logo=hashnode&logoColor=white)](https://hashnode.com/@prodevopsguy)  \n\n---\n\n## ğŸ“¢ **Stay Connected**\n\n![Follow Me](https://imgur.com/2j7GSPs.png)\n\n![Stay Connected](https://img.shields.io/badge/ğŸ“¢%20Stay%20Connected-Join%20our%20DevOps%20Community-orange?style=for-the-badge&logo=telegram&logoColor=white)\n",
      "stars_today": 162
    },
    {
      "id": 509970897,
      "name": "tailspin",
      "full_name": "bensadeh/tailspin",
      "description": "ğŸŒ€ A log file highlighter",
      "html_url": "https://github.com/bensadeh/tailspin",
      "stars": 7408,
      "forks": 126,
      "language": "Rust",
      "topics": [
        "colorizer",
        "colors",
        "file",
        "follow",
        "highlighter",
        "less",
        "log",
        "log-file",
        "logfile",
        "syntax-highlighting",
        "tail"
      ],
      "created_at": "2022-07-03T08:56:53Z",
      "updated_at": "2026-01-15T01:01:54Z",
      "pushed_at": "2026-01-12T05:30:26Z",
      "open_issues": 7,
      "owner": {
        "login": "bensadeh",
        "avatar_url": "https://avatars.githubusercontent.com/u/701096?v=4"
      },
      "readme": "<p align=\"center\">\n  <img src=\"assets/tailspin.png\" width=\"230\"/>\n</p>\n\n#                                                                                                                                                                                                                                                                                                                                                                  \n\n<p align=\"center\">\nA log file highlighter\n</p>\n\n<p align=\"center\">\n  <img src=\"assets/main.png\" width=\"700\"/>\n</p>\n\n### Features\n\n- ğŸªµ View (or `tail`) any log file of any format\n- ğŸ° No setup or config required\n- ğŸŒˆ Highlights numbers, dates, IP-addresses, UUIDs, URLs and more\n- âš™ï¸ All highlight groups are customizable\n- ğŸ§¬ Easy to integrate with other commands\n- ğŸ“¦ Also available as a [crate](https://docs.rs/tailspin)\n\n#\n\n### Table of Contents\n\n* [Overview](#overview)\n* [Usage](#usage)\n* [Installing](#installing)\n* [Highlight Groups](#highlight-groups)\n* [Customizing Highlight Groups](#customizing-highlight-groups)\n* [Working with `stdin` and `stdout`](#working-with-stdin-and-stdout)\n* [Using the pager `less`](#using-the-pager-less)\n* [Settings](#settings)\n\n***\n\n## Overview\n\n`tailspin` works by reading through a log file line by line, running a series of regexes\nagainst each line. The regexes recognize patterns you expect to find in a logfile, like dates, numbers, severity\nkeywords and more.\n\n`tailspin` does not make any assumptions on the format or position of the items it wants to highlight. For this reason,\nit requires no configuration and the highlighting will work consistently across different logfiles.\n\n## Usage\n\nThe binary name for `tailspin` is `tspin`.\n\n```console\n# Read from file and view in `less`\ntspin application.log\n\n# Pipe something into `tspin` and print to stdout\necho \"hello null\" | tspin\n\n# Read from stdin and print to stdout\nkubectl logs [pod_name] --follow | tspin\n\n# Run the provided command and view the output in `less`\ntspin --exec='kubectl logs -f pod_name'\n``` \n\n## Installing\n\n<details>\n<summary>Expand to view</summary>\n\n### Package Managers\n\n```console\n# Homebrew\nbrew install tailspin\n\n# Cargo\ncargo install tailspin\n\n# Archlinux\npacman -S tailspin\n\n# Nix\nnix-shell -p tailspin\n\n# NetBSD\npkgin install tailspin\n\n# FreeBSD\npkg install tailspin\n\n# Windows\nscoop install tailspin\n```\n\n### From Source\n\n```console\ncargo install --path .\n```\n\nBinary will be placed in `~/.cargo/bin`, make sure you add the folder to your `PATH` environment variable.\n\n> [!IMPORTANT]\n> When building from source, make sure that you are using the latest version\n> of [`less`](http://greenwoodsoftware.com/less/).\n\n</details>\n\n## Highlight Groups\n\n### Dates\n\n<p align=\"center\">\n  <img src=\"assets/examples/dates.png\" width=\"600\"/>\n</p>\n\n### Keywords\n\n<p align=\"center\">\n  <img src=\"assets/examples/keywords.png\" width=\"600\"/>\n</p>\n\n### URLs\n\n<p align=\"center\">\n  <img src=\"assets/examples/urls.png\" width=\"600\"/>\n</p>\n\n### Numbers\n\n<p align=\"center\">\n  <img src=\"assets/examples/numbers.png\" width=\"600\"/>\n</p>\n\n### IP Addresses\n\n<p align=\"center\">\n  <img src=\"assets/examples/ip.png\" width=\"600\"/>\n</p>\n\n### Quotes\n\n<p align=\"center\">\n  <img src=\"assets/examples/quotes.png\" width=\"600\"/>\n</p>\n\n### Unix file paths\n\n<p align=\"center\">\n  <img src=\"assets/examples/paths.png\" width=\"600\"/>\n</p>\n\n### HTTP methods\n\n<p align=\"center\">\n  <img src=\"assets/examples/http.png\" width=\"600\"/>\n</p>\n\n### UUIDs\n\n<p align=\"center\">\n  <img src=\"assets/examples/uuids.png\" width=\"600\"/>\n</p>\n\n### Key-value pairs\n\n<p align=\"center\">\n  <img src=\"assets/examples/kv.png\" width=\"600\"/>\n</p>\n\n### Pointer addresses\n\n<p align=\"center\">\n  <img src=\"assets/examples/pointers.png\" width=\"600\"/>\n</p>\n\n### Unix processes\n\n<p align=\"center\">\n  <img src=\"assets/examples/processes.png\" width=\"600\"/>\n</p>\n\n## Customizing Highlight Groups\n\n### Overview\n\nCreate a `theme.toml` in `~/.config/tailspin` to customize highlight groups.\n\nStyles have the following shape:\n\n```toml\nstyle = { fg = \"color\", bg = \"color\", italic = false, bold = false, underline = false }\n```\n\nTo edit the different highlight groups, include them in your `theme.toml` file. For example, to edit the `date`\nhighlight group, add the following to your `theme.toml`:\n\n```toml\n[date]\nstyle = { fg = \"green\" }\n```\n\nExpand the section below to see the default config for the highlight groups:\n\n<details>\n<summary>Default highlight groups settings</summary>\n\n```toml\n[dates]\ndate = { fg = \"magenta\" }\ntime = { fg = \"blue\" }\nzone = { fg = \"red\" }\nseparator = { faint = true }\n\n[[keywords]]\nwords = ['null', 'true', 'false']\nstyle = { fg = \"red\", italic = true }\n\n[[keywords]]\nwords = ['GET']\nstyle = { fg = \"black\", bg = \"green\" }\n\n[urls]\nhttp = { fg = \"red\", faint = true }\nhttps = { fg = \"green\", faint = true }\nhost = { fg = \"blue\", faint = true }\npath = { fg = \"blue\" }\nquery_params_key = { fg = \"magenta\" }\nquery_params_value = { fg = \"cyan\" }\nsymbols = { fg = \"red\" }\n\n[numbers]\nstyle = { fg = \"cyan\" }\n\n[ip_addresses]\nnumber = { fg = \"blue\", italic = true }\nletter = { fg = \"magenta\", italic = true }\nseparator = { fg = \"red\" }\n\n[quotes]\nstyle = { fg = \"yellow\" }\ntoken = '\"'\n\n[paths]\nsegment = { fg = \"green\", italic = true }\nseparator = { fg = \"yellow\" }\n\n[uuids]\nnumber = { fg = \"blue\", italic = true }\nletter = { fg = \"magenta\", italic = true }\nseparator = { fg = \"red\" }\n\n[pointers]\nnumber = { fg = \"blue\", italic = true }\nletter = { fg = \"magenta\", italic = true }\nseparator = { fg = \"red\" }\n\n[key_value_pairs]\nkey = { faint = true }\nseparator = { fg = \"white\" }\n\n[processes]\nname = { fg = \"green\" }\nseparator = { fg = \"red\" }\nid = { fg = \"yellow\" }\n\n[json]\nkey = { fg = \"yellow\" }\nquote_token = { fg = \"yellow\", faint = true }\ncurly_bracket = { faint = true }\nsquare_bracket = { faint = true }\ncomma = { faint = true }\ncolon = { faint = true }\n```\n\n</details>\n\n### Disabling Highlight Groups\n\nTo individually disable or enable highlight groups, use the `--enable` and `--disable` flags:\n\n```console\n# Enable only the url highlight group, disable the rest\ntspin application.log --enable=url\n\n# Disable the numbers highlight group, keep the rest\ntspin application.log --disable=numbers\n```\n\n### Adding Keywords via theme.toml\n\nTo add custom keywords, either include them in the list of keywords or add new entries:\n\n```toml\n[[keywords]]\nwords = ['MyCustomKeyword']\nstyle = { fg = \"green\" }\n\n[[keywords]]\nwords = ['null', 'true', 'false']\nstyle = { fg = \"red\", italic = true }\n```\n\n### Adding Keywords from the command line\n\nSometimes it is more convenient to add highlight groups on the fly without having to edit a TOML. To add highlights from\nthe command line, use the `--highlight` flag followed by a comma separated list of words to be highlighted.\n\nFor example:\n\n```console\ntspin --highlight=red:error,fail --highlight=green:success,ok\n```\n\n<p align=\"center\">\n  <img src=\"assets/examples/otf.png\" width=\"800\"/>\n</p>\n\n### Custom regex highlighters\n\nWhen you need more control over the highlighting, you can use the regex highlighter. This highlighter allows you to\nspecify a regex and a style to be applied to the matched text.\n\nIt supports one capture group `()`. When found, it will apply the style to the captured text.\n\n```toml\n[[regexes]]\nregex = 'Started (.*)\\.'\nstyle = { fg = \"red\" }\n```\n\n## Working with `stdin` and `stdout`\n\n### Default behavior with pipes\n\nBy default, `tailspin` will open a file in the pager `less`. However, if you pipe something into `tailspin`, it will\nprint the highlighted output directly to `stdout`. This is similar to running `tspin [file] --print`.\n\nTo let `tailspin` highlight the logs of different commands, you can pipe the output of those commands into `tailspin`\nlike so:\n\n```console\njournalctl -f | tspin\ncat /var/log/syslog | tspin\nkubectl logs -f pod_name | tspin\n```\n\n### Capturing the output of a command and viewing it in `less`\n\nTo capture the output of a command and view it in `less`, use the `--exec` flag:\n\n```console\ntspin --exec 'kubectl logs -f pod_name'\n```\n\nThis will run the command `kubectl logs -f pod_name` in the background and pipe the output to `tailspin`. The output\nwill be displayed in `less`, allowing you to navigate and search through the logs.\n\n## Using the pager `less`\n\n### Overview\n\n`tailspin` uses `less` as its pager to view the highlighted log files. You can get more info on `less` via the **man**\ncommand (`man less`) or by hitting the <kbd>h</kbd> button to access the help screen.\n\n### Navigating\n\nNavigating within `less` uses a set of keybindings that may be familiar to users of `vim` or other `vi`-like\neditors. Here's a brief overview of the most useful navigation commands:\n\n- <kbd>j</kbd>/<kbd>k</kbd>: Scroll one line up / down\n- <kbd>d</kbd>/<kbd>u</kbd>: Scroll one half-page up / down\n- <kbd>g</kbd>/<kbd>G</kbd>: Go to the top / bottom of the file\n\n### Follow mode\n\nWhen you run `tailspin` with the `-f` or `--follow` flag, it will scroll to the bottom and print new lines to the screen\nas they're added to the file.\n\nTo stop following the file, interrupt with <kbd>Ctrl + C</kbd>. This will stop the tailing, but keep the\nfile open, allowing you to review the existing content.\n\nTo resume following the file from within `less`, press <kbd>Shift + F</kbd>.\n\n### Search\n\nUse <kbd>/</kbd> followed by your search query. For example, `/ERROR` finds the first occurrence of\n**ERROR**.\n\nAfter the search, <kbd>n</kbd> finds the next instance, and <kbd>N</kbd> finds the previous instance.\n\n### Filtering\n\n`less` allows filtering lines by a keyword, using <kbd>&</kbd> followed by the pattern. For instance, `&ERROR` shows\nonly lines with **ERROR**.\n\nTo only show lines containing either `ERROR` or `WARN`, use a regular expression: `&\\(ERROR\\|WARN\\)`.\n\nTo clear the filter, use <kbd>&</kbd> with no pattern.\n\n### Custom pagers\n\nSet the `TAILSPIN_PAGER` environment variable to override the default pager.\nThe command must include the string **[FILE]** which will be replaced with the file path internally.\n\nFor example:\n\n```console\nTAILSPIN_PAGER=\"ov -f [FILE]\" tspin example-logs/example1\n```\n\n## Settings\n\n```console\n-f, --follow                     Follow the contents of the file\n-p, --print                      Print the output to stdout\n-e, --exec='[CMD]'               Run command and view the output in a pager\n                                 (e.g. `tspin --exec 'kubectl logs -f pod_name'`)\n    --config-path=[PATH]         Use the configuration file from the provided path\n    --pager=[CUSTOM_PAGER]       Set a custom pager\n                                 (e.g. `--pager=\"ov -f [FILE]\"`)\n    --highlight=[COLOR]:[WORDS]  Highlight the provided comma-separated words in the specified color\n                                 (e.g. `--highlight red:ERROR,WARNING`)\n    --enable=[HIGHLIGHT_GROUP]   Enable one or more highlight groups, disabling the rest\n                                 (e.g. `--enable=keywords,urls`)\n    --disable=[HIGHLIGHT_GROUP]  Disable one or more highlight groups, enabling the rest\n                                 (e.g. `--disable=keywords,urls`)\n    --disable-builtin-keywords   Disable the highlighting of booleans, nulls, log severities and common REST verbs\n```\n\n\n",
      "stars_today": 157
    },
    {
      "id": 769023213,
      "name": "convex-backend",
      "full_name": "get-convex/convex-backend",
      "description": "The open-source reactive database for app developers",
      "html_url": "https://github.com/get-convex/convex-backend",
      "stars": 9344,
      "forks": 525,
      "language": "Rust",
      "topics": [
        "backend",
        "convex",
        "database",
        "rust",
        "typescript"
      ],
      "created_at": "2024-03-08T07:23:15Z",
      "updated_at": "2026-01-15T01:00:32Z",
      "pushed_at": "2026-01-15T00:47:11Z",
      "open_issues": 126,
      "owner": {
        "login": "get-convex",
        "avatar_url": "https://avatars.githubusercontent.com/u/81530787?v=4"
      },
      "readme": "<p align=\"center\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://static.convex.dev/logo/convex-logo-light.svg\" width=\"600\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://static.convex.dev/logo/convex-logo.svg\" width=\"600\">\n  <img alt=\"Convex logo\" src=\"https://static.convex.dev/logo/convex-logo.svg\" width=\"600\">\n</picture>\n</p>\n\n[Convex](https://convex.dev) is the open-source reactive database designed to\nmake life easy for web app developers, whether human or LLM. Fetch data and\nperform business logic with strong consistency by writing pure TypeScript.\n\nConvex provides a database, a place to write your server functions, and client\nlibraries. It makes it easy to build and scale dynamic live-updating apps.\n[Read the docs to learn more](https://docs.convex.dev/understanding/).\n\nDevelopment of the Convex backend is led by the Convex team. We\n[welcome bug fixes](./CONTRIBUTING.md) and\n[love receiving feedback](https://discord.gg/convex). We keep this repository\nsynced with any internal development work within a handful of days.\n\n## Getting Started\n\nVisit our [documentation](https://docs.convex.dev/) to learn more about Convex\nand follow our getting started guides.\n\nThe easiest way to build with Convex is through our\n[cloud platform](https://www.convex.dev/plans), which includes a generous free\ntier and lets you focus on building your application without worrying about\ninfrastructure. Many small applications and side-projects can operate entirely\non the free tier with zero cost and zero maintenance.\n\n## Self Hosting\n\nThe self-hosted product includes most features of the cloud product, including\nthe dashboard and CLI. Self-hosted Convex works well with a variety of tools\nincluding Neon, Fly.io, Vercel, Netlify, RDS, Sqlite, Postgres, and more.\n\nYou can either use Docker (recommended) or a prebuilt binary to self host\nConvex. Check out our [self-hosting guide](./self-hosted/README.md) for detailed\ninstructions. Community support for self-hosting is available in the\n`#self-hosted` channel on [Discord](https://discord.gg/convex).\n\n## Community & Support\n\n- Join our [Discord community](https://discord.gg/convex) for help and\n  discussions.\n- Report issues when building and using the open source Convex backend through\n  [GitHub Issues](https://github.com/get-convex/convex-backend/issues)\n- By submitting pull requests, you confirm that Convex can use, modify, copy,\n  and redistribute the contribution, under the terms of its choice.\n\n## Building from source\n\nSee [BUILD.md](./BUILD.md).\n\n## Disclaimers\n\n- If you choose to self-host, we recommend following the self-hosting guide. If\n  you are instead building from source, make sure to change your instance secret\n  and admin key from the defaults in the repo.\n- Convex is battle tested most thoroughly on Linux and Mac. On Windows, it has\n  less experience. If you run into issues, please message us on\n  [Discord](https://convex.dev/community) in the `#self-hosted` channel.\n- Convex self-hosted builds contain a beacon to help Convex improve the product.\n  The information is minimal and anonymous and helpful to Convex, but if you\n  really want to disable it, you can set the `--disable-beacon` flag on the\n  backend binary. The beacon's messages print in the log and only include\n  - A random identifier for your deployment (not used elsewhere)\n  - Migration version of your database\n  - Git rev of the backend\n  - Uptime of the backend\n\n## Repository layout\n\n- `crates/` contains Rust code\n\n  - Main binary\n    - `local_backend/` is an application server on top of the `Runtime`. This is\n      the serving edge for the Convex cloud.\n\n- `npm-packages/` contains both our public and internal TypeScript packages.\n  - Internal packages\n    - `udf-runtime/` sets up the user-defined functions JS environment for\n      queries and mutations\n    - `udf-tests/` is a collection of functions used in testing the isolate\n      layer\n    - `system-udfs/` contains functions used by the Convex system e.g. the CLI\n",
      "stars_today": 134
    },
    {
      "id": 329782568,
      "name": "dioxus",
      "full_name": "DioxusLabs/dioxus",
      "description": "Fullstack app framework for web, desktop, and mobile.",
      "html_url": "https://github.com/DioxusLabs/dioxus",
      "stars": 34098,
      "forks": 1496,
      "language": "Rust",
      "topics": [
        "android",
        "css",
        "desktop",
        "html",
        "ios",
        "native",
        "react",
        "rust",
        "ssr",
        "ui",
        "virtualdom",
        "wasm",
        "web"
      ],
      "created_at": "2021-01-15T01:57:26Z",
      "updated_at": "2026-01-15T00:14:04Z",
      "pushed_at": "2026-01-14T23:37:55Z",
      "open_issues": 609,
      "owner": {
        "login": "DioxusLabs",
        "avatar_url": "https://avatars.githubusercontent.com/u/79236386?v=4"
      },
      "readme": "<p>\n    <p align=\"center\" >\n      <!-- <img src=\"./notes/header-light-updated.svg#gh-light-mode-only\" >\n      <img src=\"./notes/header-dark-updated.svg#gh-dark-mode-only\" > -->\n      <!-- <a href=\"https://dioxuslabs.com\">\n          <img src=\"./notes/flat-splash.avif\">\n      </a> -->\n      <img src=\"./notes/splash-header-darkmode.svg#gh-dark-mode-only\" style=\"width: 80%; height: auto;\">\n      <img src=\"./notes/splash-header.svg#gh-light-mode-only\" style=\"width: 80%; height: auto;\">\n      <img src=\"./notes/image-splash.avif\">\n      <br>\n    </p>\n</p>\n<div align=\"center\">\n  <!-- Crates version -->\n  <a href=\"https://crates.io/crates/dioxus\">\n    <img src=\"https://img.shields.io/crates/v/dioxus.svg?style=flat-square\"\n    alt=\"Crates.io version\" />\n  </a>\n  <!-- Downloads -->\n  <a href=\"https://crates.io/crates/dioxus\">\n    <img src=\"https://img.shields.io/crates/d/dioxus.svg?style=flat-square\"\n      alt=\"Download\" />\n  </a>\n  <!-- docs -->\n  <a href=\"https://docs.rs/dioxus\">\n    <img src=\"https://img.shields.io/badge/docs-latest-blue.svg?style=flat-square\"\n      alt=\"docs.rs docs\" />\n  </a>\n  <!-- CI -->\n  <a href=\"https://github.com/jkelleyrtp/dioxus/actions\">\n    <img src=\"https://github.com/dioxuslabs/dioxus/actions/workflows/main.yml/badge.svg\"\n      alt=\"CI status\" />\n  </a>\n\n  <!--Awesome -->\n  <a href=\"https://dioxuslabs.com/awesome\">\n    <img src=\"https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg\" alt=\"Awesome Page\" />\n  </a>\n  <!-- Discord -->\n  <a href=\"https://discord.gg/XgGxMSkvUM\">\n    <img src=\"https://img.shields.io/discord/899851952891002890.svg?logo=discord&style=flat-square\" alt=\"Discord Link\" />\n  </a>\n</div>\n\n<div align=\"center\">\n  <h3>\n    <a href=\"https://dioxuslabs.com\"> Website </a>\n    <span> | </span>\n    <a href=\"https://github.com/DioxusLabs/dioxus/tree/main/examples\"> Examples </a>\n    <span> | </span>\n    <a href=\"https://dioxuslabs.com/learn/0.7/tutorial\"> Tutorial </a>\n    <span> | </span>\n    <a href=\"https://github.com/DioxusLabs/dioxus/blob/main/notes/translations/zh-cn/README.md\"> ä¸­æ–‡ </a>\n    <span> | </span>\n    <a href=\"https://github.com/DioxusLabs/dioxus/blob/main/notes/translations/pt-br/README.md\"> PT-BR </a>\n    <span> | </span>\n    <a href=\"https://github.com/DioxusLabs/dioxus/blob/main/notes/translations/ja-jp/README.md\"> æ—¥æœ¬èª </a>\n    <span> | </span>\n    <a href=\"https://github.com/DioxusLabs/dioxus/blob/main/notes/translations/tr-tr\"> TÃ¼rkÃ§e </a>\n    <span> | </span>\n    <a href=\"https://github.com/DioxusLabs/dioxus/blob/main/notes/translations/ko-kr\"> í•œêµ­ì–´ </a>\n  </h3>\n</div>\n<br>\n<p align=\"center\">\n  <a href=\"https://github.com/DioxusLabs/dioxus/releases/tag/v0.7.0\">âœ¨ Dioxus 0.7 is out!!! âœ¨</a>\n</p>\n<br>\n\nBuild for web, desktop, and mobile, and more with a single codebase. Zero-config setup, integrated hot-reloading, and signals-based state management. Add backend functionality with Server Functions and bundle with our CLI.\n\n```rust\nfn app() -> Element {\n    let mut count = use_signal(|| 0);\n\n    rsx! {\n        h1 { \"High-Five counter: {count}\" }\n        button { onclick: move |_| count += 1, \"Up high!\" }\n        button { onclick: move |_| count -= 1, \"Down low!\" }\n    }\n}\n```\n\n## â­ï¸ Unique features:\n\n- Cross-platform apps in three lines of code (web, desktop, mobile, server, and more)\n- [Ergonomic state management](https://dioxuslabs.com/blog/release-050) combines the best of React, Solid, and Svelte\n- Built-in featureful, type-safe, fullstack web framework\n- Integrated bundler for deploying to the web, macOS, Linux, and Windows\n- Subsecond Rust hot-patching and asset hot-reloading\n- And more! [Take a tour of Dioxus](https://dioxuslabs.com/learn/0.7/).\n\n## Instant hot-reloading\n\nWith one command, `dx serve` and your app is running. Edit your markup, styles, and see changes in milliseconds. Use our experimental `dx serve --hotpatch` to update Rust code in real time.\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/DioxusLabs/screenshots/refs/heads/main/blitz/hotreload-video.webp\">\n  <!-- <video src=\"https://private-user-images.githubusercontent.com/10237910/386919031-6da371d5-3340-46da-84ff-628216851ba6.mov\" width=\"500\"></video> -->\n  <!-- <video src=\"https://private-user-images.githubusercontent.com/10237910/386919031-6da371d5-3340-46da-84ff-628216851ba6.mov\" width=\"500\"></video> -->\n</div>\n\n## Build Beautiful Apps\n\nDioxus apps are styled with HTML and CSS. Use the built-in TailwindCSS support or load your favorite CSS library. Easily call into native code (objective-c, JNI, Web-Sys) for a perfect native touch.\n\n<div align=\"center\">\n  <img src=\"./notes/ebou2.avif\">\n</div>\n\n\n\n## Truly fullstack applications\n\nDioxus deeply integrates with [axum](https://github.com/tokio-rs/axum) to provide powerful fullstack capabilities for both clients and servers. Pick from a wide array of built-in batteries like WebSockets, SSE, Streaming, File Upload/Download, Server-Side-Rendering, Forms, Middleware, and Hot-Reload, or go fully custom and integrate your existing axum backend.\n\n<div align=\"center\">\n  <img src=\"./notes/fullstack-websockets.avif\" width=\"700\">\n</div>\n\n## Experimental Native Renderer\n\nRender using web-sys, webview, server-side-rendering, liveview, or even with our experimental WGPU-based renderer. Embed Dioxus in Bevy, WGPU, or even run on embedded Linux!\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/DioxusLabs/screenshots/refs/heads/main/blitz/native-blitz-wgpu.webp\">\n</div>\n\n\n## First-party primitive components\n\nGet started quickly with a complete set of primitives modeled after shadcn/ui and Radix-Primitives.\n\n<div align=\"center\">\n  <img src=\"./notes/primitive-components.avif\" width=\"700\">\n</div>\n\n## First-class Android and iOS support\n\nDioxus is the fastest way to build native mobile apps with Rust. Simply run `dx serve --platform android` and your app is running in an emulator or on device in seconds. Call directly into JNI and Native APIs.\n\n<div align=\"center\">\n  <img src=\"./notes/android_and_ios2.avif\" width=\"500\">\n</div>\n\n\n\n## Bundle for web, desktop, and mobile\n\nSimply run `dx bundle` and your app will be built and bundled with maximization optimizations. On the web, take advantage of [`.avif` generation, `.wasm` compression, minification](https://dioxuslabs.com/learn/0.7/tutorial/assets), and more. Build WebApps weighing [less than 50kb](https://github.com/ealmloff/tiny-dioxus/) and desktop/mobile apps less than 5mb.\n\n<div align=\"center\">\n  <img src=\"./notes/bundle.gif\">\n</div>\n\n\n## Fantastic documentation\n\nWe've put a ton of effort into building clean, readable, and comprehensive documentation. All html elements and listeners are documented with MDN docs, and our Docs runs continuous integration with Dioxus itself to ensure that the docs are always up to date. Check out the [Dioxus website](https://dioxuslabs.com/learn/0.7/) for guides, references, recipes, and more. Fun fact: we use the Dioxus website as a testbed for new Dioxus features - [check it out!](https://github.com/dioxusLabs/docsite)\n\n<div align=\"center\">\n  <img src=\"./notes/docs.avif\">\n</div>\n\n\n## Modular and Customizable\n\nBuild your own renderer, or use a community renderer like [Freya](http://freyaui.dev). Use our modular components like RSX, VirtualDom, Blitz, Taffy, and Subsecond.\n\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/DioxusLabs/screenshots/refs/heads/main/blitz/freya-todo-example.webp\">\n</div>\n\n## Community\n\nDioxus is a community-driven project, with a very active [Discord](https://discord.gg/XgGxMSkvUM) and [GitHub](https://github.com/DioxusLabs/dioxus/issues) community. We're always looking for help, and we're happy to answer questions and help you get started. [Our SDK](https://github.com/DioxusLabs/dioxus-std) is community-run and we even have a [GitHub organization](https://github.com/dioxus-community/) for the best Dioxus crates that receive free upgrades and support.\n\n<div align=\"center\">\n  <img src=\"./notes/dioxus-community.avif\">\n</div>\n\n## Full-time core team\n\nDioxus has grown from a side project to a small team of fulltime engineers. Thanks to the generous support of FutureWei, Satellite.im, the GitHub Accelerator program, we're able to work on Dioxus full-time. Our long term goal is for Dioxus to become self-sustaining by providing paid high-quality enterprise tools. If your company is interested in adopting Dioxus and would like to work with us, please reach out!\n\n## Supported Platforms\n\n<div align=\"center\">\n  <table style=\"width:100%\">\n    <tr>\n      <td>\n      <b>Web</b>\n      </td>\n      <td>\n        <ul>\n          <li>Render directly to the DOM using WebAssembly</li>\n          <li>Pre-render with SSR and rehydrate on the client</li>\n          <li>Simple \"hello world\" at about 50kb, comparable to React</li>\n          <li>Built-in dev server and hot reloading for quick iteration</li>\n        </ul>\n      </td>\n    </tr>\n    <tr>\n      <td>\n      <b>Desktop</b>\n      </td>\n      <td>\n        <ul>\n          <li>Render using Webview or - experimentally - with WGPU or <a href=\"https://freyaui.dev\">Freya</a> (Skia) </li>\n          <li>Zero-config setup. Simply `cargo run` or `dx serve` to build your app </li>\n          <li>Full support for native system access without IPC </li>\n          <li>Supports macOS, Linux, and Windows. Portable <3mb binaries </li>\n        </ul>\n      </td>\n    </tr>\n    <tr>\n      <td>\n      <b>Mobile</b>\n      </td>\n      <td>\n        <ul>\n          <li>Render using Webview or - experimentally - with WGPU or Skia </li>\n          <li>Build .ipa and .apk files for iOS and Android </li>\n          <li>Call directly into Java and Objective-C with minimal overhead</li>\n          <li>From \"hello world\" to running on device in seconds</li>\n        </ul>\n      </td>\n    </tr>\n    <tr>\n      <td>\n      <b>Server-side Rendering</b>\n      </td>\n      <td>\n        <ul>\n          <li>Suspense, hydration, and server-side rendering</li>\n          <li>Quickly drop in backend functionality with server functions</li>\n          <li>Extractors, middleware, and routing integrations</li>\n          <li>Static-site generation and incremental regeneration</li>\n        </ul>\n      </td>\n    </tr>\n  </table>\n</div>\n\n## Running the examples\n\n> The examples in the main branch of this repository target the git version of dioxus and the CLI. If you are looking for examples that work with the latest stable release of dioxus, check out the [0.6 branch](https://github.com/DioxusLabs/dioxus/tree/v0.6/examples).\n\nThe examples in the top level of this repository can be run with:\n\n```sh\ncargo run --example <example>\n```\n\nHowever, we encourage you to download the dioxus-cli to test out features like hot-reloading. To install the most recent binary CLI, you can use cargo binstall.\n\n```sh\ncargo binstall dioxus-cli@0.7.0 --force\n```\n\nIf this CLI is out-of-date, you can install it directly from git\n\n```sh\ncargo install --git https://github.com/DioxusLabs/dioxus dioxus-cli --locked\n```\n\nWith the CLI, you can also run examples with the web platform. You will need to disable the default desktop feature and enable the web feature with this command:\n\n```sh\ndx serve --example <example> --platform web -- --no-default-features\n```\n\n## Contributing\n\n- Check out the website [section on contributing](https://dioxuslabs.com/learn/0.7/beyond/contributing).\n- Report issues on our [issue tracker](https://github.com/dioxuslabs/dioxus/issues).\n- [Join](https://discord.gg/XgGxMSkvUM) the discord and ask questions!\n\n<a href=\"https://github.com/dioxuslabs/dioxus/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=dioxuslabs/dioxus&max=30&columns=10\" />\n</a>\n\n## License\n\nThis project is licensed under either the [MIT license] or the [Apache-2 License].\n\n[apache-2 license]: https://github.com/DioxusLabs/dioxus/blob/master/LICENSE-APACHE\n[mit license]: https://github.com/DioxusLabs/dioxus/blob/master/LICENSE-MIT\n\nUnless you explicitly state otherwise, any contribution intentionally submitted\nfor inclusion in Dioxus by you, shall be licensed as MIT or Apache-2, without any additional\nterms or conditions.\n",
      "stars_today": 119
    },
    {
      "id": 1024118326,
      "name": "WeKnora",
      "full_name": "Tencent/WeKnora",
      "description": "LLM-powered framework for deep document understanding, semantic retrieval, and context-aware answers using RAG paradigm.",
      "html_url": "https://github.com/Tencent/WeKnora",
      "stars": 11628,
      "forks": 1274,
      "language": "Go",
      "topics": [
        "agent",
        "agentic",
        "ai",
        "chatbot",
        "chatbots",
        "embeddings",
        "evaluation",
        "generative-ai",
        "golang",
        "knowledge-base",
        "llm",
        "multi-tenant",
        "multimodel",
        "ollama",
        "openai",
        "question-answering",
        "rag",
        "reranking",
        "semantic-search",
        "vector-search"
      ],
      "created_at": "2025-07-22T08:01:23Z",
      "updated_at": "2026-01-15T01:00:51Z",
      "pushed_at": "2026-01-14T11:39:35Z",
      "open_issues": 81,
      "owner": {
        "login": "Tencent",
        "avatar_url": "https://avatars.githubusercontent.com/u/18461506?v=4"
      },
      "readme": "<p align=\"center\">\n  <picture>\n    <img src=\"./docs/images/logo.png\" alt=\"WeKnora Logo\" height=\"120\"/>\n  </picture>\n</p>\n\n<p align=\"center\">\n  <picture>\n    <a href=\"https://trendshift.io/repositories/15289\" target=\"_blank\">\n      <img src=\"https://trendshift.io/api/badge/repositories/15289\" alt=\"Tencent%2FWeKnora | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/>\n    </a>\n  </picture>\n</p>\n<p align=\"center\">\n    <a href=\"https://weknora.weixin.qq.com\" target=\"_blank\">\n        <img alt=\"å®˜æ–¹ç½‘ç«™\" src=\"https://img.shields.io/badge/å®˜æ–¹ç½‘ç«™-WeKnora-4e6b99\">\n    </a>\n    <a href=\"https://chatbot.weixin.qq.com\" target=\"_blank\">\n        <img alt=\"å¾®ä¿¡å¯¹è¯å¼€æ”¾å¹³å°\" src=\"https://img.shields.io/badge/å¾®ä¿¡å¯¹è¯å¼€æ”¾å¹³å°-5ac725\">\n    </a>\n    <a href=\"https://github.com/Tencent/WeKnora/blob/main/LICENSE\">\n        <img src=\"https://img.shields.io/badge/License-MIT-ffffff?labelColor=d4eaf7&color=2e6cc4\" alt=\"License\">\n    </a>\n    <a href=\"./CHANGELOG.md\">\n        <img alt=\"Version\" src=\"https://img.shields.io/badge/version-0.2.6-2e6cc4?labelColor=d4eaf7\">\n    </a>\n</p>\n\n<p align=\"center\">\n| <b>English</b> | <a href=\"./README_CN.md\"><b>ç®€ä½“ä¸­æ–‡</b></a> | <a href=\"./README_JA.md\"><b>æ—¥æœ¬èª</b></a> |\n</p>\n\n<p align=\"center\">\n  <h4 align=\"center\">\n\n  [Overview](#-overview) â€¢ [Architecture](#-architecture) â€¢ [Key Features](#-key-features) â€¢ [Getting Started](#-getting-started) â€¢ [API Reference](#-api-reference) â€¢ [Developer Guide](#-developer-guide)\n  \n  </h4>\n</p>\n\n# ğŸ’¡ WeKnora - LLM-Powered Document Understanding & Retrieval Framework\n\n## ğŸ“Œ Overview\n\n[**WeKnora**](https://weknora.weixin.qq.com) is an LLM-powered framework designed for deep document understanding and semantic retrieval, especially for handling complex, heterogeneous documents. \n\nIt adopts a modular architecture that combines multimodal preprocessing, semantic vector indexing, intelligent retrieval, and large language model inference. At its core, WeKnora follows the **RAG (Retrieval-Augmented Generation)** paradigm, enabling high-quality, context-aware answers by combining relevant document chunks with model reasoning.\n\n**Website:** https://weknora.weixin.qq.com\n\n## âœ¨ Latest Updates\n\n**v0.2.0 Highlights:**\n\n- ğŸ¤– **Agent Mode**: New ReACT Agent mode that can call built-in tools, MCP tools, and web search, providing comprehensive summary reports through multiple iterations and reflection\n- ğŸ“š **Multi-Type Knowledge Bases**: Support for FAQ and document knowledge base types, with new features including folder import, URL import, tag management, and online entry\n- âš™ï¸ **Conversation Strategy**: Support for configuring Agent models, normal mode models, retrieval thresholds, and Prompts, with precise control over multi-turn conversation behavior\n- ğŸŒ **Web Search**: Support for extensible web search engines with built-in DuckDuckGo search engine\n- ğŸ”Œ **MCP Tool Integration**: Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting multiple transport methods\n- ğŸ¨ **New UI**: Optimized conversation interface with Agent mode/normal mode switching, tool call process display, and comprehensive knowledge base management interface upgrade\n- âš¡ **Infrastructure Upgrade**: Introduced MQ async task management, support for automatic database migration, and fast development mode\n\n## ğŸ”’ Security Notice\n\n**Important:** Starting from v0.1.3, WeKnora includes login authentication functionality to enhance system security. For production deployments, we strongly recommend:\n\n- Deploy WeKnora services in internal/private network environments rather than public internet\n- Avoid exposing the service directly to public networks to prevent potential information leakage\n- Configure proper firewall rules and access controls for your deployment environment\n- Regularly update to the latest version for security patches and improvements\n\n## ğŸ—ï¸ Architecture\n\n![weknora-architecture.png](./docs/images/architecture.png)\n\nWeKnora employs a modern modular design to build a complete document understanding and retrieval pipeline. The system primarily includes document parsing, vector processing, retrieval engine, and large model inference as core modules, with each component being flexibly configurable and extendable.\n\n## ğŸ¯ Key Features\n\n- **ğŸ¤– Agent Mode**: Support for ReACT Agent mode that can use built-in tools to retrieve knowledge bases, MCP tools, and web search tools to access external services, providing comprehensive summary reports through multiple iterations and reflection\n- **ğŸ” Precise Understanding**: Structured content extraction from PDFs, Word documents, images and more into unified semantic views\n- **ğŸ§  Intelligent Reasoning**: Leverages LLMs to understand document context and user intent for accurate Q&A and multi-turn conversations\n- **ğŸ“š Multi-Type Knowledge Bases**: Support for FAQ and document knowledge base types, with folder import, URL import, tag management, and online entry capabilities\n- **ğŸ”§ Flexible Extension**: All components from parsing and embedding to retrieval and generation are decoupled for easy customization\n- **âš¡ Efficient Retrieval**: Hybrid retrieval strategies combining keywords, vectors, and knowledge graphs, with cross-knowledge base retrieval support\n- **ğŸŒ Web Search**: Support for extensible web search engines with built-in DuckDuckGo search engine\n- **ğŸ”Œ MCP Tool Integration**: Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting multiple transport methods\n- **âš™ï¸ Conversation Strategy**: Support for configuring Agent models, normal mode models, retrieval thresholds, and Prompts, with precise control over multi-turn conversation behavior\n- **ğŸ¯ User-Friendly**: Intuitive web interface and standardized APIs for zero technical barriers\n- **ğŸ”’ Secure & Controlled**: Support for local deployment and private cloud, ensuring complete data sovereignty\n\n## ğŸ“Š Application Scenarios\n\n| Scenario | Applications | Core Value |\n|---------|----------|----------|\n| **Enterprise Knowledge Management** | Internal document retrieval, policy Q&A, operation manual search | Improve knowledge discovery efficiency, reduce training costs |\n| **Academic Research Analysis** | Paper retrieval, research report analysis, scholarly material organization | Accelerate literature review, assist research decisions |\n| **Product Technical Support** | Product manual Q&A, technical documentation search, troubleshooting | Enhance customer service quality, reduce support burden |\n| **Legal & Compliance Review** | Contract clause retrieval, regulatory policy search, case analysis | Improve compliance efficiency, reduce legal risks |\n| **Medical Knowledge Assistance** | Medical literature retrieval, treatment guideline search, case analysis | Support clinical decisions, improve diagnosis quality |\n\n## ğŸ§© Feature Matrix\n\n| Module | Support                                                                        | Description                                                                                                                                                        |\n|---------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Agent Mode | âœ… ReACT Agent Mode                                                             | Support for using built-in tools to retrieve knowledge bases, MCP tools, and web search, with cross-knowledge base retrieval and multiple iterations               |\n| Knowledge Base Types | âœ… FAQ / Document                                                               | Support for creating FAQ and document knowledge base types, with folder import, URL import, tag management, and online entry                                       |\n| Document Formats | âœ… PDF / Word / Txt / Markdown / Images (with OCR / Caption)                    | Support for structured and unstructured documents with text extraction from images                                                                                 |\n| Model Management | âœ… Centralized configuration, built-in model sharing                            | Centralized model configuration with model selection in knowledge base settings, support for multi-tenant shared built-in models                                   |\n| Embedding Models | âœ… Local models, BGE / GTE APIs, etc.                                           | Customizable embedding models, compatible with local deployment and cloud vector generation APIs                                                                   |\n| Vector DB Integration | âœ… PostgreSQL (pgvector), Elasticsearch                                         | Support for mainstream vector index backends, flexible switching for different retrieval scenarios                                                                 |\n| Retrieval Strategies | âœ… BM25 / Dense Retrieval / GraphRAG                                            | Support for sparse/dense recall and knowledge graph-enhanced retrieval with customizable retrieve-rerank-generate pipelines                                        |\n| LLM Integration | âœ… Support for Qwen, DeepSeek, etc., with thinking/non-thinking mode switching  | Compatible with local models (e.g., via Ollama) or external API services with flexible inference configuration                                                     |\n| Conversation Strategy | âœ… Agent models, normal mode models, retrieval thresholds, Prompt configuration | Support for configuring Agent models, normal mode models, retrieval thresholds, online Prompt configuration, precise control over multi-turn conversation behavior |\n| Web Search | âœ… Extensible search engines, DuckDuckGo / Google                               | Support for extensible web search engines with built-in DuckDuckGo search engine                                                                                   |\n| MCP Tools | âœ… uvx, npx launchers, Stdio/HTTP Streamable/SSE                                | Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting three transport methods                                      |\n| QA Capabilities | âœ… Context-aware, multi-turn dialogue, prompt templates                         | Support for complex semantic modeling, instruction control and chain-of-thought Q&A with configurable prompts and context windows                                  |\n| E2E Testing | âœ… Retrieval+generation process visualization and metric evaluation             | End-to-end testing tools for evaluating recall hit rates, answer coverage, BLEU/ROUGE and other metrics                                                            |\n| Deployment Modes | âœ… Support for local deployment / Docker images                                 | Meets private, offline deployment and flexible operation requirements, with fast development mode support                                                          |\n| User Interfaces | âœ… Web UI + RESTful API                                                         | Interactive interface and standard API endpoints, with Agent mode/normal mode switching and tool call process display                                              |\n| Task Management | âœ… MQ async tasks, automatic database migration                                 | MQ-based async task state maintenance, support for automatic database schema and data migration during version upgrades                                            |\n\n## ğŸš€ Getting Started\n\n### ğŸ›  Prerequisites\n\nMake sure the following tools are installed on your system:\n\n* [Docker](https://www.docker.com/)\n* [Docker Compose](https://docs.docker.com/compose/)\n* [Git](https://git-scm.com/)\n\n### ğŸ“¦ Installation\n\n#### â‘  Clone the repository\n\n```bash\n# Clone the main repository\ngit clone https://github.com/Tencent/WeKnora.git\ncd WeKnora\n```\n\n#### â‘¡ Configure environment variables\n\n```bash\n# Copy example env file\ncp .env.example .env\n\n# Edit .env and set required values\n# All variables are documented in the .env.example comments\n```\n\n#### â‘¢ Start the services (include Ollama)\n\nCheck the images that need to be started in the .env file.\n\n```bash\n./scripts/start_all.sh\n```\n\nor\n\n```bash\nmake start-all\n```\n\n#### â‘¢.0 Start ollama services (Optional)\n\n```bash\nollama serve > /dev/null 2>&1 &\n```\n\n#### â‘¢.1 Activate different combinations of features\n\n- Minimum core services\n```bash\ndocker compose up -d\n```\n\n- All features enabled\n```bash\ndocker-compose --profile full up -d\n```\n\n- Tracing logs required\n```bash\ndocker-compose --profile jaeger up -d\n```\n\n- Neo4j knowledge graph required\n```bash\ndocker-compose --profile neo4j up -d\n```\n\n- Minio file storage service required\n```bash\ndocker-compose --profile minio up -d\n```\n\n- Multiple options combination\n```bash\ndocker-compose --profile neo4j --profile minio up -d\n```\n\n#### â‘£ Stop the services\n\n```bash\n./scripts/start_all.sh --stop\n# Or\nmake stop-all\n```\n\n### ğŸŒ Access Services\n\nOnce started, services will be available at:\n\n* Web UI: `http://localhost`\n* Backend API: `http://localhost:8080`\n* Jaeger Tracing: `http://localhost:16686`\n\n### ğŸ”Œ Using WeChat Dialog Open Platform\n\nWeKnora serves as the core technology framework for the [WeChat Dialog Open Platform](https://chatbot.weixin.qq.com), providing a more convenient usage approach:\n\n- **Zero-code Deployment**: Simply upload knowledge to quickly deploy intelligent Q&A services within the WeChat ecosystem, achieving an \"ask and answer\" experience\n- **Efficient Question Management**: Support for categorized management of high-frequency questions, with rich data tools to ensure accurate, reliable, and easily maintainable answers\n- **WeChat Ecosystem Integration**: Through the WeChat Dialog Open Platform, WeKnora's intelligent Q&A capabilities can be seamlessly integrated into WeChat Official Accounts, Mini Programs, and other WeChat scenarios, enhancing user interaction experiences\n\n### ğŸ”— Access WeKnora via MCP Server\n\n#### 1ï¸âƒ£ Clone the repository\n```\ngit clone https://github.com/Tencent/WeKnora\n```\n\n#### 2ï¸âƒ£ Configure MCP Server\n> It is recommended to directly refer to the [MCP Configuration Guide](./mcp-server/MCP_CONFIG.md) for configuration.\n\nConfigure the MCP client to connect to the server:\n```json\n{\n  \"mcpServers\": {\n    \"weknora\": {\n      \"args\": [\n        \"path/to/WeKnora/mcp-server/run_server.py\"\n      ],\n      \"command\": \"python\",\n      \"env\":{\n        \"WEKNORA_API_KEY\":\"Enter your WeKnora instance, open developer tools, check the request header x-api-key starting with sk\",\n        \"WEKNORA_BASE_URL\":\"http(s)://your-weknora-address/api/v1\"\n      }\n    }\n  }\n}\n```\n\nRun directly using stdio command:\n```\npip install weknora-mcp-server\npython -m weknora-mcp-server\n```\n\n## ğŸ”§ Initialization Configuration Guide\n\nTo help users quickly configure various models and reduce trial-and-error costs, we've improved the original configuration file initialization method by adding a Web UI interface for model configuration. Before using, please ensure the code is updated to the latest version. The specific steps are as follows:\nIf this is your first time using this project, you can skip steps â‘ â‘¡ and go directly to steps â‘¢â‘£.\n\n### â‘  Stop the services\n\n```bash\n./scripts/start_all.sh --stop\n```\n\n### â‘¡ Clear existing data tables (recommended when no important data exists)\n\n```bash\nmake clean-db\n```\n\n### â‘¢ Compile and start services\n\n```bash\n./scripts/start_all.sh\n```\n\n### â‘£ Access Web UI\n\nhttp://localhost\n\nOn your first visit, you will be automatically redirected to the registration/login page. After completing registration, please create a new knowledge base and finish the relevant settings on its configuration page.\n\n## ğŸ“± Interface Showcase\n\n### Web UI Interface\n\n<table>\n  <tr>\n    <td><b>Knowledge Base Management</b><br/><img src=\"./docs/images/knowledgebases.png\" alt=\"Knowledge Base Management\"></td>\n    <td><b>Conversation Settings</b><br/><img src=\"./docs/images/settings.png\" alt=\"Conversation Settings\"></td>\n  </tr>\n  <tr>\n    <td colspan=\"2\"><b>Agent Mode Tool Call Process</b><br/><img src=\"./docs/images/agent-qa.png\" alt=\"Agent Mode Tool Call Process\"></td>\n  </tr>\n</table>\n\n**Knowledge Base Management:** Support for creating FAQ and document knowledge base types, with multiple import methods including drag-and-drop, folder import, and URL import. Automatically identifies document structures and extracts core knowledge to establish indexes. Supports tag management and online entry. The system clearly displays processing progress and document status, achieving efficient knowledge base management.\n\n**Agent Mode:** Support for ReACT Agent mode that can use built-in tools to retrieve knowledge bases, call user-configured MCP tools and web search tools to access external services, providing comprehensive summary reports through multiple iterations and reflection. Supports cross-knowledge base retrieval, allowing selection of multiple knowledge bases for simultaneous retrieval.\n\n**Conversation Strategy:** Support for configuring Agent models, normal mode models, retrieval thresholds, and online Prompt configuration, with precise control over multi-turn conversation behavior and retrieval execution methods. The conversation input box supports Agent mode/normal mode switching, enabling/disabling web search, and selecting conversation models.\n\n### Document Knowledge Graph\n\nWeKnora supports transforming documents into knowledge graphs, displaying the relationships between different sections of the documents. Once the knowledge graph feature is enabled, the system analyzes and constructs an internal semantic association network that not only helps users understand document content but also provides structured support for indexing and retrieval, enhancing the relevance and breadth of search results.\n\nFor detailed configuration, please refer to the [Knowledge Graph Configuration Guide](./docs/KnowledgeGraph.md).\n\n### MCP Server\n\nPlease refer to the [MCP Configuration Guide](./mcp-server/MCP_CONFIG.md) for the necessary setup.\n\n## ğŸ“˜ API Reference\n\nTroubleshooting FAQ: [Troubleshooting FAQ](./docs/QA.md)\n\nDetailed API documentation is available at: [API Docs](./docs/api/README.md)\n\n## ğŸ§­ Developer Guide\n\n### âš¡ Fast Development Mode (Recommended)\n\nIf you need to frequently modify code, **you don't need to rebuild Docker images every time**! Use fast development mode:\n\n```bash\n# Method 1: Using Make commands (Recommended)\nmake dev-start      # Start infrastructure\nmake dev-app        # Start backend (new terminal)\nmake dev-frontend   # Start frontend (new terminal)\n\n# Method 2: One-click start\n./scripts/quick-dev.sh\n\n# Method 3: Using scripts\n./scripts/dev.sh start     # Start infrastructure\n./scripts/dev.sh app       # Start backend (new terminal)\n./scripts/dev.sh frontend  # Start frontend (new terminal)\n```\n\n**Development Advantages:**\n- âœ… Frontend modifications auto hot-reload (no restart needed)\n- âœ… Backend modifications quick restart (5-10 seconds, supports Air hot-reload)\n- âœ… No need to rebuild Docker images\n- âœ… Support IDE breakpoint debugging\n\n**Detailed Documentation:** [Development Environment Quick Start](./docs/å¼€å‘æŒ‡å—.md)\n\n### ğŸ“ Directory Structure\n\n```\nWeKnora/\nâ”œâ”€â”€ client/      # go client\nâ”œâ”€â”€ cmd/         # Main entry point\nâ”œâ”€â”€ config/      # Configuration files\nâ”œâ”€â”€ docker/      # docker images files\nâ”œâ”€â”€ docreader/   # Document parsing app\nâ”œâ”€â”€ docs/        # Project documentation\nâ”œâ”€â”€ frontend/    # Frontend app\nâ”œâ”€â”€ internal/    # Core business logic\nâ”œâ”€â”€ mcp-server/  # MCP server\nâ”œâ”€â”€ migrations/  # DB migration scripts\nâ””â”€â”€ scripts/     # Shell scripts\n```\n\n## ğŸ¤ Contributing\n\nWe welcome community contributions! For suggestions, bugs, or feature requests, please submit an [Issue](https://github.com/Tencent/WeKnora/issues) or directly create a Pull Request.\n\n### ğŸ¯ How to Contribute\n\n- ğŸ› **Bug Fixes**: Discover and fix system defects\n- âœ¨ **New Features**: Propose and implement new capabilities\n- ğŸ“š **Documentation**: Improve project documentation\n- ğŸ§ª **Test Cases**: Write unit and integration tests\n- ğŸ¨ **UI/UX Enhancements**: Improve user interface and experience\n\n### ğŸ“‹ Contribution Process\n\n1. **Fork the project** to your GitHub account\n2. **Create a feature branch** `git checkout -b feature/amazing-feature`\n3. **Commit changes** `git commit -m 'Add amazing feature'`\n4. **Push branch** `git push origin feature/amazing-feature`\n5. **Create a Pull Request** with detailed description of changes\n\n### ğŸ¨ Code Standards\n\n- Follow [Go Code Review Comments](https://github.com/golang/go/wiki/CodeReviewComments)\n- Format code using `gofmt`\n- Add necessary unit tests\n- Update relevant documentation\n\n### ğŸ“ Commit Guidelines\n\nUse [Conventional Commits](https://www.conventionalcommits.org/) standard:\n\n```\nfeat: Add document batch upload functionality\nfix: Resolve vector retrieval precision issue\ndocs: Update API documentation\ntest: Add retrieval engine test cases\nrefactor: Restructure document parsing module\n```\n\n## ğŸ‘¥ Contributors\n\nThanks to these excellent contributors:\n\n[![Contributors](https://contrib.rocks/image?repo=Tencent/WeKnora)](https://github.com/Tencent/WeKnora/graphs/contributors)\n\n## ğŸ“„ License\n\nThis project is licensed under the [MIT License](./LICENSE).\nYou are free to use, modify, and distribute the code with proper attribution.\n\n## ğŸ“ˆ Project Statistics\n\n<a href=\"https://www.star-history.com/#Tencent/WeKnora&type=date&legend=top-left\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=Tencent/WeKnora&type=date&theme=dark&legend=top-left\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=Tencent/WeKnora&type=date&legend=top-left\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=Tencent/WeKnora&type=date&legend=top-left\" />\n </picture>\n</a>\n",
      "stars_today": 114
    },
    {
      "id": 501045649,
      "name": "waveterm",
      "full_name": "wavetermdev/waveterm",
      "description": "An open-source, cross-platform terminal for seamless workflows",
      "html_url": "https://github.com/wavetermdev/waveterm",
      "stars": 16262,
      "forks": 699,
      "language": "Go",
      "topics": [
        "command-line",
        "developer-tools",
        "linux",
        "macos",
        "productivity",
        "terminal",
        "terminal-emulators",
        "windows"
      ],
      "created_at": "2022-06-08T00:26:00Z",
      "updated_at": "2026-01-15T00:13:06Z",
      "pushed_at": "2026-01-15T01:00:09Z",
      "open_issues": 425,
      "owner": {
        "login": "wavetermdev",
        "avatar_url": "https://avatars.githubusercontent.com/u/120279640?v=4"
      },
      "readme": "<p align=\"center\">\n  <a href=\"https://www.waveterm.dev\">\n\t<picture>\n\t\t<source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/wave-dark.png\">\n\t\t<source media=\"(prefers-color-scheme: light)\" srcset=\"./assets/wave-light.png\">\n\t\t<img alt=\"Wave Terminal Logo\" src=\"./assets/wave-light.png\" width=\"240\">\n\t</picture>\n  </a>\n  <br/>\n</p>\n\n# Wave Terminal\n\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Fwavetermdev%2Fwaveterm.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Fwavetermdev%2Fwaveterm?ref=badge_shield)\n\nWave is an open-source terminal that combines traditional terminal features with graphical capabilities like file previews, web browsing, and AI assistance. It runs on MacOS, Linux, and Windows.\n\nModern development involves constantly switching between terminals and browsers - checking documentation, previewing files, monitoring systems, and using AI tools. Wave brings these graphical tools directly into the terminal, letting you control them from the command line. This means you can stay in your terminal workflow while still having access to the visual interfaces you need.\n\n![WaveTerm Screenshot](./assets/wave-screenshot.webp)\n\n## Key Features\n\n- Flexible drag & drop interface to organize terminal blocks, editors, web browsers, and AI assistants\n- Built-in editor for seamlessly editing remote files with syntax highlighting and modern editor features\n- Rich file preview system for remote files (markdown, images, video, PDFs, CSVs, directories)\n- Quick full-screen toggle for any block - expand terminals, editors, and previews for better visibility, then instantly return to multi-block view\n- Wave AI - Context-aware terminal assistant that reads your terminal output, analyzes widgets, and performs file operations\n- AI chat widget with support for multiple models (OpenAI, Claude, Azure, Perplexity, Ollama)\n- Command Blocks for isolating and monitoring individual commands with auto-close options\n- One-click remote connections with full terminal and file system access\n- Secure secret storage using native system backends - store API keys and credentials locally, access them across SSH sessions\n- Rich customization including tab themes, terminal styles, and background images\n- Powerful `wsh` command system for managing your workspace from the CLI and sharing data between terminal sessions\n- Connected file management with `wsh file` - seamlessly copy and sync files between local, remote SSH hosts, Wave filesystem, and S3\n\n## Wave AI\n\nWave AI is your context-aware terminal assistant with access to your workspace:\n\n- **Terminal Context**: Reads terminal output and scrollback for debugging and analysis\n- **File Operations**: Read, write, and edit files with automatic backups and user approval\n- **CLI Integration**: Use `wsh ai` to pipe output or attach files directly from the command line\n- **Free Beta**: Included AI credits while we refine the experience\n- **Coming Soon**: Command execution (with approval), local model support, and alternate AI providers (BYOK)\n\nLearn more in our [Wave AI documentation](https://docs.waveterm.dev/waveai).\n\n## Installation\n\nWave Terminal works on macOS, Linux, and Windows.\n\nPlatform-specific installation instructions can be found [here](https://docs.waveterm.dev/gettingstarted).\n\nYou can also install Wave Terminal directly from: [www.waveterm.dev/download](https://www.waveterm.dev/download).\n\n### Minimum requirements\n\nWave Terminal runs on the following platforms:\n\n- macOS 11 or later (arm64, x64)\n- Windows 10 1809 or later (x64)\n- Linux based on glibc-2.28 or later (Debian 10, RHEL 8, Ubuntu 20.04, etc.) (arm64, x64)\n\nThe WSH helper runs on the following platforms:\n\n- macOS 11 or later (arm64, x64)\n- Windows 10 or later (arm64, x64)\n- Linux Kernel 2.6.32 or later (x64), Linux Kernel 3.1 or later (arm64)\n\n## Roadmap\n\nWave is constantly improving! Our roadmap will be continuously updated with our goals for each release. You can find it [here](./ROADMAP.md).\n\nWant to provide input to our future releases? Connect with us on [Discord](https://discord.gg/XfvZ334gwU) or open a [Feature Request](https://github.com/wavetermdev/waveterm/issues/new/choose)!\n\n## Links\n\n- Homepage &mdash; https://www.waveterm.dev\n- Download Page &mdash; https://www.waveterm.dev/download\n- Documentation &mdash; https://docs.waveterm.dev\n- Legacy Documentation &mdash; https://legacydocs.waveterm.dev\n- Blog &mdash; https://blog.waveterm.dev\n- X &mdash; https://x.com/wavetermdev\n- Discord Community &mdash; https://discord.gg/XfvZ334gwU\n\n## Building from Source\n\nSee [Building Wave Terminal](BUILD.md).\n\n## Contributing\n\nWave uses GitHub Issues for issue tracking.\n\nFind more information in our [Contributions Guide](CONTRIBUTING.md), which includes:\n\n- [Ways to contribute](CONTRIBUTING.md#contributing-to-wave-terminal)\n- [Contribution guidelines](CONTRIBUTING.md#before-you-start)\n\n## License\n\nWave Terminal is licensed under the Apache-2.0 License. For more information on our dependencies, see [here](./ACKNOWLEDGEMENTS.md).\n",
      "stars_today": 111
    },
    {
      "id": 340547520,
      "name": "zed",
      "full_name": "zed-industries/zed",
      "description": "Code at the speed of thought â€“ Zed is a high-performance, multiplayer code editor from the creators of Atom and Tree-sitter.",
      "html_url": "https://github.com/zed-industries/zed",
      "stars": 73316,
      "forks": 6613,
      "language": "Rust",
      "topics": [
        "gpui",
        "rust-lang",
        "text-editor",
        "zed"
      ],
      "created_at": "2021-02-20T03:01:06Z",
      "updated_at": "2026-01-15T00:47:40Z",
      "pushed_at": "2026-01-15T00:57:24Z",
      "open_issues": 3328,
      "owner": {
        "login": "zed-industries",
        "avatar_url": "https://avatars.githubusercontent.com/u/79345384?v=4"
      },
      "readme": "# Zed\n\n[![Zed](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/zed-industries/zed/main/assets/badge/v0.json)](https://zed.dev)\n[![CI](https://github.com/zed-industries/zed/actions/workflows/run_tests.yml/badge.svg)](https://github.com/zed-industries/zed/actions/workflows/run_tests.yml)\n\nWelcome to Zed, a high-performance, multiplayer code editor from the creators of [Atom](https://github.com/atom/atom) and [Tree-sitter](https://github.com/tree-sitter/tree-sitter).\n\n---\n\n### Installation\n\nOn macOS, Linux, and Windows you can [download Zed directly](https://zed.dev/download) or install Zed via your local package manager ([macOS](https://zed.dev/docs/installation#macos)/[Linux](https://zed.dev/docs/linux#installing-via-a-package-manager)/[Windows](https://zed.dev/docs/windows#package-managers)).\n\nOther platforms are not yet available:\n\n- Web ([tracking issue](https://github.com/zed-industries/zed/issues/5396))\n\n### Developing Zed\n\n- [Building Zed for macOS](./docs/src/development/macos.md)\n- [Building Zed for Linux](./docs/src/development/linux.md)\n- [Building Zed for Windows](./docs/src/development/windows.md)\n\n### Contributing\n\nSee [CONTRIBUTING.md](./CONTRIBUTING.md) for ways you can contribute to Zed.\n\nAlso... we're hiring! Check out our [jobs](https://zed.dev/jobs) page for open roles.\n\n### Licensing\n\nLicense information for third party dependencies must be correctly provided for CI to pass.\n\nWe use [`cargo-about`](https://github.com/EmbarkStudios/cargo-about) to automatically comply with open source licenses. If CI is failing, check the following:\n\n- Is it showing a `no license specified` error for a crate you've created? If so, add `publish = false` under `[package]` in your crate's Cargo.toml.\n- Is the error `failed to satisfy license requirements` for a dependency? If so, first determine what license the project has and whether this system is sufficient to comply with this license's requirements. If you're unsure, ask a lawyer. Once you've verified that this system is acceptable add the license's SPDX identifier to the `accepted` array in `script/licenses/zed-licenses.toml`.\n- Is `cargo-about` unable to find the license for a dependency? If so, add a clarification field at the end of `script/licenses/zed-licenses.toml`, as specified in the [cargo-about book](https://embarkstudios.github.io/cargo-about/cli/generate/config.html#crate-configuration).\n\n## Sponsorship\n\nZed is developed by **Zed Industries, Inc.**, a for-profit company.\n\nIf youâ€™d like to financially support the project, you can do so via GitHub Sponsors.\nSponsorships go directly to Zed Industries and are used as general company revenue.\nThere are no perks or entitlements associated with sponsorship.\n",
      "stars_today": 107
    },
    {
      "id": 1035029907,
      "name": "pi-mono",
      "full_name": "badlogic/pi-mono",
      "description": "AI agent toolkit: coding agent CLI, unified LLM API, TUI & web UI libraries, Slack bot, vLLM pods",
      "html_url": "https://github.com/badlogic/pi-mono",
      "stars": 1740,
      "forks": 224,
      "language": "TypeScript",
      "topics": [],
      "created_at": "2025-08-09T14:03:50Z",
      "updated_at": "2026-01-15T00:34:44Z",
      "pushed_at": "2026-01-14T22:28:23Z",
      "open_issues": 21,
      "owner": {
        "login": "badlogic",
        "avatar_url": "https://avatars.githubusercontent.com/u/514052?v=4"
      },
      "readme": "<p align=\"center\">\n  <a href=\"https://shittycodingagent.ai\">\n    <img src=\"https://shittycodingagent.ai/logo.svg\" alt=\"pi logo\" width=\"128\">\n  </a>\n</p>\n<p align=\"center\">\n  <a href=\"https://discord.com/invite/nKXTsAcmbT\"><img alt=\"Discord\" src=\"https://img.shields.io/badge/discord-community-5865F2?style=flat-square&logo=discord&logoColor=white\" /></a>\n  <a href=\"https://github.com/badlogic/pi-mono/actions/workflows/ci.yml\"><img alt=\"Build status\" src=\"https://img.shields.io/github/actions/workflow/status/badlogic/pi-mono/ci.yml?style=flat-square&branch=main\" /></a>\n</p>\n\n# Pi Monorepo\n\n> **Looking for the pi coding agent?** See **[packages/coding-agent](packages/coding-agent)** for installation and usage.\n\nTools for building AI agents and managing LLM deployments.\n\n## Packages\n\n| Package | Description |\n|---------|-------------|\n| **[@mariozechner/pi-ai](packages/ai)** | Unified multi-provider LLM API (OpenAI, Anthropic, Google, etc.) |\n| **[@mariozechner/pi-agent-core](packages/agent)** | Agent runtime with tool calling and state management |\n| **[@mariozechner/pi-coding-agent](packages/coding-agent)** | Interactive coding agent CLI |\n| **[@mariozechner/pi-mom](packages/mom)** | Slack bot that delegates messages to the pi coding agent |\n| **[@mariozechner/pi-tui](packages/tui)** | Terminal UI library with differential rendering |\n| **[@mariozechner/pi-web-ui](packages/web-ui)** | Web components for AI chat interfaces |\n| **[@mariozechner/pi-pods](packages/pods)** | CLI for managing vLLM deployments on GPU pods |\n\n## Development\n\n### Setup\n\n```bash\nnpm install          # Install all dependencies\nnpm run build        # Build all packages\nnpm run check        # Lint, format, and type check\n```\n\n> **Note:** `npm run check` requires `npm run build` to be run first. The web-ui package uses `tsc` which needs compiled `.d.ts` files from dependencies.\n\n### CI\n\nGitHub Actions runs on push to `main` and on pull requests. The workflow runs `npm run check` and `npm run test` for each package in parallel.\n\n**Do not add LLM API keys as secrets to this repository.** Tests that require LLM access use `describe.skipIf()` to skip when API keys are missing. This is intentional:\n\n- PRs from external contributors would have access to secrets in the CI environment\n- Malicious PR code could exfiltrate API keys\n- Tests that need LLM calls are skipped on CI and run locally by developers who have keys configured\n\nIf you need to run LLM-dependent tests, run them locally with your own API keys.\n\n### Development\n\nStart watch builds for all packages:\n```bash\nnpm run dev\n```\n\nThen run with tsx:\n```bash\ncd packages/coding-agent && npx tsx src/cli.ts\ncd packages/pods && npx tsx src/cli.ts\n```\n\nTo run tests that don't require an LLM endpoint:\n```bash\n./test.sh\n```\n\n### Versioning (Lockstep)\n\n**All packages MUST always have the same version number.** Use these commands to bump versions:\n\n```bash\nnpm run version:patch    # 0.7.5 -> 0.7.6\nnpm run version:minor    # 0.7.5 -> 0.8.0\nnpm run version:major    # 0.7.5 -> 1.0.0\n```\n\nThese commands:\n1. Update all package versions to the same number\n2. Update inter-package dependency versions (e.g., `pi-agent` depends on `pi-ai@^0.7.7`)\n3. Update `package-lock.json`\n\n**Never manually edit version numbers.** The lockstep system ensures consistency across the monorepo.\n\n### Publishing\n\n```bash\nnpm run release:patch    # Bug fixes\nnpm run release:minor    # New features\nnpm run release:major    # Breaking changes\n```\n\nThis handles version bump, CHANGELOG updates, commit, tag, publish, and push.\n\n**NPM Token Setup**: Requires a granular access token with \"Bypass 2FA on publish\" enabled.\n- Go to https://www.npmjs.com/settings/badlogic/tokens/\n- Create a new \"Granular Access Token\" with \"Bypass 2FA on publish\"\n- Set the token: `npm config set //registry.npmjs.org/:_authToken=YOUR_TOKEN`\n\n## License\n\nMIT",
      "stars_today": 84
    },
    {
      "id": 457124582,
      "name": "z03mmc",
      "full_name": "devbis/z03mmc",
      "description": "Xiaomi LYWSD03MMC Zigbee Firmware",
      "html_url": "https://github.com/devbis/z03mmc",
      "stars": 1367,
      "forks": 64,
      "language": "C",
      "topics": [
        "humidity-sensor",
        "lcd-display",
        "lywsd03mmc",
        "ota",
        "temperature-sensor",
        "zigbee"
      ],
      "created_at": "2022-02-08T22:27:12Z",
      "updated_at": "2026-01-14T21:02:49Z",
      "pushed_at": "2026-01-07T10:09:06Z",
      "open_issues": 73,
      "owner": {
        "login": "devbis",
        "avatar_url": "https://avatars.githubusercontent.com/u/12081127?v=4"
      },
      "readme": "# Zigbee 3.0 Firmware for original LYWSD03MMC Sensor\n\nThis repository contains the Zigbee firmware for Xiaomi LYWSD03MMC Bluetooth temperature and humidity sensor.\n\n## Overview\n\n![](./assets/device.jpg)\n\nThe LYWSD03MMC is a Bluetooth temperature and humidity sensor that can be integrated into a Zigbee network using\nthis firmware. This repository hosts the code and related resources to flash the device and make \nit compatible with Zigbee networks.\n\n## Features\n- Full-featured firmware to convert Xiaomi LYWSD03MC device with default ZCL battery, temperature and relative humidity clusters\n- Display support for known revisions\n- OTA support in firmware and binaries in ZCL format for update \n- Flashable over-the-air from custom ATC firmware https://devbis.github.io/telink-zigbee/\n- Flashable over SWS-UART interface using one of:\n\n  - https://pvvx.github.io/ATC_MiThermometer/USBCOMFlashTx.html\n  - https://github.com/devbis/z03mmc/blob/master/TLSR825xComFlasher.py\n\n## Getting Started\n\n### Prerequisites\n\n- Zigbee compatible hardware (e.g., Zigbee coordinator or gateway).\n- Necessary tools for flashing firmware to the sensor.\n\n### Prebuild firmware\n\nYou can download binaries from releases: https://github.com/devbis/z03mmc/releases\n\nYou may also want to compile it yourself, the instruction is below.\n\n## Flashing over the air (easy way)\n\n### NB: Version: 2.1.1_0159 is temporarily not supported\n\nIf OTA flasher displays this error, the only possible way to write a custom firmware (Zigbee or custom BLE) is to use USB-UART method.\nTo get more info, please visit https://github.com/atc1441/ATC_MiThermometer/issues/298\n\n\n1. Open an awesome tool from ATC_MiThermometer https://devbis.github.io/telink-zigbee/\n2. Click \"Connect\" button and find device LYWSD03MMC, wait for connection (Connected in logs)\n3. On a new device with stock firmware click \"Do Activation\" and wait some time.\n4. Next \"Select Firmware\", choose file with the transitional firmware [ATC_ota_400000](./assets/ATC_ota_40000.bin), click \"Start Flashing\". This step is required even if you already installed a custom bluetooth firmware. Not flashing this file will likely cause your device to get bricked and require flashing via USB/UART!\n5. You will see in logs \"Update done after NN seconds\"\n6. Connect to the device again (with name ATC_802190 or similar, based on mac-address). If it doesn't appear, remove and reinsert the battery and refresh the webpage with the flashing tool.\n7. Flash the latest [z03mmc.bin](https://github.com/devbis/z03mmc/releases) firmware over transitional firmware to convert it to zigbee.\n8. The device should now show up in your Zigbee bridge (If joining is enabled, of course). If it doesn't, reinsert the battery and/or short the RESET and GND contacts on the board for 5 seconds.\n\n## Flashing firmware with USB to UART\n\n### Prerequisites: \n1. TTL-USB adaptor\n2. 1k-1.8k Ohm resistor\n3. python3 with pyserial module installed\n\n\nTo flash a new firmware via an standard USB to UART adapter, simply connect the Thermometer as seen in the picture [Mi_SWS_Connection.jpg](./assets/Mi_SWS_Connection.jpg) to the USB to UART converter and run the TLSR825xComFlasher.py tool.\n\nExample: `python3 TLSR825xComFlasher.py -p COM3 wf 0 z03mmc.bin`\n\nExample: `python3 TLSR825xComFlasher.py -p /dev/ttyUSB0 wf 0 z03mmc.bin`\n\nIn case if the SWS pin is used by the firmware, try this sequence:\n1. Power off the sensor\n2. `python3 TLSR825xComFlasher.py -p <YOUR_COM_PORT> -t5000 wf 0 z03mmc.bin`\n3. Now you have 5 seconds to power on the sensor\n4. In case the chip has not started being flashed, run `python3 TLSR825xComFlasher.py -p <YOUR_COM_PORT> wf 0 z03mmc.bin` without the timeout again.\n   \n   If the flashing fails reduce baud rate down to 340000 or increase timeouts in the script.\n\n5. If you flashed the module but the screen is remaining blank, try `python3 TLSR825xComFlasher.py -p <YOUR_COM_PORT> ea` to erase all flash and then write the firmware again.\n\nThe UART flasher software uses the tool from https://github.com/pvvx/ATC_MiThermometer. Thanks to pvvx for the awesome work on this!\n\n## Compatibility list\n\n- Zigbee2mqtt: works without custom converter since 1.33.2, OTA is supported\n- ZHA: works without quirks, OTA update is supported\n- HOMEd: works, OTA update is supported\n- TuYa: it is reported device can connect to some hubs/gateways and visible with Smart Life app, values are rounded to integers, OTA is not supported\n- MiHome: does not work\n- Yandex Hub/Station: works, OTA is not available\n- deCONZ/Phoscon App: not supported (need to add the device to its database?)\n\n## Return to Bluetooth firmware\n1. You can use Zigbee OTA to flash [db15-0203-99993001-ATC_v46.zigbee](./assets/db15-0203-99993001-ATC_v46.zigbee). See [zigbee2mqtt local OTA index](https://www.zigbee2mqtt.io/guide/usage/ota_updates.html#local-ota-index-and-firmware-files) or [ZHA OTA folder](https://github.com/zigpy/zigpy/wiki/OTA-Device-Firmware-Updates)\n2. Using UART dongle, use .bin firmware you like, either original or custom from https://github.com/pvvx/ATC_MiThermometer\n\n## Zigbee OTA upgrades\n\nThe already flashed firmware supports OTA zigbee upgrade via standard flow.\nSee zigbee2mqtt, ZHA, and HOMEd documentation for details.\n\n\n## Configuring the device\n\nMigrated device is fully compatible with ZCL standards for zigbee devices. It reuses standard clusters where possible for Zigbee 3.0.\n\nAnd to fine tune display and values, additional attributes are implemented:\n\n* 0x0402 (Temperature)\n    * Attribute: 0x0010: (signed) int16: temperature calibration. A value in 0.01ÂºC offset to fix up incorrect values from sensor.\n* 0x0405 (Relative humidity)\n    * Attribute: 0x0010: (signed) int16: humidity calibration. A value in 0.01% offset to fix up incorrect values from sensor.\n* 0x0204 (Thermostat User Interface Configuration)\n    * Attribute: 0x0010: boolean: smiley. 0 - smiley is off, 1 - smiley is on (according to comfort values below).\n    * Attribute: 0x0011: boolean: display. 0 - display is off, 1 - display is on.\n    * Attribute: 0x0102: (signed) int16: comfort temperature min: A value in 0.01ÂºC to set minimum comfort temperature for happy face. The default value is 2100.\n    * Attribute: 0x0103: (signed) int16: comfort temperature max: A value in 0.01ÂºC to set maximum comfort temperature for happy face. The default value is 2600.\n    * Attribute: 0x0104: uint16: comfort temperature min: A value in 0.01% to set minimum comfort humidity for happy face. The default value is 3000.\n    * Attribute: 0x0105: uint16: comfort temperature max: A value in 0.01% to set maximum comfort humidity for happy face. The default value is 6000.\n\n### Using ÂºF\n\nYou can switch to displaying temperature in degrees Fahrenheit by \"pressing\" reset-gnd for 1 second.\nOr write 1 to 0x0204/0x0000 (Thermostat User Interface Configuration/TemperatureDisplayMode)\n\n### Building firmware\n\n#### Docker\n\n```sh\ndocker buildx build --target export --output type=local,dest=build .\n```\n\nFirmware binaries will be in `build/Release/` directory:\n- `z03mmc.bin` - firmware binary\n- `db15-0203-*.zigbee` - firmware with OTA header\n\nIf `docker buildx` is unavailable, use standard Docker with volume mount:\n```sh\ndocker build -t z03mmc .\ndocker run --rm -v $(pwd)/build:/output/Release z03mmc\n```\n\n#### Native build\n\n1. Clone TC32 toolchain according to your host OS:\n    ```sh\n    git clone https://github.com/devbis/tc32.git -b linux\n    ```\n    ```sh\n    git clone https://github.com/devbis/tc32.git -b macos\n    ```\n    ```sh\n    git clone https://github.com/devbis/tc32.git -b windows\n    ```\n\n2. Clone this repository and SDK:\n\n    ```sh\n    git clone https://github.com/devbis/z03mmc.git\n    git clone https://github.com/telink-semi/telink_zigbee_sdk.git -b V3.7.1.0 --depth 1\n\n    cd z03mmc\n    ```\n\n3. Configure and build:\n    ```sh\n    cmake -B build -DSDK_PREFIX=$(pwd)/../telink_zigbee_sdk/tl_zigbee_sdk -DTOOLCHAIN_PREFIX=$(pwd)/../tc32 -DMANUFACTURER_CODE=0x1141\n    cmake --build build --target z03mmc.zigbee\n    ```\n\n    Firmware binary is located at `build/src/z03mmc.bin`\n    The binary with OTA header is at the same folder, ending with `z03mmc.zigbee`\n\n\n\n## Related Work\nz03mmc is based on the original work of @pvvx, and @atc1441, who developed the initial firmware versions for bluetooth-capable device.\n- https://github.com/pvvx/ATC_MiThermometer\n- https://github.com/atc1441/ATC_MiThermometer\n\n## Usage\n\n1. Flash the firmware\n2. Enable pairing mode on Zigbee coordinator\n3. In case it is not joining, close the RESET and GND contacts on the board for 3 seconds to reset Zigbee settings. Replug the battery may require\n4. For zigbee2mqtt you need to add custom converter if you use version 1.33.1 or earlier\n\n## License\n\nThis project is licensed under the GNU General Public License 3.0 or later - see the [LICENSE.txt](LICENSE.txt) file for details.\n",
      "stars_today": 82
    },
    {
      "id": 1033778670,
      "name": "AionUi",
      "full_name": "iOfficeAI/AionUi",
      "description": "Free, local, open-source Cowork for Gemini CLI, Claude Code, Codex, Qwen Code, Goose Cli, Auggie, and more | ğŸŒŸ Star if you like it!",
      "html_url": "https://github.com/iOfficeAI/AionUi",
      "stars": 3591,
      "forks": 298,
      "language": "TypeScript",
      "topics": [
        "acp",
        "ai",
        "ai-agent",
        "banana",
        "chat",
        "chatbot",
        "claude-code",
        "codex",
        "cowork",
        "excel",
        "gemini",
        "gemini-cli",
        "gemini-pro",
        "gui",
        "llm",
        "multi-agent",
        "nano-banana",
        "office",
        "qwen-code",
        "webui"
      ],
      "created_at": "2025-08-07T10:29:51Z",
      "updated_at": "2026-01-15T00:54:13Z",
      "pushed_at": "2026-01-14T18:55:48Z",
      "open_issues": 19,
      "owner": {
        "login": "iOfficeAI",
        "avatar_url": "https://avatars.githubusercontent.com/u/145246968?v=4"
      },
      "readme": "<p align=\"center\">\n  <img src=\"./resources/aionui-banner-1 copy.png\" alt=\"AionUi - Cowork with Your CLI AI Agent\" width=\"100%\">\n</p>\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/github/v/release/iOfficeAI/AionUi?style=flat-square&color=32CD32\" alt=\"Version\">\n  &nbsp;\n  <img src=\"https://img.shields.io/badge/license-Apache--2.0-32CD32?style=flat-square&logo=apache&logoColor=white\" alt=\"License\">\n  &nbsp;\n  <img src=\"https://img.shields.io/badge/platform-macOS%20%7C%20Windows%20%7C%20Linux-6C757D?style=flat-square&logo=linux&logoColor=white\" alt=\"Platform\">\n</p>\n\n<p align=\"center\">\n  <a href=\"https://trendshift.io/repositories/15423\" target=\"_blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/15423\" alt=\"GitHub Trending\" height=\"80\">\n  </a>\n</p>\n\n---\n\n<p align=\"center\">\n  <strong>ğŸš€ Cowork with Your AI, Gemini CLI, Claude Code, Codex, Qwen Code, Goose CLI, Auggie, and more</strong><br>\n  <em>User-friendly | Visual graphical interface | Multi-model support | Local data security</em>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/iOfficeAI/AionUi/releases\">\n    <img src=\"https://img.shields.io/badge/â¬‡ï¸%20Download%20Now-Latest%20Release-32CD32?style=for-the-badge&logo=github&logoColor=white\" alt=\"Download Latest Release\" height=\"50\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <strong>English</strong> | <a href=\"./readme_ch.md\">ç®€ä½“ä¸­æ–‡</a> | <a href=\"./readme_jp.md\">æ—¥æœ¬èª</a> | <a href=\"https://www.aionui.com\" target=\"_blank\">Official Website</a> | <a href=\"https://twitter.com/AionUI\" target=\"_blank\">Twitter</a>\n</p>\n\n---\n\n## ğŸ“‹ Quick Navigation\n\n<p align=\"center\">\n\n[âœ¨ What Can AionUi Do?](#âœ¨-what-can-aionui-do) Â·\n[ğŸ¤” Why Choose AionUi?](#ğŸ¤”-why-choose-aionui) Â·\n[âœ¨ Core Features](#âœ¨-core-features) Â·\n[ğŸš€ Quick Start](#ğŸš€-quick-start) Â·\n[ğŸ“– Detailed Usage Guide](#ğŸ“–-detailed-usage-guide) Â·\n[ğŸ’¬ Community](#ğŸ¤-community--support)\n\n</p>\n\n---\n\n## âœ¨ What Can AionUi Do?\n\n<p align=\"center\">\n  <img src=\"./resources/offica-ai BANNER-function copy.png\" alt=\"AionUi - Cowork with Your CLI AI Agent\" width=\"800\">\n</p>\n\n### ğŸ¤– **Multi-Agent Mode - Cowork for Your Command-Line AI Tools, Unified Graphical Interface**\n\n_If you have installed command-line tools like Gemini CLI, Claude Code, CodeX, Qwen Code, Goose AI, Augment Code, AionUi can automatically detect them and provide a unified graphical interface_\n\n- âœ… **Auto Detection + Unified Interface** - Automatically recognizes local CLI tools, provides a unified graphical interface, say goodbye to command line\n- âœ… **Local Storage + Multi-Session** - Conversations saved locally, supports multiple parallel sessions, each session with independent context\n\n<p align=\"center\">\n  <img src=\"./resources/acp home page.gif\" alt=\"Multi-Agent Mode Demo\" width=\"800\">\n</p>\n\n---\n\n### ğŸ“„ **Preview Panel - Quickly View AI-Generated Results**\n\n_Supports 9+ formats of visual preview (PDF, Word, Excel, PPT, code, Markdown, images, HTML, Diff, etc.)_\n\n- âœ… **View Results Instantly** - After AI generates files, view preview immediately without switching apps\n- âœ… **Real-time Tracking + Editable** - Automatically tracks file changes, editor and preview sync intelligently; supports real-time editing of Markdown, code, HTML, WYSIWYG\n\n<p align=\"center\">\n  <img src=\"./resources/preview.gif\" alt=\"Preview Panel Demo\" width=\"800\">\n</p>\n\n---\n\n### ğŸ¨ **AI Image Generation & Editing**\n\n_Intelligent image generation, editing, and recognition, powered by Gemini_\n\n<p align=\"center\">\n  <img src=\"./resources/Image_Generation.gif\" alt=\"AI Image Generation Demo\" width=\"800\">\n</p>\n\n> ğŸ’¡ **Need help setting up free image generation?** [Follow the tutorial to configure image generation models](https://github.com/iOfficeAI/AionUi/wiki/AionUi-Image-Generation-Tool-Model-Configuration-Guide)\n\n---\n\n### ğŸ“ **Smart File Management**\n\n_Batch renaming, automatic organization, smart classification, file merging_\n\n<p align=\"center\">\n  <img src=\"https://github.com/iOfficeAI/AionUi/wiki/assets/gifs/file-management/file-organization.gif\" alt=\"File Management Demo\" width=\"800\">\n</p>\n\n---\n\n### ğŸ’¬ **Multi-Task Parallel Processing**\n\n_Open multiple conversations, tasks don't get mixed up, independent memory, double efficiency_\n\n<p align=\"center\">\n  <img src=\"./resources/multichat-side-by-side.gif\" alt=\"Conversation Management Demo\" width=\"800\">\n</p>\n\n---\n\n### ğŸŒ **Access Anywhere - WebUI Mode**\n\n_Remotely control your AI tools - Access AionUi from any device on the network! Securely control local Gemini CLI, Claude Code, Codex, and other tools, data never leaves your device_\n\n```bash\n# Basic startup\nAionUi --webui\n\n# Remote access (accessible from other devices on the local network)\nAionUi --webui --remote\n```\n\n> ğŸ’¡ **Need detailed configuration guide?** Check out the [WebUI Configuration Tutorial](https://github.com/iOfficeAI/AionUi/wiki/WebUI-Configuration-Guide) - includes complete startup commands for all platforms\n\n<p align=\"center\">\n  <img src=\"./resources/webui banner.png\" alt=\"WebUI Remote Access Demo\" width=\"800\">\n</p>\n\n---\n\n## ğŸ¤” Why Choose AionUi?\n\n**Just like Claude Cowork makes Claude Code easier to use, AionUi is the Cowork platform for all your command-line AI tools**\n\nWhile command-line tools like Gemini CLI, Claude Code, Codex, Qwen Code are powerful, they share common pain points: conversations can't be saved, single-session limitations, cumbersome file operations, and only support a single model.\n\nAionUi provides unified **Cowork capabilities** for these command-line tools:\n\n- ğŸ¯ **Unified Platform** - One interface to manage all command-line AI tools, no switching needed\n- ğŸš€ **Multi-Tool Support** - Not only supports Claude Code, but also Gemini CLI, Codex, Qwen Code, and more\n- ğŸŒ **Cross-Platform** - Full platform support for macOS, Windows, Linux (Claude Cowork currently only macOS)\n- ğŸ”„ **Multi-Model Switching** - Flexibly switch between different models in the same interface, meeting different task requirements\n- ğŸ“„ **Real-time Preview** - Visual preview for 9+ formats, immediately view the effects of AI-generated files\n- ğŸ’¾ **Local Data Security** - All conversations and files saved locally, data never leaves your device\n\n### ğŸ“Š AionUi vs Command-Line AI Tools\n\n| Feature              | AionUi                                  | Command-Line AI Tools              |\n| -------------------- | --------------------------------------- | ---------------------------------- |\n| Interface            | ğŸ¨ Graphical Interface                  | ğŸ’» Command Line                    |\n| Conversation Storage | âœ… Local Storage                        | âŒ Lost on Close                   |\n| Multi-Session        | âœ… Supported                            | âŒ Not Supported                   |\n| Context Management   | âœ… Independent Context, No Interference | âŒ Single Context, Easy to Confuse |\n| File Selection       | ğŸ–±ï¸ Click to Select                      | âŒ¨ï¸ @ Command                       |\n| Multi-Model          | âœ… Supported                            | âŒ Single Model                    |\n| File Preview         | âœ… 9+ Format Visual Preview             | âŒ No Preview                      |\n| Remote Access        | âœ… WebUI Mode                           | âŒ Not Supported                   |\n\n---\n\n### â“ Quick Q&A\n\n<details>\n<summary><strong>Q: Is AionUi ready to use out of the box?</strong></summary>\nA: Yes! After installation, you can directly use Google account login, AionUi will automatically associate with Gemini CLI, no additional configuration needed to start using.\n</details>\n\n<details>\n<summary><strong>Q: Is it free?</strong></summary>\nA: AionUi is completely free and open source, but using AI models requires corresponding API Keys.\n</details>\n\n<details>\n<summary><strong>Q: Which AI models are supported?</strong></summary>\nA: Supports mainstream models like Gemini, OpenAI, Claude, Qwen, as well as local models like Ollama, LM Studio.\n\nYou can also run multiple AI Agents simultaneously (such as Gemini CLI, Claude Code, Qwen Code, etc.), see the configuration guide for details.\n\n</details>\n\n<details>\n<summary><strong>Q: Is my data secure?</strong></summary>\nA: All conversation data is stored in a local SQLite database and will not be uploaded to any server.\n</details>\n\n---\n\n## âœ¨ Core Features\n\n### ğŸ’¬ **Multi-Session Chat**\n\n- **Multi-Session + Independent Context** - Open multiple chats simultaneously, each session has independent context memory, no confusion\n- **Local Storage** - All conversations are saved locally and will not be lost\n\n### ğŸ¤– **Multi-Model Support**\n\n- **Multi-Platform Support** - Supports mainstream models like Gemini, OpenAI, Claude, Qwen, flexible switching\n- **Local Model Support** - Supports local model deployment like Ollama, LM Studio, select Custom platform and set local API address (e.g., `http://localhost:11434/v1`) to connect\n- **Gemini 3 Subscription Optimization** - Automatically identifies subscribed users, recommends advanced models\n\n### ğŸ—‚ï¸ **File Management**\n\n- **File Tree Browsing + Drag & Drop Upload** - Browse files like folders, support drag and drop files or folders for one-click import\n- **Smart Organization** - You can let AI help organize folders, automatic classification\n\n### ğŸ“„ **Preview Panel - Give AI Agent a Display**\n\n- **9+ Format Preview** - Supports PDF, Word, Excel, PPT, code, Markdown, images, etc., view results immediately after AI generation\n- **Real-time Tracking + Editable** - Automatically tracks file changes, supports real-time editing and debugging of Markdown, code, HTML\n\n### ğŸ¨ **AI Image Generation & Editing**\n\n- **Intelligent Image Generation** - Supports multiple image generation models like Gemini 2.5 Flash Image Preview, Nano, Banana\n- **Image Recognition & Editing** - AI-driven image analysis and editing features\n\n### ğŸŒ **WebUI Remote Access**\n\n- **Cross-Device Access** - Access from any device on the network via browser, supports mobile devices\n- **Local Data Security** - All data stored locally in SQLite database, suitable for server deployment\n\n### ğŸ¨ **Personalized Interface Customization**\n\n_Customize with your own CSS code, make your interface match your preferences_\n\n<p align=\"center\">\n  <img src=\"./resources/css with skin.gif\" alt=\"CSS Custom Interface Demo\" width=\"800\">\n</p>\n\n- **Fully Customizable** - Freely customize interface colors, styles, layout through CSS code, create your exclusive experience\n\n---\n\n## ğŸ“– Detailed Usage Guide\n\n<details>\n<summary><strong>ğŸ“– Expand to View Complete Usage Guide</strong></summary>\n\n### ğŸš€ Quick Start\n\n- [ğŸ“– Complete Installation Guide](https://github.com/iOfficeAI/AionUi/wiki/Getting-Started) - Detailed steps from download to configuration\n- [âš™ï¸ LLM Configuration Guide](https://github.com/iOfficeAI/AionUi/wiki/LLM-Configuration) - Multi-platform AI model configuration\n- [ğŸ¤– Multi-Agent Mode Setup](https://github.com/iOfficeAI/AionUi/wiki/ACP-Setup) - Integrate terminal AI agents\n- [ğŸ”Œ MCP Tool Configuration](https://github.com/iOfficeAI/AionUi/wiki/MCP-Configuration-Guide) - Model Context Protocol server setup\n- [ğŸ¨ Image Generation Configuration](https://github.com/iOfficeAI/AionUi/wiki/AionUi-Image-Generation-Tool-Model-Configuration-Guide) - AI image generation setup tutorial\n- [ğŸŒ WebUI Configuration Guide](https://github.com/iOfficeAI/AionUi/wiki/WebUI-Configuration-Guide) - Complete WebUI setup and configuration tutorial\n\n### ğŸ¯ Use Cases\n\n- [ğŸ“ File Management](https://github.com/iOfficeAI/AionUi/wiki/file-management) - Smart file organization\n- [ğŸ“Š Excel Processing](https://github.com/iOfficeAI/AionUi/wiki/excel-processing) - AI-driven data processing\n- [ğŸ¨ Image Generation](https://github.com/iOfficeAI/AionUi/wiki/AionUi-Image-Generation-Tool-Model-Configuration-Guide) - AI image creation\n- [ğŸ“š More Use Cases](https://github.com/iOfficeAI/AionUi/wiki/Use-Cases-Overview)\n\n### â“ Support & Help\n\n- [â“ FAQ](https://github.com/iOfficeAI/AionUi/wiki/FAQ) - Questions and troubleshooting\n- [ğŸ”§ Configuration & Usage Tutorials](https://github.com/iOfficeAI/AionUi/wiki/Configuration-Guides) - Complete configuration documentation\n\n</details>\n\n---\n\n## ğŸš€ Quick Start\n\n### ğŸ’» System Requirements\n\n- **macOS**: 10.15 or higher\n- **Windows**: Windows 10 or higher\n- **Linux**: Ubuntu 18.04+ / Debian 10+ / Fedora 32+\n- **Memory**: Recommended 4GB or more\n- **Storage**: At least 500MB available space\n\n### ğŸ“¥ Download\n\n<p>\n  <a href=\"https://github.com/iOfficeAI/AionUi/releases\">\n    <img src=\"https://img.shields.io/badge/Download-Latest%20Release-32CD32?style=for-the-badge&logo=github&logoColor=white\" alt=\"Download Latest Release\" height=\"50\">\n  </a>\n</p>\n\n### ğŸ”§ Simple Installation\n\n1. **Download and install** AionUi application\n2. **Configure AI service** - Support Google account login or API Key authentication\n3. **Start using** - Immediately experience modern AI chat interface\n\n> ğŸ’¡ **Need detailed configuration guide?** Check out our [Complete Installation Tutorial](https://github.com/iOfficeAI/AionUi/wiki/Getting-Started)\n\n---\n\n## ğŸ¤ Community & Support\n\n### ğŸ’¬ Community\n\n**ğŸ’¡ Your ideas matter!** We highly value every user's suggestions and feedback. Whether it's feature ideas, user experience, or issues you encounter, feel free to contact us anytime!\n\n<p align=\"center\">\n  <a href=\"https://x.com/AionUi\" target=\"_blank\">\n    <img src=\"./resources/contactus-x.png\" alt=\"Contact Us on X\" width=\"600\">\n  </a>\n</p>\n\n- [ğŸ’¬ GitHub Discussions](https://github.com/iOfficeAI/AionUi/discussions) - **Share ideas, make suggestions, exchange usage tips**\n- [ğŸ› Report Issues](https://github.com/iOfficeAI/AionUi/issues) - Report bugs or feature requests\n- [ğŸ“¦ Release Updates](https://github.com/iOfficeAI/AionUi/releases) - Get the latest version\n\n### ğŸ¤ Contributing\n\nWelcome to submit Issues and Pull Requests!\n\n1. Fork this project\n2. Create a feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n---\n\n## ğŸ“„ License\n\nThis project is licensed under [Apache-2.0](LICENSE).\n\n---\n\n## ğŸ‘¥ Contributors\n\nThanks to all developers who have contributed to AionUi!\n\n<p align=\"center\">\n  <a href=\"https://github.com/iOfficeAI/AionUi/graphs/contributors\">\n    <img src=\"https://contrib.rocks/image?repo=iOfficeAI/AionUi&max=20\" alt=\"Contributors\" />\n  </a>\n</p>\n\n## ğŸ“Š Star History\n\n<p align=\"center\">\n  <a href=\"https://www.star-history.com/#iOfficeAI/aionui&Date\" target=\"_blank\">\n    <img src=\"https://api.star-history.com/svg?repos=iOfficeAI/aionui&type=Date\" alt=\"GitHub Star Trends\" width=\"600\">\n  </a>\n</p>\n\n<div align=\"center\">\n\n**â­ If you like it, give us a star**\n\n[Report Bug](https://github.com/iOfficeAI/AionUi/issues) Â· [Request Feature](https://github.com/iOfficeAI/AionUi/issues)\n\n</div>\n",
      "stars_today": 75
    },
    {
      "id": 995029641,
      "name": "claude-flow",
      "full_name": "ruvnet/claude-flow",
      "description": "ğŸŒŠ The leading agent orchestration platform for Claude. Deploy intelligent multi-agent swarms, coordinate autonomous workflows, and build conversational AI systems. Features    enterprise-grade architecture, distributed swarm intelligence, RAG integration, and native Claude Code support via MCP protocol. Ranked #1 in agent-based frameworks.",
      "html_url": "https://github.com/ruvnet/claude-flow",
      "stars": 11975,
      "forks": 1505,
      "language": "JavaScript",
      "topics": [
        "agentic-ai",
        "agentic-engineering",
        "agentic-framework",
        "agentic-rag",
        "agentic-workflow",
        "ai-assistant",
        "ai-tools",
        "anthropic-claude",
        "autonomous-agents",
        "claude-code",
        "codex",
        "huggingface",
        "jules",
        "mcp-server",
        "model-context-protocol",
        "multi-agent",
        "multi-agent-systems",
        "npx",
        "swarm",
        "swarm-intelligence"
      ],
      "created_at": "2025-06-02T21:24:20Z",
      "updated_at": "2026-01-15T00:52:50Z",
      "pushed_at": "2026-01-14T22:35:47Z",
      "open_issues": 335,
      "owner": {
        "login": "ruvnet",
        "avatar_url": "https://avatars.githubusercontent.com/u/2934394?v=4"
      },
      "readme": "# ğŸŒŠ Claude-Flow v2.7.0: Enterprise AI Orchestration Platform\n\n<div align=\"center\">\n\n[![ğŸŒŸ Star on GitHub](https://img.shields.io/github/stars/ruvnet/claude-flow?style=for-the-badge&logo=github&color=gold)](https://github.com/ruvnet/claude-flow)\n[![ğŸ“ˆ Downloads](https://img.shields.io/npm/dt/claude-flow?style=for-the-badge&logo=npm&color=blue&label=Downloads)](https://www.npmjs.com/package/claude-flow)\n[![ğŸ“¦ Latest Release](https://img.shields.io/npm/v/claude-flow/alpha?style=for-the-badge&logo=npm&color=green&label=v2.7.0-alpha.10)](https://www.npmjs.com/package/claude-flow)\n[![âš¡ Claude Code](https://img.shields.io/badge/Claude%20Code-SDK%20Integrated-green?style=for-the-badge&logo=anthropic)](https://github.com/ruvnet/claude-flow)\n[![ğŸ›ï¸ Agentics Foundation](https://img.shields.io/badge/Agentics-Foundation-crimson?style=for-the-badge&logo=openai)](https://discord.com/invite/dfxmpwkG2D)\n[![ğŸ›¡ï¸ MIT License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge&logo=opensourceinitiative)](https://opensource.org/licenses/MIT)\n\n</div>\n\n## ğŸŒŸ **Overview**\n\n**Claude-Flow v2.7** is an enterprise-grade AI orchestration platform that combines **hive-mind swarm intelligence**, **persistent memory**, and **100+ advanced MCP tools** to revolutionize AI-powered development workflows.\n\n### ğŸ¯ **Key Features**\n\n- **ğŸ¨ 25 Claude Skills**: Natural language-activated skills for development, GitHub, memory, and automation\n- **ğŸš€ AgentDB v1.3.9 Integration**: 96x-164x faster vector search with semantic understanding (PR #830)\n- **ğŸ§  Hybrid Memory System**: AgentDB + ReasoningBank with automatic fallback\n- **ğŸ” Semantic Vector Search**: HNSW indexing (O(log n)) + 9 RL algorithms\n- **ğŸ Hive-Mind Intelligence**: Queen-led AI coordination with specialized worker agents\n- **ğŸ”§ 100 MCP Tools**: Comprehensive toolkit for swarm orchestration and automation\n- **ğŸ”„ Dynamic Agent Architecture (DAA)**: Self-organizing agents with fault tolerance\n- **ğŸ’¾ Persistent Memory**: 150x faster search, 4-32x memory reduction (quantization)\n- **ğŸª Advanced Hooks System**: Automated workflows with pre/post operation hooks\n- **ğŸ“Š GitHub Integration**: 6 specialized modes for repository management\n- **ğŸŒ Flow Nexus Cloud**: E2B sandboxes, AI swarms, challenges, and marketplace\n\n> ğŸ”¥ **Revolutionary AI Coordination**: Build faster, smarter, and more efficiently with AI-powered development orchestration\n>\n> ğŸ†• **NEW: AgentDB Integration**: 96x-164x performance boost with semantic vector search, reflexion memory, and skill library auto-consolidation\n\n\n---\n\n## âš¡ **Quick Start**\n\n### ğŸ“‹ **Prerequisites**\n\n- **Node.js 18+** (LTS recommended)\n- **npm 9+** or equivalent package manager\n- **Windows users**: See [Windows Installation Guide](./docs/windows-installation.md) for special instructions\n\nâš ï¸ **IMPORTANT**: Claude Code must be installed first:\n\n```bash\n# 1. Install Claude Code globally\nnpm install -g @anthropic-ai/claude-code\n\n# 2. (Optional) Skip permissions check for faster setup\nclaude --dangerously-skip-permissions\n```\n\n### ğŸš€ **Install Latest Alpha**\n\n```bash\n# NPX (recommended - always latest)\nnpx claude-flow@alpha init --force\nnpx claude-flow@alpha --help\n\n# Or install globally\nnpm install -g claude-flow@alpha\nclaude-flow --version\n# v2.7.0-alpha.10\n```\n\n---\n\n## ğŸ¨ **Skills System**\n\nClaude-Flow includes **25 specialized skills** that activate automatically via natural language - no commands to memorize:\n\n```bash\n# Just describe what you want - skills activate automatically\n\"Let's pair program on this feature\"        â†’ pair-programming skill\n\"Review this PR for security issues\"       â†’ github-code-review skill\n\"Use vector search to find similar code\"   â†’ agentdb-vector-search skill\n\"Create a swarm to build this API\"         â†’ swarm-orchestration skill\n```\n\n**Skill Categories:**\n- **Development & Methodology** (3) - SPARC, pair programming, skill builder\n- **Intelligence & Memory** (6) - AgentDB integration with 150x-12,500x performance\n- **Swarm Coordination** (3) - Multi-agent orchestration and hive-mind\n- **GitHub Integration** (5) - PR review, workflows, releases, multi-repo\n- **Automation & Quality** (4) - Hooks, verification, performance analysis\n- **Flow Nexus Platform** (3) - Cloud sandboxes and neural training\n\nğŸ“š **[Complete Skills Tutorial](./docs/skills-tutorial.md)** - Full guide with usage examples\n\n---\n\n## ğŸ†• **What's New in v2.7.0-alpha.10**\n\n### âœ… **Semantic Search Fixed**\nCritical bug fix for semantic search returning 0 results:\n- âœ… Fixed stale compiled code (dist-cjs/ now uses Node.js backend)\n- âœ… Fixed result mapping for `retrieveMemories()` flat structure\n- âœ… Fixed parameter mismatch (namespace vs domain)\n- âœ… 2-3ms query latency with hash embeddings\n- âœ… Works without API keys (deterministic 1024-dim embeddings)\n\n### ğŸ§  **ReasoningBank Integration (agentic-flow@1.5.13)**\n- **Node.js Backend**: Replaced WASM with SQLite + better-sqlite3\n- **Persistent Storage**: All memories saved to `.swarm/memory.db`\n- **Semantic Search**: MMR ranking with 4-factor scoring\n- **Database Tables**: patterns, embeddings, trajectories, links\n- **Performance**: 2ms queries, 400KB per pattern with embeddings\n\n```bash\n# Semantic search now fully functional\nnpx claude-flow@alpha memory store test \"API configuration\" --namespace semantic --reasoningbank\nnpx claude-flow@alpha memory query \"configuration\" --namespace semantic --reasoningbank\n# âœ… Found 3 results (semantic search) in 2ms\n```\n\nğŸ“š **Release Notes**: [v2.7.0-alpha.10](./docs/RELEASE-NOTES-v2.7.0-alpha.10.md)\n\n## ğŸ§  **Memory System Commands**\n\n### **ğŸš€ NEW: AgentDB v1.3.9 Integration (96x-164x Performance Boost)**\n\n**Revolutionary Performance Improvements:**\n- **Vector Search**: 96x faster (9.6ms â†’ <0.1ms)\n- **Batch Operations**: 125x faster\n- **Large Queries**: 164x faster\n- **Memory Usage**: 4-32x reduction via quantization\n\n```bash\n# Semantic vector search (understands meaning, not just keywords)\nnpx claude-flow@alpha memory vector-search \"user authentication flow\" \\\n  --k 10 --threshold 0.7 --namespace backend\n\n# Store with vector embedding for semantic search\nnpx claude-flow@alpha memory store-vector api_design \"REST endpoints\" \\\n  --namespace backend --metadata '{\"version\":\"v2\"}'\n\n# Get AgentDB integration status and capabilities\nnpx claude-flow@alpha memory agentdb-info\n\n# Installation (hybrid mode - 100% backward compatible)\nnpm install agentdb@1.3.9\n```\n\n**New Features:**\n- âœ… **Semantic vector search** (HNSW indexing, O(log n))\n- âœ… **9 RL algorithms** (Q-Learning, PPO, MCTS, Decision Transformer)\n- âœ… **Reflexion memory** (learn from past experiences)\n- âœ… **Skill library** (auto-consolidate successful patterns)\n- âœ… **Causal reasoning** (understand cause-effect relationships)\n- âœ… **Quantization** (binary 32x, scalar 4x, product 8-16x reduction)\n- âœ… **100% backward compatible** (hybrid mode with graceful fallback)\n\n**Documentation**: `docs/agentdb/PRODUCTION_READINESS.md` | **PR**: #830\n\n---\n\n### **ReasoningBank (Legacy SQLite Memory - Still Supported)**\n\n```bash\n# Store memories with pattern matching\nnpx claude-flow@alpha memory store api_key \"REST API configuration\" \\\n  --namespace backend --reasoningbank\n\n# Query with pattern search (2-3ms latency)\nnpx claude-flow@alpha memory query \"API config\" \\\n  --namespace backend --reasoningbank\n# âœ… Found 3 results (pattern matching)\n\n# List all memories\nnpx claude-flow@alpha memory list --namespace backend --reasoningbank\n\n# Check status and statistics\nnpx claude-flow@alpha memory status --reasoningbank\n# âœ… Total memories: 30\n#    Embeddings: 30\n#    Storage: .swarm/memory.db\n```\n\n**Features:**\n- âœ… **No API Keys Required**: Hash-based embeddings (1024 dimensions)\n- âœ… **Persistent Storage**: SQLite database survives restarts\n- âœ… **Pattern Matching**: LIKE-based search with similarity scoring\n- âœ… **Namespace Isolation**: Organize memories by domain\n- âœ… **Fast Queries**: 2-3ms average latency\n- âœ… **Process Cleanup**: Automatic database closing\n\n**Optional Enhanced Embeddings:**\n```bash\n# For better semantic accuracy with text-embedding-3-small (1536 dimensions)\n# Set OPENAI environment variable (see ReasoningBank documentation)\n```\n\n---\n\n## ğŸ **Swarm Orchestration**\n\n### **Quick Swarm Commands**\n\n```bash\n# Quick task execution (recommended)\nnpx claude-flow@alpha swarm \"build REST API with authentication\" --claude\n\n# Multi-agent coordination\nnpx claude-flow@alpha swarm init --topology mesh --max-agents 5\nnpx claude-flow@alpha swarm spawn researcher \"analyze API patterns\"\nnpx claude-flow@alpha swarm spawn coder \"implement endpoints\"\nnpx claude-flow@alpha swarm status\n```\n\n### **Hive-Mind for Complex Projects**\n\n```bash\n# Initialize hive-mind system\nnpx claude-flow@alpha hive-mind wizard\nnpx claude-flow@alpha hive-mind spawn \"build enterprise system\" --claude\n\n# Session management\nnpx claude-flow@alpha hive-mind status\nnpx claude-flow@alpha hive-mind resume session-xxxxx\n```\n\n**When to Use:**\n| Feature | `swarm` | `hive-mind` |\n|---------|---------|-------------|\n| **Best For** | Quick tasks | Complex projects |\n| **Setup** | Instant | Interactive wizard |\n| **Memory** | Task-scoped | Project-wide SQLite |\n| **Sessions** | Temporary | Persistent + resume |\n\n---\n\n## ğŸ”§ **MCP Tools Integration**\n\n### **Setup MCP Servers**\n\n```bash\n# Add Claude Flow MCP server (required)\nclaude mcp add claude-flow npx claude-flow@alpha mcp start\n\n# Optional: Enhanced coordination\nclaude mcp add ruv-swarm npx ruv-swarm mcp start\n\n# Optional: Cloud features (requires registration)\nclaude mcp add flow-nexus npx flow-nexus@latest mcp start\n```\n\n### **Available MCP Tools (100 Total)**\n\n**Core Tools:**\n- `swarm_init`, `agent_spawn`, `task_orchestrate`\n- `memory_usage`, `memory_search`\n- `neural_status`, `neural_train`, `neural_patterns`\n\n**Memory Tools:**\n- `mcp__claude-flow__memory_usage` - Store/retrieve persistent memory\n- `mcp__claude-flow__memory_search` - Pattern-based search\n\n**GitHub Tools:**\n- `github_repo_analyze`, `github_pr_manage`, `github_issue_track`\n\n**Performance Tools:**\n- `benchmark_run`, `performance_report`, `bottleneck_analyze`\n\nğŸ“š **Full Reference**: [MCP Tools Documentation](./docs/MCP-TOOLS.md)\n\n---\n\n## ğŸª **Advanced Hooks System**\n\n### **Automated Workflow Enhancement**\n\nClaude-Flow automatically configures hooks for enhanced operations:\n\n```bash\n# Auto-configures hooks during init\nnpx claude-flow@alpha init --force\n```\n\n### **Available Hooks**\n\n**Pre-Operation:**\n- `pre-task`: Auto-assigns agents by complexity\n- `pre-edit`: Validates files and prepares resources\n- `pre-command`: Security validation\n\n**Post-Operation:**\n- `post-edit`: Auto-formats code\n- `post-task`: Trains neural patterns\n- `post-command`: Updates memory\n\n**Session Management:**\n- `session-start`: Restores previous context\n- `session-end`: Generates summaries\n- `session-restore`: Loads memory\n\n---\n\n## ğŸ¯ **Common Workflows**\n\n### **Pattern 1: Single Feature Development**\n```bash\n# Initialize once per feature\nnpx claude-flow@alpha init --force\nnpx claude-flow@alpha hive-mind spawn \"Implement authentication\" --claude\n\n# Continue same feature (reuse hive)\nnpx claude-flow@alpha memory query \"auth\" --recent\nnpx claude-flow@alpha swarm \"Add password reset\" --continue-session\n```\n\n### **Pattern 2: Multi-Feature Project**\n```bash\n# Project initialization\nnpx claude-flow@alpha init --force --project-name \"my-app\"\n\n# Feature 1: Authentication\nnpx claude-flow@alpha hive-mind spawn \"auth-system\" --namespace auth --claude\n\n# Feature 2: User management\nnpx claude-flow@alpha hive-mind spawn \"user-mgmt\" --namespace users --claude\n```\n\n### **Pattern 3: Research & Analysis**\n```bash\n# Start research session\nnpx claude-flow@alpha hive-mind spawn \"Research microservices\" \\\n  --agents researcher,analyst --claude\n\n# Check learned knowledge\nnpx claude-flow@alpha memory stats\nnpx claude-flow@alpha memory query \"microservices patterns\" --reasoningbank\n```\n\n---\n\n## ğŸ“Š **Performance & Stats**\n\n- **84.8% SWE-Bench solve rate** - Industry-leading problem-solving\n- **32.3% token reduction** - Efficient context management\n- **2.8-4.4x speed improvement** - Parallel coordination\n- **96x-164x faster search** - ğŸ†• AgentDB vector search (9.6ms â†’ <0.1ms)\n- **4-32x memory reduction** - ğŸ†• AgentDB quantization\n- **2-3ms query latency** - ReasoningBank pattern search (legacy)\n- **64 specialized agents** - Complete development ecosystem\n- **100 MCP tools** - Comprehensive automation toolkit\n- **180 AgentDB tests** - >90% coverage, production-ready\n\n---\n\n## ğŸ“š **Documentation**\n\n### **ğŸ“– Core Documentation**\n- **[Documentation Hub](./docs/)** - Complete documentation index with organized structure\n- **[Skills Tutorial](./docs/guides/skills-tutorial.md)** - Complete guide to 25 Claude Flow skills with natural language invocation\n- **[Installation Guide](./docs/INSTALLATION.md)** - Setup instructions\n- **[Memory System Guide](./docs/MEMORY-SYSTEM.md)** - ReasoningBank + AgentDB hybrid\n- **[MCP Tools Reference](./docs/MCP-TOOLS.md)** - Complete tool catalog\n- **[Agent System](./docs/AGENT-SYSTEM.md)** - All 64 agents\n\n### **ğŸš€ Release Notes & Changelogs**\n- **[v2.7.1](./docs/releases/v2.7.1/)** - Current stable release with critical fixes\n- **[v2.7.0-alpha.10](./docs/releases/v2.7.0-alpha.10/)** - Semantic search fix\n- **[v2.7.0-alpha.9](./docs/releases/v2.7.0-alpha.9/)** - Process cleanup\n- **[Changelog](./CHANGELOG.md)** - Full version history\n\n### **ğŸ§  AgentDB Integration (96x-164x Performance Boost)**\n- **[AgentDB Documentation](./docs/agentdb/)** - ğŸ†• Complete AgentDB v1.3.9 integration docs\n  - [Production Readiness Guide](./docs/agentdb/PRODUCTION_READINESS.md) - Deployment guide\n  - [Implementation Complete](./docs/agentdb/SWARM_IMPLEMENTATION_COMPLETE.md) - 3-agent swarm details (180 tests)\n  - [Backward Compatibility](./docs/agentdb/BACKWARD_COMPATIBILITY_GUARANTEE.md) - 100% compatibility guarantee\n  - [Integration Plan](./docs/agentdb/AGENTDB_INTEGRATION_PLAN.md) - Planning and design\n  - [Optimization Report](./docs/agentdb/OPTIMIZATION_REPORT.md) - Performance analysis\n\n### **âš¡ Performance & Quality**\n- **[Performance Documentation](./docs/performance/)** - Optimization guides and benchmarks\n  - [JSON Improvements](./docs/performance/PERFORMANCE-JSON-IMPROVEMENTS.md) - JSON optimization results\n  - [Metrics Guide](./docs/performance/PERFORMANCE-METRICS-GUIDE.md) - Performance tracking\n- **[Bug Fixes](./docs/fixes/)** - Bug fix documentation and patches\n- **[Validation Reports](./docs/validation/)** - Test reports and verification results\n\n### **ğŸ› ï¸ Advanced Topics**\n- **[Neural Module](./docs/NEURAL-MODULE.md)** - SAFLA self-learning\n- **[Goal Module](./docs/GOAL-MODULE.md)** - GOAP intelligent planning\n- **[Hive-Mind Intelligence](./docs/HIVE-MIND.md)** - Queen-led coordination\n- **[GitHub Integration](./docs/GITHUB-INTEGRATION.md)** - Repository automation\n\n### **âš™ï¸ Configuration & Setup**\n- **[CLAUDE.md Templates](./docs/CLAUDE-MD-TEMPLATES.md)** - Project configs\n- **[SPARC Methodology](./docs/SPARC.md)** - TDD patterns\n- **[Windows Installation](./docs/windows-installation.md)** - Windows setup\n\n---\n\n## ğŸ¤ **Community & Support**\n\n- **GitHub Issues**: [Report bugs or request features](https://github.com/ruvnet/claude-flow/issues)\n- **Discord**: [Join the Agentics Foundation community](https://discord.com/invite/dfxmpwkG2D)\n- **Documentation**: [Complete guides and tutorials](https://github.com/ruvnet/claude-flow/wiki)\n- **Examples**: [Real-world usage patterns](https://github.com/ruvnet/claude-flow/tree/main/examples)\n\n---\n\n## ğŸš€ **Roadmap & Targets**\n\n### **Immediate (Q4 2025)**\n- âœ… Semantic search fix (v2.7.0-alpha.10)\n- âœ… ReasoningBank Node.js backend\n- âœ… AgentDB v1.3.9 integration (PR #830) - 96x-164x performance boost\n- ğŸ”„ AgentDB production deployment (Q4 2025)\n- ğŸ”„ Enhanced embedding models\n- ğŸ”„ Multi-user collaboration features\n\n### **Q1 2026**\n- Advanced neural pattern recognition\n- Cloud swarm coordination\n- Real-time agent communication\n- Enterprise SSO integration\n\n### **Growth Targets**\n- 5K+ GitHub stars, 50K npm downloads/month\n- $25K MRR, 15 enterprise customers\n- 90%+ error prevention\n- 30+ minutes saved per developer per week\n\n---\n\n## Star History\n\n<a href=\"https://www.star-history.com/#ruvnet/claude-flow&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=ruvnet/claude-flow&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=ruvnet/claude-flow&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=ruvnet/claude-flow&type=Date\" />\n </picture>\n</a>\n\n---\n\n## ğŸ“„ **License**\n\nMIT License - see [LICENSE](./LICENSE) for details\n\n---\n\n**Built with â¤ï¸ by [rUv](https://github.com/ruvnet) | Powered by Revolutionary AI**\n\n*v2.7.0-alpha.10 - Semantic Search Fixed + ReasoningBank Node.js Backend*\n\n</div>\n",
      "stars_today": 67
    },
    {
      "id": 615869301,
      "name": "LocalAI",
      "full_name": "mudler/LocalAI",
      "description": ":robot: The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI,  running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more. Features: Generate Text, MCP, Audio, Video, Images, Voice Cloning, Distributed, P2P and decentralized inference",
      "html_url": "https://github.com/mudler/LocalAI",
      "stars": 41563,
      "forks": 3404,
      "language": "Go",
      "topics": [
        "ai",
        "api",
        "audio-generation",
        "decentralized",
        "distributed",
        "gemma",
        "image-generation",
        "libp2p",
        "llama",
        "llm",
        "mamba",
        "mcp",
        "mistral",
        "musicgen",
        "object-detection",
        "rerank",
        "rwkv",
        "stable-diffusion",
        "text-generation",
        "tts"
      ],
      "created_at": "2023-03-18T22:58:02Z",
      "updated_at": "2026-01-15T01:01:34Z",
      "pushed_at": "2026-01-14T22:42:02Z",
      "open_issues": 161,
      "owner": {
        "login": "mudler",
        "avatar_url": "https://avatars.githubusercontent.com/u/2420543?v=4"
      },
      "readme": "<h1 align=\"center\">\n  <br>\n  <img width=\"300\" src=\"./core/http/static/logo.png\"> <br>\n<br>\n</h1>\n\n<p align=\"center\">\n<a href=\"https://github.com/go-skynet/LocalAI/fork\" target=\"blank\">\n<img src=\"https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge\" alt=\"LocalAI forks\"/>\n</a>\n<a href=\"https://github.com/go-skynet/LocalAI/stargazers\" target=\"blank\">\n<img src=\"https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge\" alt=\"LocalAI stars\"/>\n</a>\n<a href=\"https://github.com/go-skynet/LocalAI/pulls\" target=\"blank\">\n<img src=\"https://img.shields.io/github/issues-pr/go-skynet/LocalAI?style=for-the-badge\" alt=\"LocalAI pull-requests\"/>\n</a>\n<a href='https://github.com/go-skynet/LocalAI/releases'>\n<img src='https://img.shields.io/github/release/go-skynet/LocalAI?&label=Latest&style=for-the-badge'>\n</a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://hub.docker.com/r/localai/localai\" target=\"blank\">\n<img src=\"https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker\" alt=\"LocalAI Docker hub\"/>\n</a>\n<a href=\"https://quay.io/repository/go-skynet/local-ai?tab=tags&tag=latest\" target=\"blank\">\n<img src=\"https://img.shields.io/badge/quay.io-images-important.svg?\" alt=\"LocalAI Quay.io\"/>\n</a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://twitter.com/LocalAI_API\" target=\"blank\">\n<img src=\"https://img.shields.io/badge/X-%23000000.svg?style=for-the-badge&logo=X&logoColor=white&label=LocalAI_API\" alt=\"Follow LocalAI_API\"/>\n</a>\n<a href=\"https://discord.gg/uJAeKSAGDy\" target=\"blank\">\n<img src=\"https://img.shields.io/badge/dynamic/json?color=blue&label=Discord&style=for-the-badge&query=approximate_member_count&url=https%3A%2F%2Fdiscordapp.com%2Fapi%2Finvites%2FuJAeKSAGDy%3Fwith_counts%3Dtrue&logo=discord\" alt=\"Join LocalAI Discord Community\"/>\n</a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/5539\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/5539\" alt=\"mudler%2FLocalAI | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n\n> :bulb: Get help - [â“FAQ](https://localai.io/faq/) [ğŸ’­Discussions](https://github.com/go-skynet/LocalAI/discussions) [:speech_balloon: Discord](https://discord.gg/uJAeKSAGDy) [:book: Documentation website](https://localai.io/)\n>\n> [ğŸ’» Quickstart](https://localai.io/basics/getting_started/) [ğŸ–¼ï¸ Models](https://models.localai.io/) [ğŸš€ Roadmap](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap) [ğŸ›« Examples](https://github.com/mudler/LocalAI-examples) Try on \n[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&logo=telegram&logoColor=white)](https://t.me/localaiofficial_bot)\n\n[![tests](https://github.com/go-skynet/LocalAI/actions/workflows/test.yml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/test.yml)[![Build and Release](https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml)[![build container images](https://github.com/go-skynet/LocalAI/actions/workflows/image.yml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/image.yml)[![Bump dependencies](https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml)[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/localai)](https://artifacthub.io/packages/search?repo=localai)\n\n**LocalAI** is the free, Open Source OpenAI alternative. LocalAI act as a drop-in replacement REST API that's compatible with OpenAI (Elevenlabs, Anthropic... ) API specifications for local AI inferencing. It allows you to run LLMs, generate images, audio (and not only) locally or on-prem with consumer grade hardware, supporting multiple model families. Does not require GPU. It is created and maintained by [Ettore Di Giacinto](https://github.com/mudler).\n\n\n## ğŸ“šğŸ†• Local Stack Family\n\nğŸ†• LocalAI is now part of a comprehensive suite of AI tools designed to work together:\n\n<table>\n  <tr>\n    <td width=\"50%\" valign=\"top\">\n      <a href=\"https://github.com/mudler/LocalAGI\">\n        <img src=\"https://raw.githubusercontent.com/mudler/LocalAGI/refs/heads/main/webui/react-ui/public/logo_2.png\" width=\"300\" alt=\"LocalAGI Logo\">\n      </a>\n    </td>\n    <td width=\"50%\" valign=\"top\">\n      <h3><a href=\"https://github.com/mudler/LocalAGI\">LocalAGI</a></h3>\n      <p>A powerful Local AI agent management platform that serves as a drop-in replacement for OpenAI's Responses API, enhanced with advanced agentic capabilities.</p>\n    </td>\n  </tr>\n  <tr>\n    <td width=\"50%\" valign=\"top\">\n      <a href=\"https://github.com/mudler/LocalRecall\">\n        <img src=\"https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png\" width=\"300\" alt=\"LocalRecall Logo\">\n      </a>\n    </td>\n    <td width=\"50%\" valign=\"top\">\n      <h3><a href=\"https://github.com/mudler/LocalRecall\">LocalRecall</a></h3>\n      <p>A REST-ful API and knowledge base management system that provides persistent memory and storage capabilities for AI agents.</p>\n    </td>\n  </tr>\n</table>\n\n## Screenshots / Video\n\n### Youtube video\n\n<h1 align=\"center\">\n  <br>\n  <a href=\"https://www.youtube.com/watch?v=PDqYhB9nNHA\" target=\"_blank\"> <img width=\"300\" src=\"https://img.youtube.com/vi/PDqYhB9nNHA/0.jpg\"> </a><br>\n<br>\n</h1>\n\n\n### Screenshots\n\n| Talk Interface | Generate Audio |\n| --- | --- |\n| ![Screenshot 2025-03-31 at 12-01-36 LocalAI - Talk](./docs/assets/images/screenshots/screenshot_tts.png) | ![Screenshot 2025-03-31 at 12-01-29 LocalAI - Generate audio with voice-en-us-ryan-low](./docs/assets/images/screenshots/screenshot_tts.png) |\n\n| Models Overview | Generate Images |\n| --- | --- |\n| ![Screenshot 2025-03-31 at 12-01-20 LocalAI - Models](./docs/assets/images/screenshots/screenshot_gallery.png) | ![Screenshot 2025-03-31 at 12-31-41 LocalAI - Generate images with flux 1-dev](./docs/assets/images/screenshots/screenshot_image.png) |\n\n| Chat Interface | Home |\n| --- | --- |\n| ![Screenshot 2025-03-31 at 11-57-44 LocalAI - Chat with localai-functioncall-qwen2 5-7b-v0 5](./docs/assets/images/screenshots/screenshot_chat.png) | ![Screenshot 2025-03-31 at 11-57-23 LocalAI API - c2a39e3 (c2a39e3639227cfd94ffffe9f5691239acc275a8)](./docs/assets/images/screenshots/screenshot_home.png) |\n\n| Login | Swarm |\n| --- | --- |\n|![Screenshot 2025-03-31 at 12-09-59 ](./docs/assets/images/screenshots/screenshot_login.png) | ![Screenshot 2025-03-31 at 12-10-39 LocalAI - P2P dashboard](./docs/assets/images/screenshots/screenshot_p2p.png) |\n\n## ğŸ’» Quickstart\n\n> âš ï¸ **Note:** The `install.sh` script is currently experiencing issues due to the heavy changes currently undergoing in LocalAI and may produce broken or misconfigured installations. Please use Docker installation (see below) or manual binary installation until [issue #8032](https://github.com/mudler/LocalAI/issues/8032) is resolved.\n\nRun the installer script:\n\n```bash\n# Basic installation\ncurl https://localai.io/install.sh | sh\n```\n\nFor more installation options, see [Installer Options](https://localai.io/installation/).\n\n### macOS Download:\n\n<a href=\"https://github.com/mudler/LocalAI/releases/latest/download/LocalAI.dmg\">\n  <img src=\"https://img.shields.io/badge/Download-macOS-blue?style=for-the-badge&logo=apple&logoColor=white\" alt=\"Download LocalAI for macOS\"/>\n</a>\n\n> Note: the DMGs are not signed by Apple as quarantined. See https://github.com/mudler/LocalAI/issues/6268 for a workaround, fix is tracked here: https://github.com/mudler/LocalAI/issues/6244\n\n### Containers (Docker, podman, ...)\n\n> **ğŸ’¡ Docker Run vs Docker Start**\n> \n> - `docker run` creates and starts a new container. If a container with the same name already exists, this command will fail.\n> - `docker start` starts an existing container that was previously created with `docker run`.\n> \n> If you've already run LocalAI before and want to start it again, use: `docker start -i local-ai`\n\n#### CPU only image:\n\n```bash\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest\n```\n\n#### NVIDIA GPU Images:\n\n```bash\n# CUDA 13.0\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-13\n\n# CUDA 12.0\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12\n\n# NVIDIA Jetson (L4T) ARM64\n# CUDA 12 (for Nvidia AGX Orin and similar platforms)\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-nvidia-l4t-arm64\n\n# CUDA 13 (for Nvidia DGX Spark)\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-nvidia-l4t-arm64-cuda-13\n```\n\n#### AMD GPU Images (ROCm):\n\n```bash\ndocker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-gpu-hipblas\n```\n\n#### Intel GPU Images (oneAPI):\n\n```bash\ndocker run -ti --name local-ai -p 8080:8080 --device=/dev/dri/card1 --device=/dev/dri/renderD128 localai/localai:latest-gpu-intel\n```\n\n#### Vulkan GPU Images:\n\n```bash\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest-gpu-vulkan\n```\n\n#### AIO Images (pre-downloaded models):\n\n```bash\n# CPU version\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu\n\n# NVIDIA CUDA 13 version\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-13\n\n# NVIDIA CUDA 12 version\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-12\n\n# Intel GPU version\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-gpu-intel\n\n# AMD GPU version\ndocker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-aio-gpu-hipblas\n```\n\nFor more information about the AIO images and pre-downloaded models, see [Container Documentation](https://localai.io/basics/container/).\n\nTo load models:\n\n```bash\n# From the model gallery (see available models with `local-ai models list`, in the WebUI from the model tab, or visiting https://models.localai.io)\nlocal-ai run llama-3.2-1b-instruct:q4_k_m\n# Start LocalAI with the phi-2 model directly from huggingface\nlocal-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf\n# Install and run a model from the Ollama OCI registry\nlocal-ai run ollama://gemma:2b\n# Run a model from a configuration file\nlocal-ai run https://gist.githubusercontent.com/.../phi-2.yaml\n# Install and run a model from a standard OCI registry (e.g., Docker Hub)\nlocal-ai run oci://localai/phi-2:latest\n```\n\n> âš¡ **Automatic Backend Detection**: When you install models from the gallery or YAML files, LocalAI automatically detects your system's GPU capabilities (NVIDIA, AMD, Intel) and downloads the appropriate backend. For advanced configuration options, see [GPU Acceleration](https://localai.io/features/gpu-acceleration/#automatic-backend-detection).\n\nFor more information, see [ğŸ’» Getting started](https://localai.io/basics/getting_started/index.html), if you are interested in our roadmap items and future enhancements, you can see the [Issues labeled as Roadmap here](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap)\n\n## ğŸ“° Latest project news\n\n- December 2025: [Dynamic Memory Resource reclaimer](https://github.com/mudler/LocalAI/pull/7583), [Automatic fitting of models to multiple GPUS(llama.cpp)](https://github.com/mudler/LocalAI/pull/7584), [Added Vibevoice backend](https://github.com/mudler/LocalAI/pull/7494)\n- November 2025: Major improvements to the UX. Among these: [Import models via URL](https://github.com/mudler/LocalAI/pull/7245) and [Multiple chats and history](https://github.com/mudler/LocalAI/pull/7325)\n- October 2025: ğŸ”Œ [Model Context Protocol (MCP)](https://localai.io/docs/features/mcp/) support added for agentic capabilities with external tools\n- September 2025: New Launcher application for MacOS and Linux, extended support to many backends for Mac and Nvidia L4T devices. Models: Added MLX-Audio, WAN 2.2. WebUI improvements and Python-based backends now ships portable python environments.\n- August 2025: MLX, MLX-VLM, Diffusers and llama.cpp are now supported on Mac M1/M2/M3+ chips ( with `development` suffix in the gallery ): https://github.com/mudler/LocalAI/pull/6049 https://github.com/mudler/LocalAI/pull/6119 https://github.com/mudler/LocalAI/pull/6121 https://github.com/mudler/LocalAI/pull/6060\n- July/August 2025: ğŸ” [Object Detection](https://localai.io/features/object-detection/) added to the API featuring [rf-detr](https://github.com/roboflow/rf-detr)\n- July 2025: All backends migrated outside of the main binary. LocalAI is now more lightweight, small, and automatically downloads the required backend to run the model. [Read the release notes](https://github.com/mudler/LocalAI/releases/tag/v3.2.0)\n- June 2025: [Backend management](https://github.com/mudler/LocalAI/pull/5607) has been added. Attention: extras images are going to be deprecated from the next release! Read [the backend management PR](https://github.com/mudler/LocalAI/pull/5607).\n- May 2025: [Audio input](https://github.com/mudler/LocalAI/pull/5466) and [Reranking](https://github.com/mudler/LocalAI/pull/5396) in llama.cpp backend, [Realtime API](https://github.com/mudler/LocalAI/pull/5392),  Support to Gemma, SmollVLM, and more multimodal models (available in the gallery).\n- May 2025: Important: image name changes [See release](https://github.com/mudler/LocalAI/releases/tag/v2.29.0)\n- Apr 2025: Rebrand, WebUI enhancements\n- Apr 2025: [LocalAGI](https://github.com/mudler/LocalAGI) and [LocalRecall](https://github.com/mudler/LocalRecall) join the LocalAI family stack.\n- Apr 2025: WebUI overhaul, AIO images updates\n- Feb 2025: Backend cleanup, Breaking changes, new backends (kokoro, OutelTTS, faster-whisper), Nvidia L4T images\n- Jan 2025: LocalAI model release: https://huggingface.co/mudler/LocalAI-functioncall-phi-4-v0.3, SANA support in diffusers: https://github.com/mudler/LocalAI/pull/4603\n- Dec 2024: stablediffusion.cpp backend (ggml) added ( https://github.com/mudler/LocalAI/pull/4289 )\n- Nov 2024: Bark.cpp backend added ( https://github.com/mudler/LocalAI/pull/4287 )\n- Nov 2024: Voice activity detection models (**VAD**) added to the API: https://github.com/mudler/LocalAI/pull/4204\n- Oct 2024: examples moved to [LocalAI-examples](https://github.com/mudler/LocalAI-examples)\n- Aug 2024:  ğŸ†• FLUX-1, [P2P Explorer](https://explorer.localai.io)\n- July 2024: ğŸ”¥ğŸ”¥ ğŸ†• P2P Dashboard, LocalAI Federated mode and AI Swarms: https://github.com/mudler/LocalAI/pull/2723. P2P Global community pools: https://github.com/mudler/LocalAI/issues/3113\n- May 2024: ğŸ”¥ğŸ”¥ Decentralized P2P llama.cpp:  https://github.com/mudler/LocalAI/pull/2343 (peer2peer llama.cpp!) ğŸ‘‰ Docs  https://localai.io/features/distribute/\n- May 2024: ğŸ”¥ğŸ”¥ Distributed inferencing: https://github.com/mudler/LocalAI/pull/2324\n- April 2024: Reranker API: https://github.com/mudler/LocalAI/pull/2121\n\nRoadmap items: [List of issues](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap)\n\n## ğŸš€ [Features](https://localai.io/features/)\n\n- ğŸ§© [Backend Gallery](https://localai.io/backends/): Install/remove backends on the fly, powered by OCI images â€” fully customizable and API-driven.\n- ğŸ“– [Text generation with GPTs](https://localai.io/features/text-generation/) (`llama.cpp`, `transformers`, `vllm` ... [:book: and more](https://localai.io/model-compatibility/index.html#model-compatibility-table))\n- ğŸ—£ [Text to Audio](https://localai.io/features/text-to-audio/)\n- ğŸ”ˆ [Audio to Text](https://localai.io/features/audio-to-text/) (Audio transcription with `whisper.cpp`)\n- ğŸ¨ [Image generation](https://localai.io/features/image-generation)\n- ğŸ”¥ [OpenAI-alike tools API](https://localai.io/features/openai-functions/) \n- ğŸ§  [Embeddings generation for vector databases](https://localai.io/features/embeddings/)\n- âœï¸ [Constrained grammars](https://localai.io/features/constrained_grammars/)\n- ğŸ–¼ï¸ [Download Models directly from Huggingface ](https://localai.io/models/)\n- ğŸ¥½ [Vision API](https://localai.io/features/gpt-vision/)\n- ğŸ” [Object Detection](https://localai.io/features/object-detection/)\n- ğŸ“ˆ [Reranker API](https://localai.io/features/reranker/)\n- ğŸ†•ğŸ–§ [P2P Inferencing](https://localai.io/features/distribute/)\n- ğŸ†•ğŸ”Œ [Model Context Protocol (MCP)](https://localai.io/docs/features/mcp/) - Agentic capabilities with external tools and [LocalAGI's Agentic capabilities](https://github.com/mudler/LocalAGI)\n- ğŸ”Š Voice activity detection (Silero-VAD support)\n- ğŸŒ Integrated WebUI!\n\n## ğŸ§© Supported Backends & Acceleration\n\nLocalAI supports a comprehensive range of AI backends with multiple acceleration options:\n\n### Text Generation & Language Models\n| Backend | Description | Acceleration Support |\n|---------|-------------|---------------------|\n| **llama.cpp** | LLM inference in C/C++ | CUDA 12/13, ROCm, Intel SYCL, Vulkan, Metal, CPU |\n| **vLLM** | Fast LLM inference with PagedAttention | CUDA 12/13, ROCm, Intel |\n| **transformers** | HuggingFace transformers framework | CUDA 12/13, ROCm, Intel, CPU |\n| **exllama2** | GPTQ inference library | CUDA 12/13 |\n| **MLX** | Apple Silicon LLM inference | Metal (M1/M2/M3+) |\n| **MLX-VLM** | Apple Silicon Vision-Language Models | Metal (M1/M2/M3+) |\n\n### Audio & Speech Processing\n| Backend | Description | Acceleration Support |\n|---------|-------------|---------------------|\n| **whisper.cpp** | OpenAI Whisper in C/C++ | CUDA 12/13, ROCm, Intel SYCL, Vulkan, CPU |\n| **faster-whisper** | Fast Whisper with CTranslate2 | CUDA 12/13, ROCm, Intel, CPU |\n| **bark** | Text-to-audio generation | CUDA 12/13, ROCm, Intel |\n| **bark-cpp** | C++ implementation of Bark | CUDA, Metal, CPU |\n| **coqui** | Advanced TTS with 1100+ languages | CUDA 12/13, ROCm, Intel, CPU |\n| **kokoro** | Lightweight TTS model | CUDA 12/13, ROCm, Intel, CPU |\n| **chatterbox** | Production-grade TTS | CUDA 12/13, CPU |\n| **piper** | Fast neural TTS system | CPU |\n| **kitten-tts** | Kitten TTS models | CPU |\n| **silero-vad** | Voice Activity Detection | CPU |\n| **neutts** | Text-to-speech with voice cloning | CUDA 12/13, ROCm, CPU |\n| **vibevoice** | Real-time TTS with voice cloning | CUDA 12/13, ROCm, Intel, CPU |\n| **pocket-tts** | Lightweight CPU-based TTS | CUDA 12/13, ROCm, Intel, CPU |\n\n### Image & Video Generation\n| Backend | Description | Acceleration Support |\n|---------|-------------|---------------------|\n| **stablediffusion.cpp** | Stable Diffusion in C/C++ | CUDA 12/13, Intel SYCL, Vulkan, CPU |\n| **diffusers** | HuggingFace diffusion models | CUDA 12/13, ROCm, Intel, Metal, CPU |\n\n### Specialized AI Tasks\n| Backend | Description | Acceleration Support |\n|---------|-------------|---------------------|\n| **rfdetr** | Real-time object detection | CUDA 12/13, Intel, CPU |\n| **rerankers** | Document reranking API | CUDA 12/13, ROCm, Intel, CPU |\n| **local-store** | Vector database | CPU |\n| **huggingface** | HuggingFace API integration | API-based |\n\n### Hardware Acceleration Matrix\n\n| Acceleration Type | Supported Backends | Hardware Support |\n|-------------------|-------------------|------------------|\n| **NVIDIA CUDA 12** | All CUDA-compatible backends | Nvidia hardware |\n| **NVIDIA CUDA 13** | All CUDA-compatible backends | Nvidia hardware |\n| **AMD ROCm** | llama.cpp, whisper, vllm, transformers, diffusers, rerankers, coqui, kokoro, bark, neutts, vibevoice, pocket-tts | AMD Graphics |\n| **Intel oneAPI** | llama.cpp, whisper, stablediffusion, vllm, transformers, diffusers, rfdetr, rerankers, exllama2, coqui, kokoro, bark, vibevoice, pocket-tts | Intel Arc, Intel iGPUs |\n| **Apple Metal** | llama.cpp, whisper, diffusers, MLX, MLX-VLM, bark-cpp | Apple M1/M2/M3+ |\n| **Vulkan** | llama.cpp, whisper, stablediffusion | Cross-platform GPUs |\n| **NVIDIA Jetson (CUDA 12)** | llama.cpp, whisper, stablediffusion, diffusers, rfdetr | ARM64 embedded AI (AGX Orin, etc.) |\n| **NVIDIA Jetson (CUDA 13)** | llama.cpp, whisper, stablediffusion, diffusers, rfdetr | ARM64 embedded AI (DGX Spark) |\n| **CPU Optimized** | All backends | AVX/AVX2/AVX512, quantization support |\n\n### ğŸ”— Community and integrations\n\nBuild and deploy custom containers:\n- https://github.com/sozercan/aikit\n\nWebUIs:\n- https://github.com/Jirubizu/localai-admin\n- https://github.com/go-skynet/LocalAI-frontend\n- QA-Pilot(An interactive chat project that leverages LocalAI LLMs for rapid understanding and navigation of GitHub code repository) https://github.com/reid41/QA-Pilot\n\nAgentic Libraries:\n- https://github.com/mudler/cogito\n\nMCPs:\n- https://github.com/mudler/MCPs\n\nModel galleries\n- https://github.com/go-skynet/model-gallery\n\nVoice:\n- https://github.com/richiejp/VoxInput\n\nOther:\n- Helm chart https://github.com/go-skynet/helm-charts\n- VSCode extension https://github.com/badgooooor/localai-vscode-plugin\n- Langchain: https://python.langchain.com/docs/integrations/providers/localai/\n- Terminal utility https://github.com/djcopley/ShellOracle\n- Local Smart assistant https://github.com/mudler/LocalAGI\n- Home Assistant https://github.com/sammcj/homeassistant-localai / https://github.com/drndos/hass-openai-custom-conversation / https://github.com/valentinfrlch/ha-gpt4vision\n- Discord bot https://github.com/mudler/LocalAGI/tree/main/examples/discord\n- Slack bot https://github.com/mudler/LocalAGI/tree/main/examples/slack\n- Shell-Pilot(Interact with LLM using LocalAI models via pure shell scripts on your Linux or MacOS system) https://github.com/reid41/shell-pilot\n- Telegram bot https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot\n- Another Telegram Bot https://github.com/JackBekket/Hellper\n- Auto-documentation https://github.com/JackBekket/Reflexia\n- Github bot which answer on issues, with code and documentation as context https://github.com/JackBekket/GitHelper\n- Github Actions: https://github.com/marketplace/actions/start-localai\n- Examples: https://github.com/mudler/LocalAI/tree/master/examples/\n  \n\n### ğŸ”— Resources\n\n- [LLM finetuning guide](https://localai.io/docs/advanced/fine-tuning/)\n- [How to build locally](https://localai.io/basics/build/index.html)\n- [How to install in Kubernetes](https://localai.io/basics/getting_started/index.html#run-localai-in-kubernetes)\n- [Projects integrating LocalAI](https://localai.io/docs/integrations/)\n- [How tos section](https://io.midori-ai.xyz/howtos/) (curated by our community)\n\n## :book: ğŸ¥ [Media, Blogs, Social](https://localai.io/basics/news/#media-blogs-social)\n\n- [Run Visual studio code with LocalAI (SUSE)](https://www.suse.com/c/running-ai-locally/)\n- ğŸ†• [Run LocalAI on Jetson Nano Devkit](https://mudler.pm/posts/local-ai-jetson-nano-devkit/)\n- [Run LocalAI on AWS EKS with Pulumi](https://www.pulumi.com/blog/low-code-llm-apps-with-local-ai-flowise-and-pulumi/)\n- [Run LocalAI on AWS](https://staleks.hashnode.dev/installing-localai-on-aws-ec2-instance)\n- [Create a slackbot for teams and OSS projects that answer to documentation](https://mudler.pm/posts/smart-slackbot-for-teams/)\n- [LocalAI meets k8sgpt](https://www.youtube.com/watch?v=PKrDNuJ_dfE)\n- [Question Answering on Documents locally with LangChain, LocalAI, Chroma, and GPT4All](https://mudler.pm/posts/localai-question-answering/)\n- [Tutorial to use k8sgpt with LocalAI](https://medium.com/@tyler_97636/k8sgpt-localai-unlock-kubernetes-superpowers-for-free-584790de9b65)\n\n## Citation\n\nIf you utilize this repository, data in a downstream project, please consider citing it with:\n\n```\n@misc{localai,\n  author = {Ettore Di Giacinto},\n  title = {LocalAI: The free, Open source OpenAI alternative},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/go-skynet/LocalAI}},\n```\n\n## â¤ï¸ Sponsors\n\n> Do you find LocalAI useful?\n\nSupport the project by becoming [a backer or sponsor](https://github.com/sponsors/mudler). Your logo will show up here with a link to your website.\n\nA huge thank you to our generous sponsors who support this project covering CI expenses, and our [Sponsor list](https://github.com/sponsors/mudler):\n\n<p align=\"center\">\n  <a href=\"https://www.spectrocloud.com/\" target=\"blank\">\n    <img height=\"200\" src=\"https://github.com/user-attachments/assets/72eab1dd-8b93-4fc0-9ade-84db49f24962\">\n  </a>\n  <a href=\"https://www.premai.io/\" target=\"blank\">\n    <img height=\"200\" src=\"https://github.com/mudler/LocalAI/assets/2420543/42e4ca83-661e-4f79-8e46-ae43689683d6\"> <br>\n  </a>\n</p>\n\n### Individual sponsors\n\nA special thanks to individual sponsors that contributed to the project, a full list is in [Github](https://github.com/sponsors/mudler) and [buymeacoffee](https://buymeacoffee.com/mudler), a special shout out goes to [drikster80](https://github.com/drikster80) for being generous. Thank you everyone!\n\n## ğŸŒŸ Star history\n\n[![LocalAI Star history Chart](https://api.star-history.com/svg?repos=go-skynet/LocalAI&type=Date)](https://star-history.com/#go-skynet/LocalAI&Date)\n\n## ğŸ“– License\n\nLocalAI is a community-driven project created by [Ettore Di Giacinto](https://github.com/mudler/).\n\nMIT - Author Ettore Di Giacinto <mudler@localai.io>\n\n## ğŸ™‡ Acknowledgements\n\nLocalAI couldn't have been built without the help of great software already available from the community. Thank you!\n\n- [llama.cpp](https://github.com/ggerganov/llama.cpp)\n- https://github.com/tatsu-lab/stanford_alpaca\n- https://github.com/cornelk/llama-go for the initial ideas\n- https://github.com/antimatter15/alpaca.cpp\n- https://github.com/EdVince/Stable-Diffusion-NCNN\n- https://github.com/ggerganov/whisper.cpp\n- https://github.com/rhasspy/piper\n\n## ğŸ¤— Contributors\n\nThis is a community project, a special thanks to our contributors! ğŸ¤—\n<a href=\"https://github.com/go-skynet/LocalAI/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=go-skynet/LocalAI\" />\n</a>\n",
      "stars_today": 64
    },
    {
      "id": 834966974,
      "name": "Pumpkin",
      "full_name": "Pumpkin-MC/Pumpkin",
      "description": "Empowering everyone to host fast and efficient Minecraft servers.",
      "html_url": "https://github.com/Pumpkin-MC/Pumpkin",
      "stars": 6431,
      "forks": 374,
      "language": "Rust",
      "topics": [
        "docker",
        "game-server",
        "gamedev",
        "minecraft",
        "minecraft-bedrock-edition",
        "minecraft-protocol",
        "minecraft-server",
        "networking",
        "rust",
        "server"
      ],
      "created_at": "2024-07-28T21:07:38Z",
      "updated_at": "2026-01-15T00:45:00Z",
      "pushed_at": "2026-01-14T21:45:04Z",
      "open_issues": 152,
      "owner": {
        "login": "Pumpkin-MC",
        "avatar_url": "https://avatars.githubusercontent.com/u/193089026?v=4"
      },
      "readme": "<div align=\"center\">\n\n# Pumpkin\n\n![CI](https://github.com/Pumpkin-MC/Pumpkin/actions/workflows/rust.yml/badge.svg)\n[![Discord](https://img.shields.io/discord/1268592337445978193.svg?label=&logo=discord&logoColor=ffffff&color=7389D8&labelColor=6A7EC2)](https://discord.gg/wT8XjrjKkf)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n![Current version)](https://img.shields.io/badge/current_version-1.21.11-blue)\n\n</div>\n\n[Pumpkin](https://pumpkinmc.org/) is a Minecraft server built entirely in Rust, offering a fast, efficient,\nand customizable experience. It prioritizes performance and player enjoyment while adhering to the core mechanics of the game.\n<div align=\"center\">\n\n![chunk loading](/assets/pumpkin_chunk_loading.GIF)\n\n</div>\n\n## Goals\n\n- **Performance**: Leveraging multi-threading for maximum speed and efficiency.\n- **Compatibility**: Supports the latest Java & Bedrock Minecraft server version while adhering to Vanilla game mechanics.\n- **Security**: Prioritizes security by preventing known security exploits.\n- **Flexibility**: Highly configurable, with the ability to disable unnecessary features.\n- **Extensibility**: Provides a foundation for plugin development.\n\n> [!IMPORTANT]\n> Pumpkin is currently under heavy development\n\n## Features\n\n- [x] Configuration (toml)\n- [x] Server Status/Ping\n- Networking\n  - [x] Encryption\n  - [x] Packet Compression\n- Player Configuration\n  - [x] Registries (biome types, paintings, dimensions)\n  - [x] Server Brand\n  - [x] Server Links\n  - [x] Set Resource Pack\n  - [x] Cookies\n- World\n  - [x] World Joining\n  - [x] Player Tab-list\n  - [x] Scoreboard\n  - [x] World Loading\n  - [x] World Time\n  - [x] World Borders\n  - [x] World Saving\n  - [x] Lighting\n  - [x] Entity Spawning\n  - [x] Item drops (W.I.P)\n  - [x] Bossbar\n  - [x] TNT\n  - [x] Chunk Loading (Vanilla, Linear)\n  - [x] Chunk Generation\n  - [x] Chunk Saving (Vanilla, Linear)\n  - [x] Biomes\n  - [x] Redstone (W.I.P)\n  - [x] Liquid Physics\n  - [x] Vegetation\n  - [ ] Structure Generation\n- Player\n  - [x] Skins\n  - [x] Client brand\n  - [x] Teleport\n  - [x] Movement\n  - [x] Animation\n  - [x] Inventory\n  - [x] Combat\n  - [x] Experience\n  - [x] Hunger\n  - [X] Off Hand\n  - [ ] Advancements\n  - [x] Eating\n- Entities\n  - [x] Non-Living (Minecart, Eggs...) (W.I.P)\n  - [x] Entity Effects\n  - [x] Players\n  - [x] Mobs (W.I.P)\n  - [x] Animals (W.I.P)\n  - [x] Entity AI (W.I.P)\n  - [ ] Boss\n  - [ ] Villagers\n  - [ ] Mobs Inventory\n  - [X] Entity Saving\n- Server\n  - [x] Plugins (W.I.P)\n  - [x] Query\n  - [x] RCON\n  - [x] Inventories\n  - [x] Particles\n  - [x] Chat\n  - [x] Commands (W.I.P)\n  - [x] Permissions\n  - [x] Translations\n- Proxy\n  - [x] Bungeecord\n  - [x] Velocity\n\nCheck out our [Github Project](https://github.com/orgs/Pumpkin-MC/projects/3) to see current progress.\n\n## How to run\n\nSee our [Quick Start](https://docs.pumpkinmc.org/#quick-start) guide to get Pumpkin running.\n\n## Contributions\n\nContributions are welcome! See [CONTRIBUTING.md](CONTRIBUTING.md)\n\n## Docs\n\nPumpkin's documentation can be found at <https://pumpkinmc.org/>\n\n## Communication\n\nConsider joining [our Discord server](https://discord.gg/wT8XjrjKkf) to stay up-to-date on events, updates, and connect with other members.\n\n## Funding\n\nIf you want to fund me and help the project, check out my [GitHub sponsors](https://github.com/sponsors/Snowiiii).\n",
      "stars_today": 56
    },
    {
      "id": 845594676,
      "name": "tansu",
      "full_name": "tansu-io/tansu",
      "description": "Apache KafkaÂ® compatible broker with S3, PostgreSQL, SQLite, Apache Iceberg and Delta Lake",
      "html_url": "https://github.com/tansu-io/tansu",
      "stars": 1254,
      "forks": 49,
      "language": "Rust",
      "topics": [
        "apache-arrow",
        "apache-iceberg",
        "apache-kafka",
        "built-with-rust",
        "datafusion",
        "datalake",
        "delta-lake",
        "parquet",
        "postgres",
        "postgresql",
        "s3",
        "sqlite"
      ],
      "created_at": "2024-08-21T14:47:15Z",
      "updated_at": "2026-01-15T00:58:32Z",
      "pushed_at": "2026-01-13T14:22:23Z",
      "open_issues": 58,
      "owner": {
        "login": "tansu-io",
        "avatar_url": "https://avatars.githubusercontent.com/u/161749514?v=4"
      },
      "readme": "<div align=\"center\">\n\n# Tansu ğŸ—ƒï¸\nstateless Kafka-compatible broker with pluggable storage (PostgreSQL, SQLite, S3, memory)\n\n<br>\n\n[![License](https://img.shields.io/badge/License-Apache-165dfc.svg)](https://github.com/tansu-io/tansu/blob/main/LICENSE)\n&nbsp;\n[![Built with Rust](https://img.shields.io/badge/built_with-Rust-165dfc.svg?logo=rust)](https://www.rust-lang.org/)\n&nbsp;\n<br>\n[![Docs](https://img.shields.io/badge/ğŸ“–%20docs-docs.tansu.io-165dfc.svg)](https://docs.tansu.io/)\n&nbsp;\n[![Blog](https://img.shields.io/badge/%F0%9F%93%98%20blog-blog.tansu.io-165dfc.svg)](https://blog.tansu.io/articles)\n&nbsp;\n<br>\n[![GitHub stars](https://img.shields.io/github/stars/tansu-io/tansu?style=social)](https://github.com/tansu-io/tansu)\n&nbsp;\n[![Bluesky](https://img.shields.io/bluesky/followers/tansu.io)](https://bsky.app/profile/tansu.io)\n\n<br>\n\n</div>\n\n# What is Tansu?\n\n[Tansu][github-com-tansu-io] is a drop-in replacement for\nApache Kafka with PostgreSQL, libSQL (SQLite), S3 or memory storage engines.\nSchema backed topics (Avro, JSON or Protocol buffers) can\nbe written as [Apache Iceberg](https://iceberg.apache.org) or [Delta Lake](https://delta.io) tables.\n\nFeatures:\n\n- Apache Kafka API compatible\n- Available with [PostgreSQL](https://www.postgresql.org), [libSQL](https://docs.turso.tech/libsql), [S3](https://en.wikipedia.org/wiki/Amazon_S3) or memory storage engines\n- Topics [validated](docs/schema-registry.md) by [JSON Schema][json-schema-org], [Apache Avro](https://avro.apache.org)\n  or [Protocol buffers](protocol-buffers) can be written as [Apache Iceberg](https://iceberg.apache.org) or [Delta Lake](https://delta.io) tables\n\nSee [examples using pyiceberg](https://github.com/tansu-io/example-pyiceberg), [examples using Apache Spark](https://github.com/tansu-io/example-spark) or ğŸ†• [examples using Delta Lake](https://github.com/tansu-io/example-delta-lake).\n\nFor data durability:\n\n- S3 is designed to exceed [99.999999999% (11 nines)][aws-s3-storage-classes]\n- PostgreSQL with [continuous archiving][continuous-archiving]\n  streaming transaction logs files to an archive\n- The memory storage engine is designed for ephemeral non-production environments\n\nTansu is a single statically linked binary containing the following:\n\n- **broker** an Apache Kafka API compatible broker and schema registry\n- **topic** a CLI to create/delete Topics\n- **cat** a CLI to consume or produce Avro, JSON or Protobuf messages to a topic\n- **proxy** an Apache Kafka compatible proxy\n\n## broker\n\nThe broker subcommand is default if no other command is supplied.\n\n```shell\nUsage: tansu [OPTIONS]\n       tansu <COMMAND>\n\nCommands:\n  broker  Apache Kafka compatible broker with Avro, JSON, Protobuf schema validation [default if no command supplied]\n  cat     Easily consume or produce Avro, JSON or Protobuf messages to a topic\n  topic   Create or delete topics managed by the broker\n  proxy   Apache Kafka compatible proxy\n  help    Print this message or the help of the given subcommand(s)\n\nOptions:\n      --kafka-cluster-id <KAFKA_CLUSTER_ID>\n          All members of the same cluster should use the same id [env: CLUSTER_ID=RvQwrYegSUCkIPkaiAZQlQ] [default: tansu_cluster]\n      --kafka-listener-url <KAFKA_LISTENER_URL>\n          The broker will listen on this address [env: LISTENER_URL=] [default: tcp://[::]:9092]\n      --kafka-advertised-listener-url <KAFKA_ADVERTISED_LISTENER_URL>\n          This location is advertised to clients in metadata [env: ADVERTISED_LISTENER_URL=tcp://localhost:9092] [default: tcp://localhost:9092]\n      --storage-engine <STORAGE_ENGINE>\n          Storage engine examples are: postgres://postgres:postgres@localhost, memory://tansu/ or s3://tansu/ [env: STORAGE_ENGINE=s3://tansu/] [default: memory://tansu/]\n      --schema-registry <SCHEMA_REGISTRY>\n          Schema registry examples are: file://./etc/schema or s3://tansu/, containing: topic.json, topic.proto or topic.avsc [env: SCHEMA_REGISTRY=file://./etc/schema]\n      --data-lake <DATA_LAKE>\n          Apache Parquet files are written to this location, examples are: file://./lake or s3://lake/ [env: DATA_LAKE=s3://lake/]\n      --iceberg-catalog <ICEBERG_CATALOG>\n          Apache Iceberg Catalog, examples are: http://localhost:8181/ [env: ICEBERG_CATALOG=http://localhost:8181/]\n      --iceberg-namespace <ICEBERG_NAMESPACE>\n          Iceberg namespace [env: ICEBERG_NAMESPACE=] [default: tansu]\n      --prometheus-listener-url <PROMETHEUS_LISTENER_URL>\n          Broker metrics can be scraped by Prometheus from this URL [env: PROMETHEUS_LISTENER_URL=tcp://0.0.0.0:9100] [default: tcp://[::]:9100]\n  -h, --help\n          Print help\n  -V, --version\n          Print version\n```\n\nA broker can be started by simply running `tansu`, all options have defaults. Tansu pickup any existing environment,\nloading any found in `.env`. An [example.env](example.env) is provided as part of the distribution\nand can be copied into `.env` for local modification. Sample schemas can be found in [etc/schema](etc/schema), used in the examples.\n\nIf an Apache Avro, Protobuf or JSON schema has been assigned to a topic, the\nbroker will reject any messages that are invalid. Schema backed topics are written\nas Apache Parquet when the `-data-lake` option is provided.\n\n## topic\n\nThe `tansu topic` command has the following subcommands:\n\n```shell\nCreate or delete topics managed by the broker\n\nUsage: tansu topic <COMMAND>\n\nCommands:\n  create  Create a topic\n  delete  Delete an existing topic\n  help    Print this message or the help of the given subcommand(s)\n\nOptions:\n  -h, --help  Print help\n```\n\nTo create a topic use:\n\n```shell\ntansu topic create taxi\n```\n\n## cat\n\nThe `tansu cat` command, has the following subcommands:\n\n```shell\ntansu cat --help\nEasily consume or produce Avro, JSON or Protobuf messages to a topic\n\nUsage: tansu cat <COMMAND>\n\nCommands:\n  produce  Produce Avro/JSON/Protobuf messages to a topic\n  consume  Consume Avro/JSON/Protobuf messages from a topic\n  help     Print this message or the help of the given subcommand(s)\n\nOptions:\n  -h, --help  Print help\n```\n\nThe `produce` subcommand reads JSON formatted messages encoding them into\nApache Avro, Protobuf or JSON depending on the schema used by the topic.\n\nFor example, the `taxi` topic is backed by [taxi.proto](etc/schema/taxi.proto).\nUsing [trips.json](etc/data/trips.json) containing a JSON array of objects,\n`tansu cat produce` encodes each message into protobuf into the broker:\n\n```\ntansu cat produce taxi etc/data/trips.json\n```\n\nUsing [duckdb](https://duckdb.org) we can read the\n[Apache Parquet](https://parquet.apache.org) files\ncreated by the broker:\n\n```shell\nduckdb :memory: \"SELECT * FROM 'data/taxi/*/*.parquet'\"\n```\n\nResults in the following output:\n\n```shell\n|-----------+---------+---------------+-------------+---------------|\n| vendor_id | trip_id | trip_distance | fare_amount | store_and_fwd |\n|     int64 |   int64 |         float |      double |         int32 |\n|-----------+---------+---------------+-------------+---------------|\n|         1 | 1000371 |           1.8 |       15.32 |             0 |\n|         2 | 1000372 |           2.5 |       22.15 |             0 |\n|         2 | 1000373 |           0.9 |        9.01 |             0 |\n|         1 | 1000374 |           8.4 |       42.13 |             1 |\n|-----------+---------+---------------+-------------+---------------|\n```\n\n\n### s3\n\nThe following will configure a S3 storage engine\nusing the \"tansu\" bucket (full context is in\n[compose.yaml](compose.yaml) and [example.env](example.env)):\n\nCopy `example.env` into `.env` so that you have a local working copy:\n\n```shell\ncp example.env .env\n```\n\nEdit `.env` so that `STORAGE_ENGINE` is defined as:\n\n```shell\nSTORAGE_ENGINE=\"s3://tansu/\"\n```\n\nFirst time startup, you'll need to create a bucket, an access key\nand a secret in minio.\n\nJust bring minio up, without tansu:\n\n```shell\ndocker compose up -d minio\n```\n\nCreate a minio `local` alias representing `http://localhost:9000` with the default credentials of `minioadmin`:\n\n```shell\ndocker compose exec minio \\\n   /usr/bin/mc \\\n   alias \\\n   set \\\n   local \\\n   http://localhost:9000 \\\n   minioadmin \\\n   minioadmin\n```\n\nCreate a `tansu` bucket in minio using the `local` alias:\n\n```shell\ndocker compose exec minio \\\n   /usr/bin/mc mb local/tansu\n```\n\nOnce this is done, you can start tansu with:\n\n```shell\ndocker compose up -d tansu\n```\n\nUsing the regular Apache Kafka CLI you can create topics, produce and consume\nmessages with Tansu:\n\n```shell\nkafka-topics \\\n  --bootstrap-server localhost:9092 \\\n  --partitions=3 \\\n  --replication-factor=1 \\\n  --create --topic test\n```\n\nDescribe the `test` topic:\n\n```shell\nkafka-topics \\\n  --bootstrap-server localhost:9092 \\\n  --describe \\\n  --topic test\n```\n\nNote that node 111 is the leader and ISR for each topic partition.\nThis node represents the broker handling your request. All brokers are node 111.\n\nProducer:\n\n```shell\necho \"hello world\" | kafka-console-producer \\\n    --bootstrap-server localhost:9092 \\\n    --topic test\n```\n\nGroup consumer using `test-consumer-group`:\n\n```shell\nkafka-console-consumer \\\n  --bootstrap-server localhost:9092 \\\n  --group test-consumer-group \\\n  --topic test \\\n  --from-beginning \\\n  --property print.timestamp=true \\\n  --property print.key=true \\\n  --property print.offset=true \\\n  --property print.partition=true \\\n  --property print.headers=true \\\n  --property print.value=true\n```\n\nDescribe the consumer `test-consumer-group` group:\n\n```shell\nkafka-consumer-groups \\\n  --bootstrap-server localhost:9092 \\\n  --group test-consumer-group \\\n  --describe\n```\n\n### PostgreSQL\n\nTo switch between the minio and PostgreSQL examples, firstly\nshutdown Tansu:\n\n```shell\ndocker compose down tansu\n```\n\nSwitch to the PostgreSQL storage engine by updating [.env](.env):\n\n```env\n# minio storage engine\n# STORAGE_ENGINE=\"s3://tansu/\"\n\n# PostgreSQL storage engine -- NB: @db and NOT @localhost :)\nSTORAGE_ENGINE=\"postgres://postgres:postgres@db\"\n```\n\nStart PostgreSQL:\n\n```shell\ndocker compose up -d db\n```\n\nBring Tansu back up:\n\n```shell\ndocker compose up -d tansu\n```\n\nUsing the regular Apache Kafka CLI you can create topics, produce and consume\nmessages with Tansu:\n\n```shell\nkafka-topics \\\n  --bootstrap-server localhost:9092 \\\n  --partitions=3 \\\n  --replication-factor=1 \\\n  --create --topic test\n```\n\nProducer:\n\n```shell\necho \"hello world\" | kafka-console-producer \\\n    --bootstrap-server localhost:9092 \\\n    --topic test\n```\n\nConsumer:\n\n```shell\nkafka-console-consumer \\\n  --bootstrap-server localhost:9092 \\\n  --group test-consumer-group \\\n  --topic test \\\n  --from-beginning \\\n  --property print.timestamp=true \\\n  --property print.key=true \\\n  --property print.offset=true \\\n  --property print.partition=true \\\n  --property print.headers=true \\\n  --property print.value=true\n```\n\nOr using [librdkafka][librdkafka] to produce:\n\n```shell\necho \"Lorem ipsum dolor...\" | \\\n  ./examples/rdkafka_example -P \\\n  -t test -p 1 \\\n  -b localhost:9092 \\\n  -z gzip\n```\n\nConsumer:\n\n```shell\n./examples/rdkafka_example \\\n  -C \\\n  -t test -p 1 \\\n  -b localhost:9092\n```\n\n## Feedback\n\nPlease [raise an issue][tansu-issues] if you encounter a problem.\n\n## License\n\nTansu is licensed under [Apache 2.0][apache-license].\n\n[apache-license]: https://www.apache.org/licenses/LICENSE-2.0\n[apache-zookeeper]: https://en.wikipedia.org/wiki/Apache_ZooKeeper\n[aws-s3-conditional-requests]: https://docs.aws.amazon.com/AmazonS3/latest/userguide/conditional-requests.html\n[aws-s3-conditional-writes]: https://aws.amazon.com/about-aws/whats-new/2024/08/amazon-s3-conditional-writes/\n[aws-s3-storage-classes]: https://aws.amazon.com/s3/storage-classes/\n[cloudflare-r2]: https://developers.cloudflare.com/r2/\n[continuous-archiving]: https://www.postgresql.org/docs/current/continuous-archiving.html\n[crates-io-object-store]: https://crates.io/crates/object_store\n[github-com-tansu-io]: https://github.com/tansu-io/tansu\n[json-schema-org]: https://json-schema.org/\n[librdkafka]: https://github.com/confluentinc/librdkafka\n[min-io]: https://min.io\n[minio-create-access-key]: https://min.io/docs/minio/container/administration/console/security-and-access.html#id1\n[minio-create-bucket]: https://min.io/docs/minio/container/administration/console/managing-objects.html#creating-buckets\n[object-store-dynamo-conditional-put]: https://docs.rs/object_store/0.11.0/object_store/aws/struct.DynamoCommit.html\n[protocol-buffers]: https://protobuf.dev\n[raft-consensus]: https://raft.github.io\n[rust-lang-org]: https://www.rust-lang.org\n[tansu-issues]: https://github.com/tansu-io/tansu/issues\n[tigris-conditional-writes]: https://www.tigrisdata.com/blog/s3-conditional-writes/\n",
      "stars_today": 49
    },
    {
      "id": 850267588,
      "name": "xiaozhi-esp32",
      "full_name": "78/xiaozhi-esp32",
      "description": "An MCP-based chatbot | ä¸€ä¸ªåŸºäºMCPçš„èŠå¤©æœºå™¨äºº",
      "html_url": "https://github.com/78/xiaozhi-esp32",
      "stars": 23204,
      "forks": 4879,
      "language": "C++",
      "topics": [
        "chatbot",
        "esp32",
        "mcp"
      ],
      "created_at": "2024-08-31T10:08:16Z",
      "updated_at": "2026-01-14T20:16:08Z",
      "pushed_at": "2026-01-14T17:12:31Z",
      "open_issues": 478,
      "owner": {
        "login": "78",
        "avatar_url": "https://avatars.githubusercontent.com/u/4488133?v=4"
      },
      "readme": "# An MCP-based Chatbot\n\n(English | [ä¸­æ–‡](README_zh.md) | [æ—¥æœ¬èª](README_ja.md))\n\n## Introduction\n\nğŸ‘‰ [Human: Give AI a camera vs AI: Instantly finds out the owner hasn't washed hair for three daysã€bilibiliã€‘](https://www.bilibili.com/video/BV1bpjgzKEhd/)\n\nğŸ‘‰ [Handcraft your AI girlfriend, beginner's guideã€bilibiliã€‘](https://www.bilibili.com/video/BV1XnmFYLEJN/)\n\nAs a voice interaction entry, the XiaoZhi AI chatbot leverages the AI capabilities of large models like Qwen / DeepSeek, and achieves multi-terminal control via the MCP protocol.\n\n<img src=\"docs/mcp-based-graph.jpg\" alt=\"Control everything via MCP\" width=\"320\">\n\n## Version Notes\n\nThe current v2 version is incompatible with the v1 partition table, so it is not possible to upgrade from v1 to v2 via OTA. For partition table details, see [partitions/v2/README.md](partitions/v2/README.md).\n\nAll hardware running v1 can be upgraded to v2 by manually flashing the firmware.\n\nThe stable version of v1 is 1.9.2. You can switch to v1 by running `git checkout v1`. The v1 branch will be maintained until February 2026.\n\n### Features Implemented\n\n- Wi-Fi / ML307 Cat.1 4G\n- Offline voice wake-up [ESP-SR](https://github.com/espressif/esp-sr)\n- Supports two communication protocols ([Websocket](docs/websocket.md) or MQTT+UDP)\n- Uses OPUS audio codec\n- Voice interaction based on streaming ASR + LLM + TTS architecture\n- Speaker recognition, identifies the current speaker [3D Speaker](https://github.com/modelscope/3D-Speaker)\n- OLED / LCD display, supports emoji display\n- Battery display and power management\n- Multi-language support (Chinese, English, Japanese)\n- Supports ESP32-C3, ESP32-S3, ESP32-P4 chip platforms\n- Device-side MCP for device control (Speaker, LED, Servo, GPIO, etc.)\n- Cloud-side MCP to extend large model capabilities (smart home control, PC desktop operation, knowledge search, email, etc.)\n- Customizable wake words, fonts, emojis, and chat backgrounds with online web-based editing ([Custom Assets Generator](https://github.com/78/xiaozhi-assets-generator))\n\n## Hardware\n\n### Breadboard DIY Practice\n\nSee the Feishu document tutorial:\n\nğŸ‘‰ [\"XiaoZhi AI Chatbot Encyclopedia\"](https://ccnphfhqs21z.feishu.cn/wiki/F5krwD16viZoF0kKkvDcrZNYnhb?from=from_copylink)\n\nBreadboard demo:\n\n![Breadboard Demo](docs/v1/wiring2.jpg)\n\n### Supports 70+ Open Source Hardware (Partial List)\n\n- <a href=\"https://oshwhub.com/li-chuang-kai-fa-ban/li-chuang-shi-zhan-pai-esp32-s3-kai-fa-ban\" target=\"_blank\" title=\"LiChuang ESP32-S3 Development Board\">LiChuang ESP32-S3 Development Board</a>\n- <a href=\"https://github.com/espressif/esp-box\" target=\"_blank\" title=\"Espressif ESP32-S3-BOX3\">Espressif ESP32-S3-BOX3</a>\n- <a href=\"https://docs.m5stack.com/zh_CN/core/CoreS3\" target=\"_blank\" title=\"M5Stack CoreS3\">M5Stack CoreS3</a>\n- <a href=\"https://docs.m5stack.com/en/atom/Atomic%20Echo%20Base\" target=\"_blank\" title=\"AtomS3R + Echo Base\">M5Stack AtomS3R + Echo Base</a>\n- <a href=\"https://gf.bilibili.com/item/detail/1108782064\" target=\"_blank\" title=\"Magic Button 2.4\">Magic Button 2.4</a>\n- <a href=\"https://www.waveshare.net/shop/ESP32-S3-Touch-AMOLED-1.8.htm\" target=\"_blank\" title=\"Waveshare ESP32-S3-Touch-AMOLED-1.8\">Waveshare ESP32-S3-Touch-AMOLED-1.8</a>\n- <a href=\"https://github.com/Xinyuan-LilyGO/T-Circle-S3\" target=\"_blank\" title=\"LILYGO T-Circle-S3\">LILYGO T-Circle-S3</a>\n- <a href=\"https://oshwhub.com/tenclass01/xmini_c3\" target=\"_blank\" title=\"XiaGe Mini C3\">XiaGe Mini C3</a>\n- <a href=\"https://oshwhub.com/movecall/cuican-ai-pendant-lights-up-y\" target=\"_blank\" title=\"Movecall CuiCan ESP32S3\">CuiCan AI Pendant</a>\n- <a href=\"https://github.com/WMnologo/xingzhi-ai\" target=\"_blank\" title=\"WMnologo-Xingzhi-1.54\">WMnologo-Xingzhi-1.54TFT</a>\n- <a href=\"https://www.seeedstudio.com/SenseCAP-Watcher-W1-A-p-5979.html\" target=\"_blank\" title=\"SenseCAP Watcher\">SenseCAP Watcher</a>\n- <a href=\"https://www.bilibili.com/video/BV1BHJtz6E2S/\" target=\"_blank\" title=\"ESP-HI Low Cost Robot Dog\">ESP-HI Low Cost Robot Dog</a>\n\n<div style=\"display: flex; justify-content: space-between;\">\n  <a href=\"docs/v1/lichuang-s3.jpg\" target=\"_blank\" title=\"LiChuang ESP32-S3 Development Board\">\n    <img src=\"docs/v1/lichuang-s3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/espbox3.jpg\" target=\"_blank\" title=\"Espressif ESP32-S3-BOX3\">\n    <img src=\"docs/v1/espbox3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/m5cores3.jpg\" target=\"_blank\" title=\"M5Stack CoreS3\">\n    <img src=\"docs/v1/m5cores3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/atoms3r.jpg\" target=\"_blank\" title=\"AtomS3R + Echo Base\">\n    <img src=\"docs/v1/atoms3r.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/magiclick.jpg\" target=\"_blank\" title=\"Magic Button 2.4\">\n    <img src=\"docs/v1/magiclick.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/waveshare.jpg\" target=\"_blank\" title=\"Waveshare ESP32-S3-Touch-AMOLED-1.8\">\n    <img src=\"docs/v1/waveshare.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/lilygo-t-circle-s3.jpg\" target=\"_blank\" title=\"LILYGO T-Circle-S3\">\n    <img src=\"docs/v1/lilygo-t-circle-s3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/xmini-c3.jpg\" target=\"_blank\" title=\"XiaGe Mini C3\">\n    <img src=\"docs/v1/xmini-c3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/movecall-cuican-esp32s3.jpg\" target=\"_blank\" title=\"CuiCan\">\n    <img src=\"docs/v1/movecall-cuican-esp32s3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/wmnologo_xingzhi_1.54.jpg\" target=\"_blank\" title=\"WMnologo-Xingzhi-1.54\">\n    <img src=\"docs/v1/wmnologo_xingzhi_1.54.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/sensecap_watcher.jpg\" target=\"_blank\" title=\"SenseCAP Watcher\">\n    <img src=\"docs/v1/sensecap_watcher.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/esp-hi.jpg\" target=\"_blank\" title=\"ESP-HI Low Cost Robot Dog\">\n    <img src=\"docs/v1/esp-hi.jpg\" width=\"240\" />\n  </a>\n</div>\n\n## Software\n\n### Firmware Flashing\n\nFor beginners, it is recommended to use the firmware that can be flashed without setting up a development environment.\n\nThe firmware connects to the official [xiaozhi.me](https://xiaozhi.me) server by default. Personal users can register an account to use the Qwen real-time model for free.\n\nğŸ‘‰ [Beginner's Firmware Flashing Guide](https://ccnphfhqs21z.feishu.cn/wiki/Zpz4wXBtdimBrLk25WdcXzxcnNS)\n\n### Development Environment\n\n- Cursor or VSCode\n- Install ESP-IDF plugin, select SDK version 5.4 or above\n- Linux is better than Windows for faster compilation and fewer driver issues\n- This project uses Google C++ code style, please ensure compliance when submitting code\n\n### Developer Documentation\n\n- [Custom Board Guide](docs/custom-board.md) - Learn how to create custom boards for XiaoZhi AI\n- [MCP Protocol IoT Control Usage](docs/mcp-usage.md) - Learn how to control IoT devices via MCP protocol\n- [MCP Protocol Interaction Flow](docs/mcp-protocol.md) - Device-side MCP protocol implementation\n- [MQTT + UDP Hybrid Communication Protocol Document](docs/mqtt-udp.md)\n- [A detailed WebSocket communication protocol document](docs/websocket.md)\n\n## Large Model Configuration\n\nIf you already have a XiaoZhi AI chatbot device and have connected to the official server, you can log in to the [xiaozhi.me](https://xiaozhi.me) console for configuration.\n\nğŸ‘‰ [Backend Operation Video Tutorial (Old Interface)](https://www.bilibili.com/video/BV1jUCUY2EKM/)\n\n## Related Open Source Projects\n\nFor server deployment on personal computers, refer to the following open-source projects:\n\n- [xinnan-tech/xiaozhi-esp32-server](https://github.com/xinnan-tech/xiaozhi-esp32-server) Python server\n- [joey-zhou/xiaozhi-esp32-server-java](https://github.com/joey-zhou/xiaozhi-esp32-server-java) Java server\n- [AnimeAIChat/xiaozhi-server-go](https://github.com/AnimeAIChat/xiaozhi-server-go) Golang server\n\nOther client projects using the XiaoZhi communication protocol:\n\n- [huangjunsen0406/py-xiaozhi](https://github.com/huangjunsen0406/py-xiaozhi) Python client\n- [TOM88812/xiaozhi-android-client](https://github.com/TOM88812/xiaozhi-android-client) Android client\n- [100askTeam/xiaozhi-linux](http://github.com/100askTeam/xiaozhi-linux) Linux client by 100ask\n- [78/xiaozhi-sf32](https://github.com/78/xiaozhi-sf32) Bluetooth chip firmware by Sichuan\n- [QuecPython/solution-xiaozhiAI](https://github.com/QuecPython/solution-xiaozhiAI) QuecPython firmware by Quectel\n\nCustom Assets Tools:\n\n- [78/xiaozhi-assets-generator](https://github.com/78/xiaozhi-assets-generator) Custom Assets Generator (Wake words, fonts, emojis, backgrounds)\n\n## About the Project\n\nThis is an open-source ESP32 project, released under the MIT license, allowing anyone to use it for free, including for commercial purposes.\n\nWe hope this project helps everyone understand AI hardware development and apply rapidly evolving large language models to real hardware devices.\n\nIf you have any ideas or suggestions, please feel free to raise Issues or join the QQ group: 1011329060\n\n## Star History\n\n<a href=\"https://star-history.com/#78/xiaozhi-esp32&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=78/xiaozhi-esp32&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=78/xiaozhi-esp32&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=78/xiaozhi-esp32&type=Date\" />\n </picture>\n</a> \n",
      "stars_today": 41
    },
    {
      "id": 576201,
      "name": "three.js",
      "full_name": "mrdoob/three.js",
      "description": "JavaScript 3D Library.",
      "html_url": "https://github.com/mrdoob/three.js",
      "stars": 110331,
      "forks": 36228,
      "language": "JavaScript",
      "topics": [
        "3d",
        "augmented-reality",
        "canvas",
        "html5",
        "javascript",
        "svg",
        "virtual-reality",
        "webaudio",
        "webgl",
        "webgl2",
        "webgpu",
        "webxr"
      ],
      "created_at": "2010-03-23T18:58:01Z",
      "updated_at": "2026-01-14T22:34:37Z",
      "pushed_at": "2026-01-14T16:05:05Z",
      "open_issues": 617,
      "owner": {
        "login": "mrdoob",
        "avatar_url": "https://avatars.githubusercontent.com/u/97088?v=4"
      },
      "readme": "# three.js\n\n[![NPM Package][npm]][npm-url]\n[![Build Size][build-size]][build-size-url]\n[![NPM Downloads][npm-downloads]][npmtrends-url]\n[![jsDelivr Downloads][jsdelivr-downloads]][jsdelivr-url]\n[![Discord][discord]][discord-url]\n\n#### JavaScript 3D library\n\nThe aim of the project is to create an easy-to-use, lightweight, cross-browser, general-purpose 3D library. The current builds only include WebGL and WebGPU renderers but SVG and CSS3D renderers are also available as addons.\n\n[Examples](https://threejs.org/examples/) &mdash;\n[Docs](https://threejs.org/docs/) &mdash;\n[Manual](https://threejs.org/manual/) &mdash;\n[Wiki](https://github.com/mrdoob/three.js/wiki) &mdash;\n[Migrating](https://github.com/mrdoob/three.js/wiki/Migration-Guide) &mdash;\n[Questions](https://stackoverflow.com/questions/tagged/three.js) &mdash;\n[Forum](https://discourse.threejs.org/) &mdash;\n[Discord](https://discord.gg/56GBJwAnUS)\n\n### Usage\n\nThis code creates a scene, a camera, and a geometric cube, and it adds the cube to the scene. It then creates a `WebGL` renderer for the scene and camera, and it adds that viewport to the `document.body` element. Finally, it animates the cube within the scene for the camera.\n\n```javascript\nimport * as THREE from 'three';\n\nconst width = window.innerWidth, height = window.innerHeight;\n\n// init\n\nconst camera = new THREE.PerspectiveCamera( 70, width / height, 0.01, 10 );\ncamera.position.z = 1;\n\nconst scene = new THREE.Scene();\n\nconst geometry = new THREE.BoxGeometry( 0.2, 0.2, 0.2 );\nconst material = new THREE.MeshNormalMaterial();\n\nconst mesh = new THREE.Mesh( geometry, material );\nscene.add( mesh );\n\nconst renderer = new THREE.WebGLRenderer( { antialias: true } );\nrenderer.setSize( width, height );\nrenderer.setAnimationLoop( animate );\ndocument.body.appendChild( renderer.domElement );\n\n// animation\n\nfunction animate( time ) {\n\n\tmesh.rotation.x = time / 2000;\n\tmesh.rotation.y = time / 1000;\n\n\trenderer.render( scene, camera );\n\n}\n```\n\nIf everything goes well, you should see [this](https://jsfiddle.net/w43x5Lgh/).\n\n### Cloning this repository\n\nCloning the repo with all its history results in a ~2 GB download. If you don't need the whole history you can use the `depth` parameter to significantly reduce download size.\n\n```sh\ngit clone --depth=1 https://github.com/mrdoob/three.js.git\n```\n\n### Change log\n\n[Releases](https://github.com/mrdoob/three.js/releases)\n\n\n[npm]: https://img.shields.io/npm/v/three\n[npm-url]: https://www.npmjs.com/package/three\n[build-size]: https://badgen.net/bundlephobia/minzip/three\n[build-size-url]: https://bundlephobia.com/result?p=three\n[npm-downloads]: https://img.shields.io/npm/dw/three\n[npmtrends-url]: https://www.npmtrends.com/three\n[jsdelivr-downloads]: https://data.jsdelivr.com/v1/package/npm/three/badge?style=rounded\n[jsdelivr-url]: https://www.jsdelivr.com/package/npm/three\n[discord]: https://img.shields.io/discord/685241246557667386\n[discord-url]: https://discord.gg/56GBJwAnUS\n",
      "stars_today": 39
    },
    {
      "id": 825470378,
      "name": "beszel",
      "full_name": "henrygd/beszel",
      "description": "Lightweight server monitoring hub with historical data, docker stats, and alerts.",
      "html_url": "https://github.com/henrygd/beszel",
      "stars": 18625,
      "forks": 597,
      "language": "Go",
      "topics": [
        "homelab",
        "monitoring",
        "self-hosted"
      ],
      "created_at": "2024-07-07T21:36:28Z",
      "updated_at": "2026-01-15T00:43:09Z",
      "pushed_at": "2026-01-14T21:45:24Z",
      "open_issues": 389,
      "owner": {
        "login": "henrygd",
        "avatar_url": "https://avatars.githubusercontent.com/u/8519632?v=4"
      },
      "readme": "# Beszel\n\nBeszel is a lightweight server monitoring platform that includes Docker statistics, historical data, and alert functions.\n\nIt has a friendly web interface, simple configuration, and is ready to use out of the box. It supports automatic backup, multi-user, OAuth authentication, and API access.\n\n[![agent Docker Image Size](https://img.shields.io/docker/image-size/henrygd/beszel-agent/latest?logo=docker&label=agent%20image%20size)](https://hub.docker.com/r/henrygd/beszel-agent)\n[![hub Docker Image Size](https://img.shields.io/docker/image-size/henrygd/beszel/latest?logo=docker&label=hub%20image%20size)](https://hub.docker.com/r/henrygd/beszel)\n[![MIT license](https://img.shields.io/github/license/henrygd/beszel?color=%239944ee)](https://github.com/henrygd/beszel/blob/main/LICENSE)\n[![Crowdin](https://badges.crowdin.net/beszel/localized.svg)](https://crowdin.com/project/beszel)\n\n![Screenshot of Beszel dashboard and system page, side by side. The dashboard shows metrics from multiple connected systems, while the system page shows detailed metrics for a single system.](https://henrygd-assets.b-cdn.net/beszel/screenshot-new.png)\n\n## Features\n\n- **Lightweight**: Smaller and less resource-intensive than leading solutions.\n- **Simple**: Easy setup with little manual configuration required.\n- **Docker stats**: Tracks CPU, memory, and network usage history for each container.\n- **Alerts**: Configurable alerts for CPU, memory, disk, bandwidth, temperature, load average, and status.\n- **Multi-user**: Users manage their own systems. Admins can share systems across users.\n- **OAuth / OIDC**: Supports many OAuth2 providers. Password auth can be disabled.\n- **Automatic backups**: Save to and restore from disk or S3-compatible storage.\n<!-- - **REST API**: Use or update your data in your own scripts and applications. -->\n\n## Architecture\n\nBeszel consists of two main components: the **hub** and the **agent**.\n\n- **Hub**: A web application built on [PocketBase](https://pocketbase.io/) that provides a dashboard for viewing and managing connected systems.\n- **Agent**: Runs on each system you want to monitor and communicates system metrics to the hub.\n\n## Getting started\n\nThe [quick start guide](https://beszel.dev/guide/getting-started) and other documentation is available on our website, [beszel.dev](https://beszel.dev). You'll be up and running in a few minutes.\n\n## Screenshots\n\n![Dashboard](https://beszel.dev/image/dashboard.png)\n![System page](https://beszel.dev/image/system-full.png)\n![Notification Settings](https://beszel.dev/image/settings-notifications.png)\n\n## Supported metrics\n\n- **CPU usage** - Host system and Docker / Podman containers.\n- **Memory usage** - Host system and containers. Includes swap and ZFS ARC.\n- **Disk usage** - Host system. Supports multiple partitions and devices.\n- **Disk I/O** - Host system. Supports multiple partitions and devices.\n- **Network usage** - Host system and containers.\n- **Load average** - Host system.\n- **Temperature** - Host system sensors.\n- **GPU usage / power draw** - Nvidia, AMD, and Intel.\n- **Battery** - Host system battery charge.\n- **Containers** - Status and metrics of all running Docker / Podman containers.\n- **S.M.A.R.T.** - Host system disk health.\n\n## Help and discussion\n\nPlease search existing issues and discussions before opening a new one. I try my best to respond, but may not always have time to do so.\n\n#### Bug reports and feature requests\n\nBug reports and feature requests can be posted on [GitHub issues](https://github.com/henrygd/beszel/issues).\n\n#### Support and general discussion\n\nSupport requests and general discussion can be posted on [GitHub discussions](https://github.com/henrygd/beszel/discussions) or the community-run [Matrix room](https://matrix.to/#/#beszel:matrix.org): `#beszel:matrix.org`.\n\n## License\n\nBeszel is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.\n",
      "stars_today": 39
    },
    {
      "id": 676934005,
      "name": "niri",
      "full_name": "YaLTeR/niri",
      "description": "A scrollable-tiling Wayland compositor.",
      "html_url": "https://github.com/YaLTeR/niri",
      "stars": 17339,
      "forks": 636,
      "language": "Rust",
      "topics": [
        "rust",
        "smithay",
        "tiling-window-manager",
        "wayland",
        "wayland-compositor"
      ],
      "created_at": "2023-08-10T10:53:14Z",
      "updated_at": "2026-01-14T22:34:47Z",
      "pushed_at": "2026-01-14T15:15:34Z",
      "open_issues": 447,
      "owner": {
        "login": "YaLTeR",
        "avatar_url": "https://avatars.githubusercontent.com/u/1794388?v=4"
      },
      "readme": "<h1 align=\"center\"><img alt=\"niri\" src=\"https://github.com/user-attachments/assets/07d05cd0-d5dc-4a28-9a35-51bae8f119a0\"></h1>\n<p align=\"center\">A scrollable-tiling Wayland compositor.</p>\n<p align=\"center\">\n    <a href=\"https://matrix.to/#/#niri:matrix.org\"><img alt=\"Matrix\" src=\"https://img.shields.io/badge/matrix-%23niri-blue?logo=matrix\"></a>\n    <a href=\"https://github.com/YaLTeR/niri/blob/main/LICENSE\"><img alt=\"GitHub License\" src=\"https://img.shields.io/github/license/YaLTeR/niri\"></a>\n    <a href=\"https://github.com/YaLTeR/niri/releases\"><img alt=\"GitHub Release\" src=\"https://img.shields.io/github/v/release/YaLTeR/niri?logo=github\"></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://yalter.github.io/niri/Getting-Started.html\">Getting Started</a> | <a href=\"https://yalter.github.io/niri/Configuration%3A-Introduction.html\">Configuration</a> | <a href=\"https://github.com/YaLTeR/niri/discussions/325\">Setup&nbsp;Showcase</a>\n</p>\n\n![niri with a few windows open](https://github.com/user-attachments/assets/535e6530-2f44-4b84-a883-1240a3eee6e9)\n\n## About\n\nWindows are arranged in columns on an infinite strip going to the right.\nOpening a new window never causes existing windows to resize.\n\nEvery monitor has its own separate window strip.\nWindows can never \"overflow\" onto an adjacent monitor.\n\nWorkspaces are dynamic and arranged vertically.\nEvery monitor has an independent set of workspaces, and there's always one empty workspace present all the way down.\n\nThe workspace arrangement is preserved across disconnecting and connecting monitors where it makes sense.\nWhen a monitor disconnects, its workspaces will move to another monitor, but upon reconnection they will move back to the original monitor.\n\n## Features\n\n- Built from the ground up for scrollable tiling\n- [Dynamic workspaces](https://yalter.github.io/niri/Workspaces.html) like in GNOME\n- An [Overview](https://github.com/user-attachments/assets/379a5d1f-acdb-4c11-b36c-e85fd91f0995) that zooms out workspaces and windows\n- Built-in screenshot UI\n- Monitor and window screencasting through xdg-desktop-portal-gnome\n    - You can [block out](https://yalter.github.io/niri/Configuration%3A-Window-Rules.html#block-out-from) sensitive windows from screencasts\n    - [Dynamic cast target](https://yalter.github.io/niri/Screencasting.html#dynamic-screencast-target) that can change what it shows on the go\n- [Touchpad](https://github.com/YaLTeR/niri/assets/1794388/946a910e-9bec-4cd1-a923-4a9421707515) and [mouse](https://github.com/YaLTeR/niri/assets/1794388/8464e65d-4bf2-44fa-8c8e-5883355bd000) gestures\n- Group windows into [tabs](https://yalter.github.io/niri/Tabs.html)\n- Configurable layout: gaps, borders, struts, window sizes\n- [Gradient borders](https://yalter.github.io/niri/Configuration%3A-Layout.html#gradients) with Oklab and Oklch support\n- [Animations](https://github.com/YaLTeR/niri/assets/1794388/ce178da2-af9e-4c51-876f-8709c241d95e) with support for [custom shaders](https://github.com/YaLTeR/niri/assets/1794388/27a238d6-0a22-4692-b794-30dc7a626fad)\n- Live-reloading config\n- Works with [screen readers](https://yalter.github.io/niri/Accessibility.html)\n\n## Video Demo\n\nhttps://github.com/YaLTeR/niri/assets/1794388/bce834b0-f205-434e-a027-b373495f9729\n\nAlso check out this video from Brodie Robertson that showcases a lot of the niri functionality: [Niri Is My New Favorite Wayland Compositor](https://youtu.be/DeYx2exm04M)\n\n## Status\n\nNiri is stable for day-to-day use and does most things expected of a Wayland compositor.\nMany people are daily-driving niri, and are happy to help in our [Matrix channel].\n\nGive it a try!\nFollow the instructions on the [Getting Started](https://yalter.github.io/niri/Getting-Started.html) page.\nHave your [waybar]s and [fuzzel]s ready: niri is not a complete desktop environment.\nAlso check out [awesome-niri], a list of niri-related links and projects.\n\nHere are some points you may have questions about:\n\n- **Multi-monitor**: yes, a core part of the design from the very start. Mixed DPI works.\n- **Fractional scaling**: yes, plus all niri UI stays pixel-perfect.\n- **NVIDIA**: seems to work fine.\n- **Floating windows**: yes, starting from niri 25.01.\n- **Input devices**: niri supports tablets, touchpads, and touchscreens.\nYou can map the tablet to a specific monitor, or use [OpenTabletDriver].\nWe have touchpad gestures, but no touchscreen gestures yet.\n- **Wlr protocols**: yes, we have most of the important ones like layer-shell, gamma-control, screencopy.\nYou can check on [wayland.app](https://wayland.app) at the bottom of each protocol's page.\n- **Performance**: while I run niri on beefy machines, I try to stay conscious of performance.\nI've seen someone use it fine on an EeeÂ PCÂ 900 fromÂ 2008, of all things.\n- **Xwayland**: [integrated](https://yalter.github.io/niri/Xwayland.html#using-xwayland-satellite) via xwayland-satellite starting from niri 25.08.\n\n## Media\n\n[niri: Making a Wayland compositor in Rust](https://youtu.be/Kmz8ODolnDg?list=PLRdS-n5seLRqrmWDQY4KDqtRMfIwU0U3T) Â· *December 2024*\n\nMy talk from the 2024 Moscow RustCon about niri, and how I do randomized property testing and profiling, and measure input latency.\nThe talk is in Russian, but I prepared full English subtitles that you can find in YouTube's subtitle language selector.\n\n[An interview with Ivan, the developer behind Niri](https://www.trommelspeicher.de/podcast/special_the_developer_behind_niri) Â· *June 2025*\n\nAn interview by a German tech podcast Das Triumvirat (in English).\nWe talk about niri development and history, and my experience building and maintaining niri.\n\n[A tour of the niri scrolling-tiling Wayland compositor](https://lwn.net/Articles/1025866/) Â· *July 2025*\n\nAn LWN article with a nice overview and introduction to niri.\n\n## Contributing\n\nIf you'd like to help with niri, there are plenty of both coding- and non-coding-related ways to do so.\nSee [CONTRIBUTING.md](https://github.com/YaLTeR/niri/blob/main/CONTRIBUTING.md) for an overview.\n\n## Inspiration\n\nNiri is heavily inspired by [PaperWM] which implements scrollable tiling on top of GNOME Shell.\n\nOne of the reasons that prompted me to try writing my own compositor is being able to properly separate the monitors.\nBeing a GNOME Shell extension, PaperWM has to work against Shell's global window coordinate space to prevent windows from overflowing.\n\n## Tile Scrollably Elsewhere\n\nHere are some other projects which implement a similar workflow:\n\n- [PaperWM]: scrollable tiling on top of GNOME Shell.\n- [karousel]: scrollable tiling on top of KDE.\n- [scroll](https://github.com/dawsers/scroll) and [papersway]: scrollable tiling on top of sway/i3.\n- [hyprscrolling] and [hyprslidr]: scrollable tiling on top of Hyprland.\n- [PaperWM.spoon]: scrollable tiling on top of macOS.\n\n## Contact\n\nOur main communication channel is a Matrix chat, feel free to join and ask a question: https://matrix.to/#/#niri:matrix.org\n\nWe also have a community Discord server: https://discord.gg/vT8Sfjy7sx\n\n[PaperWM]: https://github.com/paperwm/PaperWM\n[waybar]: https://github.com/Alexays/Waybar\n[fuzzel]: https://codeberg.org/dnkl/fuzzel\n[awesome-niri]: https://github.com/Vortriz/awesome-niri\n[karousel]: https://github.com/peterfajdiga/karousel\n[papersway]: https://spwhitton.name/tech/code/papersway/\n[hyprscrolling]: https://github.com/hyprwm/hyprland-plugins/tree/main/hyprscrolling\n[hyprslidr]: https://gitlab.com/magus/hyprslidr\n[PaperWM.spoon]: https://github.com/mogenson/PaperWM.spoon\n[Matrix channel]: https://matrix.to/#/#niri:matrix.org\n[OpenTabletDriver]: https://opentabletdriver.net/\n",
      "stars_today": 37
    },
    {
      "id": 854337508,
      "name": "spring-ai-alibaba",
      "full_name": "alibaba/spring-ai-alibaba",
      "description": "Agentic AI Framework for Java Developers",
      "html_url": "https://github.com/alibaba/spring-ai-alibaba",
      "stars": 7923,
      "forks": 1719,
      "language": "Java",
      "topics": [
        "agentic",
        "artificial-intelligence",
        "context-engineering",
        "graph",
        "java",
        "multi-agent",
        "reactagent",
        "spring-ai",
        "workflow"
      ],
      "created_at": "2024-09-09T01:35:50Z",
      "updated_at": "2026-01-15T00:41:43Z",
      "pushed_at": "2026-01-14T15:20:12Z",
      "open_issues": 327,
      "owner": {
        "login": "alibaba",
        "avatar_url": "https://avatars.githubusercontent.com/u/1961952?v=4"
      },
      "readme": "# [Spring AI Alibaba](https://java2ai.com)\n\n[![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)\n[![CI Status](https://github.com/alibaba/spring-ai-alibaba/workflows/%F0%9F%9B%A0%EF%B8%8F%20Build%20and%20Test/badge.svg)](https://github.com/alibaba/spring-ai-alibaba/actions?query=workflow%3A%22%F0%9F%9B%A0%EF%B8%8F+Build+and+Test%22)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/alibaba/spring-ai-alibaba)\n[![Maven central](https://img.shields.io/maven-central/v/com.alibaba.cloud.ai/spring-ai-alibaba.svg)](https://img.shields.io/maven-central/v/com.alibaba.cloud.ai/spring-ai-alibaba)\n<img alt=\"gitleaks badge\" src=\"https://img.shields.io/badge/protected%20by-gitleaks-blue\">\n\n<html>\n    <h3 align=\"center\">\n      A production-ready framework for building Agentic, Workflow, and Multi-agent applications.\n    </h3>\n    <h3 align=\"center\">\n      <a href=\"https://java2ai.com/docs/quick-start/\" target=\"_blank\">Agent Framework Docs</a>,\n      <a href=\"https://java2ai.com/docs/frameworks/graph-core/quick-start/\" target=\"_blank\">Graph Docs</a>,\n      <a href=\"https://java2ai.com/ecosystem/spring-ai/reference/concepts/\" target=\"_blank\">Spring AI</a>,\n      <a href=\"https://github.com/alibaba/spring-ai-alibaba/tree/main/examples\" target=\"_blank\">Examples</a>.\n    </h3>\n</html>\n\n## Architecture\n\n<p align=\"center\">\n    <img src=\"./docs/imgs/architecture-new.png\" alt=\"architecture\" style=\"max-width: 740px; height: auto\" />\n</p>\n\n**Spring AI Alibaba Admin** is a one-stop Agent platform that supports visualized Agent development, observability, evaluation, and MCP management, etc. It also integrates with open-source low-code platforms like Dify, enabling rapid migration from DSL to Spring AI Alibaba project.\n\n**Spring AI Alibaba Agent Framework** is an agent development framework that can quickly develop agents with builtin **Context Engineering** and **Human In The Loop** support. For scenarios requiring more complex process control, Agent Framework offers built-in workflows like `SequentialAgent`, `ParallelAgent`, `RoutingAgent`, `LoopAgent` and `SupervisorAgent`.\n\n**Spring AI Alibaba Graph** serves as the underlying runtime of the Agent Framework, providing essential capabilities such as persistence, workflow orchestration, and streaming required for long-running stateful agents. Compared to the Agent Framework, users can build more flexible multi-agent workflows based on the Graph API.\n\n## Core Features\n\n* **[Multi-Agent Orchestration](https://java2ai.com/docs/frameworks/agent-framework/advanced/multi-agent)**: Compose multiple agents with built-in patterns including `SequentialAgent`, `ParallelAgent`, `LlmRoutingAgent`, and `LoopAgent` for complex task execution.\n\n* **[Context Engineering](https://java2ai.com/docs/frameworks/agent-framework/tutorials/hooks)**: Built-in best practices for context engineering policies to improve agent reliability and performance, including human-in-the-loop, context compaction, context editing, model & tool call limit, tool retry, planning, dynamic tool selection.\n\n* **[Graph-based Workflow](https://java2ai.com/docs/frameworks/graph-core/quick-start)**: Graph based workflow runtime and api for conditional routing, nested graphs, parallel execution, and state management. Export workflows to PlantUML and Mermaid formats.\n\n* **[A2A Support](https://java2ai.com/docs/frameworks/agent-framework/advanced/a2a)**: Agent-to-Agent communication support with Nacos integration, enabling distributed agent coordination and collaboration across services.\n\n* **[Rich Model, Tool and MCP Support](https://java2ai.com/integration/chatmodels/dashScope)**: Leveraging core concepts of Spring AI, supports multiple LLM providers (DashScope, OpenAI, etc.), tool calling, and Model Context Protocol (MCP).\n\n* **[One-stop Agent Platform](https://java2ai.com/ecosystem/admin/quick-start)**: Build agent in a visualized way, deploy agent without code or export as a standalone java project.\n\n<p align=\"center\">\n    <img src=\"./docs/imgs/saa-admin.png\" alt=\"architecture\" style=\"max-width: 740px; height: auto\" />\n</p>\n\n## Getting Started\n\n### Prerequisites\n\n* Requires JDK 17+.\n* Choose your LLM provider and get the API-KEY.\n\n### Quickly Run a ChatBot\n\nThere's a ChatBot example provided by the community at [examples/chatbot](https://github.com/alibaba/spring-ai-alibaba/tree/main/examples/chatbot).\n\n1. Download the code.\n\n\t```shell\n\tgit clone --depth=1 https://github.com/alibaba/spring-ai-alibaba.git\n\tcd spring-ai-alibaba/examples/chatbot\n\t```\n\n2. Start the ChatBot.\n\n\tBefore starting, set API-KEY first (visit <a href=\"https://bailian.console.aliyun.com/?apiKey=1&tab=api#/api\" target=\"_blank\">Aliyun Bailian</a> to get API-KEY):\n\t```shell\n\t# this example uses 'spring-ai-alibaba-starter-dashscope', visit https://java2ai.com to learn how to use OpenAI/DeepSeek.\n\texport AI_DASHSCOPE_API_KEY=your-api-key\n\t```\n\t\n\t```shell\n\tmvn spring-boot:run\n\t```\n\n3. Chat with ChatBot.\n\n\tOpen the browser and visit [http://localhost:8080/chatui/index.html](http://localhost:8080/chatui/index.html) to chat with the ChatBot.\n\t\n<p align=\"center\">\n\t<img src=\"./docs/imgs/chatbot-chat-ui.gif\" alt=\"chatbot-ui\" style=\"max-width: 740px; height: auto\" />\n</p>\n\n## Chatbot Code Explained\n\n1. Add dependencies\n\n\t```xml\n\t<dependencies>\n\t  <dependency>\n\t    <groupId>com.alibaba.cloud.ai</groupId>\n\t    <artifactId>spring-ai-alibaba-agent-framework</artifactId>\n\t    <version>1.1.0.0</version>\n\t  </dependency>\n\t  <!-- Assume you are going to use DashScope Model. Refer to docs for how to choose model.-->\n\t  <dependency>\n\t    <groupId>com.alibaba.cloud.ai</groupId>\n\t    <artifactId>spring-ai-alibaba-starter-dashscope</artifactId>\n\t    <version>1.1.0.0</version>\n\t  </dependency>\n\t</dependencies>\n\t```\n\n2. Define Chatbot\n   \n\tFor more details of how to write a Chatbot, please check the [Quick Start](https://java2ai.com/docs/quick-start) on our official website.\n\n## ğŸ“š Documentation\n* [Overview](https://java2ai.com/docs/overview) - High level overview of the framework\n* [Quick Start](https://java2ai.com/docs/quick-start) - Get started with a simple agent\n* [Agent Framework Tutorials](https://java2ai.com/docs/frameworks/agent-framework/tutorials/agents) - Step by step tutorials\n* [Use Graph API to Build Complex Workflows](https://java2ai.com/docs/frameworks/agent-framework/advanced/context-engineering) - In-depth user guide for building multi-agent and workflows\n* [Spring AI Basics](https://java2ai.com/ecosystem/spring-ai/reference/concepts) - Ai Application basic concepts, including ChatModel, MCP, Tool, Messages, etc.\n\n## Project Structure\n\nThis project consists of several core components:\n\n* spring-ai-alibaba-agent-framework: A multi-agent framework designed for building intelligent agents with built-in context engineering best practices.\n* spring-ai-alibaba-graph: The underlying runtime for Agent Framework. We recommend developers to use Agent Framework but it's totally fine to use the Graph API directly.\n* spring-ai-alibaba-admin: A one-stop Agent platform that supports visualized Agent development, observability, evaluation, and MCP management, etc.\n* spring-ai-alibaba-studio: The embedded ui for quickly debugging agent in a visualized way.\n* spring-boot-starters: Starters integrating Agent Framework with Nacos to provide A2A and dynamic config features.\n\n## Spring AI Alibaba Ecosystem\n Repository | Description | â­\n  --- | --- | ---\n| [Spring AI Alibaba Graph](https://github.com/alibaba/spring-ai-alibaba/tree/main/spring-ai-alibaba-graph-core) | A low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. | ![GitHub Repo stars](https://img.shields.io/github/stars/alibaba/spring-ai-alibaba?style=for-the-badge&label=)\n| [Spring AI Alibaba Admin](https://github.com/spring-ai-alibaba/spring-ai-alibaba-admin) |  Local visualization toolkit for the development of agent applications, supporting project management, runtime visualization, tracing, and agent evaluation. | ![GitHub Repo stars](https://img.shields.io/github/stars/spring-ai-alibaba/spring-ai-alibaba-admin?style=for-the-badge&label=)\n| [Spring AI Extensions](https://github.com/spring-ai-alibaba/spring-ai-extensions) | Extended implementations for Spring AI core concepts, including DashScopeChatModel, MCP registry, etc. |  ![GitHub Repo stars](https://img.shields.io/github/stars/spring-ai-alibaba/spring-ai-extensions?style=for-the-badge&label=)\n| [Spring AI Alibaba Examples](https://github.com/spring-ai-alibaba/examples) | Spring AI Alibaba Examples. |  ![GitHub Repo stars](https://img.shields.io/github/stars/spring-ai-alibaba/examples?style=for-the-badge&label=)\n| [JManus](https://github.com/spring-ai-alibaba/jmanus) | A Java implementation of Manus built with Spring AI Alibaba, currently used in many applications within Alibaba Group. | ![GitHub Repo stars](https://img.shields.io/github/stars/spring-ai-alibaba/jmanus?style=for-the-badge&label=)\n| [DataAgent](https://github.com/spring-ai-alibaba/dataagent) | A natural language to SQL project based on Spring AI Alibaba, enabling you to query databases directly with natural language without writing complex SQL. | ![GitHub Repo stars](https://img.shields.io/github/stars/spring-ai-alibaba/dataagent?style=for-the-badge&label=)\n| [DeepResearch](https://github.com/spring-ai-alibaba/deepresearch) |  Deep Research implemented based on spring-ai-alibaba-graph. | ![GitHub Repo stars](https://img.shields.io/github/stars/spring-ai-alibaba/deepresearch?style=for-the-badge&label=)\n\n## Contact Us\n\n* Dingtalk Group (é’‰é’‰ç¾¤), search `130240015687` and join.\n* WeChat Group (å¾®ä¿¡å…¬ä¼—å·), scan the QR code below and follow us.\n\n<img src=\"./docs/imgs/wechat-account.jpg\" style=\"width: 260px; height: auto\"/>\n\n## Resources\n* [AI-Native Application Architecture White Paper](https://developer.aliyun.com/ebook/8479)ï¼šCo-authored by 40 frontline engineers and endorsed by 15 industry experts, this 200,000+ word white paper is the first comprehensive guide dedicated to the full DevOps lifecycle of AI-native applications. It systematically breaks down core concepts and key challenges, offering practical problem-solving approaches and architectural insights.\n\n\n## Star History\n\n[![Star History Chart](https://starchart.cc/alibaba/spring-ai-alibaba.svg?variant=adaptive)](https://starchart.cc/alibaba/spring-ai-alibaba)\n\n---\n\n<p align=\"center\">\n    Made with â¤ï¸ by the Spring AI Alibaba Team\n\n",
      "stars_today": 37
    },
    {
      "id": 72056048,
      "name": "fx",
      "full_name": "uber-go/fx",
      "description": "A dependency injection based application framework for Go.",
      "html_url": "https://github.com/uber-go/fx",
      "stars": 7254,
      "forks": 327,
      "language": "Go",
      "topics": [
        "app-framework",
        "dependency-injection",
        "framework",
        "go",
        "golang",
        "service"
      ],
      "created_at": "2016-10-27T00:25:00Z",
      "updated_at": "2026-01-15T00:57:17Z",
      "pushed_at": "2025-12-27T14:09:36Z",
      "open_issues": 69,
      "owner": {
        "login": "uber-go",
        "avatar_url": "https://avatars.githubusercontent.com/u/19262598?v=4"
      },
      "readme": "# :unicorn: Fx [![GoDoc](https://pkg.go.dev/badge/go.uber.org/fx)](https://pkg.go.dev/go.uber.org/fx) [![Github release](https://img.shields.io/github/release/uber-go/fx.svg)](https://github.com/uber-go/fx/releases) [![Build Status](https://github.com/uber-go/fx/actions/workflows/go.yml/badge.svg)](https://github.com/uber-go/fx/actions/workflows/go.yml) [![Coverage Status](https://codecov.io/gh/uber-go/fx/branch/master/graph/badge.svg)](https://codecov.io/gh/uber-go/fx/branch/master) [![Go Report Card](https://goreportcard.com/badge/go.uber.org/fx)](https://goreportcard.com/report/go.uber.org/fx)\n\nFx is a dependency injection system for Go.\n\n**Benefits**\n\n- Eliminate globals: Fx helps you remove global state from your application.\n  No more `init()` or global variables. Use Fx-managed singletons.\n- Code reuse: Fx lets teams within your organization build loosely-coupled\n  and well-integrated shareable components.\n- Battle tested: Fx is the backbone of nearly all Go services at Uber.\n\nSee our [docs](https://uber-go.github.io/fx/) to get started and/or\nlearn more about Fx.\n\n## Installation\n\nUse Go modules to install Fx in your application.\n\n```shell\ngo get go.uber.org/fx@v1\n```\n\n## Getting started\n\nTo get started with Fx, [start here](https://uber-go.github.io/fx/get-started/).\n\n## Stability\n\nThis library is `v1` and follows [SemVer](https://semver.org/) strictly.\n\nNo breaking changes will be made to exported APIs before `v2.0.0`.\n\nThis project follows the [Go Release Policy](https://golang.org/doc/devel/release.html#policy). Each major\nversion of Go is supported until there are two newer major releases.\n\n## Stargazers over time\n\n[![Stargazers over time](https://starchart.cc/uber-go/fx.svg)](https://starchart.cc/uber-go/fx)\n\n",
      "stars_today": 35
    },
    {
      "id": 292014229,
      "name": "zellij",
      "full_name": "zellij-org/zellij",
      "description": "A terminal workspace with batteries included",
      "html_url": "https://github.com/zellij-org/zellij",
      "stars": 28159,
      "forks": 914,
      "language": "Rust",
      "topics": [
        "multiplexer",
        "terminal",
        "workspace"
      ],
      "created_at": "2020-09-01T14:04:28Z",
      "updated_at": "2026-01-15T00:42:37Z",
      "pushed_at": "2026-01-14T16:13:41Z",
      "open_issues": 1438,
      "owner": {
        "login": "zellij-org",
        "avatar_url": "https://avatars.githubusercontent.com/u/73778475?v=4"
      },
      "readme": "<h1 align=\"center\">\n  <br>\n  <img src=\"https://raw.githubusercontent.com/zellij-org/zellij/main/assets/logo.png\" alt=\"logo\" width=\"200\">\n  <br>\n  Zellij\n  <br>\n  <br>\n</h1>\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/CrUAFH3\"><img alt=\"Discord Chat\" src=\"https://img.shields.io/discord/771367133715628073?color=5865F2&label=discord&style=flat-square\"></a>\n  <a href=\"https://matrix.to/#/#zellij_general:matrix.org\"><img alt=\"Matrix Chat\" src=\"https://img.shields.io/matrix/zellij_general:matrix.org?color=1d7e64&label=matrix%20chat&style=flat-square&logo=matrix\"></a>\n  <a href=\"https://zellij.dev/documentation/\"><img alt=\"Zellij documentation\" src=\"https://img.shields.io/badge/zellij-documentation-fc0060?style=flat-square\"></a>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/zellij-org/zellij/main/assets/demo.gif\" alt=\"demo\">\n</p>\n\n<h4 align=\"center\">\n  [<a href=\"https://zellij.dev/documentation/installation\">Installation</a>]\n  [<a href=\"https://zellij.dev/screencasts/\">Screencasts & Tutorials</a>]\n  [<a href=\"https://zellij.dev/documentation/configuration\">Configuration</a>]\n  [<a href=\"https://zellij.dev/documentation/layouts\">Layouts</a>]\n  [<a href=\"https://zellij.dev/documentation/faq\">FAQ</a>]\n</h4>\n\n# What is this?\n\n[Zellij](#origin-of-the-name) is a workspace aimed at developers, ops-oriented people and anyone who loves the terminal. Similar programs are sometimes called \"Terminal Multiplexers\".\n\nZellij is designed around the philosophy that one must not sacrifice simplicity for power, taking pride in its great experience out of the box as well as the advanced features it places at its users' fingertips.\n\nZellij is geared toward beginner and power users alike - allowing deep customizability, personal automation through [layouts](https://zellij.dev/documentation/layouts.html), true multiplayer collaboration, unique UX features such as floating and stacked panes, and a [plugin system](https://zellij.dev/documentation/plugins.html) allowing one to create plugins in any language that compiles to WebAssembly.\n\nZellij includes a built-in [web-client](https://zellij.dev/tutorials/web-client/), making a terminal optional.\n\nYou can get started by [installing](https://zellij.dev/documentation/installation.html) Zellij and checking out the [Screencasts & Tutorials](https://zellij.dev/screencasts/).\n\nFor more details about our future plans, read about upcoming features in our [roadmap](#roadmap).\n\n## How do I install it?\n\nThe easiest way to install Zellij is through a [package for your OS](./docs/THIRD_PARTY_INSTALL.md).\n\nIf one is not available for your OS, you could download a prebuilt binary from the [latest release](https://github.com/zellij-org/zellij/releases/latest) and place it in your `$PATH`. If you'd like, we could [automatically choose one for you](#try-zellij-without-installing).\n\nYou can also install (compile) with `cargo`:\n\n```\ncargo install --locked zellij\n```\n\n#### Try Zellij without installing\n\nbash/zsh:\n```bash\nbash <(curl -L https://zellij.dev/launch)\n```\nfish/xonsh:\n```bash\nbash -c 'bash <(curl -L https://zellij.dev/launch)'\n```\n\n#### Installing from `main`\nInstalling Zellij from the `main` branch is not recommended. This branch represents pre-release code, is constantly being worked on and may contain broken or unusable features. In addition, using it may corrupt the cache for future versions, forcing users to clear it before they can use the officially released version.\n\nThat being said - no-one will stop you from using it (and bug reports involving new features are greatly appreciated), but please consider using the latest release instead as detailed at the top of this section.\n\n## How do I start a development environment?\n\n* Clone the project\n* In the project folder, for debug builds run: `cargo xtask run`\n* To run all tests: `cargo xtask test`\n\nFor more build commands, see [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Configuration\nFor configuring Zellij, please see the [Configuration Documentation](https://zellij.dev/documentation/configuration.html).\n\n## About issues in this repository\nIssues in this repository, whether open or closed, do not necessarily indicate a problem or a bug in the software. They only indicate that the reporter wanted to communicate their experiences or thoughts to the maintainers. The Zellij maintainers do their best to go over and reply to all issue reports, but unfortunately cannot promise these will always be dealt with or even read. Your understanding is appreciated.\n\n## Roadmap\nPresented here is the project roadmap, divided into three main sections.\n\nThese are issues that are either being actively worked on or are planned for the near future.\n\n***If you'll click on the image, you'll be led to an SVG version of it on the website where you can directly click on every issue***\n\n[![roadmap](https://github.com/user-attachments/assets/bb55d213-4a68-4c84-ae72-7db5c9bf94fb)](https://zellij.dev/roadmap)\n\n## Origin of the Name\n[From Wikipedia, the free encyclopedia](https://en.wikipedia.org/wiki/Zellij)\n\nZellij (Arabic: Ø§Ù„Ø²Ù„ÙŠØ¬, romanized: zillÄ«j; also spelled zillij or zellige) is a style of mosaic tilework made from individually hand-chiseled tile pieces. The pieces were typically of different colours and fitted together to form various patterns on the basis of tessellations, most notably elaborate Islamic geometric motifs such as radiating star patterns composed of various polygons. This form of Islamic art is one of the main characteristics of architecture in the western Islamic world. It is found in the architecture of Morocco, the architecture of Algeria, early Islamic sites in Tunisia, and in the historic monuments of al-Andalus (in the Iberian Peninsula).\n\n## License\n\nMIT\n\n## Sponsored by\n<a href=\"https://terminaltrove.com/\"><img src=\"https://avatars.githubusercontent.com/u/121595180?s=200&v=4\" width=\"80px\"></a>\n",
      "stars_today": 32
    },
    {
      "id": 896924279,
      "name": "airi",
      "full_name": "moeru-ai/airi",
      "description": "ğŸ’–ğŸ§¸ Self hosted, you owned Grok Companion, a container of souls of waifu, cyber livings to bring them into our worlds, wishing to achieve Neuro-sama's altitude. Capable of realtime voice chat, Minecraft, Factorio playing. Web / macOS / Windows supported.",
      "html_url": "https://github.com/moeru-ai/airi",
      "stars": 16832,
      "forks": 1596,
      "language": "TypeScript",
      "topics": [
        "ai-companion",
        "ai-vtuber",
        "digital-life",
        "grok-companion",
        "live2d",
        "moeru-ai",
        "neuro-sama",
        "neurosama",
        "vrm",
        "vtuber"
      ],
      "created_at": "2024-12-01T16:33:36Z",
      "updated_at": "2026-01-15T00:57:53Z",
      "pushed_at": "2026-01-14T21:20:50Z",
      "open_issues": 47,
      "owner": {
        "login": "moeru-ai",
        "avatar_url": "https://avatars.githubusercontent.com/u/165476306?v=4"
      },
      "readme": "<picture>\n  <source\n    width=\"100%\"\n    srcset=\"./docs/content/public/banner-dark-1280x640.avif\"\n    media=\"(prefers-color-scheme: dark)\"\n  />\n  <source\n    width=\"100%\"\n    srcset=\"./docs/content/public/banner-light-1280x640.avif\"\n    media=\"(prefers-color-scheme: light), (prefers-color-scheme: no-preference)\"\n  />\n  <img width=\"250\" src=\"./docs/content/public/banner-light-1280x640.avif\" />\n</picture>\n\n<h1 align=\"center\">Project AIRI</h1>\n\n<p align=\"center\">Re-creating Neuro-sama, a soul container of AI waifu / virtual characters to bring them into our world.</p>\n\n<p align=\"center\">\n  [<a href=\"https://discord.gg/TgQ3Cu2F7A\">Join Discord Server</a>] [<a href=\"https://airi.moeru.ai\">Try it</a>] [<a href=\"https://github.com/moeru-ai/airi/blob/main/docs/README.zh-CN.md\">ç®€ä½“ä¸­æ–‡</a>] [<a href=\"https://github.com/moeru-ai/airi/blob/main/docs/README.ja-JP.md\">æ—¥æœ¬èª</a>] [<a href=\"https://github.com/moeru-ai/airi/blob/main/docs/README.ru-RU.md\">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a>] [<a href=\"https://github.com/moeru-ai/airi/blob/main/docs/README.vi.md\">Tiáº¿ng Viá»‡t</a>] [<a href=\"https://github.com/moeru-ai/airi/blob/main/docs/README.fr.md\">FranÃ§ais</a>]\n</p>\n\n<p align=\"center\">\n  <a href=\"https://deepwiki.com/moeru-ai/airi\"><img src=\"https://deepwiki.com/badge.svg\"></a>\n  <a href=\"https://github.com/moeru-ai/airi/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/moeru-ai/airi.svg?style=flat&colorA=080f12&colorB=1fa669\"></a>\n  <a href=\"https://discord.gg/TgQ3Cu2F7A\"><img src=\"https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2FTgQ3Cu2F7A%3Fwith_counts%3Dtrue&query=%24.approximate_member_count&suffix=%20members&logo=discord&logoColor=white&label=%20&color=7389D8&labelColor=6A7EC2\"></a>\n  <a href=\"https://x.com/proj_airi\"><img src=\"https://img.shields.io/badge/%40proj__airi-black?style=flat&logo=x&labelColor=%23101419&color=%232d2e30\"></a>\n  <a href=\"https://t.me/+7M_ZKO3zUHFlOThh\"><img src=\"https://img.shields.io/badge/Telegram-%235AA9E6?logo=telegram&labelColor=FFFFFF\"></a>\n  <a href=\"./docs/wechat.md\"><img src=\"https://img.shields.io/badge/WeChat-%2307C160?logo=wechat&logoColor=%2307C160&labelColor=FFFFFF\"></a>\n  <a href=\"https://qun.qq.com/universal-share/share?ac=1&authKey=9g00d%2BZS7nORzcJugNNddJ7rCghZTIR7fhXabGwch2S%2BG%2BKGIKwlN1N2nIqkh2jg&busi_data=eyJncm91cENvZGUiOiIxMDU4MTU2Njk3IiwidG9rZW4iOiJmcnkra1hWNFIxNytEcG0zcHRUdVJIaldlRDFxN0dzK080QWtvTEdOQjJkNEY2eUFta1g1clNpbkxSMS9FQWFYIiwidWluIjoiMTI2MDkwNzMzNSJ9&data=b1eJrwn3GVOUh7YIxZ7l9vHQo99HPmRxKPpMKlDCmfzx8Y57IXb2EZCMaOC9rVTd2U558qpNjwUYUWlPHxVHvg&svctype=4&tempid=h5_group_info\"><img src=\"https://img.shields.io/badge/QQ-%2312B7F5?logo=qq&labelColor=FFFFFF\"></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.producthunt.com/products/airi?embed=true&utm_source=badge-featured&utm_medium=badge&utm_source=badge-airi\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=993524&theme=neutral&t=1752696535380\" alt=\"AIRI - A&#0032;container&#0032;of&#0032;cyber&#0032;living&#0032;souls&#0044;&#0032;re&#0045;creation&#0032;of&#0032;Neuro&#0045;sama | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n  <a href=\"https://trendshift.io/repositories/14636\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/14636\" alt=\"moeru-ai%2Fairi | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n\n> Heavily inspired by [Neuro-sama](https://www.youtube.com/@Neurosama)\n\n> [!WARNING]\n> **Attention:** We **do not** have any officially minted cryptocurrency or token associated with this project. Please check the information and proceed with caution.\n\n> [!NOTE]\n>\n> We've got a whole dedicated organization [@proj-airi](https://github.com/proj-airi) for all the sub-projects born from Project AIRI. Check it out!\n>\n> RAG, memory system, embedded database, icons, Live2D utilities, and more!\n\n> [!TIP]\n> We have a translation project on [Crowdin](https://crowdin.com/project/proj-airi). If you find any inaccurate translations, feel free to contribute improvements there.\n> <a href=\"https://crowdin.com/project/proj-airi\" target=\"_blank\" rel=\"nofollow\"><img style=\"width: 140px; height: 40px;\" src=\"https://badges.crowdin.net/badge/light/crowdin-on-dark.png\" srcset=\"https://badges.crowdin.net/badge/light/crowdin-on-dark.png 1x, https://badges.crowdin.net/badge/light/crowdin-on-dark@2x.png 2x\" alt=\"Crowdin | Agile localization for tech companies\" width=\"140\" height=\"40\" /></a>\n\nHave you dreamed about having a cyber living being (cyber waifu, digital pet) or digital companion that could play with and talk to you?\n\nWith the power of modern large language models like [ChatGPT](https://chatgpt.com) and famous [Claude](https://claude.ai), asking a virtual being to roleplay and chat with us is already easy enough for everyone. Platforms like [Character.ai (a.k.a. c.ai)](https://character.ai) and [JanitorAI](https://janitorai.com/) as well as local playgrounds like [SillyTavern](https://github.com/SillyTavern/SillyTavern) are already good-enough solutions for a chat based or visual adventure game like experience.\n\n> But, what about the abilities to play games? And see what you are coding at? Chatting while playing games, watching videos, and is capable of doing many other things.\n\nPerhaps you know [Neuro-sama](https://www.youtube.com/@Neurosama) already. She is currently the best virtual streamer capable of playing games, chatting, and interacting with you and the participants. Some also call this kind of being \"digital human.\" **Sadly, as it's not open sourced, you cannot interact with her after her live streams go offline**.\n\nTherefore, this project, AIRI, offers another possibility here: **let you own your digital life, cyber living, easily, anywhere, anytime**.\n\n## DevLogs We Posted & Recent Updates\n\n- [DevLog @ 2026.01.01](https://airi.moeru.ai/docs/en/blog/DevLog-2026.01.01/) on January 1, 2026\n- [DevLog @ 2025.10.20](https://airi.moeru.ai/docs/en/blog/DevLog-2025.10.20/) on October 20, 2025\n- [DevLog @ 2025.08.05](https://airi.moeru.ai/docs/en/blog/DevLog-2025.08.05/) on August 5, 2025\n- [DevLog @ 2025.08.01](https://airi.moeru.ai/docs/en/blog/DevLog-2025.08.01/) on August 1, 2025\n- [DevLog @ 2025.07.18](https://airi.moeru.ai/docs/en/blog/DevLog-2025.07.18/) on July 18, 2025\n- [DreamLog 0x1](https://airi.moeru.ai/docs/en/blog/dreamlog-0x1/) on June 16, 2025\n- ...more on [documentation site](https://airi.moeru.ai/docs/en/)\n\n## What's So Special About This Project?\n\nUnlike the other AI driven VTuber open source projects, ã‚¢ã‚¤ãƒª was built with support of many Web technologies such as [WebGPU](https://www.w3.org/TR/webgpu/), [WebAudio](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API), [Web Workers](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers), [WebAssembly](https://webassembly.org/), [WebSocket](https://developer.mozilla.org/en-US/docs/Web/API/WebSocket), etc. from the first day.\n\n> [!TIP]\n> Worrying about the performance drop since we are using Web related technologies?\n>\n> Don't worry, while Web browser version is meant to give an insight about how much we can push and do inside browsers, and webviews, we will never fully rely on this, the desktop version of AIRI is capable of using native [NVIDIA CUDA](https://developer.nvidia.com/cuda-toolkit) and [Apple Metal](https://developer.apple.com/metal/) by default (thanks to HuggingFace & beloved [candle](https://github.com/huggingface/candle) project), without any complex dependency managements, considering the tradeoff, it was partially powered by Web technologies for graphics, layouts, animations, and the WIP plugin systems for everyone to integrate things.\n\nThis means that **ã‚¢ã‚¤ãƒª is capable of running on modern browsers and devices** and even on mobile devices (already done with PWA support). This brings a lot of possibilities for us (the developers) to build and extend the power of ã‚¢ã‚¤ãƒª VTuber to the next level, while still leaving the flexibilities for users to enable features that requires TCP connections or other non-Web technologies such as connecting to a Discord voice channel or playing Minecraft and Factorio with friends.\n\n> [!NOTE]\n>\n> We are still in the early stage of development where we are seeking out talented developers to join us and help us to make ã‚¢ã‚¤ãƒª a reality.\n>\n> It's ok if you are not familiar with Vue.js, TypeScript, and devtools required for this project, you can join us as an artist, designer, or even help us to launch our first live stream.\n>\n> Even if you are a big fan of React, Svelte or even Solid, we welcome you. You can open a sub-directory to add features that you want to see in ã‚¢ã‚¤ãƒª, or would like to experiment with.\n>\n> Fields (and related projects) that we are looking for:\n>\n> - Live2D modeller\n> - VRM modeller\n> - VRChat avatar designer\n> - Computer Vision\n> - Reinforcement Learning\n> - Speech Recognition\n> - Speech Synthesis\n> - ONNX Runtime\n> - Transformers.js\n> - vLLM\n> - WebGPU\n> - Three.js\n> - WebXR ([checkout the another project](https://github.com/moeru-ai/chat) we have under the @moeru-ai organization)\n>\n> **If you are interested, why not introduce yourself here? [Would like to join part of us to build AIRI?](https://github.com/moeru-ai/airi/discussions/33)**\n\n## Current Progress\n\nCapable of\n\n- [x] Brain\n  - [x] Play [Minecraft](https://www.minecraft.net)\n  - [x] Play [Factorio](https://www.factorio.com) (WIP, but [PoC and demo available](https://github.com/moeru-ai/airi-factorio))\n  - [x] Chat in [Telegram](https://telegram.org)\n  - [x] Chat in [Discord](https://discord.com)\n  - [ ] Memory\n    - [x] Pure in-browser database support (DuckDB WASM | `pglite`)\n    - [ ] Memory Alaya (WIP)\n  - [ ] Pure in-browser local (WebGPU) inference\n- [x] Ears\n  - [x] Audio input from browser\n  - [x] Audio input from [Discord](https://discord.com)\n  - [x] Client side speech recognition\n  - [x] Client side talking detection\n- [x] Mouth\n  - [x] [ElevenLabs](https://elevenlabs.io/) voice synthesis\n- [x] Body\n  - [x] VRM support\n    - [x] Control VRM model\n  - [x] VRM model animations\n    - [x] Auto blink\n    - [x] Auto look at\n    - [x] Idle eye movement\n  - [x] Live2D support\n    - [x] Control Live2D model\n  - [x] Live2D model animations\n    - [x] Auto blink\n    - [x] Auto look at\n    - [x] Idle eye movement\n\n## Development\n\n> For detailed instructions to develop this project, follow [CONTRIBUTING.md](./.github/CONTRIBUTING.md)\n\n> [!NOTE]\n> By default, `pnpm dev` will start the development server for the Stage Web (browser version). If you would\n> like to try developing the desktop version, please make sure you read [CONTRIBUTING.md](./.github/CONTRIBUTING.md)\n> to setup the environment correctly.\n\n```shell\npnpm i\npnpm dev\n```\n\n### Stage Web (Browser Version at [airi.moeru.ai](https://airi.moeru.ai))\n\n```shell\npnpm dev\n```\n\n### Stage Tamagotchi (Desktop Version)\n\n```shell\npnpm dev:tamagotchi\n```\n\nA Nix package for Tamagotchi is included. To run airi with Nix, first make sure to enable flakes, then run:\n\n```shell\nnix run github:moeru-ai/airi\n```\n\n### Stage Capacitor (Mobile Version)\n\nStart the development server for the capacitor web version:\n\n```shell\npnpm dev:capacitor\n```\n\nCheck your IP address in the output of the command above:\n\n```shell\n  ROLLDOWN-VITE v7.3.0  ready in 1073 ms\n\n  âœ  Local:   https://localhost:5273/\n  âœ  Network: https://<ip-will-be-here>:5273/\n  âœ  Vue DevTools: Open https://localhost:5273/__devtools__/ as a separate window\n  âœ  Vue DevTools: Press Option(âŒ¥)+Shift(â‡§)+D in App to toggle the Vue DevTools\n  âœ  UnoCSS Inspector: https://localhost:5273/__unocss/\n```\n\nOpen the Xcode project:\n\n```shell\nCAPACITOR_DEV_SERVER_URL=https://<your-ip-address>:5273 pnpm open:ios\n```\n\nThen Xcode will open and you can click the \"Run\" button to run the app on your iPhone.\n\n### Documentation Site\n\n```shell\npnpm dev:docs\n```\n\n### Publish\n\nPlease update the version in `Cargo.toml` after running `bumpp`:\n\n```shell\nnpx bumpp --no-commit --no-tag\n```\n\n## Support of LLM API Providers (powered by [xsai](https://github.com/moeru-ai/xsai))\n\n- [x] [302.AI (sponsored)](https://share.302.ai/514k2v)\n- [x] [OpenRouter](https://openrouter.ai/)\n- [x] [vLLM](https://github.com/vllm-project/vllm)\n- [x] [SGLang](https://github.com/sgl-project/sglang)\n- [x] [Ollama](https://github.com/ollama/ollama)\n- [x] [Google Gemini](https://developers.generativeai.google)\n- [x] [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)\n  - [ ] [Azure OpenAI API](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference) (PR welcome)\n- [x] [Anthropic Claude](https://anthropic.com)\n  - [ ] [AWS Claude](https://docs.anthropic.com/en/api/claude-on-amazon-bedrock) (PR welcome)\n- [x] [DeepSeek](https://www.deepseek.com/)\n- [x] [Qwen](https://help.aliyun.com/document_detail/2400395.html)\n- [x] [xAI](https://x.ai/)\n- [x] [Groq](https://wow.groq.com/)\n- [x] [Mistral](https://mistral.ai/)\n- [x] [Cloudflare Workers AI](https://developers.cloudflare.com/workers-ai/)\n- [x] [Together.ai](https://www.together.ai/)\n- [x] [Fireworks.ai](https://www.together.ai/)\n- [x] [Novita](https://www.novita.ai/)\n- [x] [Zhipu](https://bigmodel.cn)\n- [x] [SiliconFlow](https://cloud.siliconflow.cn/i/rKXmRobW)\n- [x] [Stepfun](https://platform.stepfun.com/)\n- [x] [Baichuan](https://platform.baichuan-ai.com)\n- [x] [Minimax](https://api.minimax.chat/)\n- [x] [Moonshot AI](https://platform.moonshot.cn/)\n- [x] [ModelScope](https://modelscope.cn/docs/model-service/API-Inference/intro)\n- [x] [Player2](https://player2.game/)\n- [x] [Tencent Cloud](https://cloud.tencent.com/document/product/1729)\n- [ ] [Sparks](https://www.xfyun.cn/doc/spark/Web.html) (PR welcome)\n- [ ] [Volcano Engine](https://www.volcengine.com/experience/ark?utm_term=202502dsinvite&ac=DSASUQY5&rc=2QXCA1VI) (PR welcome)\n\n## Sub-projects Born from This Project\n\n- [Awesome AI VTuber](https://github.com/proj-airi/awesome-ai-vtuber): A curated list of AI VTubers and related projects\n- [`unspeech`](https://github.com/moeru-ai/unspeech): Universal endpoint proxy server for `/audio/transcriptions` and `/audio/speech`, like LiteLLM but for any ASR and TTS\n- [`hfup`](https://github.com/moeru-ai/hfup): tools to help on deploying, bundling to HuggingFace Spaces\n- [`xsai-transformers`](https://github.com/moeru-ai/xsai-transformers): Experimental [ğŸ¤— Transformers.js](https://github.com/huggingface/transformers.js) provider for [xsAI](https://github.com/moeru-ai/xsai).\n- [WebAI: Realtime Voice Chat](https://github.com/proj-airi/webai-realtime-voice-chat): Full example of implementing ChatGPT's realtime voice from scratch with VAD + STT + LLM + TTS.\n- [`@proj-airi/drizzle-duckdb-wasm`](https://github.com/moeru-ai/airi/tree/main/packages/drizzle-duckdb-wasm/README.md): Drizzle ORM driver for DuckDB WASM\n- [`@proj-airi/duckdb-wasm`](https://github.com/moeru-ai/airi/tree/main/packages/duckdb-wasm/README.md): Easy to use wrapper for `@duckdb/duckdb-wasm`\n- [`tauri-plugin-mcp`](https://github.com/moeru-ai/airi/blob/main/crates/tauri-plugin-mcp/README.md): A Tauri plugin for interacting with MCP servers.\n- [AIRI Factorio](https://github.com/moeru-ai/airi-factorio): Allow AIRI to play Factorio\n- [Factorio RCON API](https://github.com/nekomeowww/factorio-rcon-api): RESTful API wrapper for Factorio headless server console\n- [`autorio`](https://github.com/moeru-ai/airi-factorio/tree/main/packages/autorio): Factorio automation library\n- [`tstl-plugin-reload-factorio-mod`](https://github.com/moeru-ai/airi-factorio/tree/main/packages/tstl-plugin-reload-factorio-mod): Reload Factorio mod when developing\n- [Velin](https://github.com/luoling8192/velin): Use Vue SFC and Markdown to write easy to manage stateful prompts for LLM\n- [`demodel`](https://github.com/moeru-ai/demodel): Easily boost the speed of pulling your models and datasets from various of inference runtimes.\n- [`inventory`](https://github.com/moeru-ai/inventory): Centralized model catalog and default provider configurations backend service\n- [MCP Launcher](https://github.com/moeru-ai/mcp-launcher): Easy to use MCP builder & launcher for all possible MCP servers, just like Ollama for models!\n- [ğŸ¥º SAD](https://github.com/moeru-ai/sad): Documentation and notes for self-host and browser running LLMs.\n\n```mermaid\n%%{ init: { 'flowchart': { 'curve': 'catmullRom' } } }%%\n\nflowchart TD\n  Core(\"Core\")\n  Unspeech(\"unspeech\")\n  DBDriver(\"@proj-airi/drizzle-duckdb-wasm\")\n  MemoryDriver(\"[WIP] Memory Alaya\")\n  DB1(\"@proj-airi/duckdb-wasm\")\n  SVRT(\"@proj-airi/server-runtime\")\n  Memory(\"Memory\")\n  STT(\"STT\")\n  Stage(\"Stage\")\n  StageUI(\"@proj-airi/stage-ui\")\n  UI(\"@proj-airi/ui\")\n\n  subgraph AIRI\n    DB1 --> DBDriver --> MemoryDriver --> Memory --> Core\n    UI --> StageUI --> Stage --> Core\n    Core --> STT\n    Core --> SVRT\n  end\n\n  subgraph UI_Components\n    UI --> StageUI\n    UITransitions(\"@proj-airi/ui-transitions\") --> StageUI\n    UILoadingScreens(\"@proj-airi/ui-loading-screens\") --> StageUI\n    FontCJK(\"@proj-airi/font-cjkfonts-allseto\") --> StageUI\n    FontXiaolai(\"@proj-airi/font-xiaolai\") --> StageUI\n  end\n\n  subgraph Apps\n    Stage --> StageWeb(\"@proj-airi/stage-web\")\n    Stage --> StageTamagotchi(\"@proj-airi/stage-tamagotchi\")\n    Core --> RealtimeAudio(\"@proj-airi/realtime-audio\")\n    Core --> PromptEngineering(\"@proj-airi/playground-prompt-engineering\")\n  end\n\n  subgraph Server_Components\n    Core --> ServerSDK(\"@proj-airi/server-sdk\")\n    ServerShared(\"@proj-airi/server-shared\") --> SVRT\n    ServerShared --> ServerSDK\n  end\n\n  STT -->|Speaking| Unspeech\n  SVRT -->|Playing Factorio| F_AGENT\n  SVRT -->|Playing Minecraft| MC_AGENT\n\n  subgraph Factorio_Agent\n    F_AGENT(\"Factorio Agent\")\n    F_API(\"Factorio RCON API\")\n    factorio-server(\"factorio-server\")\n    F_MOD1(\"autorio\")\n\n    F_AGENT --> F_API -.-> factorio-server\n    F_MOD1 -.-> factorio-server\n  end\n\n  subgraph Minecraft_Agent\n    MC_AGENT(\"Minecraft Agent\")\n    Mineflayer(\"Mineflayer\")\n    minecraft-server(\"minecraft-server\")\n\n    MC_AGENT --> Mineflayer -.-> minecraft-server\n  end\n\n  XSAI(\"xsAI\") --> Core\n  XSAI --> F_AGENT\n  XSAI --> MC_AGENT\n\n  Core --> TauriMCP(\"@proj-airi/tauri-plugin-mcp\")\n  Memory_PGVector(\"@proj-airi/memory-pgvector\") --> Memory\n\n  style Core fill:#f9d4d4,stroke:#333,stroke-width:1px\n  style AIRI fill:#fcf7f7,stroke:#333,stroke-width:1px\n  style UI fill:#d4f9d4,stroke:#333,stroke-width:1px\n  style Stage fill:#d4f9d4,stroke:#333,stroke-width:1px\n  style UI_Components fill:#d4f9d4,stroke:#333,stroke-width:1px\n  style Server_Components fill:#d4e6f9,stroke:#333,stroke-width:1px\n  style Apps fill:#d4d4f9,stroke:#333,stroke-width:1px\n  style Factorio_Agent fill:#f9d4f2,stroke:#333,stroke-width:1px\n  style Minecraft_Agent fill:#f9d4f2,stroke:#333,stroke-width:1px\n\n  style DBDriver fill:#f9f9d4,stroke:#333,stroke-width:1px\n  style MemoryDriver fill:#f9f9d4,stroke:#333,stroke-width:1px\n  style DB1 fill:#f9f9d4,stroke:#333,stroke-width:1px\n  style Memory fill:#f9f9d4,stroke:#333,stroke-width:1px\n  style Memory_PGVector fill:#f9f9d4,stroke:#333,stroke-width:1px\n```\n\n## Similar Projects\n\n### Open sourced ones\n\n- [kimjammer/Neuro: A recreation of Neuro-Sama originally created in 7 days.](https://github.com/kimjammer/Neuro): very well completed implementation.\n- [SugarcaneDefender/z-waif](https://github.com/SugarcaneDefender/z-waif): Great at gaming, autonomous, and prompt engineering\n- [semperai/amica](https://github.com/semperai/amica/): Great at VRM, WebXR\n- [elizaOS/eliza](https://github.com/elizaOS/eliza): Great examples and software engineering on how to integrate agent into various of systems and APIs\n- [ardha27/AI-Waifu-Vtuber](https://github.com/ardha27/AI-Waifu-Vtuber): Great about Twitch API integrations\n- [InsanityLabs/AIVTuber](https://github.com/InsanityLabs/AIVTuber): Nice UI and UX\n- [IRedDragonICY/vixevia](https://github.com/IRedDragonICY/vixevia)\n- [t41372/Open-LLM-VTuber](https://github.com/t41372/Open-LLM-VTuber)\n- [PeterH0323/Streamer-Sales](https://github.com/PeterH0323/Streamer-Sales)\n\n### Non-open-sourced ones\n\n- https://clips.twitch.tv/WanderingCaringDeerDxCat-Qt55xtiGDSoNmDDr https://www.youtube.com/watch?v=8Giv5mupJNE\n- https://clips.twitch.tv/TriangularAthleticBunnySoonerLater-SXpBk1dFso21VcWD\n- https://www.youtube.com/@NOWA_Mirai\n\n## Project Status\n\n![Repobeats analytics image](https://repobeats.axiom.co/api/embed/a1d6fe2c13ea2bb53a5154435a71e2431f70c2ee.svg 'Repobeats analytics image')\n\n## Acknowledgements\n\n- [Reka UI](https://github.com/unovue/reka-ui): for designing the documentation site, the new landing page is based on this, as well as implementing a massive amount of UI components. (shadcn-vue is using Reka UI as the headless, do checkout!)\n- [pixiv/ChatVRM](https://github.com/pixiv/ChatVRM)\n- [josephrocca/ChatVRM-js: A JS conversion/adaptation of parts of the ChatVRM (TypeScript) code for standalone use in OpenCharacters and elsewhere](https://github.com/josephrocca/ChatVRM-js)\n- Design of UI and style was inspired by [Cookard](https://store.steampowered.com/app/2919650/Cookard/), [UNBEATABLE](https://store.steampowered.com/app/2240620/UNBEATABLE/), and [Sensei! I like you so much!](https://store.steampowered.com/app/2957700/_/), and artworks of [Ayame by Mercedes Bazan](https://dribbble.com/shots/22157656-Ayame) with [Wish by Mercedes Bazan](https://dribbble.com/shots/24501019-Wish)\n- [mallorbc/whisper_mic](https://github.com/mallorbc/whisper_mic)\n- [`xsai`](https://github.com/moeru-ai/xsai): Implemented a decent amount of packages to interact with LLMs and models, like [Vercel AI SDK](https://sdk.vercel.ai/) but way small.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=moeru-ai/airi&type=Date)](https://www.star-history.com/#moeru-ai/airi&Date)\n",
      "stars_today": 32
    },
    {
      "id": 110446844,
      "name": "marktext",
      "full_name": "marktext/marktext",
      "description": "ğŸ“A simple and elegant markdown editor, available for Linux, macOS and Windows.",
      "html_url": "https://github.com/marktext/marktext",
      "stars": 53227,
      "forks": 3934,
      "language": "JavaScript",
      "topics": [
        "dark-mode",
        "editor",
        "electron",
        "element-ui",
        "emoji",
        "focus-mode",
        "latex",
        "linux",
        "mac",
        "macos",
        "markdown",
        "marktext",
        "next-generation",
        "source-code",
        "typewriter-mode",
        "vue",
        "windows"
      ],
      "created_at": "2017-11-12T16:05:25Z",
      "updated_at": "2026-01-15T00:50:20Z",
      "pushed_at": "2025-11-19T09:20:03Z",
      "open_issues": 1376,
      "owner": {
        "login": "marktext",
        "avatar_url": "https://avatars.githubusercontent.com/u/36623013?v=4"
      },
      "readme": "<p align=\"center\"><img src=\"static/logo-small.png\" alt=\"MarkText\" width=\"100\" height=\"100\"></p>\n\n<h1 align=\"center\">MarkText</h1>\n\n<div align=\"center\">\n  <a href=\"https://twitter.com/intent/tweet?via=marktextme&url=https://github.com/marktext/marktext/&text=What%20do%20you%20want%20to%20say%20to%20app?&hashtags=happyMarkText\">\n    <img src=\"https://img.shields.io/twitter/url/https/github.com/marktext/marktext.svg?style=for-the-badge\" alt=\"twitter\">\n  </a>\n</div>\n<div align=\"center\">\n  <strong>:high_brightness: Next generation markdown editor :crescent_moon:</strong><br>\n  A simple and elegant open-source markdown editor that focused on speed and usability.<br>\n  <sub>Available for Linux, macOS and Windows.</sub>\n</div>\n\n<br>\n\n<div align=\"center\">\n  <!-- License -->\n  <a href=\"LICENSE\">\n    <img src=\"https://img.shields.io/github/license/marktext/marktext.svg\" alt=\"LICENSE\">\n  </a>\n  <!-- Build Status -->\n  <a href=\"https://travis-ci.org/marktext/marktext/\">\n    <img src=\"https://travis-ci.org/marktext/marktext.svg?branch=master\" alt=\"build\">\n  </a>\n  <a href=\"https://ci.appveyor.com/project/marktext/marktext/branch/master\">\n    <img src=\"https://ci.appveyor.com/api/projects/status/l4gxgydj0i95hmxg/branch/master?svg=true\" alt=\"build\">\n  </a>\n  <!-- Downloads total -->\n  <a href=\"https://github.com/marktext/marktext/releases\">\n    <img src=\"https://img.shields.io/github/downloads/marktext/marktext/total.svg\" alt=\"total download\">\n  </a>\n  <!-- Downloads latest release -->\n  <a href=\"https://github.com/marktext/marktext/releases/latest\">\n    <img src=\"https://img.shields.io/github/downloads/marktext/marktext/v0.17.1/total.svg\" alt=\"latest download\">\n  </a>\n  <!-- sponsors -->\n  <a href=\"https://opencollective.com/marktext\">\n    <img src=\"https://opencollective.com/marktext/tiers/silver-sponsors/badge.svg?label=SilverSponsors&color=brightgreen\" alt=\"sponsors\">\n  </a>\n</div>\n\n<div align=\"center\">\n  <h3>\n    <a href=\"https://github.com/marktext/marktext\">\n      Website\n    </a>\n    <span> | </span>\n    <a href=\"https://github.com/marktext/marktext#features\">\n      Features\n    </a>\n    <span> | </span>\n    <a href=\"https://github.com/marktext/marktext#download-and-installation\">\n      Downloads\n    </a>\n    <span> | </span>\n    <a href=\"https://github.com/marktext/marktext#development\">\n      Development\n    </a>\n    <span> | </span>\n    <a href=\"https://github.com/marktext/marktext#contribution\">\n      Contribution\n    </a>\n  </h3>\n</div>\n\n<div align=\"center\">\n  <sub>Translations:</sub>\n  <a href=\"docs/i18n/zh_cn.md#readme\">\n    <span>:cn:</span>\n  </a>\n  <a href=\"docs/i18n/zh_tw.md#readme\">\n    <span>:taiwan:</span>\n  </a>\n  <a href=\"docs/i18n/pl.md#readme\">\n    <span>:poland:</span>\n  </a>\n  <a href=\"docs/i18n/ja.md#readme\">\n    <span>:jp:</span>\n  </a>\n  <a href=\"docs/i18n/french.md#readme\">\n    <span>:fr:</span>\n  </a>\n  <a href=\"docs/i18n/tr.md#readme\">\n    <span>:tr:</span>\n  </a>\n  <a href=\"docs/i18n/spanish.md#readme\">\n    <span>:es:</span>\n  </a>\n  <a href=\"docs/i18n/pt.md#readme\">\n    <span>:portugal:</span>\n  </a>\n  <a href=\"docs/i18n/ko.md#readme\">\n    <span>:kr:</span>\n  </a>\n</div>\n\n<div align=\"center\">\n  <sub>This Markdown editor that could. Built with â¤ï¸ by\n    <a href=\"https://github.com/Jocs\">Jocs</a> and\n    <a href=\"https://github.com/marktext/marktext/graphs/contributors\">\n      contributors\n    </a>\n    .\n  </sub>\n</div>\n\n<br />\n\n<h2 align=\"center\">Supporting MarkText</h2>\n\nMarkText is an MIT licensed open source project, and the latest version will always be downloadable for free from the GitHub release page. MarkText is still in development, and its development is inseparable from all sponsors. I hope you join them:\n\n- [Become a backer or sponsor on Patreon](https://www.patreon.com/ranluo) or [One time donation](https://github.com/Jocs/sponsor.me)\n- [Become a backer or sponsor on Open Collective](https://opencollective.com/marktext)\n\n##### What's the difference between Patreon and Open Collective?\n\nPatreon: Funds will be directly sponsored to Luo Ran (@jocs) who created MarkText and continues to maintain it.\nOpen Collective: All expenses are transparent. The funds will be used for the development and maintenance of MarkText, funding online and offline activities, and acquiring other necessary resources.\nNames and company logos of all sponsors (from both Patreon and Open Collective) will appear on the official website for MarkText and in its README.md file.\n\n**Looking for MarkText-like editing with cloud storage? try Inkio**\n\n<a href=\"https://inkio.me/\" target=\"_blank\">\n <img src=\"https://inkio.me/static/media/logo.35f605dc31b1a0615087.png\" width=\"100\">\n</a>\n\n**Platinum Sponsors**\n\n<a href=\"https://opencollective.com/marktext#platinum-sponsors\">\n <img src=\"https://opencollective.com/marktext/tiers/platinum-sponsors.svg?avatarHeight=36&width=600\">\n</a>\n\n**Gold Sponsors**\n\n<a href=\"https://opencollective.com/marktext#platinum-sponsors\">\n  <img src=\"https://opencollective.com/marktext/tiers/gold-sponsors.svg?avatarHeight=36&width=600\">\n</a>\n\n**Silver Sponsors**\n\n<a href=\"https://opencollective.com/marktext#platinum-sponsors\">\n  <img src=\"https://opencollective.com/marktext/tiers/silver-sponsors.svg?avatarHeight=36&width=600\">\n</a>\n\n**Bronze Sponsors**\n\n<a href=\"https://opencollective.com/marktext#platinum-sponsors\">\n  <img src=\"https://opencollective.com/marktext/tiers/bronze-sponsors.svg?avatarHeight=36&width=600\">\n</a>\n\n**Backers**\n\n<a href=\"https://opencollective.com/marktext#backers\">\n  <img src=\"https://opencollective.com/marktext/tiers/backer.svg?avatarHeight=36&width=600\">\n</a>\n\n## Screenshot\n\n![](docs/marktext.png?raw=true)\n\n## Features\n\n- Realtime preview (WYSIWYG) and a clean and simple interface to get a distraction-free writing experience.\n- Support [CommonMark Spec](https://spec.commonmark.org/0.29/), [GitHub Flavored Markdown Spec](https://github.github.com/gfm/) and selective support [Pandoc markdown](https://pandoc.org/MANUAL.html#pandocs-markdown).\n- Markdown extensions such as math expressions (KaTeX), front matter and emojis.\n- Support paragraphs and inline style shortcuts to improve your writing efficiency.\n- Output **HTML** and **PDF** files.\n- Various themes: **Cadmium Light**, **Material Dark** etc.\n- Various editing modes: **Source Code mode**, **Typewriter mode**, **Focus mode**.\n- Paste images directly from clipboard.\n\n<h4 align=\"center\">:crescent_moon:themes:high_brightness:</h4>\n\n| Cadmium Light                                     | Dark                                            |\n|:-------------------------------------------------:|:-----------------------------------------------:|\n| ![](docs/themeImages/cadmium-light.png?raw=true)  | ![](docs/themeImages/dark.png?raw=true)         |\n| Graphite Light                                    | Material Dark                                   |\n| ![](docs/themeImages/graphite-light.png?raw=true) | ![](docs/themeImages/materal-dark.png?raw=true) |\n| Ulysses Light                                     | One Dark                                        |\n| ![](docs/themeImages/ulysses-light.png?raw=true)  | ![](docs/themeImages/one-dark.png?raw=true)     |\n\n<h4 align=\"center\">:smile_cat:Edit modes:dog:</h4>\n\n| Source Code          | Typewriter               | Focus               |\n|:--------------------:|:------------------------:|:-------------------:|\n| ![](docs/source.gif) | ![](docs/typewriter.gif) | ![](docs/focus.gif) |\n\n## Why make another editor?\n\n1. I love writing. I have used a lot of markdown editors, yet there is still not an editor that can fully meet my needs. I don't like to be disturbed when I write by some unbearable bug. **MarkText** uses virtual DOM to render pages which has the added benefits of being highly efficient and being open source. That way anyone who loves markdown and writing can use MarkText.\n2. As mentioned above, **MarkText** is completely free and open source and will be open source forever. We hope that all markdown lovers will contribute their own code and help develop **MarkText** into a popular markdown editor.\n3. There are many markdown editors and all have their own merits, some have features which others don't. It's difficult to satisfy each markdown users' needs but we hope **MarkText** will be able to satisfy each markdown user as much as possible. Although the latest **MarkText** is still not perfect, we will try to make it as best as we possibly can.\n\n## Download and Installation\n\n![platform](https://img.shields.io/static/v1.svg?label=Platform&message=Linux-64%20|%20macOS-64%20|%20Win-32%20|%20Win-64&style=for-the-badge)\n\n| ![](https://raw.githubusercontent.com/wiki/ryanoasis/nerd-fonts/screenshots/v1.0.x/mac-pass-sm.png)                                                                                                  | ![](https://raw.githubusercontent.com/wiki/ryanoasis/nerd-fonts/screenshots/v1.0.x/windows-pass-sm.png)                                                                                                          | ![](https://raw.githubusercontent.com/wiki/ryanoasis/nerd-fonts/screenshots/v1.0.x/linux-pass-sm.png)                                                                                                                        |\n|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n| [![latest version](https://img.shields.io/github/downloads/marktext/marktext/latest/marktext-x64.dmg.svg)](https://github.com/marktext/marktext/releases/download/v0.17.1/marktext-x64.dmg) | [![latest version](https://img.shields.io/github/downloads/marktext/marktext/latest/marktext-setup.exe.svg)](https://github.com/marktext/marktext/releases/download/v0.17.1/marktext-setup.exe) | [![latest version](https://img.shields.io/github/downloads/marktext/marktext/latest/marktext-x86_64.AppImage.svg)](https://github.com/marktext/marktext/releases/download/v0.17.1/marktext-x86_64.AppImage) |\n\nWant to see new features of the latest version? Please refer to [CHANGELOG](.github/CHANGELOG.md).\n\n#### macOS\n\nYou can either download the latest `marktext-%version%.dmg` from the [release page](https://github.com/marktext/marktext/releases/latest) or install MarkText using [**homebrew cask**](https://github.com/caskroom/homebrew-cask). To use Homebrew-Cask you just need to have [Homebrew](https://brew.sh/) installed.\n\n```bash\nbrew install --cask mark-text\n```\n\n#### Windows\n\nSimply download and install MarkText via setup wizard (`marktext-setup-%version%.exe`) and choose whether to install per-user or machine wide. Alternatively, install MarkText using a package manager such as [Chocolatey](https://chocolatey.org/) or [Winget](https://docs.microsoft.com/en-us/windows/package-manager/winget/).\n\nTo use Chocolatey, you need to have [Chocolatey](https://chocolatey.org/install) installed:\n\n```bash\nchoco install marktext\n```\n\nTo use Winget, you need to have [Winget](https://docs.microsoft.com/en-us/windows/package-manager/winget/#install-winget) installed:\n\n```bash\nwinget install marktext\n```\n\n#### Linux\n\nPlease follow the [Linux installation instructions](docs/LINUX.md).\n\n#### Other\n\nAll binaries for Linux, macOS and Windows can be downloaded from the [release page](https://github.com/marktext/marktext/releases/latest). If a version is unavailable for your system, then please open an [issue](https://github.com/marktext/marktext/issues).\n\n## Development\n\nIf you wish to build MarkText yourself, please check out our [build instructions](docs/dev/BUILD.md).\n\n- [User documentation](docs/README.md)\n- [Developer documentation](docs/dev/README.md)\n\nIf you have any questions regarding MarkText, you are welcome to write an issue. When doing so please use the default format found when opening an issue. Of course, if you submit a PR directly, it will be greatly appreciated.\n\n## Integrations\n\n- [Alfred Workflow](http://www.packal.org/workflow/mark-text): A Workflow for the macOS app Alfred: Use \"mt\" to open files/folder with MarkText.\n\n## Contribution\n\nMarkText is in development, please make sure to read the [Contributing Guide](CONTRIBUTING.md) before making a pull request. Want to add some features to MarkText? Refer to our [roadmap](https://github.com/marktext/marktext/projects?type=classic) and open issues.\n\n\n## Contributors\n\nThank you to all the people who have already contributed to MarkText[[contributors](https://github.com/marktext/marktext/graphs/contributors)].\n\nSpecial thanks to @[Yasujizr](https://github.com/Yasujizr) who designed the MarkText logo.\n\n<a href=\"https://github.com/marktext/marktext/graphs/contributors\"><img src=\"https://opencollective.com/marktext/contributors.svg?width=890\" /></a>\n\n## License\n\n[**MIT**](LICENSE).\n\n[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Fmarktext%2Fmarktext.svg?type=large)](https://app.fossa.io/projects/git%2Bgithub.com%2Fmarktext%2Fmarktext?ref=badge_large)\n",
      "stars_today": 31
    },
    {
      "id": 709589487,
      "name": "awesome-system-design-resources",
      "full_name": "ashishps1/awesome-system-design-resources",
      "description": "Learn System Design concepts and prepare for interviews using free resources.",
      "html_url": "https://github.com/ashishps1/awesome-system-design-resources",
      "stars": 29020,
      "forks": 6652,
      "language": "Java",
      "topics": [
        "awesome",
        "backend",
        "computer-science",
        "distributed-systems",
        "high-level-design",
        "hld",
        "interview",
        "interview-questions",
        "scalability",
        "system-design"
      ],
      "created_at": "2023-10-25T01:50:42Z",
      "updated_at": "2026-01-15T00:01:02Z",
      "pushed_at": "2026-01-11T14:54:34Z",
      "open_issues": 7,
      "owner": {
        "login": "ashishps1",
        "avatar_url": "https://avatars.githubusercontent.com/u/8646889?v=4"
      },
      "readme": "<p align=\"center\">\n  <img src=\"diagrams/system-design-github-logo.png\" width=\"350\" height=\"200\">\n</p>\n\nThis repository contains free resources to learn System Design concepts and prepare for interviews.\n\nğŸ‘‰ Subscribe to my [AlgoMaster Newsletter](https://bit.ly/amghsd) and get a **FREE System Design Interview Handbook** in your inbox.\n\nâœ… If you are new to System Design, start here: [System Design was HARD until I Learned these 30 Concepts](https://blog.algomaster.io/p/30-system-design-concepts)\n\n## âš™ï¸ Core Concepts\n- [Scalability](https://algomaster.io/learn/system-design/scalability)\n- [Availability](https://algomaster.io/learn/system-design/availability)\n- [Reliability](https://algomaster.io/learn/system-design/reliability)\n- [CAP Theorem](https://blog.algomaster.io/p/cap-theorem-explained)\n- [Consistent Hashing](https://blog.algomaster.io/p/consistent-hashing-explained)\n- [SPOF](https://blog.algomaster.io/p/system-design-how-to-avoid-single-point-of-failures)\n- [Failover](https://www.druva.com/glossary/what-is-a-failover-definition-and-related-faqs)\n- [Fault Tolerance](https://www.cockroachlabs.com/blog/what-is-fault-tolerance/)\n\n## ğŸŒ Networking Fundamentals\n- [OSI Model](https://algomaster.io/learn/system-design/osi)\n- [IP Addresses](https://algomaster.io/learn/system-design/ip-address)\n- [Domain Name System (DNS)](https://blog.algomaster.io/p/how-dns-actually-works)\n- [Proxy vs Reverse Proxy](https://blog.algomaster.io/p/proxy-vs-reverse-proxy-explained)\n- [HTTP/HTTPS](https://algomaster.io/learn/system-design/http-https)\n- [TCP vs UDP](https://algomaster.io/learn/system-design/tcp-vs-udp)\n- [Load Balancing](https://blog.algomaster.io/p/load-balancing-algorithms-explained-with-code)\n- [Checksums](https://algomaster.io/learn/system-design/checksums)\n\n## ğŸ”Œ API Fundamentals\n- [APIs](https://algomaster.io/learn/system-design/what-is-an-api)\n- [API Gateway](https://blog.algomaster.io/p/what-is-an-api-gateway)\n- [REST vs GraphQL](https://blog.algomaster.io/p/rest-vs-graphql)\n- [WebSockets](https://blog.algomaster.io/p/websockets)\n- [Webhooks](https://algomaster.io/learn/system-design/webhooks)\n- [Idempotency](https://algomaster.io/learn/system-design/idempotency)\n- [Rate limiting](https://blog.algomaster.io/p/rate-limiting-algorithms-explained-with-code)\n- [API Design](https://abdulrwahab.medium.com/api-architecture-best-practices-for-designing-rest-apis-bf907025f5f)\n\n## ğŸ—„ï¸ Database Fundamentals\n- [ACID Transactions](https://algomaster.io/learn/system-design/acid-transactions)\n- [SQL vs NoSQL](https://algomaster.io/learn/system-design/sql-vs-nosql)\n- [Database Indexes](https://algomaster.io/learn/system-design/indexing)\n- [Database Sharding](https://algomaster.io/learn/system-design/sharding)\n- [Data Replication](https://redis.com/blog/what-is-data-replication/)\n- [Database Scaling](https://blog.algomaster.io/p/system-design-how-to-scale-a-database)\n- [Databases Types](https://blog.algomaster.io/p/15-types-of-databases)\n- [Bloom Filters](https://algomaster.io/learn/system-design/bloom-filters)\n- [Database Architectures](https://www.mongodb.com/developer/products/mongodb/active-active-application-architectures/)\n\n## âš¡ Caching Fundamentals\n- [Caching 101](https://algomaster.io/learn/system-design/what-is-caching)\n- [Caching Strategies](https://algomaster.io/learn/system-design/caching-strategies)\n- [Cache Eviction Policies](https://blog.algomaster.io/p/7-cache-eviction-strategies)\n- [Distributed Caching](https://blog.algomaster.io/p/distributed-caching)\n- [Content Delivery Network (CDN)](https://algomaster.io/learn/system-design/content-delivery-network-cdn)\n\n## ğŸ”„ Asynchronous Communication\n- [Pub/Sub](https://algomaster.io/learn/system-design/pub-sub)\n- [Message Queues](https://algomaster.io/learn/system-design/message-queues)\n- [Change Data Capture (CDC)](https://algomaster.io/learn/system-design/change-data-capture-cdc)\n\n## ğŸ§© Distributed System and Microservices\n- [HeartBeats](https://blog.algomaster.io/p/heartbeats-in-distributed-systems)\n- [Service Discovery](https://blog.algomaster.io/p/service-discovery-in-distributed-systems)\n- [Consensus Algorithms](https://medium.com/@sourabhatta1819/consensus-in-distributed-system-ac79f8ba2b8c)\n- [Distributed Locking](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)\n- [Microservices Guidelines](https://newsletter.systemdesign.one/p/netflix-microservices) \n- [Gossip Protocol](http://highscalability.com/blog/2023/7/16/gossip-protocol-explained.html)\n- [Circuit Breaker](https://medium.com/geekculture/design-patterns-for-microservices-circuit-breaker-pattern-276249ffab33)\n- [Disaster Recovery](https://cloud.google.com/learn/what-is-disaster-recovery)\n- [Distributed Tracing](https://www.dynatrace.com/news/blog/what-is-distributed-tracing/)\n\n## ğŸ–‡ï¸ Architectural Patterns\n- [Client-Server Architecture](https://algomaster.io/learn/system-design/client-server-architecture)\n- [Microservices Architecture](https://medium.com/hashmapinc/the-what-why-and-how-of-a-microservices-architecture-4179579423a9)\n- [Serverless Architecture](https://blog.algomaster.io/p/2edeb23b-cfa5-4b24-845e-3f6f7a39d162)\n- [Event-Driven Architecture](https://www.confluent.io/learn/event-driven-architecture/)\n- [Peer-to-Peer (P2P) Architecture](https://www.spiceworks.com/tech/networking/articles/what-is-peer-to-peer/)\n\n## âš–ï¸ System Design Tradeoffs\n- [Top 15 Tradeoffs](https://blog.algomaster.io/p/system-design-top-15-trade-offs)\n- [Vertical vs Horizontal Scaling](https://algomaster.io/learn/system-design/vertical-vs-horizontal-scaling)\n- [Concurrency vs Parallelism](https://blog.algomaster.io/p/concurrency-vs-parallelism)\n- [Long Polling vs WebSockets](https://blog.algomaster.io/p/long-polling-vs-websockets)\n- [Batch vs Stream Processing](https://blog.algomaster.io/p/batch-processing-vs-stream-processing)\n- [Stateful vs Stateless Design](https://blog.algomaster.io/p/stateful-vs-stateless-architecture)\n- [Strong vs Eventual Consistency](https://blog.algomaster.io/p/strong-vs-eventual-consistency)\n- [Read-Through vs Write-Through Cache](https://blog.algomaster.io/p/59cae60d-9717-4e20-a59e-759e370db4e5)\n- [Push vs Pull Architecture](https://blog.algomaster.io/p/af5fe2fe-9a4f-4708-af43-184945a243af)\n- [REST vs RPC](https://blog.algomaster.io/p/106604fb-b746-41de-88fb-60e932b2ff68)\n- [Synchronous vs. asynchronous communications](https://blog.algomaster.io/p/aec1cebf-6060-45a7-8e00-47364ca70761)\n- [Latency vs Throughput](https://aws.amazon.com/compare/the-difference-between-throughput-and-latency/)\n\n## âœ… [How to Answer a System Design Interview Problem](https://algomaster.io/learn/system-design-interviews/answering-framework)\n\n## ğŸ’» System Design Interview Problems\n### Easy\n- [Design URL Shortener like TinyURL](https://algomaster.io/learn/system-design-interviews/design-url-shortener)\n- [Design Autocomplete for Search Engines](https://algomaster.io/learn/system-design-interviews/design-instagram)\n- [Design Load Balancer](https://algomaster.io/learn/system-design-interviews/design-load-balancer)\n- [Design Content Delivery Network (CDN)](https://www.youtube.com/watch?v=8zX0rue2Hic)\n- [Design Parking Garage](https://www.youtube.com/watch?v=NtMvNh0WFVM)\n- [Design Vending Machine](https://www.youtube.com/watch?v=D0kDMUgo27c)\n- [Design Distributed Key-Value Store](https://www.youtube.com/watch?v=rnZmdmlR-2M)\n- [Design Distributed Cache](https://www.youtube.com/watch?v=iuqZvajTOyA)\n- [Design Authentication System](https://www.youtube.com/watch?v=uj_4vxm9u90)\n- [Design Unified Payments Interface (UPI)](https://www.youtube.com/watch?v=QpLy0_c_RXk)\n### Medium\n- [Design WhatsApp](https://algomaster.io/learn/system-design-interviews/design-whatsapp)\n- [Design Spotify](https://algomaster.io/learn/system-design-interviews/design-spotify)\n- [Design Instagram](https://algomaster.io/learn/system-design-interviews/design-instagram)\n- [Design Notification Service](https://algomaster.io/learn/system-design-interviews/design-notification-service)\n- [Design Distributed Job Scheduler](https://blog.algomaster.io/p/design-a-distributed-job-scheduler)\n- [Design Tinder](https://www.youtube.com/watch?v=tndzLznxq40)\n- [Design Facebook](https://www.youtube.com/watch?v=9-hjBGxuiEs)\n- [Design Twitter](https://www.youtube.com/watch?v=wYk0xPP_P_8)\n- [Design Reddit](https://www.youtube.com/watch?v=KYExYE_9nIY)\n- [Design Netflix](https://www.youtube.com/watch?v=psQzyFfsUGU)\n- [Design Youtube](https://www.youtube.com/watch?v=jPKTo1iGQiE)\n- [Design Google Search](https://www.youtube.com/watch?v=CeGtqouT8eA)\n- [Design E-commerce Store like Amazon](https://www.youtube.com/watch?v=EpASu_1dUdE)\n- [Design TikTok](https://www.youtube.com/watch?v=Z-0g_aJL5Fw)\n- [Design Shopify](https://www.youtube.com/watch?v=lEL4F_0J3l8)\n- [Design Airbnb](https://www.youtube.com/watch?v=YyOXt2MEkv4)\n- [Design Rate Limiter](https://www.youtube.com/watch?v=mhUQe4BKZXs)\n- [Design Distributed Message Queue like Kafka](https://www.youtube.com/watch?v=iJLL-KPqBpM)\n- [Design Flight Booking System](https://www.youtube.com/watch?v=qsGcfVGvFSs)\n- [Design Online Code Editor](https://www.youtube.com/watch?v=07jkn4jUtso)\n- [Design an Analytics Platform (Metrics & Logging)](https://www.youtube.com/watch?v=kIcq1_pBQSY)\n- [Design Payment System](https://www.youtube.com/watch?v=olfaBgJrUBI)\n- [Design a Digital Wallet](https://www.youtube.com/watch?v=4ijjIUeq6hE)\n### Hard\n- [Design Location Based Service like Yelp](https://www.youtube.com/watch?v=M4lR_Va97cQ)\n- [Design Uber](https://www.youtube.com/watch?v=umWABit-wbk)\n- [Design Food Delivery App like Doordash](https://www.youtube.com/watch?v=iRhSAR3ldTw)\n- [Design Google Docs](https://www.youtube.com/watch?v=2auwirNBvGg)\n- [Design Google Maps](https://www.youtube.com/watch?v=jk3yvVfNvds)\n- [Design Zoom](https://www.youtube.com/watch?v=G32ThJakeHk)\n- [Design Distributed Counter](https://systemdesign.one/distributed-counter-system-design/)\n- [Design File Sharing System like Dropbox](https://www.youtube.com/watch?v=U0xTu6E2CT8)\n- [Design Ticket Booking System like BookMyShow](https://www.youtube.com/watch?v=lBAwJgoO3Ek)\n- [Design Distributed Web Crawler](https://www.youtube.com/watch?v=BKZxZwUgL3Y)\n- [Design Code Deployment System](https://www.youtube.com/watch?v=q0KGYwNbf-0)\n- [Design Distributed Cloud Storage like S3](https://www.youtube.com/watch?v=UmWtcgC96X8)\n- [Design Distributed Locking Service](https://www.youtube.com/watch?v=v7x75aN9liM)\n- [Design Slack](https://systemdesign.one/slack-architecture/)\n- [Design Live Comments](https://systemdesign.one/live-comment-system-design/)\n\n## ğŸ“‡ Courses\n- [System Design Fundamentals](https://algomaster.io/learn/system-design/course-introduction)\n- [System Design Interviews](https://algomaster.io/learn/system-design-interviews/introduction)\n\n## ğŸ“© Newsletters\n- [AlgoMaster Newsletter](https://blog.algomaster.io/)\n\n## ğŸ“š Books\n- [Designing Data-Intensive Applications](https://www.amazon.in/dp/9352135245)\n\n## ğŸ“º YouTube Channels\n- [Tech Dummies Narendra L](https://www.youtube.com/@TechDummiesNarendraL)\n- [Gaurav Sen](https://www.youtube.com/@gkcs)\n- [codeKarle](https://www.youtube.com/@codeKarle)\n- [ByteByteGo](https://www.youtube.com/@ByteByteGo)\n- [System Design Interview](https://www.youtube.com/@SystemDesignInterview)\n- [sudoCODE](https://www.youtube.com/@sudocode)\n- [Success in Tech](https://www.youtube.com/@SuccessinTech/videos)\n\n## ğŸ“œ Must-Read Engineering Articles\n- [How Discord stores trillions of messages](https://discord.com/blog/how-discord-stores-trillions-of-messages)\n- [Building In-Video Search at Netflix](https://netflixtechblog.com/building-in-video-search-936766f0017c)\n- [How Canva scaled Media uploads from Zero to 50 Million per Day](https://www.canva.dev/blog/engineering/from-zero-to-50-million-uploads-per-day-scaling-media-at-canva/)\n- [How Airbnb avoids double payments in a Distributed Payments System](https://medium.com/airbnb-engineering/avoiding-double-payments-in-a-distributed-payments-system-2981f6b070bb)\n- [Stripeâ€™s payments APIs - The first 10 years](https://stripe.com/blog/payment-api-design)\n- [Real time messaging at Slack](https://slack.engineering/real-time-messaging/)\n\n## ğŸ—ï¸ Must-Read Distributed Systems Papers\n- [Paxos: The Part-Time Parliament](https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf)\n- [MapReduce: Simplified Data Processing on Large Clusters](https://research.google.com/archive/mapreduce-osdi04.pdf)\n- [The Google File System](https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)\n- [Dynamo: Amazonâ€™s Highly Available Key-value Store](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf)\n- [Kafka: a Distributed Messaging System for Log Processing](https://notes.stephenholiday.com/Kafka.pdf)\n- [Spanner: Googleâ€™s Globally-Distributed Database](https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf)\n- [Bigtable: A Distributed Storage System for Structured Data](https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf)\n- [ZooKeeper: Wait-free coordination for Internet-scale systems](https://www.usenix.org/legacy/event/usenix10/tech/full_papers/Hunt.pdf)\n- [The Log-Structured Merge-Tree (LSM-Tree)](https://www.cs.umb.edu/~poneil/lsmtree.pdf)\n- [The Chubby lock service for loosely-coupled distributed systems](https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf)\n\n---\n\n<p align=\"center\">\n  <i>If you find this resource helpful, please give it a star â­ï¸ and share it with others!</i>\n</p>\n",
      "stars_today": 30
    },
    {
      "id": 674295265,
      "name": "docmost",
      "full_name": "docmost/docmost",
      "description": "Docmost is an open-source collaborative wiki and documentation software. It is an open-source alternative to Confluence and Notion.",
      "html_url": "https://github.com/docmost/docmost",
      "stars": 18588,
      "forks": 1067,
      "language": "TypeScript",
      "topics": [
        "confluence",
        "documentation",
        "knowledge-base",
        "notion",
        "notion-alternative",
        "open-source",
        "opensource",
        "realtime-collaboration",
        "wiki"
      ],
      "created_at": "2023-08-03T15:35:48Z",
      "updated_at": "2026-01-14T23:46:28Z",
      "pushed_at": "2026-01-14T16:36:47Z",
      "open_issues": 293,
      "owner": {
        "login": "docmost",
        "avatar_url": "https://avatars.githubusercontent.com/u/150462874?v=4"
      },
      "readme": "<div align=\"center\">\n    <h1><b>Docmost</b></h1>\n    <p>\n        Open-source collaborative wiki and documentation software.\n        <br />\n        <a href=\"https://docmost.com\"><strong>Website</strong></a> | \n        <a href=\"https://docmost.com/docs\"><strong>Documentation</strong></a> |\n        <a href=\"https://twitter.com/DocmostHQ\"><strong>Twitter / X</strong></a>\n    </p>\n</div>\n<br />\n\n## Getting started\n\nTo get started with Docmost, please refer to our [documentation](https://docmost.com/docs) or try our [cloud version](https://docmost.com/pricing) .\n\n## Features\n\n- Real-time collaboration\n- Diagrams (Draw.io, Excalidraw and Mermaid)\n- Spaces\n- Permissions management\n- Groups\n- Comments\n- Page history\n- Search\n- File attachments\n- Embeds (Airtable, Loom, Miro and more)\n- Translations (10+ languages)\n\n### Screenshots\n\n<p align=\"center\">\n<img alt=\"home\" src=\"https://docmost.com/screenshots/home.png\" width=\"70%\">\n<img alt=\"editor\" src=\"https://docmost.com/screenshots/editor.png\" width=\"70%\">\n</p>\n\n### License\nDocmost core is licensed under the open-source AGPL 3.0 license.  \nEnterprise features are available under an enterprise license (Enterprise Edition).  \n\nAll files in the following directories are licensed under the Docmost Enterprise license defined in `packages/ee/License`.\n  - apps/server/src/ee\n  - apps/client/src/ee\n  - packages/ee\n\n### Contributing\n\nSee the [development documentation](https://docmost.com/docs/self-hosting/development)\n\n## Thanks\nSpecial thanks to;\n\n<img width=\"100\" alt=\"Crowdin\" src=\"https://github.com/user-attachments/assets/a6c3d352-e41b-448d-b6cd-3fbca3109f07\" />\n\n[Crowdin](https://crowdin.com/) for providing access to their localization platform.\n\n\n<img width=\"48\" alt=\"Algolia-mark-square-white\" src=\"https://github.com/user-attachments/assets/6ccad04a-9589-4965-b6a1-d5cb1f4f9e94\" />\n\n[Algolia](https://www.algolia.com/) for providing full-text search to the docs.\n\n",
      "stars_today": 27
    },
    {
      "id": 810861466,
      "name": "rig",
      "full_name": "0xPlaygrounds/rig",
      "description": "âš™ï¸ğŸ¦€ Build modular and scalable LLM Applications in Rust",
      "html_url": "https://github.com/0xPlaygrounds/rig",
      "stars": 5496,
      "forks": 627,
      "language": "Rust",
      "topics": [
        "agent",
        "ai",
        "artificial-intelligence",
        "automation",
        "generative-ai",
        "large-language-model",
        "llm",
        "llmops",
        "rust",
        "scalable-ai"
      ],
      "created_at": "2024-06-05T13:42:28Z",
      "updated_at": "2026-01-14T22:27:32Z",
      "pushed_at": "2026-01-15T00:54:58Z",
      "open_issues": 127,
      "owner": {
        "login": "0xPlaygrounds",
        "avatar_url": "https://avatars.githubusercontent.com/u/93353392?v=4"
      },
      "readme": "<p align=\"center\">\n<picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"img/rig-rebranded-logo-white.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"img/rig-rebranded-logo-black.svg\">\n    <img src=\"img/rig-rebranded-logo-white.svg\" style=\"width: 40%; height: 40%;\" alt=\"Rig logo\">\n</picture>\n<br>\n<br>\n<a href=\"https://docs.rig.rs\"><img src=\"https://img.shields.io/badge/ğŸ“– docs-rig.rs-dca282.svg\" /></a> &nbsp;\n<a href=\"https://docs.rs/rig-core/latest/rig/\"><img src=\"https://img.shields.io/badge/docs-API Reference-dca282.svg\" /></a> &nbsp;\n<a href=\"https://crates.io/crates/rig-core\"><img src=\"https://img.shields.io/crates/v/rig-core.svg?color=dca282\" /></a>\n&nbsp;\n<a href=\"https://crates.io/crates/rig-core\"><img src=\"https://img.shields.io/crates/d/rig-core.svg?color=dca282\" /></a>\n</br>\n<a href=\"https://discord.gg/playgrounds\"><img src=\"https://img.shields.io/discord/511303648119226382?color=%236d82cc&label=Discord&logo=discord&logoColor=white\" /></a>\n&nbsp;\n<a href=\"\"><img src=\"https://img.shields.io/badge/built_with-Rust-dca282.svg?logo=rust\" /></a>\n&nbsp;\n<a href=\"https://github.com/0xPlaygrounds/rig\"><img src=\"https://img.shields.io/github/stars/0xPlaygrounds/rig?style=social\" alt=\"stars - rig\" /></a>\n<br>\n\n<br>\n</p>\n&nbsp;\n\n\n<div align=\"center\">\n\n[ğŸ“‘ Docs](https://docs.rig.rs)\n<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n[ğŸŒ Website](https://rig.rs)\n<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n[ğŸ¤ Contribute](https://github.com/0xPlaygrounds/rig/issues/new)\n<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n[âœğŸ½ Blogs](https://docs.rig.rs/guides)\n\n</div>\n\nâœ¨ If you would like to help spread the word about Rig, please consider starring the repo!\n\n> [!WARNING]\n> Here be dragons! As we plan to ship a torrent of features in the following months, future updates **will** contain **breaking changes**. With Rig evolving, we'll annotate changes and highlight migration paths as we encounter them.\n\n## Table of contents\n\n- [Table of contents](#table-of-contents)\n- [What is Rig?](#what-is-rig)\n- [High-level features](#high-level-features)\n- [Who's using Rig?](#who-is-using-rig)\n- [Get Started](#get-started)\n  - [Simple example](#simple-example)\n- [Integrations](#supported-integrations)\n\n## What is Rig?\nRig is a Rust library for building scalable, modular, and ergonomic **LLM-powered** applications.\n\nMore information about this crate can be found in the [official](https://docs.rig.rs) & [crate](https://docs.rs/rig-core/latest/rig/) (API Reference) documentations.\n\n## Features\n- Agentic workflows that can handle multi-turn streaming and prompting\n- Full [GenAI Semantic Convention](https://opentelemetry.io/docs/specs/semconv/gen-ai/) compatibility\n- 20+ model providers, all under one singular unified interface\n- 10+ vector store integrations, all under one singular unified interface\n- Full support for LLM completion and embedding workflows\n- Support for transcription, audio generation and image generation model capabilities\n- Integrate LLMs in your app with minimal boilerplate\n- Full WASM compatibility (core library only)\n\n## Who is using Rig?\nBelow is a non-exhaustive list of companies and people who are using Rig:\n- [St Jude](https://www.stjude.org/) - Using Rig for a chatbot utility as part of [`proteinpaint`](https://github.com/stjude/proteinpaint), a genomics visualisation tool.\n- [Coral Protocol](https://www.coralprotocol.org/) - Using Rig extensively, both internally as well as part of the [Coral Rust SDK.](https://github.com/Coral-Protocol/coral-rs)\n- [VT Code](https://github.com/vinhnx/vtcode) - VT Code is a Rust-based terminal coding agent with semantic code intelligence via Tree-sitter and ast-grep. VT Code uses `rig` for simplifying LLM calls and implement model picker.\n- [Dria](https://dria.co/) - a decentralised AI network. Currently using Rig as part of their [compute node.](https://github.com/firstbatchxyz/dkn-compute-node)\n- [Nethermind](https://www.nethermind.io/) - Using Rig as part of their [Neural Interconnected Nodes Engine](https://github.com/NethermindEth/nine) framework.\n- [Neon](https://neon.com) - Using Rig for their [app.build](https://github.com/neondatabase/appdotbuild-agent) V2 reboot in Rust.\n- [Listen](https://github.com/piotrostr/listen) - A framework aiming to become the go-to framework for AI portfolio management agents. Powers [the Listen app.](https://app.listen-rs.com/)\n- [Cairnify](https://cairnify.com/) - helps users find documents, links, and information instantly through an intelligent search bar. Rig provides the agentic foundation behind Cairnifyâ€™s AI search experience, enabling tool-calling, reasoning, and retrieval workflows.\n- [Ryzome](https://ryzome.ai) - Ryzome is a visual AI workspace that lets you build interconnected canvases of thoughts, research, and AI agents to orchestrate complex knowledge work.\n\nFor a full list, check out our [ECOSYSTEM.md file.](https://www.github.com/0xPlaygrounds/rig/tree/main/ECOSYSTEM.md)\n\nAre you also using Rig? [Open an issue](https://www.github.com/0xPlaygrounds/rig/issues) to have your name added!\n\n## Get Started\n```bash\ncargo add rig-core\n```\n\n### Simple example\n```rust\nuse rig::{client::CompletionClient, completion::Prompt, providers::openai};\n\n#[tokio::main]\nasync fn main() {\n    // Create OpenAI client and model\n    // This requires the `OPENAI_API_KEY` environment variable to be set.\n    let openai_client = openai::Client::from_env();\n\n    let gpt4 = openai_client.agent(\"gpt-4\").build();\n\n    // Prompt the model and print its response\n    let response = gpt4\n        .prompt(\"Who are you?\")\n        .await\n        .expect(\"Failed to prompt GPT-4\");\n\n    println!(\"GPT-4: {response}\");\n}\n```\nNote using `#[tokio::main]` requires you enable tokio's `macros` and `rt-multi-thread` features\nor just `full` to enable all features (`cargo add tokio --features macros,rt-multi-thread`).\n\nYou can find more examples each crate's `examples` (ie. [`rig-core/examples`](./rig-core/examples)) directory. More detailed use cases walkthroughs are regularly published on our [Dev.to Blog](https://dev.to/0thtachi) and added to Rig's official documentation [(docs.rig.rs)](http://docs.rig.rs).\n\n## Supported Integrations\n\nVector stores are available as separate companion-crates:\n- MongoDB: [`rig-mongodb`](https://github.com/0xPlaygrounds/rig/tree/main/rig-mongodb)\n- LanceDB: [`rig-lancedb`](https://github.com/0xPlaygrounds/rig/tree/main/rig-lancedb)\n- Neo4j: [`rig-neo4j`](https://github.com/0xPlaygrounds/rig/tree/main/rig-neo4j)\n- Qdrant: [`rig-qdrant`](https://github.com/0xPlaygrounds/rig/tree/main/rig-qdrant)\n- SQLite: [`rig-sqlite`](https://github.com/0xPlaygrounds/rig/tree/main/rig-sqlite)\n- SurrealDB: [`rig-surrealdb`](https://github.com/0xPlaygrounds/rig/tree/main/rig-surrealdb)\n- Milvus: [`rig-milvus`](https://github.com/0xPlaygrounds/rig/tree/main/rig-milvus)\n- ScyllaDB: [`rig-scylladb`](https://github.com/0xPlaygrounds/rig/tree/main/rig-scylladb)\n- AWS S3Vectors: [`rig-s3vectors`](https://github.com/0xPlaygrounds/rig/tree/main/rig-s3vectors)\n- HelixDB: [`rig-helixdb`](https://github.com/0xPlaygrounds/rig/tree/main/rig-helixdb)\n\nThe following providers are available as separate companion-crates:\n- AWS Bedrock: [`rig-bedrock`](https://github.com/0xPlaygrounds/rig/tree/main/rig-bedrock)\n- Fastembed: [`rig-fastembed`](https://github.com/0xPlaygrounds/rig/tree/main/rig-fastembed)\n- Eternal AI: [`rig-eternalai`](https://github.com/0xPlaygrounds/rig/tree/main/rig-eternalai)\n- Google Vertex: [`rig-vertexai`](https://github.com/0xPlaygrounds/rig/tree/main/rig-vertexai)\n\nWe also have some other associated crates that have additional functionality you may find helpful when using Rig:\n- `rig-onchain-kit` - the [Rig Onchain Kit.](https://github.com/0xPlaygrounds/rig-onchain-kit) Intended to make interactions between Solana/EVM and Rig much easier to implement.\n\n\n<p align=\"center\">\n<br>\n<br>\n<img src=\"img/built-by-playgrounds.svg\" alt=\"Build by Playgrounds\" width=\"30%\">\n</p>\n",
      "stars_today": 27
    },
    {
      "id": 22887094,
      "name": "tesseract",
      "full_name": "tesseract-ocr/tesseract",
      "description": "Tesseract Open Source OCR Engine (main repository)",
      "html_url": "https://github.com/tesseract-ocr/tesseract",
      "stars": 71879,
      "forks": 10461,
      "language": "C++",
      "topics": [
        "hacktoberfest",
        "lstm",
        "machine-learning",
        "ocr",
        "ocr-engine",
        "tesseract",
        "tesseract-ocr"
      ],
      "created_at": "2014-08-12T18:04:59Z",
      "updated_at": "2026-01-14T22:26:11Z",
      "pushed_at": "2026-01-08T02:30:46Z",
      "open_issues": 462,
      "owner": {
        "login": "tesseract-ocr",
        "avatar_url": "https://avatars.githubusercontent.com/u/8401422?v=4"
      },
      "readme": "# Tesseract OCR\n\n[![Coverity Scan Build Status](https://scan.coverity.com/projects/tesseract-ocr/badge.svg)](https://scan.coverity.com/projects/tesseract-ocr)\n[![CodeQL](https://github.com/tesseract-ocr/tesseract/workflows/CodeQL/badge.svg)](https://github.com/tesseract-ocr/tesseract/security/code-scanning)\n[![OSS-Fuzz](https://img.shields.io/badge/oss--fuzz-fuzzing-brightgreen)](https://issues.oss-fuzz.com/issues?q=is:open%20title:tesseract-ocr)\n\\\n[![GitHub license](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](https://raw.githubusercontent.com/tesseract-ocr/tesseract/main/LICENSE)\n[![Downloads](https://img.shields.io/badge/download-all%20releases-brightgreen.svg)](https://github.com/tesseract-ocr/tesseract/releases/)\n\n## Table of Contents\n\n* [Tesseract OCR](#tesseract-ocr)\n  * [About](#about)\n  * [Brief history](#brief-history)\n  * [Installing Tesseract](#installing-tesseract)\n  * [Running Tesseract](#running-tesseract)\n  * [For developers](#for-developers)\n  * [Support](#support)\n  * [License](#license)\n  * [Dependencies](#dependencies)\n  * [Latest Version of README](#latest-version-of-readme)\n\n## About\n\nThis package contains an **OCR engine** - `libtesseract` and a **command line program** - `tesseract`.\n\nTesseract 4 adds a new neural net (LSTM) based [OCR engine](https://en.wikipedia.org/wiki/Optical_character_recognition) which is focused on line recognition, but also still supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. Compatibility with Tesseract 3 is enabled by using the Legacy OCR Engine mode (--oem 0).\nIt also needs [traineddata](https://tesseract-ocr.github.io/tessdoc/Data-Files.html) files which support the legacy engine, for example those from the [tessdata](https://github.com/tesseract-ocr/tessdata) repository.\n\nStefan Weil is the current lead developer. Ray Smith was the lead developer until 2017. The maintainer is Zdenko Podobny. For a list of contributors see [AUTHORS](https://github.com/tesseract-ocr/tesseract/blob/main/AUTHORS)\nand GitHub's log of [contributors](https://github.com/tesseract-ocr/tesseract/graphs/contributors).\n\nTesseract has **unicode (UTF-8) support**, and can **recognize [more than 100 languages](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html)** \"out of the box\".\n\nTesseract supports **[various image formats](https://tesseract-ocr.github.io/tessdoc/InputFormats)** including PNG, JPEG and TIFF.\n\nTesseract supports **various output formats**: plain text, hOCR (HTML), PDF, invisible-text-only PDF, TSV, ALTO and PAGE.\n\nYou should note that in many cases, in order to get better OCR results, you'll need to **[improve the quality](https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html) of the image** you are giving Tesseract.\n\nThis project **does not include a GUI application**. If you need one, please see the [3rdParty](https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty.html) documentation.\n\nTesseract **can be trained to recognize other languages**.\nSee [Tesseract Training](https://tesseract-ocr.github.io/tessdoc/Training-Tesseract.html) for more information.\n\n## Brief history\n\nTesseract was originally developed at Hewlett-Packard Laboratories Bristol UK and at Hewlett-Packard Co, Greeley Colorado USA between 1985 and 1994, with some more changes made in 1996 to port to Windows, and some C++izing in 1998. In 2005 Tesseract was open sourced by HP. From 2006 until August 2017 it was developed by Google.\n\nMajor version 5 is the current stable version and started with release\n[5.0.0](https://github.com/tesseract-ocr/tesseract/releases/tag/5.0.0) on November 30, 2021. Newer minor versions and bugfix versions are available from\n[GitHub](https://github.com/tesseract-ocr/tesseract/releases/).\n\nLatest source code is available from [main branch on GitHub](https://github.com/tesseract-ocr/tesseract/tree/main).\nOpen issues can be found in [issue tracker](https://github.com/tesseract-ocr/tesseract/issues),\nand [planning documentation](https://tesseract-ocr.github.io/tessdoc/Planning.html).\n\nSee **[Release Notes](https://tesseract-ocr.github.io/tessdoc/ReleaseNotes.html)**\nand **[Change Log](https://github.com/tesseract-ocr/tesseract/blob/main/ChangeLog)** for more details of the releases.\n\n## Installing Tesseract\n\nYou can either [Install Tesseract via pre-built binary package](https://tesseract-ocr.github.io/tessdoc/Installation.html)\nor [build it from source](https://tesseract-ocr.github.io/tessdoc/Compiling.html).\n\nBefore building Tesseract from source, please check that your system has a compiler which is one of the [supported compilers](https://tesseract-ocr.github.io/tessdoc/supported-compilers.html).\n\n## Running Tesseract\n\nBasic **[command line usage](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html)**:\n\n    tesseract imagename outputbase [-l lang] [--oem ocrenginemode] [--psm pagesegmode] [configfiles...]\n\nFor more information about the various command line options use `tesseract --help` or `man tesseract`.\n\nExamples can be found in the [documentation](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html#simplest-invocation-to-ocr-an-image).\n\n## For developers\n\nDevelopers can use `libtesseract` [C](https://github.com/tesseract-ocr/tesseract/blob/main/include/tesseract/capi.h) or\n[C++](https://github.com/tesseract-ocr/tesseract/blob/main/include/tesseract/baseapi.h) API to build their own application. If you need bindings to `libtesseract` for other programming languages, please see the\n[wrapper](https://tesseract-ocr.github.io/tessdoc/AddOns.html#tesseract-wrappers) section in the AddOns documentation.\n\nDocumentation of Tesseract generated from source code by doxygen can be found on [tesseract-ocr.github.io](https://tesseract-ocr.github.io/).\n\n## Support\n\nBefore you submit an issue, please review **[the guidelines for this repository](https://github.com/tesseract-ocr/tesseract/blob/main/CONTRIBUTING.md)**.\n\nFor support, first read the [documentation](https://tesseract-ocr.github.io/tessdoc/),\nparticularly the [FAQ](https://tesseract-ocr.github.io/tessdoc/FAQ.html) to see if your problem is addressed there.\nIf not, search the [Tesseract user forum](https://groups.google.com/g/tesseract-ocr), the [Tesseract developer forum](https://groups.google.com/g/tesseract-dev) and [past issues](https://github.com/tesseract-ocr/tesseract/issues), and if you still can't find what you need, ask for support in the mailing-lists.\n\nMailing-lists:\n\n* [tesseract-ocr](https://groups.google.com/g/tesseract-ocr) - For tesseract users.\n* [tesseract-dev](https://groups.google.com/g/tesseract-dev) - For tesseract developers.\n\nPlease report an issue only for a **bug**, not for asking questions.\n\n## License\n\n    The code in this repository is licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n**NOTE**: This software depends on other packages that may be licensed under different open source licenses.\n\nTesseract uses [Leptonica library](http://leptonica.com/) which essentially\nuses a [BSD 2-clause license](http://leptonica.com/about-the-license.html).\n\n## Dependencies\n\nTesseract uses [Leptonica library](https://github.com/DanBloomberg/leptonica)\nfor opening input images (e.g. not documents like pdf).\nIt is suggested to use leptonica with built-in support for [zlib](https://zlib.net),\n[png](https://sourceforge.net/projects/libpng) and\n[tiff](http://www.simplesystems.org/libtiff) (for multipage tiff).\n\n## Latest Version of README\n\nFor the latest online version of the README.md see:\n\n<https://github.com/tesseract-ocr/tesseract/blob/main/README.md>\n",
      "stars_today": 26
    },
    {
      "id": 1045247072,
      "name": "semantic-router",
      "full_name": "vllm-project/semantic-router",
      "description": "System Level Intelligent Router for Mixture-of-Models at Cloud, Data Center and Edge",
      "html_url": "https://github.com/vllm-project/semantic-router",
      "stars": 2812,
      "forks": 432,
      "language": "Go",
      "topics": [
        "ai-gateway",
        "bert-classification",
        "fine-tuning",
        "golang",
        "huggingface-candle",
        "huggingface-transformers",
        "kubernetes",
        "llm",
        "llmrouter",
        "mcp",
        "mixture-of-models",
        "pii-detection",
        "prompt-engineering",
        "prompt-guard",
        "rust",
        "semantic-router",
        "vllm"
      ],
      "created_at": "2025-08-26T21:49:50Z",
      "updated_at": "2026-01-15T00:16:01Z",
      "pushed_at": "2026-01-15T01:01:16Z",
      "open_issues": 132,
      "owner": {
        "login": "vllm-project",
        "avatar_url": "https://avatars.githubusercontent.com/u/136984999?v=4"
      },
      "readme": "<div align=\"center\">\n\n<img src=\"website/static/img/code.png\" alt=\"vLLM Semantic Router\" width=\"100%\"/>\n\n[![Documentation](https://img.shields.io/badge/docs-read%20the%20docs-blue)](https://vllm-semantic-router.com)\n[![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Community-yellow)](https://huggingface.co/LLM-Semantic-Router)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![Crates.io](https://img.shields.io/crates/v/candle-semantic-router.svg)](https://crates.io/crates/candle-semantic-router)\n![Test And Build](https://github.com/vllm-project/semantic-router/workflows/Test%20And%20Build/badge.svg)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/vllm-project/semantic-router)\n\n**ğŸ“š [Complete Documentation](https://vllm-semantic-router.com) | ğŸš€ [Quick Start](https://vllm-semantic-router.com/docs/installation) | ğŸ“£ [Blog](https://vllm-semantic-router.com/blog/) | ğŸ“– [Publications](https://vllm-semantic-router.com/publications/)**\n\n</div>\n\n---\n\n*Latest News* ğŸ”¥\n\n- [2026/01/05] Iris v0.1 is Released: [vLLM Semantic Router v0.1 Iris: The First Major Release](https://blog.vllm.ai/2026/01/05/vllm-sr-iris.html)\n- [2025/12/16] Collaboration: [AMD Ã— vLLM Semantic Router: Building the System Intelligence Together](https://blog.vllm.ai/2025/12/16/vllm-sr-amd.html)\n- [2025/12/15] New Blog: [Token-Level Truth: Real-Time Hallucination Detection for Production LLMs](https://blog.vllm.ai/2025/12/14/halugate.html)\n- [2025/11/19] New Blog: [Signal-Decision Driven Architecture: Reshaping Semantic Routing at Scale](https://blog.vllm.ai/2025/11/19/signal-decision.html)\n- [2025/11/03] Our paper [Category-Aware Semantic Caching for Heterogeneous LLM Workloads](https://arxiv.org/abs/2510.26835) published\n- [2025/10/27] New Blog: [Scaling Semantic Routing with Extensible LoRA](https://blog.vllm.ai/2025/10/27/semantic-router-modular.html)\n- [2025/10/12] Our paper [When to Reason: Semantic Router for vLLM](https://arxiv.org/abs/2510.08731) accepted by NeurIPS 2025 MLForSys.\n- [2025/10/08] Collaboration: vLLM Semantic Router with [vLLM Production Stack](https://github.com/vllm-project/production-stack) Team.\n- [2025/09/01] Released the project: [vLLM Semantic Router: Next Phase in LLM inference](https://blog.vllm.ai/2025/09/11/semantic-router.html).\n\n---\n\n## Goals\n\nWe are building the **System Level Intelligence** for Mixture-of-Models (MoM), bringing the **Collective Intelligence** into **LLM systems**, answering the following questions:\n\n1. How to capture the missing signals in request, response and context?\n2. How to combine the signals to make better decisions?\n3. How to collaborate more efficiently between different models?\n4. How to secure the real world and LLM system from jailbreaks, pii leaks, hallucinations?\n5. How to collect the valuable signals and build a self-learning system?\n\n![vLLM Semantic Router Banner](./website/static/img/banner.png)\n\n### Where it lives\n\nIt lives between the real world and models:\n\n![level](./website/static/img/level.png)\n\n### Architecture\n\nA quick overview of the current architecture:\n\n![architecture](./website/static/img/architecture.png)\n\n## Quick Start\n\n### Installation\n\n> [!TIP]\n> We recommend that you setup a Python virtual environment to manage dependencies.\n\n```bash\n$ python -m venv vsr\n$ source vsr/bin/activate\n$ pip install vllm-sr\n```\n\nInstalled successfully if you see the following help message:\n\n```bash\n$ vllm-sr\n\n       _ _     __  __       ____  ____\n__   _| | |_ _|  \\/  |     / ___||  _ \\\n\\ \\ / / | | | | |\\/| |_____\\___ \\| |_) |\n \\ V /| | | |_| | |  |_____|___) |  _ <\n  \\_/ |_|_|\\__,_|_|  |     |____/|_| \\_\\\n\nvLLM Semantic Router - Intelligent routing for vLLM\n\nUsage: vllm-sr [OPTIONS] COMMAND [ARGS]...\n\n  vLLM Semantic Router CLI - Intelligent routing and caching for vLLM\n  endpoints.\n\nOptions:\n  --version  Show version and exit.\n  --help     Show this message and exit.\n\nCommands:\n  config  Print generated configuration.\n  init    Initialize vLLM Semantic Router configuration.\n  dashboard  Launch the vLLM Semantic Router dashboard.\n  logs    Show logs from vLLM Semantic Router service.\n  serve   Start vLLM Semantic Router.\n  status  Show status of vLLM Semantic Router services.\n  stop    Stop vLLM Semantic Router.\n```\n\n> [!TIP]\n> You can specify the HF_ENDPOINT, HF_TOKEN, and HF_HOME environment variables to configure the Hugging Face credentials.\n\n```bash\n# Set environment variables (optional)\nexport HF_ENDPOINT=https://huggingface.co  # Or use mirror: https://hf-mirror.com\nexport HF_TOKEN=your_token_here  # Only for gated models\nexport HF_HOME=/path/to/cache  # Optional: custom cache directory\n\n# Start the service - models download automatically\n# Environment variables are automatically passed to the container\nvllm-sr serve\n```\n\n### Configuration\n\n**File Descriptor Limits**: The CLI automatically sets file descriptor limits to 65,536 for Envoy proxy. For custom limits:\n\n```bash\nexport VLLM_SR_NOFILE_LIMIT=100000  # Optional: custom limit (min: 8192)\nvllm-sr serve\n```\n\nSee the [vllm-sr README](src/vllm-sr/README.md#configuration) for detailed configuration options and troubleshooting.\n\n## Documentation ğŸ“–\n\nFor comprehensive documentation including detailed setup instructions, architecture guides, and API references, visit:\n\nComplete Documentation at Read the **[Docs](https://vllm-semantic-router.com/)**\n\nThe documentation includes:\n\n- **[Installation Guide](https://vllm-semantic-router.com/docs/installation/)** - Complete setup instructions\n- **[System Architecture](https://vllm-semantic-router.com/docs/overview/architecture/system-architecture/)** - Technical deep dive\n- **[Model Training](https://vllm-semantic-router.com/docs/training/training-overview/)** - How classification models work\n- **[API Reference](https://vllm-semantic-router.com/docs/api/router/)** - Complete API documentation\n\n## Community ğŸ‘‹\n\nFor questions, feedback, or to contribute, please join `#semantic-router` channel in vLLM Slack.\n\n### Community Meetings ğŸ“…\n\nWe host bi-weekly community meetings to sync up with contributors across different time zones:\n\n- **First Tuesday of the month**: 9:00-10:00 AM EST (accommodates US EST, EU, and Asia Pacific contributors)\n  - [Zoom Link](https://us05web.zoom.us/j/84122485631?pwd=BB88v03mMNLVHn60YzVk4PihuqBV9d.1)\n  - [Google Calendar Invite](https://us05web.zoom.us/meeting/tZAsdeuspj4sGdVraOOR4UaXSstrH2jjPYFq/calendar/google/add?meetingMasterEventId=4jjzUKSLSLiBHtIKZpGc3g)\n  - [ics file](https://drive.google.com/file/d/15wO8cg0ZjNxdr8OtGiZyAgkSS8_Wry0J/view?usp=sharing)\n- **Third Tuesday of the month**: 1:00-2:00 PM EST (accommodates US EST and California contributors)\n  - [Zoom Link](https://us06web.zoom.us/j/86871492845?pwd=LcTtXm9gtGu23JeWqXxbnLLCCvbumB.1)\n  - [Google Calendar Invite](https://us05web.zoom.us/meeting/tZIlcOispzkiHtH2dlkWlLym68bEqvuf3MU5/calendar/google/add?meetingMasterEventId=PqWz2vk7TOCszPXqconGAA)\n  - [ics file](https://drive.google.com/file/d/1T54mwYpXXoV9QfR76I56BFBPNbykSsTw/view?usp=sharing)\n- Meeting Recordings: [YouTube](https://www.youtube.com/@vLLMSemanticRouter/videos)\n\nJoin us to discuss the latest developments, share ideas, and collaborate on the project!\n\n## Citation\n\nIf you find Semantic Router helpful in your research or projects, please consider citing it:\n\n```\n@misc{semanticrouter2025,\n  title={vLLM Semantic Router},\n  author={vLLM Semantic Router Team},\n  year={2025},\n  howpublished={\\url{https://github.com/vllm-project/semantic-router}},\n}\n```\n\n## Star History ğŸ”¥\n\nWe opened the project at Aug 31, 2025. We love open source  and collaboration â¤ï¸\n\n[![Star History Chart](https://api.star-history.com/svg?repos=vllm-project/semantic-router&type=Date)](https://www.star-history.com/#vllm-project/semantic-router&Date)\n\n## Sponsors ğŸ‘‹\n\nWe are grateful to our sponsors who support us:\n\n---\n\n[**AMD**](https://www.amd.com) provides us with GPU resources and [ROCmâ„¢](https://www.amd.com/en/products/software/rocm.html) Software for training and researching the frontier router models, enhancing e2e testing, and building online models playground.\n\n<div align=\"center\">\n<a href=\"https://www.amd.com\">\n  <img src=\"website/static/img/amd-logo.svg\" alt=\"AMD\" width=\"40%\"/>\n</a>\n</div>\n\n---\n",
      "stars_today": 26
    },
    {
      "id": 362234372,
      "name": "cariddi",
      "full_name": "edoardottt/cariddi",
      "description": "Take a list of domains, crawl urls and scan for endpoints, secrets, api keys, file extensions, tokens and more",
      "html_url": "https://github.com/edoardottt/cariddi",
      "stars": 3215,
      "forks": 284,
      "language": "Go",
      "topics": [
        "bugbounty",
        "crawler",
        "crawling",
        "endpoint-discovery",
        "endpoints",
        "go",
        "golang",
        "hacktoberfest",
        "infosec",
        "osint",
        "penetration-testing",
        "pentesting",
        "recon",
        "reconnaissance",
        "redteam",
        "scraper",
        "secret-keys",
        "secrets-detection",
        "security",
        "security-tools"
      ],
      "created_at": "2021-04-27T19:54:43Z",
      "updated_at": "2026-01-14T21:47:13Z",
      "pushed_at": "2026-01-11T10:52:32Z",
      "open_issues": 11,
      "owner": {
        "login": "edoardottt",
        "avatar_url": "https://avatars.githubusercontent.com/u/35783570?v=4"
      },
      "readme": "<p align=\"center\">\n  <img src=\"https://github.com/edoardottt/images/blob/main/cariddi/logo.png\"><br>\n  <b>Take a list of domains, crawl urls and scan for endpoints, secrets, api keys, file extensions, tokens and more</b><br>\n  <br>\n  <!-- go-report-card -->\n  <a href=\"https://goreportcard.com/report/github.com/edoardottt/cariddi\">\n    <img src=\"https://goreportcard.com/badge/github.com/edoardottt/cariddi\" alt=\"go-report-card\" />\n  </a>\n  <!-- workflows -->\n  <a href=\"https://github.com/edoardottt/cariddi/actions\">\n    <img src=\"https://github.com/edoardottt/cariddi/actions/workflows/go.yml/badge.svg?branch=main\" alt=\"workflows\" />\n  </a>\n  <br>\n  <sub>\n    Coded with ğŸ’™ by edoardottt\n  </sub>\n  <br>\n  <!--Tweet button-->\n  <a href=\"https://twitter.com/intent/tweet?url=https://github.com/edoardottt/cariddi&text=Take%20a%20list%20of%20domains,%20crawl%20urls%20and%20scan%20for%20endpoints,%20secrets,%20api%20keys,%20file%20extensions,%20tokens%20and%20more...%20%23network%20%23security%20%23infosec%20%23oss%20%23github%20%23bugbounty%20%23linux\" target=\"_blank\">Share on Twitter!\n  </a>\n</p>\n<p align=\"center\">\n  <a href=\"#installation-\">Install</a> â€¢\n  <a href=\"#usage-\">Usage</a> â€¢\n  <a href=\"#get-started-\">Get Started</a> â€¢\n  <a href=\"#changelog-\">Changelog</a> â€¢\n  <a href=\"#contributing-\">Contributing</a> â€¢\n  <a href=\"#license-\">License</a>\n</p>\n\n<!--[![asciicast](https://asciinema.org/a/415989.svg)](https://asciinema.org/a/415989)-->\n\n<p align=\"center\">\n  <img src=\"https://github.com/edoardottt/images/blob/main/cariddi/cariddi.gif\">\n</p>\n\nInstallation ğŸ“¡\n----------\n\n#### Homebrew\n\n```console\nbrew install cariddi\n```\n\n#### Snap\n\n```console\nsudo snap install cariddi\n```\n\n#### Golang\n\n```console\ngo install -v github.com/edoardottt/cariddi/cmd/cariddi@latest\n```\n\n#### Pacman\n\n```console\npacman -Syu cariddi\n```\n\n#### NixOS\n\n```console\nnix-shell -p cariddi\n```\n\n#### Building from source\n\nYou need [Go](https://go.dev/) (>=1.24.0)\n\n<details>\n  <summary>Building from source for Linux and Windows</summary>\n\n#### Linux\n\n```console\ngit clone https://github.com/edoardottt/cariddi.git\ncd cariddi\ngo get ./...\nmake linux # (to install)\nmake unlinux # (to uninstall)\n```\n\nOne-liner: `git clone https://github.com/edoardottt/cariddi.git && cd cariddi && go get ./... && make linux`\n\n#### Windows \n\nNote that the executable works only in cariddi folder.\n\n```console\ngit clone https://github.com/edoardottt/cariddi.git\ncd cariddi\ngo get ./...\n.\\make.bat windows # (to install)\n.\\make.bat unwindows # (to uninstall)\n```\n\n</details>\n\nUsage ğŸ’¡\n----------\n\nIf you want to scan only a single target you can use\n\n```console\necho https://edoardottt.com/ | cariddi\n```\n\nWith multiple targets you can use a file instead, e.g. urls.txt containing:\n\n```console\nhttps://edoardottt.com/\nhttp://testphp.vulnweb.com/\n```\n\nFor Windows:\n\n- use `powershell.exe -Command \"cat urls.txt | .\\cariddi.exe\"` inside the Command prompt\n- or just `cat urls.txt | cariddi.exe` using PowerShell\n\n### Basics\n\n- `cariddi -version` (Print the version)\n- `cariddi -h` (Print the help)\n- `cariddi -examples` (Print the examples)\n\n### Scan options\n\n- `cat urls.txt | cariddi -intensive` (Crawl searching also subdomains, same as `*.target.com`)\n- `cat urls.txt | cariddi -s` (Hunt for secrets)\n- `cat urls.txt | cariddi -err` (Hunt for errors in websites)\n- `cat urls.txt | cariddi -e` (Hunt for juicy endpoints)\n- `cat urls.txt | cariddi -info` (Hunt for useful informations in websites)\n- `cat urls.txt | cariddi -ext 2` (Hunt for juicy (level 2 out of 7) files)\n- `cat urls.txt | cariddi -e -ef endpoints_file` (Hunt for custom endpoints)\n- `cat urls.txt | cariddi -s -sf secrets_file` (Hunt for custom secrets)\n- `cat urls.txt | cariddi -ie pdf,png,jpg` (Ignore these extensions while scanning)\n\nDefault: png, svg, jpg, jpeg, bmp, jfif, gif, webp, woff, woff2, ttf, tiff, tif are ignored while scanning for secrets, info and errors.\n\n### Configuration\n\n- `cat urls.txt | cariddi -proxy http://127.0.0.1:8080` (Set a Proxy, http and socks5 supported)\n- `cat urls.txt | cariddi -d 2` (2 seconds between a page crawled and another)\n- `cat urls.txt | cariddi -c 200` (Set the concurrency level to 200)\n- `cat urls.txt | cariddi -i forum,blog,community,open` (Ignore urls containing these words)\n- `cat urls.txt | cariddi -it ignore_file` (Ignore urls containing at least one line in the input file)\n- `cat urls.txt | cariddi -cache` (Use the .cariddi_cache folder as cache)\n- `cat urls.txt | cariddi -t 5` (Set the timeout for the requests)\n- `cat urls.txt | cariddi -headers \"Cookie: auth=admin;type=2;; X-Custom: customHeader\"`\n- `cat urls.txt | cariddi -headersfile headers.txt` (Read from an external file custom headers)\n- `cat urls.txt | cariddi -ua \"Custom User Agent\"` (Use a custom User Agent)\n- `cat urls.txt | cariddi -rua` (Use a random browser user agent on every request)\n\n### Output\n\n- `cat urls.txt | cariddi -plain` (Print only results)\n- `cat urls.txt | cariddi -ot target_name` (Results in txt file)\n- `cat urls.txt | cariddi -oh target_name` (Results in html file)\n- `cat urls.txt | cariddi -json` (Print the output as JSON in stdout)\n- `cat urls.txt | cariddi -sr` (Store HTTP responses)\n- `cat urls.txt | cariddi -debug` (Print debug information while crawling)\n- `cat urls.txt | cariddi -md 3` (Max 3 depth levels)\n\nGet Started ğŸ‰\n----------\n\n`cariddi -h` prints the help.\n\n```console\nUsage of cariddi:\n  -c int\n     Concurrency level. (default 20)\n  -cache\n     Use the .cariddi_cache folder as cache.\n  -d int\n     Delay between a page crawled and another.\n  -debug\n     Print debug information while crawling.\n  -e Hunt for juicy endpoints.\n  -ef string\n     Use an external file (txt, one per line) to use custom parameters for endpoints hunting.\n  -err\n     Hunt for errors in websites.\n  -examples\n     Print the examples.\n  -ext int\n     Hunt for juicy file extensions. Integer from 1(juicy) to 7(not juicy).\n  -h Print the help.\n  -headers string\n     Use custom headers for each request E.g. -headers \"Cookie: auth=yes;;Client: type=2\".\n  -headersfile string\n     Read from an external file custom headers (same format of headers flag).\n  -json\n     Print the output as JSON in stdout.\n  -md\n     Maximum depth level the crawler will follow from the initial target URL.\n  -i string\n     Ignore the URL containing at least one of the elements of this array.\n  -ie value\n     Comma-separated list of extensions to ignore while scanning.\n  -info\n     Hunt for useful informations in websites.\n  -intensive\n     Crawl searching for resources matching 2nd level domain.\n  -it string\n     Ignore the URL containing at least one of the lines of this file.\n  -oh string\n     Write the output into an HTML file.\n  -ot string\n     Write the output into a TXT file.\n  -plain\n     Print only the results.\n  -proxy string\n     Set a Proxy to be used (http and socks5 supported).\n  -rua\n     Use a random browser user agent on every request.\n  -s Hunt for secrets.\n  -sf string\n     Use an external file (txt, one per line) to use custom regexes for secrets hunting.\n  -sr\n     Store HTTP responses.\n  -t int\n     Set timeout for the requests. (default 10)\n  -ua string\n     Use a custom User Agent.\n  -version\n     Print the version.\n```\n\n<details>\n  <summary>Click to understand <strong>How to integrate cariddi with Burpsuite</strong></summary>\n\n   Normally you use Burpsuite within your browser, so you just have to trust the burpsuite's certificate in the browser and you're done.  \n   In order to use cariddi with the BurpSuite proxy you should do some steps further.  \n\n   If you try to use cariddi with the option `-proxy http://127.0.0.1:8080` you will find this error in the burpsuite error log section:  \n\n   ```bash\n   Received fatal alert: bad_certificate (or something similar related to the certificate).\n   ```\n\n   To make cariddi working fine with Burpsuite you have also to trust the certificate within your entire pc, not just only the browser. These are the steps you have to follow:\n\n   Go to Proxy tab in Bupsuite, then Options. Click on the CA Certificate button and export Certificate in DER format  \n\n   ```bash\n   openssl x509 -in burp.der -inform DER -out burp.pem -outform PEM\n   sudo chown root:root burp.pem\n   sudo chmod 644 burp.pem\n   sudo cp burp.pem /usr/local/share/ca-certificates/\n   sudo c_rehash\n   cd /etc/ssl/certs/\n   sudo ln -s /usr/local/share/ca-certificates/burp.pem\n   sudo c_rehash .\n   ```\n\n   Source: Trust Burp Proxy certificate in Debian/Ubuntu  \n\n   After these steps, in order to use cariddi with Burpsuite you have to:  \n\n   1. Open Burpsuite, making sure that the proxy is listening.  \n   2. Use cariddi with the flag `-proxy http://127.0.0.1:8080`.  \n   3. You will see that requests and responses will be logged in Burpsuite.\n\n</details>\n\nChangelog ğŸ“Œ\n-------\n\nDetailed changes for each release are documented in the [release notes](https://github.com/edoardottt/cariddi/releases).\n\nContributing ğŸ› \n-------\n\nJust open an [issue](https://github.com/edoardottt/cariddi/issues)/[pull request](https://github.com/edoardottt/cariddi/pulls).\n\nBefore opening a pull request, download [golangci-lint](https://golangci-lint.run/usage/install/) and run\n\n```console\ngolangci-lint run\n```\n\nIf there aren't errors, go ahead :)\n\nTest using [https://edoardottt.github.io/cariddi-test/](https://edoardottt.github.io/cariddi-test/)\n\n```console\necho \"https://edoardottt.github.io/cariddi-test/\" | cariddi\n```\n\n**Help me build this!**\n\nSpecial thanks to: [go-colly](http://go-colly.org/), [ocervell](https://github.com/ocervell), [zricethezav](https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml), [projectdiscovery](https://github.com/projectdiscovery/nuclei-templates/tree/master/file/keys), [tomnomnom](https://github.com/tomnomnom/gf/tree/master/examples), [RegexPassive](https://github.com/hahwul/RegexPassive) and [all the contributors](https://github.com/edoardottt/cariddi/graphs/contributors).\n\nLicense ğŸ“\n-------\n\nThis repository is under [GNU General Public License v3.0](https://github.com/edoardottt/cariddi/blob/main/LICENSE).  \n[edoardottt.com](https://edoardottt.com/) to contact me.\n",
      "stars_today": 26
    },
    {
      "id": 337594329,
      "name": "SmsForwarder",
      "full_name": "pppscn/SmsForwarder",
      "description": "çŸ­ä¿¡è½¬å‘å™¨â€”â€”ç›‘æ§Androidæ‰‹æœºçŸ­ä¿¡ã€æ¥ç”µã€APPé€šçŸ¥ï¼Œå¹¶æ ¹æ®æŒ‡å®šè§„åˆ™è½¬å‘åˆ°å…¶ä»–æ‰‹æœºï¼šé’‰é’‰ç¾¤è‡ªå®šä¹‰æœºå™¨äººã€é’‰é’‰ä¼ä¸šå†…æœºå™¨äººã€ä¼ä¸šå¾®ä¿¡ç¾¤æœºå™¨äººã€é£ä¹¦æœºå™¨äººã€ä¼ä¸šå¾®ä¿¡åº”ç”¨æ¶ˆæ¯ã€é‚®ç®±ã€barkã€webhookã€Telegramæœºå™¨äººã€Serveré…±ã€PushPlusã€æ‰‹æœºçŸ­ä¿¡ç­‰ã€‚åŒ…æ‹¬ä¸»åŠ¨æ§åˆ¶æœåŠ¡ç«¯ä¸å®¢æˆ·ç«¯ï¼Œè®©ä½ è½»æ¾è¿œç¨‹å‘çŸ­ä¿¡ã€æŸ¥çŸ­ä¿¡ã€æŸ¥é€šè¯ã€æŸ¥è¯ç°¿ã€æŸ¥ç”µé‡ç­‰ã€‚ï¼ˆV3.0 æ–°å¢ï¼‰PS.è¿™ä¸ªAPKä¸»è¦æ˜¯å­¦ä¹ ä¸è‡ªç”¨ï¼Œå¦‚æœ‰BUGè¯·æISSUEï¼ŒåŒæ—¶æ¬¢è¿å¤§å®¶æPRæŒ‡æ­£",
      "html_url": "https://github.com/pppscn/SmsForwarder",
      "stars": 24135,
      "forks": 3082,
      "language": "Kotlin",
      "topics": [
        "android",
        "api",
        "app",
        "bark",
        "call",
        "chatgpt",
        "dingding",
        "forward",
        "mqtt",
        "pushdear",
        "pushplus",
        "serverchan",
        "sms",
        "smtp",
        "telegram",
        "webhook",
        "wechatapp"
      ],
      "created_at": "2021-02-10T02:23:07Z",
      "updated_at": "2026-01-14T20:47:46Z",
      "pushed_at": "2025-12-10T08:23:14Z",
      "open_issues": 40,
      "owner": {
        "login": "pppscn",
        "avatar_url": "https://avatars.githubusercontent.com/u/5105854?v=4"
      },
      "readme": "![SmsForwarder](pic/SmsForwarder.png)\r\n\r\n# SmsForwarder-çŸ­ä¿¡è½¬å‘å™¨\r\n\r\n[English Version](README_en.md)\r\n\r\n[![GitHub release](https://img.shields.io/github/release/pppscn/SmsForwarder.svg)](https://github.com/pppscn/SmsForwarder/releases) [![GitHub stars](https://img.shields.io/github/stars/pppscn/SmsForwarder)](https://github.com/pppscn/SmsForwarder/stargazers) [![GitHub forks](https://img.shields.io/github/forks/pppscn/SmsForwarder)](https://github.com/pppscn/SmsForwarder/network/members) [![GitHub issues](https://img.shields.io/github/issues/pppscn/SmsForwarder)](https://github.com/pppscn/SmsForwarder/issues) [![GitHub license](https://img.shields.io/github/license/pppscn/SmsForwarder)](https://github.com/pppscn/SmsForwarder/blob/main/LICENSE)\r\n\r\n--------\r\n\r\nçŸ­ä¿¡è½¬å‘å™¨â€”â€”ä¸ä»…åªè½¬å‘çŸ­ä¿¡ï¼Œå¤‡ç”¨æœºå¿…å¤‡ç¥å™¨ï¼\r\n\r\nç›‘æ§Androidæ‰‹æœºçŸ­ä¿¡ã€æ¥ç”µã€APPé€šçŸ¥ï¼Œå¹¶æ ¹æ®æŒ‡å®šè§„åˆ™è½¬å‘åˆ°å…¶ä»–æ‰‹æœºï¼šé’‰é’‰ç¾¤è‡ªå®šä¹‰æœºå™¨äººã€é’‰é’‰ä¼ä¸šå†…æœºå™¨äººã€ä¼ä¸šå¾®ä¿¡ç¾¤æœºå™¨äººã€ä¼ä¸šå¾®ä¿¡åº”ç”¨æ¶ˆæ¯ã€é£ä¹¦ç¾¤æœºå™¨äººã€é£ä¹¦ä¼ä¸šåº”ç”¨ã€é‚®ç®±ã€barkã€webhookã€Tele****æœºå™¨äººã€Serveré…±ã€PushPlusã€æ‰‹æœºçŸ­ä¿¡ç­‰ã€‚\r\n\r\nåŒ…æ‹¬ä¸»åŠ¨æ§åˆ¶æœåŠ¡ç«¯ä¸å®¢æˆ·ç«¯ï¼Œè®©ä½ è½»æ¾è¿œç¨‹å‘çŸ­ä¿¡ã€æŸ¥çŸ­ä¿¡ã€æŸ¥é€šè¯ã€æŸ¥è¯ç°¿ã€æŸ¥ç”µé‡ç­‰ã€‚ï¼ˆV3.0 æ–°å¢ï¼‰\r\n\r\nè‡ªåŠ¨ä»»åŠ¡ãƒ»å¿«æ·æŒ‡ä»¤ï¼Œè½»æ¾è‡ªåŠ¨åŒ–ï¼ŒåŠ©æ‚¨äº‹åŠåŠŸå€ï¼Œæ›´å¤šæ—¶é—´äº«å—äº²æƒ…é™ªä¼´ï¼ï¼ˆv3.3 æ–°å¢ï¼‰\r\n\r\n> æ³¨æ„ï¼šä»`2022-06-06`å¼€å§‹ï¼ŒåŸ`Javaç‰ˆ`çš„ä»£ç å½’æ¡£åˆ°`v2.x`åˆ†æ”¯ï¼Œä¸å†æ›´æ–°ï¼\r\n\r\n> `v3.x` é€‚é… Android 4.4 ~ 13.0\r\n\r\n> `åŠ å…¥SmsFé¢„è§ˆä½“éªŒè®¡åˆ’`ï¼ˆåœ¨çº¿æ›´æ–°æ¯å‘¨æ„å»ºç‰ˆï¼Œç‡å…ˆä½“éªŒæ–°ç‰ˆ&ä¿®å¤BUGï¼‰\r\n\r\n**å‡çº§æ“ä½œæç¤ºï¼š** \r\n- `åŠ å…¥SmsFé¢„è§ˆä½“éªŒè®¡åˆ’`ååœ¨çº¿æ›´æ–°ï¼ˆ`å…³äºè½¯ä»¶`é¡µé¢å¼€å¯ï¼Œ`v3.3.0_240305+`é€‚ç”¨ï¼‰\r\n-  æ‰‹åŠ¨ä¸‹è½½ï¼šhttps://github.com/pppscn/SmsForwarder/actions/workflows/Weekly_Build.yml\r\n\r\n--------\r\n\r\n## ç‰¹åˆ«å£°æ˜:\r\n\r\n* æœ¬ä»“åº“å‘å¸ƒçš„`SmsForwarder`é¡¹ç›®ä¸­æ¶‰åŠçš„ä»»ä½•ä»£ç /APKï¼Œä»…ç”¨äºæµ‹è¯•å’Œå­¦ä¹ ç ”ç©¶ï¼Œç¦æ­¢ç”¨äºå•†ä¸šç”¨é€”ï¼Œä¸èƒ½ä¿è¯å…¶åˆæ³•æ€§ï¼Œå‡†ç¡®æ€§ï¼Œå®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§ï¼Œè¯·æ ¹æ®æƒ…å†µè‡ªè¡Œåˆ¤æ–­ã€‚\r\n\r\n* ä»»ä½•ç”¨æˆ·ç›´æ¥æˆ–é—´æ¥ä½¿ç”¨æˆ–ä¼ æ’­`SmsForwarder`çš„ä»»ä½•ä»£ç æˆ–APKï¼Œæ— è®ºè¯¥ç­‰ä½¿ç”¨æ˜¯å¦ç¬¦åˆå…¶æ‰€åœ¨å›½å®¶æˆ–åœ°åŒºï¼Œæˆ–è¯¥ç­‰ä½¿ç”¨æˆ–ä¼ æ’­å‘ç”Ÿçš„å›½å®¶æˆ–åœ°åŒºçš„æ³•å¾‹ï¼Œ`pppscn`å’Œ/æˆ–ä»£ç ä»“åº“çš„ä»»ä½•å…¶ä»–è´¡çŒ®è€…å‡ä¸å¯¹è¯¥ç­‰è¡Œä¸ºäº§ç”Ÿçš„ä»»ä½•åæœï¼ˆåŒ…æ‹¬ä½†ä¸é™äºéšç§æ³„éœ²ï¼‰è´Ÿè´£ã€‚\r\n\r\n* å¦‚æœä»»ä½•å•ä½æˆ–ä¸ªäººè®¤ä¸ºè¯¥é¡¹ç›®çš„ä»£ç /APKå¯èƒ½æ¶‰å«Œä¾µçŠ¯å…¶æƒåˆ©ï¼Œåˆ™åº”åŠæ—¶é€šçŸ¥å¹¶æä¾›èº«ä»½è¯æ˜ï¼Œæ‰€æœ‰æƒè¯æ˜ï¼Œæˆ‘ä»¬å°†åœ¨æ”¶åˆ°è®¤è¯æ–‡ä»¶ååˆ é™¤ç›¸å…³ä»£ç /APKã€‚\r\n\r\n* éšç§å£°æ˜ï¼š **SmsForwarder ä¸ä¼šæ”¶é›†ä»»ä½•æ‚¨çš„éšç§æ•°æ®ï¼ï¼ï¼** APPå¯åŠ¨æ—¶å‘é€ç‰ˆæœ¬ä¿¡æ¯å‘é€åˆ°å‹ç›Ÿç»Ÿè®¡ï¼›æ‰‹åŠ¨æ£€æŸ¥æ–°ç‰ˆæœ¬æ—¶å‘é€ç‰ˆæœ¬å·ç”¨äºæ£€æŸ¥æ–°ç‰ˆæœ¬ï¼›é™¤æ­¤ä¹‹å¤–ï¼Œæ²¡æœ‰ä»»ä½•æ•°æ®ï¼ï¼ï¼\r\n\r\n* é˜²è¯ˆæé†’ï¼š `SmsForwarder`å®Œå…¨å…è´¹å¼€æºï¼Œè¯·æ‚¨åœ¨ [æ‰“èµ](https://gitee.com/pp/SmsForwarder/wikis/pages?sort_id=4912193&doc_id=1821427) å‰åŠ¡å¿…ç¡®è®¤æ˜¯å¦å‡ºäºè‡ªæ„¿ï¼Ÿæœ¬é¡¹ç›®ä¸å‚ä¸ä»»ä½•åˆ·å•è¿”åˆ©æ‹…ä¿ï¼**è¯·æ‚¨è¿œç¦»åˆ·å•è¿”åˆ©é™·é˜±ï¼Œè°¨é˜²ç½‘ç»œè¯ˆéª—ï¼**\r\n\r\n--------\r\n\r\n## å·¥ä½œæµç¨‹ï¼š\r\n\r\n![å·¥ä½œæµç¨‹](pic/working_principle.png \"working_principle.png\")\r\n\r\n--------\r\n\r\n## ç•Œé¢é¢„è§ˆï¼š\r\n\r\n![ç•Œé¢é¢„è§ˆ](pic/screenshots.jpg \"screenshots.jpg\")\r\n\r\næ›´å¤šæˆªå›¾å‚è§ https://github.com/pppscn/SmsForwarder/wiki\r\n\r\n--------\r\n\r\n## ä¸‹è½½åœ°å€\r\n\r\n> âš  é¦–å‘åœ°å€ï¼šhttps://github.com/pppscn/SmsForwarder/releases\r\n\r\n> âš  å›½å†…é•œåƒï¼šhttps://gitee.com/pp/SmsForwarder/releases\r\n\r\n> âš  ç½‘ç›˜ä¸‹è½½ï¼šhttps://wws.lanzoui.com/b025yl86h è®¿é—®å¯†ç ï¼š`pppscn`\r\n\r\n--------\r\n\r\n## ä½¿ç”¨æ–‡æ¡£ã€æ–°ç”¨æˆ·å¿…çœ‹ï¼ã€‘\r\n\r\n> âš  GitHub Wikiï¼šhttps://github.com/pppscn/SmsForwarder/wiki\r\n\r\n> âš  Gitee Wikiï¼šhttps://gitee.com/pp/SmsForwarder/wikis/pages\r\n\r\n![ä½¿ç”¨æµç¨‹ä¸é—®é¢˜æ’æŸ¥æµç¨‹](pic/Troubleshooting_Process.png \"Troubleshooting_Process.png\")\r\n\r\n--------\r\n\r\n## åé¦ˆä¸å»ºè®®ï¼š\r\n\r\n+ æäº¤issues æˆ– pr\r\n+ åŠ å…¥äº¤æµç¾¤ï¼ˆç¾¤å†…éƒ½æ˜¯æœºæ²¹äº’å¸®äº’åŠ©ï¼Œç¦æ­¢å‘ä»»ä½•ä¸SmsForwarderä½¿ç”¨æ— å…³çš„å†…å®¹ï¼‰\r\n\r\n|                      TG Group                       |\r\n|:---------------------------------------------------:|\r\n|         ![TG Group](pic/tg.png \"TG Group\")          |\r\n| [+QBZgnL_fxYM0NjE9](https://t.me/+QBZgnL_fxYM0NjE9) |\r\n\r\n## æ„Ÿè°¢\r\n\r\n> [æ„Ÿè°¢æ‰€æœ‰èµåŠ©æœ¬é¡¹ç›®çš„çƒ­å¿ƒç½‘å‹ --> æ‰“èµåå•](https://gitee.com/pp/SmsForwarder/wikis/pages?sort_id=4912193&doc_id=1821427)\r\n\r\n> æœ¬é¡¹ç›®å¾—åˆ°ä»¥ä¸‹é¡¹ç›®çš„æ”¯æŒä¸å¸®åŠ©ï¼Œåœ¨æ­¤è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼\r\n\r\n+ https://github.com/xiaoyuanhost/TranspondSms (é¡¹ç›®åŸå‹)\r\n+ https://github.com/xuexiangjys/XUI ï¼ˆUIæ¡†æ¶ï¼‰\r\n+ https://github.com/xuexiangjys/XUpdate ï¼ˆåœ¨çº¿å‡çº§ï¼‰\r\n+ https://github.com/getActivity/XXPermissions (æƒé™è¯·æ±‚æ¡†æ¶)\r\n+ https://github.com/mainfunx/frpc_android (å†…ç½‘ç©¿é€)\r\n+ https://github.com/gyf-dev/Cactus (ä¿æ´»æªæ–½)\r\n+ https://github.com/yanzhenjie/AndServer (HttpServer)\r\n+ https://github.com/jenly1314/Location (Location)\r\n+ https://gitee.com/xuankaicat/kmnkt (socketé€šä¿¡)\r\n+ [<img src=\"https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.svg\" alt=\"GitHub license\" style=\"widthï¼š159px; height: 32px\" width=\"159\" height=\"32\" />](https://jb.gg/OpenSourceSupport)  (License Certificate for JetBrains All Products Pack)\r\n\r\n--------\r\n\r\n## å¦‚æœæ‚¨è§‰å¾—æœ¬å·¥å…·å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œä¸å¦¨åœ¨å³ä¸Šè§’ç‚¹äº®ä¸€é¢—å°æ˜Ÿæ˜Ÿï¼Œä»¥ç¤ºé¼“åŠ±ï¼\r\n\r\n<a href=\"https://star-history.com/#pppscn/SmsForwarder&Date\">\r\n  <picture>\r\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=pppscn/SmsForwarder&type=Date&theme=dark\" />\r\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=pppscn/SmsForwarder&type=Date\" />\r\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=pppscn/SmsForwarder&type=Date\" />\r\n  </picture>\r\n</a>\r\n\r\n--------\r\n\r\n## LICENSE\r\n\r\nBSD\r\n",
      "stars_today": 25
    },
    {
      "id": 21827146,
      "name": "seaweedfs",
      "full_name": "seaweedfs/seaweedfs",
      "description": "SeaweedFS is a fast distributed storage system for blobs, objects, files, and data lake, for billions of files! Blob store has O(1) disk seek, cloud tiering. Filer supports Cloud Drive, xDC replication, Kubernetes, POSIX FUSE mount, S3 API, S3 Gateway, Hadoop, WebDAV, encryption, Erasure Coding. Enterprise version is at seaweedfs.com.",
      "html_url": "https://github.com/seaweedfs/seaweedfs",
      "stars": 29529,
      "forks": 2667,
      "language": "Go",
      "topics": [
        "blob-storage",
        "cloud-drive",
        "distributed-file-system",
        "distributed-storage",
        "distributed-systems",
        "erasure-coding",
        "fuse",
        "hadoop-hdfs",
        "hdfs",
        "kubernetes",
        "object-storage",
        "posix",
        "replication",
        "s3",
        "s3-storage",
        "seaweedfs",
        "tiered-file-system"
      ],
      "created_at": "2014-07-14T16:41:37Z",
      "updated_at": "2026-01-14T22:46:13Z",
      "pushed_at": "2026-01-14T22:45:57Z",
      "open_issues": 674,
      "owner": {
        "login": "seaweedfs",
        "avatar_url": "https://avatars.githubusercontent.com/u/11985425?v=4"
      },
      "readme": "# SeaweedFS\n\n\n[![Slack](https://img.shields.io/badge/slack-purple)](https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY)\n[![Twitter](https://img.shields.io/twitter/follow/seaweedfs.svg?style=social&label=Follow)](https://twitter.com/intent/follow?screen_name=seaweedfs)\n[![Build Status](https://img.shields.io/github/actions/workflow/status/seaweedfs/seaweedfs/go.yml)](https://github.com/seaweedfs/seaweedfs/actions/workflows/go.yml)\n[![GoDoc](https://godoc.org/github.com/seaweedfs/seaweedfs/weed?status.svg)](https://godoc.org/github.com/seaweedfs/seaweedfs/weed)\n[![Wiki](https://img.shields.io/badge/docs-wiki-blue.svg)](https://github.com/seaweedfs/seaweedfs/wiki)\n[![Docker Pulls](https://img.shields.io/docker/pulls/chrislusf/seaweedfs?maxAge=4800)](https://hub.docker.com/r/chrislusf/seaweedfs/)\n[![SeaweedFS on Maven Central](https://img.shields.io/maven-central/v/com.github.chrislusf/seaweedfs-client)](https://search.maven.org/search?q=g:com.github.chrislusf)\n[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/seaweedfs)](https://artifacthub.io/packages/search?repo=seaweedfs)\n\n![SeaweedFS Logo](https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/seaweedfs.png)\n\n<h2 align=\"center\"><a href=\"https://www.patreon.com/seaweedfs\">Sponsor SeaweedFS via Patreon</a></h2>\n\nSeaweedFS is an independent Apache-licensed open source project with its ongoing development made\npossible entirely thanks to the support of these awesome [backers](https://github.com/seaweedfs/seaweedfs/blob/master/backers.md).\nIf you'd like to grow SeaweedFS even stronger, please consider joining our\n<a href=\"https://www.patreon.com/seaweedfs\">sponsors on Patreon</a>.\n\nYour support will be really appreciated by me and other supporters!\n\n<!--\n<h4 align=\"center\">Platinum</h4>\n\n<p align=\"center\">\n  <a href=\"\" target=\"_blank\">\n    Add your name or icon here\n  </a>\n</p>\n-->\n\n### Gold Sponsors\n[![nodion](https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/sponsor_nodion.png)](https://www.nodion.com)\n[![piknik](https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/piknik.png)](https://www.piknik.com)\n[![keepsec](https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/keepsec.png)](https://www.keepsec.ca)\n\n---\n\n- [Download Binaries for different platforms](https://github.com/seaweedfs/seaweedfs/releases/latest)\n- [SeaweedFS on Slack](https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY)\n- [SeaweedFS on Twitter](https://twitter.com/SeaweedFS)\n- [SeaweedFS on Telegram](https://t.me/Seaweedfs) \n- [SeaweedFS on Reddit](https://www.reddit.com/r/SeaweedFS/)\n- [SeaweedFS Mailing List](https://groups.google.com/d/forum/seaweedfs)\n- [Wiki Documentation](https://github.com/seaweedfs/seaweedfs/wiki)\n- [SeaweedFS White Paper](https://github.com/seaweedfs/seaweedfs/wiki/SeaweedFS_Architecture.pdf)\n- [SeaweedFS Introduction Slides 2025.5](https://docs.google.com/presentation/d/1tdkp45J01oRV68dIm4yoTXKJDof-EhainlA0LMXexQE/edit?usp=sharing)\n- [SeaweedFS Introduction Slides 2021.5](https://docs.google.com/presentation/d/1DcxKWlINc-HNCjhYeERkpGXXm6nTCES8mi2W5G0Z4Ts/edit?usp=sharing)\n- [SeaweedFS Introduction Slides 2019.3](https://www.slideshare.net/chrislusf/seaweedfs-introduction)\n\nTable of Contents\n=================\n\n* [Quick Start](#quick-start)\n    * [Quick Start with weed mini](#quick-start-with-weed-mini)\n    * [Quick Start for S3 API on Docker](#quick-start-for-s3-api-on-docker)\n    * [Quick Start with Single Binary](#quick-start-with-single-binary)\n* [Introduction](#introduction)\n* [Features](#features)\n    * [Additional Features](#additional-features)\n    * [Filer Features](#filer-features)\n* [Example: Using Seaweed Object Store](#example-using-seaweed-object-store)\n* [Architecture](#object-store-architecture)\n* [Compared to Other File Systems](#compared-to-other-file-systems)\n    * [Compared to HDFS](#compared-to-hdfs)\n    * [Compared to GlusterFS, Ceph](#compared-to-glusterfs-ceph)\n    * [Compared to GlusterFS](#compared-to-glusterfs)\n    * [Compared to Ceph](#compared-to-ceph)\n    * [Compared to Minio](#compared-to-minio)\n* [Dev Plan](#dev-plan)\n* [Installation Guide](#installation-guide)\n* [Disk Related Topics](#disk-related-topics)\n* [Benchmark](#benchmark)\n* [Enterprise](#enterprise)\n* [License](#license)\n\n# Quick Start #\n\n\n## Quick Start with weed mini ##\nThe easiest way to get started with SeaweedFS for development and testing:\n\n* Download the latest binary from https://github.com/seaweedfs/seaweedfs/releases and unzip a single binary file `weed` or `weed.exe`.\n\nExample:\n\n```bash\n# remove quarantine on macOS\n# xattr -d com.apple.quarantine  ./weed\n\n./weed mini -dir=/data\n```\n\nThis single command starts a complete SeaweedFS setup with:\n- **Master UI**: http://localhost:9333\n- **Volume Server**: http://localhost:9340\n- **Filer UI**: http://localhost:8888\n- **S3 Endpoint**: http://localhost:8333\n- **WebDAV**: http://localhost:7333\n- **Admin UI**: http://localhost:23646\n\nPerfect for development, testing, learning SeaweedFS, and single node deployments!\n\n## Quick Start for S3 API on Docker ##\n\n`docker run -p 8333:8333 chrislusf/seaweedfs server -s3`\n\n## Quick Start with Single Binary ##\n* Download the latest binary from https://github.com/seaweedfs/seaweedfs/releases and unzip a single binary file `weed` or `weed.exe`. Or run `go install github.com/seaweedfs/seaweedfs/weed@latest`.\n* `export AWS_ACCESS_KEY_ID=admin ; export AWS_SECRET_ACCESS_KEY=key` as the admin credentials to access the object store.\n* Run `weed server -dir=/some/data/dir -s3` to start one master, one volume server, one filer, and one S3 gateway. The difference with `weed mini` is that `weed mini` can auto configure based on the single host environment, while `weed server` requires manual configuration and are designed for production use.\n\nAlso, to increase capacity, just add more volume servers by running `weed volume -dir=\"/some/data/dir2\" -master=\"<master_host>:9333\" -port=8081` locally, or on a different machine, or on thousands of machines. That is it!\n\n# Introduction #\n\nSeaweedFS is a simple and highly scalable distributed file system. There are two objectives:\n\n1. to store billions of files!\n2. to serve the files fast!\n\nSeaweedFS started as a blob store to handle small files efficiently. \nInstead of managing all file metadata in a central master, \nthe central master only manages volumes on volume servers, \nand these volume servers manage files and their metadata. \nThis relieves concurrency pressure from the central master and spreads file metadata into volume servers, \nallowing faster file access (O(1), usually just one disk read operation).\n\nThere is only 40 bytes of disk storage overhead for each file's metadata. \nIt is so simple with O(1) disk reads that you are welcome to challenge the performance with your actual use cases.\n\nSeaweedFS started by implementing [Facebook's Haystack design paper](http://www.usenix.org/event/osdi10/tech/full_papers/Beaver.pdf). \nAlso, SeaweedFS implements erasure coding with ideas from \n[f4: Facebookâ€™s Warm BLOB Storage System](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-muralidhar.pdf), and has a lot of similarities with [Facebookâ€™s Tectonic Filesystem](https://www.usenix.org/system/files/fast21-pan.pdf) and [Google's Colossus File System](https://cloud.google.com/blog/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system)\n\nOn top of the blob store, optional [Filer] can support directories and POSIX attributes. \nFiler is a separate linearly-scalable stateless server with customizable metadata stores, \ne.g., MySql, Postgres, Redis, Cassandra, HBase, Mongodb, Elastic Search, LevelDB, RocksDB, Sqlite, MemSql, TiDB, Etcd, CockroachDB, YDB, etc.\n\nSeaweedFS can transparently integrate with the cloud. \nWith hot data on local cluster, and warm data on the cloud with O(1) access time, \nSeaweedFS can achieve both fast local access time and elastic cloud storage capacity.\nWhat's more, the cloud storage access API cost is minimized. \nFaster and cheaper than direct cloud storage!\n\n[Back to TOC](#table-of-contents)\n\n# Features #\n## Additional Blob Store Features ##\n* Support different replication levels, with rack and data center aware.\n* Automatic master servers failover - no single point of failure (SPOF).\n* Automatic compression depending on file MIME type.\n* Automatic compaction to reclaim disk space after deletion or update.\n* [Automatic entry TTL expiration][VolumeServerTTL].\n* Flexible Capacity Expansion: Any server with some disk space can add to the total storage space.\n* Adding/Removing servers does **not** cause any data re-balancing unless triggered by admin commands.\n* Optional picture resizing.\n* Support ETag, Accept-Range, Last-Modified, etc.\n* Support in-memory/leveldb/readonly mode tuning for memory/performance balance.\n* Support rebalancing the writable and readonly volumes.\n* [Customizable Multiple Storage Tiers][TieredStorage]: Customizable storage disk types to balance performance and cost.\n* [Transparent cloud integration][CloudTier]: unlimited capacity via tiered cloud storage for warm data.\n* [Erasure Coding for warm storage][ErasureCoding]  Rack-Aware 10.4 erasure coding reduces storage cost and increases availability. Enterprise version can customize EC ratio.\n\n[Back to TOC](#table-of-contents)\n\n## Filer Features ##\n* [Filer server][Filer] provides \"normal\" directories and files via HTTP.\n* [File TTL][FilerTTL] automatically expires file metadata and actual file data.\n* [Mount filer][Mount] reads and writes files directly as a local directory via FUSE.\n* [Filer Store Replication][FilerStoreReplication] enables HA for filer meta data stores.\n* [Active-Active Replication][ActiveActiveAsyncReplication] enables asynchronous one-way or two-way cross cluster continuous replication.\n* [Amazon S3 compatible API][AmazonS3API] accesses files with S3 tooling.\n* [Hadoop Compatible File System][Hadoop] accesses files from Hadoop/Spark/Flink/etc or even runs HBase.\n* [Async Replication To Cloud][BackupToCloud] has extremely fast local access and backups to Amazon S3, Google Cloud Storage, Azure, BackBlaze.\n* [WebDAV] accesses as a mapped drive on Mac and Windows, or from mobile devices.\n* [AES256-GCM Encrypted Storage][FilerDataEncryption] safely stores the encrypted data.\n* [Super Large Files][SuperLargeFiles] stores large or super large files in tens of TB.\n* [Cloud Drive][CloudDrive] mounts cloud storage to local cluster, cached for fast read and write with asynchronous write back.\n* [Gateway to Remote Object Store][GatewayToRemoteObjectStore] mirrors bucket operations to remote object storage, in addition to [Cloud Drive][CloudDrive]\n\n## Kubernetes ##\n* [Kubernetes CSI Driver][SeaweedFsCsiDriver] A Container Storage Interface (CSI) Driver. [![Docker Pulls](https://img.shields.io/docker/pulls/chrislusf/seaweedfs-csi-driver.svg?maxAge=4800)](https://hub.docker.com/r/chrislusf/seaweedfs-csi-driver/)\n* [SeaweedFS Operator](https://github.com/seaweedfs/seaweedfs-operator)\n\n[Filer]: https://github.com/seaweedfs/seaweedfs/wiki/Directories-and-Files\n[SuperLargeFiles]: https://github.com/seaweedfs/seaweedfs/wiki/Data-Structure-for-Large-Files\n[Mount]: https://github.com/seaweedfs/seaweedfs/wiki/FUSE-Mount\n[AmazonS3API]: https://github.com/seaweedfs/seaweedfs/wiki/Amazon-S3-API\n[BackupToCloud]: https://github.com/seaweedfs/seaweedfs/wiki/Async-Replication-to-Cloud\n[Hadoop]: https://github.com/seaweedfs/seaweedfs/wiki/Hadoop-Compatible-File-System\n[WebDAV]: https://github.com/seaweedfs/seaweedfs/wiki/WebDAV\n[ErasureCoding]: https://github.com/seaweedfs/seaweedfs/wiki/Erasure-coding-for-warm-storage\n[TieredStorage]: https://github.com/seaweedfs/seaweedfs/wiki/Tiered-Storage\n[CloudTier]: https://github.com/seaweedfs/seaweedfs/wiki/Cloud-Tier\n[FilerDataEncryption]: https://github.com/seaweedfs/seaweedfs/wiki/Filer-Data-Encryption\n[FilerTTL]: https://github.com/seaweedfs/seaweedfs/wiki/Filer-Stores\n[VolumeServerTTL]: https://github.com/seaweedfs/seaweedfs/wiki/Store-file-with-a-Time-To-Live\n[SeaweedFsCsiDriver]: https://github.com/seaweedfs/seaweedfs-csi-driver\n[ActiveActiveAsyncReplication]: https://github.com/seaweedfs/seaweedfs/wiki/Filer-Active-Active-cross-cluster-continuous-synchronization\n[FilerStoreReplication]: https://github.com/seaweedfs/seaweedfs/wiki/Filer-Store-Replication\n[KeyLargeValueStore]: https://github.com/seaweedfs/seaweedfs/wiki/Filer-as-a-Key-Large-Value-Store\n[CloudDrive]: https://github.com/seaweedfs/seaweedfs/wiki/Cloud-Drive-Architecture\n[GatewayToRemoteObjectStore]: https://github.com/seaweedfs/seaweedfs/wiki/Gateway-to-Remote-Object-Storage\n\n\n[Back to TOC](#table-of-contents)\n\n## Example: Using Seaweed Blob Store ##\n\nBy default, the master node runs on port 9333, and the volume nodes run on port 8080.\nLet's start one master node, and two volume nodes on port 8080 and 8081. Ideally, they should be started from different machines. We'll use localhost as an example.\n\nSeaweedFS uses HTTP REST operations to read, write, and delete. The responses are in JSON or JSONP format.\n\n### Start Master Server ###\n\n```\n> ./weed master\n```\n\n### Start Volume Servers ###\n\n```\n> weed volume -dir=\"/tmp/data1\" -max=5  -master=\"localhost:9333\" -port=8080 &\n> weed volume -dir=\"/tmp/data2\" -max=10 -master=\"localhost:9333\" -port=8081 &\n```\n\n### Write A Blob ###\n\nA blob, also referred as a needle, a chunk, or mistakenly as a file, is just a byte array. It can have attributes, such as name, mime type, create or update time, etc. But basically it is just a byte array of a relatively small size, such as 2 MB ~ 64 MB. The size is not fixed.\n\nTo upload a blob: first, send a HTTP POST, PUT, or GET request to `/dir/assign` to get an `fid` and a volume server URL:\n\n```\n> curl http://localhost:9333/dir/assign\n{\"count\":1,\"fid\":\"3,01637037d6\",\"url\":\"127.0.0.1:8080\",\"publicUrl\":\"localhost:8080\"}\n```\n\nSecond, to store the blob content, send a HTTP multi-part POST request to `url + '/' + fid` from the response:\n\n```\n> curl -F file=@/home/chris/myphoto.jpg http://127.0.0.1:8080/3,01637037d6\n{\"name\":\"myphoto.jpg\",\"size\":43234,\"eTag\":\"1cc0118e\"}\n```\n\nTo update, send another POST request with updated blob content.\n\nFor deletion, send an HTTP DELETE request to the same `url + '/' + fid` URL:\n\n```\n> curl -X DELETE http://127.0.0.1:8080/3,01637037d6\n```\n\n### Save Blob Id ###\n\nNow, you can save the `fid`, 3,01637037d6 in this case, to a database field.\n\nThe number 3 at the start represents a volume id. After the comma, it's one file key, 01, and a file cookie, 637037d6.\n\nThe volume id is an unsigned 32-bit integer. The file key is an unsigned 64-bit integer. The file cookie is an unsigned 32-bit integer, used to prevent URL guessing.\n\nThe file key and file cookie are both coded in hex. You can store the <volume id, file key, file cookie> tuple in your own format, or simply store the `fid` as a string.\n\nIf stored as a string, in theory, you would need 8+1+16+8=33 bytes. A char(33) would be enough, if not more than enough, since most uses will not need 2^32 volumes.\n\nIf space is really a concern, you can store the file id in the binary format. You would need one 4-byte integer for volume id, 8-byte long number for file key, and a 4-byte integer for the file cookie. So 16 bytes are more than enough.\n\n### Read a Blob ###\n\nHere is an example of how to render the URL.\n\nFirst look up the volume server's URLs by the file's volumeId:\n\n```\n> curl http://localhost:9333/dir/lookup?volumeId=3\n{\"volumeId\":\"3\",\"locations\":[{\"publicUrl\":\"localhost:8080\",\"url\":\"localhost:8080\"}]}\n```\n\nSince (usually) there are not too many volume servers, and volumes don't move often, you can cache the results most of the time. Depending on the replication type, one volume can have multiple replica locations. Just randomly pick one location to read.\n\nNow you can take the public URL, render the URL or directly read from the volume server via URL:\n\n```\n http://localhost:8080/3,01637037d6.jpg\n```\n\nNotice we add a file extension \".jpg\" here. It's optional and just one way for the client to specify the file content type.\n\nIf you want a nicer URL, you can use one of these alternative URL formats:\n\n```\n http://localhost:8080/3/01637037d6/my_preferred_name.jpg\n http://localhost:8080/3/01637037d6.jpg\n http://localhost:8080/3,01637037d6.jpg\n http://localhost:8080/3/01637037d6\n http://localhost:8080/3,01637037d6\n```\n\nIf you want to get a scaled version of an image, you can add some params:\n\n```\nhttp://localhost:8080/3/01637037d6.jpg?height=200&width=200\nhttp://localhost:8080/3/01637037d6.jpg?height=200&width=200&mode=fit\nhttp://localhost:8080/3/01637037d6.jpg?height=200&width=200&mode=fill\n```\n\n### Rack-Aware and Data Center-Aware Replication ###\n\nSeaweedFS applies the replication strategy at a volume level. So, when you are getting a blob id, you can specify the replication strategy. For example:\n\n```\ncurl http://localhost:9333/dir/assign?replication=001\n```\n\nThe replication parameter options are:\n\n```\n000: no replication\n001: replicate once on the same rack\n010: replicate once on a different rack, but same data center\n100: replicate once on a different data center\n200: replicate twice on two different data center\n110: replicate once on a different rack, and once on a different data center\n```\n\nMore details about replication can be found [on the wiki][Replication].\n\n[Replication]: https://github.com/seaweedfs/seaweedfs/wiki/Replication\n\nYou can also set the default replication strategy when starting the master server.\n\n### Allocate Blob Key on Specific Data Center ###\n\nVolume servers can be started with a specific data center name:\n\n```\n weed volume -dir=/tmp/1 -port=8080 -dataCenter=dc1\n weed volume -dir=/tmp/2 -port=8081 -dataCenter=dc2\n```\n\nWhen requesting a blob key, an optional \"dataCenter\" parameter can limit the assigned volume to the specific data center. For example, this specifies that the assigned volume should be limited to 'dc1':\n\n```\n http://localhost:9333/dir/assign?dataCenter=dc1\n```\n\n### Other Features ###\n  * [No Single Point of Failure][feat-1]\n  * [Insert with your own keys][feat-2]\n  * [Chunking large files][feat-3]\n  * [Collection as a Simple Name Space][feat-4]\n\n[feat-1]: https://github.com/seaweedfs/seaweedfs/wiki/Failover-Master-Server\n[feat-2]: https://github.com/seaweedfs/seaweedfs/wiki/Optimization#insert-with-your-own-keys\n[feat-3]: https://github.com/seaweedfs/seaweedfs/wiki/Optimization#upload-large-files\n[feat-4]: https://github.com/seaweedfs/seaweedfs/wiki/Optimization#collection-as-a-simple-name-space\n\n[Back to TOC](#table-of-contents)\n\n## Blob Store Architecture ##\n\nUsually distributed file systems split each file into chunks. A central server keeps a mapping of filenames to chunks, and also which chunks each chunk server has.\n\nThe main drawback is that the central server can't handle many small files efficiently, and since all read requests need to go through the central master, so it might not scale well for many concurrent users.\n\nInstead of managing chunks, SeaweedFS manages data volumes in the master server. Each data volume is 32GB in size, and can hold a lot of blobs. And each storage node can have many data volumes. So the master node only needs to store the metadata about the volumes, which is a fairly small amount of data and is generally stable.\n\nThe actual blob metadata, which are the blob volume, offset, and size, is stored in each volume on volume servers. Since each volume server only manages metadata of blobs on its own disk, with only 16 bytes for each blob, all access can read the metadata just from memory and only needs one disk operation to actually read file data.\n\nFor comparison, consider that an xfs inode structure in Linux is 536 bytes.\n\n### Master Server and Volume Server ###\n\nThe architecture is fairly simple. The actual data is stored in volumes on storage nodes. One volume server can have multiple volumes, and can both support read and write access with basic authentication.\n\nAll volumes are managed by a master server. The master server contains the volume id to volume server mapping. This is fairly static information, and can be easily cached.\n\nOn each write request, the master server also generates a file key, which is a growing 64-bit unsigned integer. Since write requests are not generally as frequent as read requests, one master server should be able to handle the concurrency well.\n\n### Write and Read files ###\n\nWhen a client sends a write request, the master server returns (volume id, file key, file cookie, volume node URL) for the blob. The client then contacts the volume node and POSTs the blob content.\n\nWhen a client needs to read a blob based on (volume id, file key, file cookie), it asks the master server by the volume id for the (volume node URL, volume node public URL), or retrieves this from a cache. Then the client can GET the content, or just render the URL on web pages and let browsers fetch the content.\n\n### Saving memory ###\n\nAll blob metadata stored on a volume server is readable from memory without disk access. Each file takes just a 16-byte map entry of <64bit key, 32bit offset, 32bit size>. Of course, each map entry has its own space cost for the map. But usually the disk space runs out before the memory does.\n\n### Tiered Storage to the cloud ###\n\nThe local volume servers are much faster, while cloud storages have elastic capacity and are actually more cost-efficient if not accessed often (usually free to upload, but relatively costly to access). With the append-only structure and O(1) access time, SeaweedFS can take advantage of both local and cloud storage by offloading the warm data to the cloud.\n\nUsually hot data are fresh and warm data are old. SeaweedFS puts the newly created volumes on local servers, and optionally upload the older volumes on the cloud. If the older data are accessed less often, this literally gives you unlimited capacity with limited local servers, and still fast for new data. \n\nWith the O(1) access time, the network latency cost is kept at minimum. \n\nIf the hot/warm data is split as 20/80, with 20 servers, you can achieve storage capacity of 100 servers. That's a cost saving of 80%! Or you can repurpose the 80 servers to store new data also, and get 5X storage throughput.\n\n[Back to TOC](#table-of-contents)\n\n## SeaweedFS Filer ##\n\nBuilt on top of the blob store, SeaweedFS Filer adds directory structure to create a file system. The directory sturcture is an interface that is implemented in many key-value stores or databases.\n\nThe content of a file is mapped to one or many blobs, distributed to multiple volumes on multiple volume servers.\n\n## Compared to Other File Systems ##\n\nMost other distributed file systems seem more complicated than necessary.\n\nSeaweedFS is meant to be fast and simple, in both setup and operation. If you do not understand how it works when you reach here, we've failed! Please raise an issue with any questions or update this file with clarifications.\n\nSeaweedFS is constantly moving forward. Same with other systems. These comparisons can be outdated quickly. Please help to keep them updated.\n\n[Back to TOC](#table-of-contents)\n\n### Compared to HDFS ###\n\nHDFS uses the chunk approach for each file, and is ideal for storing large files.\n\nSeaweedFS is ideal for serving relatively smaller files quickly and concurrently.\n\nSeaweedFS can also store extra large files by splitting them into manageable data chunks, and store the file ids of the data chunks into a meta chunk. This is managed by \"weed upload/download\" tool, and the weed master or volume servers are agnostic about it.\n\n[Back to TOC](#table-of-contents)\n\n### Compared to GlusterFS, Ceph ###\n\nThe architectures are mostly the same. SeaweedFS aims to store and read files fast, with a simple and flat architecture. The main differences are\n\n* SeaweedFS optimizes for small files, ensuring O(1) disk seek operation, and can also handle large files.\n* SeaweedFS statically assigns a volume id for a file. Locating file content becomes just a lookup of the volume id, which can be easily cached.\n* SeaweedFS Filer metadata store can be any well-known and proven data store, e.g., Redis, Cassandra, HBase, Mongodb, Elastic Search, MySql, Postgres, Sqlite, MemSql, TiDB, CockroachDB, Etcd, YDB etc, and is easy to customize.\n* SeaweedFS Volume server also communicates directly with clients via HTTP, supporting range queries, direct uploads, etc.\n\n| System         | File Metadata                   | File Content Read| POSIX  | REST API | Optimized for large number of small files |\n| -------------  | ------------------------------- | ---------------- | ------ | -------- | ------------------------- |\n| SeaweedFS      | lookup volume id, cacheable     | O(1) disk seek   |        | Yes      | Yes                       |\n| SeaweedFS Filer| Linearly Scalable, Customizable | O(1) disk seek   | FUSE   | Yes      | Yes                       |\n| GlusterFS      | hashing          |                  | FUSE, NFS          |          |                           |\n| Ceph           | hashing + rules  |                  | FUSE               | Yes      |                           |\n| MooseFS        | in memory        |                  | FUSE               |       | No                          |\n| MinIO          | separate meta file for each file  |                  |         | Yes   | No                          |\n\n[Back to TOC](#table-of-contents)\n\n### Compared to GlusterFS ###\n\nGlusterFS stores files, both directories and content, in configurable volumes called \"bricks\".\n\nGlusterFS hashes the path and filename into ids, and assigned to virtual volumes, and then mapped to \"bricks\".\n\n[Back to TOC](#table-of-contents)\n\n### Compared to MooseFS ###\n\nMooseFS chooses to neglect small file issue. From moosefs 3.0 manual, \"even a small file will occupy 64KiB plus additionally 4KiB of checksums and 1KiB for the header\", because it \"was initially designed for keeping large amounts (like several thousands) of very big files\"\n\nMooseFS Master Server keeps all meta data in memory. Same issue as HDFS namenode. \n\n[Back to TOC](#table-of-contents)\n\n### Compared to Ceph ###\n\nCeph can be setup similar to SeaweedFS as a key->blob store. It is much more complicated, with the need to support layers on top of it. [Here is a more detailed comparison](https://github.com/seaweedfs/seaweedfs/issues/120)\n\nSeaweedFS has a centralized master group to look up free volumes, while Ceph uses hashing and metadata servers to locate its objects. Having a centralized master makes it easy to code and manage.\n\nCeph, like SeaweedFS, is based on the object store RADOS. Ceph is rather complicated with mixed reviews.\n\nCeph uses CRUSH hashing to automatically manage data placement, which is efficient to locate the data. But the data has to be placed according to the CRUSH algorithm. Any wrong configuration would cause data loss. Topology changes, such as adding new servers to increase capacity, will cause data migration with high IO cost to fit the CRUSH algorithm. SeaweedFS places data by assigning them to any writable volumes. If writes to one volume failed, just pick another volume to write. Adding more volumes is also as simple as it can be.\n\nSeaweedFS is optimized for small files. Small files are stored as one continuous block of content, with at most 8 unused bytes between files. Small file access is O(1) disk read.\n\nSeaweedFS Filer uses off-the-shelf stores, such as MySql, Postgres, Sqlite, Mongodb, Redis, Elastic Search, Cassandra, HBase, MemSql, TiDB, CockroachCB, Etcd, YDB, to manage file directories. These stores are proven, scalable, and easier to manage.\n\n| SeaweedFS         | comparable to Ceph | advantage |\n| -------------  | ------------- | ---------------- |\n| Master  | MDS | simpler |\n| Volume  | OSD | optimized for small files |\n| Filer  | Ceph FS | linearly scalable, Customizable, O(1) or O(logN) |\n\n[Back to TOC](#table-of-contents)\n\n### Compared to MinIO ###\n\nMinIO follows AWS S3 closely and is ideal for testing for S3 API. It has good UI, policies, versionings, etc. SeaweedFS is trying to catch up here. It is also possible to put MinIO as a gateway in front of SeaweedFS later.\n\nMinIO metadata are in simple files. Each file write will incur extra writes to corresponding meta file.\n\nMinIO does not have optimization for lots of small files. The files are simply stored as is to local disks.\nPlus the extra meta file and shards for erasure coding, it only amplifies the LOSF problem.\n\nMinIO has multiple disk IO to read one file. SeaweedFS has O(1) disk reads, even for erasure coded files.\n\nMinIO has full-time erasure coding. SeaweedFS uses replication on hot data for faster speed and optionally applies erasure coding on warm data.\n\nMinIO does not have POSIX-like API support.\n\nMinIO has specific requirements on storage layout. It is not flexible to adjust capacity. In SeaweedFS, just start one volume server pointing to the master. That's all.\n\n## Dev Plan ##\n\n* More tools and documentation, on how to manage and scale the system.\n* Read and write stream data.\n* Support structured data.\n\nThis is a super exciting project! And we need helpers and [support](https://www.patreon.com/seaweedfs)!\n\n[Back to TOC](#table-of-contents)\n\n## Installation Guide ##\n\n> Installation guide for users who are not familiar with golang\n\nStep 1: install go on your machine and setup the environment by following the instructions at:\n\nhttps://golang.org/doc/install\n\nmake sure to define your $GOPATH\n\n\nStep 2: checkout this repo:\n```bash\ngit clone https://github.com/seaweedfs/seaweedfs.git\n```\nStep 3: download, compile, and install the project by executing the following command\n\n```bash\ncd seaweedfs/weed && make install\n```\n\nOnce this is done, you will find the executable \"weed\" in your `$GOPATH/bin` directory\n\nFor more installation options, including how to run with Docker, see the [Getting Started guide](https://github.com/seaweedfs/seaweedfs/wiki/Getting-Started).\n\n[Back to TOC](#table-of-contents)\n\n## Disk Related Topics ##\n\n### Hard Drive Performance ###\n\nWhen testing read performance on SeaweedFS, it basically becomes a performance test of your hard drive's random read speed. Hard drives usually get 100MB/s~200MB/s.\n\n### Solid State Disk ###\n\nTo modify or delete small files, SSD must delete a whole block at a time, and move content in existing blocks to a new block. SSD is fast when brand new, but will get fragmented over time and you have to garbage collect, compacting blocks. SeaweedFS is friendly to SSD since it is append-only. Deletion and compaction are done on volume level in the background, not slowing reading and not causing fragmentation.\n\n[Back to TOC](#table-of-contents)\n\n## Benchmark ##\n\nMy Own Unscientific Single Machine Results on Mac Book with Solid State Disk, CPU: 1 Intel Core i7 2.6GHz.\n\nWrite 1 million 1KB file:\n```\nConcurrency Level:      16\nTime taken for tests:   66.753 seconds\nCompleted requests:      1048576\nFailed requests:        0\nTotal transferred:      1106789009 bytes\nRequests per second:    15708.23 [#/sec]\nTransfer rate:          16191.69 [Kbytes/sec]\n\nConnection Times (ms)\n              min      avg        max      std\nTotal:        0.3      1.0       84.3      0.9\n\nPercentage of the requests served within a certain time (ms)\n   50%      0.8 ms\n   66%      1.0 ms\n   75%      1.1 ms\n   80%      1.2 ms\n   90%      1.4 ms\n   95%      1.7 ms\n   98%      2.1 ms\n   99%      2.6 ms\n  100%     84.3 ms\n```\n\nRandomly read 1 million files:\n```\nConcurrency Level:      16\nTime taken for tests:   22.301 seconds\nCompleted requests:      1048576\nFailed requests:        0\nTotal transferred:      1106812873 bytes\nRequests per second:    47019.38 [#/sec]\nTransfer rate:          48467.57 [Kbytes/sec]\n\nConnection Times (ms)\n              min      avg        max      std\nTotal:        0.0      0.3       54.1      0.2\n\nPercentage of the requests served within a certain time (ms)\n   50%      0.3 ms\n   90%      0.4 ms\n   98%      0.6 ms\n   99%      0.7 ms\n  100%     54.1 ms\n```\n\n### Run WARP and launch a mixed benchmark. ###\n\n```\nmake benchmark\nwarp: Benchmark data written to \"warp-mixed-2025-12-05[194844]-kBpU.csv.zst\"\n\nMixed operations.\nOperation: DELETE, 10%, Concurrency: 20, Ran 42s.\n * Throughput: 55.13 obj/s\n\nOperation: GET, 45%, Concurrency: 20, Ran 42s.\n * Throughput: 2477.45 MiB/s, 247.75 obj/s\n\nOperation: PUT, 15%, Concurrency: 20, Ran 42s.\n * Throughput: 825.85 MiB/s, 82.59 obj/s\n\nOperation: STAT, 30%, Concurrency: 20, Ran 42s.\n * Throughput: 165.27 obj/s\n\nCluster Total: 3302.88 MiB/s, 550.51 obj/s over 43s.\n```\n\n[Back to TOC](#table-of-contents)\n\n## Enterprise ##\n\nFor enterprise users, please visit [seaweedfs.com](https://seaweedfs.com) for the SeaweedFS Enterprise Edition, \nwhich has a self-healing storage format with better data protection.\n\n[Back to TOC](#table-of-contents)\n\n## License ##\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\nThe text of this page is available for modification and reuse under the terms of the Creative Commons Attribution-Sharealike 3.0 Unported License and the GNU Free Documentation License (unversioned, with no invariant sections, front-cover texts, or back-cover texts).\n\n[Back to TOC](#table-of-contents)\n\n## Stargazers over time\n[![Stargazers over time](https://starchart.cc/seaweedfs/seaweedfs.svg?variant=adaptive)](https://starchart.cc/seaweedfs/seaweedfs)\n",
      "stars_today": 22
    },
    {
      "id": 1013830656,
      "name": "bitchat",
      "full_name": "permissionlesstech/bitchat",
      "description": "bluetooth mesh chat, IRC vibes",
      "html_url": "https://github.com/permissionlesstech/bitchat",
      "stars": 23922,
      "forks": 2239,
      "language": "Swift",
      "topics": [],
      "created_at": "2025-07-04T14:34:38Z",
      "updated_at": "2026-01-15T00:39:43Z",
      "pushed_at": "2026-01-15T00:39:39Z",
      "open_issues": 219,
      "owner": {
        "login": "permissionlesstech",
        "avatar_url": "https://avatars.githubusercontent.com/u/220183803?v=4"
      },
      "readme": "<img width=\"256\" height=\"256\" alt=\"icon_128x128@2x\" src=\"https://github.com/user-attachments/assets/90133f83-b4f6-41c6-aab9-25d0859d2a47\" />\n\n## bitchat\n\nA decentralized peer-to-peer messaging app with dual transport architecture: local Bluetooth mesh networks for offline communication and internet-based Nostr protocol for global reach. No accounts, no phone numbers, no central servers. It's the side-groupchat.\n\n[bitchat.free](http://bitchat.free)\n\nğŸ“² [App Store](https://apps.apple.com/us/app/bitchat-mesh/id6748219622)\n\n## License\n\nThis project is released into the public domain. See the [LICENSE](LICENSE) file for details.\n\n## Features\n\n- **Dual Transport Architecture**: Bluetooth mesh for offline + Nostr protocol for internet-based messaging\n- **Location-Based Channels**: Geographic chat rooms using geohash coordinates over global Nostr relays\n- **Intelligent Message Routing**: Automatically chooses best transport (Bluetooth â†’ Nostr fallback)\n- **Decentralized Mesh Network**: Automatic peer discovery and multi-hop message relay over Bluetooth LE\n- **Privacy First**: No accounts, no phone numbers, no persistent identifiers\n- **Private Message End-to-End Encryption**: [Noise Protocol](https://noiseprotocol.org) for mesh, NIP-17 for Nostr\n- **IRC-Style Commands**: Familiar `/slap`, `/msg`, `/who` style interface\n- **Universal App**: Native support for iOS and macOS\n- **Emergency Wipe**: Triple-tap to instantly clear all data\n- **Performance Optimizations**: LZ4 message compression, adaptive battery modes, and optimized networking\n\n## [Technical Architecture](https://deepwiki.com/permissionlesstech/bitchat)\n\nBitChat uses a **hybrid messaging architecture** with two complementary transport layers:\n\n### Bluetooth Mesh Network (Offline)\n\n- **Local Communication**: Direct peer-to-peer within Bluetooth range\n- **Multi-hop Relay**: Messages route through nearby devices (max 7 hops)\n- **No Internet Required**: Works completely offline in disaster scenarios\n- **Noise Protocol Encryption**: End-to-end encryption with forward secrecy\n- **Binary Protocol**: Compact packet format optimized for Bluetooth LE constraints\n- **Automatic Discovery**: Peer discovery and connection management\n- **Adaptive Power**: Battery-optimized duty cycling\n\n### Nostr Protocol (Internet)\n\n- **Global Reach**: Connect with users worldwide via internet relays\n- **Location Channels**: Geographic chat rooms using geohash coordinates\n- **290+ Relay Network**: Distributed across the globe for reliability\n- **NIP-17 Encryption**: Gift-wrapped private messages for internet privacy\n- **Ephemeral Keys**: Fresh cryptographic identity per geohash area\n\n### Channel Types\n\n#### `mesh #bluetooth`\n\n- **Transport**: Bluetooth Low Energy mesh network\n- **Scope**: Local devices within multi-hop range\n- **Internet**: Not required\n- **Use Case**: Offline communication, protests, disasters, remote areas\n\n#### Location Channels (`block #dr5rsj7`, `neighborhood #dr5rs`, `country #dr`)\n\n- **Transport**: Nostr protocol over internet\n- **Scope**: Geographic areas defined by geohash precision\n  - `block` (7 chars): City block level\n  - `neighborhood` (6 chars): District/neighborhood\n  - `city` (5 chars): City level\n  - `province` (4 chars): State/province\n  - `region` (2 chars): Country/large region\n- **Internet**: Required (connects to Nostr relays)\n- **Use Case**: Location-based community chat, local events, regional discussions\n\n### Direct Message Routing\n\nPrivate messages use **intelligent transport selection**:\n\n1. **Bluetooth First** (preferred when available)\n\n   - Direct connection with established Noise session\n   - Fastest and most private option\n\n2. **Nostr Fallback** (when Bluetooth unavailable)\n\n   - Uses recipient's Nostr public key\n   - NIP-17 gift-wrapping for privacy\n   - Routes through global relay network\n\n3. **Smart Queuing** (when neither available)\n   - Messages queued until transport becomes available\n   - Automatic delivery when connection established\n\nFor detailed protocol documentation, see the [Technical Whitepaper](WHITEPAPER.md).\n\n## Setup\n\n### Option 1: Using Xcode\n\n   ```bash\n   cd bitchat\n   open bitchat.xcodeproj\n   ```\n\n   To run on a device there're a few steps to prepare the code:\n   - Clone the local configs: `cp Configs/Local.xcconfig.example Configs/Local.xcconfig`\n   - Add your Developer Team ID into the newly created `Configs/Local.xcconfig`\n      - Bundle ID would be set to `chat.bitchat.<team_id>` (unless you set to something else)\n   - Entitlements need to be updated manually (TODO: Automate):\n      - Search and replace `group.chat.bitchat` with `group.<your_bundle_id>` (e.g. `group.chat.bitchat.ABC123`)\n\n### Option 2: Using `just`\n\n   ```bash\n   brew install just\n   ```\n\nWant to try this on macos: `just run` will set it up and run from source.\nRun `just clean` afterwards to restore things to original state for mobile app building and development.\n\n## Localization\n\n- Base app resources live under `bitchat/Localization/Base.lproj/`. Add new copy to `Localizable.strings` and plural rules to `Localizable.stringsdict`.\n- Share extension strings are separate in `bitchatShareExtension/Localization/Base.lproj/Localizable.strings`.\n- Prefer keys that describe intent (`app_info.features.offline.title`) and reuse existing ones where possible.\n- Run `xcodebuild -project bitchat.xcodeproj -scheme \"bitchat (macOS)\" -configuration Debug CODE_SIGNING_ALLOWED=NO build` to compile-check any localization updates.\n",
      "stars_today": 22
    },
    {
      "id": 117948953,
      "name": "Atmosphere",
      "full_name": "Atmosphere-NX/Atmosphere",
      "description": "AtmosphÃ¨re is a work-in-progress customized firmware for the Nintendo Switch.",
      "html_url": "https://github.com/Atmosphere-NX/Atmosphere",
      "stars": 17711,
      "forks": 1374,
      "language": "C++",
      "topics": [],
      "created_at": "2018-01-18T07:36:33Z",
      "updated_at": "2026-01-14T23:39:17Z",
      "pushed_at": "2026-01-14T06:29:41Z",
      "open_issues": 39,
      "owner": {
        "login": "Atmosphere-NX",
        "avatar_url": "https://avatars.githubusercontent.com/u/37918415?v=4"
      },
      "readme": "\n![Banner](img/banner.png?raw=true)\n=====\n\n![License](https://img.shields.io/badge/License-GPLv2-blue.svg)\n[![Chat on Discord](https://img.shields.io/badge/Discord-5865f2?logo=discord&logoColor=white)](https://discordapp.com/invite/ZdqEhed)\n![Made with Notepad++](img/np++.png?raw=true)\n\nAtmosphÃ¨re is a work-in-progress customized firmware for the Nintendo Switch.\n\nComponents\n=====\n\nAtmosphÃ¨re consists of multiple components, each of which replaces/modifies a different component of the system:\n\n* FusÃ©e: First-stage Loader, responsible for loading and validating stage 2 (custom TrustZone) plus package2 (Kernel/FIRM sysmodules), and patching them as needed. This replaces all functionality normally in Package1loader/NX Bootloader.\n* ExosphÃ¨re: Customized TrustZone, to run a customized Secure Monitor\n* ThermosphÃ¨re: EL2 EmuNAND support, i.e. backing up and using virtualized/redirected NAND images\n* StratosphÃ¨re: Custom Sysmodule(s), both Rosalina style to extend the kernel/provide new features, and of the loader reimplementation style to hook important system actions\n* TroposphÃ¨re: Application-level Horizon OS patches, used to implement desirable CFW features\n\nLicensing\n=====\n\nThis software is licensed under the terms of the GPLv2, with exemptions for specific projects noted below.\n\nYou can find a copy of the license in the [LICENSE file](LICENSE).\n\nExemptions:\n* [Nintendo](https://github.com/Nintendo) is exempt from GPLv2 licensing and may (at its option) instead license any source code authored for the AtmosphÃ¨re project under the Zero-Clause BSD license.\n\nCredits\n=====\n\nAtmosphÃ¨re is currently being developed and maintained by __SciresM__, __TuxSH__, __hexkyz__, and __fincs__.<br>\nIn no particular order, we credit the following for their invaluable contributions:\n\n* __switchbrew__ for the [libnx](https://github.com/switchbrew/libnx) project and the extensive [documentation, research and tool development](http://switchbrew.org) pertaining to the Nintendo Switch.\n* __devkitPro__ for the [devkitA64](https://devkitpro.org/) toolchain and libnx support.\n* __ReSwitched Team__ for additional [documentation, research and tool development](https://reswitched.github.io/) pertaining to the Nintendo Switch.\n* __ChaN__ for the [FatFs](http://elm-chan.org/fsw/ff/00index_e.html) module.\n* __Marcus Geelnard__ for the [bcl-1.2.0](https://sourceforge.net/projects/bcl/files/bcl/bcl-1.2.0) library.\n* __naehrwert__ and __st4rk__ for the original [hekate](https://github.com/nwert/hekate) project and its hwinit code base.\n* __CTCaer__ for the continued [hekate](https://github.com/CTCaer/hekate) project's fork and the [minerva_tc](https://github.com/CTCaer/minerva_tc) project.\n* __m4xw__ for development of the [emuMMC](https://github.com/m4xw/emummc) project.\n* __Riley__ for suggesting \"Atmosphere\" as a Horizon OS reimplementation+customization project name.\n* __hedgeberg__ for research and hardware testing.\n* __lioncash__ for code cleanup and general improvements.\n* __jaames__ for designing and providing AtmosphÃ¨re's graphical resources.\n* Everyone who submitted entries for AtmosphÃ¨re's [splash design contest](https://github.com/Atmosphere-NX/Atmosphere-splashes).\n* _All those who actively contribute to the AtmosphÃ¨re repository._\n",
      "stars_today": 22
    },
    {
      "id": 566323731,
      "name": "Easydict",
      "full_name": "tisfeng/Easydict",
      "description": "ä¸€ä¸ªç®€æ´ä¼˜é›…çš„è¯å…¸ç¿»è¯‘ macOS Appã€‚å¼€ç®±å³ç”¨ï¼Œæ”¯æŒç¦»çº¿ OCR è¯†åˆ«ï¼Œæ”¯æŒæœ‰é“è¯å…¸ï¼ŒğŸ è‹¹æœç³»ç»Ÿè¯å…¸ï¼ŒğŸ è‹¹æœç³»ç»Ÿç¿»è¯‘ï¼ŒOpenAIï¼ŒGeminiï¼ŒDeepLï¼ŒGoogleï¼ŒBingï¼Œè…¾è®¯ï¼Œç™¾åº¦ï¼Œé˜¿é‡Œï¼Œå°ç‰›ï¼Œå½©äº‘å’Œç«å±±ç¿»è¯‘ã€‚A concise and elegant Dictionary and Translator macOS App for looking up words and translating text. ",
      "html_url": "https://github.com/tisfeng/Easydict",
      "stars": 11800,
      "forks": 579,
      "language": "Swift",
      "topics": [
        "app",
        "baidu",
        "bing",
        "deepl",
        "dictionary",
        "gemini",
        "google",
        "macos",
        "ocr",
        "openai",
        "shortcuts",
        "tencent",
        "translate",
        "translator",
        "youdao"
      ],
      "created_at": "2022-11-15T12:41:53Z",
      "updated_at": "2026-01-15T00:45:36Z",
      "pushed_at": "2026-01-11T16:31:39Z",
      "open_issues": 120,
      "owner": {
        "login": "tisfeng",
        "avatar_url": "https://avatars.githubusercontent.com/u/25194972?v=4"
      },
      "readme": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/icon_512x512@2x.png\" height=\"256\">\n  <h1 align=\"center\">Easydict</h1>\n  <h4 align=\"center\"> Easy to look up words or translate text</h4>\n<p align=\"center\"> \n<a href=\"https://github.com/tisfeng/easydict/blob/main/LICENSE\">\n<img src=\"https://img.shields.io/github/license/tisfeng/easydict\"\n            alt=\"License\"></a>\n<a href=\"https://github.com/tisfeng/Easydict/releases\">\n<img src=\"https://img.shields.io/github/downloads/tisfeng/easydict/total.svg\"\n            alt=\"Downloads\"></a>\n<a href=\"https://img.shields.io/badge/-macOS-black?&logo=apple&logoColor=white\">\n<img src=\"https://img.shields.io/badge/-macOS-black?&logo=apple&logoColor=white\"\n            alt=\"macOS\"></a>  \n</p>\n\n<div align=\"center\">\n<a href=\"./README_ZH.md\">ä¸­æ–‡</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href=\"./README.md\">English</a>\n</div>\n\n## Easydict\n\n`Easydict` is a concise and easy-to-use translation dictionary macOS App that allows you to easily and elegantly look up words or translate text.\n\nEasydict is ready to use out of the box, can automatically recognize the language of the input text, supports input translate, select translate, and OCR screenshot translate, and can query multiple translation services results at the same time.\n\n**Supported translation services:** [**ğŸ Apple Dictionary**](./docs/en/How-to-use-macOS-system-dictionary-in-Easydict.md), [ğŸ **Apple Translate**](./docs/en/How-to-use-macOS-system-translation-in-Easydict.md), [OpenAI](https://chat.openai.com/), [Gemini](https://gemini.google.com/), [DeepSeek](https://www.deepseek.com/), [Ollama](https://ollama.com/), [Groq](https://groq.com/), [Zhipu AI](https://open.bigmodel.cn/), [GitHub Models](https://github.com/marketplace/models), [DeepL](https://www.deepl.com/translator), [Google](https://translate.google.com), [Youdao](https://www.youdao.com/), [Tencent](https://fanyi.qq.com/), [Bing](https://www.bing.com/translator), [Baidu](https://fanyi.baidu.com/), [Niutrans](https://niutrans.com/), [Caiyun](https://fanyi.caiyunapp.com/), [Alibaba](https://translate.alibaba.com/), [Volcano](https://translate.volcengine.com/translate) and [Doubao](https://www.volcengine.com/docs/82379/1820188).\n\n![Log](https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/Log-1688378715.png)\n\n<table>\n    <td> <img src=\"https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/iShot_2023-05-28_16.32.18-1685262784.png\">\n    <td> <img src=\"https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/iShot_2023-05-28_16.32.26-1685262803.png\">\n</table>\n\n![immerse-1686534718.gif](https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/immerse-1686534718.gif)\n\n## Features\n\n- ğŸš€ Out of the box, automatic language recognition\n- ğŸ–±ï¸ Auto select with mouse and shortcut key\n- ğŸ“¸ OCR screenshot translation and slient screenshot OCR\n- ğŸ”Š Multiple TTS voice services\n- ğŸ“š Support ğŸ [Apple System Dictionary](./docs/en/How-to-use-macOS-system-dictionary-in-Easydict.md) and [System Translation](./docs/en/How-to-use-macOS-system-translation-in-Easydict.md)\n- ğŸŒ Support 20+ translation services (OpenAI, Gemini, DeepL, Google, Ollama, Groq, etc.)\n- ğŸ—£ï¸ Support for 48 languages\n\n**If you like this app, please consider giving it a [Star](https://github.com/tisfeng/Easydict) â­ï¸, thanks! (^-^)**\n\n## Installation\n\n### Homebrew Installation (Recommended)\n\n```bash\nbrew install --cask easydict\n```\n\n### Manual Installation\n\n[Download](https://github.com/tisfeng/Easydict/releases) the latest release.\n\n> [!NOTE]\n> Latest version supports macOS 13.0+, for older systems please use [2.7.2](https://github.com/tisfeng/Easydict/releases/tag/2.7.2)\n\n---\n\n## Usage\n\n| Ways                      | Description                                                                                                                                  | Preview                                                                                                                                        |\n| ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n| Input Translate           | Press the input translate shortcut key (default `âŒ¥ + A`), enter the text to be translated, and `Enter` key to translate          | ![iShot_2023-01-20_11.28.46-1674185354](https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/iShot_2023-01-20_11.28.46-1674185354.gif) |\n| Mouse Select Translate    | The query icon is automatically displayed after the word is selected, and the mouse hovers over it to query                                  | ![iShot_2023-01-20_11.01.35-1674183779](https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/iShot_2023-01-20_11.01.35-1674183779.gif) |\n| Shortcut Select Translate | After selecting the text to be translated, press the shortcut key (default `âŒ¥ + D`)                                                          | ![iShot_2023-01-20_11.24.37-1674185125](https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/iShot_2023-01-20_11.24.37-1674185125.gif) |\n| Screenshot Translate      | Press the screenshot translate shortcut key (default `âŒ¥ + S`) to capture the area to be translated                                           | ![iShot_2023-01-20_11.26.25-1674185209](https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/iShot_2023-01-20_11.26.25-1674185209.gif) |\n| Silent Screenshot OCR     | Press the Silent Screenshot shortcut keyï¼ˆdefault `âŒ¥ + â‡§ + S`ï¼‰to capture the area, the OCR results will be copied directly to the clipboard | ![å±å¹•å½•åˆ¶ 2023-05-20 22 39 11](https://github.com/Jerry23011/Easydict/assets/89069957/c16f3c20-1748-411e-be04-11d8fe0e61af)                     |\n\n---\n\n## Documentation\n\n- ğŸ“– [Complete Usage Guide](./docs/en/GUIDE.md) - Detailed features, configuration and tips\n- ğŸ”§ [Developer Build Guide](./docs/en/GUIDE.md#developer-build) - Build and run from source code\n- ğŸ [How to use macOS System Dictionary](./docs/en/How-to-use-macOS-system-dictionary-in-Easydict.md)\n- ğŸ [How to use macOS System Translation](./docs/en/How-to-use-macOS-system-translation-in-Easydict.md)\n- ğŸŒ [How to translate Easydict](./docs/How-to-translate-Easydict-en.md)\n\n---\n\n## Acknowledgements\n\n- This project was inspired by [saladict](https://github.com/crimx/ext-saladict) and [Bob](https://github.com/ripperhe/Bob), and the initial version was made based on [Bob (GPL-3.0)](https://github.com/1xiaocainiao/Bob). Easydict has made many improvements and optimizations on the original project, and many features and UI are referenced from Bob.\n- Screenshot feature is based on [isee15](https://github.com/isee15)'s [Capture-Screen-For-Multi-Screens-On-Mac](https://github.com/isee15/Capture-Screen-For-Multi-Screens-On-Mac), and optimized on this project.\n- Select text feature is referenced from [PopClip](https://pilotmoon.com/popclip/).\n\n## Statement\n\nEasydict is licensed under the [GPL-3.0](https://github.com/tisfeng/Easydict/blob/main/LICENSE) open source license, which is for learning and communication only. Anyone can get this product and source code for free. If you believe that your legal rights have been violated, please contact the [author](https://github.com/tisfeng) immediately. You can use the source code freely, but you must attach the corresponding license and copyright.\n\n## Sponsor\n\nEasydict is a free and open source project, currently mainly developed and maintained by the author. If you like this project and find it helpful, you can consider sponsoring this project to support it, so that it can go further.\n\nThanks to [@CanglongCl](https://github.com/CanglongCl) for providing the Apple Developer account, which solved the app [signature issue](https://github.com/tisfeng/Easydict/issues/2), allowing more people to use Easydict conveniently.\n\n<a href=\"https://afdian.com/a/tisfeng\"><img width=\"20%\" src=\"https://pic1.afdiancdn.com/static/img/welcome/button-sponsorme.jpg\" alt=\"\"></a>\n\n<div>\n  <img src=\"https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/IMG_4739-1684680971.JPG\" width=\"30%\">\n</div>\n\nThanks to all sponsors for their generous support. For details, please see the [Sponsor List](./docs/en/SPONSOR_LIST.md).\n\n---\n\n## Star History\n\n<a href=\"https://star-history.com/#tisfeng/easydict&Date\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=tisfeng/easydict&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=tisfeng/easydict&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=tisfeng/easydict&type=Date\" />\n  </picture>\n</a>",
      "stars_today": 22
    },
    {
      "id": 292065578,
      "name": "czkawka",
      "full_name": "qarmin/czkawka",
      "description": "Multi functional app to find duplicates, empty folders, similar images etc.",
      "html_url": "https://github.com/qarmin/czkawka",
      "stars": 28057,
      "forks": 911,
      "language": "Rust",
      "topics": [
        "cleaner",
        "duplicates",
        "gtk-rs",
        "multiplatform",
        "rust",
        "similar-images",
        "similar-music",
        "similar-videos"
      ],
      "created_at": "2020-09-01T17:37:29Z",
      "updated_at": "2026-01-15T00:47:56Z",
      "pushed_at": "2026-01-13T06:25:50Z",
      "open_issues": 437,
      "owner": {
        "login": "qarmin",
        "avatar_url": "https://avatars.githubusercontent.com/u/41945903?v=4"
      },
      "readme": "![krokiet_logo](https://github.com/user-attachments/assets/567a7a38-d754-4a79-86b5-3cc898dbbade)\n\n**Krokiet** ((IPA: [ËˆkrÉ”cÉ›t]), \"croquet\" in Polish) new generation GUI frontend, simple, multiplatform, fast and free app to remove unnecessary files from your computer.\n\n![czkawka_logo](https://user-images.githubusercontent.com/41945903/102616149-66490400-4137-11eb-9cd6-813b2b070834.png)\n\n**Czkawka** (_tchâ€¢kavâ€¢ka_ (IPA: [ËˆÊ§Ì‘kafka]), \"hiccup\" in Polish) older gtk4 GUI frontend, superseded by Krokiet, but still receiving bugfix updates.\n\n## Features\n\n- Written in memory-safe Rust - almost 100% unsafe code free\n- Amazingly fast - due to using more or less advanced algorithms and multithreading\n- Free, Open Source without ads\n- Multiplatform - works on Linux, Windows, macOS, FreeBSD and many more\n- Cache support - second and further scans should be much faster than the first one\n- CLI frontend - for easy automation\n- GUI frontend - uses Slint or GTK 4 frameworks\n- Core library - allows to reuse functionality in other apps\n- No spying - Czkawka does not have access to the Internet, nor does it collect any user information or statistics\n- Multilingual - support multiple languages like Polish, English or Italian\n- Multiple tools to use:\n    - Duplicates - Finds duplicates based on file name, size or hash\n    - Empty Folders - Finds empty folders with the help of an advanced algorithm\n    - Big Files - Finds the provided number of the biggest files in given location\n    - Empty Files - Looks for empty files across the drive\n    - Temporary Files - Finds temporary files\n    - Similar Images - Finds images which are not exactly the same (different resolution, watermarks)\n    - Similar Videos - Looks for visually similar videos\n    - Same Music - Searches for similar music by tags or by reading content and comparing it\n    - Invalid Symbolic Links - Shows symbolic links which point to non-existent files/directories\n    - Broken Files - Finds files that are invalid or corrupted\n    - Bad Extensions - Lists files whose content not match with their extension\n    - Exif Remover(Experimental) - Tool to remove Exif metadata from various file types\n    - Video Optimizer(Experimental) - Converts videos to more efficient formats\n\n![Krokiet](https://github.com/user-attachments/assets/720e98c3-598a-41aa-a04b-0c0c1d8a28e6)\n\n![Czkawka](https://github.com/user-attachments/assets/b0409515-1bec-4e13-8fac-7bdfa15f5848)\n\nChangelog about each version can be found in [CHANGELOG.md](Changelog.md).\n\nNew releases can be found in [Github releases](https://github.com/qarmin/czkawka/releases) and nightly builds also in [Nightly releases](https://github.com/qarmin/czkawka/releases/tag/Nightly)\n\n## Usage, installation, compilation, requirements, license\n\nEach tool uses different technologies, so you can find instructions for each of them in the appropriate file:\n\n- [Czkawka GUI (GTK frontend)](czkawka_gui/README.md)</br>\n- [Czkawka CLI](czkawka_cli/README.md)</br>\n- [Czkawka Core](czkawka_core/README.md)</br>\n- [Krokiet GUI (Slint frontend)](krokiet/README.md)</br>\n\n## Comparison to other tools\n\nBleachbit is a master at finding and removing temporary files, while Czkawka only finds the most basic ones. So these\ntwo apps shouldn't be compared directly or be considered as an alternative to one another.\n\nIn this comparison remember, that even if app have same features they may work different(e.g. one app may have more\noptions to choose than other).\n\n|                           |   Czkawka   |   Krokiet   | FSlint |     DupeGuru      |  Bleachbit  |\n|:-------------------------:|:-----------:|:-----------:|:------:|:-----------------:|:-----------:|\n|         Language          |    Rust     |    Rust     | Python |   Python/Obj-C    |   Python    |\n|  Framework base language  |      C      |    Rust     |   C    | C/C++/Obj-C/Swift |      C      |\n|         Framework         |    GTK 4    |    Slint    | PyGTK2 | Qt 5 (PyQt)/Cocoa |   PyGTK3    |\n|            OS             | Lin,Mac,Win | Lin,Mac,Win |  Lin   |    Lin,Mac,Win    | Lin,Mac,Win |\n|     Duplicate finder      |      âœ”      |      âœ”      |   âœ”    |         âœ”         |             |\n|        Empty files        |      âœ”      |      âœ”      |   âœ”    |                   |             |\n|       Empty folders       |      âœ”      |      âœ”      |   âœ”    |                   |             |\n|      Temporary files      |      âœ”      |      âœ”      |   âœ”    |                   |      âœ”      |\n|         Big files         |      âœ”      |      âœ”      |        |                   |             |\n|      Similar images       |      âœ”      |      âœ”      |        |         âœ”         |             |\n|      Similar videos       |      âœ”      |      âœ”      |        |                   |             |\n|  Music duplicates(tags)   |      âœ”      |      âœ”      |        |         âœ”         |             |\n| Music duplicates(content) |      âœ”      |      âœ”      |        |                   |             |\n|     Invalid symlinks      |      âœ”      |      âœ”      |   âœ”    |                   |             |\n|       Broken files        |      âœ”      |      âœ”      |        |                   |             |\n| Invalid names/extensions  |      âœ”      |      âœ”      |   âœ”    |                   |             |\n|      Names conflict       |             |             |   âœ”    |                   |             |\n|    Installed packages     |             |             |   âœ”    |                   |             |\n|          Bad ID           |             |             |   âœ”    |                   |             |\n|   Non stripped binaries   |             |             |   âœ”    |                   |             |\n|   Redundant whitespace    |             |             |   âœ”    |                   |             |\n|     Overwriting files     |             |             |   âœ”    |                   |      âœ”      |\n|    Multiple languages     |      âœ”      |      âœ”      |   âœ”    |         âœ”         |      âœ”      |\n|       Cache support       |      âœ”      |      âœ”      |        |         âœ”         |             |\n|   In active development   |     Yes     |     Yes     |   No   |        No<sup>*</sup>        |     Yes     |\n\n<p><sup>*</sup> Last commit in 2024 and last version released in 2023</p> \n\n## Other apps\n\nThere are many similar applications to Czkawka on the Internet, which do some things better and some things worse:\n\n### GUI\n\n- [DupeGuru](https://github.com/arsenetar/dupeguru) - Many options to customize; great photo compare tool\n- [FSlint](https://github.com/pixelb/fslint) - A little outdated, but still have some tools not available in Czkawka\n- [AntiDupl.NET](https://github.com/ermig1979/AntiDupl) - Shows a lot of metadata of compared images\n- [Video Duplicate Finder](https://github.com/0x90d/videoduplicatefinder) - Finds similar videos(surprising, isn't it), supports video thumbnails\n\n### CLI\n\nDue to limited time, the biggest emphasis is on the GUI version so if you are looking for really good and feature-packed\nconsole apps, then take a look at these:\n\n- [Fclones](https://github.com/pkolaczk/fclones) - One of the fastest tools to find duplicates; it is written also in\n  Rust\n- [Rmlint](https://github.com/sahib/rmlint) - Nice console interface and also is feature packed\n- [RdFind](https://github.com/pauldreik/rdfind) - Fast, but written in C++ Â¯\\\\\\_(ãƒ„)\\_/Â¯\n\n\n## Projects using Czkawka\n\nCzkawka exposes its common functionality through a crate called **`czkawka_core`**, which can be reused by other projects.\n\nIt is written in Rust and is used by all Czkawka frontends (`czkawka_gui`, `czkawka_cli`, `krokiet`).\n\nIt is also used by external projects, such as:\n\n- **page-dewarp** â€“ https://github.com/lmmx/page-dewarp - A library for dewarping document images using a cubic sheet model.\n\nBindings are also available for:\n\n- **Python** â€“ https://pypi.org/project/czkawka/\n\nSome projects work as wrappers around `czkawka_cli`. Without directly depending on `czkawka_core`, they allow simple scanning and retrieving results in JSON format:\n\n- **Schluckauf** â€“ https://github.com/fadykuzman/schluckauf\n\n## Thanks\n\nBig thanks to PÃ¡draig Brady, creator of fantastic FSlint, because without his work I wouldn't create this tool.\n\nThanks also to all the people who create patches for this program, make it available on other systems, create videos,\narticles about it etc.\n\nAlso, I really appreciate work of people that create crates on which Czkawka is based and for that I try to report bugs\nto make it even better.\n\n## Officially Supported Projects\nOnly this repository, [prebuild-binaries](https://github.com/qarmin/czkawka/releases), projects on [crates.io](https://crates.io/crates/czkawka_gui) and [flathub](https://flathub.org/apps/com.github.qarmin.czkawka) are directly maintained by me.  \n\nCzkawka does not have an official website, so do not trust any sites that claim to be the official one.  \n\nIf you use packages from unofficial sources, make sure they are safe.\n\n## License\n\nThe entire code in this repository is licensed under the [MIT](https://mit-license.org/) license.\n\nAll images are licensed under the [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n\nThe Czkawka GTK GUI and CLI applications are licensed under the [MIT](https://mit-license.org/) license, while the Krokiet is licensed under the [GPL-3.0-only](https://www.gnu.org/licenses/gpl-3.0.en.html) license.\n\n## Donations\n\nIf you are using the app, I would appreciate a donation for its further development, which can be\ndone [here](https://github.com/sponsors/qarmin).\n\n",
      "stars_today": 21
    },
    {
      "id": 628160489,
      "name": "SimpMusic",
      "full_name": "maxrave-dev/SimpMusic",
      "description": "A cross-platform music app using YouTube Music for backend",
      "html_url": "https://github.com/maxrave-dev/SimpMusic",
      "stars": 7101,
      "forks": 320,
      "language": "Kotlin",
      "topics": [
        "android",
        "android-16",
        "android-app",
        "android-application",
        "android-auto",
        "compose-multiplatform",
        "exoplayer",
        "kotlin",
        "linux",
        "macos",
        "media3",
        "mp3",
        "music",
        "spotify",
        "video-streaming",
        "windows",
        "youtube",
        "youtube-music"
      ],
      "created_at": "2023-04-15T04:53:33Z",
      "updated_at": "2026-01-15T00:29:08Z",
      "pushed_at": "2026-01-14T14:14:04Z",
      "open_issues": 337,
      "owner": {
        "login": "maxrave-dev",
        "avatar_url": "https://avatars.githubusercontent.com/u/113747128?v=4"
      },
      "readme": "<div align=\"center\"> <img src=\"https://raw.githubusercontent.com/maxrave-dev/SimpMusic/main/fastlane/metadata/android/en-US/images/featureGraphic.png\"> <h1>SimpMusic</h1>  \nA FOSS YouTube Music client for Android and Desktop with many features from<br>Spotify, SponsorBlock, ReturnYouTubeDislike using Compose Multiplatform to develop.\n<br> \n<br>\n<a href=\"https://github.com/maxrave-dev/SimpMusic/releases\"><img src=\"https://img.shields.io/github/v/release/maxrave-dev/SimpMusic\"></a> <a href=\"https://github.com/maxrave-dev/SimpMusic/releases\"><img src=\"https://img.shields.io/github/downloads/maxrave-dev/SimpMusic/total\"></a> <br> <br> <a href=\"https://trendshift.io/repositories/13482\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/13482\" alt=\"maxrave-dev%2FSimpMusic | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n<br>\n<br>\n<a href=\"https://www.producthunt.com/products/simpmusic/reviews?utm_source=badge-product_rating&utm_medium=badge&utm_source=badge-simpmusic\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/product_rating.svg?product_id=903836&theme=dark\" alt=\"SimpMusic - A&#0032;FOSS&#0032;YouTube&#0032;Music&#0032;client&#0032;for&#0032;Android&#0032;with&#0032;many&#0032;features | Product Hunt\" style=\"width: 242px; height: 108px;\" width=\"242\" height=\"108\" /></a>\n<br> \n<h4>Download</h4>  \n<a href=\"https://apt.izzysoft.de/packages/com.maxrave.simpmusic/\"><img src=\"https://gitlab.com/IzzyOnDroid/repo/-/raw/master/assets/IzzyOnDroid.png\" width=\"200\"></a> \n<a href=\"https://f-droid.org/en/packages/com.maxrave.simpmusic/\"><img src=\"https://fdroid.gitlab.io/artwork/badge/get-it-on.png\" width=\"200\"></a> \n<a href=\"https://www.openapk.net/simpmusic/com.maxrave.simpmusic/\"><img src=\"https://www.openapk.net/images/openapk-badge.png\" width=\"200\"></a> \n<a href=\"https://github.com/maxrave-dev/SimpMusic/releases\"><img src=\"https://raw.githubusercontent.com/NeoApplications/Neo-Backup/034b226cea5c1b30eb4f6a6f313e4dadcbb0ece4/badge_github.png\" width=\"200\"></a> \n<h4>Nightly Build</h4>  \n<a href=\"https://simpmusic.org/nightly-download\"><img src=\"https://github.com/maxrave-dev/SimpMusic/actions/workflows/android.yml/badge.svg\"></a><br/> <a href=\"https://simpmusic.org/nightly-download\"><img src=\"https://raw.githubusercontent.com/NeoApplications/Neo-Backup/034b226cea5c1b30eb4f6a6f313e4dadcbb0ece4/badge_github.png\" width=\"200\"></a> \n</div>  \n\n> SimpMusic is available on Desktop now!\n  \n## Features âœ¨ï¸    \n- Play music from YouTube Music or YouTube for free, without ads and in the background    \n- Browsing Home, Charts, Podcast, Moods & Genre with YouTube Music data at high speed    \n- Search everything on YouTube    \n- Analyze your playing data, create custom playlists, and sync with YouTube Music...    \n- Spotify Canvas supported    \n- Play 1080p video option with subtitle    \n- AI song suggestions    \n- Customize your playlist, synced with YouTube Music\n- Notifications from followed artists    \n- Caching and offline playback support    \n- Synced lyrics from SimpMusic Lyrics, LRCLIB, Spotify (require login) and YouTube Transcript - AI lyrics translation (BETA) (\\*)  \n- Personalize data (\\**) and multi-YouTube-account support    \n- Supports SponsorBlock and Return YouTube Dislike\n- Sleep Timer    \n- Android Auto with online content\n- Discord Rich Presence support\n- And many more!    \n  \n> (\\*) Use your OpenAI or Gemini API key    \n> (\\**) For users who chose \"Send back to Google\" feature    \n    \n> **Warning**    \n > This app is in the beta stage, so it may have many bugs and make it crash. If you find any bugs,      \n> please create an issue or contact me via email or Discord server.   \n> Because of depending on YouTube Music, the player error will happen and it's normally, please don't ask me about the stable state of this app.\n    \n## Screenshots    \n <p align=\"center\">          \n <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/01.png?raw=true\" width=\"200\" />          \n  <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/02.png?raw=true\" width=\"200\" />          \n   <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/03.png?raw=true\" width=\"200\" />          \n   <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/04.png?raw=true\" width=\"200\" /> </p> <p align=\"center\">          \n <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/05.png?raw=true\" width=\"200\" />          \n <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/17.png?raw=true\" width=\"200\" />  \n   <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/07.png?raw=true\" width=\"200\" />          \n   <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/08.png?raw=true\" width=\"200\" /> </p> <p align=\"center\">          \n <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/09.png?raw=true\" width=\"200\" />          \n  <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/10.png?raw=true\" width=\"200\" />         \n  <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/11.png?raw=true\" width=\"200\" /> \n     <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/12.png?raw=true\" width=\"200\" /> </p> <p align=\"center\">    \n <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/13.png?raw=true\" width=\"200\" />          \n  <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/14.png?raw=true\" width=\"200\" />         \n  <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/15.png?raw=true\" width=\"200\" /> \n     <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/16.png?raw=true\" width=\"200\" /> </p> <p align=\"center\">  \n   <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/screenshot/06.png?raw=true\" width=\"800\" />  \n</p>\n\n #### More [screenshots](https://photos.app.goo.gl/AbieoXG5ctDrpwzp7) here.\n \n ## Data    \n- This app uses hidden API from YouTube Music with some tricks to get data from YouTube Music.    \n- Use Spotify Web API and some tricks to get Spotify Canvas and Lyrics    \n- Thanks to [InnerTune](https://github.com/z-huang/InnerTune/) for the idea to get data from YouTube Music. This repo is my inspiration to create this app.    \n- Special thanks to [SmartTube](https://github.com/yuliskov/SmartTube). This repo help me to extract the streaming URL of YouTube Music.    \n- My app is using [SponsorBlock](https://sponsor.ajay.app/) to skip sponsor in YouTube videos.    \n- ReturnYouTubeDislike for getting information on votes \n- Main lyrics data from SimpMusic Lyrics\n- Alternative lyrics data from LRCLIB. More information [LRCLIB](https://lrclib.net/)    \n \n ## Privacy    \n SimpMusic doesn't have any tracker or third-party server for collecting user data in FOSS version. If YouTube      \nlogged-in users enable \"Send back to Google\" feature, SimpMusic only uses YouTube Music Tracking API to send listening history and listening record of video to Google for better recommendations and      \nsupporting artist or YouTube Creator (For API reference,      \nsee [this](https://github.com/maxrave-dev/SimpMusic/blob/13f7ab6e5fa521b62a9fd31a1cefdc2787a1a8af/kotlinYtmusicScraper/src/main/java/com/maxrave/kotlinytmusicscraper/Ytmusic.kt#L639C4-L666C1)).\n\nWe collect crash data in the Full version to improve the app.\n   \n## Full or FOSS version\nI use [Sentry](http://sentry.io) crashlytics to catch all crashes in the Full version. [Sentry](https://github.com/getsentry/sentry) is the open-source project.\n If you don't want to be collected crash data, you must use FOSS version.\n \n## Desktop app\n\n### Before downloading the Desktop app, make sure your system installed 3 applications below:\n- [Gstreamer](https://gstreamer.freedesktop.org/download/): Required for playback audio.\n- [Yt-dlp](https://github.com/yt-dlp/yt-dlp): Required for getting streaming URL from YouTube (when using 256kps or higher quality).\n\n### Which file should I download?\n- For Windows: Download the file with extension `.msi`.\n- For macOS: Download the file with extension `.dmg`.\n- For Linux: Download the file with extension `.deb`.\n\n### Log in guide: https://www.simpmusic.org/blogs/en/how-to-log-in-on-desktop-app\n\n### Some limitations on Desktop app:\n- No offline playback support.\n- No video playback support.\n- Very buggy on some Linux distributions (because of Jetbrains not fix).\n\nPlease report issues on our Discord server if you find any bugs.\n \n## Translation    \n[![Crowdin](https://badges.crowdin.net/simpmusic/localized.svg)](https://crowdin.com/project/simpmusic)\n<br/>\nYou can help me translate this app into your language by using Crowdin [SimpMusic on Crowdin](https://crowdin.com/project/simpmusic)    \n #### Special thanks to all translators on Crowdin â¤ï¸    \n ## FAQ    \n #### 1. Wrong Lyrics?    \n Lyrics are provided by LRCLIB and other sources. Sometimes lyrics may not match perfectly with YouTube\"      \nvideoId\" parameter. So I need to use some \"String Matcher\" and \"Duration\" for search lyrics. So      \nsometimes, some songs or videos get the wrong lyrics    \n    \n#### 2. Why the name or brand is \"SimpMusic\"?    \n Simply, because I love the name. It's a combination of 'Simple' and 'Music'. But SimpMusic is not a simple app, it's all you need for a powerful music streaming app.    \n  \n#### More FAQ, join [my Discord channel](https://discord.com/channels/1136988323819298856/1349800418745778196)  \n  ## Developer/Team    \n- [maxrave-dev](https://github.com/maxrave-dev/SimpMusic): Founder/Developer/Designer    \n- [Owen Connor](https://github.com/owencz1998): Discord Server Admin.    \n- [ilianoKokoro](https://github.com/ilianoKokoro): Discord Server Admin.\n- [CrazyWolf13](https://github.com/CrazyWolf13): Issues organizer/planner.\n\nWe're looking for more contributors, all contributions are welcome!\nSee our [CODE OF CONDUCT](https://github.com/maxrave-dev/SimpMusic/blob/main/CODE_OF_CONDUCT.md)\n\n ## Showcase\nThis project is following clean architecture and MVVM pattern (in UI, app module).\n\n ### Dependencies graph\n  <p float=\"left\">        \n  <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/main/asset/dependencies_graph.svg?raw=true\" width=\"800\"> \n  </p>\n\n ## Support & Donations \n #### Special thanks to all supporter â¤ï¸    \n <div align=\"left\"> \n <a href=\"https://simpmusic.org/\"><img alt=\"Visit the website\" height=\"50\" src=\"https://cdn.jsdelivr.net/npm/@intergrav/devins-badges@3/assets/cozy/documentation/website_vector.svg\"></a> &nbsp;        \n<a href=\"https://discord.gg/Rq5tWVM9Hg\"><img alt=\"Discord Server\" height=\"50\" src=\"https://cdn.jsdelivr.net/npm/@intergrav/devins-badges@3/assets/cozy/social/discord-plural_vector.svg\"></a> &nbsp;        \n<br> <a href=\"https://www.buymeacoffee.com/maxrave\"><img alt=\"Buy me a Coffee\" height=\"50\" src=\"https://cdn.jsdelivr.net/npm/@intergrav/devins-badges@3/assets/cozy/donate/buymeacoffee-singular_vector.svg\"></a> &nbsp;        \n<a href=\"https://liberapay.com/maxrave/\"><img alt=\"liberapay\" height=\"50\"        \nsrc=\"https://raw.githubusercontent.com/liberapay/liberapay.com/master/www/assets/liberapay/logo-v2_black-on-yellow.svg\"></a> \n</div>\n    \n ### MOMO or Vietnamese banking    \n <p float=\"left\">        \n <img src=\"https://github.com/maxrave-dev/SimpMusic/blob/dev/asset/52770992.jpg?raw=true\" width=\"300\"> \n </p>\n\n## SimpMusic is sponsored by:\n<br />\n<a href=\"https://vercel.com/oss\">\n  <img alt=\"Vercel OSS Program\" src=\"https://vercel.com/oss/program-badge.svg\" />\n</a>\n<br />\n<br />\n<a href=\"https://www.digitalocean.com/?refcode=d7f6eedfb9a9&utm_campaign=Referral_Invite&utm_medium=Referral_Program&utm_source=badge\"><img src=\"https://web-platforms.sfo2.cdn.digitaloceanspaces.com/WWW/Badge%201.svg\" width=\"300\" alt=\"DigitalOcean Referral Badge\" /></a>\n<br>\n<br>\n<a href=\"https://crowdin.com\">\n<img src=\"https://support.crowdin.com/assets/logos/plate/png/crowdin-logo-with-plate.png\" width=\"300\"/>\n</a>\n<br>\n<a href=\"https://sentry.io\">\n<img src=\"https://github.com/maxrave-dev/SimpMusic/blob/dev/asset/sentry.svg?raw=true\" width=\"300\"/>\n</a>\n<br>\n<br>\n\nGet a free $200 credit over 60 days on DigitalOcean: [GET NOW](https://www.digitalocean.com/?refcode=d7f6eedfb9a9&utm_campaign=Referral_Invite&utm_medium=Referral_Program&utm_source=badge)\n\nCrowdin and Sentry both have a free enterprise plan for Open-source projects. Follow the URLs: \n- [Open Source License Request Form | Crowdin](https://crowdin.com/page/open-source-project-setup-request)\n- [Sentry for Open Source | Sentry](https://sentry.io/for/open-source/)\n\nCheck out the Vercel open-source program:\n- https://vercel.com/open-source-program\n\n*This project is a part of SimpMusic.org Open-source project by me [maxrave-dev](https://github.com/maxrave-dev)*\n",
      "stars_today": 21
    },
    {
      "id": 10744183,
      "name": "netdata",
      "full_name": "netdata/netdata",
      "description": "The fastest path to AI-powered full stack observability, even for lean teams.",
      "html_url": "https://github.com/netdata/netdata",
      "stars": 77343,
      "forks": 6300,
      "language": "C",
      "topics": [
        "ai",
        "alerting",
        "cncf",
        "data-visualization",
        "database",
        "devops",
        "docker",
        "grafana",
        "influxdb",
        "kubernetes",
        "linux",
        "machine-learning",
        "mcp",
        "mongodb",
        "monitoring",
        "mysql",
        "netdata",
        "observability",
        "postgresql",
        "prometheus"
      ],
      "created_at": "2013-06-17T18:39:10Z",
      "updated_at": "2026-01-15T00:16:27Z",
      "pushed_at": "2026-01-15T00:16:21Z",
      "open_issues": 240,
      "owner": {
        "login": "netdata",
        "avatar_url": "https://avatars.githubusercontent.com/u/43390781?v=4"
      },
      "readme": "<p align=\"center\">\n<a href=\"https://www.netdata.cloud#gh-light-mode-only\">\n  <img src=\"https://www.netdata.cloud/img/readme-images/netdata_readme_logo_light.png\" alt=\"Netdata\" width=\"300\"/>\n</a>\n<a href=\"https://www.netdata.cloud#gh-dark-mode-only\">\n  <img src=\"https://www.netdata.cloud/img/readme-images/netdata_readme_logo_dark.png\" alt=\"Netdata\" width=\"300\"/>\n</a>\n</p>\n<h3 align=\"center\">X-Ray Vision for your infrastructure!</h3>\n<h4 align=\"center\">Every Metric, Every Second. No BS.</h4>\n\n<br />\n<p align=\"center\">\n  <a href=\"https://github.com/netdata/netdata/\"><img src=\"https://img.shields.io/github/stars/netdata/netdata?style=social\" alt=\"GitHub Stars\"></a>\n  <br />\n  <a href=\"https://app.netdata.cloud/spaces/netdata-demo?utm_campaign=github_readme_demo_badge\"><img src=\"https://img.shields.io/badge/Live%20Demo-green\" alt=\"Live Demo\"></a>\n  <a href=\"https://github.com/netdata/netdata/releases/latest\"><img src=\"https://img.shields.io/github/release/netdata/netdata.svg\" alt=\"Latest release\"></a>\n  <a href=\"https://github.com/netdata/netdata-nightlies/releases/latest\"><img src=\"https://img.shields.io/github/release/netdata/netdata-nightlies.svg\" alt=\"Latest nightly build\"></a>\n  <br/>\n  <a href=\"https://community.netdata.cloud\"><img alt=\"Discourse topics\" src=\"https://img.shields.io/discourse/topics?server=https%3A%2F%2Fcommunity.netdata.cloud%2F&logo=discourse&label=discourse%20forum\"></a>\n  <a href=\"https://github.com/netdata/netdata/discussions\"><img alt=\"GitHub Discussions\" src=\"https://img.shields.io/github/discussions/netdata/netdata?logo=github&label=github%20discussions\"></a>\n  <br/>\n  <a href=\"https://bestpractices.coreinfrastructure.org/projects/2231\"><img src=\"https://bestpractices.coreinfrastructure.org/projects/2231/badge\" alt=\"CII Best Practices\"></a>\n  <a href=\"https://scan.coverity.com/projects/netdata-netdata?tab=overview\"><img alt=\"Coverity Scan\" src=\"https://img.shields.io/coverity/scan/netdata\"></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://registry.my-netdata.io/#menu_netdata_submenu_registry\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&dimensions=persons&label=user%20base&units=M&value_color=blue&precision=2&divide=1000000&options=unaligned&tier=1&v44\" alt=\"User base\"></a>\n  <a href=\"https://registry.my-netdata.io/#menu_netdata_submenu_registry\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&dimensions=machines&label=servers%20monitored&units=M&divide=1000000&value_color=orange&precision=2&options=unaligned&tier=1&v44\" alt=\"Servers monitored\"></a>\n  <a href=\"https://registry.my-netdata.io/#menu_netdata_submenu_registry\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_sessions&label=sessions%20served&units=M&value_color=yellowgreen&precision=2&divide=1000000&options=unaligned&tier=1&v44\" alt=\"Sessions served\"></a>\n  <a href=\"https://hub.docker.com/r/netdata/netdata\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=dockerhub.pulls_sum&divide=1000000&precision=1&units=M&label=docker+hub+pulls&options=unaligned&tier=1&v44\" alt=\"Docker Hub pulls\"></a>\n</p>\n<p align=\"center\"><b>Visit our <a href=\"https://www.netdata.cloud\">Home Page</a></b></p>\n\n<hr class=\"solid\">\n\nMENU: **[WHO WE ARE](#who-we-are)** | **[KEY FEATURES](#key-features)** | **[GETTING STARTED](#getting-started)** | **[HOW IT WORKS](#how-it-works)** | **[FAQ](#faq)** | **[DOCS](#book-documentation)** | **[COMMUNITY](#tada-community)** | **[CONTRIBUTE](#pray-contribute)** | **[LICENSE](#scroll-license)**\n\n\n> [!WARNING]\n> People **get addicted to Netdata.**\n> Once you use it on your systems, *there's no going back.*\n\n[![Platforms](https://img.shields.io/badge/Platforms-Linux%20%7C%20macOS%20%7C%20FreeBSD%20%7C%20Windows-blue)]()\n\n---\n\n## WHO WE ARE\n\nNetdata is an open-source, real-time infrastructure monitoring platform. Monitor, detect, and act across your entire infrastructure.\n\n**Core Advantages:**\n\n* **Instant Insights** â€“ With Netdata you can access per-second metrics and visualizations.\n* **Zero Configuration** â€“ You can deploy immediately without complex setup.\n* **ML-Powered** â€“ You can detect anomalies, predict issues, and automate analysis.\n* **Efficient** â€“ You can monitor with minimal resource usage and maximum scalability.\n* **Secure & Distributed** â€“ You can keep your data local with no central collection needed.\n\nWith Netdata, you get real-time, per-second updates. Clear **insights at a glance**, no complexity.\n\n<details>\n  <summary><strong>All heroes have a great origin story. Click to discover ours.</strong></summary>\n  <br/>\n\nIn 2013, at the company where Costa Tsaousis was COO, a significant percentage of their cloud-based transactions failed silently, severely impacting business performance.\n\nCosta and his team tried every troubleshooting tool available at the time. None could identify the root cause. As Costa later wrote:\n\nâ€œ*I couldnâ€™t believe that monitoring systems provide so few metrics and with such low resolution, scale so badly, and cost so much to run.*â€\n\nFrustrated, he decided to build his own monitoring tool, starting from scratch.\n\nThat decision led to countless late nights and weekends. It also sparked a fundamental shift in how infrastructure monitoring and troubleshooting are approached, both in method and in cost.\n</details>\n\n### Most Energy-Efficient Monitoring Tool\n\n<p align=\"center\">\n<a href=\"https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf#gh-dark-mode-only\">\n  <img src=\"https://github.com/netdata/netdata/assets/139226121/7118757a-38fb-48d7-b12a-53e709a8e8c0\" alt=\"Energy Efficiency\" width=\"800\"/>\n</a>\n<a href=\"https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf#gh-light-mode-only\">\n  <img src=\"https://github.com/netdata/netdata/assets/139226121/4f64cbb6-05e4-48e3-b7c0-d1b79e37e219\" alt=\"Energy efficiency\" width=\"800\"/>\n</a>\n</p>\n\nAccording to the [University of Amsterdam study](https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf), Netdata is the most energy-efficient tool for monitoring Docker-based systems. The study also shows Netdata excels in CPU usage, RAM usage, and execution time compared to other monitoring solutions.\n\n---\n\n## Key Features\n\n| Feature                    | Description                               | What Makes It Unique                                     |\n|----------------------------|-------------------------------------------|----------------------------------------------------------|\n| **Real-Time**              | Per-second data collection and processing | Works in a beat â€“ click and see results instantly        |\n| **Zero-Configuration**     | Automatic detection and discovery         | Auto-discovers everything on the nodes it runs           |\n| **ML-Powered**             | Unsupervised anomaly detection            | Trains multiple ML models per metric at the edge         |\n| **Long-Term Retention**    | High-performance storage                  | ~0.5 bytes per sample with tiered storage for archiving  |\n| **Advanced Visualization** | Rich, interactive dashboards              | Slice and dice data without query language               |\n| **Extreme Scalability**    | Native horizontal scaling                 | Parent-Child centralization with multi-million samples/s |\n| **Complete Visibility**    | From infrastructure to applications       | Simplifies operations and eliminates silos               |\n| **Edge-Based**             | Processing at your premises               | Distributes code instead of centralizing data            |\n\n> [!NOTE]  \n> Want to put Netdata to the test against Prometheus?\n> Explore the [full comparison](https://www.netdata.cloud/blog/netdata-vs-prometheus-2025/).\n\n---\n\n## Netdata Ecosystem\n\nThis three-part architecture enables you to scale from single nodes to complex multi-cloud environments:\n\n| Component         | Description                                                                                                                                                 | License                                         |\n|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------|\n| **Netdata Agent** | â€¢ Core monitoring engine<br>â€¢ Handles collection, storage, ML, alerts, exports<br>â€¢ Runs on servers, cloud, K8s, IoT<br>â€¢ Zero production impact            | [GPL v3+](https://www.gnu.org/licenses/gpl-3.0) |\n| **Netdata Cloud** | â€¢ Enterprise features<br>â€¢ User management, RBAC, horizontal scaling<br>â€¢ Centralized alerts<br>â€¢ Free community tier<br>â€¢ No metric storage centralization |                                                 |\n| **Netdata UI**    | â€¢ Dashboards and visualizations<br>â€¢ Free to use<br>â€¢ Included in standard packages<br>â€¢ Latest version via CDN                                             | [NCUL1](https://app.netdata.cloud/LICENSE.txt)  |\n\n## What You Can Monitor\n\nWith Netdata you can monitor all these components across platforms:\n\n|                                                                                                   Component |              Linux               | FreeBSD | macOS |                      Windows                      |\n|------------------------------------------------------------------------------------------------------------:|:--------------------------------:|:-------:|:-----:|:-------------------------------------------------:|\n|                             **System Resources**<small><br/>CPU, Memory and system shared resources</small> |               Full               |   Yes   |  Yes  |                        Yes                        |\n|                                **Storage**<small><br/>Disks, Mount points, Filesystems, RAID arrays</small> |               Full               |   Yes   |  Yes  |                        Yes                        |\n|                                 **Network**<small><br/>Network Interfaces, Protocols, Firewall, etc</small> |               Full               |   Yes   |  Yes  |                        Yes                        |\n|                        **Hardware & Sensors**<small><br/>Fans, Temperatures, Controllers, GPUs, etc</small> |               Full               |  Some   | Some  |                       Some                        |\n|                                       **O/S Services**<small><br/>Resources, Performance and Status</small> | Yes<small><br/>`systemd`</small> |    -    |   -   |                         -                         |\n|                                      **Processes**<small><br/>Resources, Performance, OOM, and more</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n|                                                                             System and Application **Logs** | Yes<small><br/>`systemd`-journal |    -    |   -   | Yes<small><br/>`Windows Event Log`, `ETW`</small> |\n|                                 **Network Connections**<small><br/>Live TCP and UDP sockets per PID</small> |               Yes                |    -    |   -   |                         -                         |\n|                               **Containers**<small><br/>Docker/containerd, LXC/LXD, Kubernetes, etc</small> |               Yes                |    -    |   -   |                         -                         |\n|                                 **VMs** (from the host)<small><br/>KVM, qemu, libvirt, Proxmox, etc</small> | Yes<small><br/>`cgroups`</small> |    -    |   -   |         Yes<small><br/>`Hyper-V`</small>          |\n|                       **Synthetic Checks**<small><br/>Test APIs, TCP ports, Ping, Certificates, etc</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n| **Packaged Applications**<small><br/>nginx, apache, postgres, redis, mongodb,<br/>and hundreds more</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n|                              **Cloud Provider Infrastructure**<small><br/>AWS, GCP, Azure, and more</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n|                       **Custom Applications**<small><br/>OpenMetrics, StatsD and soon OpenTelemetry</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n\nOn Linux, you can continuously monitor all kernel features and hardware sensors for errors, including Intel/AMD/Nvidia GPUs, PCI AER, RAM EDAC, IPMI, S.M.A.R.T, Intel RAPL, NVMe, fans, power supplies, and voltage readings.\n\n---\n\n## Getting Started\n\nYou can install Netdata on all major operating systems. To begin:\n\n### 1. Install Netdata\n\nChoose your platform and follow the installation guide:\n\n* [Linux Installation](https://learn.netdata.cloud/docs/installing/one-line-installer-for-all-linux-systems)\n* [macOS](https://learn.netdata.cloud/docs/installing/macos)\n* [FreeBSD](https://learn.netdata.cloud/docs/installing/freebsd)\n* [Windows](https://learn.netdata.cloud/docs/netdata-agent/installation/windows)\n* [Docker Guide](/packaging/docker/README.md)\n* [Kubernetes Setup](https://learn.netdata.cloud/docs/installation/install-on-specific-environments/kubernetes)\n\n> [!NOTE]\n> You can access the Netdata UI at `http://localhost:19999` (or `http://NODE:19999` if remote).\n\n### 2. Configure Collectors\n\nNetdata auto-discovers most metrics, but you can manually configure some collectors:\n\n* [All collectors](https://learn.netdata.cloud/docs/data-collection/)\n* [SNMP monitoring](https://learn.netdata.cloud/docs/data-collection/monitor-anything/networking/snmp)\n\n### 3. Configure Alerts\n\nYou can use hundreds of built-in alerts and integrate with:\n\n`email`, `Slack`, `Telegram`, `PagerDuty`, `Discord`, `Microsoft Teams`, and more.\n\n> [!NOTE]  \n> Email alerts work by default if there's a configured MTA.\n\n### 4. Configure Parents\n\nYou can centralize dashboards, alerts, and storage with Netdata Parents:\n\n* [Streaming Reference](https://learn.netdata.cloud/docs/streaming/streaming-configuration-reference)\n\n> [!NOTE]  \n> You can use Netdata Parents for central dashboards, longer retention, and alert configuration.\n\n### 5. Connect to Netdata Cloud\n\n[Sign in to Netdata Cloud](https://app.netdata.cloud/sign-in) and connect your nodes for:\n\n* Access from anywhere\n* Horizontal scalability and multi-node dashboards\n* UI configuration for alerts and data collection\n* Role-based access control\n* Free tier available\n\n> [!NOTE]  \n> Netdata Cloud is optional. Your data stays in your infrastructure.\n\n## Live Demo Sites\n\n<p align=\"center\">\n  <b>See Netdata in action</b><br/>\n  <a href=\"https://frankfurt.netdata.rocks\"><b>FRANKFURT</b></a> |\n  <a href=\"https://newyork.netdata.rocks\"><b>NEWYORK</b></a> |\n  <a href=\"https://atlanta.netdata.rocks\"><b>ATLANTA</b></a> |\n  <a href=\"https://sanfrancisco.netdata.rocks\"><b>SANFRANCISCO</b></a> |\n  <a href=\"https://toronto.netdata.rocks\"><b>TORONTO</b></a> |\n  <a href=\"https://singapore.netdata.rocks\"><b>SINGAPORE</b></a> |\n  <a href=\"https://bangalore.netdata.rocks\"><b>BANGALORE</b></a>\n  <br/>\n  <i>These demo clusters run with default configuration and show real monitoring data.</i>\n  <br/>\n  <i>Choose the instance closest to you for the best performance.</i>\n</p>\n\n---\n\n## How It Works\n\nWith Netdata you can run a modular pipeline for metrics collection, processing, and visualization.\n\n```mermaid\nflowchart TB\n  A[Netdata Agent]:::mainNode\n  A1(Collect):::green --> A\n  A2(Store):::green --> A\n  A3(Learn):::green --> A\n  A4(Detect):::green --> A\n  A5(Check):::green --> A\n  A6(Stream):::green --> A\n  A7(Archive):::green --> A\n  A8(Query):::green --> A\n  A9(Score):::green --> A\n\n  classDef green fill:#bbf3bb,stroke:#333,stroke-width:1px,color:#000\n  classDef mainNode fill:#f0f0f0,stroke:#333,stroke-width:1px,color:#333\n```\n\nWith each Agent you can:\n\n1. **Collect** â€“ Gather metrics from systems, containers, apps, logs, APIs, and synthetic checks.\n2. **Store** â€“ Save metrics to a high-efficiency, tiered time-series database.\n3. **Learn** â€“ Train ML models per metric using recent behavior.\n4. **Detect** â€“ Identify anomalies using trained ML models.\n5. **Check** â€“ Evaluate metrics against pre-set or custom alert rules.\n6. **Stream** â€“ Send metrics to Netdata Parents in real time.\n7. **Archive** â€“ Export metrics to Prometheus, InfluxDB, OpenTSDB, Graphite, and others.\n8. **Query** â€“ Access metrics via an API for dashboards or third-party tools.\n9. **Score** â€“ Use a scoring engine to find patterns and correlations across metrics.\n\n> [!NOTE]  \n> Learn more: [Netdata's architecture](https://learn.netdata.cloud/docs/netdata-agent/#distributed-observability-pipeline)\n\n## Agent Capabilities\n\nWith the Netdata Agent, you can use these core capabilities out-of-the-box:\n\n| Capability                   | Description                                                                                                                                   |\n|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|\n| **Comprehensive Collection** | â€¢ 800+ integrations<br>â€¢ Systems, containers, VMs, hardware sensors<br>â€¢ OpenMetrics, StatsD, and logs<br>â€¢ OpenTelemetry support coming soon |\n| **Performance & Precision**  | â€¢ Per-second collection<br>â€¢ Real-time visualization with 1-second latency<br>â€¢ High-resolution metrics                                       |\n| **Edge-Based ML**            | â€¢ ML models trained at the edge<br>â€¢ Automatic anomaly detection per metric<br>â€¢ Pattern recognition based on historical behavior             |\n| **Advanced Log Management**  | â€¢ Direct systemd-journald and Windows Event Log integration<br>â€¢ Process logs at the edge<br>â€¢ Rich log visualization                         |\n| **Observability Pipeline**   | â€¢ Parent-Child relationships<br>â€¢ Flexible centralization<br>â€¢ Multi-level replication and retention                                          |\n| **Automated Visualization**  | â€¢ NIDL data model<br>â€¢ Auto-generated dashboards<br>â€¢ No query language needed                                                                |\n| **Smart Alerting**           | â€¢ Pre-configured alerts<br>â€¢ Multiple notification methods<br>â€¢ Proactive detection                                                           |\n| **Low Maintenance**          | â€¢ Auto-detection<br>â€¢ Zero-touch ML<br>â€¢ Easy scalability<br>â€¢ CI/CD friendly                                                                 |\n| **Open & Extensible**        | â€¢ Modular architecture<br>â€¢ Easy to customize<br>â€¢ Integrates with existing tools                                                             |\n\n---\n\n## CNCF Membership\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/white/cncf-white.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/color/cncf-color.svg\">\n    <img alt=\"CNCF Logo\" src=\"https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/color/cncf-color.svg\" width=\"300\">\n  </picture>\n  <br />\n  Netdata actively supports and is a member of the Cloud Native Computing Foundation (CNCF).<br />\n  It is one of the most starred projects in the <a href=\"https://landscape.cncf.io/?item=observability-and-analysis--observability--netdata\">CNCF landscape</a>.\n</p>\n\n---\n\n## FAQ\n\n<details>\n<summary><strong>Is Netdata secure?</strong></summary>\n<br/>\n\nYes. Netdata follows [OpenSSF best practices](https://bestpractices.coreinfrastructure.org/en/projects/2231), has a security-first design, and is regularly audited by the community.\n\n* [Security design](https://learn.netdata.cloud/docs/security-and-privacy-design)\n* [Security policies and advisories](https://github.com/netdata/netdata/security)\n\n</details>\n\n<details>\n<summary><strong>Does Netdata use a lot of resources?</strong></summary>\n<br/>\n\nNo. Even with ML and per-second metrics, Netdata uses minimal resources.\n\n* \\~5% CPU and 150MiB RAM by default on production systems\n* <1% CPU and \\~100MiB RAM when ML and alerts are disabled and using ephemeral storage\n* Parents scale to millions of metrics per second with appropriate hardware\n\n> You can use the **Netdata Monitoring** section in the dashboard to inspect its resource usage.\n\n</details>\n\n<details>\n<summary><strong>How much data retention is possible?</strong></summary>\n<br/>\n\nAs much as your disk allows.\n\nWith Netdata you can use tiered retention:\n\n* Tier 0: per-second resolution\n* Tier 1: per-minute resolution\n* Tier 2: per-hour resolution\n\nThese are queried automatically based on the zoom level.\n</details>\n\n<details>\n<summary><strong>Can Netdata scale to many servers?</strong></summary>\n<br/>\n\nYes. With Netdata you can:\n\n* Scale horizontally with many Agents\n* Scale vertically with powerful Parents\n* Scale infinitely via Netdata Cloud\n\n> You can use Netdata Cloud to merge many independent infrastructures into one logical view.\n\n</details>\n\n<details>\n<summary><strong>Is disk I/O a concern?</strong></summary>\n<br/>\n\nNo. Netdata minimizes disk usage:\n\n* Metrics are flushed to disk every 17 minutes, spread out evenly\n* Uses direct I/O and compression (ZSTD)\n* Can run entirely in RAM or stream to a Parent\n\n> You can use `alloc` or `ram` mode for no disk writes.\n\n</details>\n\n<details>\n<summary><strong>How is Netdata different from Prometheus + Grafana?</strong></summary>\n<br/>\n\nWith Netdata you get a complete monitoring solutionâ€”not just tools.\n\n* No manual setup or dashboards needed\n* Built-in ML, alerts, dashboards, and correlations\n* More efficient and easier to deploy\n\n> [Performance comparison](https://blog.netdata.cloud/netdata-vs-prometheus-performance-analysis/)\n\n</details>\n\n<details>\n<summary><strong>How is Netdata different from commercial SaaS tools?</strong></summary>\n<br/>\n\nWith Netdata you can store all metrics on your infrastructureâ€”no sampling, no aggregation, no loss.\n\n* High-resolution metrics by default\n* ML per metric, not shared models\n* Unlimited scalability without skyrocketing cost\n\n</details>\n\n<details>\n<summary><strong>Can Netdata run alongside Nagios, Zabbix, etc.?</strong></summary>\n<br/>\n\nYes. You can use Netdata together with traditional tools.\n\nWith Netdata you get:\n\n* Real-time, high-resolution monitoring\n* Zero configuration and auto-generated dashboards\n* Anomaly detection and advanced visualization\n\n</details>\n\n<details>\n<summary><strong>What if I feel overwhelmed?</strong></summary>\n<br/>\n\nYou can start small:\n\n* Use the dashboard's table of contents and search\n* Explore anomaly scoring (\"AR\" toggle)\n* Create custom dashboards in Netdata Cloud\n\n> [Docs and guides](https://learn.netdata.cloud/guides)\n\n</details>\n\n<details>\n<summary><strong>Do I have to use Netdata Cloud?</strong></summary>\n<br/>\n\nNo. Netdata Cloud is optional.\n\nNetdata works without it, but with Cloud you can:\n\n* Access remotely with SSO\n* Save dashboard customizations\n* Configure alerts centrally\n* Collaborate with role-based access\n\n</details>\n\n<details>\n<summary><strong>What telemetry does Netdata collect?</strong></summary>\n<br/>\n\nAnonymous telemetry helps improve the product. You can disable it:\n\n* Add `--disable-telemetry` to the installer, or\n* Create `/etc/netdata/.opt-out-from-anonymous-statistics` and restart Netdata\n\n> Telemetry helps us understand usage, not track users. No private data is collected.\n\n</details>\n\n<details>\n<summary><strong>Who uses Netdata?</strong></summary>\n<br/>\n\nYou'll join users including:\n\n* Major companies (Amazon, ABN AMRO Bank, Facebook, Google, IBM, Intel, Netflix, Samsung)\n* Universities (NYU, Columbia, Seoul National, UCL)\n* Government organizations worldwide\n* Infrastructure-intensive organizations\n* Technology operators\n* Startups and freelancers\n* SysAdmins and DevOps professionals\n\n</details>\n\n---\n\n## \\:book: Documentation\n\nVisit [Netdata Learn](https://learn.netdata.cloud) for full documentation and guides.\n\n> [!NOTE]  \n> Includes deployment, configuration, alerting, exporting, troubleshooting, and more.\n\n---\n\n## \\:tada: Community\n\nJoin the Netdata community:\n\n* [Discord](https://discord.com/invite/2mEmfW735j)\n* [Forum](https://community.netdata.cloud)\n* [GitHub Discussions](https://github.com/netdata/netdata/discussions)\n\n> [!NOTE]  \n> [Code of Conduct](https://github.com/netdata/.github/blob/main/CODE_OF_CONDUCT.md)\n\nFollow us on:\n[Twitter](https://twitter.com/netdatahq) | [Reddit](https://www.reddit.com/r/netdata/) | [YouTube](https://www.youtube.com/c/Netdata) | [LinkedIn](https://www.linkedin.com/company/netdata-cloud/)\n\n---\n\n## \\:pray: Contribute\n\nWe welcome your contributions.\n\nWays you help us stay sharp:\n\n* Share best practices and monitoring insights\n* Report issues or missing features\n* Improve documentation\n* Develop new integrations or collectors\n* Help users in forums and chats\n\n> [!NOTE]  \n> [Contribution guide](https://github.com/netdata/.github/blob/main/CONTRIBUTING.md)\n\n---\n\n## \\:scroll: License\n\nThe Netdata ecosystem includes:\n\n* **Netdata Agent** â€“ Open-source core (GPLv3+). **Includes** data collection, storage, ML, alerting, APIs and **redistributes** several other open-source tools and libraries.\n    * [Netdata Agent License](https://github.com/netdata/netdata/blob/master/LICENSE)\n    * [Netdata Agent Redistributed](https://github.com/netdata/netdata/blob/master/REDISTRIBUTED.md)\n* **Netdata UI** â€“ Closed-source but free to use with Netdata Agent and Cloud. Delivered via CDN. It integrates third-party open-source components.\n    * [Netdata Cloud UI License](https://app.netdata.cloud/LICENSE.txt)\n    * [Netdata UI third-party licenses](https://app.netdata.cloud/3D_PARTY_LICENSES.txt)\n* **Netdata Cloud** â€“ Closed-source, with free and paid tiers. Adds remote access, SSO, scalability.\n",
      "stars_today": 20
    },
    {
      "id": 53809090,
      "name": "anime",
      "full_name": "juliangarnier/anime",
      "description": "JavaScript animation engine",
      "html_url": "https://github.com/juliangarnier/anime",
      "stars": 65858,
      "forks": 4411,
      "language": "JavaScript",
      "topics": [
        "animation",
        "anime",
        "canvas",
        "css",
        "javascript",
        "javascript-library",
        "svg"
      ],
      "created_at": "2016-03-13T21:37:45Z",
      "updated_at": "2026-01-15T00:52:11Z",
      "pushed_at": "2026-01-13T17:21:49Z",
      "open_issues": 91,
      "owner": {
        "login": "juliangarnier",
        "avatar_url": "https://avatars.githubusercontent.com/u/1268691?v=4"
      },
      "readme": "# Anime.js\n\n<p align=\"center\">\n  <picture align=\"center\">\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/images/animejs-v4-logo-animation-dark.gif\">\n    <img align=\"center\" alt=\"Anime.js V4 logo animation\" src=\"./assets/images/animejs-v4-logo-animation.gif\" width=\"560\">\n  </picture>\n</p>\n\n<p align=\"center\">\n  <strong>\n  <em>Anime.js</em> is a fast, multipurpose and lightweight JavaScript animation library with a simple, yet powerful API.<br>\n  It works with CSS properties, SVG, DOM attributes and JavaScript Objects.\n  </strong>\n</p>\n\n\n<p align=\"center\">\n  <img alt=\"NPM Downloads\" src=\"https://img.shields.io/npm/dm/animejs?style=flat-square&logo=npm\">\n  <img alt=\"jsDelivr hits (npm)\" src=\"https://img.shields.io/jsdelivr/npm/hm/animejs?style=flat-square&logo=jsdeliver\">\n  <img alt=\"GitHub Sponsors\" src=\"https://img.shields.io/github/sponsors/juliangarnier?style=flat-square&logo=github\">\n</p>\n\n## Sponsors\n\nAnime.js is 100% free and is only made possible with the help of our sponsors.\nHelp the project become sustainable by sponsoring us on <a target=\"_blank\" href=\"https://github.com/sponsors/juliangarnier\">GitHub Sponsors</a>.\n\n### Platinum sponsors\n\n<table>\n  <tbody>\n    <tr>\n      <td>\n        <a target=\"_blank\" href=\"https://ice.io/?ref=animejs\">\n          <picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/sponsors/ice-open-network-logomark.png?v=251108\">\n            <img align=\"center\" src=\"./assets/sponsors/ice-open-network-logomark-dark.png?v=251108\" width=\"310\">\n          </picture>\n        </a>\n      </td>\n      <td>\n        <a target=\"_blank\" href=\"https://go.warp.dev/anime\">\n          <picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/sponsors/warp-logomark.png?v=251108\">\n            <img align=\"center\" src=\"./assets/sponsors/warp-logomark-dark.png?v=251108\" width=\"310\">\n          </picture>\n        </a>\n      </td>\n      <td>\n        <a target=\"_blank\" href=\"https://hyperswitch.io/?utm_source=julian&utm_medium=github&utm_campaign=animejs_sponsorship\">\n          <picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/sponsors/juspay-logomark.png?v=251108\">\n            <img align=\"center\" src=\"./assets/sponsors/juspay-logomark-dark.png?v=251108\" width=\"310\">\n          </picture>\n        </a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n### Silver sponsors\n\n<table>\n  <tbody>\n    <tr>\n      <td>\n        <a target=\"_blank\" href=\"https://www.lambdatest.com?utm_source=animeJS&utm_medium=organic&utm_campaign=july_08&utm_term=sk&utm_content=opensource\">\n          <picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/sponsors/lambdatest-logomark.png?v=251108\">\n            <img align=\"center\" src=\"./assets/sponsors/lambdatest-logomark-dark.png?v=251108\" width=\"141\">\n          </picture>\n        </a>\n      </td>\n      <td>\n        <a target=\"_blank\" href=\"https://inspatialapp.com/?ref=animejs\">\n          <picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/sponsors/inspatial-logomark.png?v=251108\">\n            <img align=\"center\" src=\"./assets/sponsors/inspatial-logomark-dark.png?v=251108\" width=\"141\">\n          </picture>\n        </a>\n      </td>\n      <td>\n        <a target=\"_blank\" href=\"https://github.com/sponsors/juliangarnier\">\n          <picture>\n            <img align=\"center\" src=\"./assets/sponsors/placeholder-small.png?v=251108\" width=\"141\">\n          </picture>\n        </a>\n      </td>\n      <td>\n        <a target=\"_blank\" href=\"https://github.com/sponsors/juliangarnier\">\n          <picture>\n            <img align=\"center\" src=\"./assets/sponsors/placeholder-small.png?v=251108\" width=\"141\">\n          </picture>\n        </a>\n      </td>\n      <td>\n        <a target=\"_blank\" href=\"https://github.com/sponsors/juliangarnier\">\n          <picture>\n            <img align=\"center\" src=\"./assets/sponsors/placeholder-small.png?v=251108\" width=\"141\">\n          </picture>\n        </a>\n      </td>\n      <td>\n        <a target=\"_blank\" href=\"https://github.com/sponsors/juliangarnier\">\n          <picture>\n            <img align=\"center\" src=\"./assets/sponsors/placeholder-small.png?v=251108\" width=\"141\">\n          </picture>\n        </a>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\nGet featured here by becoming a <a target=\"_blank\" href=\"https://github.com/sponsors/juliangarnier\">GitHub Sponsor</a>.\n\n\n## Usage\n\nAnime.js V4 works by importing ES modules like so:\n\n\n<table>\n<tr>\n  <td>\n\n```javascript\nimport {\n  animate,\n  stagger,\n} from 'animejs';\n\nanimate('.square', {\n  x: 320,\n  rotate: { from: -180 },\n  duration: 1250,\n  delay: stagger(65, { from: 'center' }),\n  ease: 'inOutQuint',\n  loop: true,\n  alternate: true\n});\n```\n\n  </td>\n  <td>\n    <img align=\"center\" alt=\"Anime.js code example\" src=\"./assets/images/usage-example-result.gif\">\n  </td>\n</tr>\n</table>\n\n## V4 Documentation\n\nThe full documentation is available [here](https://animejs.com/documentation).\n\n## V3 Migration guide\n\nYou can find the v3 to v4 migration guide [here](https://github.com/juliangarnier/anime/wiki/Migrating-from-v3-to-v4).\n\n## NPM development scripts\n\nFirst, run `npm i` to install all the necessary packages.\nThen, execute the following scripts with `npm run <script>`.\n\n| script | action |\n| ------ | ------ |\n| `dev` | Watches for changes in `src/**/*.js`, bundles the ESM version to `lib/` and creates type declarations in `types/` |\n| `dev:test` | Runs `dev` and `test:browser` concurrently |\n| `build` | Bundles ESM / UMD / CJS / IIFE versions to `lib/` and creates type declarations in `types/` |\n| `test:browser` | Starts a local server and runs all browser-related tests |\n| `test:node` | Starts Node-related tests |\n| `open:examples` | Starts a local server to browse the examples locally |\n\nÂ© [Julian Garnier](http://juliangarnier.com) | [MIT License](LICENSE.md)\n",
      "stars_today": 19
    },
    {
      "id": 6201092,
      "name": "mpv",
      "full_name": "mpv-player/mpv",
      "description": "ğŸ¥ Command line media player",
      "html_url": "https://github.com/mpv-player/mpv",
      "stars": 33583,
      "forks": 3195,
      "language": "C",
      "topics": [
        "audio",
        "c",
        "ffmpeg",
        "mplayer",
        "mpv",
        "multimedia",
        "video"
      ],
      "created_at": "2012-10-13T08:08:44Z",
      "updated_at": "2026-01-15T01:03:19Z",
      "pushed_at": "2026-01-14T00:56:18Z",
      "open_issues": 1071,
      "owner": {
        "login": "mpv-player",
        "avatar_url": "https://avatars.githubusercontent.com/u/2550273?v=4"
      },
      "readme": "![mpv logo](https://raw.githubusercontent.com/mpv-player/mpv.io/master/source/images/mpv-logo-128.png)\n\n# mpv\n\n\n* [External links](#external-links)\n* [Overview](#overview)\n* [System requirements](#system-requirements)\n* [Downloads](#downloads)\n* [Changelog](#changelog)\n* [Compilation](#compilation)\n* [Release cycle](#release-cycle)\n* [Bug reports](#bug-reports)\n* [Contributing](#contributing)\n* [License](#license)\n* [Contact](#contact)\n\n\n## External links\n\n\n* [Wiki](https://github.com/mpv-player/mpv/wiki)\n* [User Scripts](https://github.com/mpv-player/mpv/wiki/User-Scripts)\n* [FAQ][FAQ]\n* [Manual](https://mpv.io/manual/master/)\n\n\n## Overview\n\n\n**mpv** is a free (as in freedom) media player for the command line. It supports\na wide variety of media file formats, audio and video codecs, and subtitle types.\n\nThere is a [FAQ][FAQ].\n\nReleases can be found on the [release list][releases].\n\n## System requirements\n\n- A not too ancient Linux (usually, only the latest releases of distributions\n  are actively supported), Windows 10 1607 or later, or macOS 10.15 or later.\n- A somewhat capable CPU. Hardware decoding might help if the CPU is too slow to\n  decode video in realtime, but must be explicitly enabled with the `--hwdec`\n  option.\n- A not too crappy GPU. mpv's focus is not on power-efficient playback on\n  embedded or integrated GPUs (for example, hardware decoding is not even\n  enabled by default). Low power GPUs may cause issues like tearing, stutter,\n  etc. On such GPUs, it's recommended to use `--profile=fast` for smooth playback.\n  The main video output uses shaders for video rendering and scaling,\n  rather than GPU fixed function hardware. On Windows, you might want to make\n  sure the graphics drivers are current. In some cases, ancient fallback video\n  output methods can help (such as `--vo=xv` on Linux), but this use is not\n  recommended or supported.\n\nmpv does not go out of its way to break on older hardware or old, unsupported\noperating systems, but development is not done with them in mind. Keeping\ncompatibility with such setups is not guaranteed. If things work, consider it\na happy accident.\n\n## Downloads\n\n\nFor semi-official builds and third-party packages please see\n[mpv.io/installation](https://mpv.io/installation/).\n\n## Changelog\n\n\nThere is no complete changelog; however, changes to the player core interface\nare listed in the [interface changelog][interface-changes].\n\nChanges to the C API are documented in the [client API changelog][api-changes].\n\nThe [release list][releases] has a summary of most of the important changes\non every release.\n\nChanges to the default key bindings are indicated in\n[restore-old-bindings.conf][restore-old-bindings].\n\nChanges to the default OSC bindings are indicated in\n[restore-osc-bindings.conf][restore-osc-bindings].\n\n## Compilation\n\n\nCompiling with full features requires development files for several\nexternal libraries. Mpv requires [meson](https://mesonbuild.com/index.html)\nto build. Meson can be obtained from your distro or PyPI.\n\nAfter creating your build directory (e.g. `meson setup build`), you can view a list\nof all the build options via `meson configure build`. You could also just simply\nlook at the `meson_options.txt` file. Logs are stored in `meson-logs` within\nyour build directory.\n\nExample:\n\n    meson setup build\n    meson compile -C build\n    meson install -C build\n\nFor libplacebo, meson can use a git check out as a subproject for a convenient\nway to compile mpv if a sufficient libplacebo version is not easily available\nin the build environment. It will be statically linked with mpv. Example:\n\n    mkdir -p subprojects\n    git clone https://code.videolan.org/videolan/libplacebo.git --depth=1 --recursive subprojects/libplacebo\n\nEssential dependencies (incomplete list):\n\n- gcc or clang\n- X development headers (xlib, xrandr, xext, xscrnsaver, xpresent, libvdpau,\n  libGL, GLX, EGL, xv, ...)\n- Audio output development headers (libasound/ALSA, pulseaudio)\n- FFmpeg libraries (libavutil libavcodec libavformat libswscale libavfilter\n  and either libswresample or libavresample)\n- libplacebo\n- zlib\n- iconv (normally provided by the system libc)\n- libass (OSD, OSC, text subtitles)\n- Lua (optional, required for the OSC pseudo-GUI and youtube-dl integration)\n- libjpeg (optional, used for screenshots only)\n- uchardet (optional, for subtitle charset detection)\n- nvdec and vaapi libraries for hardware decoding on Linux (optional)\n\nLibass dependencies (when building libass):\n\n- gcc or clang, nasm on x86 and x86_64\n- fribidi, freetype, fontconfig development headers (for libass)\n- harfbuzz (required for correct rendering of combining characters, particularly\n  for correct rendering of non-English text on macOS, and Arabic/Indic scripts on\n  any platform)\n\nFFmpeg dependencies (when building FFmpeg):\n\n- gcc or clang, nasm on x86 and x86_64\n- OpenSSL or GnuTLS (have to be explicitly enabled when compiling FFmpeg)\n- libx264/libmp3lame/libfdk-aac if you want to use encoding (have to be\n  explicitly enabled when compiling FFmpeg)\n- For native DASH playback, FFmpeg needs to be built with --enable-libxml2\n  (although there are security implications, and DASH support has lots of bugs).\n- AV1 decoding support requires dav1d.\n- For good nvidia support on Linux, make sure nv-codec-headers is installed\n  and can be found by configure.\n\nMost of the above libraries are available in suitable versions on normal\nLinux distributions. For ease of compiling the latest git master of everything,\nyou may wish to use the separately available build wrapper ([mpv-build][mpv-build])\nwhich first compiles FFmpeg libraries and libass, and then compiles the player\nstatically linked against those.\n\nIf you want to build a Windows binary, see [Windows compilation][windows_compilation].\n\n\n## Release cycle\n\nOnce or twice a year, a release is cut off from the current development state\nand is assigned a 0.X.0 version number. No further maintenance is done, except\nin the event of security issues.\n\nThe goal of releases is to make Linux distributions happy. Linux distributions\nare also expected to apply their own patches in case of bugs.\n\nReleases other than the latest release are unsupported and unmaintained.\n\nSee the [release policy document][release-policy] for more information.\n\n## Bug reports\n\n\nPlease use the [issue tracker][issue-tracker] provided by GitHub to send us bug\nreports or feature requests. Follow the template's instructions or the issue\nwill likely be ignored or closed as invalid.\n\nQuestions can be asked in the [discussions][discussions] or on IRC (see\n[Contact](#Contact) below).\n\n## Contributing\n\n\nPlease read [contribute.md][contribute.md].\n\nFor small changes you can just send us pull requests through GitHub. For bigger\nchanges come and talk to us on IRC before you start working on them. It will\nmake code review easier for both parties later on.\n\nYou can check [the wiki](https://github.com/mpv-player/mpv/wiki/Stuff-to-do)\nor the [issue tracker](https://github.com/mpv-player/mpv/issues?q=is%3Aopen+is%3Aissue+label%3Ameta%3Afeature-request)\nfor ideas on what you could contribute with.\n\n## License\n\nGPLv2 \"or later\" by default, LGPLv2.1 \"or later\" with `-Dgpl=false`.\nSee [details.](https://github.com/mpv-player/mpv/blob/master/Copyright)\n\n## History\n\nThis software is based on the MPlayer project. Before mpv existed as a project,\nthe code base was briefly developed under the mplayer2 project. For details,\nsee the [FAQ][FAQ].\n\n## Contact\n\n\nMost activity happens on the IRC channel and the GitHub issue tracker.\n\n- **GitHub issue tracker**: [issue tracker][issue-tracker] (report bugs here)\n- **Discussions**: [discussions][discussions]\n- **User IRC Channel**: `#mpv` on `irc.libera.chat`\n- **Developer IRC Channel**: `#mpv-devel` on `irc.libera.chat`\n\n[FAQ]: https://github.com/mpv-player/mpv/wiki/FAQ\n[releases]: https://github.com/mpv-player/mpv/releases\n[mpv-build]: https://github.com/mpv-player/mpv-build\n[issue-tracker]:  https://github.com/mpv-player/mpv/issues\n[discussions]: https://github.com/mpv-player/mpv/discussions\n[release-policy]: https://github.com/mpv-player/mpv/blob/master/DOCS/release-policy.md\n[windows_compilation]: https://github.com/mpv-player/mpv/blob/master/DOCS/compile-windows.md\n[interface-changes]: https://github.com/mpv-player/mpv/blob/master/DOCS/interface-changes.rst\n[api-changes]: https://github.com/mpv-player/mpv/blob/master/DOCS/client-api-changes.rst\n[restore-old-bindings]: https://github.com/mpv-player/mpv/blob/master/etc/restore-old-bindings.conf\n[restore-osc-bindings]: https://github.com/mpv-player/mpv/blob/master/etc/restore-osc-bindings.conf\n[contribute.md]: https://github.com/mpv-player/mpv/blob/master/DOCS/contribute.md\n",
      "stars_today": 19
    },
    {
      "id": 379877112,
      "name": "open-meteo",
      "full_name": "open-meteo/open-meteo",
      "description": "Free Weather Forecast API for non-commercial use",
      "html_url": "https://github.com/open-meteo/open-meteo",
      "stars": 4493,
      "forks": 292,
      "language": "Swift",
      "topics": [
        "weather",
        "weather-api",
        "weather-forecast"
      ],
      "created_at": "2021-06-24T09:48:38Z",
      "updated_at": "2026-01-14T19:55:13Z",
      "pushed_at": "2026-01-14T15:32:28Z",
      "open_issues": 160,
      "owner": {
        "login": "open-meteo",
        "avatar_url": "https://avatars.githubusercontent.com/u/86407831?v=4"
      },
      "readme": "# ğŸŒ¤ Open-Meteo Weather API\n\n[![Test](https://github.com/open-meteo/open-meteo/actions/workflows/test.yml/badge.svg?branch=main)](https://github.com/open-meteo/open-meteo/actions/workflows/test.yml) [![GitHub license](https://img.shields.io/github/license/open-meteo/open-meteo)](https://github.com/open-meteo/open-meteo/blob/main/LICENSE) [![license: CC BY 4.0](https://img.shields.io/badge/license-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/) [![Twitter](https://img.shields.io/badge/follow-%40open_meteo-1DA1F2?logo=twitter&style=social)](https://twitter.com/open_meteo) [![Mastodon](https://img.shields.io/mastodon/follow/109320332765909743?domain=https%3A%2F%2Ffosstodon.org)](https://fosstodon.org/@openmeteo) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7970649.svg)](https://doi.org/10.5281/zenodo.7970649)\n\nOpen-Meteo is an open-source weather API and offers free access for non-commercial use. No API key is required. You can use it immediately!\n\nHead over to https://open-meteo.com! Stay up to date with our blog at https://openmeteo.substack.com.\n\n## Features\n\n- [Hourly weather forecast](https://open-meteo.com/en/docs) for up to 16 days\n- Global weather models with 11 km and regional models up to 1.5 km resolution\n- Weather model updates every hour for Europe and North America\n- 80 years [Historical Weather API](https://open-meteo.com/en/docs/historical-weather-api)\n- Based on the best weather models: [NOAA GFS with HRRR](https://open-meteo.com/en/docs/gfs-api), [DWD ICON](https://open-meteo.com/en/docs/dwd-api), [MeteoFrance Arome&Arpege](https://open-meteo.com/en/docs/meteofrance-api), [ECMWF IFS](https://open-meteo.com/en/docs/ecmwf-api), [JMA](https://open-meteo.com/en/docs/jma-api), [GEM HRDPS](https://open-meteo.com/en/docs/gem-api), [MET Norway](https://open-meteo.com/en/docs/metno-api)\n- [Marine Forecast API](https://open-meteo.com/en/docs/marine-weather-api), [Air Quality API](https://open-meteo.com/en/docs/air-quality-api), [Geocoding API](https://open-meteo.com/en/docs/geocoding-api), [Elevation API](https://open-meteo.com/en/docs/elevation-api), [Flood API](https://open-meteo.com/en/docs/flood-api)\n- Lightning fast APIs with response times below 10 ms\n- Servers located in Europe and North America with GeoDNS for best latency and high-availability\n- No API key required, CORS supported, no ads, no tracking, not even cookies\n- Free for non-commercial use with data under Attribution 4.0 International (CC BY 4.0)\n- Source code available under AGPLv3\n\n## How does Open-Meteo work?\n\nOpen-Meteo utilizes open-data weather forecasts provided by national weather services. These services offer numerical weather predictions that are free to download. However, working with these models can be challenging, as it requires expertise in binary file formats, grid-systems, projections, and the fundamentals of weather predictions.\n\nLike many other weather APIs, Open-Meteo integrates high-resolution local and global weather models. Over 2 TB of data are downloaded and processed daily from multiple national weather services. The collected data is then stored in local files using a customized file format and compression technique to enhance access to time-series data such as a 14-day temperature forecast.\n\nIn contrast to other weather APIs, Open-Meteo provides complete access to its source code, and all data sources are openly listed, crediting the national weather services for their work. With Docker or prebuilt Ubuntu packages, it is possible to launch your own weather API within minutes. By providing the source code, users can conduct detailed verifications of the weather data processing and even make modifications themselves. Contributions are highly encouraged and welcomed.\n\nThe API is available for non-commercial use at no cost. Despite being free of charge, the forecast accuracy is top-notch. The API utilizes a vast array of local weather models with rapid updates, ensuring that the most precise forecast is generated for any location globally.\n\n## Resources\n\n- All API documentation can be found on https://open-meteo.com. The source code for the website, documentation and API generator is available here: https://github.com/open-meteo/open-meteo-website\n- The free non-commerical API is hosted at [https://api.open-meteo.com](https://api.open-meteo.com/v1/forecast?latitude=52.52&longitude=13.41&hourly=temperature_2m) using to GeoDNS to servers in Europe and North America (HTTPS is optional). The API source code is in this current repository.\n- The geocoding API source code is available in a separate repository https://github.com/open-meteo/geocoding-api\n- Larger changes are announced in the [Open-Meteo Blog](https://openmeteo.substack.com)\n- The [Open-Meteo weather database](https://github.com/open-meteo/open-data) is redistributed as part of an AWS Open-Data Sponsorship\n\n## Who is using Open-Meteo?\n\nApps:\n\n- [Alpine Conditions](https://www.alpineconditions.com) Allows a user to compare multiple models at once & create ensemble forecasts for any location\n- [Breezy Weather](https://github.com/breezy-weather/breezy-weather) A feature-rich, free and open source Material 3 Expressive Android weather app.\n- [Cirrus](https://github.com/woheller69/omweather) Android Weather App\n- [Clima](https://f-droid.org/packages/co.prestosole.clima/) Beautiful, minimal, and fast weather app\n- [DroneWeather](https://play.google.com/store/apps/details?id=xyz.droneweather.app) Weather forecasts, satellite count, and KP index for drone pilots.\n- [Emojiton Weather](https://emojiton.com/weather) Get the local weather forecast for your location with fun emoji representations\n- [Evaporative Cooler Forecaster](https://SwampCooler.app) Swamp cooler effectiveness forecast with cost & energy savings, Android/iOS app\n- [Home Assistant](https://www.home-assistant.io/integrations/open_meteo/) A popular open source smart home platform.\n- [Lively Weather](https://www.rocksdanister.com/weather) Windows native weather app powered by DirectX12 animations.\n- [LunaLink](https://www.lunalink.de) A site for hunters, fishermen and nature observers: It provides sun and moon values â€‹â€‹(including moon brightness) as well as the weather for individual locations in Central Europe.\n- [Meteo-Fly](https://meteo-fly.com) Free flight-weather charts for paraglider & hang-glider pilots.\n- [MeteoHist](https://yotka.org/meteo-hist) A web app to create interactive temperature and precipitation graphs for places around the world\n- [OSS Weather](https://github.com/Akylas/oss-weather) - Multi-model/multi-provider Open Source Android/iOS Weather app\n- [Overmorrow](https://github.com/bmaroti9/Overmorrow) A modern material design Android weather app.\n- [PointWx](https://hh.guidocioni.it/pointwx/) Dash application with interactive plots (from beginner-friendly to weather-enthusiast level) easily deployable\n- [QuickWeather](https://github.com/TylerWilliamson/QuickWeather) Fast, free, and open source Android app\n- [Rain](https://github.com/DarkMooNight/Rain) Free, open source, beautiful, minimal and fast weather app\n- [Raindrop](https://github.com/metalfoxdev/Raindrop) Simple and intuitive weather app for the linux terminal.\n- [Road Vagabond](https://roadvagabond.com) A camping destination discovery app showing zones within your drive time with weather-based filtering.\n- [SkyMuse](https://github.com/cakephone/skymuse) Minimal, privacy-respecting weather app. Built with web technologies.\n- [Slideshow](https://slideshow.digital/) Digital Signage app for Android\n- [solXpect](https://github.com/woheller69/solxpect) Android app which forecasts the output of your solar power plant\n- [The Weather](https://weather.jamesdinovo.com) A detailed, installable, progressive web application\n- [truthclimate](https://www.truthclimate.com) Discover how weather and climate changed all around the world.\n- [Weather Please](https://github.com/ggaidelevicius/weather-please/) Clean and minimal new tab replacement for browsers\n- [Weather.io](https://weather.roessner.tech) A simple Progressive Web App (PWA) for checking the weather.\n- [Weather](https://github.com/GustavLindberg99/AndroidWeather) Free, open source, simple and complete weather app for Android\n- [WeatherAI](https://play.google.com/store/apps/details?id=com.kingfu.weatherai) WeatherAI offers an intuitive user experience that makes checking the weather a breeze.\n- [WeatherGraph](https://weathergraph.app) Apple Watch App\n- [WeatherMaster](https://github.com/PranshulGG/WeatherMaster) A Weather app for android inspired by the Google Pixel weather app.\n- [Weatherian](https://weatherian.com/) Multi-model meteogram (multi-platform)\n- [weewx-DWD](https://github.com/roe-dl/weewx-DWD) Weather forecasts etc. for WeeWX\n- [WetBulb](https://github.com/Isma1306/wetbulb-forecast) A simple app that shows you the wetbulb temp 24h forecast and tells you if it is dangerous.\n\nRepositories:\n\n- [Captain Cold](https://github.com/cburton-godaddy/captain-cold) Simple Open-Meteo -> Discord integration\n- [wthrr-the-weathercrab](https://github.com/tobealive/wthrr-the-weathercrab) Weather companion for the terminal\n- [Weather-Cli](https://github.com/Rayrsn/Weather-Cli) A CLI program written in golang that allows you to get weather information from the terminal\n- [Homepage](https://github.com/benphelps/homepage/) A highly customizable homepage (or startpage / application dashboard) with Docker and service API integrations.\n- [Spots Guru](https://www.spots.guru) Weather forecast for lazy, the best wind & wave spots around you.\n- [WeatherReport.jl](https://github.com/vnegi10/WeatherReport.jl) A simple weather app for the Julia REPL\n- [DIY Arduino esp8266 weather station](https://github.com/AlexeyMal/esp8266-weather-station) esp8266 weather station using Open-Meteo API, an embedded C++ implementation example\n- [biome](https://github.com/SqrtMinusOne/biome) Bountiful Interface to Open Meteo for Emacs\n\nOther:\n\n- [Menubar Weather](https://www.raycast.com/koinzhang/menubar-weather) A Raycast extension that displays live weather information in your menu bar\n- [MiniPavi](https://www.minipavi.fr/emulminitel/) Vintage French Minitel (a kind of BBS) weather forecast service (type \"METEO\" keyword on welcome Minitel screen)\n- [OFM-InternetWeatherModule](https://github.com/OpenKNX/OFM-InternetWeatherModule) An OpenKNX module to provide data of weather services on KNX-bus (configurable via ETS)\n- Contributions welcome!\n\nDo you use Open-Meteo? Please open a pull request and add your repository or app to the list!\n\n## Client SDKs\n\n- Go https://github.com/HectorMalot/omgo\n- Python https://github.com/m0rp43us/openmeteopy\n- Kotlin https://github.com/open-meteo/open-meteo-api-kotlin\n- .Net / C# https://github.com/AlienDwarf/open-meteo-dotnet\n- dotnet 8 / C# https://github.com/colinnuk/open-meteo-dotnet-client-sdk\n- PHP Laravel https://github.com/michaelnabil230/laravel-weather\n- R https://github.com/tpisel/openmeteo\n- PHP Symfony 6.2 https://gitlab.com/flibidi67/open-meteo\n- PHP for Geocoding API: https://gitlab.com/flibidi67/open-meteo-geocoding\n- Android library for Geocoding API: https://github.com/woheller69/OmGeoDialog\n- Dart / Flutter: https://github.com/neursh/open-meteo-dart\n- Rust: https://github.com/angelodlfrtr/open-meteo-rs\n\nContributions welcome! Writing a SDK for Open-Meteo is more than welcome and a great way to help users.\n\n## Support\n\nIf you encounter bugs while using Open-Meteo APIs, please file a new issue ticket. For general ideas or Q&A please use the [Discussion](https://github.com/open-meteo/open-meteo/discussions) section on Github. Thanks!\n\nFor other enquiries please contact info@open-meteo.com\n\n## Run your own API\n\nInstructions to use Docker to run your own weather API are available in the [getting started guide](/docs/getting-started.md).\n\n## Terms & Privacy\n\nOpen-Meteo APIs are free for open-source developer and non-commercial use. We do not restrict access, but ask for fair use.\n\nIf your application exceeds 10'000 requests per day, please contact us. We reserve the right to block applications and IP addresses that misuse our service.\n\nFor commercial use of Open-Meteo APIs, please contact us.\n\nAll data is provided as is without any warranty.\n\nWe do not collect any personal data. We do not share any personal information. We do not integrate any third party analytics, ads, beacons or plugins.\n\n## Data License\n\nAPI data are offered under Attribution 4.0 International (CC BY 4.0)\n\nYou are free to share: copy and redistribute the material in any medium or format and adapt: remix, transform, and build upon the material.\n\nAttribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\nYou must include a link next to any location, Open-Meteo data are displayed like:\n\n<a href=\"https://open-meteo.com/\">Weather data by Open-Meteo.com</a>\n\n## Source Code License\n\nOpen-Meteo is open-source under the GNU Affero General Public License Version 3 (AGPLv3) or any later version. You can [find the license here](LICENSE). Exceptions are third party source-code with individual licensing in each file.\n",
      "stars_today": 19
    },
    {
      "id": 536896277,
      "name": "flowpilot",
      "full_name": "flowdriveai/flowpilot",
      "description": "flow-pilot is an openpilot based driver assistance system that runs on linux, windows and android powered machines.",
      "html_url": "https://github.com/flowdriveai/flowpilot",
      "stars": 1925,
      "forks": 264,
      "language": "C",
      "topics": [],
      "created_at": "2022-09-15T06:39:03Z",
      "updated_at": "2026-01-15T00:49:26Z",
      "pushed_at": "2024-09-19T16:37:47Z",
      "open_issues": 24,
      "owner": {
        "login": "flowdriveai",
        "avatar_url": "https://avatars.githubusercontent.com/u/109819718?v=4"
      },
      "readme": "<img src=\"https://i.ibb.co/LZtKvfB/Screenshot-from-2022-09-15-22-15-14.png\" alt=\"table\" width=\"1270\" />\n\n# What is Flowpilot?\n\nFlowpilot is an open source driver assistance system built on top of openpilot, that can run on most windows/linux and android powered machines. It performs the functions of Adaptive Cruise Control (ACC), Automated Lane Centering (ALC), Forward Collision Warning (FCW), Lane Departure Warning (LDW) and Driver Monitoring (DM) for a growing variety of supported car makes, models, and model years maintained by the community.\n\n<table>\n  <tr>\n    <td><a href=\"https://youtu.be/L9O-WFmigSA\" title=\"Video By Ender\"><img src=\"https://i3.ytimg.com/vi/L9O-WFmigSA/maxresdefault.jpg\"></a></td>\n    <td><a href=\"https://youtu.be/mt86H67DhE0\" title=\"Video By Miso\"><img src=\"https://i3.ytimg.com/vi/mt86H67DhE0/maxresdefault.jpg\"></a></td>\n    <td><a href=\"https://youtu.be/06DLmtF6og4\" title=\"Video By Ender\"><img src=\"https://i3.ytimg.com/vi/06DLmtF6og4/maxresdefault.jpg\"></a></td>\n    <td><a href=\"https://youtu.be/FBB2XRMej9M\" title=\"Video By Miso\"><img src=\"https://i3.ytimg.com/vi/FBB2XRMej9M/maxresdefault.jpg\"></a></td>\n  </tr>\n</table>\n\n# Updates:\nThe main developers have less time to maintain the project now. So expect less support. Most of the stuff should be out there on discord and the wiki. Though, there are some few forks of flowpilot that are more actively maintained, for example, [PhrOOt's](https://github.com/phr00t/flowpilot/wiki/FlowPilot:-FAQ) fork. Although please note that PhrOOt's fork supports only LG G8 devices for now. \n\n# Running on a Car\n\nFor running flowpilot on your car, you need: \n\n - A supported machine to run flowpilot i.e. A windows/linux PC or an android phone.\n - A white / grey panda with giraffe or a black/red panda with car harness. \n - 1x USB-A to USB-A cable for connecting panda to PC and aditionally, an OTG cable is required if connecting panda to phone.\n - One of the [200+ supported cars](https://github.com/commaai/openpilot/blob/master/docs/CARS.md). The community supports Honda, Toyota, Hyundai, Nissan, Kia, Chrysler, Lexus, Acura, Audi, VW, and more. If your car is not supported but has adaptive cruise control and lane-keeping assist, it's likely able to run flowpilot.\n \n For a more detailed overview, see the [wiring and hardware wiki](https://github.com/flowdriveai/flowpilot/wiki/Connecting-to-Car).\n \n# Installation:\nSee the [installation wiki](https://github.com/flowdriveai/flowpilot/wiki/Installation).\n\n# Running With a Virtual Car\n\nIt is recommended to develop on a virtual car / simulation before jumping onto testing on a real car. Flowpilot supports CARLA simulation. Optionally, you can use FlowStreamer to test flowpilot with any videogame. For more thorough testing, in addition to simulation, real panda hardware can be put in the loop for a more [thorough testing](https://twitter.com/flowdrive_ai/status/1566680576962478086).\n\n# Community\n\n[<img src=\"https://assets-global.website-files.com/6257adef93867e50d84d30e2/636e0b5061df29d55a92d945_full_logo_blurple_RGB.svg\" width=\"200\">](https://discord.com/invite/APJaQR9nhz)\n\nFlowpilot's core community lives on the official flowdrive [discord server](https://discord.com/invite/APJaQR9nhz). Check the pinned messages or search history through messages to see if your issues or question has been discussed earlier. You may also join [more awesome](https://linktr.ee/flowdrive) openpilot discord communities. \n\nWe also push frequent updates on our [twitter handle](https://twitter.com/flowdrive_ai).\n\n# User Data \n\nFlowpilot will require your email address for setting up you flowdrive account. Flowpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs. The driver-facing camera is only logged if you explicitly opt-in in settings. The microphone is not recorded.\n\nYou understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of flowdrive. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to flowdrive for the use of this data.\n\n# Disclaimer \n\nTHIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT. YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS. NO WARRANTY EXPRESSED OR IMPLIED.\n",
      "stars_today": 19
    },
    {
      "id": 44662669,
      "name": "dbeaver",
      "full_name": "dbeaver/dbeaver",
      "description": "Free universal database tool and SQL client",
      "html_url": "https://github.com/dbeaver/dbeaver",
      "stars": 48225,
      "forks": 3995,
      "language": "Java",
      "topics": [
        "ai",
        "copilot",
        "database",
        "db2",
        "dbeaver",
        "erd",
        "gui",
        "java",
        "jdbc",
        "mysql",
        "nosql",
        "openai",
        "oracle",
        "postgresql",
        "redshift",
        "sql",
        "sqlite",
        "sqlserver"
      ],
      "created_at": "2015-10-21T08:26:28Z",
      "updated_at": "2026-01-15T00:17:51Z",
      "pushed_at": "2026-01-14T20:51:14Z",
      "open_issues": 3188,
      "owner": {
        "login": "dbeaver",
        "avatar_url": "https://avatars.githubusercontent.com/u/34743864?v=4"
      },
      "readme": "[![Twitter URL](https://img.shields.io/twitter/url/https/twitter.com/dbeaver_news.svg?style=social&label=Follow%20%40dbeaver_news)](https://twitter.com/dbeaver_news)\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/fa0bb9cf5a904c7d87424f8f6351ba92)](https://app.codacy.com/gh/dbeaver/dbeaver/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)\n[![Apache 2.0](https://img.shields.io/github/license/cronn-de/jira-sync.svg)](http://www.apache.org/licenses/LICENSE-2.0)\n[![Tickets in review](https://img.shields.io/github/issues/dbeaver/dbeaver/wait%20for%20review)](https://github.com/dbeaver/dbeaver/issues?q=is%3Aissue+is%3Aopen+label%3A\"wait%20for%20review\")\n<img src=\"https://github.com/dbeaver/dbeaver/wiki/images/dbeaver-icon-64x64.png\" align=\"right\"/>\n\n# DBeaver\n\nFree multi-platform database tool for developers, SQL programmers, database administrators and analysts.  \n\n* Has a lot of <a href=\"https://github.com/dbeaver/dbeaver/wiki\">features</a> including schema editor, SQL editor, data editor, AI integration, ER diagrams, data export/import/migration, SQL execution plans, database administration tools, database dashboards, Spatial data viewer, proxy and SSH tunnelling, custom database drivers editor, etc.\n* Out of the box supports more than <a href=\"#supported-databases\">100 database drivers</a>.\n* Supports any database which has JDBC or ODBC driver (basically - almost all existing databases).\n* Supports smart AI completion and code generation with OpenAI or Copilot\n\n<a href=\"https://dbeaver.io/product/dbeaver-sql-editor.png\"><img src=\"https://dbeaver.io/product/dbeaver-sql-editor.png\" width=\"400\"/></a>\n<a href=\"https://dbeaver.io/product/dbeaver-gis-viewer.png\"><img src=\"https://dbeaver.io/product/dbeaver-gis-viewer.png\" width=\"400\"/></a>\n<a href=\"https://dbeaver.io/product/dbeaver-data-editor.png\"><img src=\"https://dbeaver.io/product/dbeaver-data-editor.png\" width=\"400\"/></a>\n<a href=\"https://dbeaver.io/product/dbeaver-erd.png\"><img src=\"https://dbeaver.io/product/dbeaver-erd.png\" width=\"400\"/></a>\n\n## Download\n\nYou can download prebuilt binaries from <a href=\"https://dbeaver.io/download\" target=\"_blank\">official website</a> or directly from <a href=\"https://github.com/dbeaver/dbeaver/releases\">GitHub releases</a>.  \nYou can also download <a href=\"https://dbeaver.io/files/ea\" target=\"_blank\">Early Access</a> version. We publish daily.  \n\n## Running\n\nJust run an installer (or unzip an archive) and run `dbeaver`.  \n\nNote: DBeaver needs Java to run. <a href=\"https://adoptium.net/temurin/releases/?package=jre\" target=\"_blank\">OpenJDK 21</a> is included in all DBeaver distributions.\nYou can change default JDK version by replacing directory `jre` in dbeaver installation folder.\n\n## Documentation\n\n* [Full product documentation](https://dbeaver.com/docs/dbeaver/)\n* [WIKI](https://github.com/dbeaver/dbeaver/wiki)\n* [Issue tracker](https://github.com/dbeaver/dbeaver/issues)\n* [Building from sources](https://github.com/dbeaver/dbeaver/wiki/Build-from-sources)\n\n## Architecture\n\n- DBeaver is written mostly on Java. However, it also uses a set of native OS-specific components for desktop UI, high performance database drivers and networking.\n- Basic frameworks:\n  - [OSGI](https://en.wikipedia.org/wiki/OSGi) platform for plugins and dependency management. Community version consists of 130+ plugins.\n  - [Eclipse RCP](https://github.com/eclipse-platform/eclipse.platform.ui/blob/master/docs/Rich_Client_Platform.md) platform for rich user interface build.\n  - [JDBC](https://en.wikipedia.org/wiki/Java_Database_Connectivity) for basic database connectivity API.\n  - [JSQLParser](https://github.com/JSQLParser/JSqlParser) and [Antlr4](https://github.com/antlr/antlr4) for SQL grammar and semantic parser.\n- For networking and additional functionality we use wide range of open source libraries such as [SSHJ](https://github.com/hierynomus/sshj), [Apache POI](https://github.com/apache/poi), [JFreeChart](https://github.com/jfree/jfreechart), [JTS](https://github.com/locationtech/jts), [Apache JEXL](https://github.com/apache/commons-jexl) etc.\n- We separate model plugins from desktop UI plugins. This allows us to use the same set of \"back-end\" plugins in both DBeaver and [CloudBeaver](https://github.com/dbeaver/cloudbeaver).\n- Dependencies: being an OSGI application we use P2 repositories for third party dependencies. For additional Maven dependencies we use our own [DBeaver P2 repo](https://github.com/dbeaver/dbeaver-deps-ce).\n\n## Supported databases\n\n### Community version\n\nOut of the box DBeaver supports following database drivers: \nMySQL, MariaDB, Oracle, DB2, PostgreSQL, SQL Server, Sybase, Apache Hive, Drill, Presto, Trino, Phoenix, Exasol, Informix, Teradata, Vertica, Netezza, Firebird, Derby, H2, H2GIS, WMI, Snowflake, Greenplum, Redshift, Athena, SAP HANA, MaxDB, NuoDB, MS Access, SQLite, CSV, DBF, Firebird, TimescaleDB, Yellowbrick, CockroachDB, OrientDB, MonetDB, Google BigQuery, Google Spanner, Apache Hive/Impala/Spark, Apache Ignite, MapD, Azure SQL, CrateDB, Elasticsearch, Ocient, Ingres, OmniSci, Yugabyte, IRIS, Data Virtuality, Denodo, Virtuoso, Machbase, DuckDB, Babelfish, OceanBase, Salesforce, EnterpriseDB, Apache Druid, Apache Kylin, Databricks, OpenSearch, TiDB, TDEngine, Materialize, JDBCX, Dameng, Altibase, StarRocks, CUBRID, GaussDB, DolphinDB, LibSQL, GBase 8s, Databend, Cloudberry, Teiid, Kingbase.\n\n### PRO versions\n\n<a href=\"https://dbeaver.com/download/\">Commercial versions</a> extends functionality of many popular drivers and also support non-JDBC datasources such as:\nODBC, MongoDB, Cassandra, Couchbase, CouchDB, Redis, InfluxDB, Firestore, BigTable, DynamoDB, Kafka KSQL, Neo4j, AWS Neptune, AWS Timestream, Azure CosmosDB, Yugabyte, Salesforce, etc.  \nAlso, we support flat files as databases: CSV, XLSX, Json, XML, Parquet.  \nYou can find the list of all databases supported in commercial versions <a href=\"https://dbeaver.com/databases/\">here</a>.\n\n## Feedback\n\n- For bug reports and feature requests - please <a href=\"https://github.com/dbeaver/dbeaver/issues\">create a ticket</a>.\n- To promote <a href=\"https://github.com/dbeaver/dbeaver/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+label%3A%22wait+for+votes%22\">a ticket</a> to a higher priority - please vote for it with ğŸ‘ under the ticket description.\n- If you have any questions, ideas, etc - please <a href=\"https://github.com/dbeaver/dbeaver/discussions\">start a discussion</a>.\n- Pull requests are welcome. See our <a href=\"https://github.com/dbeaver/dbeaver/wiki/Contribute-your-code\">guide for contributors</a>.\n- Visit https://dbeaver.com for more information.\n- Follow us on [X](https://x.com/dbeaver_news/) and watch educational video on [YouTube](https://www.youtube.com/@DBeaver_video)\n- Thanks for using DBeaver! Star if you like it.\n\n## Contribution: help the Beaver!\n\nHooray, we have reached 40k+ stars on GitHub and continue to grow!  \nThat's really cool, and we are glad that you like DBeaver.\n\n- We are actively looking for new source code contributors. We have added labels â€œGood first issueâ€ and â€œHelp wantedâ€ to some tickets. If you want to be a part of our development team, just be brave and take a ticket. <a href=\"https://dbeaver.com/help-dbeaver/\">We are happy to reward</a> our most active contributors every major sprint.\n- You can buy <a href=\"https://dbeaver.com/buy/\">one of our commercial versions</a>. They include NoSQL databases support, additional extensions, and official online support. Also, licensed users have priorities in bug fixes and the development of new features.\n\nThank you!  \n\n- <a href=\"https://github.com/dbeaver/dbeaver/graphs/contributors\">DBeaver Team</a> (contributors)\n\n---------\n\n<a href=\"https://github.com/dbeaver/cloudbeaver/\"><img src=\"https://github.com/dbeaver/cloudbeaver/wiki/images/cloudbeaver-logo.png\" width=\"250\"/></a>\n\n<a href=\"https://github.com/dbeaver/cloudbeaver\">CloudBeaver</a> is a web-based database management tool built on the DBeaver platform. It brings the capabilities of DBeaver to the browser, enabling database management from any device with an internet connection and eliminating the need for local installation. Supporting any database, CloudBeaver incorporates most of DBeaver's features and includes advanced access management for secure collaboration.\nDesigned with a user-friendly interface, CloudBeaver simplifies complex database operations and is suitable for both individual developers and organizations. Its scalable architecture accommodates various needs, making it a convenient solution for managing databases anytime and anywhere through web-based accessibility.\n",
      "stars_today": 17
    },
    {
      "id": 254327261,
      "name": "Sandboxie",
      "full_name": "sandboxie-plus/Sandboxie",
      "description": "Sandboxie Plus & Classic",
      "html_url": "https://github.com/sandboxie-plus/Sandboxie",
      "stars": 16852,
      "forks": 1882,
      "language": "C",
      "topics": [],
      "created_at": "2020-04-09T09:26:37Z",
      "updated_at": "2026-01-15T00:58:21Z",
      "pushed_at": "2026-01-13T19:11:42Z",
      "open_issues": 687,
      "owner": {
        "login": "sandboxie-plus",
        "avatar_url": "https://avatars.githubusercontent.com/u/63755826?v=4"
      },
      "readme": "# Sandboxie Plus / Classic\n\n<p align='center'>\nEN | <a href='./README_zh_CN.md'>ä¸­æ–‡</a>\n</p>\n\n[![Plus license](https://img.shields.io/badge/Plus%20license-Custom%20-blue.svg)](./LICENSE.Plus) [![Classic license](https://img.shields.io/github/license/Sandboxie-Plus/Sandboxie?label=Classic%20license&color=blue)](./LICENSE.Classic) [![GitHub Release](https://img.shields.io/github/release/sandboxie-plus/Sandboxie.svg)](https://github.com/sandboxie-plus/Sandboxie/releases/latest) [![GitHub Pre-Release](https://img.shields.io/github/release/sandboxie-plus/Sandboxie/all.svg?label=pre-release)](https://github.com/sandboxie-plus/Sandboxie/releases) [![GitHub Build Status](https://github.com/sandboxie-plus/Sandboxie/actions/workflows/main.yml/badge.svg)](https://github.com/sandboxie-plus/Sandboxie/actions) [![GitHub Codespell Status](https://github.com/sandboxie-plus/Sandboxie/actions/workflows/codespell.yml/badge.svg)](https://github.com/sandboxie-plus/Sandboxie/actions/workflows/codespell.yml) [![WinGet Build Status](https://github.com/sandboxie-plus/Sandboxie/actions/workflows/winget.yml/badge.svg)](https://github.com/sandboxie-plus/Sandboxie/actions/workflows/winget.yml) [![Gurubase](https://img.shields.io/badge/Gurubase-Ask%20Sandboxie%20Guru-006BFF)](https://gurubase.io/g/sandboxie)\n\n[![Roadmap](https://img.shields.io/badge/Roadmap-Link%20-blue?style=for-the-badge)](https://www.wilderssecurity.com/threads/updated-sandboxie-plus-roadmap.456886/) [![Join our Discord Server](https://img.shields.io/badge/Join-Our%20Discord%20Server%20for%20bugs,%20feedback%20and%20more!-blue?style=for-the-badge&logo=discord)](https://discord.gg/S4tFu6Enne)\n\n|  System requirements  |      Release notes     |     Contribution guidelines   |      Security policy      |      Code of Conduct      |\n|         :---:         |          :---:         |          :---:                |          :---:            |          :---:            |\n| Windows 7 or higher (64-bit) |  [CHANGELOG.md](./CHANGELOG.md)  |  [CONTRIBUTING.md](./CONTRIBUTING.md)  |   [SECURITY.md](./SECURITY.md)  |  [CODE_OF_CONDUCT.md](./CODE_OF_CONDUCT.md)  |\n\nSandboxie is a sandbox-based isolation software for Windows NT-based operating systems that creates a secure operating environment in which applications can be run or installed without permanently modifying local & mapped drives or the Windows registry. An isolated virtual environment allows controlled testing of untrusted programs and web surfing.<br>\n\nSandboxie allows you to create virtually unlimited sandboxes and run them alone or simultaneously to isolate programs from the host and each other, while also allowing you to run as many programs simultaneously in a single box as you wish.\n\n**Note: This is a community fork that took place after the release of the Sandboxie source code and not the official continuation of the previous development (see the [project history](#project-history) and [#2926](https://github.com/sandboxie-plus/Sandboxie/issues/2926)).**\n\n## â¬ Download\n\n[Latest Release](https://github.com/sandboxie-plus/Sandboxie/releases/latest)\n\n## âœ¨ Changelog\n\n<a href='./CHANGELOG.md'>EN</a>\n\n## ğŸš€ Features\n\nSandboxie is available in two editions, Plus and Classic. They both share the same core components, this means they have the same level of security and compatibility.\nWhat's different is the availability of features in the user interface.\n\nSandboxie Plus has a modern Qt-based UI, which supports all new features that have been added since the project went open source:\n\n  * Snapshot Manager - takes a copy of any box in order to be restored when needed\n  * Maintenance menu - allows to uninstall/install/start/stop Sandboxie driver and service when needed\n  * Portable mode - you can run the installer and choose to extract all files to a directory\n  * Additional UI options to block access to Windows components like printer spooler and clipboard\n  * More customization options for Start/Run and Internet access restrictions\n  * Privacy mode sandboxes that protect user data from illegitimate access\n  * Security enhanced sandboxes that restrict the availability of syscalls and endpoints\n  * Global hotkeys to suspend or terminate all boxed processes\n  * A network firewall per sandbox which supports Windows Filtering Platform (WFP)\n  * The list of sandboxes can be searched with the shortcut key Ctrl+F\n  * A search function for Global Settings and Sandbox Options\n  * Ability to import/export sandboxes to and from 7z files\n  * Integration of sandboxes into the Windows Start menu\n  * A browser compatibility wizard to create templates for unsupported browsers\n  * Vintage View mode to reproduce the graphical appearance of Sandboxie Control\n  * A troubleshooting wizard to assist users with their problems\n  * An Add-on manager to extend or add functionality via additional components\n  * Protections of sandboxes against the host, including the prevention of taking screenshots\n  * A trigger system to perform actions, when a sandbox goes through different stages, like initialization, box start, termination or file recovery\n  * Make a process not sandboxed, but its child processes sandboxed\n  * Force programs to automatically use a user-provided SOCKS5 proxy\n  * DNS control by blocking or redirecting\n  * Limit the amount of memory space a single process in the sandbox can occupy and the total amount of memory space all processes can occupy, and You can limit the total number of sandboxed processes per box\n  * A completely different token creation mechanism from Sandboxie's pre-open-source version makes sandboxes more independent in the system\n  * Encrypted Sandbox - an AES-based reliable data storage solution\n  * Prevent sandboxed programs from generating unnecessary unique identifier in the normal way\n  * An internal INI editor that aids the user with visual hints and tooltips on the settings they have configured or want to add\n  * The ability to configure an external text editor, beside the system default\n  * Control over the alpha transparency of the border\n  * A custom UAC-dialog, allowing to fake permission, grant them or cancel the elevation attempt\n  * Modern icons, while you can use the old-school ones in certain places\n  * You can change the font of the user interface\n  * Custom colors or icons can be used for sandboxes or groups\n\nMore features can be spotted by finding the sign `=` through the shortcut key Ctrl+F in the [CHANGELOG.md](./CHANGELOG.md) file.\n\nSandboxie Classic has the old no longer developed MFC-based UI, hence it lacks native interface support for Plus features. Although some of the missing features can be configured manually in the Sandboxie.ini configuration file or even replaced with [custom scripts](https://sandboxie-website-archive.github.io/www.sandboxie.com/old-forums/viewforum1a2d1a2d.html?f=22), the Classic edition is not recommended for users who want to explore the latest security options.\n\n## ğŸ“š Documentation\n\nA GitHub copy of the [Sandboxie documentation](https://sandboxie-plus.github.io/sandboxie-docs) is currently maintained, although more volunteers are needed to keep it updated with the new changes. It is recommended to also check the following labels to track current issues: [Labels Â· sandboxie-plus/Sandboxie](https://github.com/sandboxie-plus/Sandboxie/labels).\n\nA partial archive of the [old Sandboxie forum](https://sandboxie-website-archive.github.io/www.sandboxie.com/old-forums) that was previously maintained by Invincea is still available. If you need to find something specific, it is possible to use the following search query: `site:https://sandboxie-website-archive.github.io/www.sandboxie.com/old-forums/`.\n\n\n## ğŸš€ Useful tools for Sandboxie\n\nSandboxie's functionality can be enhanced with specialized tools like the following:\n\n  * [LogApiDll](https://github.com/sandboxie-plus/LogApiDll) - adds a verbose output to Sandboxie's trace log, listing invocations of relevant Windows API functions\n  * [SbieHide](https://github.com/VeroFess/SbieHide) - attempts to hide the presence of SbieDll.dll from the application being sandboxed\n  * [SandboxToys2](https://github.com/blap/SandboxToys2) - allows to monitor files and registry changes in a sandbox\n  * [Sbiextra](https://github.com/sandboxie-plus/sbiextra) - adds additional user mode restrictions to sandboxed processes\n  * [WrapLocale](https://github.com/UserUnknownFactor/WrapLocale) - provide more flexible locale pretending options than native LangId feature\n\n<a id=\"project-history\"></a>\n## ğŸ“Œ Project history\n\n|      Timeline       |    Maintainer    |\n|        :---         |       :---       |\n| 2004 - 2013         | Ronen Tzur       |\n| 2013 - 2017         | Invincea Inc.    |\n| 2017 - 2020         | Sophos Group plc |\n| 8 April 2020 - [open-source code](https://community.sophos.com/sandboxie/f/forum/119641/important-sandboxie-open-source-code-is-available-for-download) | Sophos Ltd. |\n| 9 April 2020 onwards - project fork | David Xanatos |\n\nLooking for older Sandboxie versions? Check the [version history](https://github.com/sandboxie-plus/sandboxie-old).\n\nSee the current [roadmap](https://www.wilderssecurity.com/threads/updated-sandboxie-plus-roadmap.456886/).\n\n## ğŸ“Œ Project support / sponsorship\n\n[<img align=\"left\" height=\"64\" width=\"64\" src=\"./.github/images/binja-love.png\">](https://binary.ninja/)\nThank you [Vector 35](https://vector35.com/) for providing a [Binary Ninja](https://binary.ninja/) license to help with reverse engineering.\n<br>\nBinary Ninja is a multi-platform interactive disassembler, decompiler, and binary analysis tool for reverse engineers, malware analysts, vulnerability researchers, and software developers.<br>\n<br>\n[<img align=\"left\" height=\"64\" width=\"64\" src=\"./.github/images/Icons8_logo.png\">](https://icons8.de/)Thank you [Icons8](https://icons8.de/) for providing icons for the project.\n<br>\n<br>\n<br>\n\n## ğŸ¤ Support the project\n\nIf you find Sandboxie useful, then feel free to contribute through our [Contribution guidelines](./CONTRIBUTING.md).\n\n## ğŸ“‘ Helpful Contributors\n\n- DavidBerdik - Maintainer of [Sandboxie Website Archive](https://github.com/Sandboxie-Website-Archive/sandboxie-website-archive.github.io)\n- Jackenmen - Maintainer of Chocolatey packages for Sandboxie ([support](https://github.com/Jackenmen/choco-auto/issues?q=is%3Aissue+Sandboxie))\n- vedantmgoyal9 - Maintainer of Winget Releaser for Sandboxie ([support](https://github.com/vedantmgoyal9/winget-releaser/issues?q=is%3Aissue+Sandboxie))\n- blap - Maintainer of [SandboxToys2](https://github.com/blap/SandboxToys2) addon\n- diversenok - Security analysis & PoCs / Security fixes\n- TechLord - Team-IRA / Reversing\n- hg421 - Security analysis & PoCs / Code reviews\n- hx1997 - Security analysis & PoC\n- mpheath - Author of Plus installer / Code fixes / Collaborator\n- offhub - Documentation additions / Code fixes / Qt5 patch and build script / Collaborator\n- LumitoLuma - Qt5 patch and build script\n- QZLin - Author of [sandboxie-docs](https://sandboxie-plus.github.io/sandboxie-docs/) theme\n- isaak654 - Templates / Documentation / Code fixes / Collaborator\n- typpos - UI additions / Documentation / Code fixes\n- Yeyixiao - Feature additions\n- Deezzir - Feature additions\n- wzxjohn - Code fixes, Documentation additions\n- okrc - Code fixes\n- Sapour - Code fixes\n- lmou523 - Code fixes\n- sredna - Code fixes for Classic installer\n- weihongx9315 - Code fix\n- marti4d - Code fix\n- jorgectf - CodeQL workflow\n- stephtr - CI / Certification\n- yfdyh000 - Localization support for Plus installer\n- Dyras - Templates additions\n- cricri-pingouin - UI fixes\n- Valinwolf - UI / Icons\n- daveout - UI / Icons\n- kokofixcomputers - Support member of the [Discord](https://discord.gg/S4tFu6Enne) channel\n- NewKidOnTheBlock - Changelog fixes\n- Naeemh1 - Documentation additions\n- APMichael - Templates additions\n- 1mm0rt41PC - Documentation additions\n- Luro223 - Documentation additions\n- lwcorp - Documentation additions\n- wilders-soccerfan - Documentation additions\n- LepordCat - Documentation additions\n- stdedos - Documentation additions\n- habatake - UI additions, Code fixes\n- Polyester6719 - Documentation additions\n\n## ğŸŒ Translators\n\n- czoins - Arabic\n- yuhao2348732, 0x391F, nkh0472, yfdyh000, gexgd0419, Zerorigin, UnnamedOrange, DevSplash, Becods, okrc, 4rt3mi5, sepcnt, fzxx, Vstory, GT-Stardust, habatake - Simplified Chinese\n- TragicLifeHu, Hulen, xiongsp, habatake - Traditional Chinese\n- RockyTDR - Dutch\n- clexanis, Mmoi-Fr, hippalectryon-0, Monsieur Pissou - French (provided by email)\n- bastik-1001, APMichael - German\n- timinoun - Hungarian (provided by email)\n- isaak654, DerivativeOfLog7 - Italian\n- takahiro-itou, lllIIIlll - Japanese\n- VenusGirl - Korean\n- divinity76 - Norwegian BokmÃ¥l\n- 7zip, AndrzejRafalowski - Polish ([provided separately](https://forum.xanasoft.com/threads/polish-translation.4/page-2))\n- JNylson - Portuguese and Brazilian Portuguese\n- lufog, marat2509 - Russian\n- LumitoLuma, sebadamus - Spanish\n- 1FF, Thatagata - Swedish (provided by email or pull request)\n- xorcan, fmbxnary, offhub - Turkish\n- SuperMaxusa, lufog - Ukrainian\n- GunGunGun - Vietnamese\n\nAll translators are encouraged to look at the [Localization notes and tips](https://git.io/J9G19) before sending a translation.\n\n## ğŸ“š Documentation Translators\n\n- Vstory, GT-Stardust, wzxjohn, SOLEADO20, habatake - Simplified Chinese\n\nAll documentation translators are encouraged to look at the [Multilingual Translation Contribution Guide](https://github.com/sandboxie-plus/sandboxie-docs/issues/175#issuecomment-2840258519) before sending a translation.\n",
      "stars_today": 17
    },
    {
      "id": 301244405,
      "name": "atuin",
      "full_name": "atuinsh/atuin",
      "description": "âœ¨ Magical shell history",
      "html_url": "https://github.com/atuinsh/atuin",
      "stars": 27915,
      "forks": 762,
      "language": "Rust",
      "topics": [
        "bash",
        "fish",
        "history",
        "rust",
        "shell",
        "zsh"
      ],
      "created_at": "2020-10-04T23:01:58Z",
      "updated_at": "2026-01-15T00:43:50Z",
      "pushed_at": "2026-01-12T23:49:31Z",
      "open_issues": 465,
      "owner": {
        "login": "atuinsh",
        "avatar_url": "https://avatars.githubusercontent.com/u/122059230?v=4"
      },
      "readme": "<p align=\"center\">\n <picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/atuinsh/atuin/assets/53315310/13216a1d-1ac0-4c99-b0eb-d88290fe0efd\">\n  <img alt=\"Text changing depending on mode. Light: 'So light!' Dark: 'So dark!'\" src=\"https://github.com/atuinsh/atuin/assets/53315310/08bc86d4-a781-4aaa-8d7e-478ae6bcd129\">\n</picture>\n</p>\n\n<p align=\"center\">\n<em>magical shell history</em>\n</p>\n\n<hr/>\n\n<p align=\"center\">\n  <a href=\"https://github.com/atuinsh/atuin/actions?query=workflow%3ARust\"><img src=\"https://img.shields.io/github/actions/workflow/status/atuinsh/atuin/rust.yml?style=flat-square\" /></a>\n  <a href=\"https://crates.io/crates/atuin\"><img src=\"https://img.shields.io/crates/v/atuin.svg?style=flat-square\" /></a>\n  <a href=\"https://crates.io/crates/atuin\"><img src=\"https://img.shields.io/crates/d/atuin.svg?style=flat-square\" /></a>\n  <a href=\"https://github.com/atuinsh/atuin/blob/main/LICENSE\"><img src=\"https://img.shields.io/crates/l/atuin.svg?style=flat-square\" /></a>\n  <a href=\"https://discord.gg/Fq8bJSKPHh\"><img src=\"https://img.shields.io/discord/954121165239115808\" /></a>\n  <a rel=\"me\" href=\"https://hachyderm.io/@atuin\"><img src=\"https://img.shields.io/mastodon/follow/109944632283122560?domain=https%3A%2F%2Fhachyderm.io&style=social\"/></a>\n  <a href=\"https://twitter.com/atuinsh\"><img src=\"https://img.shields.io/twitter/follow/atuinsh?style=social\" /></a>\n</p>\n\n\n[English] | [ç®€ä½“ä¸­æ–‡]\n\n\nAtuin replaces your existing shell history with a SQLite database, and records\nadditional context for your commands. Additionally, it provides optional and\n_fully encrypted_ synchronisation of your history between machines, via an Atuin\nserver.\n\n\n\n\n<p align=\"center\">\n  <img src=\"demo.gif\" alt=\"animated\" width=\"80%\" />\n</p>\n\n<p align=\"center\">\n<em>exit code, duration, time and command shown</em>\n</p>\n\n\n\n\n\nAs well as the search UI, it can do things like this:\n\n```\n# search for all successful `make` commands, recorded after 3pm yesterday\natuin search --exit 0 --after \"yesterday 3pm\" make\n```\n\nYou may use either the server I host, or host your own! Or just don't use sync\nat all. As all history sync is encrypted, I couldn't access your data even if\nI wanted to. And I **really** don't want to.\n\n## Features\n\n- rebind `ctrl-r` and `up` (configurable) to a full screen history search UI\n- store shell history in a sqlite database\n- back up and sync **encrypted** shell history\n- the same history across terminals, across sessions, and across machines\n- log exit code, cwd, hostname, session, command duration, etc\n- calculate statistics such as \"most used command\"\n- old history file is not replaced\n- quick-jump to previous items with <kbd>Alt-\\<num\\></kbd>\n- switch filter modes via ctrl-r; search history just from the current session, directory, or globally\n- enter to execute a command, tab to edit\n\n## Documentation\n\n- [Quickstart](#quickstart)\n- [Install](https://docs.atuin.sh/guide/installation/)\n- [Setting up sync](https://docs.atuin.sh/guide/sync/)\n- [Import history](https://docs.atuin.sh/guide/import/)\n- [Basic usage](https://docs.atuin.sh/guide/basic-usage/)\n## Supported Shells\n\n- zsh\n- bash\n- fish\n- nushell\n- xonsh\n\n## Community\n\n### Forum\n\nAtuin has a community forum, please ask here for help and support: https://forum.atuin.sh/\n\n### Discord\n\nAtuin also has a community Discord, available [here](https://discord.gg/jR3tfchVvW)\n\n# Quickstart\n\nThis will sign you up for the Atuin Cloud sync server. Everything is end-to-end encrypted, so your secrets are safe!\n\nRead more in the [docs](https://docs.atuin.sh) for an offline setup, self hosted server, and more.\n\n```\ncurl --proto '=https' --tlsv1.2 -LsSf https://setup.atuin.sh | sh\n\natuin register -u <USERNAME> -e <EMAIL>\natuin import auto\natuin sync\n```\n\nThen restart your shell!\n\n> [!NOTE]\n>\n> **For Bash users**: The above sets up `bash-preexec` for necessary hooks, but\n> `bash-preexec` has limitations.  For details, please see the\n> [Bash](https://docs.atuin.sh/guide/installation/#installing-the-shell-plugin)\n> section of the shell plugin documentation.\n\n# Security\n\nIf you find any security issues, we'd appreciate it if you could alert ellie@atuin.sh\n\n# Contributors\n\n<a href=\"https://github.com/atuinsh/atuin/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=atuinsh/atuin&max=300\" />\n</a>\n\nMade with [contrib.rocks](https://contrib.rocks).\n\n[English]: ./README.md\n[ç®€ä½“ä¸­æ–‡]: ./docs/zh-CN/README.md\n",
      "stars_today": 15
    },
    {
      "id": 299832860,
      "name": "livekit",
      "full_name": "livekit/livekit",
      "description": "End-to-end realtime stack for connecting humans and AI",
      "html_url": "https://github.com/livekit/livekit",
      "stars": 16517,
      "forks": 1674,
      "language": "Go",
      "topics": [
        "golang",
        "media-server",
        "sfu",
        "video",
        "voice",
        "voice-ai",
        "webrtc"
      ],
      "created_at": "2020-09-30T06:49:46Z",
      "updated_at": "2026-01-15T00:05:01Z",
      "pushed_at": "2026-01-14T14:49:00Z",
      "open_issues": 160,
      "owner": {
        "login": "livekit",
        "avatar_url": "https://avatars.githubusercontent.com/u/69438833?v=4"
      },
      "readme": "<!--BEGIN_BANNER_IMAGE-->\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"/.github/banner_dark.png\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"/.github/banner_light.png\">\n  <img style=\"width:100%;\" alt=\"The LiveKit icon, the name of the repository and some sample code in the background.\" src=\"https://raw.githubusercontent.com/livekit/livekit/main/.github/banner_light.png\">\n</picture>\n\n<!--END_BANNER_IMAGE-->\n\n# LiveKit: Real-time video, audio and data for developers\n\n[LiveKit](https://livekit.io) is an open source project that provides scalable, multi-user conferencing based on WebRTC.\nIt's designed to provide everything you need to build real-time video audio data capabilities in your applications.\n\nLiveKit's server is written in Go, using the awesome [Pion WebRTC](https://github.com/pion/webrtc) implementation.\n\n[![GitHub stars](https://img.shields.io/github/stars/livekit/livekit?style=social&label=Star&maxAge=2592000)](https://github.com/livekit/livekit/stargazers/)\n[![Slack community](https://img.shields.io/endpoint?url=https%3A%2F%2Flivekit.io%2Fbadges%2Fslack)](https://livekit.io/join-slack)\n[![Twitter Follow](https://img.shields.io/twitter/follow/livekit)](https://twitter.com/livekit)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/livekit/livekit)\n[![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/livekit/livekit)](https://github.com/livekit/livekit/releases/latest)\n[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/livekit/livekit/buildtest.yaml?branch=master)](https://github.com/livekit/livekit/actions/workflows/buildtest.yaml)\n[![License](https://img.shields.io/github/license/livekit/livekit)](https://github.com/livekit/livekit/blob/master/LICENSE)\n\n## Features\n\n-   Scalable, distributed WebRTC SFU (Selective Forwarding Unit)\n-   Modern, full-featured client SDKs\n-   Built for production, supports JWT authentication\n-   Robust networking and connectivity, UDP/TCP/TURN\n-   Easy to deploy: single binary, Docker or Kubernetes\n-   Advanced features including:\n    -   [speaker detection](https://docs.livekit.io/home/client/tracks/subscribe/#speaker-detection)\n    -   [simulcast](https://docs.livekit.io/home/client/tracks/publish/#video-simulcast)\n    -   [end-to-end optimizations](https://blog.livekit.io/livekit-one-dot-zero/)\n    -   [selective subscription](https://docs.livekit.io/home/client/tracks/subscribe/#selective-subscription)\n    -   [moderation APIs](https://docs.livekit.io/home/server/managing-participants/)\n    -   end-to-end encryption\n    -   SVC codecs (VP9, AV1)\n    -   [webhooks](https://docs.livekit.io/home/server/webhooks/)\n    -   [distributed and multi-region](https://docs.livekit.io/home/self-hosting/distributed/)\n\n## Documentation & Guides\n\nhttps://docs.livekit.io\n\n## Live Demos\n\n-   [LiveKit Meet](https://meet.livekit.io) ([source](https://github.com/livekit-examples/meet))\n-   [Spatial Audio](https://spatial-audio-demo.livekit.io/) ([source](https://github.com/livekit-examples/spatial-audio))\n-   Livestreaming from OBS Studio ([source](https://github.com/livekit-examples/livestream))\n-   [AI voice assistant using ChatGPT](https://livekit.io/kitt) ([source](https://github.com/livekit-examples/kitt))\n\n## Ecosystem\n\n-   [Agents](https://github.com/livekit/agents): build real-time multimodal AI applications with programmable backend participants\n-   [Egress](https://github.com/livekit/egress): record or multi-stream rooms and export individual tracks\n-   [Ingress](https://github.com/livekit/ingress): ingest streams from external sources like RTMP, WHIP, HLS, or OBS Studio\n\n## SDKs & Tools\n\n### Client SDKs\n\nClient SDKs enable your frontend to include interactive, multi-user experiences.\n\n<table>\n  <tr>\n    <th>Language</th>\n    <th>Repo</th>\n    <th>\n        <a href=\"https://docs.livekit.io/home/client/events/#declarative-ui\" target=\"_blank\" rel=\"noopener noreferrer\">Declarative UI</a>\n    </th>\n    <th>Links</th>\n  </tr>\n  <!-- BEGIN Template\n  <tr>\n    <td>Language</td>\n    <td>\n      <a href=\"\" target=\"_blank\" rel=\"noopener noreferrer\"></a>\n    </td>\n    <td></td>\n    <td></td>\n  </tr>\n  END -->\n  <!-- JavaScript -->\n  <tr>\n    <td>JavaScript (TypeScript)</td>\n    <td>\n      <a href=\"https://github.com/livekit/client-sdk-js\" target=\"_blank\" rel=\"noopener noreferrer\">client-sdk-js</a>\n    </td>\n    <td>\n      <a href=\"https://github.com/livekit/livekit-react\" target=\"_blank\" rel=\"noopener noreferrer\">React</a>\n    </td>\n    <td>\n      <a href=\"https://docs.livekit.io/client-sdk-js/\" target=\"_blank\" rel=\"noopener noreferrer\">docs</a>\n      |\n      <a href=\"https://github.com/livekit/client-sdk-js/tree/main/example\" target=\"_blank\" rel=\"noopener noreferrer\">JS example</a>\n      |\n      <a href=\"https://github.com/livekit/client-sdk-js/tree/main/example\" target=\"_blank\" rel=\"noopener noreferrer\">React example</a>\n    </td>\n  </tr>\n  <!-- Swift -->\n  <tr>\n    <td>Swift (iOS / MacOS)</td>\n    <td>\n      <a href=\"https://github.com/livekit/client-sdk-swift\" target=\"_blank\" rel=\"noopener noreferrer\">client-sdk-swift</a>\n    </td>\n    <td>Swift UI</td>\n    <td>\n      <a href=\"https://docs.livekit.io/client-sdk-swift/\" target=\"_blank\" rel=\"noopener noreferrer\">docs</a>\n      |\n      <a href=\"https://github.com/livekit/client-example-swift\" target=\"_blank\" rel=\"noopener noreferrer\">example</a>\n    </td>\n  </tr>\n  <!-- Kotlin -->\n  <tr>\n    <td>Kotlin (Android)</td>\n    <td>\n      <a href=\"https://github.com/livekit/client-sdk-android\" target=\"_blank\" rel=\"noopener noreferrer\">client-sdk-android</a>\n    </td>\n    <td>Compose</td>\n    <td>\n      <a href=\"https://docs.livekit.io/client-sdk-android/index.html\" target=\"_blank\" rel=\"noopener noreferrer\">docs</a>\n      |\n      <a href=\"https://github.com/livekit/client-sdk-android/tree/main/sample-app/src/main/java/io/livekit/android/sample\" target=\"_blank\" rel=\"noopener noreferrer\">example</a>\n      |\n      <a href=\"https://github.com/livekit/client-sdk-android/tree/main/sample-app-compose/src/main/java/io/livekit/android/composesample\" target=\"_blank\" rel=\"noopener noreferrer\">Compose example</a>\n    </td>\n  </tr>\n<!-- Flutter -->\n  <tr>\n    <td>Flutter (all platforms)</td>\n    <td>\n      <a href=\"https://github.com/livekit/client-sdk-flutter\" target=\"_blank\" rel=\"noopener noreferrer\">client-sdk-flutter</a>\n    </td>\n    <td>native</td>\n    <td>\n      <a href=\"https://docs.livekit.io/client-sdk-flutter/\" target=\"_blank\" rel=\"noopener noreferrer\">docs</a>\n      |\n      <a href=\"https://github.com/livekit/client-sdk-flutter/tree/main/example\" target=\"_blank\" rel=\"noopener noreferrer\">example</a>\n    </td>\n  </tr>\n  <!-- Unity -->\n  <tr>\n    <td>Unity WebGL</td>\n    <td>\n      <a href=\"https://github.com/livekit/client-sdk-unity-web\" target=\"_blank\" rel=\"noopener noreferrer\">client-sdk-unity-web</a>\n    </td>\n    <td></td>\n    <td>\n      <a href=\"https://livekit.github.io/client-sdk-unity-web/\" target=\"_blank\" rel=\"noopener noreferrer\">docs</a>\n    </td>\n  </tr>\n  <!-- React Native -->\n  <tr>\n    <td>React Native (beta)</td>\n    <td>\n      <a href=\"https://github.com/livekit/client-sdk-react-native\" target=\"_blank\" rel=\"noopener noreferrer\">client-sdk-react-native</a>\n    </td>\n    <td>native</td>\n    <td></td>\n  </tr>\n  <!-- Rust -->\n  <tr>\n    <td>Rust</td>\n    <td>\n      <a href=\"https://github.com/livekit/client-sdk-rust\" target=\"_blank\" rel=\"noopener noreferrer\">client-sdk-rust</a>\n    </td>\n    <td></td>\n    <td></td>\n  </tr>\n</table>\n\n### Server SDKs\n\nServer SDKs enable your backend to generate [access tokens](https://docs.livekit.io/home/get-started/authentication/),\ncall [server APIs](https://docs.livekit.io/reference/server/server-apis/), and\nreceive [webhooks](https://docs.livekit.io/home/server/webhooks/). In addition, the Go SDK includes client capabilities,\nenabling you to build automations that behave like end-users.\n\n| Language                | Repo                                                                                    | Docs                                                        |\n| :---------------------- | :-------------------------------------------------------------------------------------- | :---------------------------------------------------------- |\n| Go                      | [server-sdk-go](https://github.com/livekit/server-sdk-go)                               | [docs](https://pkg.go.dev/github.com/livekit/server-sdk-go) |\n| JavaScript (TypeScript) | [server-sdk-js](https://github.com/livekit/server-sdk-js)                               | [docs](https://docs.livekit.io/server-sdk-js/)              |\n| Ruby                    | [server-sdk-ruby](https://github.com/livekit/server-sdk-ruby)                           |                                                             |\n| Java (Kotlin)           | [server-sdk-kotlin](https://github.com/livekit/server-sdk-kotlin)                       |                                                             |\n| Python (community)      | [python-sdks](https://github.com/livekit/python-sdks)                                   |                                                             |\n| PHP (community)         | [agence104/livekit-server-sdk-php](https://github.com/agence104/livekit-server-sdk-php) |                                                             |\n\n### Tools\n\n-   [CLI](https://github.com/livekit/livekit-cli) - command line interface & load tester\n-   [Docker image](https://hub.docker.com/r/livekit/livekit-server)\n-   [Helm charts](https://github.com/livekit/livekit-helm)\n\n## Install\n\n> [!TIP]\n> We recommend installing [LiveKit CLI](https://github.com/livekit/livekit-cli) along with the server. It lets you access\n> server APIs, create tokens, and generate test traffic.\n\nThe following will install LiveKit's media server:\n\n### MacOS\n\n```shell\nbrew install livekit\n```\n\n### Linux\n\n```shell\ncurl -sSL https://get.livekit.io | bash\n```\n\n### Windows\n\nDownload the [latest release here](https://github.com/livekit/livekit/releases/latest)\n\n## Getting Started\n\n### Starting LiveKit\n\nStart LiveKit in development mode by running `livekit-server --dev`. It'll use a placeholder API key/secret pair.\n\n```\nAPI Key: devkey\nAPI Secret: secret\n```\n\nTo customize your setup for production, refer to our [deployment docs](https://docs.livekit.io/deploy/)\n\n### Creating access token\n\nA user connecting to a LiveKit room requires an [access token](https://docs.livekit.io/home/get-started/authentication/#creating-a-token). Access\ntokens (JWT) encode the user's identity and the room permissions they've been granted. You can generate a token with our\nCLI:\n\n```shell\nlk token create \\\n    --api-key devkey --api-secret secret \\\n    --join --room my-first-room --identity user1 \\\n    --valid-for 24h\n```\n\n### Test with example app\n\nHead over to our [example app](https://example.livekit.io) and enter a generated token to connect to your LiveKit\nserver. This app is built with our [React SDK](https://github.com/livekit/livekit-react).\n\nOnce connected, your video and audio are now being published to your new LiveKit instance!\n\n### Simulating a test publisher\n\n```shell\nlk room join \\\n    --url ws://localhost:7880 \\\n    --api-key devkey --api-secret secret \\\n    --identity bot-user1 \\\n    --publish-demo \\\n    my-first-room\n```\n\nThis command publishes a looped demo video to a room. Due to how the video clip was encoded (keyframes every 3s),\nthere's a slight delay before the browser has sufficient data to begin rendering frames. This is an artifact of the\nsimulation.\n\n## Deployment\n\n### Use LiveKit Cloud\n\nLiveKit Cloud is the fastest and most reliable way to run LiveKit. Every project gets free monthly bandwidth and\ntranscoding credits.\n\nSign up for [LiveKit Cloud](https://cloud.livekit.io/).\n\n### Self-host\n\nRead our [deployment docs](https://docs.livekit.io/deploy/) for more information.\n\n## Building from source\n\nPre-requisites:\n\n-   Go 1.23+ is installed\n-   GOPATH/bin is in your PATH\n\nThen run\n\n```shell\ngit clone https://github.com/livekit/livekit\ncd livekit\n./bootstrap.sh\nmage\n```\n\n## Contributing\n\nWe welcome your contributions toward improving LiveKit! Please join us\n[on Slack](http://livekit.io/join-slack) to discuss your ideas and/or PRs.\n\n## License\n\nLiveKit server is licensed under Apache License v2.0.\n\n<!--BEGIN_REPO_NAV-->\n<br/><table>\n<thead><tr><th colspan=\"2\">LiveKit Ecosystem</th></tr></thead>\n<tbody>\n<tr><td>LiveKit SDKs</td><td><a href=\"https://github.com/livekit/client-sdk-js\">Browser</a> Â· <a href=\"https://github.com/livekit/client-sdk-swift\">iOS/macOS/visionOS</a> Â· <a href=\"https://github.com/livekit/client-sdk-android\">Android</a> Â· <a href=\"https://github.com/livekit/client-sdk-flutter\">Flutter</a> Â· <a href=\"https://github.com/livekit/client-sdk-react-native\">React Native</a> Â· <a href=\"https://github.com/livekit/rust-sdks\">Rust</a> Â· <a href=\"https://github.com/livekit/node-sdks\">Node.js</a> Â· <a href=\"https://github.com/livekit/python-sdks\">Python</a> Â· <a href=\"https://github.com/livekit/client-sdk-unity\">Unity</a> Â· <a href=\"https://github.com/livekit/client-sdk-unity-web\">Unity (WebGL)</a> Â· <a href=\"https://github.com/livekit/client-sdk-esp32\">ESP32</a></td></tr><tr></tr>\n<tr><td>Server APIs</td><td><a href=\"https://github.com/livekit/node-sdks\">Node.js</a> Â· <a href=\"https://github.com/livekit/server-sdk-go\">Golang</a> Â· <a href=\"https://github.com/livekit/server-sdk-ruby\">Ruby</a> Â· <a href=\"https://github.com/livekit/server-sdk-kotlin\">Java/Kotlin</a> Â· <a href=\"https://github.com/livekit/python-sdks\">Python</a> Â· <a href=\"https://github.com/livekit/rust-sdks\">Rust</a> Â· <a href=\"https://github.com/agence104/livekit-server-sdk-php\">PHP (community)</a> Â· <a href=\"https://github.com/pabloFuente/livekit-server-sdk-dotnet\">.NET (community)</a></td></tr><tr></tr>\n<tr><td>UI Components</td><td><a href=\"https://github.com/livekit/components-js\">React</a> Â· <a href=\"https://github.com/livekit/components-android\">Android Compose</a> Â· <a href=\"https://github.com/livekit/components-swift\">SwiftUI</a> Â· <a href=\"https://github.com/livekit/components-flutter\">Flutter</a></td></tr><tr></tr>\n<tr><td>Agents Frameworks</td><td><a href=\"https://github.com/livekit/agents\">Python</a> Â· <a href=\"https://github.com/livekit/agents-js\">Node.js</a> Â· <a href=\"https://github.com/livekit/agent-playground\">Playground</a></td></tr><tr></tr>\n<tr><td>Services</td><td><b>LiveKit server</b> Â· <a href=\"https://github.com/livekit/egress\">Egress</a> Â· <a href=\"https://github.com/livekit/ingress\">Ingress</a> Â· <a href=\"https://github.com/livekit/sip\">SIP</a></td></tr><tr></tr>\n<tr><td>Resources</td><td><a href=\"https://docs.livekit.io\">Docs</a> Â· <a href=\"https://github.com/livekit-examples\">Example apps</a> Â· <a href=\"https://livekit.io/cloud\">Cloud</a> Â· <a href=\"https://docs.livekit.io/home/self-hosting/deployment\">Self-hosting</a> Â· <a href=\"https://github.com/livekit/livekit-cli\">CLI</a></td></tr>\n</tbody>\n</table>\n<!--END_REPO_NAV-->\n",
      "stars_today": 14
    },
    {
      "id": 403020531,
      "name": "colima",
      "full_name": "abiosoft/colima",
      "description": "Container runtimes on macOS (and Linux) with minimal setup",
      "html_url": "https://github.com/abiosoft/colima",
      "stars": 26284,
      "forks": 514,
      "language": "Go",
      "topics": [
        "containerd",
        "containerd-compose",
        "containers",
        "docker",
        "docker-compose",
        "incus",
        "k3s",
        "k8s",
        "kubernetes",
        "lima",
        "macos",
        "nerdctl"
      ],
      "created_at": "2021-09-04T09:52:22Z",
      "updated_at": "2026-01-14T22:59:48Z",
      "pushed_at": "2026-01-11T08:00:50Z",
      "open_issues": 374,
      "owner": {
        "login": "abiosoft",
        "avatar_url": "https://avatars.githubusercontent.com/u/240448?v=4"
      },
      "readme": "![colima-logo](colima.png)\n\n## Colima - container runtimes on macOS (and Linux) with minimal setup.\n\n[![Go](https://github.com/abiosoft/colima/actions/workflows/go.yml/badge.svg)](https://github.com/abiosoft/colima/actions/workflows/go.yml)\n[![Integration](https://github.com/abiosoft/colima/actions/workflows/integration.yml/badge.svg)](https://github.com/abiosoft/colima/actions/workflows/integration.yml)\n[![Go Report Card](https://goreportcard.com/badge/github.com/abiosoft/colima)](https://goreportcard.com/report/github.com/abiosoft/colima)\n\n![Demonstration](colima.gif)\n\n## Features\n\nSupport for Intel and Apple Silicon macOS, and Linux\n\n- Simple CLI interface with sensible defaults\n- Automatic Port Forwarding\n- Volume mounts\n- Multiple instances\n- Support for multiple container runtimes\n  - [Docker](https://docker.com) (with optional Kubernetes)\n  - [Containerd](https://containerd.io) (with optional Kubernetes)\n  - [Incus](https://linuxcontainers.org/incus) (containers and virtual machines)\n\n## Getting Started\n\n### Installation\n\nColima is available on Homebrew, MacPorts, Nix and [mise](http://github.com/jdx/mise). Check [here](docs/INSTALL.md) for other installation options.\n\n```\n# Homebrew\nbrew install colima\n\n# MacPorts\nsudo port install colima\n\n# Nix\nnix-env -iA nixpkgs.colima\n\n# Mise\nmise use -g colima@latest\n\n```\n\nOr stay on the bleeding edge (only Homebrew)\n\n```\nbrew install --HEAD colima\n```\n\n### Upgrading\n\nIf upgrading from v0.5.6 or lower, it is required to start afresh by deleting existing instance.\n\n```sh\ncolima delete # delete existing instance\ncolima start\n```\n\n## Usage\n\nStart Colima with defaults\n\n```\ncolima start\n```\n\nFor more usage options\n\n```\ncolima --help\ncolima start --help\n```\n\nOr use a config file\n\n```\ncolima start --edit\n```\n\n## Using Templates\nWhen you run the `colima template` command, Colima opens the default configuration in a temporary file using your editor (VS Code by default, if installed).\n\nFor example, you might see something like:\n```sh\n/var/folders/hm/xmq4vxs13dl2hx2jyct65r080000gn/T/colima-2758922589.yaml\n\n```\nYou can edit this temporary file as needed. Once you save and close the file in the editor, Colima automatically overwrites the default template config located at:\n```sh\n~/.colima/_templates/default.yaml\n\n```\nTo see more options for working with templates, run:\n```\ncolima template --help\n\n```\n\n## Runtimes\n\nOn initial startup, Colima initiates with a user specified runtime that defaults to Docker.\n\n### Docker\n\nDocker client is required for Docker runtime. Installable with brew `brew install docker`.\n\nYou can use the `docker` client on macOS after `colima start` with no additional setup.\n\n### Containerd\n\n`colima start --runtime containerd` starts and setup Containerd. You can use `colima nerdctl` to interact with\nContainerd using [nerdctl](https://github.com/containerd/nerdctl).\n\nIt is recommended to run `colima nerdctl install` to install `nerdctl` alias script in $PATH.\n\n### Kubernetes\n\nkubectl is required for Kubernetes. Installable with `brew install kubectl`.\n\nTo enable Kubernetes, start Colima with `--kubernetes` flag.\n\n```\ncolima start --kubernetes\n```\n\n#### Interacting with Image Registry\n\nFor Docker runtime, images built or pulled with Docker are accessible to Kubernetes.\n\nFor Containerd runtime, images built or pulled in the `k8s.io` namespace are accessible to Kubernetes.\n\n### Incus\n\n<small>**Requires v0.7.0**</small>\n\n\nIncus client is required for Incus runtime. Installable with brew `brew install incus`.\n\n`colima start --runtime incus` starts and setup Incus.\n\nYou can use the `incus` client on macOS after `colima start` with no additional setup.\n\n**Note:** Running virtual machines on Incus is only supported on m3 or newer Apple Silicon devices.\n\n### None\n\n<small>**Requires v0.7.0**</small>\n\nColima can also be utilised solely as a headless virtual machine manager by specifying `none` runtime.\n\n\n### Customizing the VM\n\nThe default VM created by Colima has 2 CPUs, 2GiB memory and 100GiB storage.\n\nThe VM can be customized either by passing additional flags to `colima start`.\ne.g. `--cpu`, `--memory`, `--disk`, `--runtime`.\nOr by editing the config file with `colima start --edit`.\n\n**NOTE**: ~~disk size cannot be changed after the VM is created.~~ From v0.5.3, disk size can be increased.\n\n#### Customization Examples\n\n- create VM with 1CPU, 2GiB memory and 10GiB storage.\n\n  ```\n  colima start --cpu 1 --memory 2 --disk 10\n  ```\n\n- modify an existing VM to 4CPUs and 8GiB memory.\n\n  ```\n  colima stop\n  colima start --cpu 4 --memory 8\n  ```\n\n- create VM with Rosetta 2 emulation. Requires v0.5.3 and macOS >= 13 (Ventura) on Apple Silicon.\n\n  ```\n  colima start --vm-type=vz --vz-rosetta\n  ```\n\n## Project Goal\n\nTo provide container runtimes on macOS with minimal setup.\n\n## What is with the name?\n\nColima means Containers on [Lima](https://github.com/lima-vm/lima).\n\nSince Lima is aka Linux Machines. By transitivity, Colima can also mean Containers on Linux Machines.\n\n## And the Logo?\n\nThe logo was contributed by [Daniel Hodvogner](https://github.com/dhodvogner). Check [this issue](https://github.com/abiosoft/colima/issues/781) for more.\n\n## Troubleshooting and FAQs\n\nCheck [here](docs/FAQ.md) for Frequently Asked Questions.\n\n## How to Contribute?\n\nCheck [here](docs/CONTRIBUTE.md) for the instructions on contributing to the project.\n\n## Community\n- [GitHub Discussions](https://github.com/abiosoft/colima/discussions)\n- [GitHub Issues](https://github.com/abiosoft/colima/issues)\n- `#colima` channel in the CNCF Slack\n  - New account: <https://slack.cncf.io/>\n  - Login: <https://cloud-native.slack.com/>\n\n## Help Wanted\n\n- Documentation and project website\n\n## License\n\nMIT\n\n\n## Sponsoring the Project\n\nIf you (or your company) are benefiting from the project and would like to support the contributors, kindly sponsor.\n\n- [Github Sponsors](https://github.com/sponsors/abiosoft)\n- [Buy me a coffee](https://www.buymeacoffee.com/abiosoft)\n- [Patreon](https://patreon.com/colima)\n\n---\n\n[<img src=\"https://uploads-ssl.webflow.com/5ac3c046c82724970fc60918/5c019d917bba312af7553b49_MacStadium-developerlogo.png\" style=\"max-height: 150px\"/>](https://macstadium.com)\n\n\n",
      "stars_today": 14
    },
    {
      "id": 949523404,
      "name": "cursor-talk-to-figma-mcp",
      "full_name": "grab/cursor-talk-to-figma-mcp",
      "description": "TalkToFigma: MCP integration between Cursor and Figma, allowing Cursor Agentic AI to communicate with Figma for reading designs and modifying them programmatically.",
      "html_url": "https://github.com/grab/cursor-talk-to-figma-mcp",
      "stars": 5990,
      "forks": 635,
      "language": "JavaScript",
      "topics": [
        "agent",
        "agentic",
        "agentic-ai",
        "ai",
        "ai-agents",
        "automation",
        "cursor",
        "design",
        "figma",
        "generative-ai",
        "llm",
        "llms",
        "mcp",
        "model-context-protocol"
      ],
      "created_at": "2025-03-16T16:45:37Z",
      "updated_at": "2026-01-15T01:03:03Z",
      "pushed_at": "2025-11-03T01:00:00Z",
      "open_issues": 72,
      "owner": {
        "login": "grab",
        "avatar_url": "https://avatars.githubusercontent.com/u/17284363?v=4"
      },
      "readme": "# Cursor Talk to Figma MCP\n\nThis project implements a Model Context Protocol (MCP) integration between Cursor AI and Figma, allowing Cursor to communicate with Figma for reading designs and modifying them programmatically.\n\nhttps://github.com/user-attachments/assets/129a14d2-ed73-470f-9a4c-2240b2a4885c\n\n## Project Structure\n\n- `src/talk_to_figma_mcp/` - TypeScript MCP server for Figma integration\n- `src/cursor_mcp_plugin/` - Figma plugin for communicating with Cursor\n- `src/socket.ts` - WebSocket server that facilitates communication between the MCP server and Figma plugin\n\n## Get Started\n\n1. Install Bun if you haven't already:\n\n```bash\ncurl -fsSL https://bun.sh/install | bash\n```\n\n2. Run setup, this will also install MCP in your Cursor's active project\n\n```bash\nbun setup\n```\n\n3. Start the Websocket server\n\n```bash\nbun socket\n```\n\n4. **NEW** Install Figma plugin from [Figma community page](https://www.figma.com/community/plugin/1485687494525374295/cursor-talk-to-figma-mcp-plugin) or [install locally](#figma-plugin)\n\n## Quick Video Tutorial\n\n[Video Link](https://www.linkedin.com/posts/sonnylazuardi_just-wanted-to-share-my-latest-experiment-activity-7307821553654657024-yrh8)\n\n## Design Automation Example\n\n**Bulk text content replacement**\n\nThanks to [@dusskapark](https://github.com/dusskapark) for contributing the bulk text replacement feature. Here is the [demo video](https://www.youtube.com/watch?v=j05gGT3xfCs).\n\n**Instance Override Propagation**\nAnother contribution from [@dusskapark](https://github.com/dusskapark)\nPropagate component instance overrides from a source instance to multiple target instances with a single command. This feature dramatically reduces repetitive design work when working with component instances that need similar customizations. Check out our [demo video](https://youtu.be/uvuT8LByroI).\n\n## Development Setup\n\nTo develop, update your mcp config to direct to your local directory.\n\n```json\n{\n  \"mcpServers\": {\n    \"TalkToFigma\": {\n      \"command\": \"bun\",\n      \"args\": [\"/path-to-repo/src/talk_to_figma_mcp/server.ts\"]\n    }\n  }\n}\n```\n\n## Manual Setup and Installation\n\n### MCP Server: Integration with Cursor\n\nAdd the server to your Cursor MCP configuration in `~/.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"TalkToFigma\": {\n      \"command\": \"bunx\",\n      \"args\": [\"cursor-talk-to-figma-mcp@latest\"]\n    }\n  }\n}\n```\n\n### WebSocket Server\n\nStart the WebSocket server:\n\n```bash\nbun socket\n```\n\n### Figma Plugin\n\n1. In Figma, go to Plugins > Development > New Plugin\n2. Choose \"Link existing plugin\"\n3. Select the `src/cursor_mcp_plugin/manifest.json` file\n4. The plugin should now be available in your Figma development plugins\n\n## Windows + WSL Guide\n\n1. Install bun via powershell\n\n```bash\npowershell -c \"irm bun.sh/install.ps1|iex\"\n```\n\n2. Uncomment the hostname `0.0.0.0` in `src/socket.ts`\n\n```typescript\n// uncomment this to allow connections in windows wsl\nhostname: \"0.0.0.0\",\n```\n\n3. Start the websocket\n\n```bash\nbun socket\n```\n\n## Usage\n\n1. Start the WebSocket server\n2. Install the MCP server in Cursor\n3. Open Figma and run the Cursor MCP Plugin\n4. Connect the plugin to the WebSocket server by joining a channel using `join_channel`\n5. Use Cursor to communicate with Figma using the MCP tools\n\n## MCP Tools\n\nThe MCP server provides the following tools for interacting with Figma:\n\n### Document & Selection\n\n- `get_document_info` - Get information about the current Figma document\n- `get_selection` - Get information about the current selection\n- `read_my_design` - Get detailed node information about the current selection without parameters\n- `get_node_info` - Get detailed information about a specific node\n- `get_nodes_info` - Get detailed information about multiple nodes by providing an array of node IDs\n- `set_focus` - Set focus on a specific node by selecting it and scrolling viewport to it\n- `set_selections` - Set selection to multiple nodes and scroll viewport to show them\n\n### Annotations\n\n- `get_annotations` - Get all annotations in the current document or specific node\n- `set_annotation` - Create or update an annotation with markdown support\n- `set_multiple_annotations` - Batch create/update multiple annotations efficiently\n- `scan_nodes_by_types` - Scan for nodes with specific types (useful for finding annotation targets)\n\n### Prototyping & Connections\n\n- `get_reactions` - Get all prototype reactions from nodes with visual highlight animation\n- `set_default_connector` - Set a copied FigJam connector as the default connector style for creating connections (must be set before creating connections)\n- `create_connections` - Create FigJam connector lines between nodes, based on prototype flows or custom mapping\n\n### Creating Elements\n\n- `create_rectangle` - Create a new rectangle with position, size, and optional name\n- `create_frame` - Create a new frame with position, size, and optional name\n- `create_text` - Create a new text node with customizable font properties\n\n### Modifying text content\n\n- `scan_text_nodes` - Scan text nodes with intelligent chunking for large designs\n- `set_text_content` - Set the text content of a single text node\n- `set_multiple_text_contents` - Batch update multiple text nodes efficiently\n\n### Auto Layout & Spacing\n\n- `set_layout_mode` - Set the layout mode and wrap behavior of a frame (NONE, HORIZONTAL, VERTICAL)\n- `set_padding` - Set padding values for an auto-layout frame (top, right, bottom, left)\n- `set_axis_align` - Set primary and counter axis alignment for auto-layout frames\n- `set_layout_sizing` - Set horizontal and vertical sizing modes for auto-layout frames (FIXED, HUG, FILL)\n- `set_item_spacing` - Set distance between children in an auto-layout frame\n\n### Styling\n\n- `set_fill_color` - Set the fill color of a node (RGBA)\n- `set_stroke_color` - Set the stroke color and weight of a node\n- `set_corner_radius` - Set the corner radius of a node with optional per-corner control\n\n### Layout & Organization\n\n- `move_node` - Move a node to a new position\n- `resize_node` - Resize a node with new dimensions\n- `delete_node` - Delete a node\n- `delete_multiple_nodes` - Delete multiple nodes at once efficiently\n- `clone_node` - Create a copy of an existing node with optional position offset\n\n### Components & Styles\n\n- `get_styles` - Get information about local styles\n- `get_local_components` - Get information about local components\n- `create_component_instance` - Create an instance of a component\n- `get_instance_overrides` - Extract override properties from a selected component instance\n- `set_instance_overrides` - Apply extracted overrides to target instances\n\n### Export & Advanced\n\n- `export_node_as_image` - Export a node as an image (PNG, JPG, SVG, or PDF) - limited support on image currently returning base64 as text\n\n### Connection Management\n\n- `join_channel` - Join a specific channel to communicate with Figma\n\n### MCP Prompts\n\nThe MCP server includes several helper prompts to guide you through complex design tasks:\n\n- `design_strategy` - Best practices for working with Figma designs\n- `read_design_strategy` - Best practices for reading Figma designs\n- `text_replacement_strategy` - Systematic approach for replacing text in Figma designs\n- `annotation_conversion_strategy` - Strategy for converting manual annotations to Figma's native annotations\n- `swap_overrides_instances` - Strategy for transferring overrides between component instances in Figma\n- `reaction_to_connector_strategy` - Strategy for converting Figma prototype reactions to connector lines using the output of 'get_reactions', and guiding the use 'create_connections' in sequence\n\n## Development\n\n### Building the Figma Plugin\n\n1. Navigate to the Figma plugin directory:\n\n   ```\n   cd src/cursor_mcp_plugin\n   ```\n\n2. Edit code.js and ui.html\n\n## Best Practices\n\nWhen working with the Figma MCP:\n\n1. Always join a channel before sending commands\n2. Get document overview using `get_document_info` first\n3. Check current selection with `get_selection` before modifications\n4. Use appropriate creation tools based on needs:\n   - `create_frame` for containers\n   - `create_rectangle` for basic shapes\n   - `create_text` for text elements\n5. Verify changes using `get_node_info`\n6. Use component instances when possible for consistency\n7. Handle errors appropriately as all commands can throw exceptions\n8. For large designs:\n   - Use chunking parameters in `scan_text_nodes`\n   - Monitor progress through WebSocket updates\n   - Implement appropriate error handling\n9. For text operations:\n   - Use batch operations when possible\n   - Consider structural relationships\n   - Verify changes with targeted exports\n10. For converting legacy annotations:\n    - Scan text nodes to identify numbered markers and descriptions\n    - Use `scan_nodes_by_types` to find UI elements that annotations refer to\n    - Match markers with their target elements using path, name, or proximity\n    - Categorize annotations appropriately with `get_annotations`\n    - Create native annotations with `set_multiple_annotations` in batches\n    - Verify all annotations are properly linked to their targets\n    - Delete legacy annotation nodes after successful conversion\n11. Visualize prototype noodles as FigJam connectors:\n\n- Use `get_reactions` to extract prototype flows,\n- set a default connector with `set_default_connector`,\n- and generate connector lines with `create_connections` for clear visual flow mapping.\n\n## License\n\nMIT\n",
      "stars_today": 14
    },
    {
      "id": 7634677,
      "name": "openssl",
      "full_name": "openssl/openssl",
      "description": "TLS/SSL and crypto library",
      "html_url": "https://github.com/openssl/openssl",
      "stars": 29383,
      "forks": 10993,
      "language": "C",
      "topics": [
        "cryptography",
        "decryption",
        "encryption",
        "openssl",
        "ssl",
        "tls"
      ],
      "created_at": "2013-01-15T22:34:48Z",
      "updated_at": "2026-01-14T20:37:27Z",
      "pushed_at": "2026-01-14T10:30:13Z",
      "open_issues": 1700,
      "owner": {
        "login": "openssl",
        "avatar_url": "https://avatars.githubusercontent.com/u/3279138?v=4"
      },
      "readme": "Welcome to the OpenSSL Project\n==============================\n\n[![openssl logo]][www.openssl.org]\n\n[![github actions ci badge]][github actions ci]\n[![Nightly OS Zoo ci badge](https://github.com/openssl/openssl/actions/workflows/os-zoo.yml/badge.svg)](https://github.com/openssl/openssl/actions/workflows/os-zoo.yml)\n[![Provider Compatibility](https://github.com/openssl/openssl/actions/workflows/provider-compatibility.yml/badge.svg)](https://github.com/openssl/openssl/actions/workflows/provider-compatibility.yml)\n[![Quic Interop](https://github.com/openssl/openssl/actions/workflows/run_quic_interop.yml/badge.svg)](https://github.com/openssl/openssl/actions/workflows/run_quic_interop.yml)\n[![Daily checks](https://github.com/openssl/openssl/actions/workflows/run-checker-daily.yml/badge.svg)](https://github.com/openssl/openssl/actions/workflows/run-checker-daily.yml)\n[![LFX Health Score](https://insights.linuxfoundation.org/api/badge/health-score?project=openssl)](https://insights.linuxfoundation.org/project/openssl)\n\nOpenSSL is a robust, commercial-grade, full-featured Open Source Toolkit\nfor the Transport Layer Security (TLS, formerly SSL), Datagram TLS (DTLS), and QUIC protocols.\n\nThe protocol implementations are based on a full-strength general purpose\ncryptographic library, which can also be used stand-alone. Also included is a\ncryptographic module validated to conform with FIPS standards.\n\nOpenSSL is descended from the SSLeay library developed by Eric A. Young\nand Tim J. Hudson.\n\nThe official Home Page of the OpenSSL Project is [www.openssl.org].\n\nTable of Contents\n=================\n\n - [Overview](#overview)\n - [Download](#download)\n - [Build and Install](#build-and-install)\n - [Documentation](#documentation)\n - [License](#license)\n - [Support](#support)\n - [Contributing](#contributing)\n - [Legalities](#legalities)\n\nOverview\n========\n\nThe OpenSSL toolkit includes:\n\n- **libssl**\n  an implementation of all TLS protocol versions up to TLSv1.3 ([RFC 8446]),\n  DTLS protocol versions up to DTLSv1.2 ([RFC 6347]) and\n  the QUIC version 1 protocol ([RFC 9000]).\n\n- **libcrypto**\n  a full-strength general purpose cryptographic library. It constitutes the\n  basis of the TLS implementation, but can also be used independently.\n\n- **openssl**\n  the OpenSSL command line tool, a swiss army knife for cryptographic tasks,\n  testing and analyzing. It can be used for\n  - creation of key parameters\n  - creation of X.509 certificates, CSRs and CRLs\n  - calculation of message digests\n  - encryption and decryption\n  - SSL/TLS/DTLS and client and server tests\n  - QUIC client tests\n  - handling of S/MIME signed or encrypted mail\n  - and more...\n\nDownload\n========\n\nFor Production Use\n------------------\n\nSource code tarballs of the official releases can be downloaded from\n[openssl-library.org/source/](https://openssl-library.org/source/).\nThe OpenSSL project does not distribute the toolkit in binary form.\n\nHowever, for a large variety of operating systems precompiled versions\nof the OpenSSL toolkit are available. In particular, on Linux and other\nUnix operating systems, it is normally recommended to link against the\nprecompiled shared libraries provided by the distributor or vendor.\n\nWe also maintain a list of third parties that produce OpenSSL binaries for\nvarious Operating Systems (including Windows) on the [Binaries] page on our\nwiki.\n\nFor Testing and Development\n---------------------------\n\nAlthough testing and development could in theory also be done using\nthe source tarballs, having a local copy of the git repository with\nthe entire project history gives you much more insight into the\ncode base.\n\nThe main OpenSSL Git repository is private.\nThere is a public GitHub mirror of it at [github.com/openssl/openssl],\nwhich is updated automatically from the former on every commit.\n\nA local copy of the Git repository can be obtained by cloning it from\nthe GitHub mirror using\n\n    git clone https://github.com/openssl/openssl.git\n\nIf you intend to contribute to OpenSSL, either to fix bugs or contribute\nnew features, you need to fork the GitHub mirror and clone your public fork\ninstead.\n\n    git clone https://github.com/yourname/openssl.git\n\nThis is necessary because all development of OpenSSL nowadays is done via\nGitHub pull requests. For more details, see [Contributing](#contributing).\n\nBuild and Install\n=================\n\nAfter obtaining the Source, have a look at the [INSTALL](INSTALL.md) file for\ndetailed instructions about building and installing OpenSSL. For some\nplatforms, the installation instructions are amended by a platform specific\ndocument.\n\n * [Notes for UNIX-like platforms](NOTES-UNIX.md)\n * [Notes for Android platforms](NOTES-ANDROID.md)\n * [Notes for Windows platforms](NOTES-WINDOWS.md)\n * [Notes for the DOS platform with DJGPP](NOTES-DJGPP.md)\n * [Notes for the OpenVMS platform](NOTES-VMS.md)\n * [Notes on Perl](NOTES-PERL.md)\n * [Notes on Valgrind](NOTES-VALGRIND.md)\n\nSpecific notes on upgrading to OpenSSL 3.x from previous versions can be found\nin the [ossl-guide-migration(7ossl)] manual page.\n\nDocumentation\n=============\n\nREADME Files\n------------\n\nThere are some README.md files in the top level of the source distribution\ncontaining additional information on specific topics.\n\n * [Information about the OpenSSL QUIC protocol implementation](README-QUIC.md)\n * [Information about the OpenSSL Provider architecture](README-PROVIDERS.md)\n * [Information about using the OpenSSL FIPS validated module](README-FIPS.md)\n\nThe OpenSSL Guide\n-----------------\n\nThere are some tutorial and introductory pages on some important OpenSSL topics\nwithin the [OpenSSL Guide].\n\nManual Pages\n------------\n\nThe manual pages for the master branch and all current stable releases are\navailable online.\n\n- [OpenSSL master](https://docs.openssl.org/master/)\n- [OpenSSL 3.5](https://docs.openssl.org/3.5/)\n- [OpenSSL 3.4](https://docs.openssl.org/3.4/)\n- [OpenSSL 3.3](https://docs.openssl.org/3.3/)\n- [OpenSSL 3.2](https://docs.openssl.org/3.2/)\n- [OpenSSL 3.0](https://docs.openssl.org/3.0/)\n\nDemos\n-----\n\nThere are numerous source code demos for using various OpenSSL capabilities in the\n[demos subfolder](./demos).\n\nWiki\n----\n\nThere is a [GitHub Wiki] which is currently not very active.\n\nLicense\n=======\n\nOpenSSL is licensed under the Apache License 2.0, which means that\nyou are free to get and use it for commercial and non-commercial\npurposes as long as you fulfill its conditions.\n\nSee the [LICENSE.txt](LICENSE.txt) file for more details.\n\nSupport\n=======\n\nThere are various ways to get in touch. The correct channel depends on\nyour requirement. See the [SUPPORT](SUPPORT.md) file for more details.\n\nContributing\n============\n\nIf you are interested and willing to contribute to the OpenSSL project,\nplease take a look at the [CONTRIBUTING](CONTRIBUTING.md) file.\n\nLegalities\n==========\n\nA number of nations restrict the use or export of cryptography. If you are\npotentially subject to such restrictions, you should seek legal advice before\nattempting to develop or distribute cryptographic code.\n\nCopyright\n=========\n\nCopyright (c) 1998-2025 The OpenSSL Project Authors\n\nCopyright (c) 1995-1998 Eric A. Young, Tim J. Hudson\n\nAll rights reserved.\n\n<!-- Links  -->\n\n[www.openssl.org]:\n    <https://www.openssl.org>\n    \"OpenSSL Homepage\"\n\n[github.com/openssl/openssl]:\n    <https://github.com/openssl/openssl>\n    \"OpenSSL GitHub Mirror\"\n\n[GitHub Wiki]:\n    <https://github.com/openssl/openssl/wiki>\n    \"OpenSSL Wiki\"\n\n[ossl-guide-migration(7ossl)]:\n    <https://docs.openssl.org/master/man7/ossl-guide-migration>\n    \"OpenSSL Migration Guide\"\n\n[RFC 8446]:\n     <https://tools.ietf.org/html/rfc8446>\n\n[RFC 6347]:\n     <https://tools.ietf.org/html/rfc6347>\n\n[RFC 9000]:\n     <https://tools.ietf.org/html/rfc9000>\n\n[Binaries]:\n    <https://github.com/openssl/openssl/wiki/Binaries>\n    \"List of third party OpenSSL binaries\"\n\n[OpenSSL Guide]:\n    <https://docs.openssl.org/master/man7/ossl-guide-introduction>\n    \"An introduction to OpenSSL\"\n\n<!-- Logos and Badges -->\n\n[openssl logo]:\n    doc/images/openssl.svg\n    \"OpenSSL Logo\"\n\n[github actions ci badge]:\n    <https://github.com/openssl/openssl/workflows/GitHub%20CI/badge.svg>\n    \"GitHub Actions CI Status\"\n\n[github actions ci]:\n    <https://github.com/openssl/openssl/actions/workflows/ci.yml>\n    \"GitHub Actions CI\"\n\n[appveyor badge]:\n    <https://ci.appveyor.com/api/projects/status/8e10o7xfrg73v98f/branch/master?svg=true>\n    \"AppVeyor Build Status\"\n\n[appveyor jobs]:\n    <https://ci.appveyor.com/project/openssl/openssl/branch/master>\n    \"AppVeyor Jobs\"\n",
      "stars_today": 11
    },
    {
      "id": 149026292,
      "name": "cube",
      "full_name": "cube-js/cube",
      "description": "ğŸ“Š Cube Core is open-source semantic layer for AI, BI and embedded analytics",
      "html_url": "https://github.com/cube-js/cube",
      "stars": 19320,
      "forks": 1940,
      "language": "Rust",
      "topics": [
        "agentic-analytics",
        "agents",
        "ai",
        "analytics",
        "bi",
        "bigquery",
        "business-intelligence",
        "businessintelligence",
        "conversational-analytics",
        "cube",
        "databricks",
        "embedded-analytics",
        "headless-bi",
        "mysql",
        "postgresql",
        "rust",
        "semantic-layer",
        "snowflake",
        "sql"
      ],
      "created_at": "2018-09-16T18:58:46Z",
      "updated_at": "2026-01-14T14:36:53Z",
      "pushed_at": "2026-01-14T20:14:11Z",
      "open_issues": 865,
      "owner": {
        "login": "cube-js",
        "avatar_url": "https://avatars.githubusercontent.com/u/52467369?v=4"
      },
      "readme": "![]()\n<p align=\"center\">\n  <a href=\"https://cube.dev?ref=github-readme\"><img src=\"https://raw.githubusercontent.com/cube-js/cube/master/docs/content/cube-core-logo.png\" alt=\"Cube Core â€” Open-Source Semantic Layer\" width=\"300px\"></a>\n</p>\n<br/>\n\n[Website](https://cube.dev?ref=github-readme) â€¢ [Docs](https://cube.dev/docs?ref=github-readme) â€¢ [Examples](https://cube.dev/docs/examples?ref=github-readme) â€¢ [Blog](https://cube.dev/blog?ref=github-readme) â€¢ [Slack](https://slack.cube.dev?ref=github-readme) â€¢ [X](https://twitter.com/the_cube_dev)\n\n[![npm version](https://badge.fury.io/js/%40cubejs-backend%2Fserver.svg)](https://badge.fury.io/js/%40cubejs-backend%2Fserver)\n[![GitHub Actions](https://github.com/cube-js/cube/workflows/Build/badge.svg)](https://github.com/cube-js/cube/actions?query=workflow%3ABuild+branch%3Amaster)\n[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Fcube-js%2Fcube.js.svg?type=shield)](https://app.fossa.io/projects/git%2Bgithub.com%2Fcube-js%2Fcube.js?ref=badge_shield)\n\n__Cube Core is an open-source semantic layer.__ Cube Core can be used to build embedded analytics in your applications, create your own business intelligence tool or provide context about data to AI agents. Cube Core is headless and comes with multiple APIs for embedded analytics and BI: REST, GraphQL, and SQL.\n\nIf you are looking for a fully integrated platform, check out [Cube](https://cube.dev), a modern AI-first business intelligence platform. We use Cube Core to power it.\n\n<img\n  src=\"https://lgo0ecceic.ucarecd.net/418db1f9-7597-4e00-8c10-eba19fcac20f/\"\n  style=\"border: none\"\n  width=\"100%\"\n/>\n\n<p align=\"center\">\n  <i>Learn more about connecting Cube to <a href=\"https://cube.dev/docs/config/databases?ref=github-readme\" target=\"_blank\">data sources</a> and <a href=\"https://cube.dev/docs/config/downstream?ref=github-readme\" target=\"_blank\">analytics & visualization tools</a>.</i>\n</p>\n\nCube Core was designed to work with all SQL data sources, including cloud data warehouses like Snowflake, Databricks, and BigQuery; query engines like Presto and Amazon Athena; and application databases like Postgres. Cube Core has a built-in relational caching engine to provide sub-second latency and high concurrency for API requests.\n\n## Why Cube Core?\n\nEvery business intelligence tool relies on a semantic layer as its core engineâ€”a critical component that defines metrics, dimensions, and business logic while abstracting the complexity of underlying data sources. However, most semantic layers are proprietary, tightly coupled to specific BI platforms, and cannot be reused across different applications.\n\nCube Core is an open-source project that aims to create an open, modern semantic layer that can be used to power any analytics applications and AI agents. By decoupling the semantic layer from specific tools and making it accessible through standard APIs, Cube Core enables organizations to define their metrics once and use them everywhereâ€”from BI tools to embedded analytics to AI agents.\n\n## Getting Started ğŸš€\n\nYou can get started with Cube locally or self-host it with [Docker](https://www.docker.com/).\n\nOnce Docker is installed, in a new folder for your project, run the following command:\n\n```bash\ndocker run -p 4000:4000 \\\n  -p 15432:15432 \\\n  -v ${PWD}:/cube/conf \\\n  -e CUBEJS_DEV_MODE=true \\\n  cubejs/cube\n```\n\nThen, open http://localhost:4000 in your browser to continue setup.\n\nFor a step-by-step guide, [see the docs](https://cube.dev/docs/getting-started-docker?ref=github-readme).\n\n### Cube â€” Complete Modern BI Tool from Cube Core Creators\n\n[Cube](https://cube.dev?ref=github-readme) is a complete modern agentic analytics platform built on Cube Core. It provides a fully integrated solution with a user-friendly interface, advanced analytics capabilities, and managed infrastructure.\n\n<a href=\"https://cubecloud.dev/auth/signup?ref=github-readme\"><img src=\"https://cubedev-blog-images.s3.us-east-2.amazonaws.com/f1f1eac0-0b44-4c47-936e-33b5c06eedf0.png\" alt=\"Get started now\" width=\"200px\"></a>\n\n## Resources\n\n- [Documentation](https://cube.dev/docs?ref=github-readme)\n- [Getting Started](https://cube.dev/docs/getting-started?ref=github-readme)\n- [Examples & Tutorials](https://cube.dev/docs/examples?ref=github-readme)\n- [Architecture](https://cube.dev/docs/product/introduction#four-layers-of-semantic-layer)\n\n## Contributing\n\nThere are many ways you can contribute to Cube Core! Here are a few possibilities:\n\n* Star this repo and follow us on [X](https://twitter.com/the_cube_dev).\n* Add Cube to your stack on [Stackshare](https://stackshare.io/cube-js).\n* Upvote issues with ğŸ‘ reaction so we know what the demand is for particular issues to prioritize them within the roadmap.\n* Create issues every time you feel something is missing or goes wrong.\n* Ask questions on [Stack Overflow with cube.js tag](https://stackoverflow.com/questions/tagged/cube.js) if others might have these questions as well.\n* Provide pull requests for all open issues and especially for those with [help wanted](https://github.com/cube-js/cube/issues?q=is%3Aissue+is%3Aopen+label%3A\"help+wanted\") and [good first issue](https://github.com/cube-js/cube/issues?q=is%3Aissue+is%3Aopen+label%3A\"good+first+issue\") labels.\n\nAll sorts of contributions are **welcome and extremely helpful** ğŸ™Œ Please refer to [the contribution guide](https://github.com/cube-js/cube/blob/master/CONTRIBUTING.md) for more information.\n\n## License\n\nCube Client is [MIT licensed](./packages/cubejs-client-core/LICENSE).\n\nCube Backend is [Apache 2.0 licensed](./packages/cubejs-server/LICENSE).\n\n\n[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2Fcube-js%2Fcube.js.svg?type=large)](https://app.fossa.io/projects/git%2Bgithub.com%2Fcube-js%2Fcube.js?ref=badge_large)\n",
      "stars_today": 11
    },
    {
      "id": 105262714,
      "name": "oauth2-proxy",
      "full_name": "oauth2-proxy/oauth2-proxy",
      "description": "A reverse proxy that provides authentication with Google, Azure, OpenID Connect and many more identity providers.",
      "html_url": "https://github.com/oauth2-proxy/oauth2-proxy",
      "stars": 13638,
      "forks": 1981,
      "language": "Go",
      "topics": [
        "cloud-infrastructure",
        "hacktoberfest",
        "oauth2-proxy",
        "ssl",
        "sso"
      ],
      "created_at": "2017-09-29T10:59:10Z",
      "updated_at": "2026-01-15T00:59:27Z",
      "pushed_at": "2026-01-14T22:19:20Z",
      "open_issues": 257,
      "owner": {
        "login": "oauth2-proxy",
        "avatar_url": "https://avatars.githubusercontent.com/u/62798169?v=4"
      },
      "readme": "[![Continuous Integration](https://github.com/oauth2-proxy/oauth2-proxy/actions/workflows/ci.yml/badge.svg)](https://github.com/oauth2-proxy/oauth2-proxy/actions/workflows/ci.yml)\n[![Go Report Card](https://goreportcard.com/badge/github.com/oauth2-proxy/oauth2-proxy)](https://goreportcard.com/report/github.com/oauth2-proxy/oauth2-proxy)\n[![GoDoc](https://godoc.org/github.com/oauth2-proxy/oauth2-proxy?status.svg)](https://godoc.org/github.com/oauth2-proxy/oauth2-proxy)\n[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE)\n[![Maintainability](https://api.codeclimate.com/v1/badges/a58ff79407212e2beacb/maintainability)](https://codeclimate.com/github/oauth2-proxy/oauth2-proxy/maintainability)\n[![Test Coverage](https://api.codeclimate.com/v1/badges/a58ff79407212e2beacb/test_coverage)](https://codeclimate.com/github/oauth2-proxy/oauth2-proxy/test_coverage)\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Foauth2-proxy%2Foauth2-proxy.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2Foauth2-proxy%2Foauth2-proxy?ref=badge_shield)\n\n![OAuth2 Proxy](docs/static/img/logos/OAuth2_Proxy_horizontal.svg)\n\nOAuth2-Proxy is a flexible, open-source tool that can act as either a standalone reverse proxy or a middleware component integrated into existing reverse proxy or load balancer setups. It provides a simple and secure way to protect your web applications with OAuth2 / OIDC authentication. As a reverse proxy, it intercepts requests to your application and redirects users to an OAuth2 provider for authentication. As a middleware, it can be seamlessly integrated into your existing infrastructure to handle authentication for multiple applications.\n\nOAuth2-Proxy supports a lot of OAuth2 as well as OIDC providers. Either through a generic OIDC client or a specific implementation for Google, Microsoft Entra ID, GitHub, login.gov and others. Through specialised provider implementations oauth2-proxy can extract more details about the user like preferred usernames and groups. Those details can then be forwarded as HTTP headers to your upstream applications.\n\n![Simplified Architecture](docs/static/img/simplified-architecture.svg)\n\n## Get Started\n\nOAuth2-Proxy's [Installation Docs](https://oauth2-proxy.github.io/oauth2-proxy/installation) cover how to install and configure your setup. Additionally you can take a further look at the [example setup files](https://github.com/oauth2-proxy/oauth2-proxy/tree/master/contrib/local-environment).\n\n## Releases\n\n### Binaries\nWe publish oauth2-proxy as compiled binaries on GitHub for all major architectures as well as more exotic ones like `ppc64le` as well as `s390x`.\n\nCheck out the [latest release](https://github.com/oauth2-proxy/oauth2-proxy/releases/latest).\n\n### Images\n\nFrom `v7.6.0` and up the base image has been changed from Alpine to [GoogleContainerTools/distroless](https://github.com/GoogleContainerTools/distroless).\nThis image comes with even fewer installed dependencies and thus should improve security. The image therefore is also slightly smaller than Alpine.\nFor debugging purposes (and those who really need it. e.g. `armv6`) we still provide images based on Alpine. The tags of these images are suffixed with `-alpine`.\n\nSince 2023-11-18 we build nightly images directly from the `master` branch and provide them at `quay.io/oauth2-proxy/oauth2-proxy-nightly`.\nThese images are considered unstable and therefore should **NOT** be used for production purposes unless you know what you're doing.\n\n## Sponsors\n\n![Microsoft](https://upload.wikimedia.org/wikipedia/commons/9/96/Microsoft_logo_%282012%29.svg)\nMicrosoft Azure credits for open source projects\n\nWould you like to sponsor the project then please contact us at [sponsors@oauth2-proxy.dev](mailto:sponsors@oauth2-proxy.dev)\n\n## Getting Involved\n[![Slack](https://img.shields.io/badge/slack-Gopher_%23oauth2--proxy-red?logo=slack)](https://gophers.slack.com/archives/CM2RSS25N)\n\nJoin the #oauth2-proxy [Slack channel](https://gophers.slack.com/archives/CM2RSS25N) to chat with other users of oauth2-proxy or reach out to the maintainers directly. Use the [public invite link](https://invite.slack.golangbridge.org/) to get an invite for the Gopher Slack space.\n\nOAuth2-Proxy is a community-driven project. We rely on the contributï¸ions of our users to continually improve it. While review times can vary, we appreciate your patience and understanding. As a volunteer-driven project, we strive to keep this project stable and might take longer to merge changes.\n\nIf you want to contribute to the project. Please see our [Contributing](https://oauth2-proxy.github.io/oauth2-proxy/community/contribution) guide.\n\nWho uses OAuth2-Proxy? Have a look at our new [ADOPTERS](ADOPTERS.md) file and\nfeel free to open a PR to add your organisation.\n\nThanks to all the people who already contributed â¤\n\n<a href=\"https://github.com/oauth2-proxy/oauth2-proxy/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=oauth2-proxy/oauth2-proxy&columns=15&max=75\" />\n  <img src=\"https://img.shields.io/github/contributors/oauth2-proxy/oauth2-proxy\" />\n</a>\n\nMade with [contrib.rocks](https://contrib.rocks).\n\n## Security\n\nIf you believe you have found a vulnerability within OAuth2 Proxy or any of its dependencies, please do **NOT** open an issue or PR on GitHub, please do **NOT** post any details publicly.\n\nSecurity disclosures **MUST** be done in private. If you have found an issue that you would like to bring to the attention of the maintainers, please compose an email and send it to the list of people listed in our [MAINTAINERS](MAINTAINERS) file.\n\nFor more details read our full [Security Docs](https://oauth2-proxy.github.io/oauth2-proxy/community/security#security-disclosures)\n\n### Security Notice for v6.0.0 and older\n\nIf you are running a version older than v6.0.0 we **strongly recommend** to the current version.\n\nSee [open redirect vulnerability](https://github.com/oauth2-proxy/oauth2-proxy/security/advisories/GHSA-5m6c-jp6f-2vcv) for details.\n\n## Repository History\n\n**2018-11-27:** This repository was forked from [bitly/OAuth2_Proxy](https://github.com/bitly/oauth2_proxy). Versions v3.0.0 and up are from this fork and will have diverged from any changes in the original fork. A list of changes can be seen in the [CHANGELOG](CHANGELOG.md).\n\n**2020-03-29:** This project was formerly hosted as `pusher/oauth2_proxy` but has been renamed to `oauth2-proxy/oauth2-proxy`. Going forward, all images shall be available at `quay.io/oauth2-proxy/oauth2-proxy` and binaries will be named `oauth2-proxy`.\n\n## License\n\nOAuth2-Proxy is distributed under [The MIT License](LICENSE).\n\n\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2Foauth2-proxy%2Foauth2-proxy.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2Foauth2-proxy%2Foauth2-proxy?ref=badge_large)\n",
      "stars_today": 11
    },
    {
      "id": 119637215,
      "name": "Maccy",
      "full_name": "p0deje/Maccy",
      "description": "Lightweight clipboard manager for macOS",
      "html_url": "https://github.com/p0deje/Maccy",
      "stars": 18247,
      "forks": 825,
      "language": "Swift",
      "topics": [
        "clipboard-manager",
        "maccy",
        "macos"
      ],
      "created_at": "2018-01-31T05:01:53Z",
      "updated_at": "2026-01-14T23:18:03Z",
      "pushed_at": "2025-12-19T20:08:30Z",
      "open_issues": 119,
      "owner": {
        "login": "p0deje",
        "avatar_url": "https://avatars.githubusercontent.com/u/665846?v=4"
      },
      "readme": "\n<img width=\"128px\" src=\"https://maccy.app/img/maccy/Logo.png\" alt=\"Logo\" align=\"left\" />\n\n# [Maccy](https://maccy.app)\n\n[![Downloads](https://img.shields.io/github/downloads/p0deje/Maccy/total.svg)](https://github.com/p0deje/Maccy/releases/latest)\n[![Build Status](https://img.shields.io/bitrise/716921b669780314/master?token=3pMiCb5dpFzlO-7jTYtO3Q)](https://app.bitrise.io/app/716921b669780314)\n\nMaccy is a lightweight clipboard manager for macOS. It keeps the history of what you copy\nand lets you quickly navigate, search, and use previous clipboard contents.\n\nMaccy works on macOS Sonoma 14 or higher.\n\n<!-- vim-markdown-toc GFM -->\n\n* [Features](#features)\n* [Install](#install)\n* [Usage](#usage)\n* [Advanced](#advanced)\n  * [Ignore Copied Items](#ignore-copied-items)\n  * [Ignore Custom Copy Types](#ignore-custom-copy-types)\n  * [Speed up Clipboard Check Interval](#speed-up-clipboard-check-interval)\n* [FAQ](#faq)\n  * [Why doesn't it paste when I select an item in history?](#why-doesnt-it-paste-when-i-select-an-item-in-history)\n  * [When assigning a hotkey to open Maccy, it says that this hotkey is already used in some system setting.](#when-assigning-a-hotkey-to-open-maccy-it-says-that-this-hotkey-is-already-used-in-some-system-setting)\n  * [How to restore hidden footer?](#how-to-restore-hidden-footer)\n  * [How to ignore copies from Universal Clipboard?](#how-to-ignore-copies-from-universal-clipboard)\n  * [My keyboard shortcut stopped working in password fields. How do I fix this?](#my-keyboard-shortcut-stopped-working-in-password-fields-how-do-i-fix-this)\n* [Translations](#translations)\n* [Motivation](#motivation)\n* [License](#license)\n\n<!-- vim-markdown-toc -->\n\n## Features\n\n* Lightweight and fast\n* Keyboard-first\n* Secure and private\n* Native UI\n* Open source and free\n\n## Install\n\nDownload the latest version from the [releases](https://github.com/p0deje/Maccy/releases/latest) page, or use [Homebrew](https://brew.sh/):\n\n```sh\nbrew install maccy\n```\n\n## Usage\n\n1. <kbd>SHIFT (â‡§)</kbd> + <kbd>COMMAND (âŒ˜)</kbd> + <kbd>C</kbd> to popup Maccy or click on its icon in the menu bar.\n2. Type what you want to find.\n3. To select the history item you wish to copy, press <kbd>ENTER</kbd>, or click the item, or use <kbd>COMMAND (âŒ˜)</kbd> + `n` shortcut.\n4. To choose the history item and paste, press <kbd>OPTION (âŒ¥)</kbd> + <kbd>ENTER</kbd>, or <kbd>OPTION (âŒ¥)</kbd> + <kbd>CLICK</kbd> the item, or use <kbd>OPTION (âŒ¥)</kbd> + `n` shortcut.\n5. To choose the history item and paste without formatting, press <kbd>OPTION (âŒ¥)</kbd> + <kbd>SHIFT (â‡§)</kbd> + <kbd>ENTER</kbd>, or <kbd>OPTION (âŒ¥)</kbd> + <kbd>SHIFT (â‡§)</kbd> + <kbd>CLICK</kbd> the item, or use <kbd>OPTION (âŒ¥)</kbd> + <kbd>SHIFT (â‡§)</kbd> + `n` shortcut.\n6. To delete the history item, press <kbd>OPTION (âŒ¥)</kbd> + <kbd>DELETE (âŒ«)</kbd>.\n7. To see the full text of the history item, wait a couple of seconds for tooltip.\n8. To pin the history item so that it remains on top of the list, press <kbd>OPTION (âŒ¥)</kbd> + <kbd>P</kbd>. The item will be moved to the top with a random but permanent keyboard shortcut. To unpin it, press <kbd>OPTION (âŒ¥)</kbd> + <kbd>P</kbd> again.\n9. To clear all unpinned items, select _Clear_ in the menu, or press <kbd>OPTION (âŒ¥)</kbd> + <kbd>COMMAND (âŒ˜)</kbd> + <kbd>DELETE (âŒ«)</kbd>. To clear all items including pinned, select _Clear_ in the menu with  <kbd>OPTION (âŒ¥)</kbd> pressed, or press <kbd>SHIFT (â‡§)</kbd> + <kbd>OPTION (âŒ¥)</kbd> + <kbd>COMMAND (âŒ˜)</kbd> + <kbd>DELETE (âŒ«)</kbd>.\n10. To disable Maccy and ignore new copies, click on the menu icon with <kbd>OPTION (âŒ¥)</kbd> pressed.\n11. To ignore only the next copy, click on the menu icon with <kbd>OPTION (âŒ¥)</kbd> + <kbd>SHIFT (â‡§)</kbd> pressed.\n12. To customize the behavior, check \"Preferencesâ€¦\" window, or press <kbd>COMMAND (âŒ˜)</kbd> + <kbd>,</kbd>.\n\n## Advanced\n\n### Ignore Copied Items\n\nYou can tell Maccy to ignore all copied items:\n\n```sh\ndefaults write org.p0deje.Maccy ignoreEvents true # default is false\n```\n\nThis is useful if you have some workflow for copying sensitive data. You can set `ignoreEvents` to true, copy the data and set `ignoreEvents` back to false.\n\nYou can also click the menu icon with <kbd>OPTION (âŒ¥)</kbd> pressed. To ignore only the next copy, click with <kbd>OPTION (âŒ¥)</kbd> + <kbd>SHIFT (â‡§)</kbd> pressed.\n\n### Ignore Custom Copy Types\n\nBy default Maccy will ignore certain copy types that are considered to be confidential\nor temporary. The default list always include the following types:\n\n* `org.nspasteboard.TransientType`\n* `org.nspasteboard.ConcealedType`\n* `org.nspasteboard.AutoGeneratedType`\n\nAlso, default configuration includes the following types but they can be removed\nor overwritten:\n\n* `com.agilebits.onepassword`\n* `com.typeit4me.clipping`\n* `de.petermaurer.TransientPasteboardType`\n* `Pasteboard generator type`\n* `net.antelle.keeweb`\n\nYou can add additional custom types using settings.\nTo find what custom types are used by an application, you can use\nfree application [Pasteboard-Viewer](https://github.com/sindresorhus/Pasteboard-Viewer).\nSimply download the application, open it, copy something from the application you\nwant to ignore and look for any custom types in the left sidebar. [Here is an example\nof using this approach to ignore Adobe InDesign](https://github.com/p0deje/Maccy/issues/125).\n\n### Speed up Clipboard Check Interval\n\nBy default, Maccy checks clipboard every 500 ms, which should be enough for most users. If you want\nto speed it up, you can change it with `defaults`:\n\n```sh\ndefaults write org.p0deje.Maccy clipboardCheckInterval 0.1 # 100 ms\n```\n\n## FAQ\n\n### Why doesn't it paste when I select an item in history?\n\n1. Make sure you have \"Paste automatically\" enabled in Preferences.\n2. Make sure \"Maccy\" is added to System Settings -> Privacy & Security -> Accessibility.\n\n### When assigning a hotkey to open Maccy, it says that this hotkey is already used in some system setting.\n\n1. Open System settings -> Keyboard -> Keyboard Shortcuts.\n2. Find where that hotkey is used. For example, \"Convert text to simplified Chinese\" is under Services -> Text.\n3. Disable that hotkey or remove assigned combination ([screenshot](https://github.com/p0deje/Maccy/assets/576152/446719e6-c3e5-4eb0-95fb-5a811066487f)).\n4. Restart Maccy.\n5. Assign hotkey in Maccy settings.\n\n### How to restore hidden footer?\n\n1. Open Maccy window.\n2. Press <kbd>COMMAND (âŒ˜)</kbd> + <kbd>,</kbd> to open preferences.\n3. Enable footer in Appearance section.\n\nIf for some reason it doesn't work, run the following command in Terminal.app:\n\n```sh\ndefaults write org.p0deje.Maccy showFooter 1\n```\n\n### How to ignore copies from [Universal Clipboard](https://support.apple.com/en-us/102430)?\n\n1. Open Preferences -> Ignore -> Pasteboard Types.\n2. Add `com.apple.is-remote-clipboard`.\n\n### My keyboard shortcut stopped working in password fields. How do I fix this?\n\nIf your shortcut produces a character (like `Option+C` â†’ \"Ã§\"), macOS security may block it in password fields. Use [Karabiner-Elements](https://karabiner-elements.pqrs.org/) to remap your shortcut to a different combination like `Cmd+Shift+C`. [See detailed solution](docs/keyboard-shortcut-password-fields.md).\n\n## Translations\n\nThe translations are hosted in [Weblate](https://hosted.weblate.org/engage/maccy/).\nYou can use it to suggest changes in translations and localize the application to a new language.\n\n[![Translation status](https://hosted.weblate.org/widget/maccy/multi-auto.svg)](https://hosted.weblate.org/engage/maccy/)\n\n## Motivation\n\nThere are dozens of similar applications out there, so why build another?\nOver the past years since I moved from Linux to macOS, I struggled to find\na clipboard manager that is as free and simple as [Parcellite](http://parcellite.sourceforge.net),\nbut I couldn't. So I've decided to build one.\n\nAlso, I wanted to learn Swift and get acquainted with macOS application development.\n\n\n## License\n\n[MIT](./LICENSE)\n",
      "stars_today": 11
    },
    {
      "id": 672651553,
      "name": "LichtFeld-Studio",
      "full_name": "MrNeRF/LichtFeld-Studio",
      "description": "LichtFeld Studio: Where reality and the digital world blend.",
      "html_url": "https://github.com/MrNeRF/LichtFeld-Studio",
      "stars": 2056,
      "forks": 223,
      "language": "C++",
      "topics": [
        "computer-graphics",
        "computer-vision",
        "cuda",
        "gaussian-splatting",
        "optimization"
      ],
      "created_at": "2023-07-30T19:55:48Z",
      "updated_at": "2026-01-14T21:53:33Z",
      "pushed_at": "2026-01-14T23:43:04Z",
      "open_issues": 63,
      "owner": {
        "login": "MrNeRF",
        "avatar_url": "https://avatars.githubusercontent.com/u/33876434?v=4"
      },
      "readme": "<div align=\"center\"><picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"src/visualizer/gui/assets/logo/lichtfeld-logo-white.svg\">\n  <img src=\"src/visualizer/gui/assets/logo/lichtfeld-logo.svg\" alt=\"LichtFeld Studio\" height=\"60\">\n</picture></div>\n\n<div align=\"center\">\n**A high-performance C++ and CUDA implementation of 3D Gaussian Splatting**\n\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/TbxJST2BbC)\n[![Website](https://img.shields.io/badge/Website-Lichtfeld%20Studio-blue)](https://mrnerf.github.io/lichtfeld-studio-web/)\n[![Papers](https://img.shields.io/badge/Papers-Awesome%203DGS-orange)](https://mrnerf.github.io/awesome-3D-gaussian-splatting/)\n[![License](https://img.shields.io/badge/License-GPLv3-green.svg)](LICENSE)\n[![CUDA](https://img.shields.io/badge/CUDA-12.8+-76B900?logo=nvidia&logoColor=white)](https://developer.nvidia.com/cuda-downloads)\n[![C++](https://img.shields.io/badge/C++-23-00599C?logo=cplusplus&logoColor=white)](https://en.cppreference.com/w/cpp/23)\n\n<img src=\"docs/viewer_demo.gif\" alt=\"3D Gaussian Splatting Viewer\" width=\"85%\"/>\n\n[**Overview**](#overview) â€¢\n[**Community & Support**](#community--support) â€¢\n[**Installation**](#installation) â€¢\n[**Contributing**](#contributing) â€¢\n[**Acknowledgments**](#acknowledgments) â€¢\n[**Citation**](#citation) â€¢\n[**License**](#license)\n\n</div>\n\n---\n\n## Sponsors\n\n<div align=\"center\">\n\n<a href=\"https://www.core11.eu/\">\n  <img src=\"docs/media/core11_multi.svg\" alt=\"Core 11\" height=\"60\">\n</a>\n\n</div>\n\n---\n\n## Support LichtFeld Studio Development\n\nLichtFeld Studio is a free, open-source implementation of 3D Gaussian Splatting that pushes the boundaries of real-time rendering performance.\n\n**Why Your Support Matters**:\nThis project requires significant time and resources to develop and maintain. \n\nUnlike commercial alternatives that can cost thousands in licensing fees, LichtFeld Studio remains completely free and open. Your contribution helps ensure it stays that way while continuing to evolve with the latest research.\n\nWhether you're using it for research, production, or learning, your support enables us to dedicate more time to making LichtFeld Studio faster, more powerful, and accessible to everyone in the 3D graphics community.\n\n[![PayPal](https://img.shields.io/badge/PayPal-00457C?style=for-the-badge&logo=paypal&logoColor=white)](https://paypal.me/MrNeRF)\n[![Support on Donorbox](https://img.shields.io/badge/Donate-Donorbox-27A9E1?style=for-the-badge)](https://donorbox.org/lichtfeld-studio)\n\n---\n\n## Overview\n\nLichtFeld Studio is a high-performance implementation of 3D Gaussian Splatting that leverages modern C++23 and CUDA 12.8+ for optimal performance. Built with a modular architecture, it provides both training and real-time visualization capabilities for neural rendering research and applications.\n\n### Key Features\n\n- **2.4x faster rasterization** (winner of first bounty by Florian Hahlbohm)\n- **MCMC optimization strategy** for improved convergence\n- **Real-time interactive viewer** with OpenGL rendering\n- **Modular architecture** with separate core, training, and rendering components\n- **Multiple rendering modes** including RGB, depth, and combined views\n- **Bilateral grid appearance modeling** for handling per-image variations\n\n## Community & Support\n\nJoin our growing community for discussions, support, and updates:\n\n- **[Discord Community](https://discord.gg/TbxJST2BbC)** - Get help, share results, and discuss development\n- **[LichtFeld Studio FAQ](https://github.com/MrNeRF/LichtFeld-Studio/wiki/Frequently-Asked-Questions)** - Frequently Asked Questions about LichtFeld Studio\n- **[LichtFeld Studio Wiki](https://github.com/MrNeRF/LichtFeld-Studio/wiki/)** - Documentation WIKI\n- **[Website](https://mrnerf.com)** - Visit our website for more resources\n- **[Awesome 3D Gaussian Splatting](https://mrnerf.github.io/awesome-3D-gaussian-splatting/)** - Comprehensive paper list\n- **[@janusch_patas](https://twitter.com/janusch_patas)** - Follow for the latest updates\n\n## Installation\nFind out how to install in our [LichtFeld Studio Wiki](https://github.com/MrNeRF/LichtFeld-Studio/wiki/)\n\n## Contributing\n\nWe welcome contributions! See our [Contributing Guidelines](CONTRIBUTING.md).\n\n### Getting Started\n- Check issues labeled **good first issue**\n- Join our [Discord](https://discord.gg/TbxJST2BbC) for discussions\n- Use the pre-commit hook: `cp tools/pre-commit .git/hooks/`\n\n\n## Acknowledgments\n\nThis project builds upon and is inspired by the following:\n\n### Core Research\n| Project | Description | License |\n|---------|-------------|---------|\n| [3D Gaussian Splatting](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/) | Original work by Kerbl et al. | Custom |\n| [gsplat](https://github.com/nerfstudio-project/gsplat) | Optimized CUDA rasterization backend | Apache-2.0 |\n\n### Gaussian Splatting Tools & Inspiration\n| Project | Description | License |\n|---------|-------------|---------|\n| [SuperSplat](https://github.com/playcanvas/supersplat) | PlayCanvas Gaussian Splat editor | MIT |\n| [SplatShop](https://github.com/m-schuetz/Splatshop) | Gaussian Splat editing tool | MIT |\n| [splat-transform](https://github.com/playcanvas/splat-transform) | Transformation utilities for splats | MIT |\n| [spz](https://github.com/nianticlabs/spz) | Niantic's compressed splat format | MIT |\n\n### Graphics & UI Libraries\n| Project | Description | License |\n|---------|-------------|---------|\n| [Dear ImGui](https://github.com/ocornut/imgui) | Immediate mode GUI library | MIT |\n| [ImGuizmo](https://github.com/CedricGuillemet/ImGuizmo) | Gizmo manipulation for ImGui | MIT |\n| [GLFW](https://www.glfw.org/) | OpenGL window/context management | zlib |\n| [GLM](https://github.com/g-truc/glm) | OpenGL Mathematics library | MIT |\n| [glad](https://github.com/Dav1dde/glad) | OpenGL loader | MIT |\n\n### CUDA & GPU Libraries\n| Project | Description | License |\n|---------|-------------|---------|\n| [NVIDIA nvImageCodec](https://github.com/NVIDIA/nvImageCodec) | GPU-accelerated image encoding/decoding | Apache-2.0 |\n| [Intel TBB](https://github.com/oneapi-src/oneTBB) | Threading Building Blocks | Apache-2.0 |\n\n### Data & I/O Libraries\n| Project | Description | License |\n|---------|-------------|---------|\n| [tinyply](https://github.com/ddiakopoulos/tinyply) | Lightweight PLY file loader | Public Domain / BSD-2 |\n| [OpenImageIO](https://github.com/AcademySoftwareFoundation/OpenImageIO) | Image I/O library | Apache-2.0 |\n| [nlohmann/json](https://github.com/nlohmann/json) | JSON for Modern C++ | MIT |\n| [LibArchive](https://libarchive.org/) | Multi-format archive library | BSD |\n| [libwebp](https://github.com/webmproject/libwebp) | WebP image format library | BSD-3-Clause |\n\n### Utilities\n| Project | Description | License |\n|---------|-------------|---------|\n| [spdlog](https://github.com/gabime/spdlog) | Fast C++ logging library | MIT |\n| [cpp-httplib](https://github.com/yhirose/cpp-httplib) | HTTP/HTTPS library | MIT |\n| [FreeType](https://freetype.org/) | Font rendering library | FreeType License |\n| [args](https://github.com/Taywee/args) | Command-line argument parser | MIT |\n\n### Testing & Development\n| Project | Description | License |\n|---------|-------------|---------|\n| [PyTorch/LibTorch](https://pytorch.org/) | Used for tensor comparison tests | BSD-3-Clause |\n| [Google Test](https://github.com/google/googletest) | C++ testing framework | BSD-3-Clause |\n\n### Icons\n| Project | Description | License |\n|---------|-------------|---------|\n| [Tabler Icons](https://github.com/tabler/tabler-icons) | UI icons | MIT |\n| [Lucide Icons](https://github.com/lucide-icons/lucide) | UI icons (fork of Feather) | ISC |\n\n## Citation\n\n```bibtex\n@software{lichtfeld2025,\n  author    = {LichtFeld Studio},\n  title     = {A high-performance C++ and CUDA implementation of 3D Gaussian Splatting},\n  year      = {2025},\n  url       = {https://github.com/MrNeRF/LichtFeld-Studio}\n}\n```\n\n## License\n\nThis project is licensed under GPLv3. See [LICENSE](LICENSE) for details.\n\n---\n\n<div align=\"center\">\n\n**Connect with us:** [Website](https://mrnerf.com) â€¢ [Discord](https://discord.gg/TbxJST2BbC) â€¢ [Twitter](https://twitter.com/janusch_patas)\n\n</div>\n",
      "stars_today": 11
    },
    {
      "id": 964473113,
      "name": "yourtv",
      "full_name": "horsemail/yourtv",
      "description": "å®‰å“ç”µè§†ç›´æ’­APKï¼šIPTV/ç¶²é è¦–é »æ”¯æŒX5ï¼Œå¯è‡ªå®šç¾©æº(æ”¯æŒwebview://æ ¼å¼)ï¼ŒIPTVæ”¯æŒç•«ä¸­ç•«å’Œç†„å±æ’­æ”¾ã€‚ Android TV Live APK: IPTV/web video supports X5, customizable sources (support webview:// format), IPTV supports picture-in-picture and off-screen playback.",
      "html_url": "https://github.com/horsemail/yourtv",
      "stars": 907,
      "forks": 107,
      "language": "Kotlin",
      "topics": [],
      "created_at": "2025-04-11T09:12:13Z",
      "updated_at": "2026-01-14T23:57:50Z",
      "pushed_at": "2026-01-11T12:34:13Z",
      "open_issues": 14,
      "owner": {
        "login": "horsemail",
        "avatar_url": "https://avatars.githubusercontent.com/u/91647741?v=4"
      },
      "readme": "## ğŸŒ èªè¨€ / Languages\n\n- [ğŸ‡¨ğŸ‡³ ä¸­æ–‡èªªæ˜](README.MD)\n- [ğŸ‡ºğŸ‡¸ English Version](README.en.md)\n# ä½ çš„é›»è¦–ï¼šå®‰å“é›»è¦–/æ‰‹æœºç›´æ’­APK\næ”¯æŒå®‰å“6.0(API23)ç´šä»¥ä¸Šç‰ˆæœ¬<br>\nç¶œåˆmy-tv/my-tv-0/my-tv-1/mytv-android/WebViewTVLiveç­‰é …ç›®çš„åŠŸèƒ½ã€‚<br>  \nIPTV/ç¶²é è¦–é »æ’­æ”¾å®‰å“APKè»Ÿä»¶ï¼Œæ”¯æŒè…¾è®¯webview x5ï¼Œ<br>\nå¯è‡ªå®šç¾©æº(æ”¯æŒwebview://æ ¼å¼ç¶²é è¦–é »æº)ï¼Œæ”¯æŒæ‰‹æ©Ÿç•«ä¸­ç•«ï¼ŒIPTVæ”¯æŒæ‰‹æ©Ÿç†„å±æ’­æ”¾ã€‚<br>\n[yourtv](https://github.com/horsemail/yourtv)\n<br>\n## **è«‹ä»”ç´°é–±è®€å¾Œé¢çš„[ä½¿ç”¨èªªæ˜](#ä½¿ç”¨)ã€‚**\n## åœ¨ç·šåŠ å¯†è§£å¯†ï¼šï¼ˆå…¼å®¹Tvboxçš„æ¥å£æºåŠ å¯†è§£å¯†ï¼‰\nhttps://yourtvcrypto.horsenma.net<br>\nèˆ‡é …ç›®å…§åŠ å¯†è§£å¯†é‚è¼¯å®Œå…¨ä¸€è‡´<br>\n\né›»å ±ç¾¤çµ„<br>\nhttps://t.me/yourtvapp<br>\n<img src=\"./screenshots/appreciate.jpg\" alt=\"image\" width=200 /><br><br>\n<img src=\"./screenshots/527.jpg\" alt=\"image\"/><br><br>\n<img src=\"./screenshots/090901.jpg\" alt=\"image\"/><br><br>\n<img src=\"./screenshots/090902.jpg\" alt=\"image\"/><br><br>\n<br>\n## ä½¿ç”¨\n\né›»è¦–æ©Ÿï¼š<br>\n1ã€é–‹æ©Ÿä½¿ç”¨ï¼Œä¸‹è¼‰ç›´æ’­æºè³‡æºï¼Œè«‹è€å¿ƒç­‰å¾…3-5ç§’ï¼Œ<br>\n2ã€ç¢ºå®š/ä¸­å¿ƒéµï¼šå½ˆå‡ºçµ„/é »é“æ¸…å–®ï¼Œä¸Šä¸‹å·¦å³é¸æ“‡çµ„/é »é“ï¼Œç¢ºå®šé¸æ“‡é »é“ï¼Œå³éµæ”¶è—/å–æ¶ˆæ”¶è—<br>\n3ã€ä¸Šéµ/ä¸‹éµï¼šåˆ‡æ›é »é“<br>\n4ã€å·¦éµï¼šé¡¯ç¤ºç¯€ç›®å–®ä¿¡æ¯<br>\n5ã€å³éµï¼šåˆ‡æ›åŒä¸€é »é“çš„ä¸é€šç›´æ’­æºåœ°å€<br>\nã€é•¿æŒ‰èœå•é”®/å³é”®ï¼Œæˆ–å¿«é€ŸæŒ‰4æ¬¡èœå•é”®/å³é”®ï¼Œæ˜¾ç¤ºè®¾ç½®ç•Œé¢<br>\n7ã€é•¿æŒ‰ç¡®è®¤ä¸­å¿ƒé”®ï¼Œæˆ–å¿«é€Ÿè¿æŒ‰4æ¬¡ï¼Œæ˜¾ç¤ºå½“å‰é¢‘é“ç›´æ’­æºä¿¡æ¯ï¼Œå¹¶å¯é€‰æ‹©ä¸åŒç›´æ’­æº<br>\n8ã€é•·æŒ‰åˆ†çµ„éš±è—ï¼Œé›™æ“Šç´…è‰²Xæ¢å¾©ã€‚<br>\n\n<br>\nè§¸æ‘¸å±ï¼š<br>\n1ã€å¼€æœºä½¿ç”¨ï¼Œä¸‹è½½ç›´æ’­æºèµ„æºï¼Œè¯·ç­‰å¾…3-5ç§’ï¼Œ<br>\n2ã€å·¦ä¾§ä¸Šå±å¿«é€Ÿè°ƒèŠ‚å®½åº¦<br>\n3ã€å³ä¾§ä¸Šå±ï¼šè°ƒèŠ‚å£°éŸ³<br>\n4ã€ä¸­é—´éƒ¨åˆ†æ»‘å±ï¼šåˆ‡æ¢é¢‘é“<br>\n5ã€å•å‡»å±å¹•ï¼šå¼¹å‡ºç»„/é¢‘é“åˆ—è¡¨ï¼Œç‚¹å‡»é€‰æ‹©ï¼Œç‚¹å‡»çˆ±å¿ƒæ”¶è—/å–æ¶ˆæ”¶è—<br>\n6ã€åŒå‡»é¢‘å¹•ï¼Œæ˜¾ç¤ºIPTVç›´æ’­æºçš„è¿›åº¦æ¡ã€‚<br>\n7ã€è¿ç»­ç‚¹å‡»å±å¹•4æ¬¡ï¼šæ˜¾ç¤ºè®¾ç½®ç•Œé¢<br>\n8ã€ç‚¹å‡»è™šæ‹Ÿæ¢æºå¥ï¼šåˆ‡æ¢ç›´æ’­æºï¼ˆAPPä¹Ÿæ ¹æ®å¡é¡¿æƒ…å†µè‡ªåŠ¨åˆ‡æ¢ç›´æ’­æºï¼‰ï¼Œè®¾ç½®ç•Œé¢å¯åˆ‡æ¢æ˜¾ç¤ºè™šæ‹Ÿé”®<br>\né•¿æŒ‰è™šæ‹Ÿæ¢æºå¥ï¼šæ˜¾ç¤ºåŒä¸€é¢‘é“çš„æ‰€æœ‰ä¸åŒç›´æ’­æºå¹¶å¯åˆ‡æ¢ã€‚<br>\n9ã€é•¿æŒ‰è§¦æ‘¸å±å¹•ï¼šæ˜¾ç¤ºå½“å‰é¢‘é“èŠ‚ç›®å•<br>\n10ã€æŒ‰ä¸»é¡µï¼ˆæ‰‹æœºè™šæ‹Ÿè¯·æ±‚é”®ï¼‰é”®è¿›å…¥ç”»ä¸­ç”»<br>\n11ã€è§¦æ‘¸å±ç†„å±ä»å¯æ’­æ”¾ï¼ˆè®¾ç½®ç•Œé¢æœ‰å–æ¶ˆå¼€å…³ï¼‰<br>\n12ã€é•·æŒ‰åˆ†çµ„éš±è—ï¼Œé›™æ“Šç´…è‰²Xæ¢å¾©ã€‚<br>\n\n\n* æ‰“é–‹é…ç½®åï¼Œé¸æ“‡é ç¨‹é…ç½®ï¼ŒæƒæäºŒç¶­ç¢¼å¯ä»¥é…ç½®è¦–é »æºç­‰ã€‚ä¹Ÿå¯ä»¥ç›´æ¥é ç¨‹é…ç½®åœ°å€ http://0.0.0.0:34567\n* æ‰“é–‹â€œæ¯å¤©è‡ªå‹•æ›´æ–°ç›´æ’­æºâ€åï¼Œæ‡‰ç”¨å•Ÿå‹•åæœƒè‡ªå‹•æ›´æ–°ç›´æ’­æº\n\næ³¨æ„ï¼š\n\n* é‡åˆ°å•é¡Œå¯ä»¥å…ˆè€ƒæ…®é‡å•Ÿ/æ¢å¾©é»˜èª/æ¸…é™¤æ•¸æ“š/é‡æ–°å®‰è£ç­‰æ–¹å¼è‡ªåŠ©è§£æ±º\n\nä¸‹è¼‰å®‰è£ [releases](https://github.com/horsemail/yourtv)\n\n## å…¶ä»–\n\nå»ºè­°é€šéADBé€²è¡Œå®‰è£ï¼š\n\n```shell\nadb install YourTV.apk\n```\n\nå°ç±³é›»è¦–å¯ä»¥ä½¿ç”¨å°ç±³é›»è¦–åŠ©æ‰‹é€²è¡Œå®‰è£\n\n## å¸¸è¦‹å•é¡Œ\n\n* ç‚ºä»€éº¼é ç¨‹é…ç½®è¦–é »æºæ–‡æœ¬å¾Œï¼Œå†æ¬¡æ‰“é–‹æ‡‰ç”¨å¾Œåˆæ¢å¾©åˆ°åŸä¾†çš„é…ç½®ï¼Ÿ<br>\n\n  å¦‚æœâ€œæ‡‰ç”¨å•Ÿå‹•åæ›´æ–°è¦–é »æºâ€é–‹å•Ÿå¾Œï¼Œä¸”å­˜åœ¨è¦–é »æºåœ°å€ï¼Œå‰‡æœƒè‡ªå‹•æ›´æ–°ï¼Œå¯èƒ½æœƒè¦†è“‹å·²ä¿å­˜çš„è¦–é »æºæ–‡æœ¬ã€‚<br>\n\n* è‡ªå·±ç·¨è­¯APPæ³¨æ„äº‹é …ï¼š<br>\n  1ã€è³‡æºæ–‡ä»¶éœ€è¦è‡ªå·±é€å€‹ç¢ºèªè¨­ç½®ç‚ºè‡ªå·±çš„ä¿¡æ¯ï¼Œç‰¹åˆ¥æ˜¯cloudflare.txt/github_private.txt/sources.txt<br>\n  éœ€ä½¿ç”¨åŠ å¯†è§£å¯†å·¥å…·ç¶²ç«™ https://yourtvcrypto.horsenma.net  åŠ å¯†å¾Œå­˜å„²ã€‚<br>\n  2ã€æˆ‘ä¸Šå‚³çš„APKæ–‡ä»¶èˆ‡æºç¢¼å¯èƒ½ä¸åŒæ­¥ï¼ŒAPKæ–‡ä»¶æ¯”è¼ƒæ–°ï¼Œæºç¢¼æ›´æ–°ä¸€èˆ¬è½å¾Œå¹¾å¤©ï¼Œè«‹æ³¨æ„æŸ¥çœ‹ï¼Œ<br>\n  3ã€æˆ‘ä¸Šå‚³çš„APKæ–‡ä»¶ä½¿ç”¨çš„åŠ å¯†è§£å¯†é‚è¼¯èˆ‡é …ç›®å…§åŠ å¯†è§£å¯†é‚è¼¯ï¼šhttps://yourtvcrypto.horsenma.net  ä¸åŒï¼Œç›®çš„ä¿è­·æˆ‘çš„ç§æœ‰è³‡æºä¿¡æ¯ã€‚<br>\n* æ—§ç”µè§†æœºæ— æ³•è§‚çœ‹webviewç½‘é¡µè§†é¢‘é¢‘é“çš„ï¼Œæ‰‹åŠ¨å¼ºåˆ¶å®‰è£…<br>\n**[Android System WebView 6.0+ ä¸‹è½½](https://ftp.horsenma.net/tv/Android_System_WebView_Android_6_0.apk)**<br>\n\n## æ„Ÿè¬\n\n[live](https://github.com/fanmingming/live)<br>\n[my-tv-0](https://github.com/lizongying/my-tv-0)<br>\n[my-tv-1](https://github.com/lizongying/my-tv-1)<br>\n\n",
      "stars_today": 11
    },
    {
      "id": 569041,
      "name": "curl",
      "full_name": "curl/curl",
      "description": "A command line tool and library for transferring data with URL syntax, supporting DICT, FILE, FTP, FTPS, GOPHER, GOPHERS, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, MQTT, POP3, POP3S, RTMP, RTMPS, RTSP, SCP, SFTP, SMB, SMBS, SMTP, SMTPS, TELNET, TFTP, WS and WSS. libcurl offers a myriad of powerful features",
      "html_url": "https://github.com/curl/curl",
      "stars": 40372,
      "forks": 7017,
      "language": "C",
      "topics": [
        "c",
        "client",
        "curl",
        "ftp",
        "gopher",
        "hacktoberfest",
        "http",
        "https",
        "imaps",
        "ldap",
        "libcurl",
        "library",
        "mqtt",
        "pop3",
        "scp",
        "sftp",
        "transfer-data",
        "transferring-data",
        "user-agent",
        "websocket"
      ],
      "created_at": "2010-03-18T22:32:22Z",
      "updated_at": "2026-01-14T22:34:46Z",
      "pushed_at": "2026-01-14T22:37:49Z",
      "open_issues": 46,
      "owner": {
        "login": "curl",
        "avatar_url": "https://avatars.githubusercontent.com/u/16928085?v=4"
      },
      "readme": "<!--\nCopyright (C) Daniel Stenberg, <daniel@haxx.se>, et al.\n\nSPDX-License-Identifier: curl\n-->\n\n# [![curl logo](https://curl.se/logo/curl-logo.svg)](https://curl.se/)\n\ncurl is a command-line tool for transferring data from or to a server using\nURLs. It supports these protocols: DICT, FILE, FTP, FTPS, GOPHER, GOPHERS,\nHTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, MQTT, POP3, POP3S, RTMP, RTMPS, RTSP,\nSCP, SFTP, SMB, SMBS, SMTP, SMTPS, TELNET, TFTP, WS and WSS.\n\nLearn how to use curl by reading [the\nman page](https://curl.se/docs/manpage.html) or [everything\ncurl](https://everything.curl.dev/).\n\nFind out how to install curl by reading [the INSTALL\ndocument](https://curl.se/docs/install.html).\n\nlibcurl is the library curl is using to do its job. It is readily available to\nbe used by your software. Read [the libcurl\nman page](https://curl.se/libcurl/c/libcurl.html) to learn how.\n\n## Open Source\n\ncurl is Open Source and is distributed under an MIT-like\n[license](https://curl.se/docs/copyright.html).\n\n## Contact\n\nContact us on a suitable [mailing list](https://curl.se/mail/) or\nuse GitHub [issues](https://github.com/curl/curl/issues)/\n[pull requests](https://github.com/curl/curl/pulls)/\n[discussions](https://github.com/curl/curl/discussions).\n\nAll contributors to the project are listed in [the THANKS\ndocument](https://curl.se/docs/thanks.html).\n\n## Commercial support\n\nFor commercial support, maybe private and dedicated help with your problems or\napplications using (lib)curl visit [the support page](https://curl.se/support.html).\n\n## Website\n\nVisit the [curl website](https://curl.se/) for the latest news and downloads.\n\n## Source code\n\nDownload the latest source from the Git server:\n\n    git clone https://github.com/curl/curl\n\n## Security problems\n\nReport suspected security problems via [our HackerOne\npage](https://hackerone.com/curl) and not in public.\n\n## Backers\n\nThank you to all our backers :pray: [Become a backer](https://opencollective.com/curl#section-contribute).\n\n## Sponsors\n\nSupport this project by becoming a [sponsor](https://curl.se/sponsors.html).\n",
      "stars_today": 10
    },
    {
      "id": 483402734,
      "name": "nowinandroid",
      "full_name": "android/nowinandroid",
      "description": "A fully functional Android app built entirely with Kotlin and Jetpack Compose",
      "html_url": "https://github.com/android/nowinandroid",
      "stars": 20377,
      "forks": 4108,
      "language": "Kotlin",
      "topics": [
        "android",
        "jetpack-compose",
        "kotlin"
      ],
      "created_at": "2022-04-19T20:40:24Z",
      "updated_at": "2026-01-14T22:27:34Z",
      "pushed_at": "2026-01-14T23:38:19Z",
      "open_issues": 241,
      "owner": {
        "login": "android",
        "avatar_url": "https://avatars.githubusercontent.com/u/32689599?v=4"
      },
      "readme": "![Now in Android](docs/images/nia-splash.jpg \"Now in Android\")\n\n<a href=\"https://play.google.com/store/apps/details?id=com.google.samples.apps.nowinandroid\"><img src=\"https://play.google.com/intl/en_us/badges/static/images/badges/en_badge_web_generic.png\" height=\"70\"></a>\n\nNow in Android App\n==================\n\n**Learn how this app was designed and built in the [design case study](https://goo.gle/nia-figma), [architecture learning journey](docs/ArchitectureLearningJourney.md) and [modularization learning journey](docs/ModularizationLearningJourney.md).**\n\nThis is the repository for the [Now in Android](https://developer.android.com/series/now-in-android)\napp. It is a **work in progress** ğŸš§.\n\n**Now in Android** is a fully functional Android app built entirely with Kotlin and Jetpack Compose. It\nfollows Android design and development best practices and is intended to be a useful reference\nfor developers. As a running app, it's intended to help developers keep up-to-date with the world\nof Android development by providing regular news updates.\n\nThe app is currently in development. The `prodRelease` variant is [available on the Play Store](https://play.google.com/store/apps/details?id=com.google.samples.apps.nowinandroid).\n\n# Features\n\n**Now in Android** displays content from the\n[Now in Android](https://developer.android.com/series/now-in-android) series. Users can browse for\nlinks to recent videos, articles and other content. Users can also follow topics they are interested\nin, and be notified when new content is published which matches interests they are following.\n\n## Screenshots\n\n![Screenshot showing For You screen, Interests screen and Topic detail screen](docs/images/screenshots.png \"Screenshot showing For You screen, Interests screen and Topic detail screen\")\n\n# Development Environment\n\n**Now in Android** uses the Gradle build system and can be imported directly into Android Studio (make sure you are using the latest stable version available [here](https://developer.android.com/studio)). \n\nChange the run configuration to `app`.\n\n![image](https://user-images.githubusercontent.com/873212/210559920-ef4a40c5-c8e0-478b-bb00-4879a8cf184a.png)\n\nThe `demoDebug` and `demoRelease` build variants can be built and run (the `prod` variants use a backend server which is not currently publicly available).\n\n![image](https://user-images.githubusercontent.com/873212/210560507-44045dc5-b6d5-41ca-9746-f0f7acf22f8e.png)\n\nOnce you're up and running, you can refer to the learning journeys below to get a better\nunderstanding of which libraries and tools are being used, the reasoning behind the approaches to\nUI, testing, architecture and more, and how all of these different pieces of the project fit\ntogether to create a complete app.\n\n# Architecture\n\nThe **Now in Android** app follows the\n[official architecture guidance](https://developer.android.com/topic/architecture) \nand is described in detail in the\n[architecture learning journey](docs/ArchitectureLearningJourney.md).\n\n# Modularization\n\nThe **Now in Android** app has been fully modularized and you can find the detailed guidance and\ndescription of the modularization strategy used in\n[modularization learning journey](docs/ModularizationLearningJourney.md).\n\n# Build\n\nThe app contains the usual `debug` and `release` build variants. \n\nIn addition, the `benchmark` variant of `app` is used to test startup performance and generate a\nbaseline profile (see below for more information).\n\n`app-nia-catalog` is a standalone app that displays the list of components that are stylized for\n**Now in Android**.\n\nThe app also uses\n[product flavors](https://developer.android.com/studio/build/build-variants#product-flavors) to\ncontrol where content for the app should be loaded from.\n\nThe `demo` flavor uses static local data to allow immediate building and exploring of the UI.\n\nThe `prod` flavor makes real network calls to a backend server, providing up-to-date content. At \nthis time, there is not a public backend available.\n\nFor normal development use the `demoDebug` variant. For UI performance testing use the\n`demoRelease` variant. \n\n# Testing\n\nTo facilitate testing of components, **Now in Android** uses dependency injection with\n[Hilt](https://developer.android.com/training/dependency-injection/hilt-android).\n\nMost data layer components are defined as interfaces.\nThen, concrete implementations (with various dependencies) are bound to provide those interfaces to\nother components in the app.\nIn tests, **Now in Android** notably does _not_ use any mocking libraries.\nInstead, the production implementations can be replaced with test doubles using Hilt's testing APIs\n(or via manual constructor injection for `ViewModel` tests).\n\nThese test doubles implement the same interface as the production implementations and generally\nprovide a simplified (but still realistic) implementation with additional testing hooks.\nThis results in less brittle tests that may exercise more production code, instead of just verifying\nspecific calls against mocks.\n\nExamples:\n- In instrumentation tests, a temporary folder is used to store the user's preferences, which is\n  wiped after each test.\n  This allows using the real `DataStore` and exercising all related code, instead of mocking the \n  flow of data updates.\n\n- There are `Test` implementations of each repository, which implement the normal, full repository\n  interface and also provide test-only hooks.\n  `ViewModel` tests use these `Test` repositories, and thus can use the test-only hooks to\n  manipulate the state of the `Test` repository and verify the resulting behavior, instead of\n  checking that specific repository methods were called.\n\nTo run the tests execute the following gradle tasks: \n\n- `testDemoDebug` run all local tests against the `demoDebug` variant. Screenshot tests will fail\n(see below for explanation). To avoid this, run `recordRoborazziDemoDebug` prior to running unit tests.\n- `connectedDemoDebugAndroidTest` run all instrumented tests against the `demoDebug` variant. \n\n> [!NOTE]\n> You should not run `./gradlew test` or `./gradlew connectedAndroidTest` as this will execute \ntests against _all_ build variants which is both unnecessary and will result in failures as only the\n`demoDebug` variant is supported. No other variants have any tests (although this might change in future). \n\n## Screenshot tests\nA screenshot test takes a screenshot of a screen or a UI component within the app, and compares it \nwith a previously recorded screenshot which is known to be rendered correctly. \n\nFor example, Now in Android has [screenshot tests](https://github.com/android/nowinandroid/blob/main/app/src/testDemo/kotlin/com/google/samples/apps/nowinandroid/ui/NiaAppScreenSizesScreenshotTests.kt)\nto verify that the navigation is displayed correctly on different screen sizes \n([known correct screenshots](https://github.com/android/nowinandroid/tree/main/app/src/testDemo/screenshots)). \n\nNow In Android uses [Roborazzi](https://github.com/takahirom/roborazzi) to run screenshot tests\nof certain screens and UI components. When working with screenshot tests the following gradle tasks are useful:\n\n- `verifyRoborazziDemoDebug` run all screenshot tests, verifying the screenshots against the known\ncorrect screenshots.\n- `recordRoborazziDemoDebug` record new \"known correct\" screenshots. Use this command when you have\nmade changes to the UI and manually verified that they are rendered correctly. Screenshots will be\nstored in `modulename/src/test/screenshots`.\n- `compareRoborazziDemoDebug` create comparison images between failed tests and the known correct\nimages. These can also be found in `modulename/src/test/screenshots`. \n\n> [!NOTE]\n> **Note on failing screenshot tests**   \n> The known correct screenshots stored in this repository are recorded on CI using Linux. Other\nplatforms may (and probably will) generate slightly different images, making the screenshot tests fail. \nWhen working on a non-Linux platform, a workaround to this is to run `recordRoborazziDemoDebug` on the\n`main` branch before starting work. After making changes, `verifyRoborazziDemoDebug` will identify only\nlegitimate changes. \n\nFor more information about screenshot testing \n[check out this talk](https://www.droidcon.com/2023/11/15/easy-screenshot-testing-with-compose/).\n\n# UI\nThe app was designed using [Material 3 guidelines](https://m3.material.io/). Learn more about the design process and \nobtain the design files in the [Now in Android Material 3 Case Study](https://goo.gle/nia-figma) (design assets [also available as a PDF](docs/Now-In-Android-Design-File.pdf)).\n\nThe Screens and UI elements are built entirely using [Jetpack Compose](https://developer.android.com/jetpack/compose). \n\nThe app has two themes: \n\n- Dynamic color - uses colors based on the [user's current color theme](https://material.io/blog/announcing-material-you) (if supported)\n- Default theme - uses predefined colors when dynamic color is not supported\n\nEach theme also supports dark mode. \n\nThe app uses adaptive layouts to\n[support different screen sizes](https://developer.android.com/guide/topics/large-screens/support-different-screen-sizes).\n\nFind out more about the [UI architecture here](docs/ArchitectureLearningJourney.md#ui-layer).\n\n# Performance\n\n## Benchmarks\n\nFind all tests written using [`Macrobenchmark`](https://developer.android.com/topic/performance/benchmarking/macrobenchmark-overview)\nin the `benchmarks` module. This module also contains the test to generate the Baseline profile.\n\n## Baseline profiles\n\nThe baseline profile for this app is located at [`app/src/main/baseline-prof.txt`](app/src/main/baseline-prof.txt).\nIt contains rules that enable AOT compilation of the critical user path taken during app launch.\nFor more information on baseline profiles, read [this document](https://developer.android.com/studio/profile/baselineprofiles).\n\n> [!NOTE]\n> The baseline profile needs to be re-generated for release builds that touch code which changes app startup.\n\nTo generate the baseline profile, select the `benchmark` build variant and run the\n`BaselineProfileGenerator` benchmark test on an AOSP Android Emulator.\nThen copy the resulting baseline profile from the emulator to [`app/src/main/baseline-prof.txt`](app/src/main/baseline-prof.txt).\n\n## Compose compiler metrics\n\nRun the following command to get and analyze compose compiler metrics:\n\n```bash\n./gradlew assembleRelease -PenableComposeCompilerMetrics=true -PenableComposeCompilerReports=true\n```\n\nThe reports files will be added to [build/compose-reports](build/compose-reports). The metrics files will also be \nadded to [build/compose-metrics](build/compose-metrics).\n\nFor more information on Compose compiler metrics, see [this blog post](https://medium.com/androiddevelopers/jetpack-compose-stability-explained-79c10db270c8).\n\n# License\n\n**Now in Android** is distributed under the terms of the Apache License (Version 2.0). See the\n[license](LICENSE) for more information.\n",
      "stars_today": 10
    },
    {
      "id": 29759715,
      "name": "zstd",
      "full_name": "facebook/zstd",
      "description": "Zstandard - Fast real-time compression algorithm",
      "html_url": "https://github.com/facebook/zstd",
      "stars": 26395,
      "forks": 2378,
      "language": "C",
      "topics": [],
      "created_at": "2015-01-24T00:22:38Z",
      "updated_at": "2026-01-14T20:05:02Z",
      "pushed_at": "2025-12-22T05:05:20Z",
      "open_issues": 244,
      "owner": {
        "login": "facebook",
        "avatar_url": "https://avatars.githubusercontent.com/u/69631?v=4"
      },
      "readme": "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/facebook/zstd/dev/doc/images/zstd_logo86.png\" alt=\"Zstandard\"></p>\n\n__Zstandard__, or `zstd` as short version, is a fast lossless compression algorithm,\ntargeting real-time compression scenarios at zlib-level and better compression ratios.\nIt's backed by a very fast entropy stage, provided by [Huff0 and FSE library](https://github.com/Cyan4973/FiniteStateEntropy).\n\nZstandard's format is stable and documented in [RFC8878](https://datatracker.ietf.org/doc/html/rfc8878). Multiple independent implementations are already available.\nThis repository represents the reference implementation, provided as an open-source dual [BSD](LICENSE) OR [GPLv2](COPYING) licensed **C** library,\nand a command line utility producing and decoding `.zst`, `.gz`, `.xz` and `.lz4` files.\nShould your project require another programming language,\na list of known ports and bindings is provided on [Zstandard homepage](https://facebook.github.io/zstd/#other-languages).\n\n**Development branch status:**\n\n[![Build Status][travisDevBadge]][travisLink]\n[![Build status][CircleDevBadge]][CircleLink]\n[![Build status][CirrusDevBadge]][CirrusLink]\n[![Fuzzing Status][OSSFuzzBadge]][OSSFuzzLink]\n\n[travisDevBadge]: https://api.travis-ci.com/facebook/zstd.svg?branch=dev \"Continuous Integration test suite\"\n[travisLink]: https://travis-ci.com/facebook/zstd\n[CircleDevBadge]: https://circleci.com/gh/facebook/zstd/tree/dev.svg?style=shield \"Short test suite\"\n[CircleLink]: https://circleci.com/gh/facebook/zstd\n[CirrusDevBadge]: https://api.cirrus-ci.com/github/facebook/zstd.svg?branch=dev\n[CirrusLink]: https://cirrus-ci.com/github/facebook/zstd\n[OSSFuzzBadge]: https://oss-fuzz-build-logs.storage.googleapis.com/badges/zstd.svg\n[OSSFuzzLink]: https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:zstd\n\n## Benchmarks\n\nFor reference, several fast compression algorithms were tested and compared\non a desktop featuring a Core i7-9700K CPU @ 4.9GHz\nand running Ubuntu 24.04 (`Linux 6.8.0-53-generic`),\nusing [lzbench], an open-source in-memory benchmark by @inikep\ncompiled with [gcc] 14.2.0,\non the [Silesia compression corpus].\n\n[lzbench]: https://github.com/inikep/lzbench\n[Silesia compression corpus]: https://sun.aei.polsl.pl//~sdeor/index.php?page=silesia\n[gcc]: https://gcc.gnu.org/\n\n| Compressor name         | Ratio | Compression| Decompress.|\n| ---------------         | ------| -----------| ---------- |\n| **zstd 1.5.7 -1**       | 2.896 |   510 MB/s |  1550 MB/s |\n| brotli 1.1.0 -1         | 2.883 |   290 MB/s |   425 MB/s |\n| [zlib] 1.3.1 -1         | 2.743 |   105 MB/s |   390 MB/s |\n| **zstd 1.5.7 --fast=1** | 2.439 |   545 MB/s |  1850 MB/s |\n| quicklz 1.5.0 -1        | 2.238 |   520 MB/s |   750 MB/s |\n| **zstd 1.5.7 --fast=4** | 2.146 |   665 MB/s |  2050 MB/s |\n| lzo1x 2.10 -1           | 2.106 |   650 MB/s |   780 MB/s |\n| [lz4] 1.10.0            | 2.101 |   675 MB/s |  3850 MB/s |\n| snappy 1.2.1            | 2.089 |   520 MB/s |  1500 MB/s |\n| lzf 3.6 -1              | 2.077 |   410 MB/s |   820 MB/s |\n\n[zlib]: https://www.zlib.net/\n[lz4]: https://lz4.github.io/lz4/\n\nThe negative compression levels, specified with `--fast=#`,\noffer faster compression and decompression speed\nat the cost of compression ratio.\n\nZstd can also offer stronger compression ratios at the cost of compression speed.\nSpeed vs Compression trade-off is configurable by small increments.\nDecompression speed is preserved and remains roughly the same at all settings,\na property shared by most LZ compression algorithms, such as [zlib] or lzma.\n\nThe following tests were run\non a server running Linux Debian (`Linux version 4.14.0-3-amd64`)\nwith a Core i7-6700K CPU @ 4.0GHz,\nusing [lzbench], an open-source in-memory benchmark by @inikep\ncompiled with [gcc] 7.3.0,\non the [Silesia compression corpus].\n\nCompression Speed vs Ratio | Decompression Speed\n---------------------------|--------------------\n![Compression Speed vs Ratio](doc/images/CSpeed2.png \"Compression Speed vs Ratio\") | ![Decompression Speed](doc/images/DSpeed3.png \"Decompression Speed\")\n\nA few other algorithms can produce higher compression ratios at slower speeds, falling outside of the graph.\nFor a larger picture including slow modes, [click on this link](doc/images/DCspeed5.png).\n\n\n## The case for Small Data compression\n\nPrevious charts provide results applicable to typical file and stream scenarios (several MB). Small data comes with different perspectives.\n\nThe smaller the amount of data to compress, the more difficult it is to compress. This problem is common to all compression algorithms, and reason is, compression algorithms learn from past data how to compress future data. But at the beginning of a new data set, there is no \"past\" to build upon.\n\nTo solve this situation, Zstd offers a __training mode__, which can be used to tune the algorithm for a selected type of data.\nTraining Zstandard is achieved by providing it with a few samples (one file per sample). The result of this training is stored in a file called \"dictionary\", which must be loaded before compression and decompression.\nUsing this dictionary, the compression ratio achievable on small data improves dramatically.\n\nThe following example uses the `github-users` [sample set](https://github.com/facebook/zstd/releases/tag/v1.1.3), created from [github public API](https://developer.github.com/v3/users/#get-all-users).\nIt consists of roughly 10K records weighing about 1KB each.\n\nCompression Ratio | Compression Speed | Decompression Speed\n------------------|-------------------|--------------------\n![Compression Ratio](doc/images/dict-cr.png \"Compression Ratio\") | ![Compression Speed](doc/images/dict-cs.png \"Compression Speed\") | ![Decompression Speed](doc/images/dict-ds.png \"Decompression Speed\")\n\n\nThese compression gains are achieved while simultaneously providing _faster_ compression and decompression speeds.\n\nTraining works if there is some correlation in a family of small data samples. The more data-specific a dictionary is, the more efficient it is (there is no _universal dictionary_).\nHence, deploying one dictionary per type of data will provide the greatest benefits.\nDictionary gains are mostly effective in the first few KB. Then, the compression algorithm will gradually use previously decoded content to better compress the rest of the file.\n\n### Dictionary compression How To:\n\n1. Create the dictionary\n\n   `zstd --train FullPathToTrainingSet/* -o dictionaryName`\n\n2. Compress with dictionary\n\n   `zstd -D dictionaryName FILE`\n\n3. Decompress with dictionary\n\n   `zstd -D dictionaryName --decompress FILE.zst`\n\n\n## Build instructions\n\n`make` is the main build system of this project.\nIt is the reference, and other build systems are periodically updated to stay compatible.\nHowever, small drifts and feature differences can be present, since perfect synchronization is difficult.\nFor this reason, when your build system allows it, prefer employing `make`.\n\n### Makefile\n\nAssuming your system supports standard `make` (or `gmake`),\njust invoking `make` in root directory generates `zstd` cli at root,\nand also generates `libzstd` into `lib/`.\n\nOther standard targets include:\n- `make install` : install zstd cli, library and man pages\n- `make check` : run `zstd`, test its essential behavior on local platform\n\nThe `Makefile` follows the [GNU Standard Makefile conventions](https://www.gnu.org/prep/standards/html_node/Makefile-Conventions.html),\nallowing staged install, standard compilation flags, directory variables and command variables.\n\nFor advanced use cases, specialized flags which control binary generation and installation paths are documented\nin [`lib/README.md`](lib/README.md#modular-build) for the `libzstd` library\nand in [`programs/README.md`](programs/README.md#compilation-variables) for the `zstd` CLI.\n\n### cmake\n\nA `cmake` project generator is available for generating Makefiles or other build scripts\nto create the `zstd` binary as well as `libzstd` dynamic and static libraries.\nThe repository root now contains a minimal `CMakeLists.txt` that forwards to `build/cmake`,\nso you can configure the project with a standard `cmake -S .` invocation,\nwhile the historical `cmake -S build/cmake` entry point remains fully supported.\n\n```bash\ncmake -S . -B build-cmake\ncmake --build build-cmake\n```\n\nBy default, `CMAKE_BUILD_TYPE` is set to `Release`.\n\n#### Support for Fat (Universal2) Output\n\n`zstd` can be built and installed with support for both Apple Silicon (M1/M2) as well as Intel by using CMake's Universal2 support.\nTo perform a Fat/Universal2 build and install use the following commands:\n\n```bash\ncmake -S . -B build-cmake-debug -G Ninja -DCMAKE_OSX_ARCHITECTURES=\"x86_64;x86_64h;arm64\"\ncd build-cmake-debug\nninja\nsudo ninja install\n```\n\n### Meson\n\nA Meson project is provided within [`build/meson`](build/meson). Follow\nbuild instructions in that directory.\n\nYou can also take a look at [`.travis.yml`](.travis.yml) file for an\nexample about how Meson is used to build this project.\n\nNote that default build type is **release**.\n\n### VCPKG\nYou can build and install zstd [vcpkg](https://github.com/Microsoft/vcpkg/) dependency manager:\n\n    git clone https://github.com/Microsoft/vcpkg.git\n    cd vcpkg\n    ./bootstrap-vcpkg.sh\n    ./vcpkg integrate install\n    ./vcpkg install zstd\n\nThe zstd port in vcpkg is kept up to date by Microsoft team members and community contributors.\nIf the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.\n\n### Conan\n\nYou can install pre-built binaries for zstd or build it from source using [Conan](https://conan.io/). Use the following command:\n\n```bash\nconan install --requires=\"zstd/[*]\" --build=missing\n```\n\nThe zstd Conan recipe is kept up to date by Conan maintainers and community contributors.\nIf the version is out of date, please [create an issue or pull request](https://github.com/conan-io/conan-center-index) on the ConanCenterIndex repository.\n\n### Visual Studio (Windows)\n\nGoing into `build` directory, you will find additional possibilities:\n- Projects for Visual Studio 2008 and 2010.\n  + VS2010 project is compatible with VS2012, VS2013, VS2015 and VS2017.\n- Automated build scripts for Visual compiler by [@KrzysFR](https://github.com/KrzysFR), in `build/VS_scripts`,\n  which will build `zstd` cli and `libzstd` library without any need to open Visual Studio solution.\n- It is now recommended to generate Visual Studio solutions from `cmake`\n\n### Buck\n\nYou can build the zstd binary via buck by executing: `buck build programs:zstd` from the root of the repo.\nThe output binary will be in `buck-out/gen/programs/`.\n\n### Bazel\n\nYou can integrate zstd into your Bazel project by using the module hosted on the [Bazel Central Repository](https://registry.bazel.build/modules/zstd).\n\n## Testing\n\nYou can run quick local smoke tests by running `make check`.\nIf you can't use `make`, execute the `playTest.sh` script from the `src/tests` directory.\nTwo env variables `$ZSTD_BIN` and `$DATAGEN_BIN` are needed for the test script to locate the `zstd` and `datagen` binary.\nFor information on CI testing, please refer to `TESTING.md`.\n\n## Status\n\nZstandard is deployed within Meta and many other large cloud infrastructures,\nto compress humongous amounts of data in various formats and use cases.\nIt is also continuously fuzzed for security issues by Google's [oss-fuzz](https://github.com/google/oss-fuzz/tree/master/projects/zstd) program.\n\n## License\n\nZstandard is dual-licensed under [BSD](LICENSE) OR [GPLv2](COPYING).\n\n## Contributing\n\nThe `dev` branch is the one where all contributions are merged before reaching `release`.\nDirect commit to `release` are not permitted.\nFor more information, please read [CONTRIBUTING](CONTRIBUTING.md).\n",
      "stars_today": 10
    },
    {
      "id": 359952601,
      "name": "pgvector",
      "full_name": "pgvector/pgvector",
      "description": "Open-source vector similarity search for Postgres",
      "html_url": "https://github.com/pgvector/pgvector",
      "stars": 19248,
      "forks": 1027,
      "language": "C",
      "topics": [
        "approximate-nearest-neighbor-search",
        "nearest-neighbor-search"
      ],
      "created_at": "2021-04-20T21:13:52Z",
      "updated_at": "2026-01-15T00:55:03Z",
      "pushed_at": "2026-01-14T15:20:37Z",
      "open_issues": 13,
      "owner": {
        "login": "pgvector",
        "avatar_url": "https://avatars.githubusercontent.com/u/98363230?v=4"
      },
      "readme": "# pgvector\n\nOpen-source vector similarity search for Postgres\n\nStore your vectors with the rest of your data. Supports:\n\n- exact and approximate nearest neighbor search\n- single-precision, half-precision, binary, and sparse vectors\n- L2 distance, inner product, cosine distance, L1 distance, Hamming distance, and Jaccard distance\n- any [language](#languages) with a Postgres client\n\nPlus [ACID](https://en.wikipedia.org/wiki/ACID) compliance, point-in-time recovery, JOINs, and all of the other [great features](https://www.postgresql.org/about/) of Postgres\n\n[![Build Status](https://github.com/pgvector/pgvector/actions/workflows/build.yml/badge.svg)](https://github.com/pgvector/pgvector/actions)\n\n## Installation\n\n### Linux and Mac\n\nCompile and install the extension (supports Postgres 13+)\n\n```sh\ncd /tmp\ngit clone --branch v0.8.1 https://github.com/pgvector/pgvector.git\ncd pgvector\nmake\nmake install # may need sudo\n```\n\nSee the [installation notes](#installation-notes---linux-and-mac) if you run into issues\n\nYou can also install it with [Docker](#docker), [Homebrew](#homebrew), [PGXN](#pgxn), [APT](#apt), [Yum](#yum), [pkg](#pkg), [APK](#apk), or [conda-forge](#conda-forge), and it comes preinstalled with [Postgres.app](#postgresapp) and many [hosted providers](#hosted-postgres). There are also instructions for [GitHub Actions](https://github.com/pgvector/setup-pgvector).\n\n### Windows\n\nEnsure [C++ support in Visual Studio](https://learn.microsoft.com/en-us/cpp/build/building-on-the-command-line?view=msvc-170#download-and-install-the-tools) is installed and run `x64 Native Tools Command Prompt for VS [version]` as administrator. Then use `nmake` to build:\n\n```cmd\nset \"PGROOT=C:\\Program Files\\PostgreSQL\\18\"\ncd %TEMP%\ngit clone --branch v0.8.1 https://github.com/pgvector/pgvector.git\ncd pgvector\nnmake /F Makefile.win\nnmake /F Makefile.win install\n```\n\nSee the [installation notes](#installation-notes---windows) if you run into issues\n\nYou can also install it with [Docker](#docker) or [conda-forge](#conda-forge).\n\n## Getting Started\n\nEnable the extension (do this once in each database where you want to use it)\n\n```tsql\nCREATE EXTENSION vector;\n```\n\nCreate a vector column with 3 dimensions\n\n```sql\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(3));\n```\n\nInsert vectors\n\n```sql\nINSERT INTO items (embedding) VALUES ('[1,2,3]'), ('[4,5,6]');\n```\n\nGet the nearest neighbors by L2 distance\n\n```sql\nSELECT * FROM items ORDER BY embedding <-> '[3,1,2]' LIMIT 5;\n```\n\nAlso supports inner product (`<#>`), cosine distance (`<=>`), and L1 distance (`<+>`)\n\nNote: `<#>` returns the negative inner product since Postgres only supports `ASC` order index scans on operators\n\n## Storing\n\nCreate a new table with a vector column\n\n```sql\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(3));\n```\n\nOr add a vector column to an existing table\n\n```sql\nALTER TABLE items ADD COLUMN embedding vector(3);\n```\n\nAlso supports [half-precision](#half-precision-vectors), [binary](#binary-vectors), and [sparse](#sparse-vectors) vectors\n\nInsert vectors\n\n```sql\nINSERT INTO items (embedding) VALUES ('[1,2,3]'), ('[4,5,6]');\n```\n\nOr load vectors in bulk using `COPY` ([example](https://github.com/pgvector/pgvector-python/blob/master/examples/loading/example.py))\n\n```sql\nCOPY items (embedding) FROM STDIN WITH (FORMAT BINARY);\n```\n\nUpsert vectors\n\n```sql\nINSERT INTO items (id, embedding) VALUES (1, '[1,2,3]'), (2, '[4,5,6]')\n    ON CONFLICT (id) DO UPDATE SET embedding = EXCLUDED.embedding;\n```\n\nUpdate vectors\n\n```sql\nUPDATE items SET embedding = '[1,2,3]' WHERE id = 1;\n```\n\nDelete vectors\n\n```sql\nDELETE FROM items WHERE id = 1;\n```\n\n## Querying\n\nGet the nearest neighbors to a vector\n\n```sql\nSELECT * FROM items ORDER BY embedding <-> '[3,1,2]' LIMIT 5;\n```\n\nSupported distance functions are:\n\n- `<->` - L2 distance\n- `<#>` - (negative) inner product\n- `<=>` - cosine distance\n- `<+>` - L1 distance\n- `<~>` - Hamming distance (binary vectors)\n- `<%>` - Jaccard distance (binary vectors)\n\nGet the nearest neighbors to a row\n\n```sql\nSELECT * FROM items WHERE id != 1 ORDER BY embedding <-> (SELECT embedding FROM items WHERE id = 1) LIMIT 5;\n```\n\nGet rows within a certain distance\n\n```sql\nSELECT * FROM items WHERE embedding <-> '[3,1,2]' < 5;\n```\n\nNote: Combine with `ORDER BY` and `LIMIT` to use an index\n\n#### Distances\n\nGet the distance\n\n```sql\nSELECT embedding <-> '[3,1,2]' AS distance FROM items;\n```\n\nFor inner product, multiply by -1 (since `<#>` returns the negative inner product)\n\n```tsql\nSELECT (embedding <#> '[3,1,2]') * -1 AS inner_product FROM items;\n```\n\nFor cosine similarity, use 1 - cosine distance\n\n```sql\nSELECT 1 - (embedding <=> '[3,1,2]') AS cosine_similarity FROM items;\n```\n\n#### Aggregates\n\nAverage vectors\n\n```sql\nSELECT AVG(embedding) FROM items;\n```\n\nAverage groups of vectors\n\n```sql\nSELECT category_id, AVG(embedding) FROM items GROUP BY category_id;\n```\n\n## Indexing\n\nBy default, pgvector performs exact nearest neighbor search, which provides perfect recall.\n\nYou can add an index to use approximate nearest neighbor search, which trades some recall for speed. Unlike typical indexes, you will see different results for queries after adding an approximate index.\n\nSupported index types are:\n\n- [HNSW](#hnsw)\n- [IVFFlat](#ivfflat)\n\n## HNSW\n\nAn HNSW index creates a multilayer graph. It has better query performance than IVFFlat (in terms of speed-recall tradeoff), but has slower build times and uses more memory. Also, an index can be created without any data in the table since there isnâ€™t a training step like IVFFlat.\n\nAdd an index for each distance function you want to use.\n\nL2 distance\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_l2_ops);\n```\n\nNote: Use `halfvec_l2_ops` for `halfvec` and `sparsevec_l2_ops` for `sparsevec` (and similar with the other distance functions)\n\nInner product\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_ip_ops);\n```\n\nCosine distance\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_cosine_ops);\n```\n\nL1 distance\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_l1_ops);\n```\n\nHamming distance\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding bit_hamming_ops);\n```\n\nJaccard distance\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding bit_jaccard_ops);\n```\n\nSupported types are:\n\n- `vector` - up to 2,000 dimensions\n- `halfvec` - up to 4,000 dimensions\n- `bit` - up to 64,000 dimensions\n- `sparsevec` - up to 1,000 non-zero elements\n\n### Index Options\n\nSpecify HNSW parameters\n\n- `m` - the max number of connections per layer (16 by default)\n- `ef_construction` - the size of the dynamic candidate list for constructing the graph (64 by default)\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_l2_ops) WITH (m = 16, ef_construction = 64);\n```\n\nA higher value of `ef_construction` provides better recall at the cost of index build time / insert speed.\n\n### Query Options\n\nSpecify the size of the dynamic candidate list for search (40 by default)\n\n```sql\nSET hnsw.ef_search = 100;\n```\n\nA higher value provides better recall at the cost of speed.\n\nUse `SET LOCAL` inside a transaction to set it for a single query\n\n```sql\nBEGIN;\nSET LOCAL hnsw.ef_search = 100;\nSELECT ...\nCOMMIT;\n```\n\n### Index Build Time\n\nIndexes build significantly faster when the graph fits into `maintenance_work_mem`\n\n```sql\nSET maintenance_work_mem = '8GB';\n```\n\nA notice is shown when the graph no longer fits\n\n```text\nNOTICE:  hnsw graph no longer fits into maintenance_work_mem after 100000 tuples\nDETAIL:  Building will take significantly more time.\nHINT:  Increase maintenance_work_mem to speed up builds.\n```\n\nNote: Do not set `maintenance_work_mem` so high that it exhausts the memory on the server\n\nLike other index types, itâ€™s faster to create an index after loading your initial data\n\nYou can also speed up index creation by increasing the number of parallel workers (2 by default)\n\n```sql\nSET max_parallel_maintenance_workers = 7; -- plus leader\n```\n\nFor a large number of workers, you may need to increase `max_parallel_workers` (8 by default)\n\nThe [index options](#index-options) also have a significant impact on build time (use the defaults unless seeing low recall)\n\n### Indexing Progress\n\nCheck [indexing progress](https://www.postgresql.org/docs/current/progress-reporting.html#CREATE-INDEX-PROGRESS-REPORTING)\n\n```sql\nSELECT phase, round(100.0 * blocks_done / nullif(blocks_total, 0), 1) AS \"%\" FROM pg_stat_progress_create_index;\n```\n\nThe phases for HNSW are:\n\n1. `initializing`\n2. `loading tuples`\n\n## IVFFlat\n\nAn IVFFlat index divides vectors into lists, and then searches a subset of those lists that are closest to the query vector. It has faster build times and uses less memory than HNSW, but has lower query performance (in terms of speed-recall tradeoff).\n\nThree keys to achieving good recall are:\n\n1. Create the index *after* the table has some data\n2. Choose an appropriate number of lists - a good place to start is `rows / 1000` for up to 1M rows and `sqrt(rows)` for over 1M rows\n3. When querying, specify an appropriate number of [probes](#query-options) (higher is better for recall, lower is better for speed) - a good place to start is `sqrt(lists)`\n\nAdd an index for each distance function you want to use.\n\nL2 distance\n\n```sql\nCREATE INDEX ON items USING ivfflat (embedding vector_l2_ops) WITH (lists = 100);\n```\n\nNote: Use `halfvec_l2_ops` for `halfvec` (and similar with the other distance functions)\n\nInner product\n\n```sql\nCREATE INDEX ON items USING ivfflat (embedding vector_ip_ops) WITH (lists = 100);\n```\n\nCosine distance\n\n```sql\nCREATE INDEX ON items USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n```\n\nHamming distance\n\n```sql\nCREATE INDEX ON items USING ivfflat (embedding bit_hamming_ops) WITH (lists = 100);\n```\n\nSupported types are:\n\n- `vector` - up to 2,000 dimensions\n- `halfvec` - up to 4,000 dimensions\n- `bit` - up to 64,000 dimensions\n\n### Query Options\n\nSpecify the number of probes (1 by default)\n\n```sql\nSET ivfflat.probes = 10;\n```\n\nA higher value provides better recall at the cost of speed, and it can be set to the number of lists for exact nearest neighbor search (at which point the planner wonâ€™t use the index)\n\nUse `SET LOCAL` inside a transaction to set it for a single query\n\n```sql\nBEGIN;\nSET LOCAL ivfflat.probes = 10;\nSELECT ...\nCOMMIT;\n```\n\n### Index Build Time\n\nSpeed up index creation on large tables by increasing the number of parallel workers (2 by default)\n\n```sql\nSET max_parallel_maintenance_workers = 7; -- plus leader\n```\n\nFor a large number of workers, you may also need to increase `max_parallel_workers` (8 by default)\n\n### Indexing Progress\n\nCheck [indexing progress](https://www.postgresql.org/docs/current/progress-reporting.html#CREATE-INDEX-PROGRESS-REPORTING)\n\n```sql\nSELECT phase, round(100.0 * tuples_done / nullif(tuples_total, 0), 1) AS \"%\" FROM pg_stat_progress_create_index;\n```\n\nThe phases for IVFFlat are:\n\n1. `initializing`\n2. `performing k-means`\n3. `assigning tuples`\n4. `loading tuples`\n\nNote: `%` is only populated during the `loading tuples` phase\n\n## Filtering\n\nThere are a few ways to index nearest neighbor queries with a `WHERE` clause.\n\n```sql\nSELECT * FROM items WHERE category_id = 123 ORDER BY embedding <-> '[3,1,2]' LIMIT 5;\n```\n\nA good place to start is creating an index on the filter column. This can provide fast, exact nearest neighbor search in many cases. Postgres has a number of [index types](https://www.postgresql.org/docs/current/indexes-types.html) for this: B-tree (default), hash, GiST, SP-GiST, GIN, and BRIN.\n\n```sql\nCREATE INDEX ON items (category_id);\n```\n\nFor multiple columns, consider a [multicolumn index](https://www.postgresql.org/docs/current/indexes-multicolumn.html).\n\n```sql\nCREATE INDEX ON items (location_id, category_id);\n```\n\nExact indexes work well for conditions that match a low percentage of rows. Otherwise, [approximate indexes](#indexing) can work better.\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_l2_ops);\n```\n\nWith approximate indexes, filtering is applied *after* the index is scanned. If a condition matches 10% of rows, with HNSW and the default `hnsw.ef_search` of 40, only 4 rows will match on average. For more rows, increase `hnsw.ef_search`.\n\n```sql\nSET hnsw.ef_search = 200;\n```\n\nStarting with 0.8.0, you can enable [iterative index scans](#iterative-index-scans), which will automatically scan more of the index when needed.\n\n```sql\nSET hnsw.iterative_scan = strict_order;\n```\n\nIf filtering by only a few distinct values, consider [partial indexing](https://www.postgresql.org/docs/current/indexes-partial.html).\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_l2_ops) WHERE (category_id = 123);\n```\n\nIf filtering by many different values, consider [partitioning](https://www.postgresql.org/docs/current/ddl-partitioning.html).\n\n```sql\nCREATE TABLE items (embedding vector(3), category_id int) PARTITION BY LIST(category_id);\n```\n\n## Iterative Index Scans\n\nWith approximate indexes, queries with filtering can return less results since filtering is applied *after* the index is scanned. Starting with 0.8.0, you can enable iterative index scans, which will automatically scan more of the index until enough results are found (or it reaches `hnsw.max_scan_tuples` or `ivfflat.max_probes`).\n\nIterative scans can use strict or relaxed ordering.\n\nStrict ensures results are in the exact order by distance\n\n```sql\nSET hnsw.iterative_scan = strict_order;\n```\n\nRelaxed allows results to be slightly out of order by distance, but provides better recall\n\n```sql\nSET hnsw.iterative_scan = relaxed_order;\n# or\nSET ivfflat.iterative_scan = relaxed_order;\n```\n\nWith relaxed ordering, you can use a [materialized CTE](https://www.postgresql.org/docs/current/queries-with.html#QUERIES-WITH-CTE-MATERIALIZATION) to get strict ordering\n\n```sql\nWITH relaxed_results AS MATERIALIZED (\n    SELECT id, embedding <-> '[1,2,3]' AS distance FROM items WHERE category_id = 123 ORDER BY distance LIMIT 5\n) SELECT * FROM relaxed_results ORDER BY distance + 0;\n```\n\nNote: `+ 0` is needed for Postgres 17+\n\nFor queries that filter by distance, use a materialized CTE and place the distance filter outside of it for best performance (due to the [current behavior](https://www.postgresql.org/message-id/flat/CAOdR5yGUoMQ6j7M5hNUXrySzaqZVGf_Ne%2B8fwZMRKTFxU1nbJg%40mail.gmail.com) of the Postgres executor)\n\n```sql\nWITH nearest_results AS MATERIALIZED (\n    SELECT id, embedding <-> '[1,2,3]' AS distance FROM items ORDER BY distance LIMIT 5\n) SELECT * FROM nearest_results WHERE distance < 5 ORDER BY distance;\n```\n\nNote: Place any other filters inside the CTE\n\n### Iterative Scan Options\n\nSince scanning a large portion of an approximate index is expensive, there are options to control when a scan ends.\n\n#### HNSW\n\nSpecify the max number of tuples to visit (20,000 by default)\n\n```sql\nSET hnsw.max_scan_tuples = 20000;\n```\n\nNote: This is approximate and does not affect the initial scan\n\nSpecify the max amount of memory to use, as a multiple of `work_mem` (1 by default)\n\n```sql\nSET hnsw.scan_mem_multiplier = 2;\n```\n\nNote: Try increasing this if increasing `hnsw.max_scan_tuples` does not improve recall\n\n#### IVFFlat\n\nSpecify the max number of probes\n\n```sql\nSET ivfflat.max_probes = 100;\n```\n\nNote: If this is lower than `ivfflat.probes`, `ivfflat.probes` will be used\n\n## Half-Precision Vectors\n\nUse the `halfvec` type to store half-precision vectors\n\n```sql\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding halfvec(3));\n```\n\n## Half-Precision Indexing\n\nIndex vectors at half precision for smaller indexes\n\n```sql\nCREATE INDEX ON items USING hnsw ((embedding::halfvec(3)) halfvec_l2_ops);\n```\n\nGet the nearest neighbors\n\n```sql\nSELECT * FROM items ORDER BY embedding::halfvec(3) <-> '[1,2,3]' LIMIT 5;\n```\n\n## Binary Vectors\n\nUse the `bit` type to store binary vectors ([example](https://github.com/pgvector/pgvector-python/blob/master/examples/imagehash/example.py))\n\n```sql\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding bit(3));\nINSERT INTO items (embedding) VALUES ('000'), ('111');\n```\n\nGet the nearest neighbors by Hamming distance\n\n```sql\nSELECT * FROM items ORDER BY embedding <~> '101' LIMIT 5;\n```\n\nAlso supports Jaccard distance (`<%>`)\n\n## Binary Quantization\n\nUse expression indexing for binary quantization\n\n```sql\nCREATE INDEX ON items USING hnsw ((binary_quantize(embedding)::bit(3)) bit_hamming_ops);\n```\n\nGet the nearest neighbors by Hamming distance\n\n```sql\nSELECT * FROM items ORDER BY binary_quantize(embedding)::bit(3) <~> binary_quantize('[1,-2,3]') LIMIT 5;\n```\n\nRe-rank by the original vectors for better recall\n\n```sql\nSELECT * FROM (\n    SELECT * FROM items ORDER BY binary_quantize(embedding)::bit(3) <~> binary_quantize('[1,-2,3]') LIMIT 20\n) ORDER BY embedding <=> '[1,-2,3]' LIMIT 5;\n```\n\n## Sparse Vectors\n\nUse the `sparsevec` type to store sparse vectors\n\n```sql\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding sparsevec(5));\n```\n\nInsert vectors\n\n```sql\nINSERT INTO items (embedding) VALUES ('{1:1,3:2,5:3}/5'), ('{1:4,3:5,5:6}/5');\n```\n\nThe format is `{index1:value1,index2:value2}/dimensions` and indices start at 1 like SQL arrays\n\nGet the nearest neighbors by L2 distance\n\n```sql\nSELECT * FROM items ORDER BY embedding <-> '{1:3,3:1,5:2}/5' LIMIT 5;\n```\n\n## Hybrid Search\n\nUse together with Postgres [full-text search](https://www.postgresql.org/docs/current/textsearch-intro.html) for hybrid search.\n\n```sql\nSELECT id, content FROM items, plainto_tsquery('hello search') query\n    WHERE textsearch @@ query ORDER BY ts_rank_cd(textsearch, query) DESC LIMIT 5;\n```\n\nYou can use [Reciprocal Rank Fusion](https://github.com/pgvector/pgvector-python/blob/master/examples/hybrid_search/rrf.py) or a [cross-encoder](https://github.com/pgvector/pgvector-python/blob/master/examples/hybrid_search/cross_encoder.py) to combine results.\n\n## Indexing Subvectors\n\nUse expression indexing to index subvectors\n\n```sql\nCREATE INDEX ON items USING hnsw ((subvector(embedding, 1, 3)::vector(3)) vector_cosine_ops);\n```\n\nGet the nearest neighbors by cosine distance\n\n```sql\nSELECT * FROM items ORDER BY subvector(embedding, 1, 3)::vector(3) <=> subvector('[1,2,3,4,5]'::vector, 1, 3) LIMIT 5;\n```\n\nRe-rank by the full vectors for better recall\n\n```sql\nSELECT * FROM (\n    SELECT * FROM items ORDER BY subvector(embedding, 1, 3)::vector(3) <=> subvector('[1,2,3,4,5]'::vector, 1, 3) LIMIT 20\n) ORDER BY embedding <=> '[1,2,3,4,5]' LIMIT 5;\n```\n\n## Performance\n\n### Tuning\n\nUse a tool like [PgTune](https://pgtune.leopard.in.ua/) to set initial values for Postgres server parameters. For instance, `shared_buffers` should typically be 25% of the serverâ€™s memory. You can find the config file with:\n\n```sql\nSHOW config_file;\n```\n\nAnd check individual settings with:\n\n```sql\nSHOW shared_buffers;\n```\n\nBe sure to restart Postgres for changes to take effect.\n\n### Loading\n\nUse `COPY` for bulk loading data ([example](https://github.com/pgvector/pgvector-python/blob/master/examples/loading/example.py)).\n\n```sql\nCOPY items (embedding) FROM STDIN WITH (FORMAT BINARY);\n```\n\nAdd any indexes *after* loading the initial data for best performance.\n\n### Indexing\n\nSee index build time for [HNSW](#index-build-time) and [IVFFlat](#index-build-time-1).\n\nIn production environments, create indexes concurrently to avoid blocking writes.\n\n```sql\nCREATE INDEX CONCURRENTLY ...\n```\n\n### Querying\n\nUse `EXPLAIN (ANALYZE, BUFFERS)` to debug performance.\n\n```sql\nEXPLAIN (ANALYZE, BUFFERS) SELECT * FROM items ORDER BY embedding <-> '[3,1,2]' LIMIT 5;\n```\n\n#### Exact Search\n\nTo speed up queries without an index, increase `max_parallel_workers_per_gather`.\n\n```sql\nSET max_parallel_workers_per_gather = 4;\n```\n\nIf vectors are normalized to length 1 (like [OpenAI embeddings](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use)), use inner product for best performance.\n\n```tsql\nSELECT * FROM items ORDER BY embedding <#> '[3,1,2]' LIMIT 5;\n```\n\n#### Approximate Search\n\nTo speed up queries with an IVFFlat index, increase the number of inverted lists (at the expense of recall).\n\n```sql\nCREATE INDEX ON items USING ivfflat (embedding vector_l2_ops) WITH (lists = 1000);\n```\n\n### Vacuuming\n\nVacuuming can take a while for HNSW indexes. Speed it up by reindexing first.\n\n```sql\nREINDEX INDEX CONCURRENTLY index_name;\nVACUUM table_name;\n```\n\n## Monitoring\n\nMonitor performance with [pg_stat_statements](https://www.postgresql.org/docs/current/pgstatstatements.html) (be sure to add it to `shared_preload_libraries`).\n\n```sql\nCREATE EXTENSION pg_stat_statements;\n```\n\nGet the most time-consuming queries with:\n\n```sql\nSELECT query, calls, ROUND((total_plan_time + total_exec_time) / calls) AS avg_time_ms,\n    ROUND((total_plan_time + total_exec_time) / 60000) AS total_time_min\n    FROM pg_stat_statements ORDER BY total_plan_time + total_exec_time DESC LIMIT 20;\n```\n\nMonitor recall by comparing results from approximate search with exact search.\n\n```sql\nBEGIN;\nSET LOCAL enable_indexscan = off; -- use exact search\nSELECT ...\nCOMMIT;\n```\n\n## Scaling\n\nScale pgvector the same way you scale Postgres.\n\nScale vertically by increasing memory, CPU, and storage on a single instance. Use existing tools to [tune parameters](#tuning) and [monitor performance](#monitoring).\n\nScale horizontally with [replicas](https://www.postgresql.org/docs/current/hot-standby.html), or use [Citus](https://github.com/citusdata/citus) or another approach for sharding ([example](https://github.com/pgvector/pgvector-python/blob/master/examples/citus/example.py)).\n\n## Languages\n\nUse pgvector from any language with a Postgres client. You can even generate and store vectors in one language and query them in another.\n\nLanguage | Libraries / Examples\n--- | ---\nAda | [pgvector-ada](https://github.com/pgvector/pgvector-ada)\nAlgol | [pgvector-algol](https://github.com/pgvector/pgvector-algol)\nC | [pgvector-c](https://github.com/pgvector/pgvector-c)\nC++ | [pgvector-cpp](https://github.com/pgvector/pgvector-cpp)\nC#, F#, Visual Basic | [pgvector-dotnet](https://github.com/pgvector/pgvector-dotnet)\nCOBOL | [pgvector-cobol](https://github.com/pgvector/pgvector-cobol)\nCrystal | [pgvector-crystal](https://github.com/pgvector/pgvector-crystal)\nD | [pgvector-d](https://github.com/pgvector/pgvector-d)\nDart | [pgvector-dart](https://github.com/pgvector/pgvector-dart)\nElixir | [pgvector-elixir](https://github.com/pgvector/pgvector-elixir)\nErlang | [pgvector-erlang](https://github.com/pgvector/pgvector-erlang)\nFortran | [pgvector-fortran](https://github.com/pgvector/pgvector-fortran)\nGleam | [pgvector-gleam](https://github.com/pgvector/pgvector-gleam)\nGo | [pgvector-go](https://github.com/pgvector/pgvector-go)\nHaskell | [pgvector-haskell](https://github.com/pgvector/pgvector-haskell)\nJava, Kotlin, Groovy, Scala | [pgvector-java](https://github.com/pgvector/pgvector-java)\nJavaScript, TypeScript | [pgvector-node](https://github.com/pgvector/pgvector-node)\nJulia | [Pgvector.jl](https://github.com/pgvector/Pgvector.jl)\nLisp | [pgvector-lisp](https://github.com/pgvector/pgvector-lisp)\nLua | [pgvector-lua](https://github.com/pgvector/pgvector-lua)\nNim | [pgvector-nim](https://github.com/pgvector/pgvector-nim)\nOCaml | [pgvector-ocaml](https://github.com/pgvector/pgvector-ocaml)\nPascal | [pgvector-pascal](https://github.com/pgvector/pgvector-pascal)\nPerl | [pgvector-perl](https://github.com/pgvector/pgvector-perl)\nPHP | [pgvector-php](https://github.com/pgvector/pgvector-php)\nProlog | [pgvector-prolog](https://github.com/pgvector/pgvector-prolog)\nPython | [pgvector-python](https://github.com/pgvector/pgvector-python)\nR | [pgvector-r](https://github.com/pgvector/pgvector-r)\nRacket | [pgvector-racket](https://github.com/pgvector/pgvector-racket)\nRaku | [pgvector-raku](https://github.com/pgvector/pgvector-raku)\nRuby | [pgvector-ruby](https://github.com/pgvector/pgvector-ruby), [Neighbor](https://github.com/ankane/neighbor)\nRust | [pgvector-rust](https://github.com/pgvector/pgvector-rust)\nSwift | [pgvector-swift](https://github.com/pgvector/pgvector-swift)\nTcl | [pgvector-tcl](https://github.com/pgvector/pgvector-tcl)\nZig | [pgvector-zig](https://github.com/pgvector/pgvector-zig)\n\n## Frequently Asked Questions\n\n#### How many vectors can be stored in a single table?\n\nA non-partitioned table has a limit of 32 TB by default in Postgres. A partitioned table can have thousands of partitions of that size.\n\n#### Is replication supported?\n\nYes, pgvector uses the write-ahead log (WAL), which allows for replication and point-in-time recovery.\n\n#### What if I want to index vectors with more than 2,000 dimensions?\n\nYou can use [half-precision vectors](#half-precision-vectors) or [half-precision indexing](#half-precision-indexing) to index up to 4,000 dimensions or [binary quantization](#binary-quantization) to index up to 64,000 dimensions. Other options are [indexing subvectors](#indexing-subvectors) (for models that support it) or [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction).\n\n#### Can I store vectors with different dimensions in the same column?\n\nYou can use `vector` as the type (instead of `vector(n)`).\n\n```sql\nCREATE TABLE embeddings (model_id bigint, item_id bigint, embedding vector, PRIMARY KEY (model_id, item_id));\n```\n\nHowever, you can only create indexes on rows with the same number of dimensions (using [expression](https://www.postgresql.org/docs/current/indexes-expressional.html) and [partial](https://www.postgresql.org/docs/current/indexes-partial.html) indexing):\n\n```sql\nCREATE INDEX ON embeddings USING hnsw ((embedding::vector(3)) vector_l2_ops) WHERE (model_id = 123);\n```\n\nand query with:\n\n```sql\nSELECT * FROM embeddings WHERE model_id = 123 ORDER BY embedding::vector(3) <-> '[3,1,2]' LIMIT 5;\n```\n\n#### Can I store vectors with more precision?\n\nYou can use the `double precision[]` or `numeric[]` type to store vectors with more precision.\n\n```sql\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding double precision[]);\n\n-- use {} instead of [] for Postgres arrays\nINSERT INTO items (embedding) VALUES ('{1,2,3}'), ('{4,5,6}');\n```\n\nOptionally, add a [check constraint](https://www.postgresql.org/docs/current/ddl-constraints.html) to ensure data can be converted to the `vector` type and has the expected dimensions.\n\n```sql\nALTER TABLE items ADD CHECK (vector_dims(embedding::vector) = 3);\n```\n\nUse [expression indexing](https://www.postgresql.org/docs/current/indexes-expressional.html) to index (at a lower precision):\n\n```sql\nCREATE INDEX ON items USING hnsw ((embedding::vector(3)) vector_l2_ops);\n```\n\nand query with:\n\n```sql\nSELECT * FROM items ORDER BY embedding::vector(3) <-> '[3,1,2]' LIMIT 5;\n```\n\n#### Do indexes need to fit into memory?\n\nNo, but like other index types, youâ€™ll likely see better performance if they do. You can get the size of an index with:\n\n```sql\nSELECT pg_size_pretty(pg_relation_size('index_name'));\n```\n\n## Troubleshooting\n\n#### Why isnâ€™t a query using an index?\n\nThe query needs to have an `ORDER BY` and `LIMIT`, and the `ORDER BY` must be the result of a distance operator (not an expression) in ascending order.\n\n```sql\n-- index\nORDER BY embedding <=> '[3,1,2]' LIMIT 5;\n\n-- no index\nORDER BY 1 - (embedding <=> '[3,1,2]') DESC LIMIT 5;\n```\n\nYou can encourage the planner to use an index for a query with:\n\n```sql\nBEGIN;\nSET LOCAL enable_seqscan = off;\nSELECT ...\nCOMMIT;\n```\n\nAlso, if the table is small, a table scan may be faster.\n\n#### Why isnâ€™t a query using a parallel table scan?\n\nThe planner doesnâ€™t consider [out-of-line storage](https://www.postgresql.org/docs/current/storage-toast.html) in cost estimates, which can make a serial scan look cheaper. You can reduce the cost of a parallel scan for a query with:\n\n```sql\nBEGIN;\nSET LOCAL min_parallel_table_scan_size = 1;\nSET LOCAL parallel_setup_cost = 1;\nSELECT ...\nCOMMIT;\n```\n\nor choose to store vectors inline:\n\n```sql\nALTER TABLE items ALTER COLUMN embedding SET STORAGE PLAIN;\n```\n\n#### Why are there less results for a query after adding an HNSW index?\n\nResults are limited by the size of the dynamic candidate list (`hnsw.ef_search`), which is 40 by default. There may be even less results due to dead tuples or filtering conditions in the query. Enabling [iterative index scans](#iterative-index-scans) can help address this.\n\nAlso, note that `NULL` vectors are not indexed (as well as zero vectors for cosine distance).\n\n#### Why are there less results for a query after adding an IVFFlat index?\n\nThe index was likely created with too little data for the number of lists. Drop the index until the table has more data.\n\n```sql\nDROP INDEX index_name;\n```\n\nResults can also be limited by the number of probes (`ivfflat.probes`). Enabling [iterative index scans](#iterative-index-scans) can address this.\n\nAlso, note that `NULL` vectors are not indexed (as well as zero vectors for cosine distance).\n\n## Reference\n\n- [Vector](#vector-type)\n- [Halfvec](#halfvec-type)\n- [Bit](#bit-type)\n- [Sparsevec](#sparsevec-type)\n\n### Vector Type\n\nEach vector takes `4 * dimensions + 8` bytes of storage. Each element is a single-precision floating-point number (like the `real` type in Postgres), and all elements must be finite (no `NaN`, `Infinity` or `-Infinity`). Vectors can have up to 16,000 dimensions.\n\n### Vector Operators\n\nOperator | Description | Added\n--- | --- | ---\n\\+ | element-wise addition |\n\\- | element-wise subtraction |\n\\* | element-wise multiplication | 0.5.0\n\\|\\| | concatenate | 0.7.0\n<-> | Euclidean distance |\n<#> | negative inner product |\n<=> | cosine distance |\n<+> | taxicab distance | 0.7.0\n\n### Vector Functions\n\nFunction | Description | Added\n--- | --- | ---\nbinary_quantize(vector) â†’ bit | binary quantize | 0.7.0\ncosine_distance(vector, vector) â†’ double precision | cosine distance |\ninner_product(vector, vector) â†’ double precision | inner product |\nl1_distance(vector, vector) â†’ double precision | taxicab distance | 0.5.0\nl2_distance(vector, vector) â†’ double precision | Euclidean distance |\nl2_normalize(vector) â†’ vector | Normalize with Euclidean norm | 0.7.0\nsubvector(vector, integer, integer) â†’ vector | subvector | 0.7.0\nvector_dims(vector) â†’ integer | number of dimensions |\nvector_norm(vector) â†’ double precision | Euclidean norm |\n\n### Vector Aggregate Functions\n\nFunction | Description | Added\n--- | --- | ---\navg(vector) â†’ vector | average |\nsum(vector) â†’ vector | sum | 0.5.0\n\n### Halfvec Type\n\nEach half vector takes `2 * dimensions + 8` bytes of storage. Each element is a half-precision floating-point number, and all elements must be finite (no `NaN`, `Infinity` or `-Infinity`). Half vectors can have up to 16,000 dimensions.\n\n### Halfvec Operators\n\nOperator | Description | Added\n--- | --- | ---\n\\+ | element-wise addition | 0.7.0\n\\- | element-wise subtraction | 0.7.0\n\\* | element-wise multiplication | 0.7.0\n\\|\\| | concatenate | 0.7.0\n<-> | Euclidean distance | 0.7.0\n<#> | negative inner product | 0.7.0\n<=> | cosine distance | 0.7.0\n<+> | taxicab distance | 0.7.0\n\n### Halfvec Functions\n\nFunction | Description | Added\n--- | --- | ---\nbinary_quantize(halfvec) â†’ bit | binary quantize | 0.7.0\ncosine_distance(halfvec, halfvec) â†’ double precision | cosine distance | 0.7.0\ninner_product(halfvec, halfvec) â†’ double precision | inner product | 0.7.0\nl1_distance(halfvec, halfvec) â†’ double precision | taxicab distance | 0.7.0\nl2_distance(halfvec, halfvec) â†’ double precision | Euclidean distance | 0.7.0\nl2_norm(halfvec) â†’ double precision | Euclidean norm | 0.7.0\nl2_normalize(halfvec) â†’ halfvec | Normalize with Euclidean norm | 0.7.0\nsubvector(halfvec, integer, integer) â†’ halfvec | subvector | 0.7.0\nvector_dims(halfvec) â†’ integer | number of dimensions | 0.7.0\n\n### Halfvec Aggregate Functions\n\nFunction | Description | Added\n--- | --- | ---\navg(halfvec) â†’ halfvec | average | 0.7.0\nsum(halfvec) â†’ halfvec | sum | 0.7.0\n\n### Bit Type\n\nEach bit vector takes `dimensions / 8 + 8` bytes of storage. See the [Postgres docs](https://www.postgresql.org/docs/current/datatype-bit.html) for more info.\n\n### Bit Operators\n\nOperator | Description | Added\n--- | --- | ---\n<~> | Hamming distance | 0.7.0\n<%> | Jaccard distance | 0.7.0\n\n### Bit Functions\n\nFunction | Description | Added\n--- | --- | ---\nhamming_distance(bit, bit) â†’ double precision | Hamming distance | 0.7.0\njaccard_distance(bit, bit) â†’ double precision | Jaccard distance | 0.7.0\n\n### Sparsevec Type\n\nEach sparse vector takes `8 * non-zero elements + 16` bytes of storage. Each element is a single-precision floating-point number, and all elements must be finite (no `NaN`, `Infinity` or `-Infinity`). Sparse vectors can have up to 16,000 non-zero elements.\n\n### Sparsevec Operators\n\nOperator | Description | Added\n--- | --- | ---\n<-> | Euclidean distance | 0.7.0\n<#> | negative inner product | 0.7.0\n<=> | cosine distance | 0.7.0\n<+> | taxicab distance | 0.7.0\n\n### Sparsevec Functions\n\nFunction | Description | Added\n--- | --- | ---\ncosine_distance(sparsevec, sparsevec) â†’ double precision | cosine distance | 0.7.0\ninner_product(sparsevec, sparsevec) â†’ double precision | inner product | 0.7.0\nl1_distance(sparsevec, sparsevec) â†’ double precision | taxicab distance | 0.7.0\nl2_distance(sparsevec, sparsevec) â†’ double precision | Euclidean distance | 0.7.0\nl2_norm(sparsevec) â†’ double precision | Euclidean norm | 0.7.0\nl2_normalize(sparsevec) â†’ sparsevec | Normalize with Euclidean norm | 0.7.0\n\n## Installation Notes - Linux and Mac\n\n### Postgres Location\n\nIf your machine has multiple Postgres installations, specify the path to [pg_config](https://www.postgresql.org/docs/current/app-pgconfig.html) with:\n\n```sh\nexport PG_CONFIG=/Library/PostgreSQL/18/bin/pg_config\n```\n\nThen re-run the installation instructions (run `make clean` before `make` if needed). If `sudo` is needed for `make install`, use:\n\n```sh\nsudo --preserve-env=PG_CONFIG make install\n```\n\nA few common paths on Mac are:\n\n- EDB installer - `/Library/PostgreSQL/18/bin/pg_config`\n- Homebrew (arm64) - `/opt/homebrew/opt/postgresql@18/bin/pg_config`\n- Homebrew (x86-64) - `/usr/local/opt/postgresql@18/bin/pg_config`\n\nNote: Replace `18` with your Postgres server version\n\n### Missing Header\n\nIf compilation fails with `fatal error: postgres.h: No such file or directory`, make sure Postgres development files are installed on the server.\n\nFor Ubuntu and Debian, use:\n\n```sh\nsudo apt install postgresql-server-dev-18\n```\n\nNote: Replace `18` with your Postgres server version\n\n### Missing SDK\n\nIf compilation fails and the output includes `warning: no such sysroot directory` on Mac, your Postgres installation points to a path that no longer exists.\n\n```sh\npg_config --cppflags\n```\n\nReinstall Postgres to fix this.\n\n### Portability\n\nBy default, pgvector compiles with `-march=native` on some platforms for best performance. However, this can lead to `Illegal instruction` errors if trying to run the compiled extension on a different machine.\n\nTo compile for portability, use:\n\n```sh\nmake OPTFLAGS=\"\"\n```\n\n## Installation Notes - Windows\n\n### Missing Header\n\nIf compilation fails with `Cannot open include file: 'postgres.h': No such file or directory`, make sure `PGROOT` is correct.\n\n### Mismatched Architecture\n\nIf compilation fails with `error C2196: case value '4' already used`, make sure youâ€™re using the `x64 Native Tools Command Prompt`. Then run `nmake /F Makefile.win clean` and re-run the installation instructions.\n\n### Missing Symbol\n\nIf linking fails with `unresolved external symbol float_to_shortest_decimal_bufn` with Postgres 17.0-17.2, upgrade to Postgres 17.3+.\n\n### Permissions\n\nIf installation fails with `Access is denied`, re-run the installation instructions as an administrator.\n\n## Additional Installation Methods\n\n### Docker\n\nGet the [Docker image](https://hub.docker.com/r/pgvector/pgvector) with:\n\n```sh\ndocker pull pgvector/pgvector:pg18-trixie\n```\n\nThis adds pgvector to the [Postgres image](https://hub.docker.com/_/postgres) (replace `18` with your Postgres server version, and run it the same way).\n\nSupported tags are:\n\n- `pg18-trixie`, `0.8.1-pg18-trixie`\n- `pg18-bookworm`, `0.8.1-pg18-bookworm`, `pg18`, `0.8.1-pg18`\n- `pg17-trixie`, `0.8.1-pg17-trixie`\n- `pg17-bookworm`, `0.8.1-pg17-bookworm`, `pg17`, `0.8.1-pg17`\n- `pg16-trixie`, `0.8.1-pg16-trixie`\n- `pg16-bookworm`, `0.8.1-pg16-bookworm`, `pg16`, `0.8.1-pg16`\n- `pg15-trixie`, `0.8.1-pg15-trixie`\n- `pg15-bookworm`, `0.8.1-pg15-bookworm`, `pg15`, `0.8.1-pg15`\n- `pg14-trixie`, `0.8.1-pg14-trixie`\n- `pg14-bookworm`, `0.8.1-pg14-bookworm`, `pg14`, `0.8.1-pg14`\n- `pg13-trixie`, `0.8.1-pg13-trixie`\n- `pg13-bookworm`, `0.8.1-pg13-bookworm`, `pg13`, `0.8.1-pg13`\n\nYou can also build the image manually:\n\n```sh\ngit clone --branch v0.8.1 https://github.com/pgvector/pgvector.git\ncd pgvector\ndocker build --pull --build-arg PG_MAJOR=18 -t myuser/pgvector .\n```\n\nIf you increase `maintenance_work_mem`, make sure `--shm-size` is at least that size to avoid an error with parallel HNSW index builds.\n\n```sh\ndocker run --shm-size=1g ...\n```\n\n### Homebrew\n\nWith Homebrew Postgres, you can use:\n\n```sh\nbrew install pgvector\n```\n\nNote: This only adds it to the `postgresql@18` and `postgresql@17` formulas\n\n### PGXN\n\nInstall from the [PostgreSQL Extension Network](https://pgxn.org/dist/vector) with:\n\n```sh\npgxn install vector\n```\n\n### APT\n\nDebian and Ubuntu packages are available from the [PostgreSQL APT Repository](https://wiki.postgresql.org/wiki/Apt). Follow the [setup instructions](https://wiki.postgresql.org/wiki/Apt#Quickstart) and run:\n\n```sh\nsudo apt install postgresql-18-pgvector\n```\n\nNote: Replace `18` with your Postgres server version\n\n### Yum\n\nRPM packages are available from the [PostgreSQL Yum Repository](https://yum.postgresql.org/). Follow the [setup instructions](https://www.postgresql.org/download/linux/redhat/) for your distribution and run:\n\n```sh\nsudo yum install pgvector_18\n# or\nsudo dnf install pgvector_18\n```\n\nNote: Replace `18` with your Postgres server version\n\n### pkg\n\nInstall the FreeBSD package with:\n\n```sh\npkg install postgresql17-pgvector\n```\n\nor the port with:\n\n```sh\ncd /usr/ports/databases/pgvector\nmake install\n```\n\n### APK\n\nInstall the Alpine package with:\n\n```sh\napk add postgresql-pgvector\n```\n\n### conda-forge\n\nWith Conda Postgres, install from [conda-forge](https://anaconda.org/conda-forge/pgvector) with:\n\n```sh\nconda install -c conda-forge pgvector\n```\n\nThis method is [community-maintained](https://github.com/conda-forge/pgvector-feedstock) by [@mmcauliffe](https://github.com/mmcauliffe)\n\n### Postgres.app\n\nDownload the [latest release](https://postgresapp.com/downloads.html) with Postgres 15+.\n\n## Hosted Postgres\n\npgvector is available on [these providers](https://github.com/pgvector/pgvector/issues/54).\n\n## Upgrading\n\n[Install](#installation) the latest version (use the same method as the original installation). Then in each database you want to upgrade, run:\n\n```sql\nALTER EXTENSION vector UPDATE;\n```\n\nYou can check the version in the current database with:\n\n```sql\nSELECT extversion FROM pg_extension WHERE extname = 'vector';\n```\n\n## Thanks\n\nThanks to:\n\n- [PASE: PostgreSQL Ultra-High-Dimensional Approximate Nearest Neighbor Search Extension](https://dl.acm.org/doi/pdf/10.1145/3318464.3386131)\n- [Faiss: A Library for Efficient Similarity Search and Clustering of Dense Vectors](https://github.com/facebookresearch/faiss)\n- [Using the Triangle Inequality to Accelerate k-means](https://cdn.aaai.org/ICML/2003/ICML03-022.pdf)\n- [k-means++: The Advantage of Careful Seeding](https://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf)\n- [Concept Decompositions for Large Sparse Text Data using Clustering](https://www.cs.utexas.edu/users/inderjit/public_papers/concept_mlj.pdf)\n- [Efficient and Robust Approximate Nearest Neighbor Search using Hierarchical Navigable Small World Graphs](https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf)\n\n## History\n\nView the [changelog](https://github.com/pgvector/pgvector/blob/master/CHANGELOG.md)\n\n## Contributing\n\nEveryone is encouraged to help improve this project. Here are a few ways you can help:\n\n- [Report bugs](https://github.com/pgvector/pgvector/issues)\n- Fix bugs and [submit pull requests](https://github.com/pgvector/pgvector/pulls)\n- Write, clarify, or fix documentation\n- Suggest or add new features\n\nTo get started with development:\n\n```sh\ngit clone https://github.com/pgvector/pgvector.git\ncd pgvector\nmake\nmake install\n```\n\nTo run all tests:\n\n```sh\nmake installcheck        # regression tests\nmake prove_installcheck  # TAP tests\n```\n\nTo run single tests:\n\n```sh\nmake installcheck REGRESS=functions                            # regression test\nmake prove_installcheck PROVE_TESTS=test/t/001_ivfflat_wal.pl  # TAP test\n```\n\nTo enable assertions:\n\n```sh\nmake clean && PG_CFLAGS=\"-DUSE_ASSERT_CHECKING\" make && make install\n```\n\nTo enable benchmarking:\n\n```sh\nmake clean && PG_CFLAGS=\"-DIVFFLAT_BENCH\" make && make install\n```\n\nTo show memory usage:\n\n```sh\nmake clean && PG_CFLAGS=\"-DHNSW_MEMORY -DIVFFLAT_MEMORY\" make && make install\n```\n\nTo get k-means metrics:\n\n```sh\nmake clean && PG_CFLAGS=\"-DIVFFLAT_KMEANS_DEBUG\" make && make install\n```\n\nResources for contributors\n\n- [Extension Building Infrastructure](https://www.postgresql.org/docs/current/extend-pgxs.html)\n- [Index Access Method Interface Definition](https://www.postgresql.org/docs/current/indexam.html)\n- [Generic WAL Records](https://www.postgresql.org/docs/current/generic-wal.html)\n",
      "stars_today": 10
    },
    {
      "id": 19696006,
      "name": "engine",
      "full_name": "playcanvas/engine",
      "description": "Powerful web graphics runtime built on WebGL, WebGPU, WebXR and glTF",
      "html_url": "https://github.com/playcanvas/engine",
      "stars": 14342,
      "forks": 1715,
      "language": "JavaScript",
      "topics": [
        "3d-gaussian-splatting",
        "game-development",
        "game-engine",
        "gamedev",
        "gaussian-splatting",
        "gltf",
        "hacktoberfest",
        "javascript",
        "nodejs",
        "playcanvas",
        "typescript",
        "virtual-reality",
        "webgl",
        "webgl2",
        "webgpu",
        "webxr"
      ],
      "created_at": "2014-05-12T11:24:30Z",
      "updated_at": "2026-01-15T00:59:06Z",
      "pushed_at": "2026-01-14T14:56:38Z",
      "open_issues": 592,
      "owner": {
        "login": "playcanvas",
        "avatar_url": "https://avatars.githubusercontent.com/u/1030579?v=4"
      },
      "readme": "# PlayCanvas Engine\n\n[![NPM Version](https://img.shields.io/npm/v/playcanvas)](https://www.npmjs.com/package/playcanvas)\n[![NPM Downloads](https://img.shields.io/npm/dw/playcanvas)](https://npmtrends.com/playcanvas)\n[![Minzipped size](https://img.shields.io/bundlephobia/minzip/playcanvas)](https://bundlephobia.com/result?p=playcanvas)\n[![License](https://img.shields.io/npm/l/playcanvas)](https://github.com/playcanvas/engine/blob/main/LICENSE)\n[![Discord](https://img.shields.io/badge/Discord-5865F2?style=flat&logo=discord&logoColor=white&color=black)](https://discord.gg/RSaMRzg)\n[![Reddit](https://img.shields.io/badge/Reddit-FF4500?style=flat&logo=reddit&logoColor=white&color=black)](https://www.reddit.com/r/PlayCanvas)\n[![X](https://img.shields.io/badge/X-000000?style=flat&logo=x&logoColor=white&color=black)](https://x.com/intent/follow?screen_name=playcanvas)\n\n| [User Manual](https://developer.playcanvas.com/user-manual/engine/) | [API Reference](https://api.playcanvas.com/engine/) | [Examples](https://playcanvas.github.io) | [Blog](https://blog.playcanvas.com) | [Forum](https://forum.playcanvas.com) |\n\nPlayCanvas is an open-source game engine. It uses HTML5 and WebGL to run games and other interactive 3D content in any mobile or desktop browser.\n\n[English](https://github.com/playcanvas/engine/blob/dev/README.md)\n[ä¸­æ–‡](https://github.com/playcanvas/engine/blob/dev/README-zh.md)\n[æ—¥æœ¬èª](https://github.com/playcanvas/engine/blob/dev/README-ja.md)\n[í•œê¸€](https://github.com/playcanvas/engine/blob/dev/README-kr.md)\n\n## Project Showcase\n\n[Many games and apps](https://github.com/playcanvas/awesome-playcanvas) have been published using the PlayCanvas engine. Here is a small selection:\n\n[![Seemore](https://s3-eu-west-1.amazonaws.com/images.playcanvas.com/projects/14705/319531/O4J4VU-image-25.jpg)](https://playcanv.as/p/MflWvdTW/) [![After The Flood](https://s3-eu-west-1.amazonaws.com/images.playcanvas.com/projects/14928/440410/98554E-image-25.jpg)](https://playcanv.as/p/44MRmJRU/) [![Casino](https://s3-eu-west-1.amazonaws.com/images.playcanvas.com/projects/14928/349824/U88HJQ-image-25.jpg)](https://playcanv.as/p/LpmXGUe6/)  \n[![Swooop](https://s3-eu-west-1.amazonaws.com/images.playcanvas.com/projects/12/4763/TKYXB8-image-25.jpg)](https://playcanv.as/p/JtL2iqIH/) [![dev Archer](https://s3-eu-west-1.amazonaws.com/images.playcanvas.com/projects/12/415995/10A5A9-image-25.jpg)](https://playcanv.as/p/JERg21J8/) [![Gaussian Splat Statues](https://s3-eu-west-1.amazonaws.com/images.playcanvas.com/projects/12/1224723/266D9C-image-25.jpg)](https://playcanv.as/p/cLkf99ZV/)  \n[![Car](https://s3-eu-west-1.amazonaws.com/images.playcanvas.com/projects/12/347824/7ULQ3Y-image-25.jpg)](https://playcanv.as/p/RqJJ9oU9/) [![Star-Lord](https://s3-eu-west-1.amazonaws.com/images.playcanvas.com/projects/12/333626/BGQN9H-image-25.jpg)](https://playcanv.as/p/SA7hVBLt/) [![Global Illumination](https://s3-eu-west-1.amazonaws.com/images.playcanvas.com/projects/4373/625081/6AB32D-image-25.jpg)](https://playcanv.as/p/ZV4PW6wr/ )\n\nYou can see more games on the [PlayCanvas website](https://playcanvas.com/explore).\n\n## Users\n\nPlayCanvas is used by leading companies in video games, advertising and visualization such as:  \n**Animech, Arm, BMW, Disney, Facebook, Famobi, Funday Factory, IGT, King, Miniclip, Leapfrog, Mojiworks, Mozilla, Nickelodeon, Nordeus, NOWWA, PikPok, PlaySide Studios, Polaris, Product Madness, Samsung, Snap, Spry Fox, Zeptolab, Zynga**\n\n## Features\n\nPlayCanvas is a fully-featured game engine.\n\n* ğŸ§Š **Graphics** - Advanced 2D + 3D graphics engine built on WebGL2 & WebGPU\n* ğŸƒ **Animation** - Powerful state-based animations for characters and arbitrary scene properties\n* âš›ï¸ **Physics** - Full integration with 3D rigid-body physics engine [ammo.js](https://github.com/kripken/ammo.js)\n* ğŸ® **Input** - Mouse, keyboard, touch, gamepad and VR controller APIs\n* ğŸ”Š **Sound** - 3D positional sounds built on the Web Audio API\n* ğŸ“¦ **Assets** - Asynchronous streaming system built on [glTF 2.0](https://www.khronos.org/gltf/), [Draco](https://google.github.io/draco/) and [Basis](https://github.com/BinomialLLC/basis_universal) compression\n* ğŸ“œ **Scripts** - Write game behaviors in Typescript or JavaScript\n\n## Usage\n\nHere's a super-simple Hello World example - a spinning cube!\n\n```js\nimport * as pc from 'playcanvas';\n\nconst canvas = document.createElement('canvas');\ndocument.body.appendChild(canvas);\n\nconst app = new pc.Application(canvas);\n\n// fill the available space at full resolution\napp.setCanvasFillMode(pc.FILLMODE_FILL_WINDOW);\napp.setCanvasResolution(pc.RESOLUTION_AUTO);\n\n// ensure canvas is resized when window changes size\nwindow.addEventListener('resize', () => app.resizeCanvas());\n\n// create box entity\nconst box = new pc.Entity('cube');\nbox.addComponent('model', {\n  type: 'box'\n});\napp.root.addChild(box);\n\n// create camera entity\nconst camera = new pc.Entity('camera');\ncamera.addComponent('camera', {\n  clearColor: new pc.Color(0.1, 0.2, 0.3)\n});\napp.root.addChild(camera);\ncamera.setPosition(0, 0, 3);\n\n// create directional light entity\nconst light = new pc.Entity('light');\nlight.addComponent('light');\napp.root.addChild(light);\nlight.setEulerAngles(45, 0, 0);\n\n// rotate the box according to the delta time since the last frame\napp.on('update', dt => box.rotate(10 * dt, 20 * dt, 30 * dt));\n\napp.start();\n```\n\nWant to play with the code yourself? Edit it on [CodePen](https://codepen.io/playcanvas/pen/NPbxMj).\n\nA full guide to setting up a local development environment based on the PlayCanvas Engine can be found [here](https://developer.playcanvas.com/user-manual/engine/standalone/).\n\n## How to build\n\nEnsure you have [Node.js 18+](https://nodejs.org) installed. Then, install all of the required Node.js dependencies:\n\n```sh\nnpm install\n```\n\nNow you can run various build options:\n\n| Command         | Description                                                           | Outputs To |\n| --------------- | --------------------------------------------------------------------- | ---------- |\n| `npm run build` | Build all engine flavors and type declarations                        | `build`    |\n| `npm run docs`  | Build engine [API reference docs](https://api.playcanvas.com/engine/) | `docs`     |\n\n## PlayCanvas Editor\n\nThe PlayCanvas Engine is an open-source engine that you can use to create HTML5 apps/games. In addition to the engine, we also make the [PlayCanvas Editor](https://playcanvas.com/):\n\n[![Editor](https://github.com/playcanvas/editor/blob/main/images/editor.png?raw=true)](https://github.com/playcanvas/editor)\n\nFor Editor-related bugs and issues, please refer to the [Editor's repo](https://github.com/playcanvas/editor).\n",
      "stars_today": 10
    },
    {
      "id": 391740664,
      "name": "OpenMetadata",
      "full_name": "open-metadata/OpenMetadata",
      "description": "OpenMetadata is a unified metadata platform for data discovery, data observability, and data governance powered by a central metadata repository, in-depth column level lineage, and seamless team collaboration.",
      "html_url": "https://github.com/open-metadata/OpenMetadata",
      "stars": 8427,
      "forks": 1591,
      "language": "TypeScript",
      "topics": [
        "data-catalog",
        "data-collaboration",
        "data-contracts",
        "data-discovery",
        "data-governance",
        "data-lineage",
        "data-observability",
        "data-profiling",
        "data-quality",
        "data-quality-checks",
        "data-validation",
        "datadiscovery",
        "dataengineering",
        "dataquality",
        "hacktoberfest",
        "mcp",
        "mcp-server",
        "metadata",
        "metadata-management",
        "snowflake"
      ],
      "created_at": "2021-08-01T21:17:17Z",
      "updated_at": "2026-01-14T23:22:05Z",
      "pushed_at": "2026-01-14T23:29:10Z",
      "open_issues": 661,
      "owner": {
        "login": "open-metadata",
        "avatar_url": "https://avatars.githubusercontent.com/u/86132257?v=4"
      },
      "readme": "<br /><br />\n<p align=\"center\">\n    <a href=\"https://open-metadata.org\">\n        <img alt=\"Logo\" src=\"https://github.com/open-metadata/OpenMetadata/assets/40225091/e794ced8-7220-4393-8efc-3faf93bfb503\" width=\"49%\">\n    </a>\n</p>\n\n<p align=\"center\"><b>Empower your Data Journey with OpenMetadata</b></p>\n\n<div align=\"center\">\n    \n![Commit Activity](https://img.shields.io/github/commit-activity/m/open-metadata/OpenMetadata?style=for-the-badge)\n[![Release](https://img.shields.io/github/release/open-metadata/OpenMetadata/all.svg?style=for-the-badge)](https://github.com/open-metadata/OpenMetadata/releases)\n\n</div>\n\n## What is OpenMetadata?\n[OpenMetadata](https://open-metadata.org/)  is a unified metadata platform for data discovery, data observability, and data governance powered by a central metadata repository, in-depth column-level lineage, and seamless team collaboration. It is one of the fastest-growing open-source projects with a vibrant community and adoption by a diverse set of companies in a variety of industry verticals. Based on Open Metadata Standards and APIs, supporting connectors to a wide range of data services, OpenMetadata enables end-to-end metadata management, giving you the freedom to unlock the value of your data assets.\n<div align=\"center\">\n    <img src=\"https://github.com/open-metadata/OpenMetadata/assets/40225091/ebfb4ec5-f0a2-4d58-8ce5-a082b5cf0f76\" width=800>\n</div>\n\n<br />\nContents:\n\n- [Features](#key-features-of-openmetadata)\n- [Try our Sandbox](#try-our-sandbox)\n- [Install & Run](#install-and-run-openmetadata)\n- [Roadmap](https://docs.open-metadata.org/latest/roadmap)\n- [Documentation and Support](#documentation-and-support)\n- [Contributors](#contributors)\n\nOpenMetadata Consists of Four Main Components:\n- **Metadata Schemas**: These are the core definitions and vocabulary for metadata based on common abstractions and types. They also allow for custom extensions and properties to suit different use cases and domains.\n- **Metadata Store**: This is the central repository for storing and managing the metadata graph, which connects data assets, users, and tool-generated metadata in a unified way.\n- **Metadata APIs**: These are the interfaces for producing and consuming metadata, built on top of the metadata schemas. They enable seamless integration of user interfaces and tools, systems, and services with the metadata store.\n- **Ingestion Framework**: This is a pluggable framework for ingesting metadata from various sources and tools to the metadata store. It supports about 84+ connectors for data warehouses, databases, dashboard services, messaging services, pipeline services, and more.\n\n## Key Features of OpenMetadata\n**Data Discovery**: Find and explore all your data assets in a single place using various strategies, such as keyword search, data associations, and advanced queries. You can search across tables, topics, dashboards, pipelines, and services.\n\n![12](https://github.com/open-metadata/OpenMetadata/assets/40225091/0dbd2746-c93d-4a47-8d3e-ceb3ae01436f)\n<br><br><br>\n**Data Collaboration**: Communicate, converse, and cooperate with other users and teams on data assets. You can get event notifications, send alerts, add announcements, create tasks, and use conversation threads.\n\n![11](https://github.com/open-metadata/OpenMetadata/assets/40225091/7df29e12-8a29-44b7-9466-42474823783f)\n<br><br><br>\n**Data Quality and Profiler**: Measure and monitor the quality with **no-code** to build trust in your data. You can define and run data quality tests, group them into test suites, and view the results in an interactive dashboard. With powerful collaboration, make data quality a shared responsibility in your organization.\n\n![8](https://github.com/open-metadata/OpenMetadata/assets/40225091/6b330827-cc2d-4d06-abf0-a4d42ce532ba)\n<br><br><br>\n**Data Governance**: Enforce data policies and standards across your organization. You can define data domains and data products, assign owners and stakeholders, and classify data assets using tags and terms. Use powerful automation features to auto-classify your data.\n\n![10](https://github.com/open-metadata/OpenMetadata/assets/40225091/f7384a71-6b58-44ad-983f-e302718ee3f1)\n<br><br><br>\n**Data Insights and KPIs**: Use reports and platform analytics to understand how your organization's data is doing. Data Insights provides a single-pane view of all the key metrics to reflect the state of your data best. Define the Key Performance Indicators (KPIs) and set goals within OpenMetadata to work towards better documentation, ownership, and tiering. Alerts can be set against the KPIs to be received on a specified schedule.\n\n![9](https://github.com/open-metadata/OpenMetadata/assets/40225091/61fc2f65-2436-4fc9-9434-c27ee9b25183)\n<br><br><br>\n**Data Lineage**: Track and visualize the origin and transformation of your data assets end-to-end. You can view column-level lineage, filter queries, and edit lineage manually using a no-code editor.\n  \n**Data Documentation**: Document your data assets and metadata entities using rich text, images, and links. You can also add comments and annotations and generate data dictionaries and data catalogs.\n  \n**Data Observability**: Monitor the health and performance of your data assets and pipelines. You can view metrics such as data freshness, data volume, data quality, and data latency. You can also set up alerts and notifications for any anomalies or failures.\n  \n**Data Security**: Secure your data and metadata using various authentication and authorization mechanisms. You can integrate with different identity providers for single sign-on and define roles and policies for access control.\n  \n**Webhooks**: Integrate with external applications and services using webhooks. You can register URLs to receive metadata event notifications and integrate with Slack, Microsoft Teams, and Google Chat.\n  \n**Connectors**: Ingest metadata from various sources and tools using connectors. OpenMetadata supports about 84+ connectors for data warehouses, databases, dashboard services, messaging services, pipeline services, and more.\n\n## Try our Sandbox\n\nTake a look and play with sample data at [http://sandbox.open-metadata.org](http://sandbox.open-metadata.org)\n\n## Install and Run OpenMetadata\nGet up and running in a few minutes. See the OpenMetadata documentation for [installation instructions](https://docs.open-metadata.org/quick-start/local-docker-deployment).\n\n## Documentation and Support\n\nWe're here to help and make OpenMetadata even better! Check out [OpenMetadata documentation](https://docs.open-metadata.org/) for a complete description of OpenMetadata's features. Join our [Slack Community](https://slack.open-metadata.org/) to get in touch with us if you want to chat, need help, or discuss new feature requirements.\n\n\n## Contributors\n\nWe â¤ï¸ all contributions, big and small! Check out our [CONTRIBUTING](./CONTRIBUTING.md) guide to get started, and let us know how we can help.\n\nDon't want to miss anything? Give the project a â­ ğŸš€ \n\nA HUGE THANK YOU to all our supporters!\n\n<a href=\"https://github.com/open-metadata/OpenMetadata/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=open-metadata/OpenMetadata&max=4000&columns=30\" />\n</a>\n\n## Stargazers\n\n[![Stargazers of @open-metadata/OpenMetadata repo](http://reporoster.com/stars/open-metadata/OpenMetadata)](https://github.com/open-metadata/OpenMetadata/stargazers)\n\n## License\nOpenMetadata is released under [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)\n",
      "stars_today": 10
    },
    {
      "id": 667731914,
      "name": "kiss-translator",
      "full_name": "fishjar/kiss-translator",
      "description": "A simple, open source bilingual translation extension & Greasemonkey script (ä¸€ä¸ªç®€çº¦ã€å¼€æºçš„ åŒè¯­å¯¹ç…§ç¿»è¯‘æ‰©å±• & æ²¹çŒ´è„šæœ¬)",
      "html_url": "https://github.com/fishjar/kiss-translator",
      "stars": 8578,
      "forks": 360,
      "language": "JavaScript",
      "topics": [
        "extension",
        "translate",
        "translation",
        "translator",
        "userscript",
        "userscripts"
      ],
      "created_at": "2023-07-18T07:20:49Z",
      "updated_at": "2026-01-14T15:12:55Z",
      "pushed_at": "2026-01-14T14:26:34Z",
      "open_issues": 47,
      "owner": {
        "login": "fishjar",
        "avatar_url": "https://avatars.githubusercontent.com/u/1157624?v=4"
      },
      "readme": "# KISS Translator ç®€çº¦ç¿»è¯‘\n\n[English](README.en.md) | [ä¸­æ–‡](README.md) | [æ—¥æœ¬èª](README.ja.md) | [í•œêµ­ì–´](README.ko.md)\n\nä¸€ä¸ªç®€çº¦ã€å¼€æºçš„ [åŒè¯­å¯¹ç…§ç¿»è¯‘æ‰©å±• & æ²¹çŒ´è„šæœ¬](https://github.com/fishjar/kiss-translator)ã€‚\n\n[kiss-translator.webm](https://github.com/fishjar/kiss-translator/assets/1157624/f7ba8a5c-e4a8-4d5a-823a-5c5c67a0a47f)\n\n## ç‰¹æ€§\n\n- [x] ä¿æŒç®€çº¦\n- [x] å¼€æ”¾æºä»£ç \n- [x] é€‚é…å¸¸è§æµè§ˆå™¨\n  - [x] Chrome/Edge\n  - [x] Firefox\n  - [x] Kiwi (Android)\n  - [x] Orion (iOS)\n  - [x] Safari\n  - [x] Thunderbird\n- [x] æ”¯æŒå¤šç§ç¿»è¯‘æœåŠ¡\n  - [x] Google/Microsoft\n  - [x] Tencent/Volcengine\n  - [x] OpenAI/Gemini/Claude/Ollama/DeepSeek/OpenRouter\n  - [x] DeepL/DeepLX/NiuTrans\n  - [x] AzureAI/CloudflareAI\n  - [x] Chromeæµè§ˆå™¨å†…ç½®AIç¿»è¯‘(BuiltinAI)\n- [x] è¦†ç›–å¸¸è§ç¿»è¯‘åœºæ™¯\n  - [x] ç½‘é¡µåŒè¯­å¯¹ç…§ç¿»è¯‘\n  - [x] è¾“å…¥æ¡†ç¿»è¯‘\n    - é€šè¿‡å¿«æ·é”®ç«‹å³å°†è¾“å…¥æ¡†å†…æ–‡æœ¬ç¿»è¯‘æˆå…¶ä»–è¯­è¨€\n  - [x] åˆ’è¯ç¿»è¯‘\n    - [x] ä»»æ„é¡µé¢æ‰“å¼€ç¿»è¯‘æ¡†ï¼Œå¯ç”¨å¤šç§ç¿»è¯‘æœåŠ¡å¯¹æ¯”ç¿»è¯‘\n    - [x] è‹±æ–‡è¯å…¸ç¿»è¯‘\n    - [x] æ”¶è—è¯æ±‡\n  - [x] é¼ æ ‡æ‚¬åœç¿»è¯‘\n  - [x] YouTube å­—å¹•ç¿»è¯‘\n    - æ”¯æŒä»»æ„ç¿»è¯‘æœåŠ¡å¯¹è§†é¢‘å­—å¹•è¿›è¡Œç¿»è¯‘å¹¶åŒè¯­æ˜¾ç¤º\n    - å†…ç½®åŸºç¡€çš„å­—å¹•åˆå¹¶ä¸æ–­å¥ç®—æ³•ï¼Œæå‡ç¿»è¯‘æ•ˆæœ\n    - æ”¯æŒAIæ–­å¥åŠŸèƒ½ï¼Œå¯è¿›ä¸€æ­¥æå‡ç¿»è¯‘è´¨é‡\n    - è‡ªå®šä¹‰å­—å¹•æ ·å¼\n- [x] æ”¯æŒå¤šæ ·ç¿»è¯‘æ•ˆæœ\n  - [x] æ”¯æŒè‡ªåŠ¨è¯†åˆ«æ–‡æœ¬ä¸æ‰‹åŠ¨è§„åˆ™ä¸¤ç§æ¨¡å¼\n    - è‡ªåŠ¨è¯†åˆ«æ–‡æœ¬æ¨¡å¼ä½¿å¾—ç»å¤§éƒ¨åˆ†ç½‘ç«™æ— éœ€ç¼–å†™è§„åˆ™ä¹Ÿèƒ½ç¿»è¯‘å®Œæ•´\n    - æ‰‹åŠ¨è§„åˆ™æ¨¡å¼ï¼Œå¯ä»¥é’ˆå¯¹ç‰¹å®šç½‘ç«™æè‡´ä¼˜åŒ–\n  - [x] è‡ªå®šä¹‰è¯‘æ–‡æ ·å¼\n  - [x] æ”¯æŒå¯Œæ–‡æœ¬ç¿»è¯‘åŠæ˜¾ç¤ºï¼Œèƒ½å¤Ÿå°½é‡ä¿ç•™åŸæ–‡ä¸­çš„é“¾æ¥åŠå…¶ä»–æ–‡æœ¬æ ·å¼\n  - [x] æ”¯æŒä»…æ˜¾ç¤ºè¯‘æ–‡ï¼ˆéšè—åŸæ–‡ï¼‰\n- [x] ç¿»è¯‘æ¥å£é«˜çº§åŠŸèƒ½\n  - [x] é€šè¿‡è‡ªå®šä¹‰æ¥å£ï¼Œç†è®ºä¸Šæ”¯æŒä»»ä½•ç¿»è¯‘æ¥å£\n  - [x] èšåˆæ‰¹é‡å‘é€ç¿»è¯‘æ–‡æœ¬\n  - [x] æ”¯æŒæµå¼ä¼ è¾“ï¼Œå®æ—¶æ˜¾ç¤ºç¿»è¯‘ç»“æœ\n  - [x] æ”¯æŒAIä¸Šä¸‹æ–‡ä¼šè¯è®°å¿†åŠŸèƒ½ï¼Œæå‡ç¿»è¯‘æ•ˆæœ\n  - [x] è‡ªå®šä¹‰AIæœ¯è¯­è¯å…¸\n  - [x] æ‰€æœ‰æ¥å£å‡æ”¯æŒHookå’Œè‡ªå®šä¹‰å‚æ•°ç­‰é«˜çº§åŠŸèƒ½\n- [x] è·¨å®¢æˆ·ç«¯æ•°æ®åŒæ­¥\n  - [x] KISS-Workerï¼ˆcloudflare/dockerï¼‰\n  - [x] WebDAV\n- [x] è‡ªå®šä¹‰ç¿»è¯‘è§„åˆ™\n  - [x] è§„åˆ™è®¢é˜…/è§„åˆ™åˆ†äº«\n  - [x] è‡ªå®šä¹‰ä¸“ä¸šæœ¯è¯­\n- [x] è‡ªå®šä¹‰å¿«æ·é”®\n  - `Alt+Q` å¼€å¯ç¿»è¯‘\n  - `Alt+C` åˆ‡æ¢æ ·å¼\n  - `Alt+K` æ‰“å¼€è®¾ç½®å¼¹çª—\n  - `Alt+S` æ‰“å¼€ç¿»è¯‘å¼¹çª—/ç¿»è¯‘é€‰ä¸­æ–‡å­—\n  - `Alt+O` æ‰“å¼€è®¾ç½®é¡µé¢\n  - `Alt+I` è¾“å…¥æ¡†ç¿»è¯‘\n\n## å®‰è£…\n\n> æ³¨ï¼šåŸºäºä»¥ä¸‹åŸå› ï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨æµè§ˆå™¨æ‰©å±•\n>\n> - æµè§ˆå™¨æ‰©å±•çš„åŠŸèƒ½æ›´å®Œæ•´ï¼ˆæœ¬åœ°è¯­è¨€è¯†åˆ«ã€å³é”®èœå•ç­‰ï¼‰\n> - æ²¹çŒ´è„šæœ¬ä¼šé‡åˆ°æ›´å¤šä½¿ç”¨ä¸Šçš„é—®é¢˜ï¼ˆè·¨åŸŸé—®é¢˜ã€è„šæœ¬å†²çªç­‰ï¼‰\n\n- [x] æµè§ˆå™¨æ‰©å±•\n  - [x] Chrome [å®‰è£…åœ°å€](https://chrome.google.com/webstore/detail/kiss-translator/bdiifdefkgmcblbcghdlonllpjhhjgof?hl=zh-CN)\n    - [x] Kiwi (Android)\n    - [x] Orion (iOS)\n  - [x] Edge [å®‰è£…åœ°å€](https://microsoftedge.microsoft.com/addons/detail/%E7%AE%80%E7%BA%A6%E7%BF%BB%E8%AF%91/jemckldkclkinpjighnoilpbldbdmmlh?hl=zh-CN)\n  - [x] Firefox [å®‰è£…åœ°å€](https://addons.mozilla.org/zh-CN/firefox/addon/kiss-translator/)\n  - [ ] Safari\n    - [ ] Safari (Mac)\n    - [ ] Safari (iOS) \n  - [x] Thunderbird [ä¸‹è½½åœ°å€](https://github.com/fishjar/kiss-translator/releases)\n- [x] æ²¹çŒ´è„šæœ¬\n  - [x] Chrome/Edge/Firefox ([Tampermonkey](https://www.tampermonkey.net/)/[Violentmonkey](https://violentmonkey.github.io/)) [å®‰è£…é“¾æ¥](https://fishjar.github.io/kiss-translator/kiss-translator.user.js)\n    - [Greasy Fork](https://greasyfork.org/zh-CN/scripts/472840-kiss-translator)\n  - [x] iOS Safari ([Userscripts Safari](https://github.com/quoid/userscripts)) [å®‰è£…é“¾æ¥](https://fishjar.github.io/kiss-translator/kiss-translator-ios-safari.user.js)\n\n## å…³è”é¡¹ç›®\n\n- æ•°æ®åŒæ­¥æœåŠ¡: [https://github.com/fishjar/kiss-worker](https://github.com/fishjar/kiss-worker)\n  - å¯ç”¨äºæœ¬é¡¹ç›®çš„æ•°æ®åŒæ­¥æœåŠ¡ã€‚\n  - äº¦å¯ç”¨äºåˆ†äº«ä¸ªäººçš„ç§æœ‰è§„åˆ™åˆ—è¡¨ã€‚\n  - è‡ªå·±éƒ¨ç½²ï¼Œè‡ªå·±ç®¡ç†ï¼Œæ•°æ®ç§æœ‰ã€‚\n- ç¤¾åŒºè®¢é˜…è§„åˆ™: [https://github.com/fishjar/kiss-rules](https://github.com/fishjar/kiss-rules)\n  - æä¾›ç¤¾åŒºç»´æŠ¤çš„ï¼Œæœ€æ–°æœ€å…¨çš„è®¢é˜…è§„åˆ™åˆ—è¡¨ã€‚\n  - æ±‚åŠ©è§„åˆ™ç›¸å…³çš„é—®é¢˜ã€‚\n\n## å¸¸è§é—®é¢˜\n\n### å¦‚ä½•è®¾ç½®å¿«æ·é”®\n\nåœ¨æ’ä»¶ç®¡ç†é‚£é‡Œè®¾ç½®ï¼Œä¾‹å¦‚ï¼š \n\n- chrome [chrome://extensions/shortcuts](chrome://extensions/shortcuts)\n- firefox [about:addons](about:addons)\n\n### è§„åˆ™è®¾ç½®çš„ä¼˜å…ˆçº§æ˜¯å¦‚ä½•çš„\n\nä¸ªäººè§„åˆ™ > è®¢é˜…è§„åˆ™ > å…¨å±€è§„åˆ™\n\nå…¶ä¸­å…¨å±€è§„åˆ™ä¼˜å…ˆçº§æœ€ä½ï¼Œä½†éå¸¸é‡è¦ï¼Œç›¸å½“äºå…œåº•è§„åˆ™ã€‚\n\n### æ¥å£ï¼ˆOllamaç­‰ï¼‰æµ‹è¯•å¤±è´¥\n\nä¸€èˆ¬æ¥å£æµ‹è¯•å¤±è´¥å¸¸è§æœ‰ä»¥ä¸‹å‡ ç§åŸå› ï¼š\n\n- åœ°å€å¡«é”™äº†ï¼š\n  - æ¯”å¦‚ `Ollama` æœ‰åŸç”Ÿæ¥å£åœ°å€å’Œ `Openai` å…¼å®¹çš„åœ°å€ï¼Œæœ¬æ’ä»¶ç›®å‰ç»Ÿä¸€æ”¯æŒ `Openai` å…¼å®¹çš„åœ°å€ï¼Œä¸æ”¯æŒ `Ollama` åŸç”Ÿæ¥å£åœ°å€\n- æŸäº›AIæ¨¡å‹ä¸æ”¯æŒèšåˆç¿»è¯‘ï¼š\n  - æ­¤ç§æƒ…å†µå¯ä»¥é€‰æ‹©ç¦ç”¨èšåˆç¿»è¯‘æˆ–é€šè¿‡è‡ªå®šä¹‰æ¥å£çš„æ–¹å¼æ¥ä½¿ç”¨ã€‚\n  - æˆ–é€šè¿‡è‡ªå®šä¹‰æ¥å£çš„æ–¹å¼æ¥ä½¿ç”¨ï¼Œè¯¦æƒ…å‚è€ƒï¼š [è‡ªå®šä¹‰æ¥å£ç¤ºä¾‹æ–‡æ¡£](https://github.com/fishjar/kiss-translator/blob/master/custom-api_v2.md)\n- æŸäº›AIæ¨¡å‹çš„å‚æ•°ä¸ä¸€è‡´ï¼š\n  - æ¯”å¦‚ `Gemini` åŸç”Ÿæ¥å£å‚æ•°éå¸¸ä¸ä¸€è‡´ï¼Œéƒ¨åˆ†ç‰ˆæœ¬çš„æ¨¡å‹ä¸æ”¯æŒæŸäº›å‚æ•°ä¼šå¯¼è‡´è¿”å›é”™è¯¯ã€‚\n  - æ­¤ç§æƒ…å†µå¯ä»¥é€šè¿‡ `Hook` ä¿®æ”¹è¯·æ±‚ `body` ,æˆ–è€…æ›´æ¢ä¸º `Gemini2` (`Openai` å…¼å®¹çš„åœ°å€)\n- æœåŠ¡å™¨è·¨åŸŸé™åˆ¶è®¿é—®ï¼Œè¿”å›403é”™è¯¯ï¼š\n  - æ¯”å¦‚ `Ollama` å¯åŠ¨æ—¶é¡»æ·»åŠ ç¯å¢ƒå˜é‡ `OLLAMA_ORIGINS=*`, å‚è€ƒï¼šhttps://github.com/fishjar/kiss-translator/issues/174\n\n### å¡«å†™çš„æ¥å£åœ¨æ²¹çŒ´è„šæœ¬ä¸èƒ½ä½¿ç”¨\n\næ²¹çŒ´è„šæœ¬éœ€è¦å¢åŠ åŸŸåç™½åå•ï¼Œå¦åˆ™ä¸èƒ½å‘å‡ºè¯·æ±‚ã€‚\n\n### å¦‚ä½•è®¾ç½®è‡ªå®šä¹‰æ¥å£çš„hookå‡½æ•°\n\nè‡ªå®šä¹‰æ¥å£åŠŸèƒ½éå¸¸å¼ºå¤§ã€çµæ´»ï¼Œç†è®ºå¯ä»¥æ¥å…¥ä»»ä½•ç¿»è¯‘æ¥å£ã€‚\n\nç¤ºä¾‹å‚è€ƒï¼š [custom-api_v2.md](https://github.com/fishjar/kiss-translator/blob/master/custom-api_v2.md)\n\n### å¦‚ä½•ç›´æ¥è¿›å…¥æ²¹çŒ´è„šæœ¬è®¾ç½®é¡µé¢\n\nè®¾ç½®é¡µé¢åœ°å€ï¼š https://fishjar.github.io/kiss-translator/options.html\n\n## æœªæ¥è§„åˆ’ \n\n æœ¬é¡¹ç›®ä¸ºä¸šä½™å¼€å‘ï¼Œæ— ä¸¥æ ¼æ—¶é—´è¡¨ï¼Œæ¬¢è¿ç¤¾åŒºå…±å»ºã€‚ä»¥ä¸‹ä¸ºåˆæ­¥è®¾æƒ³çš„åŠŸèƒ½æ–¹å‘ï¼š\n\n- [x] **èšåˆå‘é€æ–‡æœ¬**ï¼šä¼˜åŒ–è¯·æ±‚ç­–ç•¥ï¼Œå‡å°‘ç¿»è¯‘æ¥å£è°ƒç”¨æ¬¡æ•°ï¼Œæå‡æ€§èƒ½ã€‚\n- [x] **å¢å¼ºå¯Œæ–‡æœ¬ç¿»è¯‘**ï¼šæ”¯æŒæ›´å¤æ‚çš„é¡µé¢ç»“æ„å’Œå¯Œæ–‡æœ¬å†…å®¹çš„å‡†ç¡®ç¿»è¯‘ã€‚\n- [x] **å¼ºåŒ–è‡ªå®šä¹‰/AI æ¥å£**ï¼šæ”¯æŒæµå¼ä¼ è¾“ã€ä¸Šä¸‹æ–‡è®°å¿†ã€å¤šè½®å¯¹è¯ç­‰é«˜çº§ AI åŠŸèƒ½ã€‚\n- [x] **è‹±æ–‡è¯å…¸å¤‡ç¾æœºåˆ¶**ï¼šå½“ç¿»è¯‘æœåŠ¡å¤±æ•ˆæ—¶ï¼Œå¯åˆ‡æ¢å…¶ä»–è¯å…¸æˆ– fallback åˆ°æœ¬åœ°è¯å…¸æŸ¥è¯¢ã€‚\n- [x] **ä¼˜åŒ– YouTube å­—å¹•æ”¯æŒ**ï¼šæ”¹è¿›æµå¼å­—å¹•çš„åˆå¹¶ä¸ç¿»è¯‘ä½“éªŒï¼Œå‡å°‘æ–­å¥ã€‚\n- [ ] **è§„åˆ™å…±å»ºæœºåˆ¶å‡çº§**ï¼šå¼•å…¥æ›´çµæ´»çš„è§„åˆ™åˆ†äº«ã€ç‰ˆæœ¬ç®¡ç†ä¸ç¤¾åŒºè¯„å®¡æµç¨‹ã€‚\n \n å¦‚æœä½ å¯¹æŸä¸ªæ–¹å‘æ„Ÿå…´è¶£ï¼Œæ¬¢è¿åœ¨ [Issues](https://github.com/fishjar/kiss-translator/issues) ä¸­è®¨è®ºæˆ–æäº¤ PRï¼\n\n## å¼€å‘æŒ‡å¼•\n\n```sh\ngit clone https://github.com/fishjar/kiss-translator.git\ncd kiss-translator\ngit checkout dev # æäº¤PRå»ºè®®æ¨é€åˆ°devåˆ†æ”¯\npnpm install\npnpm build\n```\n\n### å¤–éƒ¨è§¦å‘ç¤ºä¾‹\n\n```js\n// `toggle_translate`   åˆ‡æ¢ç¿»è¯‘\n// `toggle_styles`      åˆ‡æ¢æ ·å¼\n// `toggle_popup`       æ‰“å¼€/å…³é—­æ§åˆ¶é¢æ¿\n// `toggle_transbox`    æ‰“å¼€/å…³é—­ç¿»è¯‘å¼¹çª—\n// `toggle_hover_node`  ç¿»è¯‘é¼ æ ‡æ‚¬åœæ®µè½\n// `input_translate`    ç¿»è¯‘è¾“å…¥æ¡†\nwindow.dispatchEvent(new CustomEvent(\"kiss_translator\", {detail: { action: \"toggle_translate\" }}));\n```\n\n## äº¤æµ\n\n- åŠ å…¥ [Telegram ç¾¤](https://t.me/+RRCu_4oNwrM2NmFl)\n\n## èµèµ\n\n![appreciate](https://github.com/fishjar/kiss-translator/assets/1157624/ebaecabe-2934-4172-8085-af236f5ee399)\n",
      "stars_today": 10
    },
    {
      "id": 3777210,
      "name": "squirrel",
      "full_name": "rime/squirrel",
      "description": "ã€é¼ é¬šç®¡ã€‘Rime for macOS",
      "html_url": "https://github.com/rime/squirrel",
      "stars": 5630,
      "forks": 459,
      "language": "Swift",
      "topics": [],
      "created_at": "2012-03-20T16:17:18Z",
      "updated_at": "2026-01-15T00:23:54Z",
      "pushed_at": "2026-01-14T00:24:56Z",
      "open_issues": 191,
      "owner": {
        "login": "rime",
        "avatar_url": "https://avatars.githubusercontent.com/u/10554324?v=4"
      },
      "readme": "    é¼ é¬šç®¡\n    çˆ²ç‰©é›–å¾®æƒ…ä¸æ·º\n    æ–°è©©é†‰å¢¨æ™‚ä¸€æ®\n    åˆ¥å¾Œå¯„æˆ‘ç„¡è¾­é \n\n    ã€€ã€€ã€€â€”â€”æ­é™½ä¿®\n\nä»Šç”±ã€€[ä¸­å·éŸ»è¼¸å…¥æ³•å¼•æ“ï¼Rime Input Method Engine](https://rime.im)\nåŠå…¶ä»–é–‹æºæŠ€è¡“å¼·åŠ›é©…å‹•\n\nã€é¼ é¬šç®¡ã€‘è¼¸å…¥æ³•\n===\n[![Download](https://img.shields.io/github/v/release/rime/squirrel)](https://github.com/rime/squirrel/releases/latest)\n[![Build Status](https://github.com/rime/squirrel/actions/workflows/commit-ci.yml/badge.svg)](https://github.com/rime/squirrel/actions/workflows)\n[![GitHub Tag](https://img.shields.io/github/tag/rime/squirrel.svg)](https://github.com/rime/squirrel)\n\nå¼æ•å ‚ ç‰ˆæ¬Šæ‰€ç„¡\n\næˆæ¬Šæ¢æ¬¾ï¼š[GPL v3](https://www.gnu.org/licenses/gpl-3.0.en.html)\n\né …ç›®ä¸»é ï¼š[rime.im](https://rime.im)\n\næ‚¨å¯èƒ½é‚„éœ€è¦ Rime ç”¨æ–¼å…¶ä»–æ“ä½œç³»çµ±çš„ç™¼è¡Œç‰ˆï¼š\n\n  * ã€ä¸­å·éŸ»ã€‘ï¼ˆibus-rimeã€fcitx-rimeï¼‰ç”¨æ–¼ Linux\n  * ã€å°ç‹¼æ¯«ã€‘ç”¨æ–¼ Windows\n\nå®‰è£è¼¸å…¥æ³•\n---\n\næœ¬å“é©ç”¨æ–¼ macOS 13.0+\n\nåˆæ¬¡å®‰è£ï¼Œå¦‚æœåœ¨éƒ¨ä»½æ‡‰ç”¨ç¨‹åºä¸­æ‰“ä¸å‡ºå­—ï¼Œè«‹è¨»éŠ·ä¸¦é‡æ–°ç™»éŒ„ã€‚\n\nä½¿ç”¨è¼¸å…¥æ³•\n---\n\né¸å–è¼¸å…¥æ³•æŒ‡ç¤ºå™¨èœå–®è£çš„ã€ã„“ã€‘å­—æ¨£åœ–æ¨™ï¼Œé–‹å§‹ç”¨é¼ é¬šç®¡å¯«å­—ã€‚\né€šéå¿«æ·éµ `` Ctrl+` `` æˆ– `F4` å‘¼å‡ºæ–¹æ¡ˆé¸å–®ã€åˆ‡æ›è¼¸å…¥æ–¹å¼ã€‚\n\nå®šè£½è¼¸å…¥æ³•\n---\n\nå®šè£½æ–¹æ³•ï¼Œè«‹åƒè€ƒç·šä¸Š [å¹«åŠ©æ–‡æª”](https://rime.im/docs/)ã€‚\n\nä½¿ç”¨ç³»çµ±è¼¸å…¥æ³•èœå–®ï¼š\n\n  * é¸ä¸­ã€Œåœ¨ç·šæ–‡æª”ã€å¯æ‰“é–‹ä»¥ä¸Šç¶²å€\n  * ç·¨è¼¯ç”¨æˆ¶è¨­å®šå¾Œï¼Œé¸æ“‡ã€Œé‡æ–°éƒ¨ç½²ã€ä»¥ä»¤ä¿®æ”¹ç”Ÿæ•ˆ\n\nå®‰è£è¼¸å…¥æ–¹æ¡ˆ\n---\n\nä½¿ç”¨ [/plum/](https://github.com/rime/plum) é…ç½®ç®¡ç†å™¨ç²å–æ›´å¤šè¼¸å…¥æ–¹æ¡ˆã€‚\n\nè‡´è¬\n---\n\nè¼¸å…¥æ–¹æ¡ˆè¨­è¨ˆï¼š\n\n  * ã€æœ™æœˆæ‹¼éŸ³ã€‘ç³»åˆ—\n\n    æ„Ÿè¬ CC-CEDICTã€Android æ‹¼éŸ³ã€æ–°é…·éŸ³ã€opencc ç­‰é–‹æºé …ç›®\n\nç¨‹åºè¨­è¨ˆï¼š\n\n  * ä½›æŒ¯\n  * Linghua Zhang\n  * Chongyu Zhu\n  * é›ªé½‹\n  * faberii\n  * Chun-wei Kuo\n  * Junlu Cheng\n  * Jak Wings\n  * xiehuc\n\nç¾è¡“ï¼š\n\n  * åœ–æ¨™è¨­è¨ˆ ä½›æŒ¯ã€æ¢æµ·ã€é›¨éä¹‹å¾Œ\n  * é…è‰²æ–¹æ¡ˆ Abenã€Chongyu Zhuã€skojã€Superoutmanã€ä½›æŒ¯ã€æ¢æµ·\n\næœ¬å“å¼•ç”¨äº†ä»¥ä¸‹é–‹æºè»Ÿä»¶ï¼š\n\n  * Boost C++ Libraries  (Boost Software License)\n  * capnproto (MIT License)\n  * darts-clone  (New BSD License)\n  * google-glog  (New BSD License)\n  * Google Test  (New BSD License)\n  * LevelDB  (New BSD License)\n  * librime  (New BSD License)\n  * OpenCC / é–‹æ”¾ä¸­æ–‡è½‰æ›  (Apache License 2.0)\n  * plum / æ±é¢¨ç ´ (GNU Lesser General Public License 3.0)\n  * Sparkle  (MIT License)\n  * UTF8-CPP  (Boost Software License)\n  * yaml-cpp  (MIT License)\n\næ„Ÿè¬ç‹å…¬å­æè´ˆé–‹ç™¼ç”¨æ©Ÿã€‚\n\nå•é¡Œèˆ‡åé¥‹\n---\n\nç™¼ç¾ç¨‹åºæœ‰ BUGï¼Œæˆ–å»ºè­°ï¼Œæˆ–æ„Ÿæƒ³ï¼Œè«‹åé¥‹åˆ° [Rime ä»£ç¢¼ä¹‹å®¶è¨è«–å€](https://github.com/rime/home/discussions)\n\nè¯ç¹«æ–¹å¼\n---\n\næŠ€è¡“äº¤æµï¼Œæ­¡è¿å…‰è‡¨ [Rime ä»£ç¢¼ä¹‹å®¶](https://github.com/rime/home)ï¼Œ\næˆ–è‡´ä¿¡ Rime é–‹ç™¼è€… <rimeime@gmail.com>ã€‚\n\nè¬è¬\n",
      "stars_today": 10
    },
    {
      "id": 118105436,
      "name": "migrate",
      "full_name": "golang-migrate/migrate",
      "description": "Database migrations. CLI and Golang library.",
      "html_url": "https://github.com/golang-migrate/migrate",
      "stars": 17949,
      "forks": 1541,
      "language": "Go",
      "topics": [
        "aws-s3",
        "cassandra",
        "database",
        "databases",
        "go",
        "golang",
        "google-cloud-spanner",
        "google-cloud-storage",
        "hacktoberfest",
        "mariadb",
        "migration",
        "migrations",
        "mongodb",
        "mysql",
        "neo4j",
        "postgres",
        "spanner",
        "sql",
        "sqlite"
      ],
      "created_at": "2018-01-19T09:30:58Z",
      "updated_at": "2026-01-14T23:41:27Z",
      "pushed_at": "2025-12-14T23:16:48Z",
      "open_issues": 448,
      "owner": {
        "login": "golang-migrate",
        "avatar_url": "https://avatars.githubusercontent.com/u/35595841?v=4"
      },
      "readme": "[![GitHub Workflow Status (branch)](https://img.shields.io/github/actions/workflow/status/golang-migrate/migrate/ci.yaml?branch=master)](https://github.com/golang-migrate/migrate/actions/workflows/ci.yaml?query=branch%3Amaster)\n[![GoDoc](https://pkg.go.dev/badge/github.com/golang-migrate/migrate)](https://pkg.go.dev/github.com/golang-migrate/migrate/v4)\n[![Coverage Status](https://img.shields.io/coveralls/github/golang-migrate/migrate/master.svg)](https://coveralls.io/github/golang-migrate/migrate?branch=master)\n[![packagecloud.io](https://img.shields.io/badge/deb-packagecloud.io-844fec.svg)](https://packagecloud.io/golang-migrate/migrate?filter=debs)\n[![Docker Pulls](https://img.shields.io/docker/pulls/migrate/migrate.svg)](https://hub.docker.com/r/migrate/migrate/)\n![Supported Go Versions](https://img.shields.io/badge/Go-1.24%2C%201.25-lightgrey.svg)\n[![GitHub Release](https://img.shields.io/github/release/golang-migrate/migrate.svg)](https://github.com/golang-migrate/migrate/releases)\n[![Go Report Card](https://goreportcard.com/badge/github.com/golang-migrate/migrate/v4)](https://goreportcard.com/report/github.com/golang-migrate/migrate/v4)\n\n# migrate\n\n__Database migrations written in Go. Use as [CLI](#cli-usage) or import as [library](#use-in-your-go-project).__\n\n* Migrate reads migrations from [sources](#migration-sources)\n   and applies them in correct order to a [database](#databases).\n* Drivers are \"dumb\", migrate glues everything together and makes sure the logic is bulletproof.\n   (Keeps the drivers lightweight, too.)\n* Database drivers don't assume things or try to correct user input. When in doubt, fail.\n\nForked from [mattes/migrate](https://github.com/mattes/migrate)\n\n## Databases\n\nDatabase drivers run migrations. [Add a new database?](database/driver.go)\n\n* [PostgreSQL](database/postgres)\n* [PGX v4](database/pgx)\n* [PGX v5](database/pgx/v5)\n* [Redshift](database/redshift)\n* [Ql](database/ql)\n* [Cassandra / ScyllaDB](database/cassandra)\n* [SQLite](database/sqlite)\n* [SQLite3](database/sqlite3) ([todo #165](https://github.com/mattes/migrate/issues/165))\n* [SQLCipher](database/sqlcipher)\n* [MySQL / MariaDB](database/mysql)\n* [Neo4j](database/neo4j)\n* [MongoDB](database/mongodb)\n* [CrateDB](database/crate) ([todo #170](https://github.com/mattes/migrate/issues/170))\n* [Shell](database/shell) ([todo #171](https://github.com/mattes/migrate/issues/171))\n* [Google Cloud Spanner](database/spanner)\n* [CockroachDB](database/cockroachdb)\n* [YugabyteDB](database/yugabytedb)\n* [ClickHouse](database/clickhouse)\n* [Firebird](database/firebird)\n* [MS SQL Server](database/sqlserver)\n* [rqlite](database/rqlite)\n\n### Database URLs\n\nDatabase connection strings are specified via URLs. The URL format is driver dependent but generally has the form: `dbdriver://username:password@host:port/dbname?param1=true&param2=false`\n\nAny [reserved URL characters](https://en.wikipedia.org/wiki/Percent-encoding#Percent-encoding_reserved_characters) need to be escaped. Note, the `%` character also [needs to be escaped](https://en.wikipedia.org/wiki/Percent-encoding#Percent-encoding_the_percent_character)\n\nExplicitly, the following characters need to be escaped:\n`!`, `#`, `$`, `%`, `&`, `'`, `(`, `)`, `*`, `+`, `,`, `/`, `:`, `;`, `=`, `?`, `@`, `[`, `]`\n\nIt's easiest to always run the URL parts of your DB connection URL (e.g. username, password, etc) through an URL encoder. See the example Python snippets below:\n\n```bash\n$ python3 -c 'import urllib.parse; print(urllib.parse.quote(input(\"String to encode: \"), \"\"))'\nString to encode: FAKEpassword!#$%&'()*+,/:;=?@[]\nFAKEpassword%21%23%24%25%26%27%28%29%2A%2B%2C%2F%3A%3B%3D%3F%40%5B%5D\n$ python2 -c 'import urllib; print urllib.quote(raw_input(\"String to encode: \"), \"\")'\nString to encode: FAKEpassword!#$%&'()*+,/:;=?@[]\nFAKEpassword%21%23%24%25%26%27%28%29%2A%2B%2C%2F%3A%3B%3D%3F%40%5B%5D\n$\n```\n\n## Migration Sources\n\nSource drivers read migrations from local or remote sources. [Add a new source?](source/driver.go)\n\n* [Filesystem](source/file) - read from filesystem\n* [io/fs](source/iofs) - read from a Go [io/fs](https://pkg.go.dev/io/fs#FS)\n* [Go-Bindata](source/go_bindata) - read from embedded binary data ([jteeuwen/go-bindata](https://github.com/jteeuwen/go-bindata))\n* [pkger](source/pkger) - read from embedded binary data ([markbates/pkger](https://github.com/markbates/pkger))\n* [GitHub](source/github) - read from remote GitHub repositories\n* [GitHub Enterprise](source/github_ee) - read from remote GitHub Enterprise repositories\n* [Bitbucket](source/bitbucket) - read from remote Bitbucket repositories\n* [Gitlab](source/gitlab) - read from remote Gitlab repositories\n* [AWS S3](source/aws_s3) - read from Amazon Web Services S3\n* [Google Cloud Storage](source/google_cloud_storage) - read from Google Cloud Platform Storage\n\n## CLI usage\n\n* Simple wrapper around this library.\n* Handles ctrl+c (SIGINT) gracefully.\n* No config search paths, no config files, no magic ENV var injections.\n\n[CLI Documentation](cmd/migrate) (includes CLI install instructions)\n\n### Basic usage\n\n```bash\n$ migrate -source file://path/to/migrations -database postgres://localhost:5432/database up 2\n```\n\n### Docker usage\n\n```bash\n$ docker run -v {{ migration dir }}:/migrations --network host migrate/migrate\n    -path=/migrations/ -database postgres://localhost:5432/database up 2\n```\n\n## Use in your Go project\n\n* API is stable and frozen for this release (v3 & v4).\n* Uses [Go modules](https://golang.org/cmd/go/#hdr-Modules__module_versions__and_more) to manage dependencies.\n* To help prevent database corruptions, it supports graceful stops via `GracefulStop chan bool`.\n* Bring your own logger.\n* Uses `io.Reader` streams internally for low memory overhead.\n* Thread-safe and no goroutine leaks.\n\n__[Go Documentation](https://pkg.go.dev/github.com/golang-migrate/migrate/v4)__\n\n```go\nimport (\n    \"github.com/golang-migrate/migrate/v4\"\n    _ \"github.com/golang-migrate/migrate/v4/database/postgres\"\n    _ \"github.com/golang-migrate/migrate/v4/source/github\"\n)\n\nfunc main() {\n    m, err := migrate.New(\n        \"github://mattes:personal-access-token@mattes/migrate_test\",\n        \"postgres://localhost:5432/database?sslmode=enable\")\n    m.Steps(2)\n}\n```\n\nWant to use an existing database client?\n\n```go\nimport (\n    \"database/sql\"\n    _ \"github.com/lib/pq\"\n    \"github.com/golang-migrate/migrate/v4\"\n    \"github.com/golang-migrate/migrate/v4/database/postgres\"\n    _ \"github.com/golang-migrate/migrate/v4/source/file\"\n)\n\nfunc main() {\n    db, err := sql.Open(\"postgres\", \"postgres://localhost:5432/database?sslmode=enable\")\n    driver, err := postgres.WithInstance(db, &postgres.Config{})\n    m, err := migrate.NewWithDatabaseInstance(\n        \"file:///migrations\",\n        \"postgres\", driver)\n    m.Up() // or m.Steps(2) if you want to explicitly set the number of migrations to run\n}\n```\n\n## Getting started\n\nGo to [getting started](GETTING_STARTED.md)\n\n## Tutorials\n\n* [CockroachDB](database/cockroachdb/TUTORIAL.md)\n* [PostgreSQL](database/postgres/TUTORIAL.md)\n\n(more tutorials to come)\n\n## Migration files\n\nEach migration has an up and down migration. [Why?](FAQ.md#why-two-separate-files-up-and-down-for-a-migration)\n\n```bash\n1481574547_create_users_table.up.sql\n1481574547_create_users_table.down.sql\n```\n\n[Best practices: How to write migrations.](MIGRATIONS.md)\n\n## Coming from another db migration tool?\n\nCheck out [migradaptor](https://github.com/musinit/migradaptor/).\n*Note: migradaptor is not affiliated or supported by this project*\n\n## Versions\n\nVersion | Supported? | Import | Notes\n--------|------------|--------|------\n**master** | :white_check_mark: | `import \"github.com/golang-migrate/migrate/v4\"` | New features and bug fixes arrive here first |\n**v4** | :white_check_mark: | `import \"github.com/golang-migrate/migrate/v4\"` | Used for stable releases |\n**v3** | :x: | `import \"github.com/golang-migrate/migrate\"` (with package manager) or `import \"gopkg.in/golang-migrate/migrate.v3\"` (not recommended) | **DO NOT USE** - No longer supported |\n\n## Development and Contributing\n\nYes, please! [`Makefile`](Makefile) is your friend,\nread the [development guide](CONTRIBUTING.md).\n\nAlso have a look at the [FAQ](FAQ.md).\n\n---\n\nLooking for alternatives? [https://awesome-go.com/#database](https://awesome-go.com/#database).\n",
      "stars_today": 9
    },
    {
      "id": 958136139,
      "name": "gallery",
      "full_name": "google-ai-edge/gallery",
      "description": "A gallery that showcases on-device ML/GenAI use cases and allows people to try and use models locally.",
      "html_url": "https://github.com/google-ai-edge/gallery",
      "stars": 14852,
      "forks": 1270,
      "language": "Kotlin",
      "topics": [],
      "created_at": "2025-03-31T17:47:28Z",
      "updated_at": "2026-01-14T23:22:00Z",
      "pushed_at": "2026-01-14T18:12:54Z",
      "open_issues": 156,
      "owner": {
        "login": "google-ai-edge",
        "avatar_url": "https://avatars.githubusercontent.com/u/150697620?v=4"
      },
      "readme": "# Google AI Edge Gallery âœ¨\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)\n[![GitHub release (latest by date)](https://img.shields.io/github/v/release/google-ai-edge/gallery)](https://github.com/google-ai-edge/gallery/releases)\n\n**Explore, Experience, and Evaluate the Future of On-Device Generative AI with Google AI Edge.**\n\nThe Google AI Edge Gallery is an experimental app that puts the power of cutting-edge Generative AI models directly into your hands, running entirely on your Android *(available now)* and iOS *(available now via TestFlight)* devices. Dive into a world of creative and practical AI use cases, all running locally, without needing an internet connection once the model is loaded. Experiment with different models, chat, ask questions with images and audio clip, explore prompts, and more!\n\nInstall the app today from Google Play\n\n<a href='https://play.google.com/store/apps/details?id=com.google.ai.edge.gallery'><img alt='Get it on Google Play' width=\"250\" src='https://play.google.com/intl/en_us/badges/static/images/badges/en_badge_web_generic.png'/></a>\n\nFor users without Google Play access, install the apk from the [**latest release**](https://github.com/google-ai-edge/gallery/releases/latest/)\n\n> [!IMPORTANT]\n> You must uninstall all previous versions of the app before installing this one. Past versions will no longer be working and supported.\n\n## iOS Testing via TestFlight\n\nWe're excited to announce that the app is now available for testing on iOS through TestFlight! We invite you to be among the first to try it out and share your feedback.\n\n***How to Join***:\n\n- Follow this [**public invitation link**](https://testflight.apple.com/join/nAtSQKTF) to get access.\n\n- Availability: Access is on a first-come, first-served basis. TestFlight currently limits the number of testers to 10,000.\n  \n- Supported device models: iOS devices with at least 6GB of RAM.\n\nWe appreciate your help with this early testing phase. Your feedback is invaluable as we work to improve the app. Once we've gathered and addressed all the feedback, we aim to officially launch on the App Store early 2026.\n\n<img width=\"480\" alt=\"01\" src=\"https://github.com/user-attachments/assets/09dbcf7e-a298-4063-920e-bfc88591f4a2\" />\n<img width=\"480\" alt=\"02\" src=\"https://github.com/user-attachments/assets/e2986bba-f807-42e1-9d5e-a5a978fa97e9\" />\n<img width=\"480\" alt=\"03\" src=\"https://github.com/user-attachments/assets/ad3aa9ab-e3b6-4a12-bbd4-885bb202aa0f\" />\n<img width=\"480\" alt=\"04\" src=\"https://github.com/user-attachments/assets/6441e752-e5f5-4753-9611-fa0122cdae49\" />\n<img width=\"480\" alt=\"05\" src=\"https://github.com/user-attachments/assets/a5ebcf15-640a-4c11-93ce-b92fe365f1a3\" />\n<img width=\"480\" alt=\"06\" src=\"https://github.com/user-attachments/assets/973c7a66-1906-400e-8fac-ee9b13b21aa1\" />\n<img width=\"480\" alt=\"07\" src=\"https://github.com/user-attachments/assets/d3227bc6-8d78-47a1-bbfa-93f009117882\" />\n\n## âœ¨ Core Features\n\n*   **ğŸ“± Run Locally, Fully Offline:** Experience the magic of GenAI without an internet connection. All processing happens directly on your device.\n*   **ğŸ¤– Choose Your Model:** Easily switch between different models from Hugging Face and compare their performance.\n*   **ğŸŒ» Tiny Garden**: Play an experimental and fully offline mini game that uses natural language to plant, water, and harvest flowers.\n*   **ğŸ“³ Mobile Actions**: Use our [open-source recipe](https://github.com/google-gemini/gemma-cookbook/blob/main/FunctionGemma/%5BFunctionGemma%5DFinetune_FunctionGemma_270M_for_Mobile_Actions_with_Hugging_Face.ipynb) to learn model fine-tuning, then load it in app to unlock offline device controls.\n*   **ğŸ–¼ï¸ Ask Image:** Upload images and ask questions about them. Get descriptions, solve problems, or identify objects.\n*   **ğŸ™ï¸ Audio Scribe:** Transcribe an uploaded or recorded audio clip into text or translate it into another language.\n*   **âœï¸ Prompt Lab:** Summarize, rewrite, generate code, or use freeform prompts to explore single-turn LLM use cases.\n*   **ğŸ’¬ AI Chat:** Engage in multi-turn conversations.\n*   **ğŸ“Š Performance Insights:** Real-time benchmarks (TTFT, decode speed, latency).\n*   **ğŸ§© Bring Your Own Model:** Test your local LiteRT `.litertlm` models.\n*   **ğŸ”— Developer Resources:** Quick links to model cards and source code.\n\n## ğŸ Get Started in Minutes!\n\n1. **Check OS Requirement**: Android 12 and up\n2.  **Download the App:**\n    - Install the app from [Google Play](https://play.google.com/store/apps/details?id=com.google.ai.edge.gallery).\n    - For users without Google Play access: install the apk from the [**latest release**](https://github.com/google-ai-edge/gallery/releases/latest/)\n3.  **Install & Explore:** For detailed installation instructions (including for corporate devices) and a full user guide, head over to our [**Project Wiki**](https://github.com/google-ai-edge/gallery/wiki)!\n\n## ğŸ› ï¸ Technology Highlights\n\n*   **Google AI Edge:** Core APIs and tools for on-device ML.\n*   **LiteRT:** Lightweight runtime for optimized model execution.\n*   **LLM Inference API:** Powering on-device Large Language Models.\n*   **Hugging Face Integration:** For model discovery and download.\n\n## âŒ¨ï¸ Development\n\nCheck out the [development notes](DEVELOPMENT.md) for instructions about how to build the app locally.\n\n## ğŸ¤ Feedback\n\nThis is an **experimental Beta release**, and your input is crucial!\n\n*   ğŸ **Found a bug?** [Report it here!](https://github.com/google-ai-edge/gallery/issues/new?assignees=&labels=bug&template=bug_report.md&title=%5BBUG%5D)\n*   ğŸ’¡ **Have an idea?** [Suggest a feature!](https://github.com/google-ai-edge/gallery/issues/new?assignees=&labels=enhancement&template=feature_request.md&title=%5BFEATURE%5D)\n\n## ğŸ“„ License\n\nLicensed under the Apache License, Version 2.0. See the [LICENSE](LICENSE) file for details.\n\n## ğŸ”— Useful Links\n\n*   [**Project Wiki (Detailed Guides)**](https://github.com/google-ai-edge/gallery/wiki)\n*   [Hugging Face LiteRT Community](https://huggingface.co/litert-community)\n*   [LLM Inference guide for Android](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/android)\n*   [LiteRT-LM](https://github.com/google-ai-edge/LiteRT-LM)\n*   [Google AI Edge Documentation](https://ai.google.dev/edge)\n",
      "stars_today": 9
    },
    {
      "id": 79307564,
      "name": "WeChatTweak",
      "full_name": "sunnyyoung/WeChatTweak",
      "description": "A command-line tool for tweaking WeChat - é¦–æ¬¾å¾®ä¿¡ macOS å®¢æˆ·ç«¯æ’¤å›æ‹¦æˆªä¸å¤šå¼€ ğŸ”¨",
      "html_url": "https://github.com/sunnyyoung/WeChatTweak",
      "stars": 13256,
      "forks": 1552,
      "language": "Swift",
      "topics": [
        "alfred",
        "alfred-workflow",
        "macos",
        "no-revoke",
        "norevoke",
        "raycast-extension",
        "revoke",
        "tweak",
        "wechat",
        "wechat-macos",
        "wechat-plugin",
        "wechat-plugin-macos",
        "wechat-raycast",
        "wechat-tweak",
        "wechathook",
        "wechattweak",
        "wechattweak-macos",
        "weixin",
        "weixin-plugin",
        "weixin-tweak"
      ],
      "created_at": "2017-01-18T05:42:16Z",
      "updated_at": "2026-01-14T19:43:33Z",
      "pushed_at": "2025-12-13T17:05:51Z",
      "open_issues": 61,
      "owner": {
        "login": "sunnyyoung",
        "avatar_url": "https://avatars.githubusercontent.com/u/5926284?v=4"
      },
      "readme": "# WeChatTweak\n\n[![README](https://img.shields.io/badge/GitHub-black?logo=github&logoColor=white)](https://github.com/sunnyyoung/WeChatTweak)\n[![README](https://img.shields.io/badge/Telegram-black?logo=telegram&logoColor=white)](https://t.me/wechattweak)\n[![README](https://img.shields.io/badge/FAQ-black?logo=googledocs&logoColor=white)](https://github.com/sunnyyoung/WeChatTweak/wiki/FAQ)\n\nA command-line tool for tweaking WeChat.\n\n## åŠŸèƒ½\n\n- é˜»æ­¢æ¶ˆæ¯æ’¤å›\n- é˜»æ­¢è‡ªåŠ¨æ›´æ–°\n- å®¢æˆ·ç«¯å¤šå¼€\n\n## å®‰è£…&ä½¿ç”¨\n\n```bash\n# å®‰è£…\nbrew install sunnyyoung/tap/wechattweak\n\n# æ›´æ–°\nbrew upgrade wechattweak\n\n# æ‰§è¡Œ Patch\nwechattweak patch\n\n# æŸ¥çœ‹æ‰€æœ‰æ”¯æŒçš„ WeChat ç‰ˆæœ¬\nwechattweak versions\n```\n\n## å‚è€ƒ\n\n- [å¾®ä¿¡ macOS å®¢æˆ·ç«¯æ— é™å¤šå¼€åŠŸèƒ½å®è·µ](https://blog.sunnyyoung.net/wei-xin-macos-ke-hu-duan-wu-xian-duo-kai-gong-neng-shi-jian/)\n- [å¾®ä¿¡ macOS å®¢æˆ·ç«¯æ‹¦æˆªæ’¤å›åŠŸèƒ½å®è·µ](https://blog.sunnyyoung.net/wei-xin-macos-ke-hu-duan-lan-jie-che-hui-gong-neng-shi-jian/)\n- [è®©å¾®ä¿¡ macOS å®¢æˆ·ç«¯æ”¯æŒ Alfred](https://blog.sunnyyoung.net/rang-wei-xin-macos-ke-hu-duan-zhi-chi-alfred/)\n\n## è´¡çŒ®è€…\n\nThis project exists thanks to all the people who contribute.\n\n[![Contributors](https://contrib.rocks/image?repo=sunnyyoung/WeChatTweak)](https://github.com/sunnyyoung/WeChatTweak/graphs/contributors)\n\n## License\n\nThe [AGPL-3.0](LICENSE).\n",
      "stars_today": 9
    },
    {
      "id": 48714685,
      "name": "opa",
      "full_name": "open-policy-agent/opa",
      "description": "Open Policy Agent (OPA) is an open source, general-purpose policy engine.",
      "html_url": "https://github.com/open-policy-agent/opa",
      "stars": 11087,
      "forks": 1502,
      "language": "Go",
      "topics": [
        "authorization",
        "cloud-native",
        "compliance",
        "declarative",
        "json",
        "opa",
        "open-policy-agent",
        "policy"
      ],
      "created_at": "2015-12-28T22:08:25Z",
      "updated_at": "2026-01-15T01:02:00Z",
      "pushed_at": "2026-01-15T00:30:41Z",
      "open_issues": 384,
      "owner": {
        "login": "open-policy-agent",
        "avatar_url": "https://avatars.githubusercontent.com/u/16468693?v=4"
      },
      "readme": "# ![logo](./logo/logo-144x144.png) Open Policy Agent\n\n[![Build Status](https://github.com/open-policy-agent/opa/workflows/Post%20Merge/badge.svg)](https://github.com/open-policy-agent/opa/actions) [![Go Report Card](https://goreportcard.com/badge/github.com/open-policy-agent/opa)](https://goreportcard.com/report/github.com/open-policy-agent/opa) [![CII Best Practices](https://www.bestpractices.dev/projects/1768/badge)](https://www.bestpractices.dev/en/projects/1768/passing) [![Netlify Status](https://api.netlify.com/api/v1/badges/4a0a092a-8741-4826-a28f-826d4a576cab/deploy-status)](https://app.netlify.com/sites/openpolicyagent/deploys)\n\nOpen Policy Agent (OPA) is an open source, general-purpose policy engine that enables unified, context-aware policy enforcement across the entire stack.\n\nOPA is proud to be a graduated project in the [Cloud Native Computing Foundation](https://www.cncf.io/) (CNCF) landscape. For details read the CNCF [announcement](https://www.cncf.io/announcements/2021/02/04/cloud-native-computing-foundation-announces-open-policy-agent-graduation/).\n\n## Get started with OPA\n\n- Write your first Rego policy with the [Rego Playground](https://play.openpolicyagent.org) or use it to share your work with others for feedback and support. Have a look at the [Access Control examples](https://play.openpolicyagent.org/?example-group=access-control) if you're not sure where to start.\n- Install the [VS Code extension](https://marketplace.visualstudio.com/items?itemName=tsandall.opa) to get started locally with live diagnostics, debugging and formatting. See [Editor and IDE Support](https://www.openpolicyagent.org/docs/editor-and-ide-support) for other supported editors.\n- Go to the [OPA Documentation](https://www.openpolicyagent.org/docs) to\n  learn about the Rego language as well as how to deploy and integrate OPA.\n- Check out the learning resources in the [Learning Rego](https://www.openpolicyagent.org/ecosystem/by-feature/learning-rego) section of the ecosystem directory.\n- Follow the [Running OPA](https://www.openpolicyagent.org/docs/latest/#running-opa) instructions to get started with the OPA CLI locally.\n- See [Docker Hub](https://hub.docker.com/r/openpolicyagent/opa/tags/) for container images and the [GitHub releases](https://github.com/open-policy-agent/opa/releases) for binaries.\n- Check out the [OPA Roadmap](https://docs.google.com/presentation/d/16QV6gvLDOV3I0_guPC3_19g6jHkEg3X9xqMYgtoCKrs/edit?usp=sharing) to see a high-level snapshot of OPA features in-progress and planned.\n\n## Want to talk about OPA or get support?\n\n- Join the [OPA Slack](https://slack.openpolicyagent.org) to talk to other OPA users and maintainers. See `#help` for support.\n- Check out the [Community Discussions](https://github.com/orgs/open-policy-agent/discussions) to ask questions.\n- See the [Support](https://www.openpolicyagent.org/support) page for commercial support options.\n\n## Interested to learn what others are doing with OPA?\n\n- Browse community projects on the [OPA Ecosystem Directory](https://www.openpolicyagent.org/ecosystem) - don't forget to [list your own](https://github.com/open-policy-agent/opa/tree/main/docs#opa-ecosystem)!\n- Check out the [ADOPTERS.md](./ADOPTERS.md) file for a list of production adopters. Does your organization use OPA in production? Support the OPA project by submitting a PR to add your organization to the list with a short description of your OPA use cases!\n\n## Want to integrate OPA?\n\n- See the high-level [Go SDK](https://www.openpolicyagent.org/docs/latest/integration/#integrating-with-the-go-sdk) or the low-level Go API\n  [![GoDoc](https://godoc.org/github.com/open-policy-agent/opa?status.svg)](https://godoc.org/github.com/open-policy-agent/opa/rego)\n  to integrate OPA with services written in Go.\n- See the [REST API](https://www.openpolicyagent.org/docs/rest-api.html)\n  reference to integrate OPA with services written in other languages.\n- See the [integration docs](https://www.openpolicyagent.org/docs/latest/integration/) for more options.\n\n## Want to contribute to OPA?\n\n- Read the [Contributing Guide](https://www.openpolicyagent.org/docs/latest/contributing/) to learn how to make your first contribution.\n- Use [#contributors](https://openpolicyagent.slack.com/archives/C02L1TLPN59) in Slack to talk to other contributors and OPA maintainers.\n- File a [GitHub Issue](https://github.com/open-policy-agent/opa/issues) to request features or report bugs.\n\n## How does OPA work?\n\nOPA gives you a high-level declarative language to author and enforce policies\nacross your stack.\n\nWith OPA, you define _rules_ that govern how your system should behave. These\nrules exist to answer questions like:\n\n- Can user X call operation Y on resource Z?\n- What clusters should workload W be deployed to?\n- What tags must be set on resource R before it's created?\n\nYou integrate services with OPA so that these kinds of policy decisions do not\nhave to be _hardcoded_ in your service. Services integrate with OPA by\nexecuting _queries_ when policy decisions are needed.\n\nWhen you query OPA for a policy decision, OPA evaluates the rules and data\n(which you give it) to produce an answer. The policy decision is sent back as\nthe result of the query.\n\nFor example, in a simple API authorization use case:\n\n- You write rules that allow (or deny) access to your service APIs.\n- Your service queries OPA when it receives API requests.\n- OPA returns allow (or deny) decisions to your service.\n- Your service _enforces_ the decisions by accepting or rejecting requests accordingly.\n\nFor concrete examples of how to integrate OPA with systems like\n[Kubernetes](https://www.openpolicyagent.org/docs/kubernetes),\n[Terraform](https://www.openpolicyagent.org/docs/terraform),\n[Docker](https://www.openpolicyagent.org/docs/docker-authorization),\n[SSH](https://www.openpolicyagent.org/docs/ssh-and-sudo-authorization),\nand more, see [openpolicyagent.org](https://www.openpolicyagent.org).\n\n## Presentations\n\n- Open Policy Agent (OPA) Intro & Deep Dive @ Kubecon NA 2023: [video](https://www.youtube.com/watch?v=wJkjsvVpj_Q)\n- Open Policy Agent (OPA) Intro & Deep Dive @ Kubecon EU 2023: [video](https://www.youtube.com/watch?v=6RNp3m_THw4)\n- Running Policy in Hard to Reach Places with WASM & OPA @ CN Wasm Day EU 2023: [video](https://www.youtube.com/watch?v=BdeBhukLwt4)\n- OPA maintainers talk @ Kubecon NA 2022: [video](https://www.youtube.com/watch?v=RMiovzGGCfI)\n- Open Policy Agent (OPA) Intro & Deep Dive @ Kubecon EU 2022: [video](https://www.youtube.com/watch?v=MhyQxIp1H58)\n- Open Policy Agent Intro @ KubeCon EU 2021: [Video](https://www.youtube.com/watch?v=2CgeiWkliaw)\n- Using Open Policy Agent to Meet Evolving Policy Requirements @ KubeCon NA 2020: [video](https://www.youtube.com/watch?v=zVuM7F_BTyc)\n- Applying Policy Throughout The Application Lifecycle with Open Policy Agent @ CloudNativeCon 2019: [video](https://www.youtube.com/watch?v=cXfsaE6RKfc)\n- Open Policy Agent Introduction @ CloudNativeCon EU 2018: [video](https://youtu.be/XEHeexPpgrA), [slides](https://www.slideshare.net/TorinSandall/opa-the-cloud-native-policy-engine)\n- Rego Deep Dive @ CloudNativeCon EU 2018: [video](https://youtu.be/4mBJSIhs2xQ), [slides](https://www.slideshare.net/TorinSandall/rego-deep-dive)\n- How Netflix Is Solving Authorization Across Their Cloud @ CloudNativeCon US 2017: [video](https://www.youtube.com/watch?v=R6tUNpRpdnY), [slides](https://www.slideshare.net/TorinSandall/how-netflix-is-solving-authorization-across-their-cloud).\n- Policy-based Resource Placement in Kubernetes Federation @ LinuxCon Beijing 2017: [slides](https://www.slideshare.net/TorinSandall/policybased-resource-placement-across-hybrid-cloud), [screencast](https://www.youtube.com/watch?v=hRz13baBhfg&feature=youtu.be)\n- Enforcing Bespoke Policies In Kubernetes @ KubeCon US 2017: [video](https://www.youtube.com/watch?v=llDI8VvkUj8), [slides](https://www.slideshare.net/TorinSandall/enforcing-bespoke-policies-in-kubernetes)\n- Istio's Mixer: Policy Enforcement with Custom Adapters @ CloudNativeCon US 2017: [video](https://www.youtube.com/watch?v=czZLXUqzd24), [slides](https://www.slideshare.net/TorinSandall/istios-mixer-policy-enforcement-with-custom-adapters-cloud-nativecon-17)\n\n## Security\n\nA third party security audit was performed by Cure53, you can see the full report [here](SECURITY_AUDIT.pdf).\n\nPlease report vulnerabilities by email to [open-policy-agent-security](mailto:open-policy-agent-security@googlegroups.com).\nWe will send a confirmation message to acknowledge that we have received the\nreport and then we will send additional messages to follow up once the issue\nhas been investigated.\n",
      "stars_today": 9
    },
    {
      "id": 476427476,
      "name": "Maestro",
      "full_name": "mobile-dev-inc/Maestro",
      "description": "Painless E2E Automation for Mobile and Web",
      "html_url": "https://github.com/mobile-dev-inc/Maestro",
      "stars": 10038,
      "forks": 564,
      "language": "Kotlin",
      "topics": [
        "android",
        "blackbox-testing",
        "ios",
        "ui-automation"
      ],
      "created_at": "2022-03-31T18:17:40Z",
      "updated_at": "2026-01-14T20:02:00Z",
      "pushed_at": "2026-01-12T11:25:13Z",
      "open_issues": 481,
      "owner": {
        "login": "mobile-dev-inc",
        "avatar_url": "https://avatars.githubusercontent.com/u/65870663?v=4"
      },
      "readme": "> [!TIP]\n> Great things happen when testers connect â€” [Join the Maestro Community](https://maestrodev.typeform.com/to/FelIEe8A)\n\n\n<p align=\"center\">\n  <a href=\"https://www.maestro.dev\">\n    <img width=\"1200\" alt=\"Maestro logo\" src=\"https://github.com/mobile-dev-inc/Maestro/blob/main/assets/banne_logo.png\" />\n  </a>\n</p>\n\n\n<p align=\"center\">\n  <strong>Maestro</strong> is an open-source framework that makes UI and end-to-end testing for Android, iOS, and web apps simple and fast.<br/>\n  Write your first test in under five minutes using YAML flows and run them on any emulator, simulator, or browser.\n</p>\n\n<img src=\"https://user-images.githubusercontent.com/847683/187275009-ddbdf963-ce1d-4e07-ac08-b10f145e8894.gif\" />\n\n---\n\n## Table of Contents\n\n- [Why Maestro?](#why-maestro)\n- [Getting Started](#getting-started)\n- [Resources & Community](#resources--community)\n- [Contributing](#contributing)\n- [Maestro Studio â€“ Test IDE](#maestro-studio--test-ide)\n- [Maestro Cloud â€“ Parallel Execution & Scalability](#maestro-cloud--parallel-execution--scalability)\n\n\n---\n\n## Why Maestro?\n\nMaestro is built on learnings from its predecessors (Appium, Espresso, UIAutomator, XCTest, Selenium, Playwright) and allows you to easily define and test your Flows.\n\nBy combining a human-readable YAML syntax with an interpreted execution engine, it lets you write, run, and scale cross-platform end-to-end tests for mobile and web with ease.\n\n- **Cross-platform coverage** â€“ test Android, iOS, and web apps (React Native, Flutter, hybrid) on emulators, simulators, or real devices.  \n- **Human-readable YAML flows** â€“ express interactions as commands like `launchApp`, `tapOn`, and `assertVisible`.  \n- **Resilience & smart waiting** â€“ built-in flakiness tolerance and automatic waiting handle dynamic UIs without manual `sleep()` calls.  \n- **Fast iteration & simple install** â€“ flows are interpreted (no compilation) and installation is a single script.\n\n**Simple Example:**\n```\n# flow_contacts_android.yaml\n\nappId: com.android.contacts\n---\n- launchApp\n- tapOn: \"Create new contact\"\n- tapOn: \"First Name\"\n- inputText: \"John\"\n- tapOn: \"Last Name\"\n- inputText: \"Snow\"\n- tapOn: \"Save\"\n```\n\n---\n## Getting Started\n\nMaestro requires Java 17 or higher to be installed on your system. You can verify your Java version by running:\n\n```\njava -version\n```\n\nInstalling the CLI:\n\nRun the following command to install Maestro on macOS, Linux or Windows (WSL):\n\n```\ncurl -fsSL \"https://get.maestro.mobile.dev\" | bash\n```\n\nThe links below will guide you through the next steps.\n\n- [Installing Maestro](https://docs.maestro.dev/getting-started/installing-maestro) (includes regular Windows installation)\n- [Build and install your app](https://docs.maestro.dev/getting-started/build-and-install-your-app)\n- [Run a sample flow](https://docs.maestro.dev/getting-started/run-a-sample-flow)\n- [Writing your first flow](https://docs.maestro.dev/getting-started/writing-your-first-flow)\n\n\n---\n\n## Resources & Community\n\n- ğŸ’¬ [Join the Slack Community](https://maestrodev.typeform.com/to/FelIEe8A)\n- ğŸ“˜ [Documentation](https://docs.maestro.dev)  \n- ğŸ“° [Blog](https://maestro.dev/blog?utm_source=github-readme) \n- ğŸ¦ [Follow us on X](https://twitter.com/maestro__dev)\n\n---\n\n## Contributing\n\nMaestro is open-source under the Apache 2.0 license â€” contributions are welcome!\n\n- Check [good first issues](https://github.com/mobile-dev-inc/maestro/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n- Read the [Contribution Guide](https://github.com/mobile-dev-inc/Maestro/blob/main/CONTRIBUTING.md) \n- Fork, create a branch, and open a Pull Request.\n\nIf you find Maestro useful, â­ star the repository to support the project.\n\n---\n\n## Maestro Studio â€“ Test IDE\n\n**Maestro Studio Desktop** is a lightweight IDE that lets you design and execute tests visually â€” no terminal needed. \nIt is also free, even though Studio is not an open-source project. So you won't find the Maestro Studio code here.\n\n- **Simple setup** â€“ just download the native app for macOS, Windows, or Linux.  \n- **Visual flow builder & inspector** â€“ record interactions, inspect elements, and build flows visually.  \n- **AI assistance** â€“ use MaestroGPT to generate commands and answer questions while authoring tests.\n\n[Download Maestro Studio](https://maestro.dev/?utm_source=github-readme#maestro-studio)\n\n---\n\n## Maestro Cloud â€“ Parallel Execution & Scalability\n\nWhen your test suite grows, run hundreds of tests in parallel on dedicated infrastructure, cutting execution times by up to 90%. Includes built-in notifications, deterministic environments, and complete debugging tools.\n\nPricing for Maestro Cloud is completely transparent and can be found on the [pricing page](https://maestro.dev/pricing?utm_source=github-readme).\n\nğŸ‘‰ [Start your free 7-day trial](https://maestro.dev/cloud?utm_source=github-readme)\n\n\n\n```\n  Built with â¤ï¸ by Maestro.dev\n```\n\n\n",
      "stars_today": 9
    },
    {
      "id": 18280236,
      "name": "gitbook",
      "full_name": "GitbookIO/gitbook",
      "description": "The open source frontend for GitBook doc sites",
      "html_url": "https://github.com/GitbookIO/gitbook",
      "stars": 28560,
      "forks": 4021,
      "language": "TypeScript",
      "topics": [
        "documentation",
        "git",
        "gitbook",
        "markdown"
      ],
      "created_at": "2014-03-31T03:01:56Z",
      "updated_at": "2026-01-14T23:47:03Z",
      "pushed_at": "2026-01-14T14:39:10Z",
      "open_issues": 68,
      "owner": {
        "login": "GitbookIO",
        "avatar_url": "https://avatars.githubusercontent.com/u/7111340?v=4"
      },
      "readme": "<h1 align=\"center\">GitBook</h1>\n\n<p align=\"center\">\n  <a href=\"https://gitbook.com/docs/\">Docs</a> - <a href=\"https://github.com/GitbookIO/community\">Community</a> - <a href=\"https://developer.gitbook.com/\">Developer Docs</a> - <a href=\"https://changelog.gitbook.com/\">Changelog</a> - <a href=\"https://github.com/GitbookIO/gitbook/issues/new?assignees=&labels=bug&template=bug_report.md\">Bug reports</a> - <a href=\"https://github.com/orgs/GitbookIO/discussions/categories/feature-requests\">Feature requests</a> \n</p>\n\n<p align=\"center\">\n  <a href=\"https://gitbook.com\"><img src=\"https://img.shields.io/static/v1?message=Documented%20on%20GitBook&logo=gitbook&logoColor=ffffff&label=%20&labelColor=5c5c5c&color=3F89A1\"></a>\n  <a href=\"#\"><img src=\"https://img.shields.io/badge/Open_Source-â¤ï¸-FDA599?\"/></a>\n  <a href=\"/LICENSE\"><img src=\"https://img.shields.io/badge/License-GNU_GPLv3-F4E28D\"/></a>\n  <a href=\"/.github/CONTRIBUTING.md\"><img src=\"https://img.shields.io/github/contributors/gitbookIO/gitbook\"/></a>\n  <a href=\"https://github.com/gitbookIO/gitbook/issues\"><img src=\"https://img.shields.io/github/issues/gitbookIO/gitbook\"/></a>\n</p>\n\n<p align=\"center\">Welcome to GitBook, the platform for managing technical knowledge for teams.</p>\n\n<p align=\"center\">This repository contains the open source code used to render GitBook's published content.</p>\n\n<p align=\"center\">\n  <img alt=\"GitBook Open Published Site\" src=\"./assets/published-site.png\">\n</p>\n\n## Table of Contents\n\n-   [Getting Started](#getting-started)\n-   [Contributing](#contributing)\n    -   [Types of contributions](#types-of-contributions)\n-   [Licensing](#license)\n-   [Acknowledgements](#acknowledgements)\n-   [Legacy GitBook](#legacy-gitbook-deprecated)\n\n## Getting Started\n\nTo run a local version of this project, please follow these simple steps.\n\n### Prerequisites\n\n- Node.js (Version: >= 22.3)\n    - Use nvm for easy Node management\n- [Bun](https://bun.sh/) (Version: >=1.2.15)\n  - We use a text-based lockfile which isn't supported below 1.2.15\n\n### Set up\n\n1. Clone the repo into a **public** GitHub repository. If you plan to distribute the code, keep the source code public to comply with GNU GPLv3. To clone in a private repository, acquire a [commercial license](https://www.gitbook.com/pricing).\n\n```\ngit clone https://github.com/gitbookIO/gitbook.git\n```\n\n2. Ensure you are using the project's version of `node`. Running `nvm use` will change your local version to the correct one.\n\n3. Install the project's dependencies through Bun.\n\n```\nbun install\n```\n\n4. Start your local development server.\n\n```\nbun dev\n```\n\n6. Open a published GitBook space in your web browser, prefixing it with `http://localhost:3000/url`.\n\nexamples:\n\n-   http://localhost:3000/url/gitbook.com/docs\n-   http://localhost:3000/url/open-source.gitbook.io/midjourney\n\nAny published GitBook site can be accessed through your local development instance, and any updates you make to the codebase will be reflected in your browser.\n\n### CI and testing\n\nAll pull-requests will be tested against both visual and performances testing to prevent regressions.\n\n## Fonts and Icons\n\nGitBook Open uses fontawesome. During development, your local environment will use the free version. However, only the pro version will be accepted by CI. If you see the following error:\n\n```\nThe GitBook icon is missing. It indicates that the dependencies were installed without the correct font-awesome package. These changes have probably been persisted in the Bun lockfile. Read the README for more information.\n```\n\nIt means that you've changed the GBO dependencies and bundled in the free version. Only GitBook staff can help with this - if you're not on the GitBook team, please ping us in the PR and we'll help get things moving.\n\nIf you are GitBook staff, you'll need our NPM token in your local environment.\n\n```\n.env.local\n\nBUN_NPM_TOKEN=xxx\n```\n\nand then reinstall dependencies.\n\n## Contributing\n\nGitBook's rendering engine is fully open source and built on top of [Next.js](https://nextjs.org/). Head to our [contributing guide](https://github.com/GitbookIO/gitbook/blob/main/.github/CONTRIBUTING.md) to learn more about the workflow on adding your first Pull Request.\n\n### Types of contributions\n\nWe encourage you to contribute to GitBook to help us build the best tool for documenting technical knowledge. If you're looking for some quick ways to contribute, continue reading to learn more about popular contributions.\n\n#### Translations\n\nThe GitBook UI is rendered using a set of translation files found in [`packages/gitbook/src/intl/translations`](/packages/gitbook/src/intl/translations/). We welcome all additional translations for the UI.\n\n#### Bugs\n\nEncounter a bug or find an issue you'd like to fix? Helping us fix issues related to GitBook greatly improves the experience for everyone. Head to the issues section of this repository to learn more about the types of bugs you can already help out with.\n\n## Deployment\n\n> [!WARNING]  \n> While it is possible to self-host this project, we do not recommend this unless you are certain this option fits your need.\n>\n> _Looking to add a specific feature in GitBook? Head to our [contributing guide](https://github.com/GitbookIO/gitbook/blob/main/.github/CONTRIBUTING.md) to get started._\n>\n> Self-hosting this project puts the responsibility of maintaining and merging future updates on **you**. We cannot guarantee support, maintenance, or updates to forked and self-hosted instances of this project.\n>\n> We want to make it as easy as possible for our community to collaborate and push the future of GitBook, which is why we encourage you to contribute to our product directly instead of creating your own version.\n\nThis project allows you to self-host the rendering portion of your GitBook published content. Self-hosting has pros and cons.\n\nOn the pro side, you can customize the look and feel of your content, and better embed your documentation in your application.\n\nOn the con side, you become responsible for the reliability of your published site, and keeping the renderer up-to-date with the changes made to the GitBook platform.\n\n## License\n\nDistributed under the [GNU GPLv3 License](https://github.com/GitBookIO/gitbook/blob/main/LICENSE).\n\nIf you plan to distribute the code, you must make the source code public to comply with the GNU GPLv3. To clone in a private repository, acquire a [commercial license](https://www.gitbook.com/pricing).\n\nSee `LICENSE` for more information.\n\n## Badges\n\n<p align=\"left\">\n  <a href=\"https://gitbook.com\"><img src=\"https://img.shields.io/static/v1?message=Documented%20on%20GitBook&logo=gitbook&logoColor=ffffff&label=%20&labelColor=5c5c5c&color=3F89A1\"></a>\n  <a href=\"https://gitbook.com\"><img src=\"https://img.shields.io/static/v1?message=Documented%20on%20GitBook&logo=gitbook&logoColor=ffffff&label=%20&labelColor=5c5c5c&color=F4E28D\"></a>\n  <a href=\"https://gitbook.com\"><img src=\"https://img.shields.io/static/v1?message=Documented%20on%20GitBook&logo=gitbook&logoColor=ffffff&label=%20&labelColor=5c5c5c&color=FDA599\"></a>\n</p>\n\n```md\n[![GitBook](https://img.shields.io/static/v1?message=Documented%20on%20GitBook&logo=gitbook&logoColor=ffffff&label=%20&labelColor=5c5c5c&color=3F89A1)](https://www.gitbook.com/preview?utm_source=gitbook_readme_badge&utm_medium=organic&utm_campaign=preview_documentation&utm_content=link)\n```\n\n```html\n<a href=\"https://www.gitbook.com/preview?utm_source=gitbook_readme_badge&utm_medium=organic&utm_campaign=preview_documentation&utm_content=link\">\n    <img\n        src=\"https://img.shields.io/static/v1?message=Documented%20on%20GitBook&logo=gitbook&logoColor=ffffff&label=%20&labelColor=5c5c5c&color=3F89A1\"\n    />\n</a>\n```\n\n## Acknowledgements\n\nGitBook wouldn't be possible without these projects:\n\n-   [Next.js](https://nextjs.org/)\n-   [Bun](https://bun.sh/)\n-   [Tailwind CSS](https://tailwindcss.com/)\n-   [Framer Motion](https://www.npmjs.com/package/framer-motion)\n\n## Contributors\n\n<a href=\"https://github.com/gitbookIO/gitbook/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=gitbookIO/gitbook\" />\n</a>\n\n## Legacy GitBook (Deprecated)\n\nOur previous version of GitBook and it's CLI tool are now deprecated. You can still view the old repository and it's commits on this [branch](https://github.com/GitbookIO/gitbook/tree/legacy).\n",
      "stars_today": 8
    },
    {
      "id": 31504491,
      "name": "echo",
      "full_name": "labstack/echo",
      "description": "High performance, minimalist Go web framework",
      "html_url": "https://github.com/labstack/echo",
      "stars": 32037,
      "forks": 2309,
      "language": "Go",
      "topics": [
        "echo",
        "go",
        "http2",
        "https",
        "labstack-echo",
        "letsencrypt",
        "micro-framework",
        "microservice",
        "middleware",
        "ssl",
        "web",
        "web-framework",
        "websocket"
      ],
      "created_at": "2015-03-01T17:43:01Z",
      "updated_at": "2026-01-14T22:16:12Z",
      "pushed_at": "2026-01-13T22:03:54Z",
      "open_issues": 126,
      "owner": {
        "login": "labstack",
        "avatar_url": "https://avatars.githubusercontent.com/u/2624634?v=4"
      },
      "readme": "[![Sourcegraph](https://sourcegraph.com/github.com/labstack/echo/-/badge.svg?style=flat-square)](https://sourcegraph.com/github.com/labstack/echo?badge)\n[![GoDoc](http://img.shields.io/badge/go-documentation-blue.svg?style=flat-square)](https://pkg.go.dev/github.com/labstack/echo/v4)\n[![Go Report Card](https://goreportcard.com/badge/github.com/labstack/echo?style=flat-square)](https://goreportcard.com/report/github.com/labstack/echo)\n[![GitHub Workflow Status (with event)](https://img.shields.io/github/actions/workflow/status/labstack/echo/echo.yml?style=flat-square)](https://github.com/labstack/echo/actions)\n[![Codecov](https://img.shields.io/codecov/c/github/labstack/echo.svg?style=flat-square)](https://codecov.io/gh/labstack/echo)\n[![Forum](https://img.shields.io/badge/community-forum-00afd1.svg?style=flat-square)](https://github.com/labstack/echo/discussions)\n[![Twitter](https://img.shields.io/badge/twitter-@labstack-55acee.svg?style=flat-square)](https://twitter.com/labstack)\n[![License](http://img.shields.io/badge/license-mit-blue.svg?style=flat-square)](https://raw.githubusercontent.com/labstack/echo/master/LICENSE)\n\n## Echo\n\nHigh performance, extensible, minimalist Go web framework.\n\n* [Official website](https://echo.labstack.com)\n* [Quick start](https://echo.labstack.com/docs/quick-start)\n* [Middlewares](https://echo.labstack.com/docs/category/middleware)\n\nHelp and questions: [Github Discussions](https://github.com/labstack/echo/discussions)\n\n\n### Feature Overview\n\n- Optimized HTTP router which smartly prioritize routes\n- Build robust and scalable RESTful APIs\n- Group APIs\n- Extensible middleware framework\n- Define middleware at root, group or route level\n- Data binding for JSON, XML and form payload\n- Handy functions to send variety of HTTP responses\n- Centralized HTTP error handling\n- Template rendering with any template engine\n- Define your format for the logger\n- Highly customizable\n- Automatic TLS via Letâ€™s Encrypt\n- HTTP/2 support\n\n## Sponsors\n\n<div>\n  <a href=\"https://encore.dev\" style=\"display: inline-flex; align-items: center; gap: 10px\">\n    <img src=\"https://user-images.githubusercontent.com/78424526/214602214-52e0483a-b5fc-4d4c-b03e-0b7b23e012df.svg\" height=\"28px\" alt=\"encore icon\"></img>\n  <b>Encore â€“ the platform for building Go-based cloud backends</b>\n    </a>\n</div>\n<br/>\n\nClick [here](https://github.com/sponsors/labstack) for more information on sponsorship.\n\n## [Guide](https://echo.labstack.com/guide)\n\n### Installation\n\n```sh\n// go get github.com/labstack/echo/{version}\ngo get github.com/labstack/echo/v4\n```\nLatest version of Echo supports last four Go major [releases](https://go.dev/doc/devel/release) and might work with older versions.\n\n### Example\n\n```go\npackage main\n\nimport (\n  \"github.com/labstack/echo/v4\"\n  \"github.com/labstack/echo/v4/middleware\"\n  \"log/slog\"\n  \"net/http\"\n)\n\nfunc main() {\n  // Echo instance\n  e := echo.New()\n\n  // Middleware\n  e.Use(middleware.RequestLogger()) // use the default RequestLogger middleware with slog logger\n  e.Use(middleware.Recover()) // recover panics as errors for proper error handling\n\n  // Routes\n  e.GET(\"/\", hello)\n\n  // Start server\n  if err := e.Start(\":8080\"); err != nil && !errors.Is(err, http.ErrServerClosed) {\n    slog.Error(\"failed to start server\", \"error\", err)\n  }\n}\n\n// Handler\nfunc hello(c echo.Context) error {\n  return c.String(http.StatusOK, \"Hello, World!\")\n}\n```\n\n# Official middleware repositories\n\nFollowing list of middleware is maintained by Echo team.\n\n| Repository                                                                   | Description                                                                                                                                                                                                                                                                                                                   |\n|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [github.com/labstack/echo-jwt](https://github.com/labstack/echo-jwt)         | [JWT](https://github.com/golang-jwt/jwt) middleware                                                                                                                                                                                                                                                                           | \n| [github.com/labstack/echo-contrib](https://github.com/labstack/echo-contrib) | [casbin](https://github.com/casbin/casbin), [gorilla/sessions](https://github.com/gorilla/sessions), [jaegertracing](https://github.com/uber/jaeger-client-go), [prometheus](https://github.com/prometheus/client_golang/), [pprof](https://pkg.go.dev/net/http/pprof), [zipkin](https://github.com/openzipkin/zipkin-go) middlewares | \n\n# Third-party middleware repositories\n\nBe careful when adding 3rd party middleware. Echo teams does not have time or manpower to guarantee safety and quality\nof middlewares in this list.\n\n| Repository                                                                                           | Description                                                                                                                                                                                              |\n|------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [deepmap/oapi-codegen](https://github.com/deepmap/oapi-codegen)                                      | Automatically generate RESTful API documentation with [OpenAPI](https://swagger.io/specification/) Client and Server Code Generator                                                                      |\n| [github.com/swaggo/echo-swagger](https://github.com/swaggo/echo-swagger)                             | Automatically generate RESTful API documentation with [Swagger](https://swagger.io/) 2.0.                                                                                                                |\n| [github.com/ziflex/lecho](https://github.com/ziflex/lecho)                                           | [Zerolog](https://github.com/rs/zerolog) logging library wrapper for Echo logger interface.                                                                                                              |\n| [github.com/brpaz/echozap](https://github.com/brpaz/echozap)                                         | UberÂ´s [Zap](https://github.com/uber-go/zap) logging library wrapper for Echo logger interface.                                                                                                          |\n| [github.com/samber/slog-echo](https://github.com/samber/slog-echo)                                         | Go [slog](https://pkg.go.dev/golang.org/x/exp/slog) logging library wrapper for Echo logger interface.                                                                                                          |\n| [github.com/darkweak/souin/plugins/echo](https://github.com/darkweak/souin/tree/master/plugins/echo) | HTTP cache system based on [Souin](https://github.com/darkweak/souin) to automatically get your endpoints cached. It supports some distributed and non-distributed storage systems depending your needs. |\n| [github.com/mikestefanello/pagoda](https://github.com/mikestefanello/pagoda)                         | Rapid, easy full-stack web development starter kit built with Echo.                                                                                                                                      |\n| [github.com/go-woo/protoc-gen-echo](https://github.com/go-woo/protoc-gen-echo)                       | ProtoBuf generate Echo server side code                                                                                                                                                                  |\n\nPlease send a PR to add your own library here.\n\n## Contribute\n\n**Use issues for everything**\n\n- For a small change, just send a PR.\n- For bigger changes open an issue for discussion before sending a PR.\n- PR should have:\n  - Test case\n  - Documentation\n  - Example (If it makes sense)\n- You can also contribute by:\n  - Reporting issues\n  - Suggesting new features or enhancements\n  - Improve/fix documentation\n\n## Credits\n\n- [Vishal Rana](https://github.com/vishr) (Author)\n- [Nitin Rana](https://github.com/nr17) (Consultant)\n- [Roland Lammel](https://github.com/lammel) (Maintainer)\n- [Martti T.](https://github.com/aldas) (Maintainer)\n- [Pablo Andres Fuente](https://github.com/pafuent) (Maintainer)\n- [Contributors](https://github.com/labstack/echo/graphs/contributors)\n\n## License\n\n[MIT](https://github.com/labstack/echo/blob/master/LICENSE)\n",
      "stars_today": 8
    },
    {
      "id": 166515022,
      "name": "trino",
      "full_name": "trinodb/trino",
      "description": "Official repository of Trino, the distributed SQL query engine for big data, formerly known as PrestoSQL (https://trino.io)",
      "html_url": "https://github.com/trinodb/trino",
      "stars": 12418,
      "forks": 3448,
      "language": "Java",
      "topics": [
        "analytics",
        "big-data",
        "data-science",
        "database",
        "databases",
        "datalake",
        "delta-lake",
        "distributed-database",
        "distributed-systems",
        "hadoop",
        "hive",
        "iceberg",
        "java",
        "jdbc",
        "presto",
        "prestodb",
        "query-engine",
        "sql",
        "trino"
      ],
      "created_at": "2019-01-19T06:38:14Z",
      "updated_at": "2026-01-15T00:03:17Z",
      "pushed_at": "2026-01-14T23:54:44Z",
      "open_issues": 2504,
      "owner": {
        "login": "trinodb",
        "avatar_url": "https://avatars.githubusercontent.com/u/34147222?v=4"
      },
      "readme": "<p align=\"center\">\n    <a href=\"https://trino.io/\"><img alt=\"Trino Logo\" src=\".github/homepage.png\" /></a>\n</p>\n<p align=\"center\">\n    <b>Trino is a fast distributed SQL query engine for big data analytics.</b>\n</p>\n<p align=\"center\">\n    See the <a href=\"https://trino.io/docs/current/\">User Manual</a> for deployment instructions and end user documentation.\n</p>\n<p align=\"center\">\n  <a href=\"https://trino.io/download.html\" style=\"text-decoration: none\"><img\n    src=\"https://img.shields.io/github/v/release/trinodb/trino\"\n    alt=\"Trino download\"\n  /></a>\n  <a href=\"https://github.com/jvm-repo-rebuild/reproducible-central/blob/master/content/io/trino/README.md\" style=\"text-decoration: none\"><img\n    src=\"https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/jvm-repo-rebuild/reproducible-central/master/content/io/trino/badge.json\"\n    alt=\"Reproducible builds supported\"\n  /></a>\n  <a href=\"https://trino.io/slack.html\" style=\"text-decoration: none\"><img\n    src=\"https://img.shields.io/static/v1?logo=slack&logoColor=959DA5&label=Slack&labelColor=333a41&message=join%20conversation&color=3AC358\"\n    alt=\"Trino Slack\"\n  /></a>\n  <a href=\"https://trino.io/trino-the-definitive-guide.html\" style=\"text-decoration: none\"><img\n    src=\"https://img.shields.io/badge/Trino%3A%20The%20Definitive%20Guide-download-brightgreen\"\n    alt=\"Trino: The Definitive Guide book download\"\n  /></a>\n</p>\n\n## Development\n\nSee [DEVELOPMENT](.github/DEVELOPMENT.md) for information about development and release process,\ncode style and guidelines for implementors of Trino plugins.\n\nSee [CONTRIBUTING](.github/CONTRIBUTING.md) for contribution requirements.\n\n## Security\n\nSee the project [security policy](.github/SECURITY.md) for\ninformation about reporting vulnerabilities.\n\nTrino supports [reproducible builds](https://reproducible-builds.org) as of version 449.\n\n## Build requirements\n\n* Mac OS X or Linux\n  * Note that some npm packages used to build the web UI are only available\n    for x86 architectures, so if you're building on Apple Silicon, you need \n    to have Rosetta 2 installed\n* Java 25.0.1+, 64-bit\n* Docker\n  * Turn SELinux or other systems disabling write access to the local checkout\n    off, to allow containers to mount parts of the Trino source tree\n\n## Building Trino\n\nTrino is a standard Maven project. Simply run the following command from the\nproject root directory:\n\n    ./mvnw clean install -DskipTests\n\nOn the first build, Maven downloads all the dependencies from the internet\nand caches them in the local repository (`~/.m2/repository`), which can take a\nwhile, depending on your connection speed. Subsequent builds are faster.\n\nTrino has a comprehensive set of tests that take a considerable amount of time\nto run, and are thus disabled by the above command. These tests are run by the\nCI system when you submit a pull request. We recommend only running tests\nlocally for the areas of code that you change.\n\n## Running Trino in your IDE\n\n### Overview\n\nAfter building Trino for the first time, you can load the project into your IDE\nand run the server.  We recommend using\n[IntelliJ IDEA](http://www.jetbrains.com/idea/). Because Trino is a standard\nMaven project, you easily can import it into your IDE.  In IntelliJ, choose\n*Open Project* from the *Quick Start* box or choose *Open*\nfrom the *File* menu and select the root `pom.xml` file.\n\nAfter opening the project in IntelliJ, double check that the Java SDK is\nproperly configured for the project:\n\n* Open the File menu and select Project Structure\n* In the SDKs section, ensure that JDK 25 is selected (create one if none exist)\n* In the Project section, ensure the Project language level is set to 25\n\n### Running a testing server\n\nThe simplest way to run Trino for development is to run the `TpchQueryRunner`\nclass. It will start a development version of the server that is configured with\nthe TPCH connector. You can then use the CLI to execute queries against this\nserver. Many other connectors have their own `*QueryRunner` class that you can\nuse when working on a specific connector.\n\n### Running the full server\n\nTrino comes with sample configuration that should work out-of-the-box for\ndevelopment. Use the following options to create a run configuration:\n\n* Main Class: `io.trino.server.DevelopmentServer`\n* VM Options: `-ea -Dconfig=etc/config.properties -Dlog.levels-file=etc/log.properties -Djdk.attach.allowAttachSelf=true --sun-misc-unsafe-memory-access=allow --add-modules jdk.incubator.vector`\n* Working directory: `$MODULE_DIR$`\n* Use classpath of module: `trino-server-dev`\n\nThe working directory should be the `trino-server-dev` subdirectory. In\nIntelliJ, using `$MODULE_DIR$` accomplishes this automatically.\n\nIf `VM options` doesn't exist in the dialog, you need to select `Modify options`\nand enable `Add VM options`.\n\nTo adjust which plugins are enabled for the development server, adjust the value of\n`plugin.bundles` in `config.properties`. Each entry in this list must represent a plugin\nspecified by one of the following options:\n* A path to a `pom.xml` or `*.pom` file describing a Maven project that produces a plugin.\n* Maven coordinates, in the form `<groupId>:<artifactId>[:<extension>[:<classifier>]]:<version>`. The plugin will be loaded via Maven and therefore must be available in your local repository or a remote repository.\n* A path to a plugin directory containing JAR files. See [Deploying a custom plugin](https://trino.io/docs/current/develop/spi-overview.html#deploying-a-custom-plugin) for more details.\n\nIf you want to use a plugin in a catalog, you must add a corresponding\n`<catalog_name>.properties` file to `testing/trino-server-dev/etc/catalog`.\n\n### Running the CLI\n\nStart the CLI to connect to the server and run SQL queries:\n\n    client/trino-cli/target/trino-cli-*-executable.jar\n\nRun a query to see the nodes in the cluster:\n\n    SELECT * FROM system.runtime.nodes;\n\nRun a query against the TPCH connector:\n\n    SELECT * FROM tpch.tiny.region;\n",
      "stars_today": 8
    },
    {
      "id": 181436799,
      "name": "MNN",
      "full_name": "alibaba/MNN",
      "description": "MNN is a blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba. Full multimodal LLM Android App:[MNN-LLM-Android](./apps/Android/MnnLlmChat/README.md). MNN TaoAvatar Android - Local 3D Avatar Intelligence: apps/Android/Mnn3dAvatar/README.md",
      "html_url": "https://github.com/alibaba/MNN",
      "stars": 13908,
      "forks": 2168,
      "language": "C++",
      "topics": [
        "arm",
        "convolution",
        "deep-learning",
        "embedded-devices",
        "llm",
        "machine-learning",
        "ml",
        "mnn",
        "transformer",
        "vulkan",
        "winograd-algorithm"
      ],
      "created_at": "2019-04-15T07:40:18Z",
      "updated_at": "2026-01-14T15:17:49Z",
      "pushed_at": "2026-01-14T02:46:47Z",
      "open_issues": 70,
      "owner": {
        "login": "alibaba",
        "avatar_url": "https://avatars.githubusercontent.com/u/1961952?v=4"
      },
      "readme": "![MNN](doc/banner.png)\n---\n[![License](https://img.shields.io/github/license/alibaba/MNN)](LICENSE.txt)\n[![Documentation](https://img.shields.io/badge/Documentation-Read-green)](https://mnn-docs.readthedocs.io/en/latest/)\n[![ä¸­æ–‡ç‰ˆæœ¬](https://img.shields.io/badge/Language-%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-green)](README_CN.md)\n[![æ—¥æœ¬èªãƒãƒ¼ã‚¸ãƒ§ãƒ³](https://img.shields.io/badge/Language-%E6%97%A5%E6%9C%AC%E8%AA%9E-green)](README_JP.md)\n[![MNN Homepage](https://img.shields.io/badge/Homepage-Visit-green)](http://www.mnn.zone)\n\n[![MNN Chat App](https://img.shields.io/badge/Apps-MNN_Chat-blue)](./apps/Android/MnnLlmChat/README.md)\n[![TaoAvatar](https://img.shields.io/badge/Apps-MNN_TaoAvatar-blue)](./apps/Android/Mnn3dAvatar/README.md)\n\n\n## News ğŸ”¥\n- [2025/10/16] Support Qwen3-VL Series.\n- [2025/06/11] New App MNN TaoAvatar released, you can talk with 3DAvatar offline with LLM, ASR, TTS, A2BS and NNR models all run local on your device!! [MNN TaoAvatar](./apps/Android/Mnn3dAvatar/README.md)\n<p align=\"center\">\n  <img width=\"20%\" alt=\"Icon\"  src=\"https://meta.alicdn.com/data/mnn/avatar/avatar_demo.gif\" style=\"margin: 0 10px;\">\n</p>\n\n- [2025/05/12] android app support qwen2.5 omni 3b and 7b [MNN Chat App](./apps/Android/MnnLlmChat/README.md#releases).\n<p align=\"center\">\n  <img width=\"20%\" alt=\"Icon\"  src=\"./apps/Android/MnnLlmChat/assets/image_home_new.jpg\" style=\"margin: 0 10px;\">\n  <img width=\"20%\" alt=\"Icon\" src=\"./apps/Android/MnnLlmChat/assets/image_sound_new.jpg\" style=\"margin: 0 10px;\">\n  <img width=\"20%\" alt=\"Icon\" src=\"./apps/Android/MnnLlmChat/assets/image_image_new.jpg\" style=\"margin: 0 10px;\">\n</p>\n\n\n<details>\n<summary> History News </summary>\n\n- [2025/04/30] android app support qwen3 and dark mode [MNN Chat App](./apps/Android/MnnLlmChat/README.md#releases).\n<p align=\"center\">\n  <img width=\"20%\" alt=\"Icon\"  src=\"https://meta.alicdn.com/data/mnn/qwen_3.gif\" style=\"margin: 0 10px;\">\n</p>\n\n- [2025/02/18] iOS multimodal LLM App is released [MNN LLM iOS](./apps/iOS/MNNLLMChat/README.md).\n<p align=\"center\">\n  <img width=\"20%\" alt=\"Icon\"  src=\"./apps/iOS/MNNLLMChat/assets/introduction.gif\" style=\"margin: 0 10px;\">\n</p>\n\n- [2025/02/11] android app support for [deepseek r1 1.5b](./project/android/apps/MnnLlmApp/README.md#version-021).\n<p align=\"center\">\n  <img width=\"20%\" alt=\"Icon\"  src=\"./apps/Android/MnnLlmChat/assets/deepseek_support.gif\" style=\"margin: 0 10px;\">\n</p>\n\n- [2025/01/23] We released our full multimodal LLM Android App:[MNN-LLM-Android](./apps/Android/MnnLlmChat/README.md). including text-to-text, image-to-text, audio-to-text, and text-to-image generation.\n<p align=\"center\">\n  <img width=\"20%\" alt=\"Icon\"  src=\"./apps/Android/MnnLlmChat/assets/image_home_new.jpg\" style=\"margin: 0 10px;\">\n  <img width=\"20%\" alt=\"Icon\" src=\"./apps/Android/MnnLlmChat/assets/image_diffusion_new.jpg\" style=\"margin: 0 10px;\">\n  <img width=\"20%\" alt=\"Icon\" src=\"./apps/Android/MnnLlmChat/assets/image_sound_new.jpg\" style=\"margin: 0 10px;\">\n  <img width=\"20%\" alt=\"Icon\" src=\"./apps/Android/MnnLlmChat/assets/image_image_new.jpg\" style=\"margin: 0 10px;\">\n</p>\n</details>\n\n## Intro\nMNN is a highly efficient and lightweight deep learning framework. It supports inference and training of deep learning models and has industry-leading performance for inference and training on-device. At present, MNN has been integrated into more than 30 apps of Alibaba Inc, such as Taobao, Tmall, Youku, DingTalk, Xianyu, etc., covering more than 70 usage scenarios such as live broadcast, short video capture, search recommendation, product searching by image, interactive marketing, equity distribution, security risk control. In addition, MNN is also used on embedded devices, such as IoT.\n\n[MNN-LLM](./transformers/README.md) is a large language model runtime solution developed based on the MNN engine. The mission of this project is to deploy LLM models locally on everyone's platforms(Mobile Phone/PC/IOT). It supports popular large language models such as Qianwen, Baichuan, Zhipu, LLAMA, and others. [MNN-LLM User guide](https://mnn-docs.readthedocs.io/en/latest/transformers/llm.html)\n\n[MNN-Diffusion](https://github.com/alibaba/MNN/tree/master/transformers/diffusion) is a stable diffusion model runtime solution developed based on the MNN engine. The mission of this project is to deploy stable diffusion models locally on everyone's platforms. [MNN-Diffusion User guide](https://mnn-docs.readthedocs.io/en/latest/transformers/diffusion.html)\n\n![architecture](doc/architecture.png)\n\nInside Alibaba, [MNN](https://mp.weixin.qq.com/s/5I1ISpx8lQqvCS8tGd6EJw) works as the basic module of the compute container in the [Walle](https://mp.weixin.qq.com/s/qpeCETty0BqqNJV9CMJafA) System, the first end-to-end, general-purpose, and large-scale production system for device-cloud collaborative machine learning, which has been published in the top system conference OSDIâ€™22. The key design principles of MNN and the extensive benchmark testing results (vs. TensorFlow, TensorFlow Lite, PyTorch, PyTorch Mobile, TVM) can be found in the OSDI paper. The scripts and instructions for benchmark testing are put in the path â€œ/benchmarkâ€. If MNN or the design of Walle helps your research or production use, please cite our OSDI paper as follows:\n\n    @inproceedings {proc:osdi22:walle,\n        author = {Chengfei Lv and Chaoyue Niu and Renjie Gu and Xiaotang Jiang and Zhaode Wang and Bin Liu and Ziqi Wu and Qiulin Yao and Congyu Huang and Panos Huang and Tao Huang and Hui Shu and Jinde Song and Bin Zou and Peng Lan and Guohuan Xu and Fei Wu and Shaojie Tang and Fan Wu and Guihai Chen},\n        title = {Walle: An {End-to-End}, {General-Purpose}, and {Large-Scale} Production System for {Device-Cloud} Collaborative Machine Learning},\n        booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},\n        year = {2022},\n        isbn = {978-1-939133-28-1},\n        address = {Carlsbad, CA},\n        pages = {249--265},\n        url = {https://www.usenix.org/conference/osdi22/presentation/lv},\n        publisher = {USENIX Association},\n        month = jul,\n    }\n\n\n## Documentation and Workbench\nMNN's docs are in place in [Read the docs](https://mnn-docs.readthedocs.io/en/latest).\n\nYou can also read docs/README to build docs's html.\n\nMNN Workbench could be downloaded from [MNN's homepage](http://www.mnn.zone), which provides pretrained models, visualized training tools, and one-click deployment of models to devices.\n\n## Key Features\n### Lightweight\n- Optimized for devices, no dependencies, can be easily deployed to mobile devices and a variety of embedded devices.\n- iOS platform: static library size will full option for armv7+arm64 platforms is about 12MB, size increase of linked executables is about 2M.\n- Android platform: core so size is about 800KB (armv7a - c++_shared).\n- Using MNN_BUILD_MINI can reduce package size by about 25%, with a limit of fixed model input size\n- Support FP16 / Int8 quantize, can reduce model size 50%-70%\n\n### Versatility\n- Supports `Tensorflow`, `Caffe`, `ONNX`,`Torchscripts` and supports common neural networks such as `CNN`, `RNN`, `GAN`, `Transformer`.\n- Supports AI model with multi-inputs or multi-outputs, every kind of dimension format, dynamic inputs, controlflow.\n- MNN supports approximate full OPs used for the AI Model. The converter supports 178 `Tensorflow` OPs, 52 `Caffe` OPs, 163 `Torchscripts` OPs, 158 `ONNX` OPs.\n- Supports iOS 8.0+, Android 4.3+, and embedded devices with POSIX interface.\n- Supports hybrid computing on multiple devices. Currently supports CPU and GPU.\n\n\n### High performance\n- Implements core computing with lots of optimized assembly code to make full use of the ARM / x64 CPU.\n- Use Metal / OpenCL / Vulkan to support GPU inference on mobile.\n- Use CUDA and tensorcore to support NVIDIA GPU for better performance\n- Convolution and transposition convolution algorithms are efficient and stable. The Winograd convolution algorithm is widely used to better symmetric convolutions such as 3x3,4x4,5x5,6x6,7x7.\n- Twice speed increase for the new architecture ARM v8.2 with FP16 half-precision calculation support. 2.5 faster to use sdot for ARM v8.2 and VNNI.\n\n### Ease of use\n- Support use MNN's OP to do numerical calculating like numpy.\n- Support lightweight image process module like OpenCV, which is only 100k.\n- Support build model and train it on PC / mobile.\n- MNN Python API helps ML engineers to easily use MNN to infer, train, and process images, without dipping their toes in C++ code.\n\nThe Architecture / Precision MNN supported is shown below:\n\n- S ï¼šSupport and work well, deeply optimized, recommend to use\n- A ï¼šSupport and work well, can use\n- B ï¼šSupport but has bug or not optimized, no recommend to use\n- C ï¼šNot Support\n\n| Architecture / Precision |  | Normal | FP16 | BF16 | Int8 |\n| --- | --- | --- | --- | --- | --- |\n| CPU | Native | B | C | B | B |\n|  | x86/x64-SSE4.1 | A | C | C | A |\n|  | x86/x64-AVX2 | S | C | C | A |\n|  | x86/x64-AVX512 | S | C | C | S |\n|  | ARMv7a | S | S (ARMv8.2) | S | S |\n|  | ARMv8 | S | S (ARMv8.2) | S(ARMv8.6) | S |\n| GPU | OpenCL | A | S | C | S |\n|  | Vulkan | A | A | C | A |\n|  | Metal | A | S | C | S |\n|  | CUDA | A | S | C | A |\n| NPU | CoreML | A | C | C | C |\n|  | HIAI | A | C | C | C |\n|  | NNAPI | B | B | C | B |\n|  | QNN | C | B | C | C |\n\n\n## Tools\n\nBase on MNN (Tensor compute engine), we provided a series of tools for inference, train and general computation.\n\n- MNN-Converter: Convert other models to MNN models for inference, such as Tensorflow(lite), Caffe, ONNX, Torchscripts. And do graph optimization to reduce computation.\n- MNN-Compress: Compress model to reduce size and increase performance / speed\n- MNN-Express: Support model with controlflow, use MNN's OP to do general-purpose computing.\n- MNN-CV: An OpenCV-like library, but based on MNN and then much more lightweight.\n- MNN-Train: Support train MNN model.\n\n## How to Discuss and Get Help From the MNN Community\n\nThe group discussions are predominantly Chinese. But we welcome and will help English speakers.\n\nDingtalk discussion groups:\n\nGroup #4 (Available): 160170007549\n\nGroup #3 (Full)\n\nGroup #2 (Full): 23350225\n\nGroup #1 (Full): 23329087\n\n## Historical Paper\n\nThe preliminary version of MNN, as mobile inference engine and with the focus on manual optimization, has also been published in MLSys 2020. Please cite the paper, if MNN previously helped your research:\n\n\n    @inproceedings{alibaba2020mnn,\n      author = {Jiang, Xiaotang and Wang, Huan and Chen, Yiliu and Wu, Ziqi and Wang, Lichuan and Zou, Bin and Yang, Yafeng and Cui, Zongyang and Cai, Yu and Yu, Tianhang and Lv, Chengfei and Wu, Zhihua},\n      title = {MNN: A Universal and Efficient Inference Engine},\n      booktitle = {MLSys},\n      year = {2020}\n    }\n\n\n## License\nApache 2.0\n\n## Acknowledgement\nMNN participants: Taobao Technology Department, Search Engineering Team, DAMO Team, Youku and other Alibaba Group employees.\n\nMNN refers to the following projects:\n- [Caffe](https://github.com/BVLC/caffe)\n- [flatbuffer](https://github.com/google/flatbuffers)\n- [gemmlowp](https://github.com/google/gemmlowp)\n- [Google Vulkan demo](http://www.github.com/googlesamples/android-vulkan-tutorials)\n- [Halide](https://github.com/halide/Halide)\n- [Mace](https://github.com/XiaoMi/mace)\n- [ONNX](https://github.com/onnx/onnx)\n- [protobuffer](https://github.com/protocolbuffers/protobuf)\n- [skia](https://github.com/google/skia)\n- [Tensorflow](https://github.com/tensorflow/tensorflow)\n- [ncnn](https://github.com/Tencent/ncnn)\n- [paddle-mobile](https://github.com/PaddlePaddle/paddle-mobile)\n- [stb](https://github.com/nothings/stb)\n- [rapidjson](https://github.com/Tencent/rapidjson)\n- [pybind11](https://github.com/pybind/pybind11)\n- [pytorch](https://github.com/pytorch/pytorch)\n- [bolt](https://github.com/huawei-noah/bolt)\n- [libyuv](https://chromium.googlesource.com/libyuv/libyuv)\n- [libjpeg](https://github.com/libjpeg-turbo/libjpeg-turbo)\n- [opencv](https://github.com/opencv/opencv)\n- [onnxruntime](https://github.com/microsoft/onnxruntime)\n",
      "stars_today": 8
    },
    {
      "id": 515368123,
      "name": "burn",
      "full_name": "tracel-ai/burn",
      "description": "Burn is a next generation tensor library and Deep Learning Framework that doesn't compromise on flexibility, efficiency and portability.",
      "html_url": "https://github.com/tracel-ai/burn",
      "stars": 13905,
      "forks": 775,
      "language": "Rust",
      "topics": [
        "autodiff",
        "cross-platform",
        "cuda",
        "deep-learning",
        "kernel-fusion",
        "machine-learning",
        "metal",
        "ndarray",
        "neural-network",
        "onnx",
        "pytorch",
        "rocm",
        "rust",
        "scientific-computing",
        "tensor",
        "vulkan",
        "wasm",
        "webgpu"
      ],
      "created_at": "2022-07-18T23:11:45Z",
      "updated_at": "2026-01-15T00:55:31Z",
      "pushed_at": "2026-01-14T22:20:03Z",
      "open_issues": 294,
      "owner": {
        "login": "tracel-ai",
        "avatar_url": "https://avatars.githubusercontent.com/u/111992358?v=4"
      },
      "readme": "<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/tracel-ai/burn/main/assets/logo-burn-neutral.webp\" width=\"350px\"/>\n\n[![Discord](https://img.shields.io/discord/1038839012602941528.svg?color=7289da&&logo=discord)](https://discord.gg/uPEBbYYDB6)\n[![Current Crates.io Version](https://img.shields.io/crates/v/burn.svg)](https://crates.io/crates/burn)\n[![Minimum Supported Rust Version](https://img.shields.io/crates/msrv/burn)](https://crates.io/crates/burn)\n[![Documentation](https://img.shields.io/badge/docs-latest-blue)](https://burn.dev/docs/burn)\n[![Test Status](https://github.com/tracel-ai/burn/actions/workflows/test.yml/badge.svg)](https://github.com/tracel-ai/burn/actions/workflows/test.yml)\n[![license](https://shields.io/badge/license-MIT%2FApache--2.0-blue)](#license)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/tracel-ai/burn)\n\n[<img src=\"https://www.runblaze.dev/ci-blaze-powered.png\" width=\"125px\"/>](https://www.runblaze.dev)\n\n---\n\n**Burn is a next generation Tensor Library and Deep Learning Framework that doesn't compromise on\n<br /> flexibility, efficiency and portability.**\n\n<br/>\n</div>\n\n<div align=\"left\">\n\nBurn is both a tensor library and a deep learning framework optimized for numerical computing, model\ninference and model training. Burn leverages Rust to perform optimizations normally only available\nin static-graph frameworks, offering optimal speed without impacting flexibility.\n\n## Backend\n\n<div align=\"left\">\n<img align=\"right\" src=\"https://raw.githubusercontent.com/tracel-ai/burn/main/assets/backend-chip.png\" height=\"96px\"/>\n\nBurn strives to be as fast as possible on as many hardwares as possible, with robust\nimplementations. We believe this flexibility is crucial for modern needs where you may train your\nmodels in the cloud, then deploy on customer hardwares, which vary from user to user.\n\n</div>\n\n### Supported Backends\n\nMost backends support all operating systems, so we don't mention them in the tables below.\n\n**GPU Backends:**\n\n|         | CUDA | ROCm | Metal | Vulkan | WebGPU | Candle | LibTorch |\n| ------- | ---- | ---- | ----- | ------ | ------ | ------ | -------- |\n| Nvidia  | â˜‘ï¸   | -    | -     | â˜‘ï¸     | â˜‘ï¸     | â˜‘ï¸     | â˜‘ï¸       |\n| AMD     | -    | â˜‘ï¸   | -     | â˜‘ï¸     | â˜‘ï¸     | -      | â˜‘ï¸       |\n| Apple   | -    | -    | â˜‘ï¸    | -      | â˜‘ï¸     | -      | â˜‘ï¸       |\n| Intel   | -    | -    | -     | â˜‘ï¸     | â˜‘ï¸     | -      | -        |\n| Qualcom | -    | -    | -     | â˜‘ï¸     | â˜‘ï¸     | -      | -        |\n| Wasm    | -    | -    | -     | -      | â˜‘ï¸     | -      | -        |\n\n**CPU Backends:**\n\n|        | Cpu (CubeCL) | NdArray | Candle | LibTorch |\n| ------ | ------------ | ------- | ------ | -------- |\n| X86    | â˜‘ï¸           | â˜‘ï¸      | â˜‘ï¸     | â˜‘ï¸       |\n| Arm    | â˜‘ï¸           | â˜‘ï¸      | â˜‘ï¸     | â˜‘ï¸       |\n| Wasm   | -            | â˜‘ï¸      | â˜‘ï¸     | -        |\n| no-std | -            | â˜‘ï¸      | -      | -        |\n\n<br />\n\nCompared to other frameworks, Burn has a very different approach to supporting many backends. By\ndesign, most code is generic over the Backend trait, which allows us to build Burn with swappable\nbackends. This makes composing backend possible, augmenting them with additional functionalities\nsuch as autodifferentiation and automatic kernel fusion.\n\n<details>\n<summary>\nAutodiff: Backend decorator that brings backpropagation to any backend ğŸ”„\n</summary>\n<br />\n\nContrary to the aforementioned backends, Autodiff is actually a backend _decorator_. This means that\nit cannot exist by itself; it must encapsulate another backend.\n\nThe simple act of wrapping a base backend with Autodiff transparently equips it with\nautodifferentiation support, making it possible to call backward on your model.\n\n```rust\nuse burn::backend::{Autodiff, Wgpu};\nuse burn::tensor::{Distribution, Tensor};\n\nfn main() {\n    type Backend = Autodiff<Wgpu>;\n\n    let device = Default::default();\n\n    let x: Tensor<Backend, 2> = Tensor::random([32, 32], Distribution::Default, &device);\n    let y: Tensor<Backend, 2> = Tensor::random([32, 32], Distribution::Default, &device).require_grad();\n\n    let tmp = x.clone() + y.clone();\n    let tmp = tmp.matmul(x);\n    let tmp = tmp.exp();\n\n    let grads = tmp.backward();\n    let y_grad = y.grad(&grads).unwrap();\n    println!(\"{y_grad}\");\n}\n```\n\nOf note, it is impossible to make the mistake of calling backward on a model that runs on a backend\nthat does not support autodiff (for inference), as this method is only offered by an Autodiff\nbackend.\n\nSee the [Autodiff Backend README](./crates/burn-autodiff/README.md) for more details.\n\n</details>\n\n<details>\n<summary>\nFusion: Backend decorator that brings kernel fusion to all first-party backends\n</summary>\n<br />\n\nThis backend decorator enhances a backend with kernel fusion, provided that the inner backend\nsupports it. Note that you can compose this backend with other backend decorators such as Autodiff.\nAll first-party accelerated backends (like WGPU and CUDA) use Fusion by default (`burn/fusion`\nfeature flag), so you typically don't need to apply it manually.\n\n```rust\n#[cfg(not(feature = \"fusion\"))]\npub type Cuda<F = f32, I = i32> = CubeBackend<CudaRuntime, F, I, u8>;\n\n#[cfg(feature = \"fusion\")]\npub type Cuda<F = f32, I = i32> = burn_fusion::Fusion<CubeBackend<CudaRuntime, F, I, u8>>;\n```\n\nOf note, we plan to implement automatic gradient checkpointing based on compute bound and memory\nbound operations, which will work gracefully with the fusion backend to make your code run even\nfaster during training, see [this issue](https://github.com/tracel-ai/burn/issues/936).\n\nSee the [Fusion Backend README](./crates/burn-fusion/README.md) for more details.\n\n</details>\n\n<details>\n<summary>\nRouter (Beta): Backend decorator that composes multiple backends into a single one\n</summary>\n<br />\n\nThat backend simplifies hardware operability, if for instance you want to execute some operations on\nthe CPU and other operations on the GPU.\n\n```rust\nuse burn::tensor::{Distribution, Tensor};\nuse burn::backend::{\n    NdArray, Router, Wgpu, ndarray::NdArrayDevice, router::duo::MultiDevice, wgpu::WgpuDevice,\n};\n\nfn main() {\n    type Backend = Router<(Wgpu, NdArray)>;\n\n    let device_0 = MultiDevice::B1(WgpuDevice::DiscreteGpu(0));\n    let device_1 = MultiDevice::B2(NdArrayDevice::Cpu);\n\n    let tensor_gpu =\n        Tensor::<Backend, 2>::random([3, 3], burn::tensor::Distribution::Default, &device_0);\n    let tensor_cpu =\n        Tensor::<Backend, 2>::random([3, 3], burn::tensor::Distribution::Default, &device_1);\n}\n\n```\n\n</details>\n\n<details>\n<summary>\nRemote (Beta): Backend decorator for remote backend execution, useful for distributed computations\n</summary>\n<br />\n\nThat backend has two parts, one client and one server. The client sends tensor operations over the\nnetwork to a remote compute backend. You can use any first-party backend as server in a single line\nof code:\n\n```rust\nfn main_server() {\n    // Start a server on port 3000.\n    burn::server::start::<burn::backend::Cuda>(Default::default(), 3000);\n}\n\nfn main_client() {\n    // Create a client that communicate with the server on port 3000.\n    use burn::backend::{Autodiff, RemoteBackend};\n\n    type Backend = Autodiff<RemoteDevice>;\n\n    let device = RemoteDevice::new(\"ws://localhost:3000\");\n    let tensor_gpu =\n        Tensor::<Backend, 2>::random([3, 3], Distribution::Default, &device);\n}\n\n```\n\n</details>\n\n<br />\n\n## Training & Inference\n\n<div align=\"left\">\n<img align=\"right\" src=\"https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-wall.png\" height=\"96px\"/>\n\nThe whole deep learning workflow is made easy with Burn, as you can monitor your training progress\nwith an ergonomic dashboard, and run inference everywhere from embedded devices to large GPU\nclusters.\n\nBurn was built from the ground up with training and inference in mind. It's also worth noting how\nBurn, in comparison to frameworks like PyTorch, simplifies the transition from training to\ndeployment, eliminating the need for code changes.\n\n</div>\n\n<div align=\"center\">\n\n<br />\n\n<a href=\"https://www.youtube.com/watch?v=N9RM5CQbNQc\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/tracel-ai/burn/main/assets/burn-train-tui.png\" alt=\"Burn Train TUI\" width=\"75%\">\n  </a>\n</div>\n\n<br />\n\n**Click on the following sections to expand ğŸ‘‡**\n\n<details>\n<summary>\nTraining Dashboard ğŸ“ˆ\n</summary>\n<br />\n\nAs you can see in the previous video (click on the picture!), a new terminal UI dashboard based on\nthe [Ratatui](https://github.com/ratatui-org/ratatui) crate allows users to follow their training\nwith ease without having to connect to any external application.\n\nYou can visualize your training and validation metrics updating in real-time and analyze the\nlifelong progression or recent history of any registered metrics using only the arrow keys. Break\nfrom the training loop without crashing, allowing potential checkpoints to be fully written or\nimportant pieces of code to complete without interruption ğŸ›¡\n\n</details>\n\n<details>\n<summary>\nONNX Support ğŸ«\n</summary>\n<br />\n\nBurn supports importing ONNX (Open Neural Network Exchange) models, allowing you to easily port\nmodels from TensorFlow or PyTorch to Burn. The ONNX model is converted into Rust code that uses\nBurn's native APIs, enabling the imported model to run on any Burn backend (CPU, GPU, WebAssembly)\nand benefit from all of Burn's optimizations like automatic kernel fusion.\n\nOur ONNX support is further described in\n[this section of the Burn Book ğŸ”¥](https://burn.dev/books/burn/import/onnx-model.html).\n\n> **Note**: This crate is in active development and currently supports a\n> [limited set of ONNX operators](./crates/burn-import/SUPPORTED-ONNX-OPS.md).\n\n</details>\n\n<details>\n<summary>\nImporting PyTorch or Safetensors Models ğŸšš\n</summary>\n<br />\n\nYou can load weights from PyTorch or Safetensors formats directly into your Burn-defined models.\nThis makes it easy to reuse existing models while benefiting from Burn's performance and deployment\nfeatures.\n\nLearn more:\n\n- [Import pre-trained PyTorch models into Burn](https://burn.dev/books/burn/import/pytorch-model.html)\n- [Load models from Safetensors format](https://burn.dev/books/burn/import/safetensors-model.html)\n\n</details>\n\n<details>\n<summary>\nInference in the Browser ğŸŒ\n</summary>\n<br />\n\nSeveral of our backends can run in WebAssembly environments: Candle and NdArray for CPU execution,\nand WGPU for GPU acceleration via WebGPU. This means that you can run inference directly within a\nbrowser. We provide several examples of this:\n\n- [MNIST](./examples/mnist-inference-web) where you can draw digits and a small convnet tries to\n  find which one it is! 2ï¸âƒ£ 7ï¸âƒ£ ğŸ˜°\n- [Image Classification](./examples/image-classification-web) where you can upload images and\n  classify them! ğŸŒ„\n\n</details>\n\n<details>\n<summary>\nEmbedded: <i>no_std</i> support âš™ï¸\n</summary>\n<br />\n\nBurn's core components support [no_std](https://docs.rust-embedded.org/book/intro/no-std.html). This\nmeans it can run in bare metal environment such as embedded devices without an operating system.\n\n> As of now, only the NdArray backend can be used in a _no_std_ environment.\n\n</details>\n\n<br />\n\n### Benchmarks\n\nTo evaluate performance across different backends and track improvements over time, we provide a\ndedicated benchmarking suite.\n\nRun and compare benchmarks using [burn-bench](https://github.com/tracel-ai/burn-bench).\n\n> âš ï¸ **Warning** When using one of the `wgpu` backends, you may encounter compilation errors related\n> to recursive type evaluation. This is due to complex type nesting within the `wgpu` dependency\n> chain. To resolve this issue, add the following line at the top of your `main.rs` or `lib.rs`\n> file:\n>\n> ```rust\n> #![recursion_limit = \"256\"]\n> ```\n>\n> The default recursion limit (128) is often just below the required depth (typically 130-150) due\n> to deeply nested associated types and trait bounds.\n\n## Getting Started\n\n<div align=\"left\">\n<img align=\"right\" src=\"https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-walking.png\" height=\"96px\"/>\n\nJust heard of Burn? You are at the right place! Just continue reading this section and we hope you\ncan get on board really quickly.\n\n</div>\n\n<details>\n<summary>\nThe Burn Book ğŸ”¥\n</summary>\n<br />\n\nTo begin working effectively with Burn, it is crucial to understand its key components and\nphilosophy. This is why we highly recommend new users to read the first sections of\n[The Burn Book ğŸ”¥](https://burn.dev/books/burn/). It provides detailed examples and explanations\ncovering every facet of the framework, including building blocks like tensors, modules, and\noptimizers, all the way to advanced usage, like coding your own GPU kernels.\n\n> The project is constantly evolving, and we try as much as possible to keep the book up to date\n> with new additions. However, we might miss some details sometimes, so if you see something weird,\n> let us know! We also gladly accept Pull Requests ğŸ˜„\n\n</details>\n\n<details>\n<summary>\nExamples ğŸ™\n</summary>\n<br />\n\nLet's start with a code snippet that shows how intuitive the framework is to use! In the following,\nwe declare a neural network module with some parameters along with its forward pass.\n\n```rust\nuse burn::nn;\nuse burn::module::Module;\nuse burn::tensor::backend::Backend;\n\n#[derive(Module, Debug)]\npub struct PositionWiseFeedForward<B: Backend> {\n    linear_inner: nn::Linear<B>,\n    linear_outer: nn::Linear<B>,\n    dropout: nn::Dropout,\n    gelu: nn::Gelu,\n}\n\nimpl<B: Backend> PositionWiseFeedForward<B> {\n    pub fn forward<const D: usize>(&self, input: Tensor<B, D>) -> Tensor<B, D> {\n        let x = self.linear_inner.forward(input);\n        let x = self.gelu.forward(x);\n        let x = self.dropout.forward(x);\n\n        self.linear_outer.forward(x)\n    }\n}\n```\n\nWe have a somewhat large amount of [examples](./examples) in the repository that shows how to use\nthe framework in different scenarios.\n\nFollowing [the book](https://burn.dev/books/burn/):\n\n- [Basic Workflow](./examples/guide) : Creates a custom CNN `Module` to train on the MNIST dataset\n  and use for inference.\n- [Custom Training Loop](./examples/custom-training-loop) : Implements a basic training loop instead\n  of using the `Learner`.\n- [Custom WGPU Kernel](./examples/custom-wgpu-kernel) : Learn how to create your own custom\n  operation with the WGPU backend.\n\nAdditional examples:\n\n- [Custom CSV Dataset](./examples/custom-csv-dataset) : Implements a dataset to parse CSV data for a\n  regression task.\n- [Regression](./examples/simple-regression) : Trains a simple MLP on the California Housing dataset\n  to predict the median house value for a district.\n- [Custom Image Dataset](./examples/custom-image-dataset) : Trains a simple CNN on custom image\n  dataset following a simple folder structure.\n- [Custom Renderer](./examples/custom-renderer) : Implements a custom renderer to display the\n  [`Learner`](./building-blocks/learner.md) progress.\n- [Image Classification Web](./examples/image-classification-web) : Image classification web browser\n  demo using Burn, WGPU and WebAssembly.\n- [MNIST Inference on Web](./examples/mnist-inference-web) : An interactive MNIST inference demo in\n  the browser. The demo is available [online](https://burn.dev/demo/).\n- [MNIST Training](./examples/mnist) : Demonstrates how to train a custom `Module` (MLP) with the\n  `Learner` configured to log metrics and keep training checkpoints.\n- [Named Tensor](./examples/named-tensor) : Performs operations with the experimental `NamedTensor`\n  feature.\n- [ONNX Import Inference](./examples/onnx-inference) : Imports an ONNX model pre-trained on MNIST to\n  perform inference on a sample image with Burn.\n- [PyTorch Import Inference](./examples/import-model-weights) : Imports a PyTorch model pre-trained\n  on MNIST to perform inference on a sample image with Burn.\n- [Text Classification](./examples/text-classification) : Trains a text classification transformer\n  model on the AG News or DbPedia dataset. The trained model can then be used to classify a text\n  sample.\n- [Text Generation](./examples/text-generation) : Trains a text generation transformer model on the\n  DbPedia dataset.\n- [Wasserstein GAN MNIST](./examples/wgan) : Trains a WGAN model to generate new handwritten digits\n  based on MNIST.\n\nFor more practical insights, you can clone the repository and run any of them directly on your\ncomputer!\n\n</details>\n\n<details>\n<summary>\nPre-trained Models ğŸ¤–\n</summary>\n<br />\n\nWe keep an updated and curated list of models and examples built with Burn, see the\n[tracel-ai/models repository](https://github.com/tracel-ai/models) for more details.\n\nDon't see the model you want? Don't hesitate to open an issue, and we may prioritize it. Built a\nmodel using Burn and want to share it? You can also open a Pull Request and add your model under the\ncommunity section!\n\n</details>\n\n<details>\n<summary>\nWhy use Rust for Deep Learning? ğŸ¦€\n</summary>\n<br />\n\nDeep Learning is a special form of software where you need very high level abstractions as well as\nextremely fast execution time. Rust is the perfect candidate for that use case since it provides\nzero-cost abstractions to easily create neural network modules, and fine-grained control over memory\nto optimize every detail.\n\nIt's important that a framework be easy to use at a high level so that its users can focus on\ninnovating in the AI field. However, since running models relies so heavily on computations,\nperformance can't be neglected.\n\nTo this day, the mainstream solution to this problem has been to offer APIs in Python, but rely on\nbindings to low-level languages such as C/C++. This reduces portability, increases complexity and\ncreates frictions between researchers and engineers. We feel like Rust's approach to abstractions\nmakes it versatile enough to tackle this two languages dichotomy.\n\nRust also comes with the Cargo package manager, which makes it incredibly easy to build, test, and\ndeploy from any environment, which is usually a pain in Python.\n\nAlthough Rust has the reputation of being a difficult language at first, we strongly believe it\nleads to more reliable, bug-free solutions built faster (after some practice ğŸ˜…)!\n\n</details>\n\n<br />\n\n> **Deprecation Note**<br />Since `0.14.0`, the internal structure for tensor data has changed. The\n> previous `Data` struct was deprecated and officially removed since `0.17.0` in favor of the new\n> `TensorData` struct, which allows for more flexibility by storing the underlying data as bytes and\n> keeping the data type as a field. If you are using `Data` in your code, make sure to switch to\n> `TensorData`.\n\n<!-- >\n> In the event that you are trying to load a model record saved in a previous version, make sure to\n> enable the `record-backward-compat` feature using a previous version of burn (<=0.16.0). Otherwise,\n> the record won't be deserialized correctly and you will get an error message (which will also point\n> you to the backward compatible feature flag). The backward compatibility was maintained for\n> deserialization (loading), so as soon as you have saved the record again it will be saved according\n> to the new structure and you will be able to upgrade to this version. Please note that binary formats\n> are not backward compatible. Thus, you will need to load your record in a previous version and save it\n> to another of the self-describing record formats before using a compatible version (as described) with the\n> `record-backward-compat` feature flag. -->\n\n<details id=\"deprecation\">\n<summary>\nLoading Model Records From Previous Versions âš ï¸\n</summary>\n<br />\n\nIn the event that you are trying to load a model record saved in a version older than `0.14.0`, make\nsure to use a compatible version (`0.14`, `0.15` or `0.16`) with the `record-backward-compat`\nfeature flag.\n\n```\nfeatures = [..., \"record-backward-compat\"]\n```\n\nOtherwise, the record won't be deserialized correctly and you will get an error message. This error\nwill also point you to the backward compatible feature flag.\n\nThe backward compatibility was maintained for deserialization when loading records. Therefore, as\nsoon as you have saved the record again it will be saved according to the new structure and you can\nupgrade back to the current version\n\nPlease note that binary formats are not backward compatible. Thus, you will need to load your record\nin a previous version and save it in any of the other self-describing record format (e.g., using the\n`NamedMpkFileRecorder`) before using a compatible version (as described) with the\n`record-backward-compat` feature flag.\n\n</details>\n\n## Community\n\n<div align=\"left\">\n<img align=\"right\" src=\"https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-community.png\" height=\"96px\"/>\n\nIf you are excited about the project, don't hesitate to join our\n[Discord](https://discord.gg/uPEBbYYDB6)! We try to be as welcoming as possible to everybody from\nany background. You can ask your questions and share what you built with the community!\n\n</div>\n\n<br/>\n\n**Contributing**\n\nBefore contributing, please take a moment to review our\n[code of conduct](https://github.com/tracel-ai/burn/tree/main/CODE-OF-CONDUCT.md). It's also highly\nrecommended to read the\n[architecture overview](https://github.com/tracel-ai/burn/tree/main/contributor-book/src/project-architecture),\nwhich explains some of our architectural decisions. Refer to our\n[contributing guide](/CONTRIBUTING.md) for more details.\n\n## Status\n\nBurn is currently in active development, and there will be breaking changes. While any resulting\nissues are likely to be easy to fix, there are no guarantees at this stage.\n\n## License\n\nBurn is distributed under the terms of both the MIT license and the Apache License (Version 2.0).\nSee [LICENSE-APACHE](./LICENSE-APACHE) and [LICENSE-MIT](./LICENSE-MIT) for details. Opening a pull\nrequest is assumed to signal agreement with these licensing terms.\n\n</div>\n",
      "stars_today": 8
    },
    {
      "id": 137451403,
      "name": "nacos",
      "full_name": "alibaba/nacos",
      "description": "an easy-to-use dynamic service discovery, configuration and service management platform for building AI cloud native applications.",
      "html_url": "https://github.com/alibaba/nacos",
      "stars": 32519,
      "forks": 13232,
      "language": "Java",
      "topics": [
        "a2a-registry",
        "ai-registry",
        "alibaba",
        "config",
        "configuration-management",
        "distributed-configuration",
        "dns",
        "dubbo",
        "istio",
        "kubernetes",
        "mcp",
        "mcp-management",
        "mcp-registry",
        "microservices",
        "nacos",
        "service-discovery",
        "service-mesh",
        "spring-cloud"
      ],
      "created_at": "2018-06-15T06:49:27Z",
      "updated_at": "2026-01-14T11:13:47Z",
      "pushed_at": "2026-01-14T11:13:40Z",
      "open_issues": 265,
      "owner": {
        "login": "alibaba",
        "avatar_url": "https://avatars.githubusercontent.com/u/1961952?v=4"
      },
      "readme": "\n<img src=\"doc/Nacos_Logo.png\" width=\"50%\" syt height=\"50%\" />\n\n# Nacos: Dynamic  *Na*ming and *Co*nfiguration *S*ervice\n\n[![Gitter](https://badges.gitter.im/alibaba/nacos.svg)](https://gitter.im/alibaba/nacos?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)   [![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)\n[![Gitter](https://travis-ci.org/alibaba/nacos.svg?branch=master)](https://travis-ci.org/alibaba/nacos)\n[![](https://img.shields.io/badge/Nacos-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=nacos)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/alibaba/nacos)\n\n-------\n\n## What does it do\n\nNacos (official site: [nacos.io](https://nacos.io)) is an easy-to-use platform designed for dynamic service discovery and configuration and service management. It helps you to build cloud native applications and microservices platform easily.\n\nService is a first-class citizen in Nacos. Nacos supports almost all type of servicesï¼Œfor exampleï¼Œ[Dubbo/gRPC service](https://nacos.io/docs/latest/ecology/use-nacos-with-dubbo/), [Spring Cloud RESTFul service](https://nacos.io/docs/latest/ecology/use-nacos-with-spring-cloud/) or [Kubernetes service](https://nacos.io/docs/latest/quickstart/quick-start-kubernetes/).\n\nNacos provides four major functions.\n\n* **Service Discovery and Service Health Check** \n    \n    Nacos makes it simple for services to register themselves and to discover other services via a DNS or HTTP interface. Nacos also provides real-time health checks of services to prevent sending requests to unhealthy hosts or service instances.\n\n* **Dynamic Configuration Management**\n  \n    Dynamic Configuration Service allows you to manage configurations of all services in a centralized and dynamic manner across all environments. Nacos eliminates the need to redeploy applications and services when configurations are updated, which makes configuration changes more efficient and agile.\n\n* **Dynamic DNS Service**\n    \n    Nacos supports weighted routing, making it easier for you to implement mid-tier load balancing, flexible routing policies, flow control, and simple DNS resolution services in the production environment within your data center. It helps you to implement DNS-based service discovery easily and prevent applications from coupling to vendor-specific service discovery APIs.\n\n* **Service and MetaData Management**\n\t\n    Nacos provides an easy-to-use service dashboard to help you manage your services metadata, configuration, kubernetes DNS, service health and metrics statistics.\n \n\n## Quick Start\nIt is super easy to get started with your first project.\n\n### Deploying Nacos on cloud\n\nYou can deploy Nacos on cloud, which is the easiest and most convenient way to start Nacos. \n\nUse the following [Nacos deployment guide](https://cn.aliyun.com/product/aliware/mse?spm=nacos-website.topbar.0.0.0) to see more information and deploy a stable and out-of-the-box Nacos server.\n\n\n### Start by the provided startup package\n\n#### Step 1: Download the binary package \n\nYou can download the package from the [latest stable release](https://github.com/alibaba/nacos/releases).  \n\nTake release `nacos-server-1.0.0.zip` for example:\n```sh\nunzip nacos-server-1.0.0.zip\ncd nacos/bin \n``` \n\n#### Step 2: Start Server\n\nOn the **Linux/Unix/Mac** platform, run the following command to start server with standalone mode: \n```sh\nsh startup.sh -m standalone\n```\n\nOn the **Windows** platform, run the following command to start server with standalone mode.  Alternatively, you can also double-click the `startup.cmd` to run NacosServer.\n```\nstartup.cmd -m standalone\n```\n\nFor more details, see [quick-start.](https://nacos.io/docs/latest/quickstart/quick-start/)\n\n## Quick start for other open-source projects:\n* [Quick start with Nacos command and console](https://nacos.io/docs/latest/quickstart/quick-start/)\n\n* [Quick start with dubbo](https://nacos.io/docs/latest/ecology/use-nacos-with-dubbo/)\n\n* [Quick start with spring cloud](https://nacos.io/docs/latest/ecology/use-nacos-with-spring-cloud/)\n\n* [Quick start with kubernetes](https://nacos.io/docs/latest/quickstart/quick-start-kubernetes/)\n\n\n## Documentation\n\nYou can view the full documentation from the [Nacos website](https://nacos.io/docs/latest/overview/).\n\nYou can also read this online eBook from the [NACOS ARCHITECTURE & PRINCIPLES](https://nacos.io/docs/ebook/kbyo6n/).\n\nAll the latest and long-term notice can also be found here from [GitHub notice issue](https://github.com/alibaba/nacos/labels/notice).\n\n## Contributing\n\nContributors are welcomed to join Nacos project. Please check [CONTRIBUTING](./CONTRIBUTING.md) about how to contribute to this project.\n\n### How can I contribute?\n\n* Take a look at issues with tags marked [`good first issue`](https://github.com/alibaba/nacos/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) or [`contribution welcome`](https://github.com/alibaba/nacos/issues?q=is%3Aopen+is%3Aissue+label%3A%22contribution+welcome%22).\n* Answer questions on [issues](https://github.com/alibaba/nacos/issues).\n* Fix bugs reported on [issues](https://github.com/alibaba/nacos/issues), and send us a pull request.\n* Review the existing [pull request](https://github.com/alibaba/nacos/pulls).\n* Improve the [website](https://github.com/nacos-group/nacos-group.github.io), typically we need\n  * blog post\n  * translation on documentation\n  * use cases around the integration of Nacos in enterprise systems.\n\n## Other Related Project Repositories\n\n* [nacos-spring-project](https://github.com/nacos-group/nacos-spring-project) provides the integration functionality for Spring.\n* [nacos-group](https://github.com/nacos-group) is the repository that hosts the eco tools for Nacos, such as SDK, synchronization tool, etc.\n* [spring-cloud-alibaba](https://github.com/spring-cloud-incubator/spring-cloud-alibaba) provides the one-stop solution for application development over Alibaba middleware which includes Nacos.\n\n## Contact\n\n* [Gitter](https://gitter.im/alibaba/nacos): Nacos's IM tool for community messaging, collaboration and discovery.\n* [Twitter](https://twitter.com/nacos2): Follow along for latest nacos news on Twitter.\n* [Weibo](https://weibo.com/u/6574374908): Follow along for latest nacos news on Weibo (Twitter of China version).\n* [Nacos Segmentfault](https://segmentfault.com/t/nacos): Get latest notice and prompt help from Segmentfault.\n* Email Group:\n     * users-nacos@googlegroups.com: Nacos usage general discussion.\n     * dev-nacos@googlegroups.com: Nacos developer discussion (APIs, feature design, etc).\n     * commits-nacos@googlegroups.com: Commits notice, very high frequency.\n* Join us from DingDing(Group 1: 21708933(full), Group 2: 30438813(full), Group 3: 31222241(full), Group 4: 12810027056). \n\n### DingDing Group QR Code\n\n![](https://cdn.nlark.com/yuque/0/2025/png/1577777/1750054497446-f834cba6-fa83-4421-b202-a0dc1d5cc28b.png)\n\n### DingDing MCP Group QR Code\n\n![](https://cdn.nlark.com/yuque/0/2025/png/1577777/1750054500395-e271cbe4-2dd8-4723-8cd0-bd8a731b812a.png)\n\n### WeChat Group QR Code\n\n![](https://cdn.nlark.com/yuque/0/2025/png/1577777/1750054421702-a7d1421a-ab8e-42da-bc59-01b5d287b290.png)\n\n## Enterprise Service\nIf you need Nacos enterprise service support, or purchase cloud product services, you can join the discussion by scanning the following DingTalk group. It can also be directly activated and used through the microservice engine (MSE) provided by Alibaba Cloud.\nhttps://cn.aliyun.com/product/aliware/mse?spm=nacos-website.topbar.0.0.0\n\n<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01RTfN7q1KUzX4TcH08_!!6000000001168-2-tps-864-814.png\" width=\"500\">\n\n\n## Download\n\n- [Nacos Official Website](https://nacos.io/download/nacos-server)\n- [GitHub Release](https://github.com/alibaba/nacos/releases)\n  \n## Who is using\n\nThese are only part of the companies using Nacos, for reference only. If you are using Nacos, please [add your company here](https://github.com/alibaba/nacos/issues/273) to tell us your scenario to make Nacos better.\n\n<table>\n  <tr>\n    <td><img src=\"https://data.alibabagroup.com/ecms-files/886024452/296d05a1-c52a-4f5e-abf2-0d49d4c0d6b3.png\"  alt=\"Alibaba Group\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://a.msstatic.com/huya/main/img/logo.png\"  alt=\"è™ç‰™ç›´æ’­\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://v.icbc.com.cn/userfiles/Resources/ICBC/shouye/images/2017/logo.png\"  alt=\"ICBC\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://pic2.iqiyipic.com/lequ/20220422/e7fe69c75e2541f2a931c9e538e2ab9d.jpg\"  alt=\"çˆ±å¥‡è‰º\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1pwi9EwHqK1RjSZJnXXbNLpXa-479-59.png\"  alt=\"å¹³å®‰ç§‘æŠ€\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1MZWSEzDpK1RjSZFrXXa78VXa-269-69.png\"  alt=\"åå¤ä¿¡è´¢\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://www.urwork.cn/public/images/ui/logo.png\"  alt=\"ä¼˜å®¢å·¥åœº\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1ebu.EAvoK1RjSZFwXXciCFXa-224-80.png\"  alt=\"è´å£³æ‰¾æˆ¿\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1lxu7EBLoK1RjSZFuXXXn0XXa-409-74.png\"  alt=\"ç‘å®‰å†œæ‘å•†ä¸šé“¶è¡Œ\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1L16eEzTpK1RjSZKPXXa3UpXa-302-50.png\"  alt=\"å¸æ³•å¤§æ•°æ®\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://www.souyidai.com/www-style/images/logo.gif\"  alt=\"æœæ˜“è´·\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1OigyDyLaK1RjSZFxXXamPFXa-168-70.png\"  alt=\"å¹³è¡Œäº‘\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1gJ4vIhTpK1RjSZR0XXbEwXXa-462-60.jpg\"  alt=\"ç”˜è‚ƒç´«å…‰\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"http://www.seaskylight.com/cn/uploadfiles/image/logo.png\"  alt=\"æµ·äº‘å¤©\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1DZWSEzDpK1RjSZFrXXa78VXa-240-62.png\"  alt=\"Acmedcare+\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://14605854.s21i.faiusr.com/4/ABUIABAEGAAg4OvkzwUo8b-qlwUwxQ449gM!300x300.png\"  alt=\"åŒ—äº¬å¤©åˆäº’è”ä¿¡æ¯æœ‰é™å…¬å¸\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"http://www.mwclg.com/static-resource/front/images/home/img_logo_nav.png\"  alt=\"ä¸Šæµ·å¯†å°”å…‹å«åŒ–å·¥\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://www.synwe.com/logo-full.png\"  alt=\"å¤§è¿æ–°å”¯\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://user-images.githubusercontent.com/10215557/51593180-7563af00-1f2c-11e9-95b1-ec2c645d6a0b.png\"  alt=\"ç«‹æ€è¾°\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1zWW2EpYqK1RjSZLeXXbXppXa-262-81.png\"  alt=\"ä¸œå®¶\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"http://www.sh-guiyao.com/images/logo.jpg\"  alt=\"ä¸Šæµ·å…‹åš\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"http://www.lckjep.com:80//theme/img/logoTop.png\"  alt=\"è”é‡‡ç§‘æŠ€\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1G216EsbpK1RjSZFyXXX_qFXa-325-53.jpg\"  alt=\"å—äº¬28ç ”ç©¶æ‰€\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://p1.ifengimg.com/auto/image/2017/0922/auto_logo.png\"  alt=\"å‡¤å‡°ç½‘-æ±½è½¦\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"http://www.sinochemitech.com/zhxx/lib/images/-logo.png\"  alt=\"ä¸­åŒ–ä¿¡æ¯\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1DXerNgDqK1RjSZSyXXaxEVXa-333-103.png\"  alt=\"ä¸€ç‚¹è½¦\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1VfOANgHqK1RjSZFPXXcwapXa-313-40.png\"  alt=\"æ˜ä¼ æ— çº¿\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1lvCyNhTpK1RjSZFMXXbG_VXa-130-60.png\"  alt=\"å¦™ä¼˜è½¦\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1kY9qNgTqK1RjSZPhXXXfOFXa-120-50.png\"  alt=\"èœ‚å·¢\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1G.GBNbrpK1RjSZTEXXcWAVXa-234-65.png\"  alt=\"åå­˜æ•°æ®\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1qsurNgDqK1RjSZSyXXaxEVXa-300-90.png\"  alt=\"æ•°äº‘\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB13aywNhTpK1RjSZR0XXbEwXXa-98-38.png\"  alt=\"å¹¿é€šè½¯ä»¶\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1xqmBNjTpK1RjSZKPXXa3UpXa-162-70.png\"  alt=\"èœèœ\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB18DmINcfpK1RjSZFOXXa6nFXa-200-200.png\"  alt=\"ç§‘è“å…¬å¸\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB15uqANXzqK1RjSZFoXXbfcXXa-188-86.png\"  alt=\"æµ©é²¸\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1mvmyNkvoK1RjSZPfXXXPKFXa-238-46.png\"  alt=\"æœªåå¤©æ—¥è¯­\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1PSWsNmrqK1RjSZK9XXXyypXa-195-130.jpg\"  alt=\"é‡‘è”åˆ›\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1k1qzNbvpK1RjSZFqXXcXUVXa-160-69.png\"  alt=\"åŒçª—é“¾\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1HdyvNmzqK1RjSZFLXXcn2XXa-143-143.jpg\"  alt=\"é¡ºèƒ½\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1UdaGNgHqK1RjSZJnXXbNLpXa-277-62.png\"  alt=\"ç™¾ä¸–å¿«é€’\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"https://img.alicdn.com/tfs/TB17OqENbrpK1RjSZTEXXcWAVXa-240-113.jpg\"  alt=\"æ±½è½¦ä¹‹å®¶\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1q71ANkvoK1RjSZPfXXXPKFXa-257-104.png\"  alt=\"é²¸æ‰“å¡\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1UzuyNhTpK1RjSZR0XXbEwXXa-201-86.jpg\"  alt=\"æ—¶ä»£å…‰å\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB19RCANgHqK1RjSZFPXXcwapXa-180-180.jpg\"  alt=\"åº·ç¾\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1iCGyNb2pK1RjSZFsXXaNlXXa-143-143.jpg\"  alt=\"ç¯çƒæ˜“è´­\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://avatars0.githubusercontent.com/u/16344119?s=200&v=4\"  alt=\"Nepxion\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1aUe5EpzqK1RjSZSgXXcpAVXa-248-124.png\"  alt=\"chigua\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1H9O5EAvoK1RjSZFNXXcxMVXa-221-221.jpg\"  alt=\"å®…æ— é™\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1rNq4EwHqK1RjSZFgXXa7JXXa-200-200.jpg\"  alt=\"å¤©é˜™\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1CRAxDxYaK1RjSZFnXXa80pXa-190-190.jpg\"  alt=\"è”åˆæ°¸é“\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img.alicdn.com/tfs/TB1.q14ErrpK1RjSZTEXXcWAVXa-219-219.jpg\"  alt=\"æ˜æºäº‘\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://www.daocloud.io/static/Logo-Light.png\"  alt=\"DaoCloud\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"https://www.meicai.cn/img/logo.9210b6eb.jpg\"  alt=\"ç¾èœ\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://img5.tianyancha.com/logo/lll/3aad34039972b57e70874df8c919ae8b.png@!f_200x200\"  alt=\"æ¾æ ¼ç§‘æŠ€\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://www.jsic-tech.com/Public/uploads/20191206/5de9b9baac696.jpg\"  alt=\"é›†èƒæ™ºèƒ½\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://www.wuuxiang.com/theme/images/common/logo1.png\"  alt=\"å¾äº«\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"http://www.tpson.cn/static/upload/image/20230111/1673427385140440.png\"  alt=\"æ‹“æ·±ç§‘æŠ€\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://www.sunline.cn/u_file/fileUpload/2021-06/25/2021062586431.png\"  alt=\"é•¿äº®ç§‘æŠ€\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"http://pmt2f499f.pic44.websiteonline.cn/upload/wv0c.png\"  alt=\"æ·±åœ³æ˜“åœè½¦åº“\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"http://www.dragonwake.cn/static/css/default/img/logo.png\"  alt=\"æ­¦æ±‰æ—¥åˆ›ç§‘æŠ€\" width=\"180\" height=\"120\"></td>\n  </tr>\n  <tr>\n    <td><img src=\"https://i4im-web.oss-cn-shanghai.aliyuncs.com/images/logo.png\"  alt=\"æ˜“ç®¡æ™ºèƒ½\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://www.yunzhangfang.com/assets/img/logo.4096cf52.png\"  alt=\"äº‘å¸æˆ¿\" width=\"180\" height=\"120\"></td>\n    <td><img src=\"https://www.sinocare.com/sannuo/templates/web/img/bocweb-logo.svg\"  alt=\"ä¸‰è¯ºç”Ÿç‰©\" width=\"180\" height=\"120\"></td>\n    <td></td>\n  </tr>\n  <tr>\n    <td>éƒ‘å·å±±æ°´</td>\n    <td>çŸ¥æ°æ•™è‚²</td>\n    <td></td>\n    <td></td>\n  </tr>\n</table>\n",
      "stars_today": 7
    },
    {
      "id": 26038648,
      "name": "spdlog",
      "full_name": "gabime/spdlog",
      "description": "Fast C++ logging library.",
      "html_url": "https://github.com/gabime/spdlog",
      "stars": 28116,
      "forks": 5019,
      "language": "C++",
      "topics": [
        "cpp",
        "cpp11",
        "header-only",
        "logging",
        "spdlog"
      ],
      "created_at": "2014-11-01T01:28:53Z",
      "updated_at": "2026-01-14T18:24:23Z",
      "pushed_at": "2026-01-12T07:21:17Z",
      "open_issues": 50,
      "owner": {
        "login": "gabime",
        "avatar_url": "https://avatars.githubusercontent.com/u/6052198?v=4"
      },
      "readme": "# spdlog\r\n\r\n \r\n[![ci](https://github.com/gabime/spdlog/actions/workflows/linux.yml/badge.svg)](https://github.com/gabime/spdlog/actions/workflows/linux.yml)&nbsp;\r\n[![ci](https://github.com/gabime/spdlog/actions/workflows/windows.yml/badge.svg)](https://github.com/gabime/spdlog/actions/workflows/windows.yml)&nbsp;\r\n[![ci](https://github.com/gabime/spdlog/actions/workflows/macos.yml/badge.svg)](https://github.com/gabime/spdlog/actions/workflows/macos.yml)&nbsp;\r\n[![Build status](https://ci.appveyor.com/api/projects/status/d2jnxclg20vd0o50?svg=true&branch=v1.x)](https://ci.appveyor.com/project/gabime/spdlog) [![Release](https://img.shields.io/github/release/gabime/spdlog.svg)](https://github.com/gabime/spdlog/releases/latest)\r\n\r\nFast C++ logging library\r\n\r\n\r\n## Install\r\n#### Header-only version\r\nCopy the include [folder](include/spdlog) to your build tree and use a C++11 compiler.\r\n\r\n#### Compiled version (recommended - much faster compile times)\r\n```console\r\n$ git clone https://github.com/gabime/spdlog.git\r\n$ cd spdlog && mkdir build && cd build\r\n$ cmake .. && cmake --build .\r\n```\r\nsee example [CMakeLists.txt](example/CMakeLists.txt) on how to use.\r\n\r\n## Platforms\r\n* Linux, FreeBSD, OpenBSD, Solaris, AIX\r\n* Windows (msvc 2013+, cygwin)\r\n* macOS (clang 3.5+)\r\n* Android\r\n\r\n## Package managers:\r\n* Debian: `sudo apt install libspdlog-dev`\r\n* Homebrew: `brew install spdlog`\r\n* MacPorts: `sudo port install spdlog`\r\n* FreeBSD:  `pkg install spdlog`\r\n* Fedora: `dnf install spdlog`\r\n* Gentoo: `emerge dev-libs/spdlog`\r\n* Arch Linux: `pacman -S spdlog`\r\n* openSUSE: `sudo zypper in spdlog-devel`\r\n* ALT Linux: `apt-get install libspdlog-devel`\r\n* vcpkg: `vcpkg install spdlog`\r\n* conan: `conan install --requires=spdlog/[*]`\r\n* conda: `conda install -c conda-forge spdlog`\r\n* build2: ```depends: spdlog ^1.8.2```\r\n\r\n\r\n## Features\r\n* Very fast (see [benchmarks](#benchmarks) below).\r\n* Headers only or compiled\r\n* Feature-rich formatting, using the excellent [fmt](https://github.com/fmtlib/fmt) library.\r\n* Asynchronous mode (optional)\r\n* [Custom](https://github.com/gabime/spdlog/wiki/Custom-formatting) formatting.\r\n* Multi/Single threaded loggers.\r\n* Various log targets:\r\n  * Rotating log files.\r\n  * Daily log files.\r\n  * Console logging (colors supported).\r\n  * syslog.\r\n  * Windows event log.\r\n  * Windows debugger (```OutputDebugString(..)```).\r\n  * Log to Qt widgets ([example](#log-to-qt-with-nice-colors)).\r\n  * Easily [extendable](https://github.com/gabime/spdlog/wiki/Sinks#implementing-your-own-sink) with custom log targets.\r\n* Log filtering - log levels can be modified at runtime as well as compile time.\r\n* Support for loading log levels from argv or environment var.\r\n* [Backtrace](#backtrace-support) support - store debug messages in a ring buffer and display them later on demand.\r\n\r\n## Usage samples\r\n\r\n#### Basic usage\r\n```c++\r\n#include \"spdlog/spdlog.h\"\r\n\r\nint main() \r\n{\r\n    spdlog::info(\"Welcome to spdlog!\");\r\n    spdlog::error(\"Some error message with arg: {}\", 1);\r\n    \r\n    spdlog::warn(\"Easy padding in numbers like {:08d}\", 12);\r\n    spdlog::critical(\"Support for int: {0:d};  hex: {0:x};  oct: {0:o}; bin: {0:b}\", 42);\r\n    spdlog::info(\"Support for floats {:03.2f}\", 1.23456);\r\n    spdlog::info(\"Positional args are {1} {0}..\", \"too\", \"supported\");\r\n    spdlog::info(\"{:<30}\", \"left aligned\");\r\n    \r\n    spdlog::set_level(spdlog::level::debug); // Set *global* log level to debug\r\n    spdlog::debug(\"This message should be displayed..\");    \r\n    \r\n    // change log pattern\r\n    spdlog::set_pattern(\"[%H:%M:%S %z] [%n] [%^---%L---%$] [thread %t] %v\");\r\n    \r\n    // Compile time log levels\r\n    // Note that this does not change the current log level, it will only\r\n    // remove (depending on SPDLOG_ACTIVE_LEVEL) the call on the release code.\r\n    SPDLOG_TRACE(\"Some trace message with param {}\", 42);\r\n    SPDLOG_DEBUG(\"Some debug message\");\r\n}\r\n\r\n```\r\n---\r\n#### Create stdout/stderr logger object\r\n```c++\r\n#include \"spdlog/spdlog.h\"\r\n#include \"spdlog/sinks/stdout_color_sinks.h\"\r\nvoid stdout_example()\r\n{\r\n    // create a color multi-threaded logger\r\n    auto console = spdlog::stdout_color_mt(\"console\");    \r\n    auto err_logger = spdlog::stderr_color_mt(\"stderr\");    \r\n    spdlog::get(\"console\")->info(\"loggers can be retrieved from a global registry using the spdlog::get(logger_name)\");\r\n}\r\n```\r\n\r\n---\r\n#### Basic file logger\r\n```c++\r\n#include \"spdlog/sinks/basic_file_sink.h\"\r\nvoid basic_logfile_example()\r\n{\r\n    try \r\n    {\r\n        auto logger = spdlog::basic_logger_mt(\"basic_logger\", \"logs/basic-log.txt\");\r\n    }\r\n    catch (const spdlog::spdlog_ex &ex)\r\n    {\r\n        std::cout << \"Log init failed: \" << ex.what() << std::endl;\r\n    }\r\n}\r\n```\r\n---\r\n#### Rotating files\r\n```c++\r\n#include \"spdlog/sinks/rotating_file_sink.h\"\r\nvoid rotating_example()\r\n{\r\n    // Create a file rotating logger with 5 MB size max and 3 rotated files\r\n    auto max_size = 1048576 * 5;\r\n    auto max_files = 3;\r\n    auto logger = spdlog::rotating_logger_mt(\"some_logger_name\", \"logs/rotating.txt\", max_size, max_files);\r\n}\r\n```\r\n\r\n---\r\n#### Daily files\r\n```c++\r\n\r\n#include \"spdlog/sinks/daily_file_sink.h\"\r\nvoid daily_example()\r\n{\r\n    // Create a daily logger - a new file is created every day at 2:30 am\r\n    auto logger = spdlog::daily_logger_mt(\"daily_logger\", \"logs/daily.txt\", 2, 30);\r\n}\r\n\r\n```\r\n\r\n---\r\n#### Backtrace support\r\n```c++\r\n// Debug messages can be stored in a ring buffer instead of being logged immediately.\r\n// This is useful to display debug logs only when needed (e.g. when an error happens).\r\n// When needed, call dump_backtrace() to dump them to your log.\r\n\r\nspdlog::enable_backtrace(32); // Store the latest 32 messages in a buffer. \r\n// or my_logger->enable_backtrace(32)..\r\nfor(int i = 0; i < 100; i++)\r\n{\r\n  spdlog::debug(\"Backtrace message {}\", i); // not logged yet..\r\n}\r\n// e.g. if some error happened:\r\nspdlog::dump_backtrace(); // log them now! show the last 32 messages\r\n// or my_logger->dump_backtrace(32)..\r\n```\r\n\r\n---\r\n#### Periodic flush\r\n```c++\r\n// periodically flush all *registered* loggers every 3 seconds:\r\n// warning: only use if all your loggers are thread-safe (\"_mt\" loggers)\r\nspdlog::flush_every(std::chrono::seconds(3));\r\n\r\n```\r\n\r\n---\r\n#### Stopwatch\r\n```c++\r\n// Stopwatch support for spdlog\r\n#include \"spdlog/stopwatch.h\"\r\nvoid stopwatch_example()\r\n{\r\n    spdlog::stopwatch sw;    \r\n    spdlog::debug(\"Elapsed {}\", sw);\r\n    spdlog::debug(\"Elapsed {:.3}\", sw);       \r\n}\r\n\r\n```\r\n\r\n---\r\n#### Log binary data in hex\r\n```c++\r\n// many types of std::container<char> types can be used.\r\n// ranges are supported too.\r\n// format flags:\r\n// {:X} - print in uppercase.\r\n// {:s} - don't separate each byte with space.\r\n// {:p} - don't print the position on each line start.\r\n// {:n} - don't split the output into lines.\r\n// {:a} - show ASCII if :n is not set.\r\n\r\n#include \"spdlog/fmt/bin_to_hex.h\"\r\n\r\nvoid binary_example()\r\n{\r\n    auto console = spdlog::get(\"console\");\r\n    std::array<char, 80> buf;\r\n    console->info(\"Binary example: {}\", spdlog::to_hex(buf));\r\n    console->info(\"Another binary example:{:n}\", spdlog::to_hex(std::begin(buf), std::begin(buf) + 10));\r\n    // more examples:\r\n    // logger->info(\"uppercase: {:X}\", spdlog::to_hex(buf));\r\n    // logger->info(\"uppercase, no delimiters: {:Xs}\", spdlog::to_hex(buf));\r\n    // logger->info(\"uppercase, no delimiters, no position info: {:Xsp}\", spdlog::to_hex(buf));\r\n}\r\n\r\n```\r\n\r\n---\r\n#### Logger with multi sinks - each with a different format and log level\r\n```c++\r\n\r\n// create a logger with 2 targets, with different log levels and formats.\r\n// The console will show only warnings or errors, while the file will log all. \r\nvoid multi_sink_example()\r\n{\r\n    auto console_sink = std::make_shared<spdlog::sinks::stdout_color_sink_mt>();\r\n    console_sink->set_level(spdlog::level::warn);\r\n    console_sink->set_pattern(\"[multi_sink_example] [%^%l%$] %v\");\r\n\r\n    auto file_sink = std::make_shared<spdlog::sinks::basic_file_sink_mt>(\"logs/multisink.txt\", true);\r\n    file_sink->set_level(spdlog::level::trace);\r\n\r\n    spdlog::logger logger(\"multi_sink\", {console_sink, file_sink});\r\n    logger.set_level(spdlog::level::debug);\r\n    logger.warn(\"this should appear in both console and file\");\r\n    logger.info(\"this message should not appear in the console, only in the file\");\r\n}\r\n```\r\n\r\n---\r\n#### Register several loggers - change global level\r\n```c++\r\n\r\n// Creation of loggers. Set levels to all registered loggers. \r\nvoid set_level_example()\r\n{\r\n    auto logger1 = spdlog::basic_logger_mt(\"logger1\", \"logs/logger1.txt\");\r\n    auto logger2 = spdlog::basic_logger_mt(\"logger2\", \"logs/logger2.txt\");\r\n\r\n    spdlog::set_default_logger(logger2);\r\n    spdlog::default_logger()->set_level(spdlog::level::trace); // set level for the default logger (logger2) to trace\r\n\r\n    spdlog::trace(\"trace message to the logger2 (specified as default)\");\r\n\r\n    spdlog::set_level(spdlog::level::off) // (sic!) set level for *all* registered loggers to off (disable)\r\n  \r\n    logger1.warn(\"warn message will not appear because the level set to off\");\r\n    logger2.warn(\"warn message will not appear because the level set to off\");\r\n    spdlog::warn(\"warn message will not appear because the level set to off\");\r\n}\r\n```\r\n\r\n---\r\n#### User-defined callbacks about log events\r\n```c++\r\n\r\n// create a logger with a lambda function callback, the callback will be called\r\n// each time something is logged to the logger\r\nvoid callback_example()\r\n{\r\n    auto callback_sink = std::make_shared<spdlog::sinks::callback_sink_mt>([](const spdlog::details::log_msg &msg) {\r\n         // for example you can be notified by sending an email to yourself\r\n    });\r\n    callback_sink->set_level(spdlog::level::err);\r\n\r\n    auto console_sink = std::make_shared<spdlog::sinks::stdout_color_sink_mt>();\r\n    spdlog::logger logger(\"custom_callback_logger\", {console_sink, callback_sink});\r\n\r\n    logger.info(\"some info log\");\r\n    logger.error(\"critical issue\"); // will notify you\r\n}\r\n```\r\n\r\n---\r\n#### Asynchronous logging\r\n```c++\r\n#include \"spdlog/async.h\"\r\n#include \"spdlog/sinks/basic_file_sink.h\"\r\nvoid async_example()\r\n{\r\n    // default thread pool settings can be modified *before* creating the async logger:\r\n    // spdlog::init_thread_pool(8192, 1); // queue with 8k items and 1 backing thread.\r\n    auto async_file = spdlog::basic_logger_mt<spdlog::async_factory>(\"async_file_logger\", \"logs/async_log.txt\");\r\n    // alternatively:\r\n    // auto async_file = spdlog::create_async<spdlog::sinks::basic_file_sink_mt>(\"async_file_logger\", \"logs/async_log.txt\");   \r\n}\r\n\r\n```\r\n\r\n---\r\n#### Asynchronous logger with multi sinks\r\n```c++\r\n#include \"spdlog/async.h\"\r\n#include \"spdlog/sinks/stdout_color_sinks.h\"\r\n#include \"spdlog/sinks/rotating_file_sink.h\"\r\n\r\nvoid multi_sink_example2()\r\n{\r\n    spdlog::init_thread_pool(8192, 1);\r\n    auto stdout_sink = std::make_shared<spdlog::sinks::stdout_color_sink_mt >();\r\n    auto rotating_sink = std::make_shared<spdlog::sinks::rotating_file_sink_mt>(\"mylog.txt\", 1024*1024*10, 3);\r\n    std::vector<spdlog::sink_ptr> sinks {stdout_sink, rotating_sink};\r\n    auto logger = std::make_shared<spdlog::async_logger>(\"loggername\", sinks.begin(), sinks.end(), spdlog::thread_pool(), spdlog::async_overflow_policy::block);\r\n    spdlog::register_logger(logger);\r\n}\r\n```\r\n \r\n---\r\n#### User-defined types\r\n```c++\r\ntemplate<>\r\nstruct fmt::formatter<my_type> : fmt::formatter<std::string>\r\n{\r\n    auto format(my_type my, format_context &ctx) const -> decltype(ctx.out())\r\n    {\r\n        return fmt::format_to(ctx.out(), \"[my_type i={}]\", my.i);\r\n    }\r\n};\r\n\r\nvoid user_defined_example()\r\n{\r\n    spdlog::info(\"user defined type: {}\", my_type(14));\r\n}\r\n\r\n```\r\n\r\n---\r\n#### User-defined flags in the log pattern\r\n```c++ \r\n// Log patterns can contain custom flags.\r\n// the following example will add new flag '%*' - which will be bound to a <my_formatter_flag> instance.\r\n#include \"spdlog/pattern_formatter.h\"\r\nclass my_formatter_flag : public spdlog::custom_flag_formatter\r\n{\r\npublic:\r\n    void format(const spdlog::details::log_msg &, const std::tm &, spdlog::memory_buf_t &dest) override\r\n    {\r\n        std::string some_txt = \"custom-flag\";\r\n        dest.append(some_txt.data(), some_txt.data() + some_txt.size());\r\n    }\r\n\r\n    std::unique_ptr<custom_flag_formatter> clone() const override\r\n    {\r\n        return spdlog::details::make_unique<my_formatter_flag>();\r\n    }\r\n};\r\n\r\nvoid custom_flags_example()\r\n{    \r\n    auto formatter = std::make_unique<spdlog::pattern_formatter>();\r\n    formatter->add_flag<my_formatter_flag>('*').set_pattern(\"[%n] [%*] [%^%l%$] %v\");\r\n    spdlog::set_formatter(std::move(formatter));\r\n}\r\n\r\n```\r\n\r\n---\r\n#### Custom error handler\r\n```c++\r\nvoid err_handler_example()\r\n{\r\n    // can be set globally or per logger(logger->set_error_handler(..))\r\n    spdlog::set_error_handler([](const std::string &msg) { spdlog::get(\"console\")->error(\"*** LOGGER ERROR ***: {}\", msg); });\r\n    spdlog::get(\"console\")->info(\"some invalid message to trigger an error {}{}{}{}\", 3);\r\n}\r\n\r\n```\r\n\r\n---\r\n#### syslog\r\n```c++\r\n#include \"spdlog/sinks/syslog_sink.h\"\r\nvoid syslog_example()\r\n{\r\n    std::string ident = \"spdlog-example\";\r\n    auto syslog_logger = spdlog::syslog_logger_mt(\"syslog\", ident, LOG_PID);\r\n    syslog_logger->warn(\"This is warning that will end up in syslog.\");\r\n}\r\n```\r\n---\r\n#### Android example\r\n```c++\r\n#include \"spdlog/sinks/android_sink.h\"\r\nvoid android_example()\r\n{\r\n    std::string tag = \"spdlog-android\";\r\n    auto android_logger = spdlog::android_logger_mt(\"android\", tag);\r\n    android_logger->critical(\"Use \\\"adb shell logcat\\\" to view this message.\");\r\n}\r\n```\r\n\r\n---\r\n#### Load log levels from the env variable or argv\r\n\r\n```c++\r\n#include \"spdlog/cfg/env.h\"\r\nint main (int argc, char *argv[])\r\n{\r\n    spdlog::cfg::load_env_levels();\r\n    // or specify the env variable name:\r\n    // MYAPP_LEVEL=info,mylogger=trace && ./example\r\n    // spdlog::cfg::load_env_levels(\"MYAPP_LEVEL\");\r\n    // or from the command line:\r\n    // ./example SPDLOG_LEVEL=info,mylogger=trace\r\n    // #include \"spdlog/cfg/argv.h\" // for loading levels from argv\r\n    // spdlog::cfg::load_argv_levels(argc, argv);\r\n}\r\n```\r\nSo then you can:\r\n\r\n```console\r\n$ export SPDLOG_LEVEL=info,mylogger=trace\r\n$ ./example\r\n```\r\n\r\n\r\n---\r\n#### Log file open/close event handlers\r\n```c++\r\n// You can get callbacks from spdlog before/after a log file has been opened or closed. \r\n// This is useful for cleanup procedures or for adding something to the start/end of the log file.\r\nvoid file_events_example()\r\n{\r\n    // pass the spdlog::file_event_handlers to file sinks for open/close log file notifications\r\n    spdlog::file_event_handlers handlers;\r\n    handlers.before_open = [](spdlog::filename_t filename) { spdlog::info(\"Before opening {}\", filename); };\r\n    handlers.after_open = [](spdlog::filename_t filename, std::FILE *fstream) { fputs(\"After opening\\n\", fstream); };\r\n    handlers.before_close = [](spdlog::filename_t filename, std::FILE *fstream) { fputs(\"Before closing\\n\", fstream); };\r\n    handlers.after_close = [](spdlog::filename_t filename) { spdlog::info(\"After closing {}\", filename); };\r\n    auto my_logger = spdlog::basic_logger_st(\"some_logger\", \"logs/events-sample.txt\", true, handlers);        \r\n}\r\n```\r\n\r\n---\r\n#### Replace the Default Logger\r\n```c++\r\nvoid replace_default_logger_example()\r\n{\r\n    auto new_logger = spdlog::basic_logger_mt(\"new_default_logger\", \"logs/new-default-log.txt\", true);\r\n    spdlog::set_default_logger(new_logger);\r\n    spdlog::info(\"new logger log message\");\r\n}\r\n```\r\n\r\n---\r\n#### Log to Qt with nice colors\r\n```c++\r\n#include \"spdlog/spdlog.h\"\r\n#include \"spdlog/sinks/qt_sinks.h\"\r\nMainWindow::MainWindow(QWidget *parent) : QMainWindow(parent)\r\n{\r\n    setMinimumSize(640, 480);\r\n    auto log_widget = new QTextEdit(this);\r\n    setCentralWidget(log_widget);\r\n    int max_lines = 500; // keep the text widget to max 500 lines. remove old lines if needed.\r\n    auto logger = spdlog::qt_color_logger_mt(\"qt_logger\", log_widget, max_lines);\r\n    logger->info(\"Some info message\");\r\n}\r\n```\r\n---\r\n\r\n#### Mapped Diagnostic Context\r\n```c++\r\n// Mapped Diagnostic Context (MDC) is a map that stores key-value pairs (string values) in thread local storage.\r\n// Each thread maintains its own MDC, which loggers use to append diagnostic information to log outputs.\r\n// Note: it is not supported in asynchronous mode due to its reliance on thread-local storage.\r\n#include \"spdlog/mdc.h\"\r\nvoid mdc_example()\r\n{\r\n    spdlog::mdc::put(\"key1\", \"value1\");\r\n    spdlog::mdc::put(\"key2\", \"value2\");\r\n    // if not using the default format, use the %& formatter to print mdc data\r\n    // spdlog::set_pattern(\"[%H:%M:%S %z] [%^%L%$] [%&] %v\");\r\n}\r\n```\r\n---\r\n## Benchmarks\r\n\r\nBelow are some [benchmarks](bench/bench.cpp) done in Ubuntu 64 bit, Intel i7-4770 CPU @ 3.40GHz\r\n\r\n#### Synchronous mode\r\n```\r\n[info] **************************************************************\r\n[info] Single thread, 1,000,000 iterations\r\n[info] **************************************************************\r\n[info] basic_st         Elapsed: 0.17 secs        5,777,626/sec\r\n[info] rotating_st      Elapsed: 0.18 secs        5,475,894/sec\r\n[info] daily_st         Elapsed: 0.20 secs        5,062,659/sec\r\n[info] empty_logger     Elapsed: 0.07 secs       14,127,300/sec\r\n[info] **************************************************************\r\n[info] C-string (400 bytes). Single thread, 1,000,000 iterations\r\n[info] **************************************************************\r\n[info] basic_st         Elapsed: 0.41 secs        2,412,483/sec\r\n[info] rotating_st      Elapsed: 0.72 secs        1,389,196/sec\r\n[info] daily_st         Elapsed: 0.42 secs        2,393,298/sec\r\n[info] null_st          Elapsed: 0.04 secs       27,446,957/sec\r\n[info] **************************************************************\r\n[info] 10 threads, competing over the same logger object, 1,000,000 iterations\r\n[info] **************************************************************\r\n[info] basic_mt         Elapsed: 0.60 secs        1,659,613/sec\r\n[info] rotating_mt      Elapsed: 0.62 secs        1,612,493/sec\r\n[info] daily_mt         Elapsed: 0.61 secs        1,638,305/sec\r\n[info] null_mt          Elapsed: 0.16 secs        6,272,758/sec\r\n```\r\n#### Asynchronous mode\r\n```\r\n[info] -------------------------------------------------\r\n[info] Messages     : 1,000,000\r\n[info] Threads      : 10\r\n[info] Queue        : 8,192 slots\r\n[info] Queue memory : 8,192 x 272 = 2,176 KB \r\n[info] -------------------------------------------------\r\n[info] \r\n[info] *********************************\r\n[info] Queue Overflow Policy: block\r\n[info] *********************************\r\n[info] Elapsed: 1.70784 secs     585,535/sec\r\n[info] Elapsed: 1.69805 secs     588,910/sec\r\n[info] Elapsed: 1.7026 secs      587,337/sec\r\n[info] \r\n[info] *********************************\r\n[info] Queue Overflow Policy: overrun\r\n[info] *********************************\r\n[info] Elapsed: 0.372816 secs    2,682,285/sec\r\n[info] Elapsed: 0.379758 secs    2,633,255/sec\r\n[info] Elapsed: 0.373532 secs    2,677,147/sec\r\n\r\n```\r\n\r\n## Documentation\r\n\r\nDocumentation can be found in the [wiki](https://github.com/gabime/spdlog/wiki) pages.\r\n\r\n---\r\n\r\n### Powered by\r\n<a href=\"https://jb.gg/OpenSource\">\r\n  <img src=\"https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.svg\" alt=\"JetBrains logo\" width=\"200\">\r\n</a>\r\n",
      "stars_today": 7
    },
    {
      "id": 75277003,
      "name": "thingsboard",
      "full_name": "thingsboard/thingsboard",
      "description": "Open-source IoT Platform - Device management, data collection, processing and visualization.",
      "html_url": "https://github.com/thingsboard/thingsboard",
      "stars": 20920,
      "forks": 6031,
      "language": "Java",
      "topics": [
        "cloud",
        "coap",
        "dashboard",
        "iot",
        "iot-analytics",
        "iot-platform",
        "iot-solutions",
        "java",
        "kafka",
        "lwm2m",
        "microservices",
        "middleware",
        "mqtt",
        "netty",
        "platform",
        "snmp",
        "thingsboard",
        "visualization",
        "websockets",
        "widgets"
      ],
      "created_at": "2016-12-01T09:33:30Z",
      "updated_at": "2026-01-14T12:40:00Z",
      "pushed_at": "2026-01-14T15:52:05Z",
      "open_issues": 157,
      "owner": {
        "login": "thingsboard",
        "avatar_url": "https://avatars.githubusercontent.com/u/24291394?v=4"
      },
      "readme": "![banner](https://github.com/user-attachments/assets/3584b592-33dd-4fb4-91d4-47b62b34806c)\n\n<div align=\"center\">\n\n# Open-source IoT platform for data collection, processing, visualization, and device management.\n\n</div>\n<br>\n<div align=\"center\">\n \nğŸ’¡ [Get started](https://thingsboard.io/docs/getting-started-guides/helloworld/)&ensp;â€¢&ensp;ğŸŒ [Website](https://thingsboard.io/)&ensp;â€¢&ensp;ğŸ“š [Documentation](https://thingsboard.io/docs/)&ensp;â€¢&ensp;ğŸ“” [Blog](https://thingsboard.io/blog/)&ensp;â€¢&ensp;â–¶ï¸ [Live demo](https://demo.thingsboard.io/signup)&ensp;â€¢&ensp;ğŸ”— [LinkedIn](https://www.linkedin.com/company/thingsboard/posts/?feedView=all)\n\n</div>\n\n## ğŸš€ Installation options\n\n* Install ThingsBoard [On-premise](https://thingsboard.io/docs/user-guide/install/installation-options/?ceInstallType=onPremise)\n* Try [ThingsBoard Cloud](https://thingsboard.io/installations/)\n* or [Use our Live demo](https://demo.thingsboard.io/signup)\n\n## ğŸ’¡ Getting started with ThingsBoard\n\nCheck out our [Getting Started guide](https://thingsboard.io/docs/getting-started-guides/helloworld/) or [watch the video](https://www.youtube.com/watch?v=80L0ubQLXsc) to learn the basics of ThingsBoard and create your first dashboard! You will learn to:\n\n* Connect devices to ThingsBoard\n* Push data from devices to ThingsBoard\n* Build real-time dashboards\n* Create a Customer and assign the dashboard with them.\n* Define thresholds and trigger alarms\n* Set up notifications via email, SMS, mobile apps, or integrate with third-party services.\n\n## âœ¨ Features\n\n<table>\n  <tr>\n    <td width=\"50%\" valign=\"top\">\n      <br>\n      <div align=\"center\">\n        <img src=\"https://github.com/user-attachments/assets/255cca4f-b111-44e8-99ea-0af55f8e3681\" alt=\"Provision and manage devices and assets\" width=\"378\" />\n        <h3>Provision and manage <br> devices and assets</h3>\n      </div>\n      <div align=\"center\">\n        <p>Provision, monitor and control your IoT entities in secure way using rich server-side APIs. Define relations between your devices, assets, customers or any other entities.</p>\n      </div>\n      <br>\n      <div align=\"center\">\n        <a href=\"https://thingsboard.io/docs/user-guide/entities-and-relations/\">Read more âœ</a>\n      </div>\n      <br>\n    </td>\n    <td width=\"50%\" valign=\"top\">\n      <br>\n      <div align=\"center\">\n        <img src=\"https://github.com/user-attachments/assets/24b41d10-150a-42dd-ab1a-32ac9b5978c1\" alt=\"Collect and visualize your data\" width=\"378\" />\n        <h3>Collect and visualize <br> your data</h3>\n      </div>\n      <div align=\"center\">\n        <p>Collect and store telemetry data in scalable and fault-tolerant way. Visualize your data with built-in or custom widgets and flexible dashboards. Share dashboards with your customers.</p>\n      </div>\n      <br>\n      <div align=\"center\">\n        <a href=\"https://thingsboard.io/iot-data-visualization/\">Read more âœ</a>\n      </div>\n      <br>\n    </td>\n  </tr>\n  <tr>\n    <td width=\"50%\" valign=\"top\">\n      <br>\n      <div align=\"center\">\n        <img src=\"https://github.com/user-attachments/assets/6f2a6dd2-7b33-4d17-8b92-d1f995adda2c\" alt=\"SCADA Dashboards\" width=\"378\" />\n        <h3>SCADA Dashboards</h3>\n      </div>\n      <div align=\"center\">\n        <p>Monitor and control your industrial processes in real time with SCADA. Use SCADA symbols on dashboards to create and manage any workflow, offering full flexibility to design and oversee operations according to your requirements.</p>\n      </div>\n      <br>\n      <div align=\"center\">\n        <a href=\"https://thingsboard.io/use-cases/scada/\">Read more âœ</a>\n      </div>\n      <br>\n    </td>\n    <td width=\"50%\" valign=\"top\">\n      <br>\n      <div align=\"center\">\n        <img src=\"https://github.com/user-attachments/assets/c23dcc9b-aeba-40ef-9973-49b953fc1257\" alt=\"Process and React\" width=\"378\" />\n        <h3>Process and React</h3>\n      </div>\n      <div align=\"center\">\n        <p>Define data processing rule chains. Transform and normalize your device data. Raise alarms on incoming telemetry events, attribute updates, device inactivity and user actions.<br></p>\n      </div>\n      <br>\n      <br>\n      <div align=\"center\">\n        <a href=\"https://thingsboard.io/docs/user-guide/rule-engine-2-0/re-getting-started/\">Read more âœ</a>\n      </div>\n      <br>\n    </td>\n  </tr>\n</table>\n\n## âš™ï¸ Powerful IoT Rule Engine\n\nThingsBoard allows you to create complex [Rule Chains](https://thingsboard.io/docs/user-guide/rule-engine-2-0/re-getting-started/) to process data from your devices and match your application specific use cases.\n\n[![IoT Rule Engine](https://github.com/user-attachments/assets/43d21dc9-0e18-4f1b-8f9a-b72004e12f07 \"IoT Rule Engine\")](https://thingsboard.io/docs/user-guide/rule-engine-2-0/re-getting-started/)\n\n<div align=\"center\">\n\n[**Read more about Rule Engine âœ**](https://thingsboard.io/docs/user-guide/rule-engine-2-0/re-getting-started/)\n\n</div>\n\n## ğŸ“¦ Real-Time IoT Dashboards\n\nThingsBoard is a scalable, user-friendly, and device-agnostic IoT platform that speeds up time-to-market with powerful built-in solution templates. It enables data collection and analysis from any devices, saving resources on routine tasks and letting you focus on your solutionâ€™s unique aspects. See more our Use Cases [here](https://thingsboard.io/iot-use-cases/).\n\n[**Smart energy**](https://thingsboard.io/use-cases/smart-energy/)\n\n[![Smart energy](https://github.com/user-attachments/assets/2a0abf13-6dc5-4f5e-9c30-1aea1d39af1e \"Smart energy\")](https://thingsboard.io/use-cases/smart-energy/)\n\n[**SCADA swimming pool**](https://thingsboard.io/use-cases/scada/)\n\n[![SCADA Swimming pool](https://github.com/user-attachments/assets/68fd9e29-99f1-4c16-8c4c-476f4ccb20c0 \"SCADA Swimming pool\")](https://thingsboard.io/use-cases/scada/)\n\n[**Fleet tracking**](https://thingsboard.io/use-cases/fleet-tracking/)\n\n[![Fleet tracking](https://github.com/user-attachments/assets/9e8938ba-ee0c-4599-9494-d74b7de8a63d \"Fleet tracking\")](https://thingsboard.io/use-cases/fleet-tracking/)\n\n[**Smart farming**](https://thingsboard.io/use-cases/smart-farming/)\n\n[![Smart farming](https://github.com/user-attachments/assets/56b84c99-ef24-44e5-a903-b925b7f9d142 \"Smart farming\")](https://thingsboard.io/use-cases/smart-farming/)\n\n[**Smart metering**](https://thingsboard.io/smart-metering/)\n\n[![Smart metering](https://github.com/user-attachments/assets/adc05e3d-397c-48ef-bed6-535bbd698455 \"Smart metering\")](https://thingsboard.io/smart-metering/)\n\n<div align=\"center\">\n\n[**Check more of our use cases âœ**](https://thingsboard.io/iot-use-cases/)\n\n</div>\n\n## ğŸ«¶ Support\n\nTo get support, please visit our [GitHub issues page](https://github.com/thingsboard/thingsboard/issues)\n\n## ğŸ“„ Licenses\n\nThis project is released under [Apache 2.0 License](./LICENSE)\n",
      "stars_today": 7
    },
    {
      "id": 51905353,
      "name": "arrow",
      "full_name": "apache/arrow",
      "description": "Apache Arrow is the universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics",
      "html_url": "https://github.com/apache/arrow",
      "stars": 16395,
      "forks": 3983,
      "language": "C++",
      "topics": [
        "arrow",
        "parquet"
      ],
      "created_at": "2016-02-17T08:00:23Z",
      "updated_at": "2026-01-15T00:55:51Z",
      "pushed_at": "2026-01-15T00:55:45Z",
      "open_issues": 4096,
      "owner": {
        "login": "apache",
        "avatar_url": "https://avatars.githubusercontent.com/u/47359?v=4"
      },
      "readme": "<!---\n  Licensed to the Apache Software Foundation (ASF) under one\n  or more contributor license agreements.  See the NOTICE file\n  distributed with this work for additional information\n  regarding copyright ownership.  The ASF licenses this file\n  to you under the Apache License, Version 2.0 (the\n  \"License\"); you may not use this file except in compliance\n  with the License.  You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.\n-->\n\n# Apache Arrow\n\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/arrow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:arrow)\n[![License](https://img.shields.io/:license-Apache%202-blue.svg)](https://github.com/apache/arrow/blob/main/LICENSE.txt)\n[![BlueSky Follow](https://img.shields.io/badge/bluesky-Follow-blue?logo=bluesky)](https://bsky.app/profile/arrow.apache.org)\n\n## Powering In-Memory Analytics\n\nApache Arrow is a universal columnar format and multi-language toolbox for fast\ndata interchange and in-memory analytics. It contains a set of technologies that\nenable data systems to efficiently store, process, and move data.\n\nMajor components of the project include:\n\n - [The Arrow Columnar Format](https://arrow.apache.org/docs/dev/format/Columnar.html):\n   a standard and efficient in-memory representation of various datatypes, plain or nested\n - [The Arrow IPC Format](https://arrow.apache.org/docs/dev/format/Columnar.html#serialization-and-interprocess-communication-ipc):\n   an efficient serialization of the Arrow format and associated metadata,\n   for communication between processes and heterogeneous environments\n - [ADBC (Arrow Database Connectivity)](https://github.com/apache/arrow-adbc/) `â†—`: Arrow-powered API,\n   drivers, and libraries for access to databases and query engines\n - [The Arrow Flight RPC protocol](https://github.com/apache/arrow/tree/main/format/Flight.proto):\n   based on the Arrow IPC format, a building block for remote services exchanging\n   Arrow data with application-defined semantics (for example a storage server or a database)\n - [C++ libraries](https://github.com/apache/arrow/tree/main/cpp)\n - [C bindings using GLib](https://github.com/apache/arrow/tree/main/c_glib)\n - [.NET libraries](https://github.com/apache/arrow-dotnet) `â†—`\n - [Gandiva](https://github.com/apache/arrow/tree/main/cpp/src/gandiva):\n   an [LLVM](https://llvm.org)-based Arrow expression compiler, part of the C++ codebase\n - [Go libraries](https://github.com/apache/arrow-go) `â†—`\n - [Java libraries](https://github.com/apache/arrow-java) `â†—`\n - [JavaScript libraries](https://github.com/apache/arrow-js) `â†—`\n - [Julia implementation](https://github.com/apache/arrow-julia) `â†—`\n - [Python libraries](https://github.com/apache/arrow/tree/main/python)\n - [R libraries](https://github.com/apache/arrow/tree/main/r)\n - [Ruby libraries](https://github.com/apache/arrow/tree/main/ruby)\n - [Rust libraries](https://github.com/apache/arrow-rs) `â†—`\n - [Swift libraries](https://github.com/apache/arrow-swift) `â†—`\n\nThe `â†—` icon denotes that this component of the project is maintained in a separate\nrepository.\n\nArrow is an [Apache Software Foundation](https://www.apache.org) project. Learn more at\n[arrow.apache.org](https://arrow.apache.org).\n\n## What's in the Arrow libraries?\n\nThe reference Arrow libraries contain many distinct software components:\n\n- Columnar vector and table-like containers (similar to data frames) supporting\n  flat or nested types\n- Fast, language agnostic metadata messaging layer (using Google's FlatBuffers\n  library)\n- Reference-counted off-heap buffer memory management, for zero-copy memory\n  sharing and handling memory-mapped files\n- IO interfaces to local and remote filesystems\n- Self-describing binary wire formats (streaming and batch/file-like) for\n  remote procedure calls (RPC) and interprocess communication (IPC)\n- Integration tests for verifying binary compatibility between the\n  implementations (e.g. sending data from Java to C++)\n- Conversions to and from other in-memory data structures\n- Readers and writers for various widely-used file formats (such as Parquet, CSV)\n\n## Implementation status\n\nThe official Arrow libraries in this repository are in different stages of\nimplementing the Arrow format and related features.  See our current\n[feature matrix](https://arrow.apache.org/docs/dev/status.html)\non git main.\n\n## How to Contribute\n\nPlease read our latest [project contribution guide][5].\n\n## Getting involved\n\nEven if you do not plan to contribute to Apache Arrow itself or Arrow\nintegrations in other projects, we'd be happy to have you involved:\n\n- Join the mailing list: send an email to\n  [dev-subscribe@arrow.apache.org][1]. Share your ideas and use cases for the\n  project.\n- Follow our activity on [GitHub issues][3]\n- [Learn the format][2]\n- Contribute code to one of the reference implementations\n\n[1]: mailto:dev-subscribe@arrow.apache.org\n[2]: https://github.com/apache/arrow/tree/main/format\n[3]: https://github.com/apache/arrow/issues\n[4]: https://github.com/apache/arrow\n[5]: https://arrow.apache.org/docs/dev/developers/index.html\n",
      "stars_today": 7
    },
    {
      "id": 693187866,
      "name": "rolldown",
      "full_name": "rolldown/rolldown",
      "description": "Fast Rust bundler for JavaScript/TypeScript with Rollup-compatible API.",
      "html_url": "https://github.com/rolldown/rolldown",
      "stars": 12632,
      "forks": 682,
      "language": "Rust",
      "topics": [
        "bundler",
        "javascript",
        "typescript"
      ],
      "created_at": "2023-09-18T14:20:28Z",
      "updated_at": "2026-01-14T23:26:51Z",
      "pushed_at": "2026-01-14T20:11:16Z",
      "open_issues": 233,
      "owner": {
        "login": "rolldown",
        "avatar_url": "https://avatars.githubusercontent.com/u/94954945?v=4"
      },
      "readme": "<p align=\"center\">\n  <br>\n  <br>\n  <a href=\"https://rolldown.rs\" target=\"_blank\" rel=\"noopener noreferrer\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://rolldown.rs/rolldown-light.svg\">\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"https://rolldown.rs/rolldown-dark.svg\">\n      <img alt=\"rolldown logo\" src=\"https://rolldown.rs/rolldown-dark.svg\" height=\"60\">\n    </picture>\n  </a>\n  <br>\n  <br>\n  <br>\n</p>\n\n<div align=\"center\">\n\n[![MIT licensed][badge-license]][url-license]\n[![NPM version][badge-npm-version]][url-npm]\n[![CodSpeed Badge](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/rolldown/rolldown)\n[![Discord chat][badge-discord]][discord-url]\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/rolldown/rolldown)\n\n</div>\n\n<div align=\"center\">\n\n[![NPM Unpacked Size (with version)](https://img.shields.io/npm/unpacked-size/rolldown/latest?label=npm)][url-npm]\n[![NPM Unpacked Size darwin-arm64](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-darwin-arm64/latest?label=darwin-arm64)](https://www.npmjs.com/package/@rolldown/binding-darwin-arm64)\n[![NPM Unpacked Size darwin-x64](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-darwin-x64/latest?label=darwin-x64)](https://www.npmjs.com/package/@rolldown/binding-darwin-x64)\n[![NPM Unpacked Size linux-x64-gnu](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-linux-x64-gnu/latest?label=linux-x64-gnu)](https://www.npmjs.com/package/@rolldown/binding-linux-x64-gnu)\n[![NPM Unpacked Size win32-x64](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-win32-x64-msvc/latest?label=win32-x64)](https://www.npmjs.com/package/@rolldown/binding-win32-x64-msvc)\n[![NPM Unpacked Size wasm32-wasi](https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-wasm32-wasi/latest?label=wasm32-wasi)](https://www.npmjs.com/package/@rolldown/binding-wasm32-wasi)\n\n</div>\n\n<div align=\"center\">\n\n[![pkg.pr.new](https://pkg.pr.new/badge/pkg.pr.new/pkg.pr.new?style=flat&color=000&logoSize=auto)](https://pkg.pr.new/~/rolldown/rolldown)\n\n</div>\n\n<div align=\"center\">\n\n[![rolldown-starter-stackblitz](https://developer.stackblitz.com/img/open_in_stackblitz.svg)](https://stackblitz.com/fork/github/rolldown/rolldown-starter-stackblitz)\n\n</div>\n\n> ğŸš§ **Beta Software**\n>\n> Rolldown is currently in beta status. While it can already handle most production use cases, there may still be bugs and rough edges. Most notably, the built-in minification feature is still in alpha status.\n\n# Rolldown\n\nRolldown is a JavaScript/TypeScript bundler written in Rust intended to serve as the future bundler used in [Vite](https://vitejs.dev/). It provides Rollup-compatible APIs and plugin interface, but will be more similar to esbuild in scope.\n\nFor more information, please check out the documentation at [rolldown.rs](https://rolldown.rs/guide/getting-started).\n\n## VoidZero Inc.\n\nRolldown is a project of [VoidZero](https://voidzero.dev/), see our announcement [Announcing VoidZero - Next Generation Toolchain for JavaScript](https://voidzero.dev/posts/announcing-voidzero-inc).\n\nIf you have requirements for JavaScript tools at scale, please [get in touch](https://forms.gle/WQgjyzYJpwurpxWKA)!\n\n## Contributing\n\nWe would love to have more contributors involved!\n\nTo get started, please read our [Contributing Guide](https://rolldown.rs/contribution-guide/).\n\n## Credits\n\nThe Rolldown project is heavily inspired by:\n\n- [Rollup](https://github.com/rollup/rollup), created by [Rich Harris](https://github.com/Rich-Harris) and maintained by [Lukas Taegert-Atkinson](https://github.com/lukastaegert).\n- [esbuild](https://github.com/evanw/esbuild), created by [Evan Wallace](https://github.com/evanw).\n\nAnd supported by:\n\n- [napi-rs](https://github.com/napi-rs/napi-rs) for Node.js add-ons in Rust via Node-API.\n- [oxc](https://github.com/oxc-project/oxc) for the underlying parser, resolver, and sourcemap support.\n\n## Licenses\n\nThis project is licensed under the [MIT License](LICENSE).\n\nThis project also partially contains code derived or copied from the following projects:\n\n- [rollup(MIT)](https://github.com/rollup/rollup/blob/680912e2ceb42c8d5e571e01c6ece0e4889aecbb/LICENSE-CORE.md)\n- [esbuild(MIT)](https://github.com/evanw/esbuild/blob/0c8a0a901d9a6c7bbff9b4dd347c8a3f65f6c6dd/LICENSE.md)\n\nLicenses of these projects are listed in [THIRD-PARTY-LICENSE](/THIRD-PARTY-LICENSE)\n\n[badge-discord]: https://img.shields.io/discord/1079625926024900739?logo=discord&label=Discord\n[discord-url]: https://chat.rolldown.rs\n[badge-license]: https://img.shields.io/badge/license-MIT-blue.svg\n[url-license]: https://github.com/rolldown/rolldown/blob/main/LICENSE\n[badge-npm-version]: https://img.shields.io/npm/v/rolldown/latest?color=brightgreen\n[url-npm]: https://www.npmjs.com/package/rolldown/v/latest\n\n[badge-binary-size-windows]: [https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-win32-x64-msvc/latest]\n[badge-binary-size-macos]: [https://img.shields.io/npm/unpacked-size/%40rolldown%2Fbinding-darwin-arm64/latest]\n",
      "stars_today": 7
    },
    {
      "id": 206121828,
      "name": "FreeRTOS",
      "full_name": "FreeRTOS/FreeRTOS",
      "description": "'Classic' FreeRTOS distribution.  Started as Git clone of FreeRTOS SourceForge SVN repo.  Submodules the kernel.",
      "html_url": "https://github.com/FreeRTOS/FreeRTOS",
      "stars": 6862,
      "forks": 1925,
      "language": "C",
      "topics": [],
      "created_at": "2019-09-03T16:25:27Z",
      "updated_at": "2026-01-15T00:55:19Z",
      "pushed_at": "2026-01-01T00:25:56Z",
      "open_issues": 25,
      "owner": {
        "login": "FreeRTOS",
        "avatar_url": "https://avatars.githubusercontent.com/u/54647343?v=4"
      },
      "readme": "The [FreeRTOS 202411.00](https://github.com/FreeRTOS/FreeRTOS/tree/202411.00) release updates FreeRTOS Kernel, FreeRTOS+TCP, coreMQTT, corePKCS11, coreHTTP, coreJSON, AWS IoT Over-the-air-Updates (OTA), AWS IoT Device Shadow, AWS IoT Jobs, AWS IoT Device Defender, Backoff Algorithm, AWS IoT Fleet Provisioning, coreSNTP, SigV4, and FreeRTOS Cellular Interface libraries to their [202406-LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/blob/202406-LTS/CHANGELOG.md) versions. It also updates coreMQTT Agent to v1.3.0 and MbedTLS to v3.5.1. This release also adds ARMv7-R No_GIC Port Demo, ARMv7-R MPU Port Demos and FreeRTOS_Plus_TCP_IPv6_Demo Windows Simulator Demo. Additionally, all WinSim Demos are updated to use TLSv1.3. This release also updates WolfSSL to version v5.6.4.\n\nThe [FreeRTOS 202212.00](https://github.com/FreeRTOS/FreeRTOS/tree/202212.00) release updates FreeRTOS Kernel, FreeRTOS+TCP, coreMQTT, corePKCS11, coreHTTP, coreJSON, AWS IoT Over-the-air-Updates (OTA), AWS IoT Device Shadow, AWS IoT Jobs, AWS IoT Device Defender, Backoff Algorithm, AWS IoT Fleet Provisioning, coreSNTP, SigV4, and FreeRTOS Cellular Interface libraries to their [LTS 2.0](https://github.com/FreeRTOS/FreeRTOS-LTS/blob/202210-LTS/CHANGELOG.md) versions. It also updates coreMQTT Agent to v1.2.0 to be compatible with coreMQTT v2.X.X, and updates MbedTLS to v3.2.1. This release also adds Visual Studio static library projects for the FreeRTOS Kernel, FreeRTOS+TCP, Logging, MbedTLS, coreHTTP, and corePKCS11. With the addition of the static library projects, all Visual Studio projects have been updated to use them. Additionally, all demos dependent on coreMQTT have been updated to work with coreMQTT v2.X.X.\n\n## Getting started\nThe [FreeRTOS.org](https://www.freertos.org) website contains a [FreeRTOS Kernel Quick Start Guide](https://www.freertos.org/Documentation/01-FreeRTOS-quick-start/01-Beginners-guide/02-Quick-start-guide), a [list of supported devices and compilers](https://www.freertos.org/RTOS_ports.html), the [API reference](https://www.freertos.org/Documentation/02-Kernel/04-API-references/01-Task-creation/00-TaskHandle), and many other resources.\n\n### Getting help\nYou can use your Github login to get support from both the FreeRTOS community and directly from the primary FreeRTOS developers on our [active support forum](https://forums.freertos.org).  The [FAQ](https://www.freertos.org/Why-FreeRTOS/FAQs) provides another support resource.\n\n## Cloning this repository\nThis repo uses [Git Submodules](https://git-scm.com/book/en/v2/Git-Tools-Submodules) to bring in dependent components.\n\n**Note:** If you download the ZIP file provided by the GitHub UI, you will not get the contents of the submodules. (The ZIP file is also not a valid git repository)\n\nIf using Windows, because this repository and its submodules contain symbolic links, set `core.symlinks` to true with the following command:\n```\ngit config --global core.symlinks true\n```\nIn addition to this, either enable [Developer Mode](https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development) or, whenever using a git command that writes to the system (e.g. `git pull`, `git clone`, and `git submodule update --init --recursive`), use a console elevated as administrator so that git can properly create symbolic links for this repository. Otherwise, symbolic links will be written as normal files with the symbolic links' paths in them as text. [This](https://blogs.windows.com/windowsdeveloper/2016/12/02/symlinks-windows-10/) gives more explanation.\n\nTo clone using HTTPS:\n```\ngit clone https://github.com/FreeRTOS/FreeRTOS.git --recurse-submodules\n```\nUsing SSH:\n```\ngit clone git@github.com:FreeRTOS/FreeRTOS.git --recurse-submodules\n```\n\nIf you have downloaded the repo without using the `--recurse-submodules` argument, you need to run:\n```\ngit submodule update --init --recursive\n```\n\n## Repository structure\nThis repository contains the FreeRTOS Kernel, a number of supplementary libraries including the LTS ones, and a comprehensive set of example projects.  Many libraries (including the FreeRTOS kernel) are included as Git submodules from their own Git repositories.\n\n### Kernel source code and example projects\n```FreeRTOS/Source``` contains the FreeRTOS kernel source code (submoduled from https://github.com/FreeRTOS/FreeRTOS-Kernel).\n\n```FreeRTOS/Demo``` contains pre-configured example projects that demonstrate the FreeRTOS kernel executing on different hardware platforms and using different compilers.\n\n### Supplementary library source code and example projects\n```FreeRTOS-Plus/Source``` contains source code for additional FreeRTOS component libraries, as well as select partner provided libraries. These subdirectories contain further readme files and links to documentation.\n\n```FreeRTOS-Plus/Demo``` contains pre-configured example projects that demonstrate the FreeRTOS kernel used with the additional FreeRTOS component libraries.\n\n## Previous releases\n[Releases](https://github.com/FreeRTOS/FreeRTOS/releases) contains older FreeRTOS releases.\n\n\n## Learning FreeRTOS\n\nFor detailed and up-to-date information about FreeRTOS, including getting started guides and documentation for both new and experienced users, please refer to the official FreeRTOS website:\nhttps://www.freertos.org/\n\n## FreeRTOS Lab Projects\nFreeRTOS Lab projects are libraries and demos that are fully functional, but may be experimental or undergoing optimizations and refactorization to improve memory usage, modularity, documentation, demo usability, or test coverage.\n\nMost FreeRTOS Lab libraries can be found in the [FreeRTOS-Labs repository](https://github.com/FreeRTOS/FreeRTOS-Labs).\n\nA number of FreeRTOS Lab Demos can be found in the [FreeRTOS Github Organization](https://github.com/FreeRTOS) by searching for \"Lab\" or following [this link](https://github.com/FreeRTOS?q=Lab&type=&language=) to the search results.\n\n## coreMQTT Agent Demos\nThe [FreeRTOS/coreMQTT-Agent-Demos](https://github.com/FreeRTOS/coreMQTT-Agent-Demos) repository contains demos to showcase use of the [coreMQTT-Agent](https://github.com/FreeRTOS/coreMQTT-Agent) library to share an MQTT connection between multiple application tasks.\n\nThe demos show a single MQTT connection usage between multiple application tasks for interacting with AWS services (including [Over-the-air-Updates](https://docs.aws.amazon.com/freertos/latest/userguide/freertos-ota-dev.html), [Device Shadow](https://docs.aws.amazon.com/iot/latest/developerguide/iot-device-shadows.html),\n [Device Defender](https://docs.aws.amazon.com/iot/latest/developerguide/device-defender.html)) alongside performing simple Publish-Subscribe operations.\n## CBMC\n\nThe `FreeRTOS/Test/CBMC/proofs` directory contains CBMC proofs.\n\nTo learn more about CBMC and proofs specifically, review the training material [here](https://model-checking.github.io/cbmc-training).\n\nIn order to run these proofs you will need to install CBMC and other tools by following the instructions [here](https://model-checking.github.io/cbmc-training/installation.html).\n",
      "stars_today": 7
    },
    {
      "id": 53370988,
      "name": "lighthouse",
      "full_name": "GoogleChrome/lighthouse",
      "description": "Automated auditing, performance metrics, and best practices for the web.",
      "html_url": "https://github.com/GoogleChrome/lighthouse",
      "stars": 29716,
      "forks": 9635,
      "language": "JavaScript",
      "topics": [
        "audit",
        "best-practices",
        "chrome-devtools",
        "developer-tools",
        "performance-analysis",
        "performance-metrics",
        "pwa",
        "web"
      ],
      "created_at": "2016-03-08T01:03:11Z",
      "updated_at": "2026-01-15T00:55:37Z",
      "pushed_at": "2026-01-12T23:55:18Z",
      "open_issues": 490,
      "owner": {
        "login": "GoogleChrome",
        "avatar_url": "https://avatars.githubusercontent.com/u/1778935?v=4"
      },
      "readme": "# Lighthouse  [![GitHub Actions Status Badge](https://github.com/GoogleChrome/lighthouse/workflows/CI/badge.svg)](https://github.com/GoogleChrome/lighthouse/actions/workflows/ci.yml) [![GitHub Actions Status Badge](https://github.com/GoogleChrome/lighthouse/workflows/unit/badge.svg)](https://github.com/GoogleChrome/lighthouse/actions/workflows/unit.yml) [![GitHub Actions Status Badge](https://github.com/GoogleChrome/lighthouse/workflows/smoke/badge.svg)](https://github.com/GoogleChrome/lighthouse/actions/workflows/smoke.yml) [![Coverage Status](https://codecov.io/gh/GoogleChrome/lighthouse/branch/main/graph/badge.svg)](https://codecov.io/gh/GoogleChrome/lighthouse) [![Build tracker for Lighthouse](https://img.shields.io/badge/buildtracker-ok-blue)](https://lh-build-tracker.herokuapp.com/) [![NPM lighthouse package](https://img.shields.io/npm/v/lighthouse.svg)](https://npmjs.org/package/lighthouse)\n\n> Lighthouse analyzes web apps and web pages, collecting modern performance metrics and insights on developer best practices.\n\n- Using Lighthouse\n  - [Using Lighthouse in Chrome DevTools](#using-lighthouse-in-chrome-devtools)\n  - [Using the Chrome extension](#using-the-chrome-extension)\n  - [Using the Node CLI](#using-the-node-cli)\n    * [CLI options](#cli-options)\n  - [Using the Node module](#using-the-node-module)\n  - [Viewing a report](#viewing-a-report)\n    * [Online Viewer](#online-viewer)\n  - [Docs & Recipes](#docs--recipes)\n  - [Developing Lighthouse](#develop)\n    * [Setup](#setup)\n    * [Run](#run)\n    * [Tests](#tests)\n    * [Docs](#docs)\n- Associated Products and Projects\n  - [Lighthouse Integrations in Web Perf services](#lighthouse-integrations-in-web-perf-services)\n  - [Lighthouse Integrations in non-Web Perf services](#lighthouse-integrations-in-non-web-perf-services)\n  - [Plugins](#plugins)\n  - [Related projects](#related-projects)\n- [FAQ](#faq)\n  * [How does Lighthouse work?](#how-does-lighthouse-work)\n  * [Can I configure the lighthouse run?](#can-i-configure-the-lighthouse-run)\n  * [How does Lighthouse use network throttling, and how can I make it better?](#how-does-lighthouse-use-network-throttling-and-how-can-i-make-it-better)\n  * [Are results sent to a remote server?](#are-results-sent-to-a-remote-server)\n  * [How do I get localized Lighthouse results?](#how-do-i-get-localized-lighthouse-results-via-the-cli)\n  * [How do I author custom audits to extend Lighthouse?](#how-do-i-author-custom-audits-to-extend-lighthouse)\n  * [How do I contribute?](#how-do-i-contribute)\n\n## Using Lighthouse in Chrome DevTools\n\nLighthouse is integrated directly into the Chrome DevTools, under the \"Lighthouse\" panel.\n\n**Installation**: install [Chrome](https://www.google.com/chrome/browser).\n\n**Run it**: open Chrome DevTools, select the Lighthouse panel, and hit \"Generate report\".\n\n<img width=\"550\" alt=\"Lighthouse integration in Chrome DevTools.\" src=\"https://user-images.githubusercontent.com/2766281/204185043-9c49abe5-baee-4b26-b5ce-ece410661213.png\">\n\n## Using the Chrome extension\n\nThe Chrome extension was available prior to Lighthouse being available in Chrome Developer Tools, and offers similar functionality.\n\n**Installation**: [install the extension](https://chrome.google.com/webstore/detail/lighthouse/blipmdconlkpinefehnmjammfjpmpbjk) from the Chrome Web Store.\n\n**Run it**: follow the [extension quick-start guide](https://developers.google.com/web/tools/lighthouse/#extension).\n\n## Using the Node CLI\n\nThe Node CLI provides the most flexibility in how Lighthouse runs can be configured and reported. Users who want more advanced usage, or want to run Lighthouse in an automated fashion should use the Node CLI.\n\n> [!NOTE]\n> Lighthouse requires Node 22 (LTS) or later.\n\n**Installation**:\n\n```sh\nnpm install -g lighthouse\n# or use yarn:\n# yarn global add lighthouse\n```\n\n**Run it**: `lighthouse https://airhorner.com/`\n\nBy default, Lighthouse writes the report to an HTML file. You can control the output format by passing flags.\n\n### CLI options\n\n<!-- To update the help output:\n  node cli --help | pbcopy\n-->\n\n```\n$ lighthouse --help\n\nlighthouse <url> <options>\n\nLogging:\n  --verbose  Displays verbose logging  [boolean] [default: false]\n  --quiet    Displays no progress, debug logs, or errors  [boolean] [default: false]\n\nConfiguration:\n  --save-assets                  Save the trace contents & devtools logs to disk  [boolean] [default: false]\n  --list-all-audits              Prints a list of all available audits and exits  [boolean] [default: false]\n  --list-trace-categories        Prints a list of all required trace categories and exits  [boolean] [default: false]\n  --additional-trace-categories  Additional categories to capture with the trace (comma-delimited).  [string]\n  --config-path                  The path to the config JSON.\n                                 An example config file: core/config/lr-desktop-config.js  [string]\n  --preset                       Use a built-in configuration.\n                                 WARNING: If the --config-path flag is provided, this preset will be ignored.  [string] [choices: \"perf\", \"experimental\", \"desktop\"]\n  --chrome-flags                 Custom flags to pass to Chrome (space-delimited). For a full list of flags, see https://bit.ly/chrome-flags\n                                 Additionally, use the CHROME_PATH environment variable to use a specific Chrome binary. Requires Chromium version 66.0 or later. If omitted, any detected Chrome Canary or Chrome stable will be used.  [string] [default: \"\"]\n  --port                         The port to use for the debugging protocol. Use 0 for a random port  [number] [default: 0]\n  --hostname                     The hostname to use for the debugging protocol.  [string] [default: \"localhost\"]\n  --form-factor                  Determines how performance metrics are scored and if mobile-only audits are skipped. For desktop, use --preset=desktop instead.  [string] [choices: \"mobile\", \"desktop\"]\n  --screenEmulation              Sets screen emulation parameters. See also --preset. Use --screenEmulation.disabled to disable. Otherwise set these 4 parameters individually: --screenEmulation.mobile --screenEmulation.width=360 --screenEmulation.height=640 --screenEmulation.deviceScaleFactor=2\n  --emulatedUserAgent            Sets useragent emulation  [string]\n  --max-wait-for-load            The timeout (in milliseconds) to wait before the page is considered done loading and the run should continue. WARNING: Very high values can lead to large traces and instability  [number]\n  --enable-error-reporting       Enables error reporting, overriding any saved preference. --no-enable-error-reporting will do the opposite. More: https://github.com/GoogleChrome/lighthouse/blob/main/docs/error-reporting.md  [boolean]\n  --gather-mode, -G              Collect artifacts from a connected browser and save to disk. (Artifacts folder path may optionally be provided). If audit-mode is not also enabled, the run will quit early.\n  --audit-mode, -A               Process saved artifacts from disk. (Artifacts folder path may be provided, otherwise defaults to ./latest-run/)\n  --only-audits                  Only run the specified audits  [array]\n  --only-categories              Only run the specified categories. Available categories: accessibility, best-practices, performance, seo  [array]\n  --skip-audits                  Run everything except these audits  [array]\n  --disable-full-page-screenshot Disables collection of the full page screenshot, which can be quite large  [boolean]\n\nOutput:\n  --output       Reporter for the results, supports multiple values. choices: \"json\", \"html\", \"csv\"  [array] [default: [\"html\"]]\n  --output-path  The file path to output the results. Use 'stdout' to write to stdout.\n                   If using JSON output, default is stdout.\n                   If using HTML or CSV output, default is a file in the working directory with a name based on the test URL and date.\n                   If using multiple outputs, --output-path is appended with the standard extension for each output type. \"reports/my-run\" -> \"reports/my-run.report.html\", \"reports/my-run.report.json\", etc.\n                   Example: --output-path=./lighthouse-results.html  [string]\n  --view         Open HTML report in your browser  [boolean] [default: false]\n\nOptions:\n  --version                            Show version number  [boolean]\n  --help                               Show help  [boolean]\n  --cli-flags-path                     The path to a JSON file that contains the desired CLI flags to apply. Flags specified at the command line will still override the file-based ones.\n  --locale                             The locale/language the report should be formatted in\n  --blocked-url-patterns               Block any network requests to the specified URL patterns  [array]\n  --disable-storage-reset              Disable clearing the browser cache and other storage APIs before a run  [boolean]\n  --throttling-method                  Controls throttling method  [string] [choices: \"devtools\", \"provided\", \"simulate\"]\n  --throttling\n  --throttling.rttMs                   Controls simulated network RTT (TCP layer)\n  --throttling.throughputKbps          Controls simulated network download throughput\n  --throttling.requestLatencyMs        Controls emulated network RTT (HTTP layer)\n  --throttling.downloadThroughputKbps  Controls emulated network download throughput\n  --throttling.uploadThroughputKbps    Controls emulated network upload throughput\n  --throttling.cpuSlowdownMultiplier   Controls simulated + emulated CPU throttling\n  --extra-headers                      Set extra HTTP Headers to pass with request\n  --precomputed-lantern-data-path      Path to the file where lantern simulation data should be read from, overwriting the lantern observed estimates for RTT and server latency.  [string]\n  --lantern-data-output-path           Path to the file where lantern simulation data should be written to, can be used in a future run with the `precomputed-lantern-data-path` flag.  [string]\n  --plugins                            Run the specified plugins  [array]\n  --channel  [string] [default: \"cli\"]\n  --chrome-ignore-default-flags  [boolean] [default: false]\n\nExamples:\n  lighthouse <url> --view                                                                          Opens the HTML report in a browser after the run completes\n  lighthouse <url> --config-path=./myconfig.js                                                     Runs Lighthouse with your own configuration: custom audits, report generation, etc.\n  lighthouse <url> --output=json --output-path=./report.json --save-assets                         Save trace, screenshots, and named JSON report.\n  lighthouse <url> --screenEmulation.disabled --throttling-method=provided --no-emulatedUserAgent  Disable device emulation and all throttling\n  lighthouse <url> --chrome-flags=\"--window-size=412,660\"                                          Launch Chrome with a specific window size\n  lighthouse <url> --quiet --chrome-flags=\"--headless\"                                             Launch Headless Chrome, turn off logging\n  lighthouse <url> --extra-headers \"{\\\"Cookie\\\":\\\"monster=blue\\\", \\\"x-men\\\":\\\"wolverine\\\"}\"        Stringify'd JSON HTTP Header key/value pairs to send in requests\n  lighthouse <url> --extra-headers=./path/to/file.json                                             Path to JSON file of HTTP Header key/value pairs to send in requests\n  lighthouse <url> --only-categories=performance,seo                                               Only run the specified categories. Available categories: accessibility, best-practices, performance, seo\n\nFor more information on Lighthouse, see https://developers.google.com/web/tools/lighthouse/.\n```\n\n##### Output Examples\n\n```sh\nlighthouse\n# saves `./<HOST>_<DATE>.report.html`\n\nlighthouse --output json\n# json output sent to stdout\n\nlighthouse --output html --output-path ./report.html\n# saves `./report.html`\n\n# NOTE: specifying an output path with multiple formats ignores your specified extension for *ALL* formats\nlighthouse --output json --output html --output-path ./myfile.json\n# saves `./myfile.report.json` and `./myfile.report.html`\n\nlighthouse --output json --output html\n# saves `./<HOST>_<DATE>.report.json` and `./<HOST>_<DATE>.report.html`\n\nlighthouse --output-path=~/mydir/foo.out --save-assets\n# saves `~/mydir/foo.report.html`\n# saves `~/mydir/foo-0.trace.json` and `~/mydir/foo-0.devtoolslog.json`\n\nlighthouse --output-path=./report.json --output json\n# saves `./report.json`\n```\n\n##### Lifecycle Examples\nYou can run a subset of Lighthouse's lifecycle if desired via the `--gather-mode` (`-G`) and  `--audit-mode` (`-A`) CLI flags.\n\n```sh\nlighthouse http://example.com -G\n# launches browser, collects artifacts, saves them to disk (in `./latest-run/`) and quits\n\nlighthouse http://example.com -A\n# skips browser interaction, loads artifacts from disk (in `./latest-run/`), runs audits on them, generates report\n\nlighthouse http://example.com -GA\n# Normal gather + audit run, but also saves collected artifacts to disk for subsequent -A runs.\n\n\n# You can optionally provide a custom folder destination to -G/-A/-GA. Without a value, the default will be `$PWD/latest-run`.\nlighthouse -GA=./gmailartifacts https://gmail.com\n```\n\n\n#### Notes on Error Reporting\n\nThe first time you run the CLI you will be prompted with a message asking you if Lighthouse can anonymously report runtime exceptions. The Lighthouse team uses this information to detect new bugs and avoid regressions. Opting out will not affect your ability to use Lighthouse in any way. [Learn more](https://github.com/GoogleChrome/lighthouse/blob/main/docs/error-reporting.md).\n\n## Using the Node module\nYou can also use Lighthouse programmatically with the Node module.\n\nRead [Using Lighthouse programmatically](./docs/readme.md#using-programmatically) for help getting started.\\\nRead [Lighthouse Configuration](./docs/configuration.md) to learn more about the configuration options available.\n\n## Viewing a report\n\nLighthouse can produce a report as JSON or HTML.\n\nHTML report:\n\n<img src=\"https://raw.githubusercontent.com/GoogleChrome/lighthouse/443ff2c8a297dfd2297dfaca86c4966a87c8574a/assets/example_audit.png\" alt=\"Lighthouse example audit\" width=\"500px\">\n\n### Online Viewer\n\nRunning Lighthouse with the `--output=json` flag generates a JSON dump of the run.\nYou can view this report online by visiting <https://googlechrome.github.io/lighthouse/viewer/>\nand dragging the file onto the app. You can also use the \"Export\" button from the\ntop of any Lighthouse HTML report and open the report in the\n[Lighthouse Viewer](https://googlechrome.github.io/lighthouse/viewer/).\n\nIn the Viewer, reports can be shared by clicking the share icon in the top\nright corner and signing in to GitHub.\n\n> [!NOTE]\n>  shared reports are stashed as a secret Gist in GitHub, under your account.\n\n## Docs & Recipes\n\nUseful documentation, examples, and recipes to get you started.\n\n**Docs**\n\n- [Dealing with variance](./docs/variability.md)\n- [Using Lighthouse programmatically](./docs/readme.md#using-programmatically)\n- [Testing a site with authentication](./docs/authenticated-pages.md)\n- [Developing Plugins](./docs/plugins.md)\n- [Making a New Audit](./docs/new-audits.md)\n- [Testing on a mobile device](./docs/readme.md#testing-on-a-mobile-device)\n- [Lighthouse Architecture](./docs/architecture.md)\n\n**Recipes**\n\n- [Plugin](./docs/recipes/lighthouse-plugin-example) - example Lighthouse plugin\n- [Custom Audit example](./docs/recipes/custom-audit) - extend Lighthouse, run your own audits\n\n**Videos**\n\nThe session from Google I/O 2018 covers the new performance engine, upcoming Lighthouse REST API, and using the Chrome UX report to evaluate real-user data.\n\n[![Watch the Lighthouse @ Google I/O 2018 session.](https://img.youtube.com/vi/UvK9zAsSM8Q/0.jpg)](https://www.youtube.com/watch?v=UvK9zAsSM8Q)\n\nThe session from Google I/O 2017 covers architecture, writing custom audits,\nGitHub/Travis/CI integration, headless Chrome, and more:\n\n[![Watch the Lighthouse @ Google I/O 2017 session.](https://img.youtube.com/vi/NoRYn6gOtVo/0.jpg)](https://www.youtube.com/watch?v=NoRYn6gOtVo)\n\n_Click the image to watch the video on YouTube._\n\n## Develop\n\nRead on for the basics of hacking on Lighthouse. Also, see [Contributing](./CONTRIBUTING.md)\nfor detailed information.\n\n### Setup\n\n```sh\n# yarn should be installed first\n\ngit clone https://github.com/GoogleChrome/lighthouse\n\ncd lighthouse\nyarn\nyarn build-all\n```\n\n### Run\n\n```sh\nnode cli http://example.com\n# append --chrome-flags=\"--no-sandbox --headless --disable-gpu\" if you run into problems connecting to Chrome\n```\n\n> **Getting started tip**: `node --inspect-brk cli http://example.com` to open up Chrome DevTools and step\nthrough the entire app. See [Debugging Node.js with Chrome\nDevTools](https://medium.com/@paul_irish/debugging-node-js-nightlies-with-chrome-devtools-7c4a1b95ae27#.59rma3ukm)\nfor more info.\n\n### Tests\n\n```sh\n# lint and test all files\nyarn test\n\n# run all unit tests\nyarn unit\n\n# run a given unit test (e.g. core/test/audits/byte-efficiency/uses-long-cache-ttl-test.js)\nyarn mocha uses-long-cache-ttl\n\n# watch for file changes and run tests\n#   Requires http://entrproject.org : brew install entr\nyarn watch\n\n## run linting, unit, and smoke tests separately\nyarn lint\nyarn unit\nyarn smoke\n\n## run tsc compiler\nyarn type-check\n```\n\n### Docs\n\nSome of our docs have tests that run only in CI by default. To modify our documentation, you'll need to run `yarn build-pack && yarn test-docs` locally to make sure they pass.\n\n**Additional Dependencies**\n- `brew install jq`\n\n## Lighthouse Integrations in Web Perf services\n\nThis section details services that have integrated Lighthouse data. If you're working on a cool project integrating Lighthouse and would like to be featured here, file an issue to this repo or tweet at us [@_____lighthouse](https://twitter.com/____lighthouse)!\n\n* **[Web Page Test](https://www.webpagetest.org)** â€” An [open source](https://github.com/WPO-Foundation/webpagetest) tool for measuring and analyzing the performance of web pages on real devices. Users can choose to produce a Lighthouse report alongside the analysis of WebPageTest results.\n\n* **[HTTPArchive](http://httparchive.org/)** - HTTPArchive tracks how the web is built by crawling 500k pages with Web Page Test, including Lighthouse results, and stores the information in BigQuery where it is [publicly available](https://discuss.httparchive.org/t/quickstart-guide-to-exploring-the-http-archive/682).\n\n* **[Calibre](https://calibreapp.com)** - Calibre is a comprehensive performance monitoring platform running on Lighthouse. See the performance impact of your work before it hits production with GitHub Pull Request Reviews. Track the impact of Third Party scripts. Automate your performance system with a developer-first Node.js API. Try Calibre with a free 15-day trial.\n\n* **[DebugBear](https://www.debugbear.com/)** - DebugBear is a website monitoring tool based on Lighthouse. See how your scores and metrics changed over time, with a focus on understanding what caused each change. DebugBear is a paid product with a free 30-day trial.\n\n* **[Treo](https://treo.sh)** - Treo is Lighthouse as a Service. It provides regression testing, geographical regions, custom networks, and integrations with GitHub & Slack. Treo is a paid product with plans for solo-developers and teams.\n\n* **[PageVitals](https://pagevitals.com)** - PageVitals combines Lighthouse, CrUX and field testing to monitor the performance of websites. See how your website performs over time and get alerted if it gets too slow. Drill down and find the real cause of any performance issue. PageVitals is a paid product with a free 14-day trial.\n\n* **[Screpy](https://screpy.com)** - Screpy is a web analysis tool that can analyze all pages of your websites in one dashboard and monitor them with your team. It's powered by Lighthouse and it also includes some different analysis tools (SERP, W3C, Uptime, etc). Screpy has free and paid plans.\n\n* **[Siteimprove Performance](https://siteimprove.com/en/performance/)** â€” Siteimprove Performance is a web Performance monitoring solution that enables a marketer, manager or decision maker to understand and optimize website load times. Get easy-to-use insights with a focus on quick and impactful wins. Siteimprove Performance is a paid product with a free 14-day trial.\n\n* **[SpeedCurve](https://speedcurve.com)** â€” SpeedCurve is a tool for continuously monitoring web performance across different browsers, devices, and regions. It can aggregate any metric including Lighthouse scores across multiple pages and sites, and allows you to set performance budgets with Slack or email alerts. SpeedCurve is a paid product with a free 30-day trial.\n\n* **[Foo](https://www.foo.software/lighthouse)** - Lighthouse-as-a-service offering free and premium plans. Provides monitoring and historical reporting of Lighthouse audits with CircleCI, GitHub, and other integrations. Features include Slack notifications, PR comment reporting and more.\n\n* **[Apdex](https://apdex.co)** - Apdex is a website performance service. The main features are historical Lighthouse report visualizations, mobile/desktop options, alerts, uptime monitoring, and more. There are flexible paid plans and a 30-day free trial.\n\n* **[Websu](https://websu.io)** - Websu is an open source project to provide Lighthouse-as-a-Service through a simple HTTP REST API. The main features are ability to host and deploy in your own environment and historical Lighthouse report summaries.\n\n* **[DTEKT.IO](https://dtekt.io)** - DTEKT is a website performance and uptime monitoring service. It uses lighthouse to provide visibility into the performance of websites from multiple locations on multiple devices. It offers three months free trial and paid plans.\n\n* **[SpeedVitals](https://speedvitals.com)** - SpeedVitals is a Lighthouse powered tool to measure web performance across multiple devices and locations. It has various features like Layout Shift Visualization, Waterfall Chart, Field Data and Resource Graphs. SpeedVitals offers both free and paid plans.\n\n* **[Lighthouse Metrics](https://lighthouse-metrics.com/)** - Lighthouse Metrics gives you global performance insights with a single test. You can also monitor your websites on a daily or hourly base. Lighthouse Metrics offers free global one-time tests and performance monitoring as a paid feature with a free 14-day trial.\n\n* **[Auditzy](https://auditzy.com)** - Auditzyâ„¢ is a robust website auditing & monitoring tool which lets you analyze your web page(s) pre-user journey. Analyze the Competitor Health Metric, Core Web Vitals, and Technology. Compare your web pages with your competitors to understand where you are leading or lagging. Real-time notification with Slack. Have Seamless Collaboration with Multiple Teams. Automate your Audits hourly, daily, weekly, and so on. It has a free trial with pay as you go plans.\n\n* **[Lighthouse Metrics China](http://lighthousemetricschina.com)** - The first Lighthouse metrics tool specifically designed for China. Experience unparalleled website monitoring capabilities with Lighthouse. Gain insights into the fluctuations of your scores and metrics within the realm of the [Great Firewall of China](https://www.chinafirewalltest.co), enabling a comprehensive understanding of the factors influencing each change. Lighthouse Metrics China offers both free and paid plans.\n\n* **[DeploymentHawk](https://deploymenthawk.com)** - DeploymentHawk is an automated site auditing tool powered by Lighthouse. Effortlessly catch performance, accessibility, and SEO issues before they impact your users. DeploymentHawk is a paid product with a free 7-day trial.\n\n* **[Guardius](https://guardius.io)** - Guardius is a DevOps and DevSecOps SaaS platform that integrates Lighthouse to deliver automated web performance analysis. It not only provides metrics evaluation and automatic scanning but also enables performance comparisons across different periods and ongoing observation over time. Additionally, Guardius offers predefined and customized alerts tailored to your specific requirements. A free version of Guardius is available for users to explore its features.\n\n* **[SonÄ](https://getsona.io)** - Powered by Lighthouse amongst others, SonÄ delivers in-depth insights into your websiteâ€™s health. Track changes over time, share reports, and receive actionable recommendations to improve performance, accessibility, SEO, best practices, and security. SonÄ is free during its beta period.\n\n* **[FERU](https://feru.app)** - Run Google Lighthouse speed tests from multiple regions worldwide. Lighthouse scores, Core Web Vitals, and mobile performance metrics to easily test your site's speed, accessibility, and SEO. FERU offers an always-free plan alongside premium features for advanced analysis and monitoring.\n\n* **[LightKeeper](https://www.lightkeeper.cloud)** - Lighthouse testing service with free HAR Matrix view and multi-region testing (3 free regions, 25+ paid), supporting authenticated pages and cross-region performance comparison\n\n## Lighthouse Integrations in non-Web Perf services\n\n* **[PageWatch](https://pagewatch.dev/)** â€” PageWatch is a tool to find problem pages on your website.  It provides insights into spelling errors, layout issues, slow pages (powered by Lighthouse) and more.  PageWatch is offered via free and paid plans.\n\n* **[Fluxguard](https://fluxguard.com/)** - Fluxguard provides website DOM change monitoring orchestrated with Google Puppeteer, and audited by Lighthouse. Fluxguard is a freemium product, with monthly monitoring of up to 75 pages for free.\n\n* **[Microlink](https://microlink.io)** â€” Microlink is a cloud browser as API. It offers Lighthouse reports on demand, making it easy to build any service on top. Similar functionality is available via the underlying open-source project named browserless.\n\n* **[Wattspeed](https://wattspeed.com/)** â€” Wattspeed is a free tool that generates snapshots - historical captures of your web pages that include Lighthouse scores, a list of technologies, W3C HTML validator results, DOM size, mixed content info, and more.\n\n## Plugins\n\n* **[lighthouse-plugin-field-performance](https://github.com/treosh/lighthouse-plugin-field-performance)** - a plugin that adds real-user performance metrics for the URL using the data from [Chrome UX Report](https://developers.google.com/web/tools/chrome-user-experience-report/).\n\n* **[lighthouse-plugin-publisher-ads](https://github.com/googleads/publisher-ads-lighthouse-plugin)** - a tool to improve ad speed and overall quality through a series of automated audits. At the moment, this is primarily targeted at sites using Google Ad Manager. This tool will aid in resolving discovered problems, providing a tool to be used to evaluate effectiveness of iterative changes while suggesting actionable feedback.\n\n* **[lighthouse-plugin-crux](https://github.com/dvelasquez/lighthouse-plugin-crux)** - a plugin that quickly gathers real-user-metrics data from the [Chrome UX Report API](https://developers.google.com/web/tools/chrome-user-experience-report/api/reference).\n\n## Related projects\n\nOther awesome open source projects that use Lighthouse.\n\n* **[auto-lighthouse](https://github.com/TGiles/auto-lighthouse)** - a CLI for crawling a domain and generating mobile and desktop reports for each page.\n* **[Exthouse](https://github.com/treosh/exthouse)** - Analyze the impact of a browser extension on web performance.\n* **[Gimbal](https://labs.moduscreate.com/gimbal-web-performance-audit-budgeting)** - An [open source (MIT licensed)](https://github.com/ModusCreateOrg/gimbal) tool used to measure, analyze, and budget aspects of a web application. Gimbal also integrates reports with GitHub pull requests.\n* **[Gradle Lighthouse Plugin](https://github.com/Cognifide/gradle-lighthouse-plugin)** - An open source Gradle plugin that runs Lighthouse tests on multiple URLs and asserts category score thresholds (useful in continuous integration).\n* **[lighthouse-badges](https://github.com/emazzotta/lighthouse-badges)** - Generate gh-badges (shields.io) based on Lighthouse performance.\n* **[lighthouse-batch](https://github.com/mikestead/lighthouse-batch)** - Run Lighthouse over a number of sites and generate a summary of their metrics/scores.\n* **[lighthouse-batch-parallel](https://github.com/Carr1005/lighthouse-batch-parallel)** - Run multiple Lighthouse runs in parallel to accelerate the data collecting process, get the result stream (csv, json, js object) in your own process (warning: performance results may be volatile).\n* **[lighthouse-check-action](https://github.com/foo-software/lighthouse-check-action)** - A GitHub Action to run Lighthouse in a workflow, featuring Slack notifications and report upload to S3.\n* **[lighthouse-check-orb](https://circleci.com/orbs/registry/orb/foo-software/lighthouse-check)** - A CircleCI Orb to run Lighthouse in a workflow, featuring Slack notifications and report upload to S3.\n* **[andreasonny83/lighthouse-ci](https://github.com/andreasonny83/lighthouse-ci)** - Run Lighthouse and assert scores satisfy your custom thresholds.\n* **[GoogleChrome/lighthouse-ci](https://github.com/GoogleChrome/lighthouse-ci)** - (**official**) Automate running Lighthouse for every commit, viewing the changes, and preventing regressions.\n* **[lighthouse-ci-action](https://github.com/treosh/lighthouse-ci-action)** - A GitHub Action that makes it easy to run Lighthouse in CI and keep your pages small using performance budgets.\n* **[lighthouse-gh-reporter](https://github.com/carlesnunez/lighthouse-gh-reporter)** - Run Lighthouse in CI and report back in a comment on your pull requests\n* **[lighthouse-jest-example](https://github.com/justinribeiro/lighthouse-jest-example)** - Gather performance metrics via Lighthouse and assert results with Jest; uses Puppeteer to start Chrome with network emulation settings defined by WebPageTest.\n* **[lighthouse-lambda](https://github.com/Otterseer/lighthouse-lambda)** - Run Lighthouse on AWS Lambda with prebuilt stable desktop Headless Chrome.\n* **[lighthouse-matchers](https://github.com/ackama/lighthouse-matchers)** - Provides RSpec matchers for executing and evaluating Google Chrome Lighthouse audit scores.\n* **[lighthouse-mocha-example](https://github.com/rishichawda/lighthouse-mocha-example)** - Run Lighthouse performance tests with Mocha and chrome-launcher.\n* **[lighthouse-monitor](https://github.com/verivox/lighthouse-monitor)** - Run Lighthouse against all your URLs. Send metrics to any backend you want, save all reports with automatic data retention, and compare any two results in a web UI.\n* **[lighthouse-persist](https://github.com/foo-software/lighthouse-persist)** - Run Lighthouse and upload HTML reports to an AWS S3 bucket.\n* **[lighthouse-viewer](https://github.com/dvelasquez/lighthouse-viewer/tree/main/packages/lighthouse-viewer)** - Render the Lighthouse JSON into a report, using the Lighthouse Report Renderer repackaged as UMD and ESM. Also available with React, Svelte and Vue wrappers.\n* **[lighthouse4u](https://github.com/godaddy/lighthouse4u)** - LH4U provides Google Lighthouse as a service, surfaced by both a friendly UI+API, and backed by Elastic Search for easy querying and visualization.\n* **[react-lighthouse-viewer](https://www.npmjs.com/package/react-lighthouse-viewer)** - Render a Lighthouse JSON report in a React Component.\n* **[site-audit-seo](https://github.com/viasite/site-audit-seo)** - CLI tool for SEO site audit, crawl site, lighthouse each page. Output to console and tables in csv, xlsx, json, web or Google Drive.\n* **[webpack-lighthouse-plugin](https://github.com/addyosmani/webpack-lighthouse-plugin)** - Run Lighthouse from a Webpack build.\n* **[cypress-audit](https://github.com/mfrachet/cypress-audit)** - Run Lighthouse and Pa11y audits directly in your E2E test suites.\n* **[laravel-lighthouse](https://github.com/adityadees/laravel-lighthouse)** - Google Lighthouse wrapper for laravel framework to run Google Lighthouse CLI with custom option and can automatically save result in your server directory.\n* **[Neodymium](https://github.com/Xceptance/neodymium/wiki/Accessibility)** - The Neodymium test automation framework integrates Lighthouse for accessibility and Web Vitals verification, allowing programmatic validation and assertion of all audit values.\n\n## FAQ\n\n### How does Lighthouse work?\n\nSee [Lighthouse Architecture](./docs/architecture.md).\n\n### Why is the performance score so low? It looks fine to me.\n\nLighthouse reports the performance metrics as they would be experienced by a typical mobile user on a 4G connection and a mid-tier ~$200 phone. Even if it loads quickly on your device and network, users in other environments will experience the site very differently.\n\nRead more in our [guide to throttling](./docs/throttling.md).\n\n### Why does the performance score change so much?\n\nLighthouse performance scores will change due to inherent variability in web and network technologies, even if there hasn't been a code change. Test in consistent environments, run Lighthouse multiple times, and beware of variability before drawing conclusions about a performance-impacting change.\n\nRead more in our [guide to reducing variability](./docs/variability.md).\n\n### Can I configure the lighthouse run?\n\nYes! Details in [Lighthouse configuration](./docs/configuration.md).\n\n### How does Lighthouse use network throttling, and how can I make it better?\n\nGood question. Network and CPU throttling are applied by default in a Lighthouse run. The network\nattempts to emulate slow 4G connectivity and the CPU is slowed down 4x from your machine's default speed. If you\nprefer to run Lighthouse without throttling, you'll have to use the CLI and disable it with the\n`--throttling.*` flags mentioned above.\n\nRead more in our [guide to network throttling](./docs/throttling.md).\n\n### Are results sent to a remote server?\n\nNope. Lighthouse runs locally, auditing a page using a local version of the Chrome browser installed on the\nmachine. Report results are never processed or beaconed to a remote server.\n\n### How do I get localized Lighthouse results via the CLI?\n\nStarting in Lighthouse 8.0, Lighthouse relies entirely on native `Intl` support and no longer uses an `Intl` polyfill. If you're using Node 14 or later, there should be no issue because Node is now [built with `full-icu` by default](https://nodejs.medium.com/node-js-12-to-lts-and-node-js-13-is-here-e28d6a4a2bd#9514).\n\nHowever, if you're using a `small-icu` Node build, you may see Lighthouse log messages about your locale not being available. To remedy this, you can manually install ICU data by using the [`full-icu`](https://www.npmjs.com/package/full-icu) module and the [`--icu-data-dir` node flag](https://nodejs.org/api/intl.html#intl_providing_icu_data_at_runtime) at launch.\n\n### How do I author custom audits to extend Lighthouse?\n\n> **Tip**: see [Lighthouse Architecture](./docs/architecture.md) for more information\non terminology and architecture.\n\nLighthouse can be extended to run custom audits and gatherers that you author.\nThis is great if you're already tracking performance metrics in your site and\nwant to surface those metrics within a Lighthouse report.\n\nIf you're interested in running your own custom audits, check out our\n[Custom Audit Example](./docs/recipes/custom-audit) over in recipes.\n\n### How do I contribute?\n\nWe'd love help writing audits, fixing bugs, and making the tool more useful!\nSee [Contributing](./CONTRIBUTING.md) to get started.\n\n---\n<p align=\"center\">\n  <img src=\"./assets/lighthouse-logo_512px.png\" alt=\"Lighthouse logo\" height=\"150\">\n  <br>\n  <b>Lighthouse</b>, ËˆlÄ«tËŒhous (n): a <s>tower or other structure</s> tool containing a beacon light\n  to warn or guide <s>ships at sea</s> developers.\n</p>\n",
      "stars_today": 6
    },
    {
      "id": 15337142,
      "name": "micropython",
      "full_name": "micropython/micropython",
      "description": "MicroPython - a lean and efficient Python implementation for microcontrollers and constrained systems",
      "html_url": "https://github.com/micropython/micropython",
      "stars": 21343,
      "forks": 8651,
      "language": "C",
      "topics": [
        "embedded",
        "microcontroller",
        "micropython",
        "python"
      ],
      "created_at": "2013-12-20T11:47:07Z",
      "updated_at": "2026-01-14T16:03:01Z",
      "pushed_at": "2026-01-14T11:59:52Z",
      "open_issues": 1831,
      "owner": {
        "login": "micropython",
        "avatar_url": "https://avatars.githubusercontent.com/u/6298560?v=4"
      },
      "readme": "[![Unix CI badge](https://github.com/micropython/micropython/actions/workflows/ports_unix.yml/badge.svg)](https://github.com/micropython/micropython/actions?query=branch%3Amaster+event%3Apush) [![STM32 CI badge](https://github.com/micropython/micropython/actions/workflows/ports_stm32.yml/badge.svg)](https://github.com/micropython/micropython/actions?query=branch%3Amaster+event%3Apush) [![Docs CI badge](https://github.com/micropython/micropython/actions/workflows/docs.yml/badge.svg)](https://docs.micropython.org/) [![codecov](https://codecov.io/gh/micropython/micropython/branch/master/graph/badge.svg?token=I92PfD05sD)](https://codecov.io/gh/micropython/micropython)\n\nThe MicroPython project\n=======================\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/micropython/micropython/master/logo/upython-with-micro.jpg\" alt=\"MicroPython Logo\"/>\n</p>\n\nThis is the MicroPython project, which aims to put an implementation\nof Python 3.x on microcontrollers and small embedded systems.\nYou can find the official website at [micropython.org](http://www.micropython.org).\n\nWARNING: this project is in beta stage and is subject to changes of the\ncode-base, including project-wide name changes and API changes.\n\nMicroPython implements the entire Python 3.4 syntax (including exceptions,\n`with`, `yield from`, etc., and additionally `async`/`await` keywords from\nPython 3.5 and some select features from later versions). The following core\ndatatypes are provided: `str`(including basic Unicode support), `bytes`,\n`bytearray`, `tuple`, `list`, `dict`, `set`, `frozenset`, `array.array`,\n`collections.namedtuple`, classes and instances. Builtin modules include\n`os`, `sys`, `time`, `re`, and `struct`, etc. Some ports have support for\n`_thread` module (multithreading), `socket` and `ssl` for networking, and\n`asyncio`. Note that only a subset of Python 3 functionality is implemented\nfor the data types and modules.\n\nMicroPython can execute scripts in textual source form (.py files) or from\nprecompiled bytecode (.mpy files), in both cases either from an on-device\nfilesystem or \"frozen\" into the MicroPython executable.\n\nMicroPython also provides a set of MicroPython-specific modules to access\nhardware-specific functionality and peripherals such as GPIO, Timers, ADC,\nDAC, PWM, SPI, I2C, CAN, Bluetooth, and USB.\n\nGetting started\n---------------\n\nSee the [online documentation](https://docs.micropython.org/) for the API\nreference and information about using MicroPython and information about how\nit is implemented.\n\nWe use [GitHub Discussions](https://github.com/micropython/micropython/discussions)\nas our forum, and [Discord](https://discord.gg/RB8HZSAExQ) for chat. These\nare great places to ask questions and advice from the community or to discuss your\nMicroPython-based projects.\n\nFor bugs and feature requests, please [raise an issue](https://github.com/micropython/micropython/issues/new/choose)\nand follow the templates there.\n\nFor information about the [MicroPython pyboard](https://store.micropython.org/pyb-features),\nthe officially supported board from the\n[original Kickstarter campaign](https://www.kickstarter.com/projects/214379695/micro-python-python-for-microcontrollers),\nsee the [schematics and pinouts](http://github.com/micropython/pyboard) and\n[documentation](https://docs.micropython.org/en/latest/pyboard/quickref.html).\n\nContributing\n------------\n\nMicroPython is an open-source project and welcomes contributions. To be\nproductive, please be sure to follow the\n[Contributors' Guidelines](https://github.com/micropython/micropython/wiki/ContributorGuidelines)\nand the [Code Conventions](https://github.com/micropython/micropython/blob/master/CODECONVENTIONS.md).\nNote that MicroPython is licenced under the MIT license, and all contributions\nshould follow this license.\n\nAbout this repository\n---------------------\n\nThis repository contains the following components:\n- [py/](py/) -- the core Python implementation, including compiler, runtime, and\n  core library.\n- [mpy-cross/](mpy-cross/) -- the MicroPython cross-compiler which is used to turn scripts\n  into precompiled bytecode.\n- [ports/](ports/) -- platform-specific code for the various ports and architectures that MicroPython runs on.\n- [lib/](lib/) -- submodules for external dependencies.\n- [tests/](tests/) -- test framework and test scripts.\n- [docs/](docs/) -- user documentation in Sphinx reStructuredText format. This is used to generate the [online documentation](http://docs.micropython.org).\n- [extmod/](extmod/) -- additional (non-core) modules implemented in C.\n- [tools/](tools/) -- various tools, including the pyboard.py module.\n- [examples/](examples/) -- a few example Python scripts.\n\n\"make\" is used to build the components, or \"gmake\" on BSD-based systems.\nYou will also need bash, gcc, and Python 3.3+ available as the command `python3`.\nSome ports (rp2 and esp32) additionally use CMake.\n\nSupported platforms & architectures\n-----------------------------------\n\nMicroPython runs on a wide range of microcontrollers, as well as on Unix-like\n(including Linux, BSD, macOS, WSL) and Windows systems.\n\nMicrocontroller targets can be as small as 256kiB flash + 16kiB RAM, although\ndevices with at least 512kiB flash + 128kiB RAM allow a much more\nfull-featured experience.\n\nThe [Unix](ports/unix) and [Windows](ports/windows) ports allow both\ndevelopment and testing of MicroPython itself, as well as providing\nlightweight alternative to CPython on these platforms (in particular on\nembedded Linux systems).\n\nOver twenty different MicroPython ports are provided in this repository,\nsplit across three\n[MicroPython Support Tiers](https://docs.micropython.org/en/latest/develop/support_tiers.html).\n\nTier 1 Ports\n============\n\nğŸ‘‘ Ports in [Tier 1](https://docs.micropython.org/en/latest/develop/support_tiers.html)\nare mature and have the most active development, support and testing:\n\n| Port                     | Target                                                                                 | Quick Reference                                                      |\n|--------------------------|----------------------------------------------------------------------------------------|----------------------------------------------------------------------|\n| [esp32](ports/esp32)*    | Espressif ESP32 SoCs (ESP32, ESP32S2, ESP32S3, ESP32C3, ESP32C6)                       | [here](https://docs.micropython.org/en/latest/esp32/quickref.html)   |\n| [mimxrt](ports/mimxrt)   | NXP m.iMX RT                                                                           | [here](https://docs.micropython.org/en/latest/mimxrt/quickref.html)  |\n| [rp2](ports/rp2)         | Raspberry Pi RP2040 and RP2350                                                         | [here](https://docs.micropython.org/en/latest/rp2/quickref.html)     |\n| [samd](ports/samd)       | Microchip (formerly Atmel) SAMD21 and SAMD51                                           | [here](https://docs.micropython.org/en/latest/samd/quickref.html)    |\n| [stm32](ports/stm32)     | STMicroelectronics STM32 MCUs (F0, F4, F7, G0, G4, H5, H7, L0, L1, L4, N6, WB, WL)     | [here](https://docs.micropython.org/en/latest/pyboard/quickref.html) |\n| [unix](ports/unix)       | Linux, BSD, macOS, WSL                                                                 | [here](https://docs.micropython.org/en/latest/unix/quickref.html)    |\n| [windows](ports/windows) | Microsoft Windows                                                                      | [here](https://docs.micropython.org/en/latest/unix/quickref.html)    |\n\nAn asterisk indicates that the port has ongoing financial support from the vendor.\n\nTier 2 Ports\n============\n\nâœ” Ports in [Tier 2](https://docs.micropython.org/en/latest/develop/support_tiers.html)\nare less mature and less actively developed and tested than Tier 1, but\nstill fully supported:\n\n| Port                             | Target                                                      | Quick Reference                                                         |\n|----------------------------------|-------------------------------------------------------------|-------------------------------------------------------------------------|\n| [alif](ports/alif)               | Alif Semiconductor Ensemble MCUs (E3, E7)                   |                                                                         |\n| [embed](ports/embed)             | Generates a set of .c/.h files for embedding into a project |                                                                         |\n| [nrf](ports/nrf)                 | Nordic Semiconductor nRF51 and nRF52                        |                                                                         |\n| [renesas-ra](ports/renesas-ra)   | Renesas RA family                                           | [here](https://docs.micropython.org/en/latest/renesas-ra/quickref.html) |\n| [webassembly](ports/webassembly) | Emscripten port targeting browsers and NodeJS               |                                                                         |\n| [zephyr](ports/zephyr)           | Zephyr RTOS                                                 | [here](https://docs.micropython.org/en/latest/zephyr/quickref.html)     |\n\nTier 3 Ports\n============\n\nPorts in [Tier 3](https://docs.micropython.org/en/latest/develop/support_tiers.html)\nare built in CI but not regularly tested by the MicroPython maintainers:\n\n| Port                       | Target                                                            | Quick Reference                                                         |\n|----------------------------|-------------------------------------------------------------------|-------------------------------------------------------------------------|\n| [cc3200](ports/cc3200)     | Texas Instruments CC3200                                          | [For WiPy](https://docs.micropython.org/en/latest/wipy/quickref.html)   |\n| [esp8266](ports/esp8266)   | Espressif ESP8266 SoC                                             | [here](https://docs.micropython.org/en/latest/esp8266/quickref.html)    |\n| [pic16bit](ports/pic16bit) | Microchip PIC 16-bit                                              |                                                                         |\n| [powerpc](ports/powerpc)   | IBM PowerPC (including Microwatt)                                 |                                                                         |\n\nAdditional Ports\n================\n\nIn addition to the above there is a Tier M containing ports that are used\nprimarily for maintenance, development and testing:\n\n- The [\"bare-arm\"](ports/bare-arm) port is an example of the absolute minimum\n  configuration that still includes the compiler, and is used to keep track\n  of the code size of the core runtime and VM.\n\n- The [\"minimal\"](ports/minimal) port provides an example of a very basic\n  MicroPython port and can be compiled as both a standalone Linux binary as\n  well as for ARM Cortex-M4. Start with this if you want to port MicroPython\n  to another microcontroller.\n\n- The [qemu](ports/qemu) port is a QEMU-based emulated target for Cortex-A,\n  Cortex-M, RISC-V 32-bit and RISC-V 64-bit architectures.\n\nThe MicroPython cross-compiler, mpy-cross\n-----------------------------------------\n\nMost ports require the [MicroPython cross-compiler](mpy-cross) to be built\nfirst.  This program, called mpy-cross, is used to pre-compile Python scripts\nto .mpy files which can then be included (frozen) into the\nfirmware/executable for a port.  To build mpy-cross use:\n\n    $ cd mpy-cross\n    $ make\n\nExternal dependencies\n---------------------\n\nThe core MicroPython VM and runtime has no external dependencies, but a given\nport might depend on third-party drivers or vendor HALs. This repository\nincludes [several submodules](lib/) linking to these external dependencies.\nBefore compiling a given port, use\n\n    $ cd ports/name\n    $ make submodules\n\nto ensure that all required submodules are initialised.\n",
      "stars_today": 6
    },
    {
      "id": 26337322,
      "name": "rancher",
      "full_name": "rancher/rancher",
      "description": "Complete container management platform",
      "html_url": "https://github.com/rancher/rancher",
      "stars": 25140,
      "forks": 3142,
      "language": "Go",
      "topics": [
        "cattle",
        "containers",
        "docker",
        "kubernetes",
        "orchestration",
        "rancher"
      ],
      "created_at": "2014-11-07T20:49:31Z",
      "updated_at": "2026-01-15T00:59:01Z",
      "pushed_at": "2026-01-14T20:37:14Z",
      "open_issues": 3184,
      "owner": {
        "login": "rancher",
        "avatar_url": "https://avatars.githubusercontent.com/u/9343010?v=4"
      },
      "readme": "# Rancher\n\n[![Docker Pulls](https://img.shields.io/docker/pulls/rancher/rancher.svg)](https://store.docker.com/community/images/rancher/rancher)\n[![Go Report Card](https://goreportcard.com/badge/github.com/rancher/rancher)](https://goreportcard.com/report/github.com/rancher/rancher)\n\nRancher is an open source container management platform built for organizations that deploy containers in production. Rancher makes it easy to run Kubernetes everywhere, meet IT requirements, and empower DevOps teams.\n\n## Stable Release\n\n\n<!-- stable v2.13.1 DO NOT REMOVE THIS LINE -->\n* v2.13\n  * Stable - v2.13.1 - `rancher/rancher:v2.13.1` / `rancher/rancher:stable` - Read the full release [notes](https://github.com/rancher/rancher/releases/tag/v2.13.1).\n* v2.12\n  * Stable - v2.12.3 - `rancher/rancher:v2.12.3` - Read the full release [notes](https://github.com/rancher/rancher/releases/tag/v2.12.3).\n* v2.11\n  * Stable - v2.11.3 - `rancher/rancher:v2.11.3` - Read the full release [notes](https://github.com/rancher/rancher/releases/tag/v2.11.3).\n  \nTo get automated notifications of our latest release, you can watch the announcements category in our [forums](http://forums.rancher.com/c/announcements), or subscribe to the RSS feed `https://forums.rancher.com/c/announcements.rss`.\n\n## Quick Start\n\n    sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher\n\nOpen your browser to https://localhost\n\n## Installation\n\nSee [Installing/Upgrading Rancher](https://ranchermanager.docs.rancher.com/v2.8/pages-for-subheaders/installation-and-upgrade) for all installation options.\n\n### Minimum Requirements\n\n* Operating Systems\n  * Please see [Support Matrix](https://rancher.com/support-matrix/) for specific OS versions for each Rancher version. Note that the link will default to the support matrix for the latest version of Rancher. Use the left navigation menu to select a different Rancher version. \n* Hardware & Software\n  * Please see [Installation Requirements](https://ranchermanager.docs.rancher.com/v2.8/pages-for-subheaders/installation-requirements) for hardware and software requirements.\n\n### Using Rancher\n\nTo learn more about using Rancher, please refer to our [Rancher Documentation](https://ranchermanager.docs.rancher.com/v2.8).\n\n## Source Code\n\nThis repo is a meta-repo used for packaging and contains the majority of Rancher codebase. For other Rancher projects and modules, [see go.mod](https://github.com/rancher/rancher/blob/release/v2.8/go.mod) for the full list.\n\nRancher also includes other open source libraries and projects, [see go.mod](https://github.com/rancher/rancher/blob/release/v2.8/go.mod) for the full list.\n\n## Build configuration\n\nRefer to the [build docs](docs/build.md) on how to customize the building and packaging of Rancher.\n\n## Support, Discussion, and Community\nIf you need any help with Rancher, please join us at either our [Rancher forums](http://forums.rancher.com/) or [Slack](https://slack.rancher.io/) where most of our team hangs out at.\n\nPlease submit any Rancher bugs, issues, and feature requests to [rancher/rancher](https://github.com/rancher/rancher/issues).\n\nFor security issues, please first check our [security policy](https://github.com/rancher/rancher/security) and email security-rancher@suse.com instead of posting a public issue in GitHub.  You may (but are not required to) use the GPG key located on [Keybase](https://keybase.io/rancher).\n\n# License\n\nCopyright (c) 2014-2025 [SUSE](http://rancher.com)\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n[http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n",
      "stars_today": 6
    },
    {
      "id": 36071697,
      "name": "hydra",
      "full_name": "ory/hydra",
      "description": "Internet-scale OpenID Certifiedâ„¢ OpenID Connect and OAuth2.1 provider that integrates with your user management through headless APIs. Solve OIDC/OAuth2 user cases over night. Consume as a service on Ory Network or self-host. Trusted by OpenAI and many others for scale and security. Written in Go.",
      "html_url": "https://github.com/ory/hydra",
      "stars": 16849,
      "forks": 1605,
      "language": "Go",
      "topics": [
        "authorization",
        "cloud",
        "docker",
        "federation",
        "hacktoberfest",
        "hydra",
        "identity",
        "oauth",
        "oauth-provider",
        "oauth2",
        "oauth2-provider",
        "oauth2-server",
        "oidc",
        "openid",
        "openid-connect",
        "openid-connect-provider",
        "openid-provider",
        "security",
        "server",
        "sso"
      ],
      "created_at": "2015-05-22T12:42:08Z",
      "updated_at": "2026-01-14T20:02:12Z",
      "pushed_at": "2026-01-13T09:02:16Z",
      "open_issues": 113,
      "owner": {
        "login": "ory",
        "avatar_url": "https://avatars.githubusercontent.com/u/25334553?v=4"
      },
      "readme": "<h1 align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/ory/meta/master/static/banners/hydra.svg\" alt=\"Ory Hydra - Open Source OAuth 2 and OpenID Connect server\">\n</h1>\n\n<h4 align=\"center\">\n  <a href=\"https://www.ory.sh/chat\">Chat</a> Â·\n  <a href=\"https://github.com/ory/hydra/discussions\">Discussions</a> Â·\n  <a href=\"https://www.ory.sh/l/sign-up-newsletter\">Newsletter</a> Â·\n  <a href=\"https://www.ory.sh/docs/\">Docs</a> Â·\n  <a href=\"https://console.ory.sh/\">Try Ory Network</a> Â·\n  <a href=\"https://www.ory.sh/jobs/\">Jobs</a>\n</h4>\n\nOry Hydra is a hardened, OpenID Certified OAuth 2.0 Server and OpenID Connect\nProvider optimized for low-latency, high throughput, and low resource\nconsumption. It connects to your existing identity provider through a login and\nconsent app, giving you absolute control over the user interface and experience.\n\n---\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n- [What is Ory Hydra?](#what-is-ory-hydra)\n  - [Why Ory Hydra](#why-ory-hydra)\n  - [OAuth2 and OpenID Connect: Open Standards](#oauth2-and-openid-connect-open-standards)\n  - [OpenID Connect Certified](#openid-connect-certified)\n- [Deployment options](#deployment-options)\n  - [Use Ory Hydra on the Ory Network](#use-ory-hydra-on-the-ory-network)\n  - [Self-host Ory Hydra](#self-host-ory-hydra)\n- [Quickstart](#quickstart)\n- [Who is using Ory Hydra](#who-is-using-ory-hydra)\n- [Ecosystem](#ecosystem)\n  - [Ory Kratos: Identity and User Infrastructure and Management](#ory-kratos-identity-and-user-infrastructure-and-management)\n  - [Ory Hydra: OAuth2 & OpenID Connect Server](#ory-hydra-oauth2--openid-connect-server)\n  - [Ory Oathkeeper: Identity & Access Proxy](#ory-oathkeeper-identity--access-proxy)\n  - [Ory Keto: Access Control Policies as a Server](#ory-keto-access-control-policies-as-a-server)\n- [Documentation](#documentation)\n- [Developing Ory Hydra](#developing-ory-hydra)\n- [Security](#security)\n  - [Disclosing vulnerabilities](#disclosing-vulnerabilities)\n- [Telemetry](#telemetry)\n- [Libraries and third-party projects](#libraries-and-third-party-projects)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## What is Ory Hydra?\n\nOry Hydra is a server implementation of the OAuth 2.0 authorization framework\nand the OpenID Connect Core 1.0. It follows\n[cloud architecture best practices](https://www.ory.sh/docs/ecosystem/software-architecture-philosophy)\nand focuses on:\n\n- OAuth 2.0 and OpenID Connect flows\n- Token issuance and validation\n- Client management\n- Consent and login flow orchestration\n- JWKS management\n- Low latency and high throughput\n\nWe recommend starting with the\n[Ory Hydra introduction docs](https://www.ory.sh/docs/hydra) to learn more about\nits architecture, feature set, and how it compares to other systems.\n\n### Why Ory Hydra\n\nOry Hydra is designed to:\n\n- Be a standalone OAuth 2.0 and OpenID Connect server without user management\n- Connect to any existing identity provider through a login and consent app\n- Give you absolute control over the user interface and experience flows\n- Work with any authentication endpoint:\n  [Ory Kratos](https://github.com/ory/kratos),\n  [authboss](https://github.com/go-authboss/authboss),\n  [User Frosting](https://www.userfrosting.com/), or your proprietary system\n- Scale to large numbers of clients and tokens\n- Fit into modern cloud native environments such as Kubernetes and managed\n  platforms\n\n### OAuth2 and OpenID Connect: Open Standards\n\nOry Hydra implements Open Standards set by the IETF:\n\n- [The OAuth 2.0 Authorization Framework](https://tools.ietf.org/html/rfc6749)\n- [OAuth 2.0 Threat Model and Security Considerations](https://tools.ietf.org/html/rfc6819)\n- [OAuth 2.0 Token Revocation](https://tools.ietf.org/html/rfc7009)\n- [OAuth 2.0 Token Introspection](https://tools.ietf.org/html/rfc7662)\n- [OAuth 2.0 for Native Apps](https://tools.ietf.org/html/draft-ietf-oauth-native-apps-10)\n- [OAuth 2.0 Dynamic Client Registration Protocol](https://datatracker.ietf.org/doc/html/rfc7591)\n- [OAuth 2.0 Dynamic Client Registration Management Protocol](https://datatracker.ietf.org/doc/html/rfc7592)\n- [Proof Key for Code Exchange by OAuth Public Clients](https://tools.ietf.org/html/rfc7636)\n- [JSON Web Token (JWT) Profile for OAuth 2.0 Client Authentication and Authorization Grants](https://tools.ietf.org/html/rfc7523)\n\nand the OpenID Foundation:\n\n- [OpenID Connect Core 1.0](http://openid.net/specs/openid-connect-core-1_0.html)\n- [OpenID Connect Discovery 1.0](https://openid.net/specs/openid-connect-discovery-1_0.html)\n- [OpenID Connect Dynamic Client Registration 1.0](https://openid.net/specs/openid-connect-registration-1_0.html)\n- [OpenID Connect Front-Channel Logout 1.0](https://openid.net/specs/openid-connect-frontchannel-1_0.html)\n- [OpenID Connect Back-Channel Logout 1.0](https://openid.net/specs/openid-connect-backchannel-1_0.html)\n\n### OpenID Connect Certified\n\nOry Hydra is an OpenID Foundation\n[certified OpenID Provider (OP)](http://openid.net/certification/#OPs).\n\n<p align=\"center\">\n    <img src=\"https://github.com/ory/docs/blob/master/docs/hydra/images/oidc-cert.png\" alt=\"Ory Hydra is a certified OpenID Providier\" width=\"256px\">\n</p>\n\nThe following OpenID profiles are certified:\n\n- [Basic OpenID Provider](http://openid.net/specs/openid-connect-core-1_0.html#CodeFlowAuth)\n  (response types `code`)\n- [Implicit OpenID Provider](http://openid.net/specs/openid-connect-core-1_0.html#ImplicitFlowAuth)\n  (response types `id_token`, `id_token+token`)\n- [Hybrid OpenID Provider](http://openid.net/specs/openid-connect-core-1_0.html#HybridFlowAuth)\n  (response types `code+id_token`, `code+id_token+token`, `code+token`)\n- [OpenID Provider Publishing Configuration Information](https://openid.net/specs/openid-connect-discovery-1_0.html)\n- [Dynamic OpenID Provider](https://openid.net/specs/openid-connect-registration-1_0.html)\n\nTo obtain certification, we deployed the\n[reference user login and consent app](https://github.com/ory/hydra-login-consent-node)\n(unmodified) and Ory Hydra v1.0.0.\n\n## Deployment options\n\nYou can run Ory Hydra in two main ways:\n\n- As a managed service on the Ory Network\n- As a self hosted service under your own control, with or without the Ory\n  Enterprise License\n\n### Use Ory Hydra on the Ory Network\n\nThe [Ory Network](https://www.ory.sh/cloud) is the fastest way to use Ory\nservices in production. **Ory OAuth2 & OpenID Connect** is powered by the open\nsource Ory Hydra server and is API compatible.\n\nThe Ory Network provides:\n\n- OAuth2 and OpenID Connect for single sign on, API access, and machine to\n  machine authorization\n- Identity and credential management that scales to billions of users and\n  devices\n- Registration, login, and account management flows for passkeys, biometrics,\n  social login, SSO, and multi factor authentication\n- Prebuilt login, registration, and account management pages and components\n- Low latency permission checks based on the Zanzibar model with the Ory\n  Permission Language\n- GDPR friendly storage with data locality and compliance in mind\n- Web based Ory Console and Ory CLI for administration and operations\n- Cloud native APIs compatible with the open source servers\n- Fair, usage based [pricing](https://www.ory.sh/pricing)\n\nSign up for a\n[free developer account](https://console.ory.sh/registration?utm_source=github&utm_medium=banner&utm_campaign=hydra-readme)\nto get started.\n\n### Self-host Ory Hydra\n\nYou can run Ory Hydra yourself for full control over infrastructure, deployment,\nand customization.\n\nThe [install guide](https://www.ory.sh/docs/hydra/install) explains how to:\n\n- Install Hydra on Linux, macOS, Windows, and Docker\n- Configure databases such as PostgreSQL, MySQL, and CockroachDB\n- Deploy to Kubernetes and other orchestration systems\n- Build Hydra from source\n\nThis guide uses the open source distribution to get you started without license\nrequirements. It is a great fit for individuals, researchers, hackers, and\ncompanies that want to experiment, prototype, or run unimportant workloads\nwithout SLAs. You get the full core engine, and you are free to inspect, extend,\nand build it from source.\n\nIf you run Hydra as part of a business-critical system, for example OAuth2 and\nOpenID Connect for all your users, you should use a commercial agreement to\nreduce operational and security risk. The **Ory Enterprise License (OEL)**\nlayers on top of self-hosted Hydra and provides:\n\n- Additional enterprise features that are not available in the open source\n  version\n- Regular security releases, including CVE patches, with service level\n  agreements\n- Support for advanced scaling, multi-tenancy, and complex deployments\n- Premium support options with SLAs, direct access to engineers, and onboarding\n  help\n- Access to a private Docker registry with frequent and vetted, up-to-date\n  enterprise builds\n\nFor guaranteed CVE fixes, current enterprise builds, advanced features, and\nsupport in production, you need a valid\n[Ory Enterprise License](https://www.ory.com/ory-enterprise-license) and access\nto the Ory Enterprise Docker registry. To learn more,\n[contact the Ory team](https://www.ory.sh/contact/).\n\n## Quickstart\n\nInstall the [Ory CLI](https://www.ory.sh/docs/guides/cli/installation) and\ncreate a new project to try Ory OAuth2 & OpenID Connect.\n\n```bash\n# Install the Ory CLI if you do not have it yet:\nbash <(curl https://raw.githubusercontent.com/ory/meta/master/install.sh) -b . ory\nsudo mv ./ory /usr/local/bin/\n\n# Sign in or sign up\nory auth\n\n# Create a new project\nory create project --create-workspace \"Ory Open Source\" --name \"GitHub Quickstart\" --use-project\n```\n\nTry out the OAuth 2.0 Client Credentials flow:\n\n```bash\nory create oauth2-client \\\n    --name \"Client Credentials Demo\" \\\n    --grant-type client_credentials\n# Note the client ID and secret from output\n\nory perform client-credentials \\\n    --client-id <your-client-id> \\\n    --client-secret <your-client-secret>\n# Note the access token from output\n\nory introspect token <your-access-token>\n```\n\nTry out the OAuth 2.0 Authorize Code + OpenID Connect flow:\n\n```bash\nory create oauth2-client \\\n    --name \"Authorize Code with OpenID Connect Demo\" \\\n    --grant-type authorization_code,refresh_token \\\n    --response-type code \\\n    --redirect-uri http://127.0.0.1:4446/callback\n\nory perform authorization-code \\\n    --client-id <your-client-id> \\\n    --client-secret <your-client-secret>\n```\n\n## Who is using Ory Hydra\n\n<!--BEGIN ADOPTERS-->\n\nThe Ory community stands on the shoulders of individuals, companies, and\nmaintainers. The Ory team thanks everyone involved - from submitting bug reports\nand feature requests, to contributing patches and documentation. The Ory\ncommunity counts more than 50.000 members and is growing. The Ory stack protects\n7.000.000.000+ API requests every day across thousands of companies. None of\nthis would have been possible without each and everyone of you!\n\nThe following list represents companies that have accompanied us along the way\nand that have made outstanding contributions to our ecosystem. _If you think\nthat your company deserves a spot here, reach out to\n<a href=\"mailto:office@ory.sh\">office@ory.sh</a> now_!\n\n<table>\n    <thead>\n        <tr>\n            <th>Name</th>\n            <th>Logo</th>\n            <th>Website</th>\n            <th>Case Study</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>OpenAI</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/openai.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/openai.svg\" alt=\"OpenAI\">\n                </picture>\n            </td>\n            <td><a href=\"https://openai.com/\">openai.com</a></td>\n            <td><a href=\"https://www.ory.sh/case-studies/openai\">OpenAI Case Study</a></td>\n        </tr>\n        <tr>\n            <td>Fandom</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/fandom.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/fandom.svg\" alt=\"Fandom\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.fandom.com/\">fandom.com</a></td>\n            <td><a href=\"https://www.ory.sh/case-studies/fandom\">Fandom Case Study</a></td>\n        </tr>\n        <tr>\n            <td>Lumin</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/lumin.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/lumin.svg\" alt=\"Lumin\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.luminpdf.com/\">luminpdf.com</a></td>\n            <td><a href=\"https://www.ory.sh/case-studies/lumin\">Lumin Case Study</a></td>\n        </tr>\n        <tr>\n            <td>Sencrop</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/sencrop.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/sencrop.svg\" alt=\"Sencrop\">\n                </picture>\n            </td>\n            <td><a href=\"https://sencrop.com/\">sencrop.com</a></td>\n            <td><a href=\"https://www.ory.sh/case-studies/sencrop\">Sencrop Case Study</a></td>\n        </tr>\n        <tr>\n            <td>OSINT Industries</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/osint.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/osint.svg\" alt=\"OSINT Industries\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.osint.industries/\">osint.industries</a></td>\n            <td><a href=\"https://www.ory.sh/case-studies/osint\">OSINT Industries Case Study</a></td>\n        </tr>\n        <tr>\n            <td>HGV</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/hgv.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/hgv.svg\" alt=\"HGV\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.hgv.it/\">hgv.it</a></td>\n            <td><a href=\"https://www.ory.sh/case-studies/hgv\">HGV Case Study</a></td>\n        </tr>\n        <tr>\n            <td>Maxroll</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/maxroll.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/maxroll.svg\" alt=\"Maxroll\">\n                </picture>\n            </td>\n            <td><a href=\"https://maxroll.gg/\">maxroll.gg</a></td>\n            <td><a href=\"https://www.ory.sh/case-studies/maxroll\">Maxroll Case Study</a></td>\n        </tr>\n        <tr>\n            <td>Zezam</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/zezam.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/zezam.svg\" alt=\"Zezam\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.zezam.io/\">zezam.io</a></td>\n            <td><a href=\"https://www.ory.sh/case-studies/zezam\">Zezam Case Study</a></td>\n        </tr>\n        <tr>\n            <td>T.RowePrice</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/troweprice.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/troweprice.svg\" alt=\"T.RowePrice\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.troweprice.com/\">troweprice.com</a></td>\n        </tr>\n        <tr>\n            <td>Mistral</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/mistral.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/mistral.svg\" alt=\"Mistral\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.mistral.ai/\">mistral.ai</a></td>\n        </tr>\n        <tr>\n            <td>Axel Springer</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/axelspringer.svg\" />\n                    <img height=\"22px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/axelspringer.svg\" alt=\"Axel Springer\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.axelspringer.com/\">axelspringer.com</a></td>\n        </tr>\n        <tr>\n            <td>Hemnet</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/hemnet.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/hemnet.svg\" alt=\"Hemnet\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.hemnet.se/\">hemnet.se</a></td>\n        </tr>\n        <tr>\n            <td>Cisco</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/cisco.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/cisco.svg\" alt=\"Cisco\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.cisco.com/\">cisco.com</a></td>\n        </tr>\n        <tr>\n            <td>Presidencia de la RepÃºblica Dominicana</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/republica-dominicana.svg\" />\n                    <img height=\"42px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/republica-dominicana.svg\" alt=\"Presidencia de la RepÃºblica Dominicana\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.presidencia.gob.do/\">presidencia.gob.do</a></td>\n        </tr>\n        <tr>\n            <td>Moonpig</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/moonpig.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/moonpig.svg\" alt=\"Moonpig\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.moonpig.com/\">moonpig.com</a></td>\n        </tr>\n        <tr>\n            <td>Booster</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/booster.svg\" />\n                    <img height=\"18px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/booster.svg\" alt=\"Booster\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.choosebooster.com/\">choosebooster.com</a></td>\n        </tr>\n        <tr>\n            <td>Zaptec</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/zaptec.svg\" />\n                    <img height=\"24px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/zaptec.svg\" alt=\"Zaptec\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.zaptec.com/\">zaptec.com</a></td>\n        </tr>\n        <tr>\n            <td>Klarna</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/klarna.svg\" />\n                    <img height=\"24px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/klarna.svg\" alt=\"Klarna\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.klarna.com/\">klarna.com</a></td>\n        </tr>\n        <tr>\n            <td>Raspberry PI Foundation</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/raspi.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/raspi.svg\" alt=\"Raspberry PI Foundation\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.raspberrypi.org/\">raspberrypi.org</a></td>\n        </tr>\n        <tr>\n            <td>Tulip</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/tulip.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/tulip.svg\" alt=\"Tulip Retail\">\n                </picture>\n            </td>\n            <td><a href=\"https://tulip.com/\">tulip.com</a></td>\n        </tr>\n        <tr>\n            <td>Hootsuite</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/hootsuite.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/hootsuite.svg\" alt=\"Hootsuite\">\n                </picture>\n            </td>\n            <td><a href=\"https://hootsuite.com/\">hootsuite.com</a></td>\n        </tr>\n        <tr>\n            <td>Segment</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/segment.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/segment.svg\" alt=\"Segment\">\n                </picture>\n            </td>\n            <td><a href=\"https://segment.com/\">segment.com</a></td>\n        </tr>\n        <tr>\n            <td>Arduino</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/arduino.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/arduino.svg\" alt=\"Arduino\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.arduino.cc/\">arduino.cc</a></td>\n        </tr>\n        <tr>\n            <td>Sainsbury's</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/sainsburys.svg\" />\n                    <img height=\"24px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/sainsburys.svg\" alt=\"Sainsbury's\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.sainsburys.co.uk/\">sainsburys.co.uk</a></td>\n        </tr>\n        <tr>\n            <td>Contraste</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/contraste.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/contraste.svg\" alt=\"Contraste\">\n                </picture>\n            </td>\n            <td><a href=\"https://www.contraste.com/en\">contraste.com</a></td>\n        </tr>\n        <tr>\n            <td>inMusic</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/inmusic.svg\" />\n                    <img height=\"24px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/inmusic.svg\" alt=\"InMusic\">\n                </picture>\n            </td>\n            <td><a href=\"https://inmusicbrands.com/\">inmusicbrands.com</a></td>\n        </tr>\n        <tr>\n            <td>Buhta</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/buhta.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/buhta.svg\" alt=\"Buhta\">\n                </picture>\n            </td>\n            <td><a href=\"https://buhta.com/\">buhta.com</a></td>\n        </tr>\n        </tr>\n            <tr>\n            <td>Amplitude</td>\n            <td align=\"center\">\n                <picture>\n                    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/amplitude.svg\" />\n                    <img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/amplitude.svg\" alt=\"amplitude.com\">\n                </picture>\n            </td>\n            <td><a href=\"https://amplitude.com/\">amplitude.com</a></td>\n        </tr>\n    <tr>\n      <td align=\"center\"><a href=\"https://tier4.jp/en/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/tieriv.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/tieriv.svg\" alt=\"TIER IV\"></picture></a></td>\n      <td align=\"center\"><a href=\"https://kyma-project.io\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/kyma.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/kyma.svg\" alt=\"Kyma Project\"></picture></a></td>\n      <td align=\"center\"><a href=\"https://serlo.org/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/serlo.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/serlo.svg\" alt=\"Serlo\"></picture></a></td>\n      <td align=\"center\"><a href=\"https://padis.io/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/padis.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/padis.svg\" alt=\"Padis\"></picture></a></td>\n    </tr>\n    <tr>\n      <td align=\"center\"><a href=\"https://cloudbear.eu/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/cloudbear.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/cloudbear.svg\" alt=\"Cloudbear\"></picture></a></td>\n      <td align=\"center\"><a href=\"https://securityonionsolutions.com/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/securityonion.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/securityonion.svg\" alt=\"Security Onion Solutions\"></picture></a></td>\n      <td align=\"center\"><a href=\"https://factlylabs.com/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/factly.svg\" /><img height=\"24px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/factly.svg\" alt=\"Factly\"></picture></a></td>\n      <td align=\"center\"><a href=\"https://cashdeck.com.au/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/allmyfunds.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/allmyfunds.svg\" alt=\"All My Funds\"></picture></a></td>\n    </tr>\n    <tr>\n      <td align=\"center\"><a href=\"https://nortal.com/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/nortal.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/nortal.svg\" alt=\"Nortal\"></picture></a></td>\n      <td align=\"center\"><a href=\"https://www.ordermygear.com/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/ordermygear.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/ordermygear.svg\" alt=\"OrderMyGear\"></picture></a></td>\n      <td align=\"center\"><a href=\"https://r2devops.io/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/r2devops.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/r2devops.svg\" alt=\"R2Devops\"></picture></a></td>\n      <td align=\"center\"><a href=\"https://www.paralus.io/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/paralus.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/paralus.svg\" alt=\"Paralus\"></picture></a></td>\n    </tr>\n    <tr>\n      <td align=\"center\"><a href=\"https://dyrector.io/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/dyrector_io.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/dyrector_io.svg\" alt=\"dyrector.io\"></picture></a></td>\n      <td align=\"center\"><a href=\"https://pinniped.dev/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/pinniped.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/pinniped.svg\" alt=\"pinniped.dev\"></picture></a></td>\n      <td align=\"center\"><a href=\"https://pvotal.tech/\"><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/light/pvotal.svg\" /><img height=\"32px\" src=\"https://raw.githubusercontent.com/ory/meta/master/static/adopters/dark/pvotal.svg\" alt=\"pvotal.tech\"></picture></a></td>\n      <td></td>\n    </tr>\n    </tbody>\n</table>\n\nMany thanks to all individual contributors\n\n<a href=\"https://opencollective.com/ory\" target=\"_blank\"><img src=\"https://opencollective.com/ory/contributors.svg?width=890&limit=714&button=false\" /></a>\n\n<!--END ADOPTERS-->\n\n## Ecosystem\n\n<!--BEGIN ECOSYSTEM-->\n\nWe build Ory on several guiding principles when it comes to our architecture\ndesign:\n\n- Minimal dependencies\n- Runs everywhere\n- Scales without effort\n- Minimize room for human and network errors\n\nOry's architecture is designed to run best on a Container Orchestration system\nsuch as Kubernetes, CloudFoundry, OpenShift, and similar projects. Binaries are\nsmall (5-15MB) and available for all popular processor types (ARM, AMD64, i386)\nand operating systems (FreeBSD, Linux, macOS, Windows) without system\ndependencies (Java, Node, Ruby, libxml, ...).\n\n### Ory Kratos: Identity and User Infrastructure and Management\n\n[Ory Kratos](https://github.com/ory/kratos) is an API-first Identity and User\nManagement system that is built according to\n[cloud architecture best practices](https://www.ory.sh/docs/next/ecosystem/software-architecture-philosophy).\nIt implements core use cases that almost every software application needs to\ndeal with: Self-service Login and Registration, Multi-Factor Authentication\n(MFA/2FA), Account Recovery and Verification, Profile, and Account Management.\n\n### Ory Hydra: OAuth2 & OpenID Connect Server\n\n[Ory Hydra](https://github.com/ory/hydra) is an OpenID Certifiedâ„¢ OAuth2 and\nOpenID Connect Provider which easily connects to any existing identity system by\nwriting a tiny \"bridge\" application. It gives absolute control over the user\ninterface and user experience flows.\n\n### Ory Oathkeeper: Identity & Access Proxy\n\n[Ory Oathkeeper](https://github.com/ory/oathkeeper) is a BeyondCorp/Zero Trust\nIdentity & Access Proxy (IAP) with configurable authentication, authorization,\nand request mutation rules for your web services: Authenticate JWT, Access\nTokens, API Keys, mTLS; Check if the contained subject is allowed to perform the\nrequest; Encode resulting content into custom headers (`X-User-ID`), JSON Web\nTokens and more!\n\n### Ory Keto: Access Control Policies as a Server\n\n[Ory Keto](https://github.com/ory/keto) is a policy decision point. It uses a\nset of access control policies, similar to AWS IAM Policies, in order to\ndetermine whether a subject (user, application, service, car, ...) is authorized\nto perform a certain action on a resource.\n\n<!--END ECOSYSTEM-->\n\n## Documentation\n\nThe full Ory Hydra documentation is available at\n[www.ory.sh/docs/hydra](https://www.ory.sh/docs/hydra), including:\n\n- [Installation guides](https://www.ory.sh/docs/hydra/install)\n- [Configuration reference](https://www.ory.sh/docs/hydra/reference/configuration)\n- [HTTP API documentation](https://www.ory.sh/docs/hydra/sdk/api)\n- [Security architecture](https://www.ory.sh/docs/hydra/security-architecture)\n- [Performance benchmarks](https://www.ory.sh/docs/performance/hydra)\n\nFor upgrading and changelogs, check\n[releases tab](https://github.com/ory/hydra/releases) and\n[CHANGELOG.md](./CHANGELOG.md).\n\n## Developing Ory Hydra\n\nSee [DEVELOP.md](./DEVELOP.md) for information on:\n\n- Contribution guidelines\n- Prerequisites\n- Install from source\n- Running tests\n- Build Docker image\n- Preview API documentation\n\n## Security\n\nOAuth2 and OAuth2 related specifications are over 400 written pages.\nImplementing OAuth2 is easy, getting it right is hard. Ory Hydra is trusted by\ncompanies all around the world, has a vibrant community and faces millions of\nrequests in production each day. Read\n[the security guide](https://www.ory.sh/docs/hydra/security-architecture) for\nmore details on cryptography and security concepts.\n\n### Disclosing vulnerabilities\n\nIf you think you found a security vulnerability, please refrain from posting it\npublicly on the forums, the chat, or GitHub. You can find all info for\nresponsible disclosure in our\n[security.txt](https://www.ory.sh/.well-known/security.txt).\n\n## Telemetry\n\nOur services collect summarized, anonymized data that can optionally be turned\noff. Click [here](https://www.ory.sh/docs/ecosystem/sqa) to learn more.\n\n## Libraries and third-party projects\n\nOfficial:\n\n- [User Login & Consent Example](https://github.com/ory/hydra-login-consent-node)\n\nCommunity:\n\n- Visit\n  [this document for an overview of community projects and articles](https://www.ory.sh/docs/ecosystem/community)\n\nDeveloper Blog:\n\n- Visit the [Ory Blog](https://www.ory.sh/blog/) for guides, tutorials and\n  articles around Ory Hydra and the Ory ecosystem.\n",
      "stars_today": 6
    },
    {
      "id": 184657328,
      "name": "TensorRT",
      "full_name": "NVIDIA/TensorRT",
      "description": "NVIDIAÂ® TensorRTâ„¢ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains the open source components of TensorRT.",
      "html_url": "https://github.com/NVIDIA/TensorRT",
      "stars": 12596,
      "forks": 2301,
      "language": "C++",
      "topics": [
        "deep-learning",
        "gpu-acceleration",
        "inference",
        "nvidia",
        "tensorrt"
      ],
      "created_at": "2019-05-02T22:02:08Z",
      "updated_at": "2026-01-14T23:35:06Z",
      "pushed_at": "2025-12-11T04:38:52Z",
      "open_issues": 556,
      "owner": {
        "login": "NVIDIA",
        "avatar_url": "https://avatars.githubusercontent.com/u/1728152?v=4"
      },
      "readme": "[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![Documentation](https://img.shields.io/badge/TensorRT-documentation-brightgreen.svg)](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html) [![Roadmap](https://img.shields.io/badge/Roadmap-Q1_2025-brightgreen.svg)](documents/tensorrt_roadmap_2025q1.pdf)\n\n# TensorRT Open Source Software\n\nThis repository contains the Open Source Software (OSS) components of NVIDIA TensorRT. It includes the sources for TensorRT plugins and ONNX parser, as well as sample applications demonstrating usage and capabilities of the TensorRT platform. These open source software components are a subset of the TensorRT General Availability (GA) release with some extensions and bug-fixes.\n\n- For code contributions to TensorRT-OSS, please see our [Contribution Guide](CONTRIBUTING.md) and [Coding Guidelines](CODING-GUIDELINES.md).\n- For a summary of new additions and updates shipped with TensorRT-OSS releases, please refer to the [Changelog](CHANGELOG.md).\n- For business inquiries, please contact [researchinquiries@nvidia.com](mailto:researchinquiries@nvidia.com)\n- For press and other inquiries, please contact Hector Marinez at [hmarinez@nvidia.com](mailto:hmarinez@nvidia.com)\n\nNeed enterprise support? NVIDIA global support is available for TensorRT with the [NVIDIA AI Enterprise software suite](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/). Check out [NVIDIA LaunchPad](https://www.nvidia.com/en-us/launchpad/ai/ai-enterprise/) for free access to a set of hands-on labs with TensorRT hosted on NVIDIA infrastructure.\n\nJoin the [TensorRT and Triton community](https://www.nvidia.com/en-us/deep-learning-ai/triton-tensorrt-newsletter/) and stay current on the latest product updates, bug fixes, content, best practices, and more.\n\n# Prebuilt TensorRT Python Package\n\nWe provide the TensorRT Python package for an easy installation. \\\nTo install:\n\n```bash\npip install tensorrt\n```\n\nYou can skip the **Build** section to enjoy TensorRT with Python.\n\n# Build\n\n## Prerequisites\n\nTo build the TensorRT-OSS components, you will first need the following software packages.\n\n**TensorRT GA build**\n\n- TensorRT v10.14.1.48\n  - Available from direct download links listed below\n\n**System Packages**\n\n- [CUDA](https://developer.nvidia.com/cuda-toolkit)\n  - Recommended versions:\n  - cuda-13.0.0\n  - cuda-12.9.0\n- [CUDNN (optional)](https://developer.nvidia.com/cudnn)\n  - cuDNN 8.9\n- [GNU make](https://ftp.gnu.org/gnu/make/) >= v4.1\n- [cmake](https://github.com/Kitware/CMake/releases) >= v3.31\n- [python](https://www.python.org/downloads/) >= v3.10, <= v3.13.x\n- [pip](https://pypi.org/project/pip/#history) >= v19.0\n- Essential utilities\n  - [git](https://git-scm.com/downloads), [pkg-config](https://www.freedesktop.org/wiki/Software/pkg-config/), [wget](https://www.gnu.org/software/wget/faq.html#download)\n\n**Optional Packages**\n\n- Containerized build\n  - [Docker](https://docs.docker.com/install/) >= 19.03\n  - [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker)\n- PyPI packages (for demo applications/tests)\n  - [onnx](https://pypi.org/project/onnx/)\n  - [onnxruntime](https://pypi.org/project/onnxruntime/)\n  - [tensorflow-gpu](https://pypi.org/project/tensorflow/) >= 2.5.1\n  - [Pillow](https://pypi.org/project/Pillow/) >= 9.0.1\n  - [pycuda](https://pypi.org/project/pycuda/) < 2021.1\n  - [numpy](https://pypi.org/project/numpy/)\n  - [pytest](https://pypi.org/project/pytest/)\n- Code formatting tools (for contributors)\n\n  - [Clang-format](https://clang.llvm.org/docs/ClangFormat.html)\n  - [Git-clang-format](https://github.com/llvm-mirror/clang/blob/master/tools/clang-format/git-clang-format)\n\n  > NOTE: [onnx-tensorrt](https://github.com/onnx/onnx-tensorrt), [cub](http://nvlabs.github.io/cub/), and [protobuf](https://github.com/protocolbuffers/protobuf.git) packages are downloaded along with TensorRT OSS, and not required to be installed.\n\n## Downloading TensorRT Build\n\n1. #### Download TensorRT OSS\n\n   ```bash\n   git clone -b main https://github.com/nvidia/TensorRT TensorRT\n   cd TensorRT\n   git submodule update --init --recursive\n   ```\n\n2. #### (Optional - if not using TensorRT container) Specify the TensorRT GA release build path\n\n   If using the TensorRT OSS build container, TensorRT libraries are preinstalled under `/usr/lib/x86_64-linux-gnu` and you may skip this step.\n\n   Else download and extract the TensorRT GA build from [NVIDIA Developer Zone](https://developer.nvidia.com) with the direct links below:\n\n   - [TensorRT 10.14.1.48 for CUDA 13.0, Linux x86_64](https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.14.1/tars/TensorRT-10.14.1.48.Linux.x86_64-gnu.cuda-13.0.tar.gz)\n   - [TensorRT 10.14.1.48 for CUDA 12.9, Linux x86_64](https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.14.1/tars/TensorRT-10.14.1.48.Linux.x86_64-gnu.cuda-12.9.tar.gz)\n   - [TensorRT 10.14.1.48 for CUDA 13.0, Windows x86_64](https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.14.1/zip/TensorRT-10.14.1.48.Windows.win10.cuda-13.0.zip)\n   - [TensorRT 10.14.1.48 for CUDA 12.9, Windows x86_64](https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.14.1/zip/TensorRT-10.14.1.48.Windows.win10.cuda-12.9.zip)\n\n   **Example: Ubuntu 22.04 on x86-64 with cuda-13.0**\n\n   ```bash\n   cd ~/Downloads\n   tar -xvzf TensorRT-10.14.1.48.Linux.x86_64-gnu.cuda-13.0.tar.gz\n   export TRT_LIBPATH=`pwd`/TensorRT-10.14.1.48\n   ```\n\n   **Example: Windows on x86-64 with cuda-12.9**\n\n   ```powershell\n   Expand-Archive -Path TensorRT-10.14.1.48.Windows.win10.cuda-12.9.zip\n   $env:TRT_LIBPATH=\"$pwd\\TensorRT-10.14.1.48\\lib\"\n   ```\n\n## Setting Up The Build Environment\n\nFor Linux platforms, we recommend that you generate a docker container for building TensorRT OSS as described below. For native builds, please install the [prerequisite](#prerequisites) _System Packages_.\n\n1. #### Generate the TensorRT-OSS build container.\n\n   **Example: Ubuntu 24.04 on x86-64 with cuda-13.0 (default)**\n\n   ```bash\n   ./docker/build.sh --file docker/ubuntu-24.04.Dockerfile --tag tensorrt-ubuntu24.04-cuda13.0\n   ```\n\n   **Example: Rockylinux8 on x86-64 with cuda-13.0**\n\n   ```bash\n   ./docker/build.sh --file docker/rockylinux8.Dockerfile --tag tensorrt-rockylinux8-cuda13.0\n   ```\n\n   **Example: Ubuntu 24.04 cross-compile for Jetson (aarch64) with cuda-13.0 (JetPack SDK)**\n\n   ```bash\n   ./docker/build.sh --file docker/ubuntu-cross-aarch64.Dockerfile --tag tensorrt-jetpack-cuda13.0\n   ```\n\n   **Example: Ubuntu 24.04 on aarch64 with cuda-13.0**\n\n   ```bash\n   ./docker/build.sh --file docker/ubuntu-24.04-aarch64.Dockerfile --tag tensorrt-aarch64-ubuntu24.04-cuda13.0\n   ```\n\n2. #### Launch the TensorRT-OSS build container.\n   **Example: Ubuntu 24.04 build container**\n   ```bash\n   ./docker/launch.sh --tag tensorrt-ubuntu24.04-cuda13.0 --gpus all\n   ```\n   > NOTE:\n   > <br> 1. Use the `--tag` corresponding to build container generated in Step 1.\n   > <br> 2. [NVIDIA Container Toolkit](#prerequisites) is required for GPU access (running TensorRT applications) inside the build container.\n   > <br> 3. `sudo` password for Ubuntu build containers is 'nvidia'.\n   > <br> 4. Specify port number using `--jupyter <port>` for launching Jupyter notebooks.\n   > <br> 5. Write permission to this folder is required as this folder will be mounted inside the docker container for uid:gid of 1000:1000.\n\n## Building TensorRT-OSS\n\n- Generate Makefiles and build\n\n  **Example: Linux (x86-64) build with default cuda-13.0**\n\n  ```bash\n  cd $TRT_OSSPATH\n  mkdir -p build && cd build\n  cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out\n  make -j$(nproc)\n  ```\n\n  **Example: Linux (aarch64) build with default cuda-13.0**\n\n  ```bash\n  cd $TRT_OSSPATH\n  mkdir -p build && cd build\n  cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out -DCMAKE_TOOLCHAIN_FILE=$TRT_OSSPATH/cmake/toolchains/cmake_aarch64-native.toolchain\n  make -j$(nproc)\n  ```\n\n  **Example: Native build on Jetson Thor (aarch64) with cuda-13.0**\n\n  ```bash\n  cd $TRT_OSSPATH\n  mkdir -p build && cd build\n  cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out -DTRT_PLATFORM_ID=aarch64\n  CC=/usr/bin/gcc make -j$(nproc)\n  ```\n\n  > NOTE: C compiler must be explicitly specified via CC= for native aarch64 builds of protobuf.\n\n  **Example: Ubuntu 24.04 Cross-Compile for Jetson Thor (aarch64) with cuda-13.0 (JetPack)**\n\n  ```bash\n  cd $TRT_OSSPATH\n  mkdir -p build && cd build\n  cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DCMAKE_TOOLCHAIN_FILE=$TRT_OSSPATH/cmake/toolchains/cmake_aarch64_cross.toolchain\n  make -j$(nproc)\n  ```\n\n  **Example: Ubuntu 24.04 Cross-Compile for DriveOS (aarch64) with cuda-13.0**\n\n  ```bash\n  cd $TRT_OSSPATH\n  mkdir -p build && cd build\n  cmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DCMAKE_TOOLCHAIN_FILE=$TRT_OSSPATH/cmake/toolchains/cmake_aarch64_dos_cross.toolchain\n  make -j$(nproc)\n  ```\n\n  **Example: Native builds on Windows (x86) with cuda-13.0**\n\n  ```bash\n  cd $TRT_OSSPATH\n  mkdir -p build\n  cd -p build\n  cmake .. -DTRT_LIB_DIR=\"$env:TRT_LIBPATH\" -DTRT_OUT_DIR=\"$pwd\\\\out\"\n  msbuild TensorRT.sln /property:Configuration=Release -m:$env:NUMBER_OF_PROCESSORS\n  ```\n\n  > NOTE: The default CUDA version used by CMake is 13.0. To override this, for example to 12.9, append `-DCUDA_VERSION=12.9` to the cmake command.\n\n- Required CMake build arguments are:\n  - `TRT_LIB_DIR`: Path to the TensorRT installation directory containing libraries.\n  - `TRT_OUT_DIR`: Output directory where generated build artifacts will be copied.\n- Optional CMake build arguments:\n  - `CMAKE_BUILD_TYPE`: Specify if binaries generated are for release or debug (contain debug symbols). Values consists of [`Release`] | `Debug`\n  - `CUDA_VERSION`: The version of CUDA to target, for example [`12.9.9`].\n  - `CUDNN_VERSION`: The version of cuDNN to target, for example [`8.9`].\n  - `PROTOBUF_VERSION`: The version of Protobuf to use, for example [`3.20.1`]. Note: Changing this will not configure CMake to use a system version of Protobuf, it will configure CMake to download and try building that version.\n  - `CMAKE_TOOLCHAIN_FILE`: The path to a toolchain file for cross compilation.\n  - `BUILD_PARSERS`: Specify if the parsers should be built, for example [`ON`] | `OFF`. If turned OFF, CMake will try to find precompiled versions of the parser libraries to use in compiling samples. First in `${TRT_LIB_DIR}`, then on the system. If the build type is Debug, then it will prefer debug builds of the libraries before release versions if available.\n  - `BUILD_PLUGINS`: Specify if the plugins should be built, for example [`ON`] | `OFF`. If turned OFF, CMake will try to find a precompiled version of the plugin library to use in compiling samples. First in `${TRT_LIB_DIR}`, then on the system. If the build type is Debug, then it will prefer debug builds of the libraries before release versions if available.\n  - `BUILD_SAMPLES`: Specify if the samples should be built, for example [`ON`] | `OFF`.\n  - `GPU_ARCHS`: GPU (SM) architectures to target. By default we generate CUDA code for all major SMs. Specific SM versions can be specified here as a quoted space-separated list to reduce compilation time and binary size. Table of compute capabilities of NVIDIA GPUs can be found [here](https://developer.nvidia.com/cuda-gpus). Examples: - NVidia A100: `-DGPU_ARCHS=\"80\"` - RTX 50 series: `-DGPU_ARCHS=\"120\"` - Multiple SMs: `-DGPU_ARCHS=\"80 120\"`\n  - `TRT_PLATFORM_ID`: Bare-metal build (unlike containerized cross-compilation). Currently supported options: `x86_64` (default).\n\n# References\n\n## TensorRT Resources\n\n- [TensorRT Developer Home](https://developer.nvidia.com/tensorrt)\n- [TensorRT QuickStart Guide](https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html)\n- [TensorRT Developer Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html)\n- [TensorRT Sample Support Guide](https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html)\n- [TensorRT ONNX Tools](https://docs.nvidia.com/deeplearning/tensorrt/index.html#tools)\n- [TensorRT Discussion Forums](https://devtalk.nvidia.com/default/board/304/tensorrt/)\n- [TensorRT Release Notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/index.html)\n\n## Known Issues\n\n- Please refer to [TensorRT Release Notes](https://docs.nvidia.com/deeplearning/tensorrt/release-notes)\n",
      "stars_today": 6
    },
    {
      "id": 108035205,
      "name": "purchases-ios",
      "full_name": "RevenueCat/purchases-ios",
      "description": "In-app purchases and subscriptions made easy. Support for iOS, watchOS, tvOS, macOS, and visionOS.",
      "html_url": "https://github.com/RevenueCat/purchases-ios",
      "stars": 2899,
      "forks": 410,
      "language": "Swift",
      "topics": [
        "apple",
        "hacktoberfest",
        "iap",
        "ios",
        "objective-c",
        "storekit",
        "storekit-wrapper",
        "storekit2",
        "swift",
        "swiftui",
        "visionos"
      ],
      "created_at": "2017-10-23T20:23:47Z",
      "updated_at": "2026-01-14T17:35:32Z",
      "pushed_at": "2026-01-14T20:23:21Z",
      "open_issues": 132,
      "owner": {
        "login": "RevenueCat",
        "avatar_url": "https://avatars.githubusercontent.com/u/33013347?v=4"
      },
      "readme": "<h3 align=\"center\">ğŸ˜» In-App Subscriptions Made Easy ğŸ˜»</h3>\n\n[![License](https://img.shields.io/cocoapods/l/RevenueCat.svg?style=flat)](http://cocoapods.org/pods/RevenueCat)\n[![Version](https://img.shields.io/cocoapods/v/RevenueCat.svg?style=flat)](https://cocoapods.org/pods/RevenueCat)\n[![Carthage compatible](https://img.shields.io/badge/Carthage-compatible-4BC51D.svg?style=flat)](https://docs.revenuecat.com/docs/ios#section-install-via-carthage)\n[![SwiftPM compatible](https://img.shields.io/badge/SwiftPM-compatible-orange.svg)](https://docs.revenuecat.com/docs/ios#section-install-via-swift-package-manager)\n[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2FRevenueCat%2Fpurchases-ios%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/RevenueCat/purchases-ios)\n[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2FRevenueCat%2Fpurchases-ios%2Fbadge%3Ftype%3Dplatforms)](https://swiftpackageindex.com/RevenueCat/purchases-ios)\n\nRevenueCat is a powerful, reliable, and free to use in-app purchase server with cross-platform support. Our open-source framework provides a backend and a wrapper around StoreKit and Google Play Billing to make implementing in-app purchases and subscriptions easy. \n\nWhether you are building a new app or already have millions of customers, you can use RevenueCat to:\n\n  * Fetch products, make purchases, and check subscription status with our [native SDKs](https://docs.revenuecat.com/docs/installation). \n  * Host and [configure products](https://docs.revenuecat.com/docs/entitlements) remotely from our dashboard. \n  * Analyze the most important metrics for your app business [in one place](https://docs.revenuecat.com/docs/charts).\n  * See customer transaction histories, chart lifetime value, and [grant promotional subscriptions](https://www.revenuecat.com/docs/dashboard-and-metrics/customer-history/promotionals).\n  * Get notified of real-time events through [webhooks](https://docs.revenuecat.com/docs/webhooks).\n  * Send enriched purchase events to analytics and attribution tools with our easy integrations.\n\nSign up to [get started for free](https://app.revenuecat.com/signup).\n\n## RevenueCat.framework\n\n*RevenueCat* is the client for the [RevenueCat](https://www.revenuecat.com/) subscription and purchase tracking system. It's 100% `Swift` and compatible with `Objective-C`.\n\n## Migrating from Purchases v4 to v5\n- See our [Migration guide](https://revenuecat.github.io/purchases-ios-docs/v5_api_migration_guide.html)\n\n## Migrating from Purchases v3 to v4\n- See our [Migration guide](https://revenuecat.github.io/purchases-ios-docs/v4_api_migration_guide.html)\n\n## RevenueCat SDK Features\n|   | RevenueCat |\n| --- | --- |\nâœ… | Server-side receipt validation\nâ¡ï¸ | [Webhooks](https://docs.revenuecat.com/docs/webhooks) - enhanced server-to-server communication with events for purchases, renewals, cancellations, and more\nğŸ–¥ | iOS, tvOS, macOS, watchOS, Mac Catalyst, and visionOS support\nğŸ¯ | Subscription status tracking - know whether a user is subscribed whether they're on iOS, Android or web\nğŸ“Š | Analytics - automatic calculation of metrics like conversion, mrr, and churn\nğŸ“ | [Online documentation](https://docs.revenuecat.com/docs) and [SDK Reference](http://revenuecat.github.io/purchases-ios-docs/) up to date\nğŸ”€ | [Integrations](https://www.revenuecat.com/integrations) - over a dozen integrations to easily send purchase data where you need it\nğŸ’¯ | Well maintained - [frequent releases](https://github.com/RevenueCat/purchases-ios/releases)\nğŸ“® | Great support - [Contact us](https://revenuecat.com/support)\n\n## Getting Started\nFor more detailed information, you can view our complete documentation at [docs.revenuecat.com](https://docs.revenuecat.com/docs).\n\nPlease follow the [Quickstart Guide](https://docs.revenuecat.com/docs/) for more information on how to install the SDK.\n\n> [!TIP]\n> When integrating with SPM, it is recommended to add the SPM mirror repository for faster download/integration times: https://github.com/RevenueCat/purchases-ios-spm\n\nOr view our iOS sample apps:\n- [MagicWeather](Examples/MagicWeather)\n- [MagicWeather SwiftUI](Examples/MagicWeatherSwiftUI)\n\n## Requirements\n- Xcode 15.0+\n\n| Platform | Minimum target |\n|----------|----------------|\n| iOS      | 13.0+          |\n| tvOS     | 13.0+          |\n| macOS    | 10.15+         |\n| watchOS  | 6.2+           |\n| visionOS | 1.0+           |\n\n## SDK Reference\nOur full SDK reference [can be found here](https://revenuecat.github.io/purchases-ios-docs).\n\n## Contributing\nContributions are always welcome! To learn how you can contribute, please see the [Contributing Guide](./Contributing/CONTRIBUTING.md).\n",
      "stars_today": 6
    },
    {
      "id": 1058017661,
      "name": "LaunchNext",
      "full_name": "RoversX/LaunchNext",
      "description": "Bring your Launchpad back in MacOS26+ ,highly customizable, powerful, free.",
      "html_url": "https://github.com/RoversX/LaunchNext",
      "stars": 1939,
      "forks": 101,
      "language": "Swift",
      "topics": [],
      "created_at": "2025-09-16T14:08:59Z",
      "updated_at": "2026-01-14T21:40:30Z",
      "pushed_at": "2026-01-13T00:10:24Z",
      "open_issues": 124,
      "owner": {
        "login": "RoversX",
        "avatar_url": "https://avatars.githubusercontent.com/u/85817538?v=4"
      },
      "readme": "# LaunchNext\n\n**Languages**: [English](README.md) | [ä¸­æ–‡](i18n/README.zh.md) | [æ—¥æœ¬èª](i18n/README.ja.md) | [í•œêµ­ì–´](i18n/README.ko.md) | [FranÃ§ais](i18n/README.fr.md) | [EspaÃ±ol](i18n/README.es.md) | [Deutsch](i18n/README.de.md) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](i18n/README.ru.md) | [à¤¹à¤¿à¤¨à¥à¤¦à¥€](i18n/README.hi.md) | [Tiáº¿ng Viá»‡t](i18n/README.vi.md) | [Italiano](i18n/README.it.md) | [ÄŒeÅ¡tina](i18n/README.cs.md)\n\n## ğŸ“¥ Download\n\n**[Download here](https://github.com/RoversX/LaunchNext/releases/latest)** - Get the latest release\n\nâ­ Consider starring [LaunchNext](https://github.com/RoversX/LaunchNext) and especially [LaunchNow](https://github.com/ggkevinnnn/LaunchNow)!\n\n| | |\n|:---:|:---:|\n| ![](./public/banner.webp) | ![](./public/setting1.webp) |\n| ![](./public/setting2.webp) | ![](./public/setting3.webp) |\n\nMacOS Tahoe removed launchpad,and it's so hard to use, it's doesn't use your Bio GPU, please apple, at least give people an option to switch back. Before that, here is LaunchNext\n\n*Built upon [LaunchNow](https://github.com/ggkevinnnn/LaunchNow) by ggkevinnnn - huge thanks to the original project! I hope this enhanced version can be merged back to the original repository*\n\n*LaunchNow has chosen the GPL 3 license. LaunchNext follows the same licensing terms.*\n\nâš ï¸ **If macOS blocks the app, run this in Terminal:**\n```bash\nsudo xattr -r -d com.apple.quarantine /Applications/LaunchNext.app\n```\n**Why**: I can't afford Apple's developer certificate ($99/year), so macOS blocks unsigned apps. This command removes the quarantine flag to let it run. **Only use this command on apps you trust.**\n\n### What LaunchNext Delivers\n- âœ… **One-click import from old system Launchpad** - directly reads your native Launchpad SQLite database (`/private$(getconf DARWIN_USER_DIR)com.apple.dock.launchpad/db/db`) to perfectly recreate your existing folders, app positions, and layout\n- âœ… **Classic Launchpad experience** - works exactly like the beloved original interface\n- âœ… **Multi-language support** - full internationalization with English, Chinese, Japanese, French, Spanish, German, and Russian\n- âœ… **Hide icon labels** - clean, minimalist view when you don't need app names\n- âœ… **Custom icon sizes** - adjust icon dimensions to fit your preferences\n- âœ… **Smart folder management** - create and organize folders just like before\n- âœ… **Instant search and keyboard navigation** - find apps quickly\n\n### What We Lost in macOS Tahoe\n- âŒ No custom app organization\n- âŒ No user-created folders\n- âŒ No drag-and-drop customization\n- âŒ No visual app management\n- âŒ Forced categorical grouping\n\n\n### Data Storage\nApplication data is safely stored in:\n```\n~/Library/Application Support/LaunchNext/Data.store\n```\n\n### Native Launchpad Integration\nReads directly from the system Launchpad database:\n```bash\n/private$(getconf DARWIN_USER_DIR)com.apple.dock.launchpad/db/db\n```\n\n## Installation\n\n### Requirements\n- macOS 26 (Tahoe) or later\n- Apple Silicon or Intel processor\n- Xcode 26 (for building from source)\n\n### Build from Source\n\n1. **Clone the repository**\n   ```bash\n   git clone https://github.com/yourusername/LaunchNext.git\n   cd LaunchNext\n   ```\n\n2. **Open in Xcode**\n   ```bash\n   open LaunchNext.xcodeproj\n   ```\n\n3. **Build and run**\n   - Select your target device\n   - Press `âŒ˜+R` to build and run\n   - Or `âŒ˜+B` to build only\n\n### Command Line Build\n\n**Regular Build:**\n```bash\nxcodebuild -project LaunchNext.xcodeproj -scheme LaunchNext -configuration Release\n```\n\n**Universal Binary Build (Intel + Apple Silicon):**\n```bash\nxcodebuild -project LaunchNext.xcodeproj -scheme LaunchNext -configuration Release ARCHS=\"arm64 x86_64\" ONLY_ACTIVE_ARCH=NO clean build\n```\n\n## Usage\n\n### Getting Started\n1. **First Launch**: LaunchNext automatically scans all installed applications\n2. **Select**: Click to select apps, double-click to launch\n3. **Search**: Type to instantly filter applications\n4. **Organize**: Drag apps to create folders and custom layouts\n\n### Import Your Launchpad\n1. Open Settings (gear icon)\n2. Click **\"Import Launchpad\"**\n3. Your existing layout and folders are automatically imported\n\n\n### Display Modes\n- **Windowed**: Floating window with rounded corners\n- **Fullscreen**: Full-screen mode for maximum visibility\n- Switch modes in Settings\n\n## Advanced Features\n\n### Smart Background Interaction\n- Intelligent click detection prevents accidental dismissal\n- Context-aware gesture handling\n- Search field protection\n\n### Performance Optimization\n- **Icon Caching**: Intelligent image caching for smooth scrolling\n- **Lazy Loading**: Efficient memory usage\n- **Background Scanning**: Non-blocking app discovery\n\n### Multi-Display Support\n- Automatic screen detection\n- Per-display positioning\n- Seamless multi-monitor workflows\n\n## Troubleshooting\n\n### Common Issues\n\n**Q: App won't start?**\nA: Ensure macOS 26.0+ and check system permissions.\n\n## Contributing\n\nWe welcome contributions! Please:\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Commit changes (`git commit -m 'Add amazing feature'`)\n4. Push to branch (`git push origin feature/amazing-feature`)\n5. Open a Pull Request\n\n### Development Guidelines\n- Follow Swift style conventions\n- Add meaningful comments for complex logic\n- Test on multiple macOS versions\n- Maintain backward compatibility\n\n## The Future of App Management\n\nAs Apple moves away from customizable interfaces, LaunchNext represents the community's commitment to user control and personalization. I hope apple cound bring launchpad back.\n\n**LaunchNext** isn't just a Launchpad replacementâ€”it's a statement that user choice matters.\n\n\n---\n\n**LaunchNext** - Reclaim Your App Launcher ğŸš€\n\n*Built for macOS users who refuse to compromise on customization.*\n\n## Development Tools\n\n- Claude Code \n- Cursor \n- OpenAI Codex Cli\n- Perplexity\n- Google\n\n\n![GitHub downloads](https://img.shields.io/github/downloads/RoversX/LaunchNext/total)\n",
      "stars_today": 6
    },
    {
      "id": 1046959217,
      "name": "TEESimulator",
      "full_name": "JingMatrix/TEESimulator",
      "description": "Software simulation for Android hardware-backed key pairs with key attestation",
      "html_url": "https://github.com/JingMatrix/TEESimulator",
      "stars": 549,
      "forks": 37,
      "language": "Kotlin",
      "topics": [
        "android",
        "hardware-simulation",
        "playintegrity",
        "tee",
        "trickystore"
      ],
      "created_at": "2025-08-29T14:02:27Z",
      "updated_at": "2026-01-14T16:53:47Z",
      "pushed_at": "2026-01-11T15:25:17Z",
      "open_issues": 11,
      "owner": {
        "login": "JingMatrix",
        "avatar_url": "https://avatars.githubusercontent.com/u/24476093?v=4"
      },
      "readme": "# TEESimulator â€“ A Full TEE Emulation Framework\n\n**TEESimulator** is a system module designed to create a complete, software-based simulation of a hardware-backed Trusted Execution Environment ([TEE](https://source.android.com/docs/security/features/trusty)) for [Key Attestation](https://developer.android.com/privacy-and-security/security-key-attestation).\n\nThe project's goal is to move beyond simple certificate patching and build a robust framework that can create and manage virtual, self-consistent cryptographic keys.\n\n## âœ¨ Core Principles\n\n*   **Bypass Hardware-Backed Attestation:** The primary goal of this project is to defeat Key Attestation, a security mechanism that allows apps to verify that they are running on a secure, unmodified device. This module provides the tools to bypass these checks on rooted or modified devices.\n*   **Stateful Emulation:** Instead of patching responses from the real TEE, the ultimate goal is to create and manage virtual keys entirely in a simulated software environment. Any request concerning a virtual key will be handled by the simulator, ensuring perfect consistency without ever touching the real hardware.\n*   **Architectural Interception:** By hooking low-level Binder IPC calls to the Keystore, the framework can transparently redirect requests for virtual keys to the software-based simulator, while allowing requests for real keys to pass through to the hardware TEE.\n*   **100% FOSS:** Licensed under GPLv3, ensuring it stays free, auditable, and compliant with open-source laws.\n\n## ğŸ“± Requirements\n- Android 10 or above\n\n## ğŸ“¦ Installation & Configuration\n\n1.  Flash this module via (Magisk / KernelSU / APatch) and reboot. It will replace [TrickyStore](https://github.com/5ec1cff/TrickyStore), [TrickyStoreOSS](https://github.com/beakthoven/TrickyStoreOSS) and their forks.\n2.  (Optional) Place a hardware-backed `keybox.xml` at `/data/adb/tricky_store/keybox.xml`. This provides the cryptographic \"root of trust\" for the simulator.\n3.  (Optional) Customize target packages in `/data/adb/tricky_store/target.txt`.\n4.  (Optional) Customize the simulated security patch level in `/data/adb/tricky_store/security_patch.txt`.\n5.  Enjoy!\n\n**All configuration files are monitored and will take effect immediately upon saving.**\n\n### The `keybox.xml` Root of Trust\n\nThis file provides the master cryptographic identity for the simulator. It contains a private key and a valid, hardware-backed certificate chain from a real device. The simulator uses this to sign the virtual certificates it generates, making them appear legitimate to verifiers.\n\n```xml\n<?xml version=\"1.0\"?>\n<AndroidAttestation>\n    <Keybox DeviceID=\"...\">\n        <Key algorithm=\"ecdsa|rsa\">\n            <PrivateKey format=\"pem\">...</PrivateKey>\n            <CertificateChain>...</CertificateChain>\n        </Key>\n    </Keybox>\n</AndroidAttestation>\n```\n\n### Mode and Keybox Configuration (`target.txt`)\n\nTEESimulator currently operates in two primary modes as it transitions towards full emulation.\nYou can control the simulation mode and the specific keybox.xml file used on a per-package basis.\n\n#### Mode Suffixes\n\n*   **`!` â†’ Force Generation Mode:** Creates a complete, software-based virtual key. This is the foundation of the full TEE simulation.\n*   **`?` â†’ Force Leaf Hacking Mode:** A legacy mode where a real TEE key is generated, but its attestation certificate is intercepted and modified.\n*   **No symbol â†’ Automatic Mode:** The module selects the most appropriate mode for the device.\n\n#### Multi-Keybox Configuration\n\nYou can specify different keybox files for different groups of applications. This is done by adding a line with the filename in square brackets (e.g., [demo_keybox.xml]).\n\nAll applications listed after this line will use the specified keybox file, until a new keybox is declared. Applications listed before any custom keybox declaration will use the default `keybox.xml`.\n\nFor example:\n```\n# These two apps will use the default /data/adb/tricky_store/keybox.xml\ncom.google.android.gms!\nio.github.vvb2060.keyattestation?\n\n# Switch to a different keybox for the following apps.\n# The file must be located at /data/adb/tricky_store/aosp_keybox.xml\n[aosp_keybox.xml]\ncom.google.android.gsf\n\n# Switch again to another keybox.\n# The file must be located at /data/adb/tricky_store/demo_keybox.xml\n[demo_keybox.xml]\norg.matrix.demo\n```\n\n### Security Patch Level (`security_patch.txt`)\n\nThis file allows you to configure the `osPatchLevel`, `vendorPatchLevel`, and `bootPatchLevel` that the simulator will report in its patched or forged attestation certificates.\n\n**Note:** This only affects the Key Attestation data generated by the simulator. It does not change the actual system properties of your device.\n\n#### Global and Per-Package Configuration\n\nYou can set a global patch level that applies to all applications, and you can also override these settings for specific packages. The syntax is hierarchical:\n\n*   Settings defined at the top of the file, before any `[package.name]` line, are **global** and serve as the default for all apps.\n*   To create a specific configuration for an application, add its package name in square brackets (e.g., `[com.google.android.gms]`). All settings following this line will apply *only* to that package until a new package context is declared.\n\n#### Configuration Keys and Values\n\nYou can specify the patch level for the following components using a `key=value` format:\n\n*   `system`: The main OS patch level.\n*   `vendor`: The vendor patch level.\n*   `boot`: The boot/kernel patch level.\n*   `all`: A convenient shorthand to set the same date for `system`, `vendor`, and `boot` simultaneously. Any individual key can still be used to override the value set by `all`.\n\nDates should be provided in `YYYY-MM-DD` format (e.g., `2025-11-05`).\n\n#### Special Keywords\n\nIn addition to static dates, several special keywords provide advanced, dynamic control:\n\n*   **`today`**: Dynamically uses the current date every time an attestation is generated. This ensures the device always appears up-to-date without needing manual edits.\n\n*   **Date Templates**: You can create semi-dynamic dates using `YYYY`, `MM`, and `DD` as placeholders for the current year, month, and day. For example, `YYYY-MM-05` will always resolve to the 5th of the current month and year.\n\n*   **`no`**: This keyword instructs the simulator to **completely omit** the corresponding patch level tag from the generated attestation.\n\n*   **`device_default`**: This keyword forces the simulator to fall back and use the device's **real hardware value** for that specific patch level. This is essential for creating exceptions to a global override or an `all` rule.\n\n#### Example Configuration\n\nThis example demonstrates how to combine global settings, per-package overrides, and special keywords for fine-grained control.\n\n```\n# --- Global Configuration ---\n# This is the default for all apps unless specified otherwise.\n# - Forge a recent system patch level, the 5th of the current month (a common patch date).\n# - Use the device's real vendor patch level.\n# - Do not report a boot patch level at all.\nsystem=YYYY-MM-05\nvendor=device_default\nboot=no\n\n# --- Per-Package Override for Google Play Services ---\n# This app will report an older, specific date for its system patch.\n# It will inherit the global settings for vendor (device_default) and boot (no).\n[com.google.android.gms]\nsystem=2024-10-01\n\n# --- Per-Package Override for a Demo App ---\n# This app gets a completely custom configuration.\n[org.matrix.demo]\n# Set a base date for all patch levels...\nall=2025-09-15\n# ...but make an exception: use the real boot patch level instead of the one from 'all'.\nboot=device_default\n```\n",
      "stars_today": 6
    },
    {
      "id": 99919302,
      "name": "doris",
      "full_name": "apache/doris",
      "description": "Apache Doris is an easy-to-use, high performance and unified analytics database.",
      "html_url": "https://github.com/apache/doris",
      "stars": 14881,
      "forks": 3681,
      "language": "Java",
      "topics": [
        "agent",
        "ai",
        "bigquery",
        "database",
        "dbt",
        "delta-lake",
        "elt",
        "hudi",
        "iceberg",
        "lakehouse",
        "olap",
        "paimon",
        "query-engine",
        "real-time",
        "redshift",
        "snowflake",
        "spark",
        "sql"
      ],
      "created_at": "2017-08-10T12:13:30Z",
      "updated_at": "2026-01-14T21:36:46Z",
      "pushed_at": "2026-01-15T00:14:28Z",
      "open_issues": 776,
      "owner": {
        "login": "apache",
        "avatar_url": "https://avatars.githubusercontent.com/u/47359?v=4"
      },
      "readme": "<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n\n## ğŸŒ Read this in other languages\n\n[English](README.md) â€¢ [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](docs/ar-SA/README.md) â€¢ [à¦¬à¦¾à¦‚à¦²à¦¾](docs/bn-BD/README.md) â€¢ [Deutsch](docs/de-DE/README.md) â€¢ [EspaÃ±ol](docs/es-ES/README.md) â€¢ [ÙØ§Ø±Ø³ÛŒ](docs/fa-IR/README.md) â€¢ [FranÃ§ais](docs/fr-FR/README.md) â€¢ [à¤¹à¤¿à¤¨à¥à¤¦à¥€](docs/hi-IN/README.md) â€¢ [Bahasa Indonesia](docs/id-ID/README.md) â€¢ [Italiano](docs/it-IT/README.md) â€¢ [æ—¥æœ¬èª](docs/ja-JP/README.md) â€¢ [í•œêµ­ì–´](docs/ko-KR/README.md) â€¢ [Polski](docs/pl-PL/README.md) â€¢ [PortuguÃªs](docs/pt-BR/README.md) â€¢ [RomÃ¢nÄƒ](docs/ro-RO/README.md) â€¢ [Ğ ÑƒÑÑĞºĞ¸Ğ¹](docs/ru-RU/README.md) â€¢ [SlovenÅ¡Äina](docs/sl-SI/README.md) â€¢ [à¹„à¸—à¸¢](docs/th-TH/README.md) â€¢ [TÃ¼rkÃ§e](docs/tr-TR/README.md) â€¢ [Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°](docs/uk-UA/README.md) â€¢ [Tiáº¿ng Viá»‡t](docs/vi-VN/README.md) â€¢ [ç®€ä½“ä¸­æ–‡](docs/zh-CN/README.md) â€¢ [ç¹é«”ä¸­æ–‡](docs/zh-TW/README.md)\n\n<div align=\"center\">\n\n# Apache Doris\n\n[![License](https://img.shields.io/badge/license-Apache%202-4EB1BA.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)\n[![GitHub release](https://img.shields.io/github/release/apache/doris.svg)](https://github.com/apache/doris/releases)\n[![OSSRank](https://shields.io/endpoint?url=https://ossrank.com/shield/516)](https://ossrank.com/p/516)\n[![Commit activity](https://img.shields.io/github/commit-activity/m/apache/doris)](https://github.com/apache/doris/commits/master/)\n[![EN doc](https://img.shields.io/badge/Docs-English-blue.svg)](https://doris.apache.org/docs/gettingStarted/what-is-apache-doris)\n[![CN doc](https://img.shields.io/badge/æ–‡æ¡£-ä¸­æ–‡ç‰ˆ-blue.svg)](https://doris.apache.org/zh-CN/docs/gettingStarted/what-is-apache-doris)\n\n<div>\n\n[![Official Website](<https://img.shields.io/badge/-Visit%20the%20Official%20Website%20%E2%86%92-rgb(15,214,106)?style=for-the-badge>)](https://doris.apache.org/)\n[![Quick Download](<https://img.shields.io/badge/-Quick%20%20Download%20%E2%86%92-rgb(66,56,255)?style=for-the-badge>)](https://doris.apache.org/download)\n\n\n</div>\n\n\n<div>\n    <a href=\"https://twitter.com/doris_apache\"><img src=\"https://img.shields.io/badge/- @Doris_Apache -424549?style=social&logo=x\" height=25></a>\n    &nbsp;\n    <a href=\"https://github.com/apache/doris/discussions\"><img src=\"https://img.shields.io/badge/- Discussion -red?style=social&logo=discourse\" height=25></a>\n    &nbsp;\n    <a href=\"https://doris.apache.org/slack\" height=25></a>\n    &nbsp;\n    <a href=\"https://medium.com/@ApacheDoris\"><img src=\"https://img.shields.io/badge/-Medium-red?style=social&logo=medium\" height=25></a>\n\n</div>\n\n</div>\n\n---\n\n<p align=\"center\">\n\n  <a href=\"https://trendshift.io/repositories/1156\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/1156\" alt=\"apache%2Fdoris | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n</p>\n\n\n\n\nApache Doris is an easy-to-use, high-performance and real-time analytical database based on MPP architecture, known for its extreme speed and ease of use. It only requires a sub-second response time to return query results under massive data and can support not only high-concurrency point query scenarios but also high-throughput complex analysis scenarios.\n\nAll this makes Apache Doris an ideal tool for scenarios including report analysis, ad-hoc query, unified data warehouse, and data lake query acceleration. On Apache Doris, users can build various applications, such as user behavior analysis, AB test platform, log retrieval analysis, user portrait analysis, and order analysis.\n\nğŸ‰ Check out the ğŸ”—[All releases](https://doris.apache.org/docs/releasenotes/all-release), where you'll find a chronological summary of Apache Doris versions released over the past year.\n\nğŸ‘€ Explore the ğŸ”—[Official Website](https://doris.apache.org/) to discover Apache Doris's core features, blogs, and user cases in detail.\n\n## ğŸ“ˆ Usage Scenarios\n\nAs shown in the figure below, after various data integration and processing, the data sources are usually stored in the real-time data warehouse Apache Doris and the offline data lake or data warehouse (in Apache Hive, Apache Iceberg or Apache Hudi).\n\n<br />\n\n<img src=\"https://cdn.selectdb.com/static/What_is_Apache_Doris_3_a61692c2ce.png\" />\n\n<br />\n\n\nApache Doris is widely used in the following scenarios:\n\n- **Real-time Data Analysis**:\n\n  - **Real-time Reporting and Decision-making**: Doris provides real-time updated reports and dashboards for both internal and external enterprise use, supporting real-time decision-making in automated processes.\n  \n  - **Ad Hoc Analysis**: Doris offers multidimensional data analysis capabilities, enabling rapid business intelligence analysis and ad hoc queries to help users quickly uncover insights from complex data.\n  \n  - **User Profiling and Behavior Analysis**: Doris can analyze user behaviors such as participation, retention, and conversion, while also supporting scenarios like population insights and crowd selection for behavior analysis.\n\n- **Lakehouse Analytics**:\n\n  - **Lakehouse Query Acceleration**: Doris accelerates lakehouse data queries with its efficient query engine.\n  \n  - **Federated Analytics**: Doris supports federated queries across multiple data sources, simplifying architecture and eliminating data silos.\n  \n  - **Real-time Data Processing**: Doris combines real-time data streams and batch data processing capabilities to meet the needs of high concurrency and low-latency complex business requirements.\n\n- **SQL-based Observability**:\n\n  - **Log and Event Analysis**: Doris enables real-time or batch analysis of logs and events in distributed systems, helping to identify issues and optimize performance.\n\n\n## Overall Architecture\n\nApache Doris uses the MySQL protocol, is highly compatible with MySQL syntax, and supports standard SQL. Users can access Apache Doris through various client tools, and it seamlessly integrates with BI tools.\n\n### Storage-Compute Integrated Architecture\n\nThe storage-compute integrated architecture of Apache Doris is streamlined and easy to maintain. As shown in the figure below, it consists of only two types of processes:\n\n- **Frontend (FE):** Primarily responsible for handling user requests, query parsing and planning, metadata management, and node management tasks.\n\n- **Backend (BE):** Primarily responsible for data storage and query execution. Data is partitioned into shards and stored with multiple replicas across BE nodes.\n\n![The overall architecture of Apache Doris](https://cdn.selectdb.com/static/What_is_Apache_Doris_adb26397e2.png)\n\n<br />\n\nIn a production environment, multiple FE nodes can be deployed for disaster recovery. Each FE node maintains a full copy of the metadata. The FE nodes are divided into three roles:\n\n| Role      | Function                                                     |\n| --------- | ------------------------------------------------------------ |\n| Master    | The FE Master node is responsible for metadata read and write operations. When metadata changes occur in the Master, they are synchronized to Follower or Observer nodes via the BDB JE protocol. |\n| Follower  | The Follower node is responsible for reading metadata. If the Master node fails, a Follower node can be selected as the new Master. |\n| Observer  | The Observer node is responsible for reading metadata and is mainly used to increase query concurrency. It does not participate in cluster leadership elections. |\n\nBoth FE and BE processes are horizontally scalable, enabling a single cluster to support hundreds of machines and tens of petabytes of storage capacity. The FE and BE processes use a consistency protocol to ensure high availability of services and high reliability of data. The storage-compute integrated architecture is highly integrated, significantly reducing the operational complexity of distributed systems.\n\n\n## Core Features of Apache Doris\n\n- **High Availability**: In Apache Doris, both metadata and data are stored with multiple replicas, synchronizing data logs via the quorum protocol. Data write is considered successful once a majority of replicas have completed the write, ensuring that the cluster remains available even if a few nodes fail. Apache Doris supports both same-city and cross-region disaster recovery, enabling dual-cluster master-slave modes. When some nodes experience failures, the cluster can automatically isolate the faulty nodes, preventing the overall cluster availability from being affected.\n\n- **High Compatibility**: Apache Doris is highly compatible with the MySQL protocol and supports standard SQL syntax, covering most MySQL and Hive functions. This high compatibility allows users to seamlessly migrate and integrate existing applications and tools. Apache Doris supports the MySQL ecosystem, enabling users to connect Doris using MySQL Client tools for more convenient operations and maintenance. It also supports MySQL protocol compatibility for BI reporting tools and data transmission tools, ensuring efficiency and stability in data analysis and data transmission processes.\n\n- **Real-Time Data Warehouse**: Based on Apache Doris, a real-time data warehouse service can be built. Apache Doris offers second-level data ingestion capabilities, capturing incremental changes from upstream online transactional databases into Doris within seconds. Leveraging vectorized engines, MPP architecture, and Pipeline execution engines, Doris provides sub-second data query capabilities, thereby constructing a high-performance, low-latency real-time data warehouse platform.\n\n- **Unified Lakehouse**: Apache Doris can build a unified lakehouse architecture based on external data sources such as data lakes or relational databases. The Doris unified lakehouse solution enables seamless integration and free data flow between data lakes and data warehouses, helping users directly utilize data warehouse capabilities to solve data analysis problems in data lakes while fully leveraging data lake data management capabilities to enhance data value.\n\n- **Flexible Modeling**: Apache Doris offers various modeling approaches, such as wide table models, pre-aggregation models, star/snowflake schemas, etc. During data import, data can be flattened into wide tables and written into Doris through compute engines like Flink or Spark, or data can be directly imported into Doris, performing data modeling operations through views, materialized views, or real-time multi-table joins.\n\n## Technical overview\n\nDoris provides an efficient SQL interface and is fully compatible with the MySQL protocol. Its query engine is based on an MPP (Massively Parallel Processing) architecture, capable of efficiently executing complex analytical queries and achieving low-latency real-time queries. Through columnar storage technology for data encoding and compression, it significantly optimizes query performance and storage compression ratio.\n\n### Interface\n\nApache Doris adopts the MySQL protocol, supports standard SQL, and is highly compatible with MySQL syntax. Users can access Apache Doris through various client tools and seamlessly integrate it with BI tools, including but not limited to Smartbi, DataEase, FineBI, Tableau, Power BI, and Apache Superset. Apache Doris can work as the data source for any BI tools that support the MySQL protocol.\n\n### Storage engine\n\nApache Doris has a columnar storage engine, which encodes, compresses, and reads data by column. This enables a very high data compression ratio and largely reduces unnecessary data scanning, thus making more efficient use of IO and CPU resources.\n\nApache Doris supports various index structures to minimize data scans:\n\n- **Sorted Compound Key Index**: Users can specify three columns at most to form a compound sort key. This can effectively prune data to better support highly concurrent reporting scenarios.\n\n- **Min/Max Index**: This enables effective data filtering in equivalence and range queries of numeric types.\n\n- **BloomFilter Index**: This is very effective in equivalence filtering and pruning of high-cardinality columns.\n\n- **Inverted Index**: This enables fast searching for any field.\n\nApache Doris supports a variety of data models and has optimized them for different scenarios:\n\n- **Detail Model (Duplicate Key Model):** A detail data model designed to meet the detailed storage requirements of fact tables.\n\n- **Primary Key Model (Unique Key Model):** Ensures unique keys; data with the same key is overwritten, enabling row-level data updates.\n\n- **Aggregate Model (Aggregate Key Model):** Merges value columns with the same key, significantly improving performance through pre-aggregation.\n\nApache Doris also supports strongly consistent single-table materialized views and asynchronously refreshed multi-table materialized views. Single-table materialized views are automatically refreshed and maintained by the system, requiring no manual intervention from users. Multi-table materialized views can be refreshed periodically using in-cluster scheduling or external scheduling tools, reducing the complexity of data modeling.\n\n### ğŸ” Query Engine\n\nApache Doris has an MPP-based query engine for parallel execution between and within nodes. It supports distributed shuffle join for large tables to better handle complicated queries.\n\n<br />\n\n![Query Engine](https://cdn.selectdb.com/static/What_is_Apache_Doris_1_c6f5ba2af9.png)\n\n<br />\n\nThe query engine of Apache Doris is fully vectorized, with all memory structures laid out in a columnar format. This can largely reduce virtual function calls, increase cache hit rates, and make efficient use of SIMD instructions. Apache Doris delivers a 5~10 times higher performance in wide table aggregation scenarios than non-vectorized engines.\n\n<br />\n\n![Doris query engine](https://cdn.selectdb.com/static/What_is_Apache_Doris_2_29cf58cc6b.png)\n\n<br />\n\nApache Doris uses adaptive query execution technology to dynamically adjust the execution plan based on runtime statistics. For example, it can generate a runtime filter and push it to the probe side. Specifically, it pushes the filters to the lowest-level scan node on the probe side, which largely reduces the data amount to be processed and increases join performance. The runtime filter of Apache Doris supports In/Min/Max/Bloom Filter.\n\nApache Doris uses a Pipeline execution engine that breaks down queries into multiple sub-tasks for parallel execution, fully leveraging multi-core CPU capabilities. It simultaneously addresses the thread explosion problem by limiting the number of query threads. The Pipeline execution engine reduces data copying and sharing, optimizes sorting and aggregation operations, thereby significantly improving query efficiency and throughput.\n\nIn terms of the optimizer, Apache Doris employs a combined optimization strategy of CBO (Cost-Based Optimizer), RBO (Rule-Based Optimizer), and HBO (History-Based Optimizer). RBO supports constant folding, subquery rewriting, predicate pushdown, and more. CBO supports join reordering and other optimizations. HBO recommends the optimal execution plan based on historical query information. These multiple optimization measures ensure that Doris can enumerate high-performance query plans across various types of queries.\n\n\n## ğŸ† Why choose Apache Doris?\n\n- ğŸ¯ **Easy to Use:** Two processes, no other dependencies; online cluster scaling, automatic replica recovery; compatible with MySQL protocol, and using standard SQL.\n\n- ğŸš€ **High Performance:** Extremely fast performance for low-latency and high-throughput queries with columnar storage engine, modern MPP architecture, vectorized query engine, pre-aggregated materialized view and data index.\n\n- ğŸ–¥ï¸ **Single Unified:** A single system can support real-time data serving, interactive data analysis and offline data processing scenarios.\n\n- âš›ï¸ **Federated Querying:** Supports federated querying of data lakes such as Hive, Iceberg, Hudi, and databases such as MySQL and Elasticsearch.\n\n- â© **Various Data Import Methods:** Supports batch import from HDFS/S3 and stream import from MySQL Binlog/Kafka; supports micro-batch writing through HTTP interface and real-time writing using Insert in JDBC.\n\n- ğŸš™ **Rich Ecology:** Spark uses Spark-Doris-Connector to read and write Doris; Flink-Doris-Connector enables Flink CDC to implement exactly-once data writing to Doris; DBT Doris Adapter is provided to transform data in Doris with DBT.\n\n## ğŸ™Œ Contributors\n\n**Apache Doris has graduated from Apache incubator successfully and become a Top-Level Project in June 2022**. \n\nWe deeply appreciate ğŸ”—[community contributors](https://github.com/apache/doris/graphs/contributors) for their contribution to Apache Doris.\n\n[![contrib graph](https://contrib.rocks/image?repo=apache/doris)](https://github.com/apache/doris/graphs/contributors)\n\n## ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Users\n\nApache Doris now has a wide user base in China and around the world, and as of today, **Apache Doris is used in production environments in thousands of companies worldwide.** More than 80% of the top 50 Internet companies in China in terms of market capitalization or valuation have been using Apache Doris for a long time, including Baidu, Meituan, Xiaomi, Jingdong, Bytedance, Tencent, NetEase, Kwai, Sina, 360, Mihoyo, and Ke Holdings. It is also widely used in some traditional industries such as finance, energy, manufacturing, and telecommunications.\n\nThe users of Apache Doris: ğŸ”—[Users](https://doris.apache.org/users)\n\nAdd your company logo at Apache Doris Website: ğŸ”—[Add Your Company](https://github.com/apache/doris/discussions/27683)\n \n## ğŸ‘£ Get Started\n\n### ğŸ“š Docs\n\nAll Documentation   ğŸ”—[Docs](https://doris.apache.org/docs/gettingStarted/what-is-apache-doris)  \n\n### â¬‡ï¸ Download \n\nAll release and binary version ğŸ”—[Download](https://doris.apache.org/download) \n\n### ğŸ—„ï¸ Compile\n\nSee how to compile  ğŸ”—[Compilation](https://doris.apache.org/community/source-install/compilation-with-docker))\n\n### ğŸ“® Install\n\nSee how to install and deploy ğŸ”—[Installation and deployment](https://doris.apache.org/docs/install/preparation/env-checking) \n\n## ğŸ§© Components\n\n### ğŸ“ Doris Connector\n\nDoris provides support for Spark/Flink to read data stored in Doris through Connector, and also supports to write data to Doris through Connector.\n\nğŸ”—[apache/doris-flink-connector](https://github.com/apache/doris-flink-connector)\n\nğŸ”—[apache/doris-spark-connector](https://github.com/apache/doris-spark-connector)\n\n\n## ğŸŒˆ Community and Support\n\n### ğŸ“¤ Subscribe Mailing Lists\n\nMail List is the most recognized form of communication in Apache community. See how to ğŸ”—[Subscribe Mailing Lists](https://doris.apache.org/community/subscribe-mail-list)\n\n### ğŸ™‹ Report Issues or Submit Pull Request\n\nIf you meet any questions, feel free to file a ğŸ”—[GitHub Issue](https://github.com/apache/doris/issues) or post it in ğŸ”—[GitHub Discussion](https://github.com/apache/doris/discussions) and fix it by submitting a ğŸ”—[Pull Request](https://github.com/apache/doris/pulls) \n\n### ğŸ» How to Contribute\n\nWe welcome your suggestions, comments (including criticisms), comments and contributions. See ğŸ”—[How to Contribute](https://doris.apache.org/community/how-to-contribute/) and ğŸ”—[Code Submission Guide](https://doris.apache.org/community/how-to-contribute/pull-request/)\n\n### âŒ¨ï¸ Doris Improvement Proposals (DSIP)\n\nğŸ”—[Doris Improvement Proposal (DSIP)](https://cwiki.apache.org/confluence/display/DORIS/Doris+Improvement+Proposals) can be thought of as **A Collection of Design Documents for all Major Feature Updates or Improvements**.\n\n### ğŸ”‘ Backend C++ Coding Specification\nğŸ”— [Backend C++ Coding Specification](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=240883637) should be strictly followed, which will help us achieve better code quality.\n\n## ğŸ’¬ Contact Us\n\nContact us through the following mailing list.\n\n| Name                                                                          | Scope                           |                                                                 |                                                                     |                                                                              |\n|:------------------------------------------------------------------------------|:--------------------------------|:----------------------------------------------------------------|:--------------------------------------------------------------------|:-----------------------------------------------------------------------------|\n| [dev@doris.apache.org](mailto:dev@doris.apache.org)     | Development-related discussions | [Subscribe](mailto:dev-subscribe@doris.apache.org)   | [Unsubscribe](mailto:dev-unsubscribe@doris.apache.org)   | [Archives](http://mail-archives.apache.org/mod_mbox/doris-dev/)   |\n\n## ğŸ§° Links\n\n* Apache Doris Official Website - [Site](https://doris.apache.org)\n* Developer Mailing list - <dev@doris.apache.org>. Mail to <dev-subscribe@doris.apache.org>, follow the reply to subscribe the mail list.\n* Slack channel - [Join the Slack](https://doris.apache.org/slack)\n* Twitter - [Follow @doris_apache](https://twitter.com/doris_apache)\n\n\n## ğŸ“œ License\n\n[Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n\n> **Note**\n> Some licenses of the third-party dependencies are not compatible with Apache 2.0 License. So you need to disable\nsome Doris features to be complied with Apache 2.0 License. For details, refer to the `thirdparty/LICENSE.txt`\n\n\n\n",
      "stars_today": 5
    },
    {
      "id": 261039251,
      "name": "swift-composable-architecture",
      "full_name": "pointfreeco/swift-composable-architecture",
      "description": "A library for building applications in a consistent and understandable way, with composition, testing, and ergonomics in mind.",
      "html_url": "https://github.com/pointfreeco/swift-composable-architecture",
      "stars": 14256,
      "forks": 1624,
      "language": "Swift",
      "topics": [
        "architecture",
        "composition",
        "modularity",
        "swiftui",
        "testability",
        "uikit"
      ],
      "created_at": "2020-05-03T23:18:40Z",
      "updated_at": "2026-01-14T15:38:32Z",
      "pushed_at": "2025-12-30T21:38:44Z",
      "open_issues": 23,
      "owner": {
        "login": "pointfreeco",
        "avatar_url": "https://avatars.githubusercontent.com/u/29466629?v=4"
      },
      "readme": "# The Composable Architecture\n\n[![CI](https://github.com/pointfreeco/swift-composable-architecture/actions/workflows/ci.yml/badge.svg)](https://github.com/pointfreeco/swift-composable-architecture/actions/workflows/ci.yml)\n[![Slack](https://img.shields.io/badge/slack-chat-informational.svg?label=Slack&logo=slack)](https://www.pointfree.co/slack-invite)\n[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fpointfreeco%2Fswift-composable-architecture%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/pointfreeco/swift-composable-architecture)\n[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fpointfreeco%2Fswift-composable-architecture%2Fbadge%3Ftype%3Dplatforms)](https://swiftpackageindex.com/pointfreeco/swift-composable-architecture)\n\nThe Composable Architecture (TCA, for short) is a library for building applications in a consistent \nand understandable way, with composition, testing, and ergonomics in mind. It can be used in \nSwiftUI, UIKit, and more, and on any Apple platform (iOS, macOS, iPadOS, visionOS, tvOS, and watchOS).\n\n* [What is the Composable Architecture?](#what-is-the-composable-architecture)\n* [Learn more](#learn-more)\n* [Examples](#examples)\n* [Basic usage](#basic-usage)\n* [Documentation](#documentation)\n* [FAQ](#faq)\n* [Community](#community)\n* [Installation](#installation)\n* [Translations](#translations)\n\n## What is the Composable Architecture?\n\nThis library provides a few core tools that can be used to build applications of varying purpose and \ncomplexity. It provides compelling stories that you can follow to solve many problems you encounter \nday-to-day when building applications, such as:\n\n* **State management**\n  <br> How to manage the state of your application using simple value types, and share state across \n  many screens so that mutations in one screen can be immediately observed in another screen.\n\n* **Composition**\n  <br> How to break down large features into smaller components that can be extracted to their own, \n  isolated modules and be easily glued back together to form the feature.\n\n* **Side effects**\n  <br> How to let certain parts of the application talk to the outside world in the most testable \n  and understandable way possible.\n\n* **Testing**\n  <br> How to not only test a feature built in the architecture, but also write integration tests \n  for features that have been composed of many parts, and write end-to-end tests to understand how \n  side effects influence your application. This allows you to make strong guarantees that your \n  business logic is running in the way you expect.\n\n* **Ergonomics**\n  <br> How to accomplish all of the above in a simple API with as few concepts and moving parts as \n  possible.\n\n## Learn More\n\nThe Composable Architecture was designed over the course of many episodes on \n[Point-Free][pointfreeco], a video series exploring advanced programming topics in the Swift language, \nhosted by [Brandon Williams][mbrandonw] and [Stephen Celis][stephencelis].\n\nYou can watch all of the episodes [here][tca-episode-collection], as well as a dedicated, [multipart\ntour][tca-tour] of the architecture from scratch.\n\n<a href=\"https://www.pointfree.co/collections/tours/composable-architecture-1-0\">\n  <img alt=\"video poster image\" src=\"https://d3rccdn33rt8ze.cloudfront.net/episodes/0243.jpeg\" width=\"600\">\n</a>\n\n## Examples\n\n[![Screen shots of example applications](https://d3rccdn33rt8ze.cloudfront.net/composable-architecture/demos.png)](./Examples)\n\nThis repo comes with _lots_ of examples to demonstrate how to solve common and complex problems with \nthe Composable Architecture. Check out [this](./Examples) directory to see them all, including:\n\n* [Case Studies](./Examples/CaseStudies)\n  * Getting started\n  * Effects\n  * Navigation\n  * Higher-order reducers\n  * Reusable components\n* [Location manager](https://github.com/pointfreeco/composable-core-location/tree/main/Examples/LocationManager)\n* [Motion manager](https://github.com/pointfreeco/composable-core-motion/tree/main/Examples/MotionManager)\n* [Search](./Examples/Search)\n* [Speech Recognition](./Examples/SpeechRecognition)\n* [SyncUps app](./Examples/SyncUps)\n* [Tic-Tac-Toe](./Examples/TicTacToe)\n* [Todos](./Examples/Todos)\n* [Voice memos](./Examples/VoiceMemos)\n\nLooking for something more substantial? Check out the source code for [isowords][gh-isowords], an \niOS word search game built in SwiftUI and the Composable Architecture.\n\n## Basic Usage\n\n> [!Note] \n> For a step-by-step interactive tutorial, be sure to check out [Meet the Composable\n> Architecture][meet-tca].\n\nTo build a feature using the Composable Architecture you define some types and values that model \nyour domain:\n\n* **State**: A type that describes the data your feature needs to perform its logic and render its \nUI.\n* **Action**: A type that represents all of the actions that can happen in your feature, such as \nuser actions, notifications, event sources and more.\n* **Reducer**: A function that describes how to evolve the current state of the app to the next \nstate given an action. The reducer is also responsible for returning any effects that should be \nrun, such as API requests, which can be done by returning an `Effect` value.\n* **Store**: The runtime that actually drives your feature. You send all user actions to the store \nso that the store can run the reducer and effects, and you can observe state changes in the store \nso that you can update UI.\n\nThe benefits of doing this are that you will instantly unlock testability of your feature, and you \nwill be able to break large, complex features into smaller domains that can be glued together.\n\nAs a basic example, consider a UI that shows a number along with \"+\" and \"âˆ’\" buttons that increment \nand decrement the number. To make things interesting, suppose there is also a button that when \ntapped makes an API request to fetch a random fact about that number and displays it in the view.\n\nTo implement this feature we create a new type that will house the domain and behavior of the \nfeature, and it will be annotated with the `@Reducer` macro:\n\n```swift\nimport ComposableArchitecture\n\n@Reducer\nstruct Feature {\n}\n```\n\nIn here we need to define a type for the feature's state, which consists of an integer for the \ncurrent count, as well as an optional string that represents the fact being presented:\n\n```swift\n@Reducer\nstruct Feature {\n  @ObservableState\n  struct State: Equatable {\n    var count = 0\n    var numberFact: String?\n  }\n}\n```\n\n> [!Note] \n> We've applied the `@ObservableState` macro to `State` in order to take advantage of the\n> observation tools in the library.\n\nWe also need to define a type for the feature's actions. There are the obvious actions, such as \ntapping the decrement button, increment button, or fact button. But there are also some slightly \nnon-obvious ones, such as the action that occurs when we receive a response from the fact API \nrequest:\n\n```swift\n@Reducer\nstruct Feature {\n  @ObservableState\n  struct State: Equatable { /* ... */ }\n  enum Action {\n    case decrementButtonTapped\n    case incrementButtonTapped\n    case numberFactButtonTapped\n    case numberFactResponse(String)\n  }\n}\n```\n\nAnd then we implement the `body` property, which is responsible for composing the actual logic and \nbehavior for the feature. In it we can use the `Reduce` reducer to describe how to change the\ncurrent state to the next state, and what effects need to be executed. Some actions don't need to\nexecute effects, and they can return `.none` to represent that:\n\n```swift\n@Reducer\nstruct Feature {\n  @ObservableState\n  struct State: Equatable { /* ... */ }\n  enum Action { /* ... */ }\n\n  var body: some Reducer<State, Action> {\n    Reduce { state, action in\n      switch action {\n      case .decrementButtonTapped:\n        state.count -= 1\n        return .none\n\n      case .incrementButtonTapped:\n        state.count += 1\n        return .none\n\n      case .numberFactButtonTapped:\n        return .run { [count = state.count] send in\n          let (data, _) = try await URLSession.shared.data(\n            from: URL(string: \"http://numbersapi.com/\\(count)/trivia\")!\n          )\n          await send(\n            .numberFactResponse(String(decoding: data, as: UTF8.self))\n          )\n        }\n\n      case let .numberFactResponse(fact):\n        state.numberFact = fact\n        return .none\n      }\n    }\n  }\n}\n```\n\nAnd then finally we define the view that displays the feature. It holds onto a `StoreOf<Feature>` \nso that it can observe all changes to the state and re-render, and we can send all user actions to \nthe store so that state changes:\n\n```swift\nstruct FeatureView: View {\n  let store: StoreOf<Feature>\n\n  var body: some View {\n    Form {\n      Section {\n        Text(\"\\(store.count)\")\n        Button(\"Decrement\") { store.send(.decrementButtonTapped) }\n        Button(\"Increment\") { store.send(.incrementButtonTapped) }\n      }\n\n      Section {\n        Button(\"Number fact\") { store.send(.numberFactButtonTapped) }\n      }\n      \n      if let fact = store.numberFact {\n        Text(fact)\n      }\n    }\n  }\n}\n```\n\nIt is also straightforward to have a UIKit controller driven off of this store. You can observe\nstate changes in the store in `viewDidLoad`, and then populate the UI components with data from\nthe store. The code is a bit longer than the SwiftUI version, so we have collapsed it here:\n\n<details>\n  <summary>Click to expand!</summary>\n\n  ```swift\n  class FeatureViewController: UIViewController {\n    let store: StoreOf<Feature>\n\n    init(store: StoreOf<Feature>) {\n      self.store = store\n      super.init(nibName: nil, bundle: nil)\n    }\n\n    required init?(coder: NSCoder) {\n      fatalError(\"init(coder:) has not been implemented\")\n    }\n\n    override func viewDidLoad() {\n      super.viewDidLoad()\n\n      let countLabel = UILabel()\n      let decrementButton = UIButton()\n      let incrementButton = UIButton()\n      let factLabel = UILabel()\n      \n      // Omitted: Add subviews and set up constraints...\n      \n      observe { [weak self] in\n        guard let self \n        else { return }\n        \n        countLabel.text = \"\\(self.store.count)\"\n        factLabel.text = self.store.numberFact\n      }\n    }\n\n    @objc private func incrementButtonTapped() {\n      self.store.send(.incrementButtonTapped)\n    }\n    @objc private func decrementButtonTapped() {\n      self.store.send(.decrementButtonTapped)\n    }\n    @objc private func factButtonTapped() {\n      self.store.send(.numberFactButtonTapped)\n    }\n  }\n  ```\n</details>\n\nOnce we are ready to display this view, for example in the app's entry point, we can construct a \nstore. This can be done by specifying the initial state to start the application in, as well as \nthe reducer that will power the application:\n\n```swift\nimport ComposableArchitecture\n\n@main\nstruct MyApp: App {\n  var body: some Scene {\n    WindowGroup {\n      FeatureView(\n        store: Store(initialState: Feature.State()) {\n          Feature()\n        }\n      )\n    }\n  }\n}\n```\n\nAnd that is enough to get something on the screen to play around with. It's definitely a few more \nsteps than if you were to do this in a vanilla SwiftUI way, but there are a few benefits. It gives \nus a consistent manner to apply state mutations, instead of scattering logic in some observable \nobjects and in various action closures of UI components. It also gives us a concise way of \nexpressing side effects. And we can immediately test this logic, including the effects, without \ndoing much additional work.\n\n### Testing\n\n> [!Note] \n> For more in-depth information on testing, see the dedicated [testing][testing-article] article. \n\nTo test use a `TestStore`, which can be created with the same information as the `Store`, but it \ndoes extra work to allow you to assert how your feature evolves as actions are sent:\n\n```swift\n@Test\nfunc basics() async {\n  let store = TestStore(initialState: Feature.State()) {\n    Feature()\n  }\n}\n```\n\nOnce the test store is created we can use it to make an assertion of an entire user flow of steps. \nEach step of the way we need to prove that state changed how we expect. For example, we can \nsimulate the user flow of tapping on the increment and decrement buttons:\n\n```swift\n// Test that tapping on the increment/decrement buttons changes the count\nawait store.send(.incrementButtonTapped) {\n  $0.count = 1\n}\nawait store.send(.decrementButtonTapped) {\n  $0.count = 0\n}\n```\n\nFurther, if a step causes an effect to be executed, which feeds data back into the store, we must \nassert on that. For example, if we simulate the user tapping on the fact button we expect to \nreceive a fact response back with the fact, which then causes the `numberFact` state to be \npopulated:\n\n```swift\nawait store.send(.numberFactButtonTapped)\n\nawait store.receive(\\.numberFactResponse) {\n  $0.numberFact = ???\n}\n```\n\nHowever, how do we know what fact is going to be sent back to us?\n\nCurrently our reducer is using an effect that reaches out into the real world to hit an API server, \nand that means we have no way to control its behavior. We are at the whims of our internet \nconnectivity and the availability of the API server in order to write this test.\n\nIt would be better for this dependency to be passed to the reducer so that we can use a live \ndependency when running the application on a device, but use a mocked dependency for tests. We can \ndo this by adding a property to the `Feature` reducer:\n\n```swift\n@Reducer\nstruct Feature {\n  let numberFact: (Int) async throws -> String\n  // ...\n}\n```\n\nThen we can use it in the `reduce` implementation:\n\n```swift\ncase .numberFactButtonTapped:\n  return .run { [count = state.count] send in \n    let fact = try await self.numberFact(count)\n    await send(.numberFactResponse(fact))\n  }\n```\n\nAnd in the entry point of the application we can provide a version of the dependency that actually \ninteracts with the real world API server:\n\n```swift\n@main\nstruct MyApp: App {\n  var body: some Scene {\n    WindowGroup {\n      FeatureView(\n        store: Store(initialState: Feature.State()) {\n          Feature(\n            numberFact: { number in\n              let (data, _) = try await URLSession.shared.data(\n                from: URL(string: \"http://numbersapi.com/\\(number)\")!\n              )\n              return String(decoding: data, as: UTF8.self)\n            }\n          )\n        }\n      )\n    }\n  }\n}\n```\n\nBut in tests we can use a mock dependency that immediately returns a deterministic, predictable \nfact: \n\n```swift\n@Test\nfunc basics() async {\n  let store = TestStore(initialState: Feature.State()) {\n    Feature(numberFact: { \"\\($0) is a good number Brent\" })\n  }\n}\n```\n\nWith that little bit of upfront work we can finish the test by simulating the user tapping on the \nfact button, and then receiving the response from the dependency to present the fact:\n\n```swift\nawait store.send(.numberFactButtonTapped)\n\nawait store.receive(\\.numberFactResponse) {\n  $0.numberFact = \"0 is a good number Brent\"\n}\n```\n\nWe can also improve the ergonomics of using the `numberFact` dependency in our application. Over \ntime the application may evolve into many features, and some of those features may also want access \nto `numberFact`, and explicitly passing it through all layers can get annoying. There is a process \nyou can follow to â€œregisterâ€ dependencies with the library, making them instantly available to any \nlayer in the application.\n\n> [!Note] \n> For more in-depth information on dependency management, see the dedicated\n> [dependencies][dependencies-article] article. \n\nWe can start by wrapping the number fact functionality in a new type:\n\n```swift\nstruct NumberFactClient {\n  var fetch: (Int) async throws -> String\n}\n```\n\nAnd then registering that type with the dependency management system by conforming the client to\nthe `DependencyKey` protocol, which requires you to specify the live value to use when running the\napplication in simulators or devices:\n\n```swift\nextension NumberFactClient: DependencyKey {\n  static let liveValue = Self(\n    fetch: { number in\n      let (data, _) = try await URLSession.shared\n        .data(from: URL(string: \"http://numbersapi.com/\\(number)\")!\n      )\n      return String(decoding: data, as: UTF8.self)\n    }\n  )\n}\n\nextension DependencyValues {\n  var numberFact: NumberFactClient {\n    get { self[NumberFactClient.self] }\n    set { self[NumberFactClient.self] = newValue }\n  }\n}\n```\n\nWith that little bit of upfront work done you can instantly start making use of the dependency in \nany feature by using the `@Dependency` property wrapper:\n\n```diff\n @Reducer\n struct Feature {\n-  let numberFact: (Int) async throws -> String\n+  @Dependency(\\.numberFact) var numberFact\n   \n   â€¦\n\n-  try await self.numberFact(count)\n+  try await self.numberFact.fetch(count)\n }\n```\n\nThis code works exactly as it did before, but you no longer have to explicitly pass the dependency \nwhen constructing the feature's reducer. When running the app in previews, the simulator or on a \ndevice, the live dependency will be provided to the reducer, and in tests the test dependency will \nbe provided.\n\nThis means the entry point to the application no longer needs to construct dependencies:\n\n```swift\n@main\nstruct MyApp: App {\n  var body: some Scene {\n    WindowGroup {\n      FeatureView(\n        store: Store(initialState: Feature.State()) {\n          Feature()\n        }\n      )\n    }\n  }\n}\n```\n\nAnd the test store can be constructed without specifying any dependencies, but you can still \noverride any dependency you need to for the purpose of the test:\n\n```swift\nlet store = TestStore(initialState: Feature.State()) {\n  Feature()\n} withDependencies: {\n  $0.numberFact.fetch = { \"\\($0) is a good number Brent\" }\n}\n\n// ...\n```\n\nThat is the basics of building and testing a feature in the Composable Architecture. There are \n_a lot_ more things to be explored, such as composition, modularity, adaptability, and complex \neffects. The [Examples](./Examples) directory has a bunch of projects to explore to see more \nadvanced usages.\n\n## Documentation\n\nThe documentation for releases and `main` are available here:\n\n* [`main`](https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture)\n* [1.x.x](https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/~/documentation/composablearchitecture)\n\nThere are a number of articles in the documentation that you may find helpful as you become more \ncomfortable with the library:\n\n* [Getting started][getting-started-article]\n* [Dependencies][dependencies-article]\n* [Testing][testing-article]\n* [Navigation][navigation-article]\n* [Sharing state][sharing-state-article]\n* [Performance][performance-article]\n* [Concurrency][concurrency-article]\n* [Bindings][bindings-article]\n\n## FAQ\n\nWe have a [dedicated article][faq-article] for all of the most frequently asked questions and\ncomments people have concerning the library.\n\n## Community\n\nIf you want to discuss the Composable Architecture or have a question about how to use it to solve \na particular problem, there are a number of places you can discuss with fellow \n[Point-Free](http://www.pointfree.co) enthusiasts:\n\n* For long-form discussions, we recommend the [discussions][gh-discussions] tab of this repo.\n* For casual chat, we recommend the [Point-Free Community slack](http://pointfree.co/slack-invite).\n\n## Installation\n\nYou can add ComposableArchitecture to an Xcode project by adding it as a package dependency.\n\n  1. From the **File** menu, select **Add Package Dependencies...**\n  2. Enter \"https://github.com/pointfreeco/swift-composable-architecture\" into the package \n     repository URL text field\n  3. Depending on how your project is structured:\n      - If you have a single application target that needs access to the library, then add \n        **ComposableArchitecture** directly to your application.\n      - If you want to use this library from multiple Xcode targets, or mix Xcode targets and SPM \n        targets, you must create a shared framework that depends on **ComposableArchitecture** and \n        then depend on that framework in all of your targets. For an example of this, check out the \n        [Tic-Tac-Toe](./Examples/TicTacToe) demo application, which splits lots of features into \n        modules and consumes the static library in this fashion using the **tic-tac-toe** Swift \n        package.\n\n## Companion libraries\n\nThe Composable Architecture is built with extensibility in mind, and there are a number of\ncommunity-supported libraries available to enhance your applications:\n\n* [Composable Architecture Extras](https://github.com/Ryu0118/swift-composable-architecture-extras):\n  A companion library to the Composable Architecture.\n* [TCAComposer](https://github.com/mentalflux/tca-composer): A macro framework for generating\n  boiler-plate code in the Composable Architecture.\n* [TCACoordinators](https://github.com/johnpatrickmorgan/TCACoordinators): The coordinator pattern\n  in the Composable Architecture.\n\nIf you'd like to contribute a library, please [open a\nPR](https://github.com/pointfreeco/swift-composable-architecture/edit/main/README.md) with a link\nto it!\n\n## Translations\n\nThe following translations of this README have been contributed by members of the community:\n\n* [Arabic](https://gist.github.com/NorhanBoghdadi/1b98d55c02b683ddef7e05c2ebcccd47)\n* [French](https://gist.github.com/nikitamounier/0e93eb832cf389db12f9a69da030a2dc)\n* [Hindi](https://gist.github.com/akashsoni01/b358ee0b3b747167964ef6946123c88d)\n* [Indonesian](https://gist.github.com/wendyliga/792ea9ac5cc887f59de70a9e39cc7343)\n* [Italian](https://gist.github.com/Bellaposa/5114e6d4d55fdb1388e8186886d48958)\n* [Japanese](https://gist.github.com/Achoo-kr/2d0712deb77f78b3379551ac7baea3e4)\n* [Korean](https://gist.github.com/Achoo-kr/5d8936d12e71028fcc4a7c5e078ca038)\n* [Polish](https://gist.github.com/MarcelStarczyk/6b6153051f46912a665c32199f0d1d54)\n* [Portuguese](https://gist.github.com/SevioCorrea/2bbf337cd084a58c89f2f7f370626dc8)\n* [Russian](https://gist.github.com/SubvertDev/3317d0c3b35ed601be330d6fc0df5aba)\n* [Simplified Chinese](https://gist.github.com/sh3l6orrr/10c8f7c634a892a9c37214f3211242ad)\n* [Spanish](https://gist.github.com/pitt500/f5e32fccb575ce112ffea2827c7bf942)\n* [Turkish](https://gist.github.com/gokhanamal/93001244ef0c1cec58abeb1afc0de37c)\n* [Ukrainian](https://gist.github.com/barabashd/33b64676195ce41f4bb73c327ea512a8)\n\nIf you'd like to contribute a translation, please [open a\nPR](https://github.com/pointfreeco/swift-composable-architecture/edit/main/README.md) with a link \nto a [Gist](https://gist.github.com)!\n\n## Credits and thanks\n\nThe following people gave feedback on the library at its early stages and helped make the library \nwhat it is today:\n\nPaul Colton, Kaan Dedeoglu, Matt Diephouse, Josef DoleÅ¾al, Eimantas, Matthew Johnson, George \nKaimakas, Nikita Leonov, Christopher Liscio, Jeffrey Macko, Alejandro Martinez, Shai Mishali, Willis \nPlummer, Simon-Pierre Roy, Justin Price, Sven A. Schmidt, Kyle Sherman, Petr Å Ã­ma, Jasdev Singh, \nMaxim Smirnov, Ryan Stone, Daniel Hollis Tavares, and all of the [Point-Free][pointfreeco] \nsubscribers ğŸ˜.\n\nSpecial thanks to [Chris Liscio](https://twitter.com/liscio) who helped us work through many strange \nSwiftUI quirks and helped refine the final API.\n\nAnd thanks to [Shai Mishali](https://github.com/freak4pc) and the\n[CombineCommunity](https://github.com/CombineCommunity/CombineExt/) project, from which we took \ntheir implementation of `Publishers.Create`, which we use in `Effect` to help bridge delegate and \ncallback-based APIs, making it much easier to interface with 3rd party frameworks.\n\n## Other libraries\n\nThe Composable Architecture was built on a foundation of ideas started by other libraries, in \nparticular [Elm](https://elm-lang.org) and [Redux](https://redux.js.org/).\n\nThere are also many architecture libraries in the Swift and iOS community. Each one of these has \ntheir own set of priorities and trade-offs that differ from the Composable Architecture.\n\n* [RIBs](https://github.com/uber/RIBs)\n* [Loop](https://github.com/ReactiveCocoa/Loop)\n* [ReSwift](https://github.com/ReSwift/ReSwift)\n* [Workflow](https://github.com/square/workflow)\n* [ReactorKit](https://github.com/ReactorKit/ReactorKit)\n* [RxFeedback](https://github.com/NoTests/RxFeedback.swift)\n* [Mobius.swift](https://github.com/spotify/mobius.swift)\n* <details>\n  <summary>And more</summary>\n\n  * [Fluxor](https://github.com/FluxorOrg/Fluxor)\n  * [PromisedArchitectureKit](https://github.com/RPallas92/PromisedArchitectureKit)\n  </details>\n\n## License\n\nThis library is released under the MIT license. See [LICENSE](LICENSE) for details.\n\n[pointfreeco]: https://www.pointfree.co\n[mbrandonw]: https://twitter.com/mbrandonw\n[stephencelis]: https://twitter.com/stephencelis\n[tca-episode-collection]: https://www.pointfree.co/collections/composable-architecture\n[tca-tour]: https://www.pointfree.co/collections/tours/composable-architecture-1-0\n[gh-isowords]: https://github.com/pointfreeco/isowords\n[gh-discussions]: https://github.com/pointfreeco/swift-composable-architecture/discussions\n[swift-forum]: https://forums.swift.org/c/related-projects/swift-composable-architecture\n[testing-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/testingtca\n[faq-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/faq\n[dependencies-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/dependencymanagement\n[getting-started-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/gettingstarted\n[navigation-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/navigation\n[performance-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/performance\n[concurrency-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/swiftconcurrency\n[bindings-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/bindings\n[sharing-state-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/sharingstate\n[meet-tca]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/tutorials/meetcomposablearchitecture\n",
      "stars_today": 5
    },
    {
      "id": 97716052,
      "name": "XcodeGen",
      "full_name": "yonaskolb/XcodeGen",
      "description": "A Swift command line tool for generating your Xcode project",
      "html_url": "https://github.com/yonaskolb/XcodeGen",
      "stars": 7980,
      "forks": 868,
      "language": "Swift",
      "topics": [
        "ci",
        "cli",
        "generator",
        "specification",
        "swift",
        "xcode",
        "xcodeproj",
        "xcodeproject",
        "yaml"
      ],
      "created_at": "2017-07-19T12:56:04Z",
      "updated_at": "2026-01-14T23:13:12Z",
      "pushed_at": "2025-07-25T03:27:54Z",
      "open_issues": 388,
      "owner": {
        "login": "yonaskolb",
        "avatar_url": "https://avatars.githubusercontent.com/u/2393781?v=4"
      },
      "readme": "<p align=\"center\">\n<a href=\"https://github.com/yonaskolb/XcodeGen\">\n<img src=\"Assets/Logo_animated.gif\" alt=\"XcodeGen\" />\n</a>\n</p>\n<p align=\"center\">\n  <a href=\"https://github.com/yonaskolb/XcodeGen/releases\">\n    <img src=\"https://img.shields.io/github/release/yonaskolb/xcodegen.svg\"/>\n  </a>\n  <a href=\"https://swiftpackageindex.com/yonaskolb/XcodeGen\">\n    <img src=\"https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fyonaskolb%2FXcodeGen%2Fbadge%3Ftype%3Dplatforms\" alt=\"Swift Package Manager Platforms\" />\n  </a>\n  <a href=\"https://swiftpackageindex.com/yonaskolb/XcodeGen\">\n    <img src=\"https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fyonaskolb%2FXcodeGen%2Fbadge%3Ftype%3Dswift-versions\" alt=\"Swift Versions\" />\n  </a>\n  <a href=\"https://github.com/yonaskolb/XcodeGen/blob/master/LICENSE\">\n    <img src=\"https://img.shields.io/github/license/yonaskolb/XcodeGen.svg\"/>\n  </a>\n</p>\n\n# XcodeGen\n\nXcodeGen is a command line tool written in Swift that generates your Xcode project using your folder structure and a project spec.\n\nThe project spec is a YAML or JSON file that defines your targets, configurations, schemes, custom build settings and many other options. All your source directories are automatically parsed and referenced appropriately while preserving your folder structure. Sensible defaults are used in many places, so you only need to customize what is needed. Very complex projects can also be defined using more advanced features.\n\n- âœ… Generate projects on demand and remove your `.xcodeproj` from git, which means **no more merge conflicts**!\n- âœ… Groups and files in Xcode are always **synced** to your directories on disk\n- âœ… Easy **configuration** of projects which is human readable and git friendly\n- âœ… Easily copy and paste **files and directories** without having to edit anything in Xcode\n- âœ… Share build settings across multiple targets with **build setting groups**\n- âœ… Automatically generate Schemes for **different environments** like test and production\n- âœ… Easily **create new projects** with complicated setups on demand without messing around with Xcode\n- âœ… Generate from anywhere including on **CI**\n- âœ… Distribute your spec amongst multiple files for easy **sharing** and overriding\n- âœ… Easily create **multi-platform** frameworks\n- âœ… Integrate **Carthage** frameworks without any work\n\nGiven an example project spec:\n\n```yaml\nname: MyProject\ninclude:\n  - base_spec.yml\noptions:\n  bundleIdPrefix: com.myapp\npackages:\n  Yams:\n    url: https://github.com/jpsim/Yams\n    from: 2.0.0\ntargets:\n  MyApp:\n    type: application\n    platform: iOS\n    deploymentTarget: \"10.0\"\n    sources: [MyApp]\n    settings:\n      configs:\n        debug:\n          CUSTOM_BUILD_SETTING: my_debug_value\n        release:\n          CUSTOM_BUILD_SETTING: my_release_value\n    dependencies:\n      - target: MyFramework\n      - carthage: Alamofire\n      - framework: Vendor/MyFramework.framework\n      - sdk: Contacts.framework\n      - sdk: libc++.tbd\n      - package: Yams\n  MyFramework:\n    type: framework\n    platform: iOS\n    sources: [MyFramework]\n```\nA project would be created with 2 connected targets, with all the required configurations and build settings. See the [Project Spec](Docs/ProjectSpec.md) documentation for all the options you can specify, and [Usage](Docs/Usage.md) for more general documentation.\n\n## Installing\n\nMake sure the latest stable (non-beta) version of Xcode is installed first.\n\n### [Mint](https://github.com/yonaskolb/mint)\n```sh\nmint install yonaskolb/xcodegen\n```\n\n### Make\n\n```shell\ngit clone https://github.com/yonaskolb/XcodeGen.git\ncd XcodeGen\nmake install\n```\n\n### Homebrew\n\n```shell\nbrew install xcodegen\n```\n\n### Swift Package Manager\n\n**Use as CLI**\n\n```shell\ngit clone https://github.com/yonaskolb/XcodeGen.git\ncd XcodeGen\nswift run xcodegen\n```\n\n**Use as dependency**\n\nAdd the following to your Package.swift file's dependencies:\n\n```swift\n.package(url: \"https://github.com/yonaskolb/XcodeGen.git\", from: \"2.44.1\"),\n```\n\nAnd then import wherever needed: `import XcodeGenKit`\n\n## Usage\n\nSimply run:\n\n```shell\nxcodegen generate\n```\n\nThis will look for a project spec in the current directory called `project.yml` and generate an Xcode project with the name defined in the spec.\n\nOptions:\n\n- **--spec**: An optional path to a `.yml` or `.json` project spec. Defaults to `project.yml`. (It is also possible to link to multiple spec files by comma separating them. Note that all other flags will be the same.)\n- **--project**: An optional path to a directory where the project will be generated. By default this is the directory the spec lives in.\n- **--quiet**: Suppress informational and success messages.\n- **--use-cache**: Used to prevent unnecessarily generating the project. If this is set, then a cache file will be written to when a project is generated. If `xcodegen` is later run but the spec and all the files it contains are the same, the project won't be generated.\n- **--cache-path**: A custom path to use for your cache file. This defaults to `~/.xcodegen/cache/{PROJECT_SPEC_PATH_HASH}`\n\nThere are other commands as well such as `xcodegen dump` which lets one output the resolved spec in many different formats, or write it to a file. Use `xcodegen help` to see more detailed usage information.\n\n## Editing\n```shell\ngit clone https://github.com/yonaskolb/XcodeGen.git\ncd XcodeGen\nswift package generate-xcodeproj\n```\nThis uses Swift Package Manager to create an `xcodeproj` file that you can open, edit and run in Xcode, which makes editing any code easier.\n\nIf you want to pass any required arguments when running in Xcode, you can edit the scheme to include launch arguments.\n\n## Documentation\n- See [Project Spec](Docs/ProjectSpec.md) documentation for all the various properties and options that can be set\n- See [Usage](Docs/Usage.md) for more specific usage and use case documentation\n- See [FAQ](Docs/FAQ.md) for a list of some frequently asked questions\n- See [Examples](Docs/Examples.md) for some real world XcodeGen project specs out in the wild\n\n## Alternatives\nIf XcodeGen doesn't meet your needs try these great alternatives:\n- [Tuist](https://github.com/tuist/tuist)\n- [Xcake](https://github.com/igor-makarov/xcake)\n- [struct](https://github.com/workshop/struct)\n\n## Attributions\nThis tool is powered by:\n\n- [XcodeProj](https://github.com/tuist/XcodeProj)\n- [JSONUtilities](https://github.com/yonaskolb/JSONUtilities)\n- [Spectre](https://github.com/kylef/Spectre)\n- [PathKit](https://github.com/kylef/PathKit)\n- [Yams](https://github.com/jpsim/Yams)\n- [SwiftCLI](https://github.com/jakeheis/SwiftCLI)\n\nInspiration for this tool came from:\n\n- [struct](https://github.com/workshop/struct)\n- [Xcake](https://github.com/igor-makarov/xcake)\n- [CocoaPods Xcodeproj](https://github.com/CocoaPods/Xcodeproj)\n\n## Contributions\nPull requests and issues are always welcome. Please open any issues and PRs for bugs, features, or documentation.\n\n[![](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/images/0)](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/links/0)[![](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/images/1)](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/links/1)[![](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/images/2)](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/links/2)[![](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/images/3)](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/links/3)[![](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/images/4)](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/links/4)[![](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/images/5)](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/links/5)[![](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/images/6)](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/links/6)[![](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/images/7)](https://sourcerer.io/fame/yonaskolb/yonaskolb/XcodeGen/links/7)\n\n## License\n\nXcodeGen is licensed under the MIT license. See [LICENSE](LICENSE) for more info.\n",
      "stars_today": 5
    },
    {
      "id": 756673810,
      "name": "gemma.cpp",
      "full_name": "google/gemma.cpp",
      "description": "lightweight, standalone C++ inference engine for Google's Gemma models.",
      "html_url": "https://github.com/google/gemma.cpp",
      "stars": 6694,
      "forks": 589,
      "language": "C++",
      "topics": [],
      "created_at": "2024-02-13T04:36:03Z",
      "updated_at": "2026-01-15T00:37:31Z",
      "pushed_at": "2026-01-09T14:35:41Z",
      "open_issues": 35,
      "owner": {
        "login": "google",
        "avatar_url": "https://avatars.githubusercontent.com/u/1342004?v=4"
      },
      "readme": "# gemma.cpp\n\ngemma.cpp is a lightweight, standalone C++ inference engine for the Gemma\nfoundation models from Google.\n\nFor additional information about Gemma, see\n[ai.google.dev/gemma](https://ai.google.dev/gemma). Model weights, including\ngemma.cpp specific artifacts, are\n[available on kaggle](https://www.kaggle.com/models/google/gemma-2).\n\n## Who is this project for?\n\nModern LLM inference engines are sophisticated systems, often with bespoke\ncapabilities extending beyond traditional neural network runtimes. With this\ncomes opportunities for research and innovation through co-design of high level\nalgorithms and low-level computation. However, there is a gap between\ndeployment-oriented C++ inference runtimes, which are not designed for\nexperimentation, and Python-centric ML research frameworks, which abstract away\nlow-level computation through compilation.\n\ngemma.cpp provides a minimalist implementation of Gemma-2, Gemma-3, and\nPaliGemma-2 models, focusing on simplicity and directness rather than full\ngenerality. This is inspired by vertically-integrated model implementations such\nas [ggml](https://github.com/ggerganov/ggml),\n[llama.c](https://github.com/karpathy/llama2.c), and\n[llama.rs](https://github.com/srush/llama2.rs).\n\ngemma.cpp targets experimentation and research use cases. It is intended to be\nstraightforward to embed in other projects with minimal dependencies and also\neasily modifiable with a small ~2K LoC core implementation (along with ~4K LoC\nof supporting utilities). We use the [Google\nHighway](https://github.com/google/highway) Library to take advantage of\nportable SIMD for CPU inference.\n\nFor production-oriented edge deployments we recommend standard deployment\npathways using Python frameworks like JAX, Keras, PyTorch, and Transformers\n([all model variations here](https://www.kaggle.com/models/google/gemma)).\n\n## Contributing\n\nCommunity contributions large and small are welcome. See\n[DEVELOPERS.md](https://github.com/google/gemma.cpp/blob/main/DEVELOPERS.md)\nfor additional notes contributing developers and [join the discord by following\nthis invite link](https://discord.gg/H5jCBAWxAe). This project follows\n[Google's Open Source Community\nGuidelines](https://opensource.google.com/conduct/).\n\n> [!NOTE] Active development is currently done on the `dev` branch. Please open\n> pull requests targeting `dev` branch instead of `main`, which is intended to\n> be more stable.\n\n## What's inside?\n\n-   LLM\n\n    -   CPU-only inference for: Gemma 2-3, PaliGemma 2.\n    -   Sampling with TopK and temperature.\n    -   Backward pass (VJP) and Adam optimizer for Gemma research.\n\n-   Optimizations\n\n    -   Mixed-precision (fp8, bf16, fp32, fp64 bit) GEMM:\n        -   Designed for BF16 instructions, can efficiently emulate them.\n        -   Automatic runtime autotuning 7 parameters per matrix shape.\n    -   Weight compression integrated directly into GEMM:\n        -   Custom fp8 format with 2..3 mantissa bits; tensor scaling.\n        -   Also bf16, f32 and non-uniform 4-bit (NUQ); easy to add new formats.\n\n-   Infrastructure\n\n    -   SIMD: single implementation via Highway. Chooses ISA at runtime.\n    -   Tensor parallelism: CCX-aware, multi-socket thread pool.\n    -   Disk I/O: memory map or parallel read (heuristic with user override).\n    -   Custom format with forward/backward-compatible metadata serialization.\n    -   Model conversion from Safetensors, not yet open sourced.\n    -   Portability: Linux, Windows/OS X supported. CMake/Bazel. 'Any' CPU.\n\n-   Frontends\n\n    -   C++ APIs with streaming for single query and batched inference.\n    -   Basic interactive command-line app.\n    -   Basic Python bindings (pybind11).\n\n## Quick Start\n\n### System requirements\n\nBefore starting, you should have installed:\n\n- [CMake](https://cmake.org/)\n- [Clang C++ compiler](https://clang.llvm.org/get_started.html), supporting at\n  least C++17.\n- `tar` for extracting archives from Kaggle.\n\nBuilding natively on Windows requires the Visual Studio 2012 Build Tools with the\noptional Clang/LLVM C++ frontend (`clang-cl`). This can be installed from the\ncommand line with\n[`winget`](https://learn.microsoft.com/en-us/windows/package-manager/winget/):\n\n```sh\nwinget install --id Kitware.CMake\nwinget install --id Microsoft.VisualStudio.2022.BuildTools --force --override \"--passive --wait --add Microsoft.VisualStudio.Workload.VCTools;installRecommended --add Microsoft.VisualStudio.Component.VC.Llvm.Clang --add Microsoft.VisualStudio.Component.VC.Llvm.ClangToolset\"\n```\n\n### Step 1: Obtain model weights and tokenizer from Kaggle or Hugging Face Hub\n\nVisit the\n[Kaggle page for Gemma-2](https://www.kaggle.com/models/google/gemma-2/gemmaCpp)\nand select `Model Variations |> Gemma C++`.\n\nOn this tab, the `Variation` dropdown includes the options below. Note bfloat16\nweights are higher fidelity, while 8-bit switched floating point weights enable\nfaster inference. In general, we recommend starting with the `-sfp` checkpoints.\n\n> [!NOTE] **Important**: We strongly recommend starting off with the\n> `gemma2-2b-it-sfp` model to get up and running.\n\nGemma 2 models are named `gemma2-2b-it` for 2B and `9b-it` or `27b-it`. See the\n`ModelPrefix` function in `configs.cc`.\n\n### Step 2: Extract Files\n\nAfter filling out the consent form, the download should proceed to retrieve a\ntar archive file `archive.tar.gz`. Extract files from `archive.tar.gz` (this can\ntake a few minutes):\n\n```\ntar -xf archive.tar.gz\n```\n\nThis should produce a file containing model weights such as `2b-it-sfp.sbs` and\na tokenizer file (`tokenizer.spm`). You may want to move these files to a\nconvenient directory location (e.g. the `build/` directory in this repo).\n\n### Step 3: Build\n\nThe build system uses [CMake](https://cmake.org/). To build the gemma inference\nruntime, create a build directory and generate the build files using `cmake`\nfrom the top-level project directory. Note if you previous ran `cmake` and are\nre-running with a different setting, be sure to delete all files in the `build/`\ndirectory with `rm -rf build/*`.\n\n#### Unix-like Platforms\n```sh\ncmake -B build\n```\n\nAfter running `cmake`, you can enter the `build/` directory and run `make` to\nbuild the `./gemma` executable:\n\n```sh\n# Configure `build` directory\ncmake --preset make\n\n# Build project using make\ncmake --build --preset make -j [number of parallel threads to use]\n```\n\nReplace `[number of parallel threads to use]` with a number - the number of\ncores available on your system is a reasonable heuristic. For example, `make -j4\ngemma` will build using 4 threads. If the `nproc` command is available, you can\nuse `make -j$(nproc) gemma` as a reasonable default for the number of threads.\n\nIf you aren't sure of the right value for the `-j` flag, you can simply run\n`make gemma` instead and it should still build the `./gemma` executable.\n\n> [!NOTE]\n> On Windows Subsystem for Linux (WSL) users should set the number of\n> parallel threads to 1. Using a larger number may result in errors.\n\nIf the build is successful, you should now have a `gemma` executable in the\n`build/` directory.\n\n#### Windows\n\n```sh\n# Configure `build` directory\ncmake --preset windows\n\n# Build project using Visual Studio Build Tools\ncmake --build --preset windows -j [number of parallel threads to use]\n```\n\nIf the build is successful, you should now have a `gemma.exe` executable in the\n`build/` directory.\n\n#### Bazel\n\n```sh\nbazel build -c opt --cxxopt=-std=c++20 :gemma\n```\n\nIf the build is successful, you should now have a `gemma` executable in the\n`bazel-bin/` directory.\n\n#### Make\n\nIf you prefer Makefiles, @jart has made one available here:\n\nhttps://github.com/jart/gemma3/blob/main/Makefile\n\n### Step 4: Run\n\nYou can now run `gemma` from inside the `build/` directory.\n\n`gemma` has the following required arguments:\n\nArgument      | Description                  | Example value\n------------- | ---------------------------- | ---------------\n`--weights`   | The compressed weights file. | `2b-it-sfp.sbs`\n`--tokenizer` | The tokenizer file.          | `tokenizer.spm`\n\nExample invocation for the following configuration:\n\n-   weights file `gemma2-2b-it-sfp.sbs` (Gemma2 2B instruction-tuned model,\n    8-bit switched floating point).\n-   Tokenizer file `tokenizer.spm` (can omit for single-format weights files\n    created after 2025-05-06, or output by migrate_weights.cc).\n\n```sh\n./gemma \\\n--tokenizer tokenizer.spm --weights gemma2-2b-it-sfp.sbs\n```\n\n### PaliGemma Vision-Language Model\n\nThis repository includes a version of the PaliGemma 2 VLM\n([paper](https://arxiv.org/abs/2412.03555)). We provide a C++ implementation of\nthe PaliGemma 2 model here.\n\nTo use the version of PaliGemma included in this repository, build the gemma\nbinary as noted above in Step 3. Download the compressed weights and tokenizer\nfrom\n[Kaggle](https://www.kaggle.com/models/google/paligemma-2/gemmaCpp/paligemma2-3b-mix-224)\nand run the binary as follows:\n\n```sh\n./gemma \\\n--tokenizer paligemma_tokenizer.model \\\n--weights paligemma2-3b-mix-224-sfp.sbs \\\n--image_file paligemma/testdata/image.ppm\n```\n\nNote that the image reading code is very basic to avoid depending on an image\nprocessing library for now. We currently only support reading binary PPMs (P6).\nSo use a tool like `convert` to first convert your images into that format, e.g.\n\n`convert image.jpeg -resize 224x224^ image.ppm`\n\n(As the image will be resized for processing anyway, we can already resize at\nthis stage for slightly faster loading.)\n\nThe interaction with the image (using the mix-224 checkpoint) may then look\nsomething like this:\n\n```\n> Describe the image briefly\nA large building with two towers in the middle of a city.\n> What type of building is it?\nchurch\n> What color is the church?\ngray\n> caption image\nA large building with two towers stands tall on the water's edge. The building\nhas a brown roof and a window on the side. A tree stands in front of the\nbuilding, and a flag waves proudly from its top. The water is calm and blue,\nreflecting the sky above. A bridge crosses the water, and a red and white boat\nrests on its surface. The building has a window on the side, and a flag on top.\nA tall tree stands in front of the building, and a window on the building is\nvisible from the water. The water is green, and the sky is blue.\n```\n\n### Migrating to single-file format\n\nThere is now a new format for the weights file, which is a single file that\nallows to contain the tokenizer (and the model type) directly. A tool to migrate\nfrom the multi-file format to the single-file format is available.\n\n```sh\nio/migrate_weights \\\n  --tokenizer .../tokenizer.spm --weights .../gemma2-2b-it-sfp.sbs \\\n  --output_weights .../gemma2-2b-it-sfp-single.sbs\n```\n\nAfter migration, you can omit the tokenizer argument like this:\n\n```sh\n./gemma --weights .../gemma2-2b-it-sfp-single.sbs\n```\n\n### Troubleshooting and FAQs\n\n**Problems building in Windows / Visual Studio**\n\nCurrently if you're using Windows, we recommend building in WSL (Windows\nSubsystem for Linux). We are exploring options to enable other build\nconfigurations, see issues for active discussion.\n\n**Model does not respond to instructions and produces strange output**\n\nA common issue is that you are using a pre-trained model, which is not\ninstruction-tuned and thus does not respond to instructions. Make sure you are\nusing an instruction-tuned model (`gemma2-2b-it-sfp`) and not a pre-trained\nmodel (any model with a `-pt` suffix).\n\n**What sequence lengths are supported?**\n\nSee `max_seq_len` in `configs.cc` and `InferenceArgs.seq_len`. For the Gemma 3\nmodels larger than 1B, this is typically 32K but 128K would also work given\nenough RAM. Note that long sequences will be slow due to the quadratic cost of\nattention.\n\n**How do I convert my fine-tune to a `.sbs` compressed model file?**\n\nFor PaliGemma 2 checkpoints, you can use python/convert_from_safetensors.py to\nconvert from safetensors format (tested with building via bazel). For an adapter\nmodel, you will likely need to call merge_and_unload() to convert the adapter\nmodel to a single-file format before converting it.\n\nHere is how to use it using a bazel build of the compression library assuming\nlocally installed (venv) torch, numpy, safetensors, absl-py, etc.:\n\n```sh\nbazel build //compression/python:compression\nBAZEL_OUTPUT_DIR=\"${PWD}/bazel-bin/compression\"\npython3 -c \"import site; print(site.getsitepackages())\"\n# Use your sites-packages file here:\nln -s $BAZEL_OUTPUT_DIR [...]/site-packages/compression\npython3 python/convert_from_safetensors.py --load_path [...].safetensors.index.json\n```\n\n**What are some easy ways to make the model run faster?**\n\n1.  Make sure you are using the 8-bit switched floating point `-sfp` models.\n    These are half the size of bf16 and thus use less memory bandwidth and cache\n    space.\n2.  Due to auto-tuning, the second and especially third query will be faster.\n3.  If you're on a laptop, make sure power mode is set to maximize performance\n    and saving mode is **off**. For most laptops, the power saving modes get\n    activated automatically if the computer is not plugged in.\n4.  Close other unused cpu-intensive applications.\n5.  On macs, anecdotally we observe a \"warm-up\" ramp-up in speed as performance\n    cores get engaged.\n\nWe're also working on algorithmic and optimization approaches for faster\ninference, stay tuned.\n\n## Usage\n\n`gemma` has different usage modes, controlled by the verbosity flag.\n\nAll usage modes are currently interactive, triggering text generation upon\nnewline input.\n\n| Verbosity       | Usage mode | Details                                       |\n| --------------- | ---------- | --------------------------------------------- |\n| `--verbosity 0` | Minimal | Only prints generation output. Suitable as a CLI tool. |\n| `--verbosity 1` | Default | Standard user-facing terminal UI. |\n| `--verbosity 2` | Detailed | Shows additional developer and debug info. |\n\n### Interactive Terminal App\n\nBy default, verbosity is set to 1, bringing up a terminal-based interactive\ninterface when `gemma` is invoked:\n\n```sh\n$ ./gemma [...]\n  __ _  ___ _ __ ___  _ __ ___   __ _   ___ _ __  _ __\n / _` |/ _ \\ '_ ` _ \\| '_ ` _ \\ / _` | / __| '_ \\| '_ \\\n| (_| |  __/ | | | | | | | | | | (_| || (__| |_) | |_) |\n \\__, |\\___|_| |_| |_|_| |_| |_|\\__,_(_)___| .__/| .__/\n  __/ |                                    | |   | |\n |___/                                     |_|   |_|\n\n...\n\n*Usage*\n  Enter an instruction and press enter (%C reset conversation, %Q quits).\n\n*Examples*\n  - Write an email to grandma thanking her for the cookies.\n  - What are some historical attractions to visit around Massachusetts?\n  - Compute the nth fibonacci number in javascript.\n  - Write a standup comedy bit about WebGPU programming.\n\n> What are some outdoorsy places to visit around Boston?\n\n[ Reading prompt ] .....................\n\n\n**Boston Harbor and Islands:**\n\n* **Boston Harbor Islands National and State Park:** Explore pristine beaches, wildlife, and maritime history.\n* **Charles River Esplanade:** Enjoy scenic views of the harbor and city skyline.\n* **Boston Harbor Cruise Company:** Take a relaxing harbor cruise and admire the city from a different perspective.\n* **Seaport Village:** Visit a charming waterfront area with shops, restaurants, and a seaport museum.\n\n**Forest and Nature:**\n\n* **Forest Park:** Hike through a scenic forest with diverse wildlife.\n* **Quabbin Reservoir:** Enjoy boating, fishing, and hiking in a scenic setting.\n* **Mount Forest:** Explore a mountain with breathtaking views of the city and surrounding landscape.\n\n...\n```\n\n### Usage as a Command Line Tool\n\nFor using the `gemma` executable as a command line tool, it may be useful to\ncreate an alias for gemma.cpp with arguments fully specified:\n\n```sh\nalias gemma2b=\"~/gemma.cpp/build/gemma -- --tokenizer ~/gemma.cpp/build/tokenizer.spm --weights ~/gemma.cpp/build/gemma2-2b-it-sfp.sbs --verbosity 0\"\n```\n\nReplace the above paths with your own paths to the model and tokenizer paths\nfrom the download.\n\nHere is an example of prompting `gemma` with a truncated input\nfile (using a `gemma2b` alias like defined above):\n\n```sh\ncat configs.h | tail -n 35 | tr '\\n' ' ' | xargs -0 echo \"What does this C++ code do: \" | gemma2b\n```\n\n> [!NOTE]\n> CLI usage of gemma.cpp is experimental and should take context length\n> limitations into account.\n\nThe output of the above command should look like:\n\n```sh\n[ Reading prompt ] [...]\nThis C++ code snippet defines a set of **constants** used in a large language model (LLM) implementation, likely related to the **attention mechanism**.\n\nLet's break down the code:\n[...]\n```\n\n### Incorporating gemma.cpp as a Library in your Project\n\nThe easiest way to incorporate gemma.cpp in your own project is to pull in\ngemma.cpp and dependencies using `FetchContent`. You can add the following to\nyour CMakeLists.txt:\n\n```\ninclude(FetchContent)\n\nFetchContent_Declare(sentencepiece GIT_REPOSITORY https://github.com/google/sentencepiece GIT_TAG 53de76561cfc149d3c01037f0595669ad32a5e7c)\nFetchContent_MakeAvailable(sentencepiece)\n\nFetchContent_Declare(gemma GIT_REPOSITORY https://github.com/google/gemma.cpp GIT_TAG origin/main)\nFetchContent_MakeAvailable(gemma)\n\nFetchContent_Declare(highway GIT_REPOSITORY https://github.com/google/highway.git GIT_TAG 2a16a50ff61071bb25ddef0ce35d92b0e2b9c579)\nFetchContent_MakeAvailable(highway)\n```\n\nNote for the gemma.cpp `GIT_TAG`, you may replace `origin/main` for a specific\ncommit hash if you would like to pin the library version.\n\nAfter your executable is defined (substitute your executable name for\n`[Executable Name]` below):\n\n```\ntarget_link_libraries([Executable Name] libgemma hwy hwy_contrib sentencepiece)\nFetchContent_GetProperties(gemma)\nFetchContent_GetProperties(sentencepiece)\ntarget_include_directories([Executable Name] PRIVATE ${gemma_SOURCE_DIR})\ntarget_include_directories([Executable Name] PRIVATE ${sentencepiece_SOURCE_DIR})\n```\n\n### Building gemma.cpp as a Library\n\ngemma.cpp can also be used as a library dependency in your own project. The\nshared library artifact can be built by modifying the make invocation to build\nthe `libgemma` target instead of `gemma`.\n\n> [!NOTE]\n> If you are using gemma.cpp in your own project with the `FetchContent` steps\n> in the previous section, building the library is done automatically by `cmake`\n> and this section can be skipped.\n\nFirst, run `cmake`:\n\n```sh\ncmake -B build\n```\n\nThen, run `make` with the `libgemma` target:\n\n```sh\ncd build\nmake -j [number of parallel threads to use] libgemma\n```\n\nIf this is successful, you should now have a `libgemma` library file in the\n`build/` directory. On Unix platforms, the filename is `libgemma.a`.\n\n## Independent Projects Using gemma.cpp\n\nSome independent projects using gemma.cpp:\n\n- [gemma-cpp-python - Python bindings](https://github.com/namtranase/gemma-cpp-python)\n- [lua-cgemma - Lua bindings](https://github.com/ufownl/lua-cgemma)\n- [Godot engine demo project](https://github.com/Rliop913/Gemma-godot-demo-project)\n\nIf you would like to have your project included, feel free to get in touch or\nsubmit a PR with a `README.md` edit.\n\n## Acknowledgements and Contacts\n\ngemma.cpp was started in fall 2023 by\n[Austin Huang](mailto:austinvhuang@google.com) and\n[Jan Wassenberg](mailto:janwas@google.com), and subsequently released February\n2024 thanks to contributions from Phil Culliton, Paul Chang, and Dan Zheng.\n\nGriffin support was implemented in April 2024 thanks to contributions by Andrey\nMikhaylov, Eugene Kliuchnikov, Jan Wassenberg, Jyrki Alakuijala, Lode\nVandevenne, Luca Versari, Martin Bruse, Phil Culliton, Sami Boukortt, Thomas\nFischbacher and Zoltan Szabadka. It was removed in 2025-09.\n\nGemma-2 support was implemented in June/July 2024 with the help of several\npeople.\n\nPaliGemma support was implemented in September 2024 with contributions from\nDaniel Keysers.\n\n[Jan Wassenberg](mailto:janwas@google.com) has continued to contribute many\nimprovements, including major gains in efficiency, since the initial release.\n\nThis is not an officially supported Google product.\n",
      "stars_today": 5
    },
    {
      "id": 52909975,
      "name": "PlotJuggler",
      "full_name": "facontidavide/PlotJuggler",
      "description": "The Time Series Visualization Tool that you deserve.",
      "html_url": "https://github.com/facontidavide/PlotJuggler",
      "stars": 5608,
      "forks": 746,
      "language": "C++",
      "topics": [
        "chart",
        "csv",
        "labstreaminglayer",
        "lua",
        "mqtt",
        "plot",
        "px4",
        "qt5",
        "ros",
        "rosbag",
        "time-series",
        "ulog",
        "visualize-data"
      ],
      "created_at": "2016-03-01T21:05:42Z",
      "updated_at": "2026-01-14T21:47:08Z",
      "pushed_at": "2026-01-13T10:00:13Z",
      "open_issues": 147,
      "owner": {
        "login": "facontidavide",
        "avatar_url": "https://avatars.githubusercontent.com/u/2822888?v=4"
      },
      "readme": "![PlotJuggler](docs/plotjuggler3_banner.svg)\n\n[![windows](https://github.com/facontidavide/PlotJuggler/actions/workflows/windows.yaml/badge.svg)](https://github.com/facontidavide/PlotJuggler/actions/workflows/windows.yaml)\n[![ubuntu](https://github.com/facontidavide/PlotJuggler/actions/workflows/ubuntu.yaml/badge.svg)](https://github.com/facontidavide/PlotJuggler/actions/workflows/ubuntu.yaml)\n[![macos](https://github.com/facontidavide/PlotJuggler/actions/workflows/macos.yaml/badge.svg)](https://github.com/facontidavide/PlotJuggler/actions/workflows/macos.yaml)\n[![ROS1](https://github.com/facontidavide/PlotJuggler/workflows/ros1/badge.svg)](https://github.com/facontidavide/PlotJuggler/actions?query=workflow%3Aros1)\n[![ROS2](https://github.com/facontidavide/PlotJuggler/workflows/ros2/badge.svg)](https://github.com/facontidavide/PlotJuggler/actions?query=workflow%3Aros2)\n[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=I%20use%20PlotJuggler%20and%20it%20is%20amazing%0D%0A&url=https://github.com/facontidavide/PlotJuggler&via=facontidavide&hashtags=dataviz,plotjuggler,GoROS,PX4)\n\n## Gold Sponsor:\n[![Greenzie](docs/sponsor_greenzie.png)](https://www.greenzie.com/)\n[![Intermodalics](docs/sponsor_intermodalics.png)](https://www.intermodalics.ai/)\n[![Greenzie](docs/sponsor_ark.png)](https://arkelectron.com/)\n\n# PlotJuggler 3.15\n\nPlotJuggler is a tool to visualize time series that is **fast**, **powerful** and  **intuitive**.\n\nNoteworthy features:\n\n- Simple Drag & Drop user interface.\n- Load __data from file__.\n- Connect to live __streaming__ of data.\n- Save the visualization layout and configurations to reuse them later.\n- Fast **OpenGL** visualization.\n- Can handle **thousands** of timeseries and **millions** of data points.\n- Transform your data using a simple editor: derivative, moving average, integral, etcâ€¦\n- PlotJuggler can be easily extended using __plugins__.\n\n![PlotJuggler](docs/plotjuggler3.gif)\n\n\n## Data sources (file and streaming)\n\n- Load CSV files.\n- Load [ULog](https://docs.px4.io/main/en/dev_log/ulog_file_format) (PX4).\n- Subscribe to many different streaming sources: MQTT, WebSockets, ZeroMQ, UDP, etc.\n- Understand data formats such as JSON, CBOR, BSON, Message Pack, etc.\n- Well integrated with [ROS](https://www.ros.org/): open *rosbags* and/or subscribe to ROS *topics* (both ROS1 and ROS2).\n- Supports the [Lab Streaming Layer](https://labstreaminglayer.readthedocs.io/info/intro.html), that is used by [many devices](https://labstreaminglayer.readthedocs.io/info/supported_devices.html).\n- Easily add your custom data source and/or formats...\n\n![](docs/data_sources.svg)\n\n## Transform and analyze your data\nPlotJuggler makes it easy to visualize data but also to analyze it.\nYou can manipulate your time series using a simple and extendable Transform Editor.\n\n![](docs/function_editor.png)\n\nAlternatively, you may use the Custom Function Editor, which allows you to create Multi-input / Single-output functions\nusing a scripting language based on [Lua](https://www.tutorialspoint.com/lua/index.htm).\n\nIf you are not familiar with Lua, don't be afraid, you won't need more than 5 minutes to learn it ;)\n\n![](docs/custom_editor.png)\n\n## Tutorials\n\nTo learn how to use PlotJuggler, check the tutorials here:\n\n| Tutorial 1   |  Tutorial 2 | Tutorial 3 |\n:-------------------------:|:-------------------------:|:-------------------------:\n| [![](docs/tutorial_1.png)](https://slides.com/davidefaconti/introduction-to-plotjuggler) | [![](docs/tutorial_2.png)](https://slides.com/davidefaconti/plotjuggler-data) | [![](docs/tutorial_3.png)](https://slides.com/davidefaconti/plotjuggler-transforms) |\n\n## Supported plugins\n\nSome plugins can be found in a different repository. The individual README files\n*should* include all the information needed to compile and use the plugin.\n\nPlease submit specific issues, Pull Requests and questions on the related Github repository:\n\n- [MQTT DataStreamer](https://github.com/PlotJuggler/plotjuggler-mqtt).\n- [Lab Streaming Layer DataStreamer](https://github.com/PlotJuggler/plotjuggler-lsl).\n- [ROS plugins](https://github.com/PlotJuggler/plotjuggler-ros-plugins).\n- [CAN .dbg DataLoader](https://github.com/PlotJuggler/plotjuggler-CAN-dbs).\n\nIf you want a simple example to learn how to write your own plugins, have a look at\n[PlotJuggler/plotjuggler-sample-plugins](https://github.com/PlotJuggler/plotjuggler-sample-plugins)\n\n# Installation\n\nYou can download the latest ready to use binaries from the [Release page](https://github.com/facontidavide/PlotJuggler/releases).\n\n<div align=\"center\">\n\n| ğŸ§ Linux | ğŸ macOS | ğŸªŸ Windows | ğŸ“¦ Debian |\n|:--------:|:--------:|:----------:|:---------:|\n| **AppImage** | **Installer** | **Installer** | **Packages** |\n| x86 / arm64 | x86 / arm64 | x64 | bookworm, trixie |\n\n</div>\n\n## Snap (recommended in Ubuntu, to ROS users too)\n\nThe snap contains a version of PlotJuggler that works (with some limitations) with ROS2.\n\n![Get it from the Snap Store](https://snapcraft.io/static/images/badges/en/snap-store-black.svg)\n\nTo install it in Ubuntu, run:\n\n```\nsudo snap install plotjuggler\n```\n\n### Debian packages for ROS User\n\nInstall the ROS packages with:\n\n```\nsudo apt install ros-$ROS_DISTRO-plotjuggler-ros\n```\nTo launch PlotJuggler, use the command:\n\n```\nros2 run plotjuggler plotjuggler\n```\n\nROS plugins are available in a separate repository: https://github.com/PlotJuggler/plotjuggler-ros-plugins\n\nPlease take a look at the instructions in that repository if you want to compile PJ and its ROS plugins from source.\n\n\n## Compile from source\n\nYou can find the detailed instructions here: [COMPILE.md](COMPILE.md).\n\n# Sponsorship and commercial support\n\nPlotJuggler required a lot of work to develop and maintain; my goal is to build the most\nintuitive and powerful tool to visualize data and timeseries.\n\nIf you find PlotJuggler useful, consider donating [PayPal](https://www.paypal.me/facontidavide) or becoming a\n[Github Sponsor](https://github.com/sponsors/facontidavide).\n\nIf you need to extend any of the functionalities of PlotJuggler to cover a specific\nneed or to parse your custom data formats, you can receive commercial\nsupport from the main author, [Davide Faconti](mailto:davide.faconti@gmail.com).\n\n# License\n\nPlotJuggler is released under the [Mozilla Public License Version 2.0](LICENSE.md),\nwhich allows users to develop closed-source plugins.\n\nPlease note that some third-party dependencies (including Qt) use the\n**GNU Lesser General Public License**.\n\n# Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=facontidavide/PlotJuggler&type=Date)](https://star-history.com/#facontidavide/PlotJuggler&Date)\n\n# Contributors\n\n<a href=\"https://github.com/facontidavide/plotjuggler/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=facontidavide/plotjuggler\" />\n</a>\n",
      "stars_today": 5
    },
    {
      "id": 358887741,
      "name": "aniyomi",
      "full_name": "aniyomiorg/aniyomi",
      "description": "An app for manga and anime",
      "html_url": "https://github.com/aniyomiorg/aniyomi",
      "stars": 6832,
      "forks": 487,
      "language": "Kotlin",
      "topics": [
        "android",
        "anime",
        "manga"
      ],
      "created_at": "2021-04-17T13:35:37Z",
      "updated_at": "2026-01-14T22:22:52Z",
      "pushed_at": "2025-11-05T10:57:36Z",
      "open_issues": 308,
      "owner": {
        "login": "aniyomiorg",
        "avatar_url": "https://avatars.githubusercontent.com/u/136799407?v=4"
      },
      "readme": "<div align=\"center\">\n\n<a href=\"https://aniyomi.org\">\n    <img src=\"./.github/assets/logo.png\" alt=\"Aniyomi logo\" title=\"Aniyomi logo\" width=\"80\"/>\n</a>\n\n# Aniyomi [App](#)\n\n### Full-featured player and reader, based on ~~Tachiyomi~~ Mihon.\nDiscover and watch anime, cartoons, series, and more â€“ easier than ever on your Android device.\n\n[![Discord server](https://img.shields.io/discord/841701076242530374.svg?label=&labelColor=6A7EC2&color=7389D8&logo=discord&logoColor=FFFFFF)](https://discord.gg/F32UjdJZrR)\n[![GitHub downloads](https://img.shields.io/github/downloads/aniyomiorg/aniyomi/total?label=downloads&labelColor=27303D&color=0D1117&logo=github&logoColor=FFFFFF&style=flat)](https://github.com/aniyomiorg/aniyomi/releases)\n\n[![CI](https://img.shields.io/github/actions/workflow/status/aniyomiorg/aniyomi/build_push.yml?labelColor=27303D)](https://github.com/aniyomiorg/aniyomi/actions/workflows/build_push.yml)\n[![License: Apache-2.0](https://img.shields.io/github/license/aniyomiorg/aniyomi?labelColor=27303D&color=818cf8)](/LICENSE)\n[![Translation status](https://img.shields.io/weblate/progress/aniyomi?labelColor=27303D&color=946300)](https://hosted.weblate.org/engage/aniyomi/)\n\n## Download\n\n[![Aniyomi Stable](https://img.shields.io/github/release/aniyomiorg/aniyomi.svg?maxAge=3600&label=Stable&labelColor=06599d&color=043b69)](https://github.com/aniyomiorg/aniyomi/releases)\n[![Aniyomi Preview](https://img.shields.io/github/v/release/aniyomiorg/aniyomi-preview.svg?maxAge=3600&label=Beta&labelColor=2c2c47&color=1c1c39)](https://github.com/aniyomiorg/aniyomi-preview/releases)\n\n*Requires Android 8.0 or higher.*\n\n## Features\n\n<div align=\"left\">\n\n* Local reading and watching of content.\n* A configurable reader with multiple viewers, reading directions and other settings.\n* A configurable player built on mpv-android with multiple options and settings.\n* Tracker support: [MyAnimeList](https://myanimelist.net/), [AniList](https://anilist.co/), [Kitsu](https://kitsu.app/), [MangaUpdates](https://mangaupdates.com), [Shikimori](https://shikimori.one), [Simkl](https://simkl.com/), and [Bangumi](https://bgm.tv/) support.\n* Categories to organize your library.\n* Light and dark themes.\n* Schedule updating your library for new chapters/episodes.\n* Create backups locally to read/watch offline or to your desired cloud service.\n* Plus much more...\n\n</div>\n\n## Contributing\n\n[Code of conduct](./CODE_OF_CONDUCT.md) Â· [Contributing guide](./CONTRIBUTING.md)\n\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n\nBefore reporting a new issue, take a look at the [FAQ](https://aniyomi.org/docs/faq/general), the [changelog](https://aniyomi.org/changelogs/) and the already opened [issues](https://github.com/aniyomiorg/aniyomi/issues); if you got any questions, join our [Discord server](https://discord.gg/F32UjdJZrR).\n\n### Repositories\n\n[![aniyomiorg/aniyomi-website - GitHub](https://github-readme-stats.vercel.app/api/pin/?username=aniyomiorg&repo=aniyomi-website&bg_color=161B22&text_color=c9d1d9&title_color=818cf8&icon_color=818cf8&border_radius=8&hide_border=true&description_lines_count=2)](https://github.com/aniyomiorg/aniyomi-website/)\n[![aniyomiorg/aniyomi-mpv-lib - GitHub](https://github-readme-stats.vercel.app/api/pin/?username=aniyomiorg&repo=aniyomi-mpv-lib&bg_color=161B22&text_color=c9d1d9&title_color=818cf8&icon_color=818cf8&border_radius=8&hide_border=true&description_lines_count=2)](https://github.com/aniyomiorg/aniyomi-mpv-lib/)\n\n### Credits\n\nThank you to all the people who have contributed!\n\n<a href=\"https://github.com/aniyomiorg/aniyomi/graphs/contributors\">\n    <img src=\"https://contrib.rocks/image?repo=aniyomiorg/aniyomi\" alt=\"Aniyomi app contributors\" title=\"Aniyomi app contributors\" width=\"800\"/>\n</a>\n\n### Disclaimer\n\nThe developer(s) of this application does not have any affiliation with the content providers available, and this application hosts zero content.\n\n### License\n\n<pre>\nCopyright Â© 2015 Javier TomÃ¡s\nCopyright Â© 2024 Mihon Open Source Project\nCopyright Â© 2024 Aniyomi Open Source Project\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n</pre>\n\n</div>\n",
      "stars_today": 5
    },
    {
      "id": 207354223,
      "name": "FreeRTOS-Kernel",
      "full_name": "FreeRTOS/FreeRTOS-Kernel",
      "description": "FreeRTOS kernel files only, submoduled into https://github.com/FreeRTOS/FreeRTOS and various other repos.",
      "html_url": "https://github.com/FreeRTOS/FreeRTOS-Kernel",
      "stars": 3786,
      "forks": 1421,
      "language": "C",
      "topics": [],
      "created_at": "2019-09-09T16:28:01Z",
      "updated_at": "2026-01-14T22:59:59Z",
      "pushed_at": "2026-01-13T21:35:02Z",
      "open_issues": 36,
      "owner": {
        "login": "FreeRTOS",
        "avatar_url": "https://avatars.githubusercontent.com/u/54647343?v=4"
      },
      "readme": "[![CMock Unit Tests](https://github.com/FreeRTOS/FreeRTOS-Kernel/actions/workflows/unit-tests.yml/badge.svg?branch=main&event=push)](https://github.com/FreeRTOS/FreeRTOS-Kernel/actions/workflows/unit-tests.yml?query=branch%3Amain+event%3Apush+workflow%3A%22CMock+Unit+Tests%22++)\n[![codecov](https://app.codecov.io/gh/FreeRTOS/FreeRTOS-Kernel/badge.svg?branch=main)](https://codecov.io/gh/FreeRTOS/FreeRTOS-Kernel)\n\n## Getting started\n\nThis repository contains FreeRTOS kernel source/header files and kernel\nports only. This repository is referenced as a submodule in\n[FreeRTOS/FreeRTOS](https://github.com/FreeRTOS/FreeRTOS)\nrepository, which contains pre-configured demo application projects under\n```FreeRTOS/Demo``` directory.\n\nThe easiest way to use FreeRTOS is to start with one of the pre-configured demo\napplication projects.  That way you will have the correct FreeRTOS source files\nincluded, and the correct include paths configured. Once a demo application is\nbuilding and executing you can remove the demo application files, and start to\nadd in your own application source files.  See the\n[FreeRTOS Kernel Quick Start Guide](https://www.freertos.org/Documentation/01-FreeRTOS-quick-start/01-Beginners-guide/02-Quick-start-guide)\nfor detailed instructions and other useful links.\n\nAdditionally, for FreeRTOS kernel feature information refer to the\n[Developer Documentation](https://www.freertos.org/Documentation/02-Kernel/02-Kernel-features/00-Developer-docs),\nand [API Reference](https://www.freertos.org/Documentation/02-Kernel/04-API-references/01-Task-creation/00-TaskHandle).\n\nAlso for contributing and creating a Pull Request please refer to\n[the instructions here](.github/CONTRIBUTING.md#contributing-via-pull-request).\n\n**FreeRTOS-Kernel V11.1.0\n[source code](https://github.com/FreeRTOS/FreeRTOS-Kernel/tree/V11.1.0) is part\nof the\n[FreeRTOS 202406.00 LTS](https://github.com/FreeRTOS/FreeRTOS-LTS/tree/202406-LTS)\nrelease.**\n\n### Getting help\n\nIf you have any questions or need assistance troubleshooting your FreeRTOS project,\nwe have an active community that can help on the\n[FreeRTOS Community Support Forum](https://forums.freertos.org).\n\n## To consume FreeRTOS-Kernel\n\n### Consume with CMake\n\nIf using CMake, it is recommended to use this repository using FetchContent.\nAdd the following into your project's main or a subdirectory's `CMakeLists.txt`:\n\n- Define the source and version/tag you want to use:\n\n```cmake\nFetchContent_Declare( freertos_kernel\n  GIT_REPOSITORY https://github.com/FreeRTOS/FreeRTOS-Kernel.git\n  GIT_TAG        main #Note: Best practice to use specific git-hash or tagged version\n)\n```\n\nIn case you prefer to add it as a git submodule, do:\n\n```bash\ngit submodule add https://github.com/FreeRTOS/FreeRTOS-Kernel.git <path of the submodule>\ngit submodule update --init\n```\n\n- Add a freertos_config library (typically an INTERFACE library) The following assumes the directory structure:\n  - `include/FreeRTOSConfig.h`\n\n```cmake\nadd_library(freertos_config INTERFACE)\n\ntarget_include_directories(freertos_config SYSTEM\nINTERFACE\n    include\n)\n\ntarget_compile_definitions(freertos_config\n  INTERFACE\n    projCOVERAGE_TEST=0\n)\n```\n\nIn case you installed FreeRTOS-Kernel as a submodule, you will have to add it as a subdirectory:\n\n```cmake\nadd_subdirectory(${FREERTOS_PATH})\n```\n\n- Configure the FreeRTOS-Kernel and make it available\n  - this particular example supports a native and cross-compiled build option.\n\n```cmake\nset( FREERTOS_HEAP \"4\" CACHE STRING \"\" FORCE)\n# Select the native compile PORT\nset( FREERTOS_PORT \"GCC_POSIX\" CACHE STRING \"\" FORCE)\n# Select the cross-compile PORT\nif (CMAKE_CROSSCOMPILING)\n  set(FREERTOS_PORT \"GCC_ARM_CA9\" CACHE STRING \"\" FORCE)\nendif()\n\nFetchContent_MakeAvailable(freertos_kernel)\n```\n\n- In case of cross compilation, you should also add the following to `freertos_config`:\n\n```cmake\ntarget_compile_definitions(freertos_config INTERFACE ${definitions})\ntarget_compile_options(freertos_config INTERFACE ${options})\n```\n\n### Consuming stand-alone - Cloning this repository\n\nTo clone using HTTPS:\n\n```\ngit clone https://github.com/FreeRTOS/FreeRTOS-Kernel.git\n```\n\nUsing SSH:\n\n```\ngit clone git@github.com:FreeRTOS/FreeRTOS-Kernel.git\n```\n\n## Repository structure\n\n- The root of this repository contains the three files that are common to\nevery port - list.c, queue.c and tasks.c.  The kernel is contained within these\nthree files.  croutine.c implements the optional co-routine functionality - which\nis normally only used on very memory limited systems.\n\n- The ```./portable``` directory contains the files that are specific to a particular microcontroller and/or compiler.\nSee the readme file in the ```./portable``` directory for more information.\n\n- The ```./include``` directory contains the real time kernel header files.\n\n- The ```./template_configuration``` directory contains a sample `FreeRTOSConfig.h` to help jumpstart a new project.\nSee the [FreeRTOSConfig.h](examples/template_configuration/FreeRTOSConfig.h) file for instructions.\n\n### Code Formatting\n\nFreeRTOS files are formatted using the\n\"[uncrustify](https://github.com/uncrustify/uncrustify)\" tool.\nThe configuration file used by uncrustify can be found in the\n[FreeRTOS/CI-CD-GitHub-Actions's](https://github.com/FreeRTOS/CI-CD-Github-Actions)\n[uncrustify.cfg](https://github.com/FreeRTOS/CI-CD-Github-Actions/tree/main/formatting)\nfile.\n\n### Line Endings\n\nFile checked into the FreeRTOS-Kernel repository use unix-style LF line endings\nfor the best compatibility with git.\n\nFor optimal compatibility with Microsoft Windows tools, it is best to enable\nthe git autocrlf feature. You can enable this setting for the current\nrepository using the following command:\n\n```\ngit config core.autocrlf true\n```\n\n### Git History Optimizations\n\nSome commits in this repository perform large refactors which touch many lines\nand lead to unwanted behavior when using the `git blame` command. You can\nconfigure git to ignore the list of large refactor commits in this repository\nwith the following command:\n\n```\ngit config blame.ignoreRevsFile .git-blame-ignore-revs\n```\n\n### Spelling and Formatting\n\nWe recommend using [Visual Studio Code](https://code.visualstudio.com),\ncommonly referred to as VSCode, when working on the FreeRTOS-Kernel.\nThe FreeRTOS-Kernel also uses [cSpell](https://cspell.org/) as part of its\nspelling check. The config file for which can be found at [cspell.config.yaml](cspell.config.yaml)\nThere is additionally a\n[cSpell plugin for VSCode](https://marketplace.visualstudio.com/items?itemName=streetsidesoftware.code-spell-checker)\nthat can be used as well.\n*[.cSpellWords.txt](.github/.cSpellWords.txt)* contains words that are not\ntraditionally found in an English dictionary. It is used by the spellchecker\nto verify the various jargon, variable names, and other odd words used in the\nFreeRTOS code base are correct. If your pull request fails to pass the spelling\nand you believe this is a mistake, then add the word to\n*[.cSpellWords.txt](.github/.cSpellWords.txt)*. When adding a word please\nthen sort the list, which can be done by running the bash command:\n`sort -u .cSpellWords.txt -o .cSpellWords.txt`\nNote that only the FreeRTOS-Kernel Source Files, [include](include),\n[portable/MemMang](portable/MemMang), and [portable/Common](portable/Common)\nfiles are checked for proper spelling, and formatting at this time.\n\n## Third Party Tools\nVisit [this link](.github/third_party_tools.md) for detailed information about\nthird-party tools with FreeRTOS support.\n",
      "stars_today": 5
    },
    {
      "id": 879054347,
      "name": "MiCTS",
      "full_name": "parallelcc/MiCTS",
      "description": "Trigger Circle to Search on any Android 9â€“15 device",
      "html_url": "https://github.com/parallelcc/MiCTS",
      "stars": 1180,
      "forks": 61,
      "language": "Kotlin",
      "topics": [
        "android",
        "google",
        "non-root",
        "root"
      ],
      "created_at": "2024-10-26T20:55:01Z",
      "updated_at": "2026-01-15T00:27:19Z",
      "pushed_at": "2025-11-24T11:23:53Z",
      "open_issues": 27,
      "owner": {
        "login": "parallelcc",
        "avatar_url": "https://avatars.githubusercontent.com/u/19646812?v=4"
      },
      "readme": "# MiCTS\r\n\r\n[![Stars](https://img.shields.io/github/stars/parallelcc/MiCTS)](https://github.com/parallelcc/MiCTS) [![Downloads](https://img.shields.io/github/downloads/parallelcc/MiCTS/total)](https://github.com/parallelcc/MiCTS/releases) [![Release](https://img.shields.io/github/v/release/parallelcc/MiCTS)](https://github.com/parallelcc/MiCTS/releases/latest) [![DeepWiki](https://img.shields.io/badge/DeepWiki-parallelcc%2FMiCTS-blue.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAyCAYAAAAnWDnqAAAAAXNSR0IArs4c6QAAA05JREFUaEPtmUtyEzEQhtWTQyQLHNak2AB7ZnyXZMEjXMGeK/AIi+QuHrMnbChYY7MIh8g01fJoopFb0uhhEqqcbWTp06/uv1saEDv4O3n3dV60RfP947Mm9/SQc0ICFQgzfc4CYZoTPAswgSJCCUJUnAAoRHOAUOcATwbmVLWdGoH//PB8mnKqScAhsD0kYP3j/Yt5LPQe2KvcXmGvRHcDnpxfL2zOYJ1mFwrryWTz0advv1Ut4CJgf5uhDuDj5eUcAUoahrdY/56ebRWeraTjMt/00Sh3UDtjgHtQNHwcRGOC98BJEAEymycmYcWwOprTgcB6VZ5JK5TAJ+fXGLBm3FDAmn6oPPjR4rKCAoJCal2eAiQp2x0vxTPB3ALO2CRkwmDy5WohzBDwSEFKRwPbknEggCPB/imwrycgxX2NzoMCHhPkDwqYMr9tRcP5qNrMZHkVnOjRMWwLCcr8ohBVb1OMjxLwGCvjTikrsBOiA6fNyCrm8V1rP93iVPpwaE+gO0SsWmPiXB+jikdf6SizrT5qKasx5j8ABbHpFTx+vFXp9EnYQmLx02h1QTTrl6eDqxLnGjporxl3NL3agEvXdT0WmEost648sQOYAeJS9Q7bfUVoMGnjo4AZdUMQku50McDcMWcBPvr0SzbTAFDfvJqwLzgxwATnCgnp4wDl6Aa+Ax283gghmj+vj7feE2KBBRMW3FzOpLOADl0Isb5587h/U4gGvkt5v60Z1VLG8BhYjbzRwyQZemwAd6cCR5/XFWLYZRIMpX39AR0tjaGGiGzLVyhse5C9RKC6ai42ppWPKiBagOvaYk8lO7DajerabOZP46Lby5wKjw1HCRx7p9sVMOWGzb/vA1hwiWc6jm3MvQDTogQkiqIhJV0nBQBTU+3okKCFDy9WwferkHjtxib7t3xIUQtHxnIwtx4mpg26/HfwVNVDb4oI9RHmx5WGelRVlrtiw43zboCLaxv46AZeB3IlTkwouebTr1y2NjSpHz68WNFjHvupy3q8TFn3Hos2IAk4Ju5dCo8B3wP7VPr/FGaKiG+T+v+TQqIrOqMTL1VdWV1DdmcbO8KXBz6esmYWYKPwDL5b5FA1a0hwapHiom0r/cKaoqr+27/XcrS5UwSMbQAAAABJRU5ErkJggg==)](https://deepwiki.com/parallelcc/MiCTS)\r\n\r\nç®€ä½“ä¸­æ–‡&nbsp;&nbsp;|&nbsp;&nbsp;[English](/README_en.md)&nbsp;&nbsp;|&nbsp;&nbsp;[Ğ ÑƒÑÑĞºĞ¸Ğ¹](/README_ru.md)&nbsp;&nbsp;|&nbsp;&nbsp;[TÃ¼rkÃ§e](/README_tr.md)&nbsp;&nbsp;|&nbsp;&nbsp;[ÙØ§Ø±Ø³ÛŒ](/README_fa.md)\r\n\r\nåœ¨ä»»æ„Android 9â€“15è®¾å¤‡ä¸Šè§¦å‘åœˆå®šå³æœï¼ˆCircle to Searchï¼‰åŠŸèƒ½\r\n\r\n*æœ¬åº”ç”¨åªè´Ÿè´£è§¦å‘åœˆå®šå³æœï¼Œæ— æ³•å¤„ç†è§¦å‘æˆåŠŸåå¯èƒ½å‡ºç°çš„é—®é¢˜*\r\n\r\n## ä½¿ç”¨æ–¹æ³•\r\n\r\n1. å®‰è£…æœ€æ–°ç‰ˆ[Google](https://play.google.com/store/apps/details?id=com.google.android.googlequicksearchbox)åº”ç”¨ï¼Œå¼€å¯è‡ªå¯åŠ¨ï¼Œå…³é—­åå°é™åˆ¶ï¼Œå°†é»˜è®¤åŠ©ç†åº”ç”¨è®¾ç½®ä¸ºGoogle\r\n\r\n\r\n2. å®‰è£…å¹¶æ‰“å¼€MiCTS\r\n    - å¦‚æœå¹¸è¿çš„è¯ï¼Œåœ¨ä¸éœ€è¦rootçš„æƒ…å†µä¸‹ï¼Œæ‰“å¼€MiCTSå°±ä¼šç›´æ¥è§¦å‘åœˆå®šå³æœ\r\n    - å¦‚æœæ²¡æœ‰ååº”ï¼Œå¤§æ¦‚ç‡æ˜¯å› ä¸ºGoogleå¯¹ä½ çš„è®¾å¤‡ç¦ç”¨äº†åœˆå®šå³æœåŠŸèƒ½ï¼ˆå¯ä»¥é€šè¿‡åœ¨Logcatæ—¥å¿—ä¸­æŸ¥æ‰¾`Omni invocation failed: not enabled`ç¡®è®¤ï¼‰ï¼Œåœ¨æœ‰rootçš„æƒ…å†µä¸‹ï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š\r\n        - åœ¨LSPosedé‡Œæ¿€æ´»æ¨¡å—ï¼Œåœ¨[MiCTSè®¾ç½®](#è¿›å…¥è®¾ç½®çš„æ–¹å¼)é‡Œå¼€å¯`Googleæœºå‹ä¼ªè£…`åï¼Œå¼ºåˆ¶é‡å¯Google\r\n        - å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œä½¿ç”¨[GMS-Flags](https://github.com/polodarb/GMS-Flags)ï¼Œå°†`com.google.android.apps.search.omnient.device`çš„flag`45631784`è®¾ä¸ºtrue\r\n\r\n\r\n3. è®¾ç½®è§¦å‘æ–¹å¼\r\n    - æ‰“å¼€MiCTSå³å¯è§¦å‘ï¼Œå› æ­¤å¯ä»¥åˆ©ç”¨å…¶ä»–è½¯ä»¶ï¼Œæ¯”å¦‚æ‚¬æµ®çƒã€Xposed Edgeã€ShortXç­‰ï¼Œå°†åŠ¨ä½œè®¾ç½®ä¸ºæ‰“å¼€MiCTSï¼Œå®ç°è‡ªå®šä¹‰è§¦å‘æ–¹å¼\r\n    - MiCTSæä¾›äº†ä¸€ä¸ªè§¦å‘ç£è´´ï¼Œå¯å°†å…¶æ·»åŠ åˆ°å¿«é€Ÿè®¾ç½®é¢æ¿é‡Œï¼Œé€šè¿‡ç‚¹å‡»ç£è´´è§¦å‘\r\n    - å¯¹äºå°ç±³è®¾å¤‡ï¼ŒMiCTSå†…ç½®äº†é•¿æŒ‰å°ç™½æ¡è§¦å‘å’Œé•¿æŒ‰Homeé”®è§¦å‘çš„åŠŸèƒ½ï¼Œå¯ä»¥åœ¨MiCTSè®¾ç½®é‡Œå¼€å¯ï¼ˆå®‰è£…MiCTSåéœ€è¦æ¿€æ´»æ¨¡å—å¹¶é‡å¯æ‰‹æœºæ‰èƒ½ä½¿ç”¨ï¼‰\r\n    - å¯¹äºAndroidç‰ˆæœ¬>=13çš„ä¸‰æ˜Ÿè®¾å¤‡ï¼Œå¯ä»¥ä»[ä¸‰æ˜Ÿåº”ç”¨å•†åº—](https://galaxystore.samsung.com/detail/com.samsung.android.app.routineplus)æˆ–[Good Lock](https://galaxystore.samsung.com/detail/com.samsung.android.goodlock)é‡Œä¸‹è½½å®‰è£…â€œæ—¥å¸¸ç¨‹åº+â€ï¼Œç„¶ååœ¨â€œè®¾ç½®-æ¨¡å¼å’Œæ—¥å¸¸ç¨‹åºâ€é‡Œï¼Œåˆ›å»ºæ—¥å¸¸ç¨‹åºé€šè¿‡æŒ‰é’®æ“ä½œå®ç°é•¿æŒ‰ç”µæºæŒ‰é’®ç­‰æ–¹å¼æ¥å¯åŠ¨MiCTS\r\n\r\n## è®¾ç½®\r\n\r\n### è¿›å…¥è®¾ç½®çš„æ–¹å¼\r\n- é•¿æŒ‰MiCTSåº”ç”¨å›¾æ ‡ï¼Œå‡ºç°è®¾ç½®é€‰é¡¹ï¼Œç‚¹å‡»è¿›å…¥\r\n- ä»LSPosedæ¨¡å—é¡µé¢ï¼Œç‚¹å‡»MiCTSï¼Œå†ç‚¹å‡»è®¾ç½®å›¾æ ‡è¿›å…¥\r\n- é•¿æŒ‰å¿«é€Ÿè®¾ç½®é¢æ¿çš„ç£è´´è¿›å…¥\r\n\r\n### åº”ç”¨è®¾ç½®\r\n- é»˜è®¤è§¦å‘å»¶è¿Ÿï¼šé€šè¿‡æ‰“å¼€MiCTSè§¦å‘çš„å»¶è¿Ÿ\r\n- ç£è´´è§¦å‘å»¶è¿Ÿï¼šé€šè¿‡ç‚¹å‡»å¿«é€Ÿè®¾ç½®é¢æ¿çš„ç£è´´è§¦å‘çš„å»¶è¿Ÿ\r\n\r\n### æ¨¡å—è®¾ç½®\r\néœ€è¦åœ¨LSPosedé‡Œæ¿€æ´»æ¨¡å—\r\n\r\n- ç³»ç»Ÿè§¦å‘æœåŠ¡ï¼šè§¦å‘æ‰€ä½¿ç”¨çš„ç³»ç»ŸæœåŠ¡ï¼Œåªä¼šæ˜¾ç¤ºå½“å‰æ”¯æŒçš„é€‰é¡¹ï¼Œä¾èµ–ä½œç”¨åŸŸé€‰æ‹©ç³»ç»Ÿæ¡†æ¶\r\n   - VISï¼šæ”¯æŒAndroid 9â€“15ï¼Œéœ€è¦å°†é»˜è®¤åŠ©ç†åº”ç”¨è®¾ç½®ä¸ºGoogleï¼Œè§¦å‘æ—¶ä¸€äº›è®¾å¤‡çš„å±å¹•è¾¹ç¼˜ä¼šé—ªï¼Œæ²¡æœ‰æ¿€æ´»æ¨¡å—çš„æƒ…å†µä¸‹åªèƒ½ä½¿ç”¨æ­¤æœåŠ¡\r\n   - CSHelperï¼šæ”¯æŒAndroid 14 QPR3åŠä»¥ä¸Šï¼Œä¸éœ€è¦è®¾ç½®é»˜è®¤åŠ©ç†åº”ç”¨ï¼Œè§¦å‘æ—¶å±å¹•è¾¹ç¼˜ä¸ä¼šé—ª\r\n   - CSServiceï¼šæ”¯æŒAndroid 15åŠä»¥ä¸Šï¼Œåœˆå®šå³æœä¸“ç”¨çš„æœåŠ¡ï¼Œæ•ˆæœåŒCSHelper\r\n\r\n\r\n- é•¿æŒ‰å°ç™½æ¡è§¦å‘ï¼šä»…æ”¯æŒå°ç±³è®¾å¤‡ï¼Œä¾èµ–ä½œç”¨åŸŸé€‰æ‹©ç³»ç»Ÿæ¡Œé¢\r\n\r\n\r\n- é•¿æŒ‰Homeé”®è§¦å‘ï¼šä»…æ”¯æŒå°ç±³è®¾å¤‡ï¼Œä¾èµ–ä½œç”¨åŸŸé€‰æ‹©ç³»ç»Ÿæ¡†æ¶\r\n\r\n \r\n- Googleæœºå‹ä¼ªè£…ï¼šä¾èµ–ä½œç”¨åŸŸé€‰æ‹©Google\r\n   - åˆ¶é€ å•†ï¼šä¿®æ”¹Googleè¯»å–åˆ°çš„ro.product.manufacturer\r\n   - å“ç‰Œï¼šä¿®æ”¹Googleè¯»å–åˆ°çš„ro.product.brand\r\n   - å‹å·ï¼šä¿®æ”¹Googleè¯»å–åˆ°çš„ro.product.model\r\n   - è®¾å¤‡ï¼šä¿®æ”¹Googleè¯»å–åˆ°çš„ro.product.device \r\n\r\n## å¸¸è§é—®é¢˜\r\n\r\n### æç¤ºâ€œè§¦å‘å¤±è´¥ï¼â€\r\n\r\nå¤§æ¦‚ç‡æ˜¯æ²¡æœ‰å°†Googleè®¾ä¸ºé»˜è®¤åŠ©ç†ï¼Œæ£€æŸ¥ä¸€ä¸‹\r\n\r\n### è§¦å‘å‡ºæ¥æ˜¯GoogleåŠ©ç†ï¼Œä¸æ˜¯åœˆå®šå³æœ\r\n\r\nGoogleä¸æ˜¯æœ€æ–°ç‰ˆï¼Œæ›´æ–°ä¸€ä¸‹\r\n\r\n### æœ‰æ—¶æ— æ³•æˆåŠŸè§¦å‘ï¼Œæ‰‹åŠ¨æ‰“å¼€Googleåæ‰ä¼šå‡ºç°åˆšæ‰åœˆå®šå³æœçš„ç•Œé¢\r\n\r\nåŸå› åº”è¯¥æ˜¯å¢“ç¢‘æœºåˆ¶å¯¼è‡´çš„ï¼Œçœ‹çœ‹æ‰‹æœºæœ‰æ²¡æœ‰ç›¸å…³çš„è®¾ç½®å¯ä»¥æŠŠGoogleåŠ åˆ°ç™½åå•é‡Œï¼Œæ¯”å¦‚ç”µæ± ä¼˜åŒ–é€‰æ‹©æ— é™åˆ¶ç­‰ï¼Œåœ¨æ¨¡å—è®¾ç½®é‡Œ`ç³»ç»Ÿè§¦å‘æœåŠ¡`ä½¿ç”¨`CSHelper`åº”è¯¥æ²¡æœ‰è¿™ä¸ªé—®é¢˜\r\n\r\n## è´¡çŒ®ç¿»è¯‘\r\n\r\nä½ å¯ä»¥é€šè¿‡[Crowdin](https://crowdin.com/project/micts)è´¡çŒ®ç¿»è¯‘\r\n\r\nå¦‚æœä½ éœ€è¦è´¡çŒ®ä¸€ä¸ªæ–°çš„è¯­è¨€ï¼Œè¯·å…ˆæäº¤ä¸€ä¸ªissue\r\n\r\n## Star History\r\n\r\n<a href=\"https://star-history.com/#parallelcc/micts&Date\">\r\n <picture>\r\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=parallelcc/micts&type=Date&theme=dark\" />\r\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=parallelcc/micts&type=Date\" />\r\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=parallelcc/micts&type=Date\" />\r\n </picture>\r\n</a>\r\n",
      "stars_today": 5
    },
    {
      "id": 1060901286,
      "name": "Wholphin",
      "full_name": "damontecres/Wholphin",
      "description": "An OSS Android TV client for Jellyfin",
      "html_url": "https://github.com/damontecres/Wholphin",
      "stars": 948,
      "forks": 26,
      "language": "Kotlin",
      "topics": [
        "android-tv",
        "fire-tv",
        "jellyfin",
        "jellyfin-client"
      ],
      "created_at": "2025-09-20T20:31:30Z",
      "updated_at": "2026-01-15T00:32:19Z",
      "pushed_at": "2026-01-14T23:40:37Z",
      "open_issues": 171,
      "owner": {
        "login": "damontecres",
        "avatar_url": "https://avatars.githubusercontent.com/u/154766448?v=4"
      },
      "readme": "# Wholphin - an OSS Android TV client for Jellyfin\n\n> \"Never half-phin two jellies. Always wholphin one jelly.\"\n\nWholphin is an open-source Android TV client for Jellyfin. It aims to provide a different app UI that's inspired by Plex for users interested in migrating to Jellyfin.\n\nThis is not a fork of the [official client](https://github.com/jellyfin/jellyfin-androidtv). Wholphin's user interface and controls have been written completely from scratch. Wholphin `v0.3.0+` supports playing media using either ExoPlayer/Media3 or MPV (experimental).\n\n<p align=\"center\">\n<a href=\"https://github.com/damontecres/Wholphin/releases\">\n<img alt=\"Current Release\" src=\"https://img.shields.io/github/release/damontecres/wholphin.svg\"/>\n</a>\n<a href=\"https://translate.codeberg.org/engage/wholphin/\">\n<img src=\"https://translate.codeberg.org/widget/wholphin/wholphin/svg-badge.svg\" alt=\"Translation status\" />\n</a>\n<br/>\n<a href=\"https://play.google.com/store/apps/details?id=com.github.damontecres.wholphin\">\n<img width=\"180\" alt=\"Get Wholphin on Google Play\" src=\"https://github.com/user-attachments/assets/2550a4cb-ce46-47a1-ae24-f33a169234b7\"/>\n</a>\n<a href=\"https://www.amazon.com/gp/product/B0G8RQQR9T/ref=mas_pm_wholphin\">\n<img width=\"180\" alt=\"Get Wholphin on Amazon AppStore\" src=\"https://github.com/user-attachments/assets/1f3a3b26-4b4f-44b1-9741-f4c895c8a53b\"/>\n</a>\n\n\n</p>\n\n<img width=\"1280\" height=\"720\" alt=\"0_3_5_home\" src=\"https://github.com/user-attachments/assets/a485c015-ec21-442d-a757-1f18381bf799\" />\n\n## Features\n\n### User interface\n\n- A navigation drawer for quick access to libraries, favorites, search, and settings from almost anywhere in the app\n- Option to combine Continue Watching & Next Up rows\n- Show Movie/TV Show titles when browsing libraries\n- Play theme music, if available\n- Search & download subtitles (requires compatible server plugin such as [OpenSubtitles](https://github.com/jellyfin/jellyfin-plugin-opensubtitles))\n- Customize layout grids for libraries\n- Multiple app color themes\n\n### Playback\n\n- Different media playback engines, including:\n  - Default ExoPlayer/Media3\n  - Experimental MPV\n- Plex inspired playback controls, such as:\n  - Using D-Pad left/right for seeking during playback\n  - Quickly access video chapters & queue during playback\n  - Optionally skip back a few seconds when resuming playback\n- Live TV & DVR support\n- Auto play next episodes with pass out protection\n- Option for automatic refresh rate switching on supported displays\n- Trickplay support\n- Other (subjective) enhancements:\n  - Subtly show playback position along the bottom of the screen while seeking w/ D-Pad\n  - Force Continue Watching & Next Up TV episodes to use their Series posters\n\n### Roadmap\n\nSee [here for the roadmap](https://github.com/damontecres/Wholphin/wiki#roadmap)\n\n## Installation\n\nDownloader Code: `8668671`\n\n1. Enable side-loading \"unknown\" apps\n    - https://androidtvnews.com/unknown-sources-chromecast-google-tv/\n    - https://www.xda-developers.com/how-to-sideload-apps-android-tv/\n    - https://developer.android.com/distribute/marketing-tools/alternative-distribution#unknown-sources\n    - https://www.aftvnews.com/how-to-enable-apps-from-unknown-sources-on-an-amazon-fire-tv-or-fire-tv-stick/\n1. Install the APK on your Android TV device with one of these options:\n    - Install a browser program such as [Downloader](https://www.aftvnews.com/downloader/), use it to get the latest apk with short code `8668671` or URL: http://aftv.news/8668671\n    - Download the latest APK release from the [releases page](https://github.com/damontecres/Wholphin/releases/latest) or http://aftv.news/8668671\n        - Put the APK on an SD Card/USB stick/network share and use a file manager app from the Google Play Store / Amazon AppStore (e.g. `FX File Explorer`). Android's preinstalled file manager probably will not work!\n        - Use `Send files to TV` from the Google Play Store on your phone & TV\n        - (Expert) Use [ADB](https://developer.android.com/studio/command-line/adb) to install the APK from your computer ([guide](https://fossbytes.com/side-load-apps-android-tv/#h-how-to-sideload-apps-on-your-android-tv-using-adb))\n\n### Upgrading the app\n\nAfter the initial install above, the app will automatically check for updates. The updates can be installed in settings.\n\nThe first time you attempt an update, the OS should guide you through enabling the required additional permissions for the app to install updates.\n\n## Compatibility\n\nRequires Android 6+ (or Fire TV OS 6+) and Jellyfin server `10.10.x` or `10.11.x` (tested on primarily `10.11.3`).\n\nThe app is tested on a variety of Android TV/Fire TV OS devices, but if you encounter issues, please file an issue!\n\n## Contributions\n\nIssues and pull requests are always welcome! Please check before submitting that your issue or pull request is not a duplicate.\n\nIf you plan to contribute, please read the [contributing guide](CONTRIBUTING.md)!\n\nYou can [help translate Wholphin](https://translate.codeberg.org/engage/wholphin/)!\n\n## Acknowledgements\n\n- Thanks to the Jellyfin team for creating and maintaining such a great open-source media server\n- Thanks to the official Jellyfin Android TV client developers, some code for creating the device direct play profile is adapted from there\n- Thanks to the Jellyfin Kotlin SDK developers for making it easier to interact with the Jellyfin server API\n- Thanks to numerous other libraries that make app development even possible\n\n## Additional screenshots\n\n### Movie library browsing\n<img width=\"1280\" height=\"771\" alt=\"0 3 0_movies\" src=\"https://github.com/user-attachments/assets/a49829b5-bc2c-4af9-8d5d-2f7d0973ce01\" />\n\n### Movie page\n<img width=\"1280\" height=\"720\" alt=\"0_3_5_movie\" src=\"https://github.com/user-attachments/assets/86af5889-6761-426a-8649-422f9d0a1dc0\" />\n\n### Series page\n<img width=\"1280\" height=\"720\" alt=\"0_3_5_series\" src=\"https://github.com/user-attachments/assets/2dcb2260-53ce-49d6-9088-72cbd4563c48\" />\n\n### Playlist\n<img width=\"1280\" height=\"771\" alt=\"0 3 0_playlist\" src=\"https://github.com/user-attachments/assets/7ca589ab-9c88-483a-b769-35ffb5663d9e\" />\n",
      "stars_today": 5
    },
    {
      "id": 5152285,
      "name": "okhttp",
      "full_name": "square/okhttp",
      "description": "Squareâ€™s meticulous HTTP client for the JVM, Android, and GraalVM.",
      "html_url": "https://github.com/square/okhttp",
      "stars": 46849,
      "forks": 9258,
      "language": "Kotlin",
      "topics": [
        "android",
        "graalvm",
        "java",
        "kotlin"
      ],
      "created_at": "2012-07-23T13:42:55Z",
      "updated_at": "2026-01-15T00:53:38Z",
      "pushed_at": "2026-01-14T08:37:02Z",
      "open_issues": 88,
      "owner": {
        "login": "square",
        "avatar_url": "https://avatars.githubusercontent.com/u/82592?v=4"
      },
      "readme": "OkHttp\n======\n\nSee the [project website][okhttp] for documentation and APIs.\n\nHTTP is the way modern applications network. Itâ€™s how we exchange data & media. Doing HTTP\nefficiently makes your stuff load faster and saves bandwidth.\n\nOkHttp is an HTTP client thatâ€™s efficient by default:\n\n * HTTP/2 support allows all requests to the same host to share a socket.\n * Connection pooling reduces request latency (if HTTP/2 isnâ€™t available).\n * Transparent GZIP shrinks download sizes.\n * Response caching avoids the network completely for repeat requests.\n\nOkHttp perseveres when the network is troublesome: it will silently recover from common connection\nproblems. If your service has multiple IP addresses, OkHttp will attempt alternate addresses if the\nfirst connect fails. This is necessary for IPv4+IPv6 and services hosted in redundant data\ncenters. OkHttp supports modern TLS features (TLS 1.3, ALPN, certificate pinning). It can be\nconfigured to fall back for broad connectivity.\n\nUsing OkHttp is easy. Its request/response API is designed with fluent builders and immutability. It\nsupports both synchronous blocking calls and async calls with callbacks.\n\nA well behaved user agent\n-------------------------\n\nOkHttp follows modern HTTP specifications such as\n\n* HTTP Semantics - [RFC 9110](https://datatracker.ietf.org/doc/html/rfc9110)\n* HTTP Caching- [RFC 9111](https://datatracker.ietf.org/doc/html/rfc9111)\n* HTTP/1.1 - [RFC 9112](https://datatracker.ietf.org/doc/html/rfc9112)\n* HTTP/2 - [RFC 9113](https://datatracker.ietf.org/doc/html/rfc9113)\n* Websockets - [RFC 6455](https://datatracker.ietf.org/doc/html/rfc6455)\n* SSE - [Server-sent events](https://html.spec.whatwg.org/multipage/server-sent-events.html#server-sent-events)\n\nWhere the spec is ambiguous, OkHttp follows modern user agents such as popular Browsers or common HTTP Libraries.\n\nOkHttp is principled and avoids being overly configurable, especially when such configuration is\nto workaround a buggy server, test invalid scenarios or that contradict the relevant RFC.\nOther HTTP libraries exist that fill that gap allowing extensive customisation including potentially\ninvalid requests.\n\nExample Limitations\n\n* Does not allow GET with a body.\n* Cache is not an interface with alternative implementations.\n\nGet a URL\n---------\n\nThis program downloads a URL and prints its contents as a string. [Full source][get_example].\n\n```java\nOkHttpClient client = new OkHttpClient();\n\nString run(String url) throws IOException {\n  Request request = new Request.Builder()\n      .url(url)\n      .build();\n\n  try (Response response = client.newCall(request).execute()) {\n    return response.body().string();\n  }\n}\n```\n\n\nPost to a Server\n----------------\n\nThis program posts data to a service. [Full source][post_example].\n\n```java\npublic static final MediaType JSON = MediaType.get(\"application/json\");\n\nOkHttpClient client = new OkHttpClient();\n\nString post(String url, String json) throws IOException {\n  RequestBody body = RequestBody.create(json, JSON);\n  Request request = new Request.Builder()\n      .url(url)\n      .post(body)\n      .build();\n  try (Response response = client.newCall(request).execute()) {\n    return response.body().string();\n  }\n}\n```\n\nFurther examples are on the [OkHttp Recipes page][recipes].\n\n\nRequirements\n------------\n\nOkHttp works on Android 5.0+ (API level 21+) and Java 8+.\n\nOn Android, OkHttp uses [AndroidX Startup][androidx_startup]. If you disable the initializer in the manifest,\nthen apps are responsible for calling `OkHttp.initialize(applicationContext)` in `Application.onCreate`.\n\nOkHttp depends on [Okio][okio] for high-performance I/O and the [Kotlin standard library][kotlin]. Both are small libraries with strong backward-compatibility.\n\nWe highly recommend you keep OkHttp up-to-date. As with auto-updating web browsers, staying current\nwith HTTPS clients is an important defense against potential security problems. [We\ntrack][tls_history] the dynamic TLS ecosystem and adjust OkHttp to improve connectivity and\nsecurity.\n\nOkHttp uses your platform's built-in TLS implementation. On Java platforms OkHttp also supports\n[Conscrypt][conscrypt], which integrates [BoringSSL](https://github.com/google/boringssl) with Java. OkHttp will use Conscrypt if it is\nthe first security provider:\n\n```java\nSecurity.insertProviderAt(Conscrypt.newProvider(), 1);\n```\n\nThe OkHttp `3.12.x` branch supports Android 2.3+ (API level 9+) and Java 7+. These platforms lack\nsupport for TLS 1.2 and should not be used.\n\n\nReleases\n--------\n\nOur [change log][changelog] has release history.\n\nThe latest release is available on [Maven Central](https://search.maven.org/artifact/com.squareup.okhttp3/okhttp/5.3.0/jar).\n\n```kotlin\nimplementation(\"com.squareup.okhttp3:okhttp:5.3.0\")\n```\n\nSnapshot builds are [available][snap]. [R8 and ProGuard][r8_proguard] rules are available.\n\nAlso, we have a [bill of materials (BOM)][bom] available to help you keep OkHttp artifacts up to date and be sure about version compatibility.\n\n```kotlin\n    dependencies {\n       // define a BOM and its version\n       implementation(platform(\"com.squareup.okhttp3:okhttp-bom:5.3.0\"))\n\n       // define any required OkHttp artifacts without version\n       implementation(\"com.squareup.okhttp3:okhttp\")\n       implementation(\"com.squareup.okhttp3:logging-interceptor\")\n    }\n```\n\nMaven and JVM Projects\n----------------------\n\nOkHttp is published as a Kotlin Multiplatform project. While Gradle handles this automatically,\nMaven projects must select between `okhttp-jvm` and `okhttp-android`. The `okhttp` artifact will be empty in\nMaven projects.\n\n```xml\n<dependencyManagement>\n  <dependencies>\n    <dependency>\n      <groupId>com.squareup.okhttp3</groupId>\n      <artifactId>okhttp-bom</artifactId>\n      <version>5.2.0</version>\n      <type>pom</type>\n      <scope>import</scope>\n    </dependency>\n  </dependencies>\n</dependencyManagement>\n```\n\n\n\n```xml\n<dependency>\n  <groupId>com.squareup.okhttp3</groupId>\n  <artifactId>okhttp-jvm</artifactId>\n  <!-- Remove after OkHttp 5.2.0 with updated BOM. -->\n  <version>5.1.0</version>\n</dependency>\n\n<dependency>\n  <groupId>com.squareup.okhttp3</groupId>\n  <artifactId>mockwebserver3</artifactId>\n</dependency>\n\n<dependency>\n  <groupId>com.squareup.okhttp3</groupId>\n  <artifactId>logging-interceptor</artifactId>\n</dependency>\n```\n\nMockWebServer\n-------------\n\nOkHttp includes a library for testing HTTP, HTTPS, and HTTP/2 clients.\n\nThe latest release is available on [Maven Central](https://search.maven.org/artifact/com.squareup.okhttp3/mockwebserver/5.3.0/jar).\n\n```kotlin\ntestImplementation(\"com.squareup.okhttp3:mockwebserver3:5.3.0\")\n```\n\nMockWebServer is used for firstly for internal testing, and for basic testing of apps using OkHttp client.\nIt is not a full featured HTTP testing library that is developed standalone. It is not being actively developed\nfor new features. As such you might find your needs outgrow MockWebServer and you may which to use a\nmore full featured testing library such as [MockServer](https://www.mock-server.com/).\n\nGraalVM Native Image\n--------------------\n\nBuilding your native images with [GraalVM] should work automatically.\n\nSee the okcurl module for an example build.\n\n```shell\n$ ./gradlew okcurl:nativeImage\n$ ./okcurl/build/graal/okcurl https://httpbin.org/get\n```\n\nJava Modules\n------------\n\nOkHttp (5.2+) implements Java 9 Modules.\n\nWith this in place Java builds should fail if apps attempt to use internal packages.\n\n```\nerror: package okhttp3.internal.platform is not visible\n    okhttp3.internal.platform.Platform.get();\n                    ^\n  (package okhttp3.internal.platform is declared in module okhttp3,\n    which does not export it to module com.bigco.sdk)\n```\n\nThe stable public API is based on the list of defined modules:\n\n- okhttp3\n- okhttp3.brotli\n- okhttp3.coroutines\n- okhttp3.dnsoverhttps\n- okhttp3.java.net.cookiejar\n- okhttp3.logging\n- okhttp3.sse\n- okhttp3.tls\n- okhttp3.urlconnection\n- mockwebserver3\n- mockwebserver3.junit4\n- mockwebserver3.junit5\n\nLicense\n-------\n\n```\nCopyright 2019 Square, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n [GraalVM]: https://www.graalvm.org/\n [androidx_startup]: https://developer.android.com/jetpack/androidx/releases/startup\n [bom]: https://docs.gradle.org/6.2/userguide/platforms.html#sub:bom_import\n [changelog]: https://square.github.io/okhttp/changelog/\n [conscrypt]: https://github.com/google/conscrypt/\n [get_example]: https://raw.github.com/square/okhttp/master/samples/guide/src/main/java/okhttp3/guide/GetExample.java\n [kotlin]: https://kotlinlang.org/\n [okhttp3_pro]: https://raw.githubusercontent.com/square/okhttp/master/okhttp/src/main/resources/META-INF/proguard/okhttp3.pro\n [okhttp]: https://square.github.io/okhttp/\n [okhttp_312x]: https://github.com/square/okhttp/tree/okhttp_3.12.x\n [okio]: https://github.com/square/okio\n [post_example]: https://raw.github.com/square/okhttp/master/samples/guide/src/main/java/okhttp3/guide/PostExample.java\n [r8_proguard]: https://square.github.io/okhttp/features/r8_proguard/\n [recipes]: https://square.github.io/okhttp/recipes/\n [snap]: https://s01.oss.sonatype.org/content/repositories/snapshots/\n [tls_history]: https://square.github.io/okhttp/tls_configuration_history/\n",
      "stars_today": 4
    },
    {
      "id": 7508411,
      "name": "RxJava",
      "full_name": "ReactiveX/RxJava",
      "description": "RxJava â€“ Reactive Extensions for the JVM â€“ a library for composing asynchronous and event-based programs using observable sequences for the Java VM.",
      "html_url": "https://github.com/ReactiveX/RxJava",
      "stars": 48513,
      "forks": 7604,
      "language": "Java",
      "topics": [
        "flow",
        "java",
        "reactive-streams",
        "rxjava"
      ],
      "created_at": "2013-01-08T20:11:48Z",
      "updated_at": "2026-01-14T16:07:00Z",
      "pushed_at": "2026-01-13T16:58:46Z",
      "open_issues": 15,
      "owner": {
        "login": "ReactiveX",
        "avatar_url": "https://avatars.githubusercontent.com/u/6407041?v=4"
      },
      "readme": "# RxJava: Reactive Extensions for the JVM\n\n<a href='https://github.com/ReactiveX/RxJava/actions?query=workflow%3ASnapshot'><img src='https://github.com/ReactiveX/RxJava/workflows/Snapshot/badge.svg'></a>\n[![codecov.io](http://codecov.io/github/ReactiveX/RxJava/coverage.svg?branch=3.x)](https://codecov.io/gh/ReactiveX/RxJava/branch/3.x)\n[![Maven Central](https://maven-badges.sml.io/sonatype-central/io.reactivex.rxjava3/rxjava/badge.svg)](https://maven-badges.sml.io/sonatype-central/io.reactivex.rxjava3/rxjava)\n[![Contribute with Gitpod](https://img.shields.io/badge/Contribute%20with-Gitpod-908a85?logo=gitpod)](https://gitpod.io/#https://github.com/ReactiveX/RxJava)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/ReactiveX/RxJava/badge)](https://securityscorecards.dev/viewer/?uri=github.com/ReactiveX/RxJava)\n\nRxJava is a Java VM implementation of [Reactive Extensions](http://reactivex.io): a library for composing asynchronous and event-based programs by using observable sequences.\n\nIt extends the [observer pattern](http://en.wikipedia.org/wiki/Observer_pattern) to support sequences of data/events and adds operators that allow you to compose sequences together declaratively while abstracting away concerns about things like low-level threading, synchronization, thread-safety and concurrent data structures.\n\n#### Version 3.x ([Javadoc](http://reactivex.io/RxJava/3.x/javadoc/))\n\n- Single dependency: [Reactive-Streams](https://github.com/reactive-streams/reactive-streams-jvm).\n- Java 8+ or Android API 21+ required.\n- Java 8 lambda-friendly API.\n- [Android](https://github.com/ReactiveX/RxAndroid) desugar friendly.\n- Fixed API mistakes and many limits of RxJava 2.\n- Intended to be a replacement for RxJava 2 with relatively few binary incompatible changes.\n- Non-opinionated about the source of concurrency (threads, pools, event loops, fibers, actors, etc.).\n- Async or synchronous execution.\n- Virtual time and schedulers for parameterized concurrency.\n- Test and diagnostic support via test schedulers, test consumers and plugin hooks.\n- Interop with newer JDK versions via 3rd party libraries, such as\n  - [Java 9 Flow API](https://github.com/akarnokd/RxJavaJdk9Interop#rxjavajdk9interop)\n  - [Java 21 Virtual Threads](https://github.com/akarnokd/RxJavaFiberInterop#rxjavafiberinterop)\n\nLearn more about RxJava in general on the <a href=\"https://github.com/ReactiveX/RxJava/wiki\">Wiki Home</a>.\n\n:information_source: Please read the [What's different in 3.0](https://github.com/ReactiveX/RxJava/wiki/What's-different-in-3.0) for details on the changes and migration information when upgrading from 2.x.\n\n#### Version 2.x\n\nThe [2.x version](https://github.com/ReactiveX/RxJava/tree/2.x) is end-of-life as of **February 28, 2021**. No further development, support, maintenance, PRs and updates will happen. The [Javadoc]([Javadoc](http://reactivex.io/RxJava/2.x/javadoc/)) of the very last version, **2.2.21**, will remain accessible.\n\n#### Version 1.x\n\nThe [1.x version](https://github.com/ReactiveX/RxJava/tree/1.x) is end-of-life as of **March 31, 2018**. No further development, support, maintenance, PRs and updates will happen. The [Javadoc]([Javadoc](http://reactivex.io/RxJava/1.x/javadoc/)) of the very last version, **1.3.8**, will remain accessible.\n\n## Getting started\n\n### Setting up the dependency\n\nThe first step is to include RxJava 3 into your project, for example, as a Gradle compile dependency:\n\n```groovy\nimplementation \"io.reactivex.rxjava3:rxjava:3.x.y\"\n```\n\n(Please replace `x` and `y` with the latest version numbers: [![Maven Central](https://maven-badges.sml.io/sonatype-central/io.reactivex.rxjava3/rxjava/badge.svg)](https://maven-badges.sml.io/sonatype-central/io.reactivex.rxjava3/rxjava)\n)\n\n### Hello World\n\nThe second is to write the **Hello World** program:\n\n```java\npackage rxjava.examples;\n\nimport io.reactivex.rxjava3.core.*;\n\npublic class HelloWorld {\n    public static void main(String[] args) {\n        Flowable.just(\"Hello world\").subscribe(System.out::println);\n    }\n}\n```\n\nNote that RxJava 3 components now live under `io.reactivex.rxjava3` and the base classes and interfaces live under `io.reactivex.rxjava3.core`.\n\n### Base classes\n\nRxJava 3 features several base classes you can discover operators on:\n\n  - [`io.reactivex.rxjava3.core.Flowable`](http://reactivex.io/RxJava/3.x/javadoc/io/reactivex/rxjava3/core/Flowable.html): 0..N flows, supporting Reactive-Streams and backpressure\n  - [`io.reactivex.rxjava3.core.Observable`](http://reactivex.io/RxJava/3.x/javadoc/io/reactivex/rxjava3/core/Observable.html): 0..N flows, no backpressure,\n  - [`io.reactivex.rxjava3.core.Single`](http://reactivex.io/RxJava/3.x/javadoc/io/reactivex/rxjava3/core/Single.html): a flow of exactly 1 item or an error,\n  - [`io.reactivex.rxjava3.core.Completable`](http://reactivex.io/RxJava/3.x/javadoc/io/reactivex/rxjava3/core/Completable.html): a flow without items but only a completion or error signal,\n  - [`io.reactivex.rxjava3.core.Maybe`](http://reactivex.io/RxJava/3.x/javadoc/io/reactivex/rxjava3/core/Maybe.html): a flow with no items, exactly one item or an error.\n\n### Some terminology\n\n#### Upstream, downstream\n\nThe dataflows in RxJava consist of a source, zero or more intermediate steps followed by a data consumer or combinator step (where the step is responsible to consume the dataflow by some means):\n\n```java\nsource.operator1().operator2().operator3().subscribe(consumer);\n\nsource.flatMap(value -> source.operator1().operator2().operator3());\n```\n\nHere, if we imagine ourselves on `operator2`, looking to the left towards the source is called the **upstream**. Looking to the right towards the subscriber/consumer is called the **downstream**. This is often more apparent when each element is written on a separate line:\n\n```java\nsource\n  .operator1()\n  .operator2()\n  .operator3()\n  .subscribe(consumer)\n```\n\n#### Objects in motion\n\nIn RxJava's documentation, **emission**, **emits**, **item**, **event**, **signal**, **data** and **message** are considered synonyms and represent the object traveling along the dataflow.\n\n#### Backpressure\n\nWhen the dataflow runs through asynchronous steps, each step may perform different things with different speed. To avoid overwhelming such steps, which usually would manifest itself as increased memory usage due to temporary buffering or the need for skipping/dropping data, so-called backpressure is applied, which is a form of flow control where the steps can express how many items are they ready to process. This allows constraining the memory usage of the dataflows in situations where there is generally no way for a step to know how many items the upstream will send to it.\n\nIn RxJava, the dedicated `Flowable` class is designated to support backpressure and `Observable` is dedicated to the non-backpressured operations (short sequences, GUI interactions, etc.). The other types, `Single`, `Maybe` and `Completable` don't support backpressure nor should they; there is always room to store one item temporarily.\n\n#### Assembly time\n\nThe preparation of dataflows by applying various intermediate operators happens in the so-called **assembly time**:\n\n```java\nFlowable<Integer> flow = Flowable.range(1, 5)\n.map(v -> v * v)\n.filter(v -> v % 3 == 0)\n;\n```\n\nAt this point, the data is not flowing yet and no side-effects are happening.\n\n#### Subscription time\n\nThis is a temporary state when `subscribe()` is called on a flow that establishes the chain of processing steps internally:\n\n```java\nflow.subscribe(System.out::println)\n````\n\nThis is when the **subscription side-effects** are triggered (see `doOnSubscribe`). Some sources block or start emitting items right away in this state.\n\n#### Runtime\n\nThis is the state when the flows are actively emitting items, errors or completion signals:\n\n```java\n\nObservable.create(emitter -> {\n     while (!emitter.isDisposed()) {\n         long time = System.currentTimeMillis();\n         emitter.onNext(time);\n         if (time % 2 != 0) {\n             emitter.onError(new IllegalStateException(\"Odd millisecond!\"));\n             break;\n         }\n     }\n})\n.subscribe(System.out::println, Throwable::printStackTrace);\n```\n\nPractically, this is when the body of the given example above executes.\n\n### Simple background computation\n\nOne of the common use cases for RxJava is to run some computation, network request on a background thread and show the results (or error) on the UI thread:\n\n```java\nimport io.reactivex.rxjava3.schedulers.Schedulers;\n\nFlowable.fromCallable(() -> {\n    Thread.sleep(1000); //  imitate expensive computation\n    return \"Done\";\n})\n  .subscribeOn(Schedulers.io())\n  .observeOn(Schedulers.single())\n  .subscribe(System.out::println, Throwable::printStackTrace);\n\nThread.sleep(2000); // <--- wait for the flow to finish\n```\n\nThis style of chaining methods is called a **fluent API** which resembles the **builder pattern**. However, RxJava's reactive types are immutable; each of the method calls returns a new `Flowable` with added behavior. To illustrate, the example can be rewritten as follows:\n\n```java\nFlowable<String> source = Flowable.fromCallable(() -> {\n    Thread.sleep(1000); //  imitate expensive computation\n    return \"Done\";\n});\n\nFlowable<String> runBackground = source.subscribeOn(Schedulers.io());\n\nFlowable<String> showForeground = runBackground.observeOn(Schedulers.single());\n\nshowForeground.subscribe(System.out::println, Throwable::printStackTrace);\n\nThread.sleep(2000);\n```\n\nTypically, you can move computations or blocking IO to some other thread via `subscribeOn`. Once the data is ready, you can make sure they get processed on the foreground or GUI thread via `observeOn`. \n\n### Schedulers\n\nRxJava operators don't work with `Thread`s or `ExecutorService`s directly but with so-called `Scheduler`s that abstract away sources of concurrency behind a uniform API. RxJava 3 features several standard schedulers accessible via `Schedulers` utility class. \n\n- `Schedulers.computation()`: Run computation intensive work on a fixed number of dedicated threads in the background. Most asynchronous operators use this as their default `Scheduler`.\n- `Schedulers.io()`: Run I/O-like or blocking operations on a dynamically changing set of threads.\n- `Schedulers.single()`: Run work on a single thread in a sequential and FIFO manner.\n- `Schedulers.trampoline()`: Run work in a sequential and FIFO manner in one of the participating threads, usually for testing purposes.\n\nThese are available on all JVM platforms but some specific platforms, such as Android, have their own typical `Scheduler`s defined: `AndroidSchedulers.mainThread()`, `SwingScheduler.instance()` or `JavaFXScheduler.platform()`.\n\nIn addition, there is an option to wrap an existing `Executor` (and its subtypes such as `ExecutorService`) into a `Scheduler` via `Schedulers.from(Executor)`. This can be used, for example, to have a larger but still fixed pool of threads (unlike `computation()` and `io()` respectively).\n\nThe `Thread.sleep(2000);` at the end is no accident. In RxJava the default `Scheduler`s run on daemon threads, which means once the Java main thread exits, they all get stopped and background computations may never happen. Sleeping for some time in this example situations lets you see the output of the flow on the console with time to spare.\n\n### Concurrency within a flow\n\nFlows in RxJava are sequential in nature split into processing stages that may run **concurrently** with each other:\n\n```java\nFlowable.range(1, 10)\n  .observeOn(Schedulers.computation())\n  .map(v -> v * v)\n  .blockingSubscribe(System.out::println);\n```\n\nThis example flow squares the numbers from 1 to 10 on the **computation** `Scheduler` and consumes the results on the \"main\" thread (more precisely, the caller thread of `blockingSubscribe`). However, the lambda `v -> v * v` doesn't run in parallel for this flow; it receives the values 1 to 10 on the same computation thread one after the other.\n\n### Parallel processing\n\nProcessing the numbers 1 to 10 in parallel is a bit more involved:\n\n```java\nFlowable.range(1, 10)\n  .flatMap(v ->\n      Flowable.just(v)\n        .subscribeOn(Schedulers.computation())\n        .map(w -> w * w)\n  )\n  .blockingSubscribe(System.out::println);\n```\n\nPractically, parallelism in RxJava means running independent flows and merging their results back into a single flow. The operator `flatMap` does this by first mapping each number from 1 to 10 into its own individual `Flowable`, runs them and merges the computed squares.\n\nNote, however, that `flatMap` doesn't guarantee any order and the items from the inner flows may end up interleaved. There are alternative operators:\n\n  - `concatMap` that maps and runs one inner flow at a time and\n  - `concatMapEager` which runs all inner flows \"at once\" but the output flow will be in the order those inner flows were created.\n\nAlternatively, the `Flowable.parallel()` operator and the `ParallelFlowable` type help achieve the same parallel processing pattern:\n\n```java\nFlowable.range(1, 10)\n  .parallel()\n  .runOn(Schedulers.computation())\n  .map(v -> v * v)\n  .sequential()\n  .blockingSubscribe(System.out::println);\n```\n\n### Dependent sub-flows\n\n`flatMap` is a powerful operator and helps in a lot of situations. For example, given a service that returns a `Flowable`, we'd like to call another service with values emitted by the first service:\n\n```java\nFlowable<Inventory> inventorySource = warehouse.getInventoryAsync();\n\ninventorySource\n    .flatMap(inventoryItem -> erp.getDemandAsync(inventoryItem.getId())\n            .map(demand -> \"Item \" + inventoryItem.getName() + \" has demand \" + demand))\n    .subscribe(System.out::println);\n```\n\n### Continuations\n\nSometimes, when an item has become available, one would like to perform some dependent computations on it. This is sometimes called **continuations** and, depending on what should happen and what types are involved, may involve various operators to accomplish.\n\n#### Dependent\n\nThe most typical scenario is to given a value, invoke another service, await and continue with its result:\n\n```java\nservice.apiCall()\n.flatMap(value -> service.anotherApiCall(value))\n.flatMap(next -> service.finalCall(next))\n```\n\nIt is often the case also that later sequences would require values from earlier mappings. This can be achieved by moving the outer `flatMap` into the inner parts of the previous `flatMap` for example:\n\n```java\nservice.apiCall()\n.flatMap(value ->\n    service.anotherApiCall(value)\n    .flatMap(next -> service.finalCallBoth(value, next))\n)\n```\n\nHere, the original `value` will be available inside the inner `flatMap`, courtesy of lambda variable capture.\n\n#### Non-dependent\n\nIn other scenarios, the result(s) of the first source/dataflow is irrelevant and one would like to continue with a quasi independent another source. Here, `flatMap` works as well:\n\n```java\nObservable continued = sourceObservable.flatMapSingle(ignored -> someSingleSource)\ncontinued.map(v -> v.toString())\n  .subscribe(System.out::println, Throwable::printStackTrace);\n```\n\nhowever, the continuation in this case stays `Observable` instead of the likely more appropriate `Single`. (This is understandable because\nfrom the perspective of `flatMapSingle`, `sourceObservable` is a multi-valued source and thus the mapping may result in multiple values as well).\n\nOften though there is a way that is somewhat more expressive (and also lower overhead) by using `Completable` as the mediator and its operator `andThen` to resume with something else:\n\n```java\nsourceObservable\n  .ignoreElements()           // returns Completable\n  .andThen(someSingleSource)\n  .map(v -> v.toString())\n```\n\nThe only dependency between the `sourceObservable` and the `someSingleSource` is that the former should complete normally in order for the latter to be consumed.\n\n#### Deferred-dependent\n\nSometimes, there is an implicit data dependency between the previous sequence and the new sequence that, for some reason, was not flowing through the \"regular channels\". One would be inclined to write such continuations as follows:\n\n```java\nAtomicInteger count = new AtomicInteger();\n\nObservable.range(1, 10)\n  .doOnNext(ignored -> count.incrementAndGet())\n  .ignoreElements()\n  .andThen(Single.just(count.get()))\n  .subscribe(System.out::println);\n```\n\nUnfortunately, this prints `0` because `Single.just(count.get())` is evaluated at **assembly time** when the dataflow hasn't even run yet. We need something that defers the evaluation of this `Single` source until **runtime** when the main source completes:\n\n```java\nAtomicInteger count = new AtomicInteger();\n\nObservable.range(1, 10)\n  .doOnNext(ignored -> count.incrementAndGet())\n  .ignoreElements()\n  .andThen(Single.defer(() -> Single.just(count.get())))\n  .subscribe(System.out::println);\n```\n\nor\n\n```java\nAtomicInteger count = new AtomicInteger();\n\nObservable.range(1, 10)\n  .doOnNext(ignored -> count.incrementAndGet())\n  .ignoreElements()\n  .andThen(Single.fromCallable(() -> count.get()))\n  .subscribe(System.out::println);\n```\n\n\n### Type conversions\n\nSometimes, a source or service returns a different type than the flow that is supposed to work with it. For example, in the inventory example above, `getDemandAsync` could return a `Single<DemandRecord>`. If the code example is left unchanged, this will result in a compile-time error (however, often with a misleading error message about lack of overload).\n\nIn such situations, there are usually two options to fix the transformation: 1) convert to the desired type or 2) find and use an overload of the specific operator supporting the different type.\n\n#### Converting to the desired type\n\nEach reactive base class features operators that can perform such conversions, including the protocol conversions, to match some other type. The following matrix shows the available conversion options:\n\n|          | Flowable | Observable | Single | Maybe | Completable |\n|----------|----------|------------|--------|-------|-------------|\n|**Flowable**  |          | `toObservable` | `first`, `firstOrError`, `single`, `singleOrError`, `last`, `lastOrError`<sup>1</sup> | `firstElement`, `singleElement`, `lastElement` | `ignoreElements` |\n|**Observable**| `toFlowable`<sup>2</sup> |  | `first`, `firstOrError`, `single`, `singleOrError`, `last`, `lastOrError`<sup>1</sup> | `firstElement`, `singleElement`, `lastElement` | `ignoreElements` |\n|**Single** | `toFlowable`<sup>3</sup> | `toObservable` |  | `toMaybe` | `ignoreElement` |\n|**Maybe** | `toFlowable`<sup>3</sup> | `toObservable` | `toSingle` |  | `ignoreElement` |\n|**Completable** | `toFlowable` | `toObservable` | `toSingle` | `toMaybe` |  |\n\n<sup>1</sup>: When turning a multi-valued source into a single-valued source, one should decide which of the many source values should be considered as the result.\n\n<sup>2</sup>: Turning an `Observable` into `Flowable` requires an additional decision: what to do with the potential unconstrained flow\nof the source `Observable`? There are several strategies available (such as buffering, dropping, keeping the latest) via the `BackpressureStrategy` parameter or via standard `Flowable` operators such as `onBackpressureBuffer`, `onBackpressureDrop`, `onBackpressureLatest` which also\nallow further customization of the backpressure behavior.\n\n<sup>3</sup>: When there is only (at most) one source item, there is no problem with backpressure as it can be always stored until the downstream is ready to consume.\n\n\n#### Using an overload with the desired type\n\nMany frequently used operator has overloads that can deal with the other types. These are usually named with the suffix of the target type:\n\n| Operator | Overloads |\n|----------|-----------|\n| `flatMap` | `flatMapSingle`, `flatMapMaybe`, `flatMapCompletable`, `flatMapIterable` |\n| `concatMap` | `concatMapSingle`, `concatMapMaybe`, `concatMapCompletable`, `concatMapIterable` |\n| `switchMap` | `switchMapSingle`, `switchMapMaybe`, `switchMapCompletable` |\n\nThe reason these operators have a suffix instead of simply having the same name with different signature is type erasure. Java doesn't consider signatures such as `operator(Function<T, Single<R>>)` and `operator(Function<T, Maybe<R>>)` different (unlike C#) and due to erasure, the two `operator`s would end up as duplicate methods with the same signature.\n\n### Operator naming conventions\n\nNaming in programming is one of the hardest things as names are expected to be not long, expressive, capturing and easily memorable. Unfortunately, the target language (and pre-existing conventions) may not give too much help in this regard (unusable keywords, type erasure, type ambiguities, etc.).\n\n#### Unusable keywords\n\nIn the original Rx.NET, the operator that emits a single item and then completes is called `Return(T)`. Since the Java convention is to have a lowercase letter start a method name, this would have been `return(T)` which is a keyword in Java and thus not available. Therefore, RxJava chose to name this operator `just(T)`. The same limitation exists for the operator `Switch`, which had to be named `switchOnNext`. Yet another example is `Catch` which was named `onErrorResumeNext`.\n\n#### Type erasure\n\nMany operators that expect the user to provide some function returning a reactive type can't be overloaded because the type erasure around a `Function<T, X>` turns such method signatures into duplicates. RxJava chose to name such operators by appending the type as suffix as well:\n\n```java\nFlowable<R> flatMap(Function<? super T, ? extends Publisher<? extends R>> mapper)\n\nFlowable<R> flatMapMaybe(Function<? super T, ? extends MaybeSource<? extends R>> mapper)\n```\n\n#### Type ambiguities\n\nEven though certain operators have no problems from type erasure, their signature may turn up being ambiguous, especially if one uses Java 8 and lambdas. For example, there are several overloads of `concatWith` taking the various other reactive base types as arguments (for providing convenience and performance benefits in the underlying implementation):\n\n```java\nFlowable<T> concatWith(Publisher<? extends T> other);\n\nFlowable<T> concatWith(SingleSource<? extends T> other);\n```\n\nBoth `Publisher` and `SingleSource` appear as functional interfaces (types with one abstract method) and may encourage users to try to provide a lambda expression:\n\n```java\nsomeSource.concatWith(s -> Single.just(2))\n.subscribe(System.out::println, Throwable::printStackTrace);\n```\n\nUnfortunately, this approach doesn't work and the example does not print `2` at all. In fact, since version 2.1.10, it doesn't\neven compile because at least 4 `concatWith` overloads exist and the compiler finds the code above ambiguous.\n\nThe user in such situations probably wanted to defer some computation until the `someSource` has completed, thus the correct\nunambiguous operator should have been `defer`:\n\n```java\nsomeSource.concatWith(Single.defer(() -> Single.just(2)))\n.subscribe(System.out::println, Throwable::printStackTrace);\n```\n\nSometimes, a suffix is added to avoid logical ambiguities that may compile but produce the wrong type in a flow:\n\n```java\nFlowable<T> merge(Publisher<? extends Publisher<? extends T>> sources);\n\nFlowable<T> mergeArray(Publisher<? extends T>... sources);\n```\n\nThis can get also ambiguous when functional interface types get involved as the type argument `T`.\n\n#### Error handling\n\nDataflows can fail, at which point the error is emitted to the consumer(s). Sometimes though, multiple sources may fail at which point there is a choice whether or not wait for all of them to complete or fail. To indicate this opportunity, many operator names are suffixed with the `DelayError` words (while others feature a `delayError` or `delayErrors` boolean flag in one of their overloads):\n\n```java\nFlowable<T> concat(Publisher<? extends Publisher<? extends T>> sources);\n\nFlowable<T> concatDelayError(Publisher<? extends Publisher<? extends T>> sources);\n```\n\nOf course, suffixes of various kinds may appear together:\n\n```java\nFlowable<T> concatArrayEagerDelayError(Publisher<? extends T>... sources);\n```\n\n#### Base class vs base type\n\nThe base classes can be considered heavy due to the sheer number of static and instance methods on them. RxJava 3's design was heavily influenced by the [Reactive Streams](https://github.com/reactive-streams/reactive-streams-jvm#reactive-streams) specification, therefore, the library features a class and an interface per each reactive type:\n\n| Type | Class | Interface | Consumer |\n|------|-------|-----------|----------|\n| 0..N backpressured | `Flowable` | `Publisher`<sup>1</sup> | `Subscriber` |\n| 0..N unbounded | `Observable` | `ObservableSource`<sup>2</sup> | `Observer` |\n| 1 element or error | `Single` | `SingleSource` | `SingleObserver` |\n| 0..1 element or error | `Maybe` | `MaybeSource` | `MaybeObserver` |\n| 0 element or error | `Completable` | `CompletableSource` | `CompletableObserver` |\n\n<sup>1</sup>The `org.reactivestreams.Publisher` is part of the external Reactive Streams library. It is the main type to interact with other reactive libraries through a standardized mechanism governed by the [Reactive Streams specification](https://github.com/reactive-streams/reactive-streams-jvm#specification).\n\n<sup>2</sup>The naming convention of the interface was to append `Source` to the semi-traditional class name. There is no `FlowableSource` since `Publisher` is provided by the Reactive Streams library (and subtyping it wouldn't have helped with interoperation either). These interfaces are, however, not standard in the sense of the Reactive Streams specification and are currently RxJava specific only.\n\n### R8 and ProGuard settings\n\nBy default, RxJava itself doesn't require any ProGuard/R8 settings and should work without problems. Unfortunately, the Reactive Streams dependency since version 1.0.3 has embedded Java 9 class files in its JAR that can cause warnings with the plain ProGuard:\n\n```\nWarning: org.reactivestreams.FlowAdapters$FlowPublisherFromReactive: can't find superclass or interface java.util.concurrent.Flow$Publisher\nWarning: org.reactivestreams.FlowAdapters$FlowToReactiveProcessor: can't find superclass or interface java.util.concurrent.Flow$Processor\nWarning: org.reactivestreams.FlowAdapters$FlowToReactiveSubscriber: can't find superclass or interface java.util.concurrent.Flow$Subscriber\nWarning: org.reactivestreams.FlowAdapters$FlowToReactiveSubscription: can't find superclass or interface java.util.concurrent.Flow$Subscription\nWarning: org.reactivestreams.FlowAdapters: can't find referenced class java.util.concurrent.Flow$Publisher\n```\n\nIt is recommended one sets up the following `-dontwarn` entry in the application's `proguard-ruleset` file:\n\n```\n-dontwarn java.util.concurrent.Flow*\n```\n\nFor R8, the RxJava jar includes the `META-INF/proguard/rxjava3.pro` with the same no-warning clause and should apply automatically.\n\n### Further reading\n\nFor further details, consult the [wiki](https://github.com/ReactiveX/RxJava/wiki).\n\n## Communication\n\n- Google Group: [RxJava](http://groups.google.com/d/forum/rxjava)\n- Twitter: [@RxJava](http://twitter.com/RxJava)\n- [GitHub Issues](https://github.com/ReactiveX/RxJava/issues)\n- StackOverflow: [rx-java](http://stackoverflow.com/questions/tagged/rx-java), [rx-java2](http://stackoverflow.com/questions/tagged/rx-java2) and  [rx-java3](http://stackoverflow.com/questions/tagged/rx-java3)\n- [Gitter.im](https://gitter.im/ReactiveX/RxJava)\n\n## Versioning\n\nVersion 3.x is in development. Bugfixes will be applied to both 2.x and 3.x branches, but new features will only be added to 3.x.\n\nMinor 3.x increments (such as 3.1, 3.2, etc) will occur when non-trivial new functionality is added or significant enhancements or bug fixes occur that may have behavioral changes that may affect some edge cases (such as dependence on behavior resulting from a bug). An example of an enhancement that would classify as this is adding reactive pull backpressure support to an operator that previously did not support it. This should be backwards compatible but does behave differently.\n\nPatch 3.x.y increments (such as 3.0.0 -> 3.0.1, 3.3.1 -> 3.3.2, etc) will occur for bug fixes and trivial functionality (like adding a method overload). New functionality marked with an [`@Beta`][beta source link] or [`@Experimental`][experimental source link] annotation can also be added in the patch releases to allow rapid exploration and iteration of unstable new functionality. \n\n#### @Beta\n\nAPIs marked with the [`@Beta`][beta source link] annotation at the class or method level are subject to change. They can be modified in any way, or even removed, at any time. If your code is a library itself (i.e. it is used on the CLASSPATH of users outside your control), you should not use beta APIs, unless you repackage them (e.g. using ProGuard, shading, etc).\n\n#### @Experimental\n\nAPIs marked with the [`@Experimental`][experimental source link] annotation at the class or method level will almost certainly change. They can be modified in any way, or even removed, at any time. You should not use or rely on them in any production code. They are purely to allow broad testing and feedback. \n\n#### @Deprecated\n\nAPIs marked with the `@Deprecated` annotation at the class or method level will remain supported until the next major release, but it is recommended to stop using them. \n\n#### io.reactivex.rxjava3.internal.*\n\nAll code inside the `io.reactivex.rxjava3.internal.*` packages are considered private API and should not be relied upon at all. It can change at any time. \n\n## Full Documentation\n\n- [Wiki](https://github.com/ReactiveX/RxJava/wiki)\n- [Javadoc](http://reactivex.io/RxJava/3.x/javadoc/)\n- [Latest snaphot Javadoc](http://reactivex.io/RxJava/3.x/javadoc/snapshot/)\n- Javadoc of a specific [release version](https://github.com/ReactiveX/RxJava/tags): `http://reactivex.io/RxJava/3.x/javadoc/3.x.y/`\n\n## Binaries\n\nBinaries and dependency information for Maven, Ivy, Gradle and others can be found at [http://search.maven.org](http://search.maven.org/#search%7Cga%7C1%7Cio.reactivex.rxjava3).\n\nExample for Gradle:\n\n```groovy\nimplementation 'io.reactivex.rxjava3:rxjava:x.y.z'\n```\n\nand for Maven:\n\n```xml\n<dependency>\n    <groupId>io.reactivex.rxjava3</groupId>\n    <artifactId>rxjava</artifactId>\n    <version>x.y.z</version>\n</dependency>\n```\nand for Ivy:\n\n```xml\n<dependency org=\"io.reactivex.rxjava3\" name=\"rxjava\" rev=\"x.y.z\" />\n```\n\n### Snapshots\n\nSnapshots after May 19st, 2025 are available via https://central.sonatype.com/repository/maven-snapshots/io/reactivex/rxjava3/rxjava/\n\n```groovy\nrepositories {\n  maven { url 'https://central.sonatype.com/repository/maven-snapshots' }\n}\n\ndependencies {\n  implementation 'io.reactivex.rxjava3:rxjava:3.0.0-SNAPSHOT'\n}\n```\n\nJavaDoc snapshots are available at https://reactivex.io/RxJava/3.x/javadoc/snapshot\n\n## Build\n\nTo build:\n\n```\n$ git clone git@github.com:ReactiveX/RxJava.git\n$ cd RxJava/\n$ ./gradlew build\n```\n\nFurther details on building can be found on the [Getting Started](https://github.com/ReactiveX/RxJava/wiki/Getting-Started) page of the wiki.\n\n## Bugs and Feedback\n\nFor bugs, questions and discussions please use the [Github Issues](https://github.com/ReactiveX/RxJava/issues).\n\n \n## LICENSE\n\n    Copyright (c) 2016-present, RxJava Contributors.\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n[beta source link]: https://github.com/ReactiveX/RxJava/blob/3.x/src/main/java/io/reactivex/rxjava3/annotations/Beta.java\n[experimental source link]: https://github.com/ReactiveX/RxJava/blob/3.x/src/main/java/io/reactivex/rxjava3/annotations/Experimental.java\n",
      "stars_today": 4
    },
    {
      "id": 1903522,
      "name": "php-src",
      "full_name": "php/php-src",
      "description": "The PHP Interpreter",
      "html_url": "https://github.com/php/php-src",
      "stars": 39782,
      "forks": 7990,
      "language": "C",
      "topics": [],
      "created_at": "2011-06-16T01:52:25Z",
      "updated_at": "2026-01-14T22:11:04Z",
      "pushed_at": "2026-01-14T21:59:25Z",
      "open_issues": 1694,
      "owner": {
        "login": "php",
        "avatar_url": "https://avatars.githubusercontent.com/u/25158?v=4"
      },
      "readme": "<div align=\"center\">\n    <a href=\"https://www.php.net\">\n        <img\n            alt=\"PHP\"\n            src=\"https://www.php.net/images/logos/new-php-logo.svg\"\n            width=\"150\">\n    </a>\n</div>\n\n# The PHP Interpreter\n\nPHP is a popular general-purpose scripting language that is especially suited to\nweb development. Fast, flexible and pragmatic, PHP powers everything from your\nblog to the most popular websites in the world. PHP is distributed under the\n[PHP License v3.01](LICENSE).\n\n[![Push](https://github.com/php/php-src/actions/workflows/push.yml/badge.svg)](https://github.com/php/php-src/actions/workflows/push.yml)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/php.svg)](https://issues.oss-fuzz.com/issues?q=project:php)\n\n## Documentation\n\nThe PHP manual is available at [php.net/docs](https://www.php.net/docs).\n\n## Installation\n\n### Prebuilt packages and binaries\n\nPrebuilt packages and binaries can be used to get up and running fast with PHP.\n\nFor Windows, the PHP binaries can be obtained from\n[windows.php.net](https://windows.php.net). After extracting the archive the\n`*.exe` files are ready to use.\n\nFor other systems, see the [installation chapter](https://www.php.net/install).\n\n### Building PHP source code\n\n*For Windows, see [Build your own PHP on Windows](https://wiki.php.net/internals/windows/stepbystepbuild_sdk_2).*\n\nFor a minimal PHP build from Git, you will need autoconf, bison, and re2c. For\na default build, you will additionally need libxml2 and libsqlite3.\n\nOn Ubuntu, you can install these using:\n\n```shell\nsudo apt install -y pkg-config build-essential autoconf bison re2c libxml2-dev libsqlite3-dev\n```\n\nOn Fedora, you can install these using:\n\n```shell\nsudo dnf install re2c bison autoconf make libtool ccache libxml2-devel sqlite-devel\n```\n\nOn MacOS, you can install these using `brew`:\n\n```shell\nbrew install autoconf bison re2c libiconv libxml2 sqlite\n```\n\nor with `MacPorts`:\n\n```shell\nsudo port install autoconf bison re2c libiconv libxml2 sqlite3\n```\n\nGenerate configure:\n\n```shell\n./buildconf\n```\n\nConfigure your build. `--enable-debug` is recommended for development, see\n`./configure --help` for a full list of options.\n\n```shell\n# For development\n./configure --enable-debug\n# For production\n./configure\n```\n\nBuild PHP. To speed up the build, specify the maximum number of jobs using the\n`-j` argument:\n\n```shell\nmake -j4\n```\n\nThe number of jobs should usually match the number of available cores, which\ncan be determined using `nproc`.\n\n## Testing PHP source code\n\nPHP ships with an extensive test suite, the command `make test` is used after\nsuccessful compilation of the sources to run this test suite.\n\nIt is possible to run tests using multiple cores by setting `-jN` in\n`TEST_PHP_ARGS` or `TESTS`:\n\n```shell\nmake TEST_PHP_ARGS=-j4 test\n```\n\nShall run `make test` with a maximum of 4 concurrent jobs: Generally the maximum\nnumber of jobs should not exceed the number of cores available.\n\nUse the `TEST_PHP_ARGS` or `TESTS` variable to test only specific directories:\n\n```shell\nmake TESTS=tests/lang/ test\n```\n\nThe [qa.php.net](https://qa.php.net) site provides more detailed info about\ntesting and quality assurance.\n\n## Installing PHP built from source\n\nAfter a successful build (and test), PHP may be installed with:\n\n```shell\nmake install\n```\n\nDepending on your permissions and prefix, `make install` may need superuser\npermissions.\n\n## PHP extensions\n\nExtensions provide additional functionality on top of PHP. PHP consists of many\nessential bundled extensions. Additional extensions can be found in the PHP\nExtension Community Library - [PECL](https://pecl.php.net).\n\n## Contributing\n\nThe PHP source code is located in the Git repository at\n[github.com/php/php-src](https://github.com/php/php-src). Contributions are most\nwelcome by forking the repository and sending a pull request.\n\nDiscussions are done on GitHub, but depending on the topic can also be relayed\nto the official PHP developer mailing list internals@lists.php.net.\n\nNew features require an RFC and must be accepted by the developers. See\n[Request for comments - RFC](https://wiki.php.net/rfc) and\n[Voting on PHP features](https://wiki.php.net/rfc/voting) for more information\non the process.\n\nBug fixes don't require an RFC. If the bug has a GitHub issue, reference it in\nthe commit message using `GH-NNNNNN`. Use `#NNNNNN` for tickets in the old\n[bugs.php.net](https://bugs.php.net) bug tracker.\n\n    Fix GH-7815: php_uname doesn't recognise latest Windows versions\n    Fix #55371: get_magic_quotes_gpc() throws deprecation warning\n\nSee [Git workflow](https://wiki.php.net/vcs/gitworkflow) for details on how pull\nrequests are merged.\n\n### Guidelines for contributors\n\nSee further documents in the repository for more information on how to\ncontribute:\n\n- [Contributing to PHP](/CONTRIBUTING.md)\n- [PHP coding standards](/CODING_STANDARDS.md)\n- [Internal documentation](https://php.github.io/php-src/)\n- [Mailing list rules](/docs/mailinglist-rules.md)\n- [PHP release process](/docs/release-process.md)\n\n## Credits\n\nFor the list of people who've put work into PHP, please see the\n[PHP credits page](https://www.php.net/credits.php).\n",
      "stars_today": 4
    },
    {
      "id": 62117812,
      "name": "pulsar",
      "full_name": "apache/pulsar",
      "description": "Apache Pulsar - distributed pub-sub messaging system",
      "html_url": "https://github.com/apache/pulsar",
      "stars": 15049,
      "forks": 3698,
      "language": "Java",
      "topics": [
        "event-streaming",
        "messaging",
        "pubsub",
        "pulsar",
        "queuing",
        "streaming"
      ],
      "created_at": "2016-06-28T07:00:03Z",
      "updated_at": "2026-01-14T23:07:25Z",
      "pushed_at": "2026-01-14T23:07:19Z",
      "open_issues": 1628,
      "owner": {
        "login": "apache",
        "avatar_url": "https://avatars.githubusercontent.com/u/47359?v=4"
      },
      "readme": "<!--\n\n    Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n-->\n\n![logo](https://pulsar.apache.org/img/pulsar.svg)\n\n[![docker pull](https://img.shields.io/docker/pulls/apachepulsar/pulsar-all.svg)](https://hub.docker.com/r/apachepulsar/pulsar)\n[![contributors](https://img.shields.io/github/contributors-anon/apache/pulsar)](https://github.com/apache/pulsar/graphs/contributors)\n[![last commit](https://img.shields.io/github/last-commit/apache/pulsar)](https://github.com/apache/pulsar/commits/master)\n[![release](https://img.shields.io/github/v/release/apache/pulsar?sort=semver)](https://pulsar.apache.org/download/)\n[![downloads](https://img.shields.io/github/downloads/apache/pulsar/total)](https://pulsar.apache.org/download/)\n\nPulsar is a distributed pub-sub messaging platform with a very\nflexible messaging model and an intuitive client API.\n\nLearn more about Pulsar at https://pulsar.apache.org\n\n## Main features\n\n- Horizontally scalable (Millions of independent topics and millions\n  of messages published per second)\n- Strong ordering and consistency guarantees\n- Low latency durable storage\n- Topic and queue semantics\n- Load balancer\n- Designed for being deployed as a hosted service:\n  - Multi-tenant\n  - Authentication\n  - Authorization\n  - Quotas\n  - Support mixing very different workloads\n  - Optional hardware isolation\n- Keeps track of consumer cursor position\n- REST API for provisioning, admin and stats\n- Geo replication\n- Transparent handling of partitioned topics\n- Transparent batching of messages\n\n## Repositories\n\nThis repository is the main repository of Apache Pulsar. Pulsar PMC also maintains other repositories for\ncomponents in the Pulsar ecosystem, including connectors, adapters, and other language clients.\n\n- [Pulsar Core](https://github.com/apache/pulsar)\n\n### Helm Chart\n\n- [Pulsar Helm Chart](https://github.com/apache/pulsar-helm-chart)\n\n### Ecosystem\n\n- [Pulsar Adapters](https://github.com/apache/pulsar-adapters)\n\n### Clients\n\n- [.NET/C# Client](https://github.com/apache/pulsar-dotpulsar)\n- [C++ Client](https://github.com/apache/pulsar-client-cpp)\n- [Go Client](https://github.com/apache/pulsar-client-go)\n- [NodeJS Client](https://github.com/apache/pulsar-client-node)\n- [Python Client](https://github.com/apache/pulsar-client-python)\n- [Reactive Java Client](https://github.com/apache/pulsar-client-reactive)\n\n### Dashboard & Management Tools\n\n- [Pulsar Manager](https://github.com/apache/pulsar-manager)\n\n### Website\n\n- [Pulsar Site](https://github.com/apache/pulsar-site)\n\n### CI/CD\n\n- [Pulsar CI](https://github.com/apache/pulsar-test-infra)\n\n### Archived/Halted\n\n- [Pulsar Connectors](https://github.com/apache/pulsar-connectors) (bundled as [pulsar-io](pulsar-io))\n- [Pulsar Translation](https://github.com/apache/pulsar-translation)\n- [Pulsar SQL (Pulsar Presto Connector)](https://github.com/apache/pulsar-presto) (bundled as [pulsar-sql](pulsar-sql))\n- [Ruby Client](https://github.com/apache/pulsar-client-ruby)\n\n## Pulsar Runtime Java Version Recommendation\n\n> **Note**:\n>\n> When using Java versions, it is recommended to\n> use [a recent version of a particular Java release (17 or 21)](https://adoptium.net/en-GB/temurin/releases?version=21&os=linux&arch=any)\n> with the most recent bug fixes and security patches.\n> For example, the JVM bug [JDK-8351933](https://bugs.openjdk.org/browse/JDK-8351933) can cause stability issues in\n> Pulsar.\n> [JDK-8351933](https://bugs.openjdk.org/browse/JDK-8351933) was fixed in Java 17.0.17+ and Java 21.0.8+.\n> Pulsar Docker images come with the most recent Java version at the time of release.\n\nWhen using the Pulsar Java client, it is recommended to [set specific system properties and JVM options](https://pulsar.apache.org/docs/next/client-libraries-java-setup/#java-client-performance) to allow optimal performance.\n\n\n### pulsar ver >= 4.1 and master branch\n\n| Component       | Java Version |\n|-----------------|:------------:|\n| Broker          |      21      |\n| Functions / IO  |      21      |\n| CLI             |   17 or 21   |\n| Java Client     |   17 or 21   |\n\nDocker image Java runtime: 21\n\n### 3.3 <= pulsar ver <= 4.0\n\n| Component       |  Java Version   |\n|-----------------|:---------------:|\n| Broker          |       21        |\n| Functions / IO  |       21        |\n| CLI             |    17 or 21     |\n| Java Client     | 8, 11, 17 or 21 |\n\nDocker image Java runtime: 21\n\n### 2.10 <= pulsar ver <= 3.0 \n\n| Component      | Java Version  |\n|----------------|:-------------:|\n| Broker         |      17       |\n| Functions / IO |      17       |\n| CLI            |      17       |\n| Java Client    | 8 or 11 or 17 |\n\nDocker image Java runtime: 17\n\n### 2.8 <= pulsar ver <= 2.10\n\n| Component       | Java Version |\n|-----------------|:------------:|\n| Broker          |      11      |\n| Functions / IO  |      11      |\n| CLI             |   8 or 11    |\n| Java Client     |   8 or 11    |\n\n### pulsar ver < 2.8\n\n| Component   | Java Version |\n|-------------|:------------:|\n| All         |   8 or 11    |\n\n## Build Pulsar\n\n### Requirements\n\n- JDK\n\n    | Pulsar Version   |                                   JDK Version                                    |\n    |------------------|:--------------------------------------------------------------------------------:|\n    | master and 4.0+  | [JDK 21](https://adoptium.net/en-GB/temurin/releases?version=21&os=any&arch=any) | \n    | 2.11 +           | [JDK 17](https://adoptium.net/en-GB/temurin/releases?version=17&os=any&arch=any) |\n    | 2.8 / 2.9 / 2.10 | [JDK 11](https://adoptium.net/en-GB/temurin/releases?version=11&os=any&arch=any) |\n    | 2.7 -            |  [JDK 8](https://adoptium.net/en-GB/temurin/releases?version=8&os=any&arch=any)  |\n\n- Maven 3.9.9+\n- zip\n\nThere is also a guide for [setting up the tooling for building Pulsar](https://pulsar.apache.org/contribute/setup-buildtools/).\n\n> **Note**:\n>\n> This project includes a [Maven Wrapper](https://maven.apache.org/wrapper/) that can be used instead of a system-installed Maven.\n> Use it by replacing `mvn` by `./mvnw` on Linux and `mvnw.cmd` on Windows in the commands below.    \n\n### Build\n\nCompile and install:\n\n```bash\n$ mvn install -DskipTests\n```\n\nCompile and install individual module\n\n```bash\n$ mvn -pl module-name (e.g: pulsar-broker) install -DskipTests\n```\n\n### Minimal build (This skips most of external connectors and tiered storage handlers)\n\n```bash\nmvn install -Pcore-modules,-main -DskipTests\n```\n\nRun Unit Tests:\n\n```bash\n$ mvn test\n```\n\nRun Individual Unit Test:\n\n```bash\n$ mvn -pl module-name (e.g: pulsar-client) test -Dtest=unit-test-name (e.g: ConsumerBuilderImplTest)\n```\n\nRun Selected Test packages:\n\n```bash\n$ mvn test -pl module-name (for example, pulsar-broker) -Dinclude=org/apache/pulsar/**/*.java\n```\n\nStart standalone Pulsar service:\n\n```bash\n$ bin/pulsar standalone\n```\n\nCheck https://pulsar.apache.org for documentation and examples.\n\n## Build custom docker images\n\nThe commands used in the Apache Pulsar release process can be found in the [release process documentation](https://pulsar.apache.org/contribute/release-process/#stage-docker-images).\n\nHere are some general instructions for building custom docker images:\n\n* Docker images must be built with Java 8 for `branch-2.7` or previous branches because of [ISSUE-8445](https://github.com/apache/pulsar/issues/8445).\n* Java 11 is the recommended JDK version in `branch-2.8`, `branch-2.9` and `branch-2.10`.\n* Java 17 is the recommended JDK version in `branch-2.11`, `branch-3.0` and `branch-3.3`.\n* Java 21 is the recommended JDK version since `branch-4.0`.\n\nThe following command builds the docker images `apachepulsar/pulsar-all:latest` and `apachepulsar/pulsar:latest`:\n\n```bash\nmvn clean install -DskipTests\n# setting DOCKER_CLI_EXPERIMENTAL=enabled is required in some environments with older docker versions\nexport DOCKER_CLI_EXPERIMENTAL=enabled\nmvn package -Pdocker,-main -am -pl docker/pulsar-all -DskipTests\n```\n\nAfter the images are built, they can be tagged and pushed to your custom repository. Here's an example of a bash script that tags the docker images with the current version and git revision and pushes them to `localhost:32000/apachepulsar`.\n\n```bash\nimage_repo_and_project=localhost:32000/apachepulsar\npulsar_version=$(mvn initialize help:evaluate -Dexpression=project.version -pl . -q -DforceStdout)\ngitrev=$(git rev-parse HEAD | colrm 10)\ntag=\"${pulsar_version}-${gitrev}\"\necho \"Using tag $tag\"\ndocker tag apachepulsar/pulsar-all:latest ${image_repo_and_project}/pulsar-all:$tag\ndocker push ${image_repo_and_project}/pulsar-all:$tag\ndocker tag apachepulsar/pulsar:latest ${image_repo_and_project}/pulsar:$tag\ndocker push ${image_repo_and_project}/pulsar:$tag\n```\n\n## Setting up your IDE\n\nRead https://pulsar.apache.org/contribute/setup-ide for setting up IntelliJ IDEA or Eclipse for developing Pulsar.\n\n## Documentation\n\n> **Note**:\n>\n> For how to make contributions to Pulsar documentation, see [Pulsar Documentation Contribution Guide](https://pulsar.apache.org/contribute/document-intro/).\n\n## Contact\n\n##### Mailing lists\n\n* The mailing lists are the primary contact for the Apache Pulsar project.\n* Your email to the mailing list might be placed in a moderation queue if you haven't joined the mailing list by subscribing before posting.\n\n> **Note**\n>\n> Please note that security-related issues or concerns should not be reported in public channels.\n> Follow the instructions in the [Security Policy](https://pulsar.apache.org/security/) to contact the [ASF Security Team](https://www.apache.org/security/) and the Apache Pulsar PMC directly.\n\n| Name                                                      | Scope                           | Subscribe                                             | Unsubscribe                                               | Archives                                                           |\n|:----------------------------------------------------------|:--------------------------------|:------------------------------------------------------|:----------------------------------------------------------|:-------------------------------------------------------------------|\n| [users@pulsar.apache.org](mailto:users@pulsar.apache.org) | User-related discussions        | [Subscribe](mailto:users-subscribe@pulsar.apache.org?subject=subscribe&body=subscribe) | [Unsubscribe](mailto:users-unsubscribe@pulsar.apache.org?subject=unsubscribe&body=unsubscribe) | [Archives](https://lists.apache.org/list.html?users@pulsar.apache.org) |\n| [dev@pulsar.apache.org](mailto:dev@pulsar.apache.org)     | Development-related discussions | [Subscribe](mailto:dev-subscribe@pulsar.apache.org?subject=subscribe&body=subscribe)   | [Unsubscribe](mailto:dev-unsubscribe@pulsar.apache.org?subject=unsubscribe&body=unsubscribe)   | [Archives](https://lists.apache.org/list.html?dev@pulsar.apache.org)   |\n\n##### Slack\n\nPulsar slack channel at https://apache-pulsar.slack.com/\n\nYou can self-register at https://communityinviter.com/apps/apache-pulsar/apache-pulsar\n\n## Security Policy\n\nIf you find a security issue with Pulsar then please [read the security policy](https://pulsar.apache.org/security/#security-policy). It is critical to avoid public disclosure.\n\n### Reporting a security vulnerability\n\nTo report a vulnerability for Pulsar, contact the [Apache Security Team](https://www.apache.org/security/). When reporting a vulnerability to [security@apache.org](mailto:security@apache.org), you can copy your email to [private@pulsar.apache.org](mailto:private@pulsar.apache.org) to send your report to the Apache Pulsar Project Management Committee. This is a private mailing list.\n\nhttps://github.com/apache/pulsar/security/policy contains more details.\n\n## License\n\nLicensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0\n\n## Crypto Notice\n\nThis distribution includes cryptographic software. The country in which you currently reside may have restrictions on the import, possession, use, and/or re-export to another country, of encryption software. BEFORE using any encryption software, please check your country's laws, regulations and policies concerning the import, possession, or use, and re-export of encryption software, to see if this is permitted. See [The Wassenaar Arrangement](http://www.wassenaar.org/) for more information.\n\nThe U.S. Government Department of Commerce, Bureau of Industry and Security (BIS), has classified this software as Export Commodity Control Number (ECCN) 5D002.C.1, which includes information security software using or performing cryptographic functions with asymmetric algorithms. The form and manner of this Apache Software Foundation distribution makes it eligible for export under the License Exception ENC Technology Software Unrestricted (TSU) exception (see the BIS Export Administration Regulations, Section 740.13) for both object code and source code.\n\nThe following provides more details on the included cryptographic software: Pulsar uses the SSL library from Bouncy Castle written by http://www.bouncycastle.org.\n",
      "stars_today": 4
    },
    {
      "id": 158256479,
      "name": "iceberg",
      "full_name": "apache/iceberg",
      "description": "Apache Iceberg",
      "html_url": "https://github.com/apache/iceberg",
      "stars": 8429,
      "forks": 2963,
      "language": "Java",
      "topics": [
        "apache",
        "hacktoberfest",
        "iceberg"
      ],
      "created_at": "2018-11-19T16:26:46Z",
      "updated_at": "2026-01-14T21:15:01Z",
      "pushed_at": "2026-01-14T21:14:44Z",
      "open_issues": 542,
      "owner": {
        "login": "apache",
        "avatar_url": "https://avatars.githubusercontent.com/u/47359?v=4"
      },
      "readme": "<!--\n  - Licensed to the Apache Software Foundation (ASF) under one\n  - or more contributor license agreements.  See the NOTICE file\n  - distributed with this work for additional information\n  - regarding copyright ownership.  The ASF licenses this file\n  - to you under the Apache License, Version 2.0 (the\n  - \"License\"); you may not use this file except in compliance\n  - with the License.  You may obtain a copy of the License at\n  -\n  -   http://www.apache.org/licenses/LICENSE-2.0\n  -\n  - Unless required by applicable law or agreed to in writing,\n  - software distributed under the License is distributed on an\n  - \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  - KIND, either express or implied.  See the License for the\n  - specific language governing permissions and limitations\n  - under the License.\n  -->\n\n![Iceberg](https://iceberg.apache.org/assets/images/Iceberg-logo.svg)\n\n[![](https://github.com/apache/iceberg/actions/workflows/java-ci.yml/badge.svg)](https://github.com/apache/iceberg/actions/workflows/java-ci.yml)\n[![Slack](https://img.shields.io/badge/chat-on%20Slack-brightgreen.svg)](https://apache-iceberg.slack.com/)\n\nIceberg is a high-performance format for huge analytic tables. Iceberg brings the reliability and simplicity of SQL tables to big data, while making it possible for engines like Spark, Trino, Flink, Presto, Hive and Impala to safely work with the same tables, at the same time.\n\nBackground and documentation is available at <https://iceberg.apache.org>\n\n\n## Status\n\nIceberg is under active development at the Apache Software Foundation.\n\nThe [Iceberg format specification][iceberg-spec] is stable and new features are added with each version.\n\nThe core Java library is located in this repository and is the reference implementation for other libraries.\n\n[Documentation][iceberg-docs] is available for all libraries and integrations.\n\n[iceberg-docs]: https://iceberg.apache.org/docs/latest/\n[iceberg-spec]: https://iceberg.apache.org/spec/\n\n## Collaboration\n\nIceberg tracks issues in GitHub and prefers to receive contributions as pull requests.\n\nCommunity discussions happen primarily on the [dev mailing list][dev-list] or on specific issues.\n\n[dev-list]: mailto:dev@iceberg.apache.org\n\n\n### Building\n\nIceberg is built using Gradle with Java 17 or 21.\n\n* To invoke a build and run tests: `./gradlew build`\n* To skip tests: `./gradlew build -x test -x integrationTest`\n* To fix code style for default versions: `./gradlew spotlessApply`\n* To fix code style for all versions of Spark/Hive/Flink:`./gradlew spotlessApply -DallModules`\n\nIceberg table support is organized in library modules:\n\n* `iceberg-common` contains utility classes used in other modules\n* `iceberg-api` contains the public Iceberg API\n* `iceberg-core` contains implementations of the Iceberg API and support for Avro data files, **this is what processing engines should depend on**\n* `iceberg-parquet` is an optional module for working with tables backed by Parquet files\n* `iceberg-arrow` is an optional module for reading Parquet into Arrow memory\n* `iceberg-orc` is an optional module for working with tables backed by ORC files\n* `iceberg-hive-metastore` is an implementation of Iceberg tables backed by the Hive metastore Thrift client\n* `iceberg-data` is an optional module for working with tables directly from JVM applications\n\nIceberg also has modules for adding Iceberg support to processing engines:\n\n* `iceberg-spark` is an implementation of Spark's Datasource V2 API for Iceberg with submodules for each spark versions (use [runtime jars](https://iceberg.apache.org/multi-engine-support/#runtime-jar) for a shaded version to avoid dependency conflicts)\n* `iceberg-flink` contains classes for integrating with Apache Flink (use [iceberg-flink-runtime](https://iceberg.apache.org/multi-engine-support/#runtime-jar) for a shaded version)\n* `iceberg-mr` contains an InputFormat and other classes for integrating with Apache Hive\n\n---\n**NOTE**\n\nThe tests require Docker to execute. On macOS (with Docker Desktop), you might need to create a symbolic name to the docker socket in order to be detected by the tests:\n\n```\nsudo ln -s $HOME/.docker/run/docker.sock /var/run/docker.sock\n```\n\nIn some cases the testcontainer may exit with an initialization error because of an illegal state exception in the GenericContainer.  One work around for this problem is to set `selinux` into permissive mode before running the tests. \n\n```\nsudo setenforce Permissive\n./gradlew ...\nsudo setenforce Enforcing\n```\n\n---\n\n### Engine Compatibility\n\nSee the [Multi-Engine Support](https://iceberg.apache.org/multi-engine-support/) page to know about Iceberg compatibility with different Spark, Flink and Hive versions.\nFor other engines such as Presto or Trino, please visit their websites for Iceberg integration details.\n\n### Implementations\n\nThis repository contains the Java implementation of Iceberg. Other implementations can be found at:\n\n* **Go**: [iceberg-go](https://github.com/apache/iceberg-go)\n* **PyIceberg** (Python): [iceberg-python](https://github.com/apache/iceberg-python)\n* **Rust**: [iceberg-rust](https://github.com/apache/iceberg-rust)\n* **C++**: [iceberg-cpp](https://github.com/apache/iceberg-cpp)\n",
      "stars_today": 4
    },
    {
      "id": 340402576,
      "name": "react-native-vision-camera",
      "full_name": "mrousavy/react-native-vision-camera",
      "description": "ğŸ“¸ A powerful, high-performance React Native Camera library.",
      "html_url": "https://github.com/mrousavy/react-native-vision-camera",
      "stars": 9106,
      "forks": 1315,
      "language": "Swift",
      "topics": [
        "ai",
        "android",
        "barcode",
        "camera",
        "instagram",
        "ios",
        "javascript",
        "jsi",
        "library",
        "native",
        "qr",
        "qrcode",
        "react",
        "react-native",
        "react-native-camera",
        "scanner",
        "snapchat",
        "typescript",
        "vision",
        "worklet"
      ],
      "created_at": "2021-02-19T14:59:44Z",
      "updated_at": "2026-01-14T15:45:26Z",
      "pushed_at": "2026-01-07T18:05:33Z",
      "open_issues": 361,
      "owner": {
        "login": "mrousavy",
        "avatar_url": "https://avatars.githubusercontent.com/u/15199031?v=4"
      },
      "readme": "<a href=\"https://margelo.com\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"./docs/static/img/banner-dark.png\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"./docs/static/img/banner-light.png\" />\n    <img alt=\"VisionCamera\" src=\"./docs/static/img/banner-light.png\" />\n  </picture>\n</a>\n\n<br />\n\n<div>\n  <img align=\"right\" width=\"35%\" src=\"docs/static/img/example.png\">\n</div>\n\n### Features\n\nVisionCamera is a powerful, high-performance Camera library for React Native. It features:\n\n* ğŸ“¸ Photo and Video capture\n* ğŸ‘ï¸ QR/Barcode scanner\n* ğŸ“± Customizable devices and multi-cameras (\"fish-eye\" zoom)\n* ğŸï¸ Customizable resolutions and aspect-ratios (4k/8k images)\n* â±ï¸ Customizable FPS (30..240 FPS)\n* ğŸ§© [Frame Processors](https://react-native-vision-camera.com/docs/guides/frame-processors) (JS worklets to run facial recognition, AI object detection, realtime video chats, ...)\n* ğŸ¨ Drawing shapes, text, filters or shaders onto the Camera\n* ğŸ” Smooth zooming (Reanimated)\n* â¯ï¸ Fast pause and resume\n* ğŸŒ“ HDR & Night modes\n* âš¡ Custom C++/GPU accelerated video pipeline (OpenGL)\n\nInstall VisionCamera from npm:\n\n```sh\nnpm i react-native-vision-camera\ncd ios && pod install\n```\n\n..and get started by [setting up permissions](https://react-native-vision-camera.com/docs/guides)!\n\n### Documentation\n\n* [Guides](https://react-native-vision-camera.com/docs/guides)\n* [API](https://react-native-vision-camera.com/docs/api)\n* [Example](./example/)\n* [Frame Processor Plugins](https://react-native-vision-camera.com/docs/guides/frame-processor-plugins-community)\n\n### ShadowLens\n\nTo see VisionCamera in action, check out [ShadowLens](https://mrousavy.com/projects/shadowlens)!\n\n<div>\n  <a href=\"https://apps.apple.com/app/shadowlens/id6471849004\">\n    <img height=\"40\" src=\"docs/static/img/appstore.svg\" />\n  </a>\n  <a href=\"https://play.google.com/store/apps/details?id=com.mrousavy.shadowlens\">\n    <img height=\"40\" src=\"docs/static/img/googleplay.svg\" />\n  </a>\n</div>\n\n### Example\n\n```tsx\nfunction App() {\n  const device = useCameraDevice('back')\n\n  if (device == null) return <NoCameraErrorView />\n  return (\n    <Camera\n      style={StyleSheet.absoluteFill}\n      device={device}\n      isActive={true}\n    />\n  )\n}\n```\n\n> See the [example](./example/) app\n\n### Adopting at scale\n\n<a href=\"https://github.com/sponsors/mrousavy\">\n  <img align=\"right\" width=\"160\" alt=\"This library helped you? Consider sponsoring!\" src=\".github/funding-octocat.svg\">\n</a>\n\nVisionCamera is provided _as is_, I work on it in my free time.\n\nIf you're integrating VisionCamera in a production app, consider [funding this project](https://github.com/sponsors/mrousavy) and <a href=\"mailto:me@mrousavy.com?subject=Adopting VisionCamera at scale\">contact me</a> to receive premium enterprise support, help with issues, prioritize bugfixes, request features, help at integrating VisionCamera and/or Frame Processors, and more.\n\n### Socials\n\n* ğŸ¦ [**Follow me on Twitter**](https://twitter.com/mrousavy) for updates\n* ğŸ“ [**Check out my blog**](https://mrousavy.com/blog) for examples and experiments\n* ğŸ’¬ [**Join the Margelo Community Discord**](https://margelo.com/discord) for chatting about VisionCamera\n* ğŸ’– [**Sponsor me on GitHub**](https://github.com/sponsors/mrousavy) to support my work\n* ğŸª [**Buy me a Ko-Fi**](https://ko-fi.com/mrousavy) to support my work\n",
      "stars_today": 4
    },
    {
      "id": 26537135,
      "name": "u-boot",
      "full_name": "u-boot/u-boot",
      "description": "\"Das U-Boot\" Source Tree",
      "html_url": "https://github.com/u-boot/u-boot",
      "stars": 4875,
      "forks": 4243,
      "language": "C",
      "topics": [],
      "created_at": "2014-11-12T13:29:02Z",
      "updated_at": "2026-01-14T15:44:42Z",
      "pushed_at": "2026-01-13T21:17:03Z",
      "open_issues": 228,
      "owner": {
        "login": "u-boot",
        "avatar_url": "https://avatars.githubusercontent.com/u/9681997?v=4"
      },
      "readme": " # SPDX-License-Identifier: GPL-2.0+\n#\n# (C) Copyright 2000 - 2013\n# Wolfgang Denk, DENX Software Engineering, wd@denx.de.\n\nSummary:\n========\n\nThis directory contains the source code for U-Boot, a boot loader for\nEmbedded boards based on PowerPC, ARM, MIPS and several other\nprocessors, which can be installed in a boot ROM and used to\ninitialize and test the hardware or to download and run application\ncode.\n\nThe development of U-Boot is closely related to Linux: some parts of\nthe source code originate in the Linux source tree, we have some\nheader files in common, and special provision has been made to\nsupport booting of Linux images.\n\nSome attention has been paid to make this software easily\nconfigurable and extendable. For instance, all monitor commands are\nimplemented with the same call interface, so that it's very easy to\nadd new commands. Also, instead of permanently adding rarely used\ncode (for instance hardware test utilities) to the monitor, you can\nload and run it dynamically.\n\n\nStatus:\n=======\n\nIn general, all boards for which a default configuration file exists in the\nconfigs/ directory have been tested to some extent and can be considered\n\"working\". In fact, many of them are used in production systems.\n\nIn case of problems you can use\n\n     scripts/get_maintainer.pl <path>\n\nto identify the people or companies responsible for various boards and\nsubsystems. Or have a look at the git log.\n\n\nWhere to get help:\n==================\n\nIn case you have questions about, problems with or contributions for\nU-Boot, you should send a message to the U-Boot mailing list at\n<u-boot@lists.denx.de>. There is also an archive of previous traffic\non the mailing list - please search the archive before asking FAQ's.\nPlease see https://lists.denx.de/pipermail/u-boot and\nhttps://marc.info/?l=u-boot\n\nWhere to get source code:\n=========================\n\nThe U-Boot source code is maintained in the Git repository at\nhttps://source.denx.de/u-boot/u-boot.git ; you can browse it online at\nhttps://source.denx.de/u-boot/u-boot\n\nThe \"Tags\" links on this page allow you to download tarballs of\nany version you might be interested in. Official releases are also\navailable from the DENX file server through HTTPS or FTP.\nhttps://ftp.denx.de/pub/u-boot/\nftp://ftp.denx.de/pub/u-boot/\n\n\nWhere we come from:\n===================\n\n- start from 8xxrom sources\n- create PPCBoot project (https://sourceforge.net/projects/ppcboot)\n- clean up code\n- make it easier to add custom boards\n- make it possible to add other [PowerPC] CPUs\n- extend functions, especially:\n  * Provide extended interface to Linux boot loader\n  * S-Record download\n  * network boot\n  * ATA disk / SCSI ... boot\n- create ARMBoot project (https://sourceforge.net/projects/armboot)\n- add other CPU families (starting with ARM)\n- create U-Boot project (https://sourceforge.net/projects/u-boot)\n- current project page: see https://www.denx.de/wiki/U-Boot\n\n\nNames and Spelling:\n===================\n\nThe \"official\" name of this project is \"Das U-Boot\". The spelling\n\"U-Boot\" shall be used in all written text (documentation, comments\nin source files etc.). Example:\n\n\tThis is the README file for the U-Boot project.\n\nFile names etc. shall be based on the string \"u-boot\". Examples:\n\n\tinclude/asm-ppc/u-boot.h\n\n\t#include <asm/u-boot.h>\n\nVariable names, preprocessor constants etc. shall be either based on\nthe string \"u_boot\" or on \"U_BOOT\". Example:\n\n\tU_BOOT_VERSION\t\tu_boot_logo\n\tIH_OS_U_BOOT\t\tu_boot_hush_start\n\n\nSoftware Configuration:\n=======================\n\nSelection of Processor Architecture and Board Type:\n---------------------------------------------------\n\nFor all supported boards there are ready-to-use default\nconfigurations available; just type \"make <board_name>_defconfig\".\n\nExample: For a TQM823L module type:\n\n\tcd u-boot\n\tmake TQM823L_defconfig\n\nNote: If you're looking for the default configuration file for a board\nyou're sure used to be there but is now missing, check the file\ndoc/README.scrapyard for a list of no longer supported boards.\n\nSandbox Environment:\n--------------------\n\nU-Boot can be built natively to run on a Linux host using the 'sandbox'\nboard. This allows feature development which is not board- or architecture-\nspecific to be undertaken on a native platform. The sandbox is also used to\nrun some of U-Boot's tests.\n\nSee doc/arch/sandbox/sandbox.rst for more details.\n\nThe following options need to be configured:\n\n- CPU Type:\tDefine exactly one, e.g. CONFIG_MPC85XX.\n\n- Board Type:\tDefine exactly one, e.g. CONFIG_MPC8540ADS.\n\n- 85xx CPU Options:\n\t\tCONFIG_SYS_PPC64\n\n\t\tSpecifies that the core is a 64-bit PowerPC implementation (implements\n\t\tthe \"64\" category of the Power ISA). This is necessary for ePAPR\n\t\tcompliance, among other possible reasons.\n\n\t\tCONFIG_SYS_FSL_ERRATUM_A004510\n\n\t\tEnables a workaround for erratum A004510.  If set,\n\t\tthen CONFIG_SYS_FSL_ERRATUM_A004510_SVR_REV and\n\t\tCFG_SYS_FSL_CORENET_SNOOPVEC_COREONLY must be set.\n\n\t\tCONFIG_SYS_FSL_ERRATUM_A004510_SVR_REV\n\t\tCONFIG_SYS_FSL_ERRATUM_A004510_SVR_REV2 (optional)\n\n\t\tDefines one or two SoC revisions (low 8 bits of SVR)\n\t\tfor which the A004510 workaround should be applied.\n\n\t\tThe rest of SVR is either not relevant to the decision\n\t\tof whether the erratum is present (e.g. p2040 versus\n\t\tp2041) or is implied by the build target, which controls\n\t\twhether CONFIG_SYS_FSL_ERRATUM_A004510 is set.\n\n\t\tSee Freescale App Note 4493 for more information about\n\t\tthis erratum.\n\n\t\tCFG_SYS_FSL_CORENET_SNOOPVEC_COREONLY\n\n\t\tThis is the value to write into CCSR offset 0x18600\n\t\taccording to the A004510 workaround.\n\n\t\tCONFIG_SYS_FSL_SINGLE_SOURCE_CLK\n\t\tSingle Source Clock is clocking mode present in some of FSL SoC's.\n\t\tIn this mode, a single differential clock is used to supply\n\t\tclocks to the sysclock, ddrclock and usbclock.\n\n- Generic CPU options:\n\n\t\tCONFIG_SYS_FSL_DDR\n\t\tFreescale DDR driver in use. This type of DDR controller is\n\t\tfound in mpc83xx, mpc85xx as well as some ARM core SoCs.\n\n\t\tCFG_SYS_FSL_DDR_ADDR\n\t\tFreescale DDR memory-mapped register base.\n\n\t\tCONFIG_SYS_FSL_IFC_CLK_DIV\n\t\tDefines divider of platform clock(clock input to IFC controller).\n\n\t\tCONFIG_SYS_FSL_LBC_CLK_DIV\n\t\tDefines divider of platform clock(clock input to eLBC controller).\n\n\t\tCFG_SYS_FSL_DDR_SDRAM_BASE_PHY\n\t\tPhysical address from the view of DDR controllers. It is the\n\t\tsame as CFG_SYS_DDR_SDRAM_BASE for  all Power SoCs. But\n\t\tit could be different for ARM SoCs.\n\n- ARM options:\n\t\tCFG_SYS_EXCEPTION_VECTORS_HIGH\n\n\t\tSelect high exception vectors of the ARM core, e.g., do not\n\t\tclear the V bit of the c1 register of CP15.\n\n\t\tCOUNTER_FREQUENCY\n\t\tGeneric timer clock source frequency.\n\n\t\tCOUNTER_FREQUENCY_REAL\n\t\tGeneric timer clock source frequency if the real clock is\n\t\tdifferent from COUNTER_FREQUENCY, and can only be determined\n\t\tat run time.\n\n- Linux Kernel Interface:\n\t\tCONFIG_OF_LIBFDT\n\n\t\tNew kernel versions are expecting firmware settings to be\n\t\tpassed using flattened device trees (based on open firmware\n\t\tconcepts).\n\n\t\tCONFIG_OF_LIBFDT\n\t\t * New libfdt-based support\n\t\t * Adds the \"fdt\" command\n\t\t * The bootm command automatically updates the fdt\n\n\t\tOF_TBCLK - The timebase frequency.\n\n\t\tboards with QUICC Engines require OF_QE to set UCC MAC\n\t\taddresses\n\n\t\tCONFIG_OF_IDE_FIXUP\n\n\t\tU-Boot can detect if an IDE device is present or not.\n\t\tIf not, and this new config option is activated, U-Boot\n\t\tremoves the ATA node from the DTS before booting Linux,\n\t\tso the Linux IDE driver does not probe the device and\n\t\tcrash. This is needed for buggy hardware (uc101) where\n\t\tno pull down resistor is connected to the signal IDE5V_DD7.\n\n- vxWorks boot parameters:\n\n\t\tbootvx constructs a valid bootline using the following\n\t\tenvironments variables: bootdev, bootfile, ipaddr, netmask,\n\t\tserverip, gatewayip, hostname, othbootargs.\n\t\tIt loads the vxWorks image pointed bootfile.\n\n\t\tNote: If a \"bootargs\" environment is defined, it will override\n\t\tthe defaults discussed just above.\n\n- Cache Configuration for ARM:\n\t\tCFG_SYS_PL310_BASE - Physical base address of PL310\n\t\t\t\t\tcontroller register space\n\n- Serial Ports:\n\t\tCFG_PL011_CLOCK\n\n\t\tIf you have Amba PrimeCell PL011 UARTs, set this variable to\n\t\tthe clock speed of the UARTs.\n\n\t\tCFG_PL01x_PORTS\n\n\t\tIf you have Amba PrimeCell PL010 or PL011 UARTs on your board,\n\t\tdefine this to a list of base addresses for each (supported)\n\t\tport. See e.g. include/configs/versatile.h\n\n\t\tCONFIG_SERIAL_HW_FLOW_CONTROL\n\n\t\tDefine this variable to enable hw flow control in serial driver.\n\t\tCurrent user of this option is drivers/serial/nsl16550.c driver\n\n- Removal of commands\n\t\tIf no commands are needed to boot, you can disable\n\t\tCONFIG_CMDLINE to remove them. In this case, the command line\n\t\twill not be available, and when U-Boot wants to execute the\n\t\tboot command (on start-up) it will call board_run_command()\n\t\tinstead. This can reduce image size significantly for very\n\t\tsimple boot procedures.\n\n- Regular expression support:\n\t\tCONFIG_REGEX\n\t\tIf this variable is defined, U-Boot is linked against\n\t\tthe SLRE (Super Light Regular Expression) library,\n\t\twhich adds regex support to some commands, as for\n\t\texample \"env grep\" and \"setexpr\".\n\n- Watchdog:\n\t\tCFG_SYS_WATCHDOG_FREQ\n\t\tSome platforms automatically call WATCHDOG_RESET()\n\t\tfrom the timer interrupt handler every\n\t\tCFG_SYS_WATCHDOG_FREQ interrupts. If not set by the\n\t\tboard configuration file, a default of CONFIG_SYS_HZ/2\n\t\t(i.e. 500) is used. Setting CFG_SYS_WATCHDOG_FREQ\n\t\tto 0 disables calling WATCHDOG_RESET() from the timer\n\t\tinterrupt.\n\n- GPIO Support:\n\t\tThe CFG_SYS_I2C_PCA953X_WIDTH option specifies a list of\n\t\tchip-ngpio pairs that tell the PCA953X driver the number of\n\t\tpins supported by a particular chip.\n\n\t\tNote that if the GPIO device uses I2C, then the I2C interface\n\t\tmust also be configured. See I2C Support, below.\n\n- Timestamp Support:\n\n\t\tWhen CONFIG_TIMESTAMP is selected, the timestamp\n\t\t(date and time) of an image is printed by image\n\t\tcommands like bootm or iminfo. This option is\n\t\tautomatically enabled when you select CONFIG_CMD_DATE .\n\n- Partition Labels (disklabels) Supported:\n\t\tZero or more of the following:\n\t\tCONFIG_MAC_PARTITION   Apple's MacOS partition table.\n\t\tCONFIG_ISO_PARTITION   ISO partition table, used on CDROM etc.\n\t\tCONFIG_EFI_PARTITION   GPT partition table, common when EFI is the\n\t\t\t\t       bootloader.  Note 2TB partition limit; see\n\t\t\t\t       disk/part_efi.c\n\t\tCONFIG_SCSI) you must configure support for at\n\t\tleast one non-MTD partition type as well.\n\n- NETWORK Support (PCI):\n\t\tCONFIG_E1000_SPI\n\t\tUtility code for direct access to the SPI bus on Intel 8257x.\n\t\tThis does not do anything useful unless you set at least one\n\t\tof CONFIG_CMD_E1000 or CONFIG_E1000_SPI_GENERIC.\n\n\t\tCONFIG_NATSEMI\n\t\tSupport for National dp83815 chips.\n\n\t\tCONFIG_NS8382X\n\t\tSupport for National dp8382[01] gigabit chips.\n\n- NETWORK Support (other):\n\t\tCONFIG_CALXEDA_XGMAC\n\t\tSupport for the Calxeda XGMAC device\n\n\t\tCONFIG_LAN91C96\n\t\tSupport for SMSC's LAN91C96 chips.\n\n\t\t\tCONFIG_LAN91C96_USE_32_BIT\n\t\t\tDefine this to enable 32 bit addressing\n\n\t\t\tCFG_SYS_DAVINCI_EMAC_PHY_COUNT\n\t\t\tDefine this if you have more then 3 PHYs.\n\n\t\tCONFIG_FTGMAC100\n\t\tSupport for Faraday's FTGMAC100 Gigabit SoC Ethernet\n\n\t\t\tCONFIG_FTGMAC100_EGIGA\n\t\t\tDefine this to use GE link update with gigabit PHY.\n\t\t\tDefine this if FTGMAC100 is connected to gigabit PHY.\n\t\t\tIf your system has 10/100 PHY only, it might not occur\n\t\t\twrong behavior. Because PHY usually return timeout or\n\t\t\tuseless data when polling gigabit status and gigabit\n\t\t\tcontrol registers. This behavior won't affect the\n\t\t\tcorrectnessof 10/100 link speed update.\n\n\t\tCONFIG_SH_ETHER\n\t\tSupport for Renesas on-chip Ethernet controller\n\n- TPM Support:\n\t\tCONFIG_TPM\n\t\tSupport TPM devices.\n\n\t\tCONFIG_TPM_TIS_INFINEON\n\t\tSupport for Infineon i2c bus TPM devices. Only one device\n\t\tper system is supported at this time.\n\n\t\t\tCONFIG_TPM_TIS_I2C_BURST_LIMITATION\n\t\t\tDefine the burst count bytes upper limit\n\n\t\tCONFIG_TPM_ATMEL_TWI\n\t\tSupport for Atmel TWI TPM device. Requires I2C support.\n\n\t\tCONFIG_TPM_TIS_LPC\n\t\tSupport for generic parallel port TPM devices. Only one device\n\t\tper system is supported at this time.\n\n\t\tCONFIG_TPM\n\t\tDefine this to enable the TPM support library which provides\n\t\tfunctional interfaces to some TPM commands.\n\t\tRequires support for a TPM device.\n\n\t\tCONFIG_TPM_AUTH_SESSIONS\n\t\tDefine this to enable authorized functions in the TPM library.\n\t\tRequires CONFIG_TPM and CONFIG_SHA1.\n\n- USB Support:\n\t\tAt the moment only the UHCI host controller is\n\t\tsupported (PIP405, MIP405); define\n\t\tCONFIG_USB_UHCI to enable it.\n\t\tdefine CONFIG_USB_KEYBOARD to enable the USB Keyboard\n\t\tand define CONFIG_USB_STORAGE to enable the USB\n\t\tstorage devices.\n\t\tNote:\n\t\tSupported are USB Keyboards and USB Floppy drives\n\t\t(TEAC FD-05PUB).\n\n\t\tCONFIG_USB_DWC2_REG_ADDR the physical CPU address of the DWC2\n\t\tHW module registers.\n\n- USB Device:\n\t\tDefine the below if you wish to use the USB console.\n\t\tOnce firmware is rebuilt from a serial console issue the\n\t\tcommand \"setenv stdin usbtty; setenv stdout usbtty\" and\n\t\tattach your USB cable. The Unix command \"dmesg\" should print\n\t\tit has found a new device. The environment variable usbtty\n\t\tcan be set to gserial or cdc_acm to enable your device to\n\t\tappear to a USB host as a Linux gserial device or a\n\t\tCommon Device Class Abstract Control Model serial device.\n\t\tIf you select usbtty = gserial you should be able to enumerate\n\t\ta Linux host by\n\t\t# modprobe usbserial vendor=0xVendorID product=0xProductID\n\t\telse if using cdc_acm, simply setting the environment\n\t\tvariable usbtty to be cdc_acm should suffice. The following\n\t\tmight be defined in YourBoardName.h\n\n\t\tIf you have a USB-IF assigned VendorID then you may wish to\n\t\tdefine your own vendor specific values either in BoardName.h\n\t\tor directly in usbd_vendor_info.h. If you don't define\n\t\tCONFIG_USBD_MANUFACTURER, CONFIG_USBD_PRODUCT_NAME,\n\t\tCONFIG_USBD_VENDORID and CONFIG_USBD_PRODUCTID, then U-Boot\n\t\tshould pretend to be a Linux device to it's target host.\n\n\t\t\tCONFIG_USBD_MANUFACTURER\n\t\t\tDefine this string as the name of your company for\n\t\t\t- CONFIG_USBD_MANUFACTURER \"my company\"\n\n\t\t\tCONFIG_USBD_PRODUCT_NAME\n\t\t\tDefine this string as the name of your product\n\t\t\t- CONFIG_USBD_PRODUCT_NAME \"acme usb device\"\n\n\t\t\tCONFIG_USBD_VENDORID\n\t\t\tDefine this as your assigned Vendor ID from the USB\n\t\t\tImplementors Forum. This *must* be a genuine Vendor ID\n\t\t\tto avoid polluting the USB namespace.\n\t\t\t- CONFIG_USBD_VENDORID 0xFFFF\n\n\t\t\tCONFIG_USBD_PRODUCTID\n\t\t\tDefine this as the unique Product ID\n\t\t\tfor your device\n\t\t\t- CONFIG_USBD_PRODUCTID 0xFFFF\n\n- MMC Support:\n\t\tCONFIG_SH_MMCIF\n\t\tSupport for Renesas on-chip MMCIF controller\n\n\t\t\tCONFIG_SH_MMCIF_ADDR\n\t\t\tDefine the base address of MMCIF registers\n\n\t\t\tCONFIG_SH_MMCIF_CLK\n\t\t\tDefine the clock frequency for MMCIF\n\n- USB Device Firmware Update (DFU) class support:\n\t\tCONFIG_DFU_OVER_USB\n\t\tThis enables the USB portion of the DFU USB class\n\n\t\tCONFIG_DFU_NAND\n\t\tThis enables support for exposing NAND devices via DFU.\n\n\t\tCONFIG_DFU_RAM\n\t\tThis enables support for exposing RAM via DFU.\n\t\tNote: DFU spec refer to non-volatile memory usage, but\n\t\tallow usages beyond the scope of spec - here RAM usage,\n\t\tone that would help mostly the developer.\n\n\t\tCONFIG_SYS_DFU_DATA_BUF_SIZE\n\t\tDfu transfer uses a buffer before writing data to the\n\t\traw storage device. Make the size (in bytes) of this buffer\n\t\tconfigurable. The size of this buffer is also configurable\n\t\tthrough the \"dfu_bufsiz\" environment variable.\n\n\t\tCONFIG_SYS_DFU_MAX_FILE_SIZE\n\t\tWhen updating files rather than the raw storage device,\n\t\twe use a static buffer to copy the file into and then write\n\t\tthe buffer once we've been given the whole file.  Define\n\t\tthis to the maximum filesize (in bytes) for the buffer.\n\t\tDefault is 4 MiB if undefined.\n\n\t\tDFU_DEFAULT_POLL_TIMEOUT\n\t\tPoll timeout [ms], is the timeout a device can send to the\n\t\thost. The host must wait for this timeout before sending\n\t\ta subsequent DFU_GET_STATUS request to the device.\n\n\t\tDFU_MANIFEST_POLL_TIMEOUT\n\t\tPoll timeout [ms], which the device sends to the host when\n\t\tentering dfuMANIFEST state. Host waits this timeout, before\n\t\tsending again an USB request to the device.\n\n- Keyboard Support:\n\t\tSee Kconfig help for available keyboard drivers.\n\n- MII/PHY support:\n\t\tCONFIG_PHY_CLOCK_FREQ (ppc4xx)\n\n\t\tThe clock frequency of the MII bus\n\n\t\tCONFIG_PHY_CMD_DELAY (ppc4xx)\n\n\t\tSome PHY like Intel LXT971A need extra delay after\n\t\tcommand issued before MII status register can be read\n\n- BOOTP Recovery Mode:\n\t\tCONFIG_BOOTP_RANDOM_DELAY\n\n\t\tIf you have many targets in a network that try to\n\t\tboot using BOOTP, you may want to avoid that all\n\t\tsystems send out BOOTP requests at precisely the same\n\t\tmoment (which would happen for instance at recovery\n\t\tfrom a power failure, when all systems will try to\n\t\tboot, thus flooding the BOOTP server. Defining\n\t\tCONFIG_BOOTP_RANDOM_DELAY causes a random delay to be\n\t\tinserted before sending out BOOTP requests. The\n\t\tfollowing delays are inserted then:\n\n\t\t1st BOOTP request:\tdelay 0 ... 1 sec\n\t\t2nd BOOTP request:\tdelay 0 ... 2 sec\n\t\t3rd BOOTP request:\tdelay 0 ... 4 sec\n\t\t4th and following\n\t\tBOOTP requests:\t\tdelay 0 ... 8 sec\n\n\t\tCFG_BOOTP_ID_CACHE_SIZE\n\n\t\tBOOTP packets are uniquely identified using a 32-bit ID. The\n\t\tserver will copy the ID from client requests to responses and\n\t\tU-Boot will use this to determine if it is the destination of\n\t\tan incoming response. Some servers will check that addresses\n\t\taren't in use before handing them out (usually using an ARP\n\t\tping) and therefore take up to a few hundred milliseconds to\n\t\trespond. Network congestion may also influence the time it\n\t\ttakes for a response to make it back to the client. If that\n\t\ttime is too long, U-Boot will retransmit requests. In order\n\t\tto allow earlier responses to still be accepted after these\n\t\tretransmissions, U-Boot's BOOTP client keeps a small cache of\n\t\tIDs. The CFG_BOOTP_ID_CACHE_SIZE controls the size of this\n\t\tcache. The default is to keep IDs for up to four outstanding\n\t\trequests. Increasing this will allow U-Boot to accept offers\n\t\tfrom a BOOTP client in networks with unusually high latency.\n\n- DHCP Advanced Options:\n\n - Link-local IP address negotiation:\n\t\tNegotiate with other link-local clients on the local network\n\t\tfor an address that doesn't require explicit configuration.\n\t\tThis is especially useful if a DHCP server cannot be guaranteed\n\t\tto exist in all environments that the device must operate.\n\n\t\tSee doc/README.link-local for more information.\n\n - MAC address from environment variables\n\n\t\tFDT_SEQ_MACADDR_FROM_ENV\n\n\t\tFix-up device tree with MAC addresses fetched sequentially from\n\t\tenvironment variables. This config work on assumption that\n\t\tnon-usable ethernet node of device-tree are either not present\n\t\tor their status has been marked as \"disabled\".\n\n - CDP Options:\n\t\tCONFIG_CDP_DEVICE_ID\n\n\t\tThe device id used in CDP trigger frames.\n\n\t\tCONFIG_CDP_DEVICE_ID_PREFIX\n\n\t\tA two character string which is prefixed to the MAC address\n\t\tof the device.\n\n\t\tCONFIG_CDP_PORT_ID\n\n\t\tA printf format string which contains the ascii name of\n\t\tthe port. Normally is set to \"eth%d\" which sets\n\t\teth0 for the first Ethernet, eth1 for the second etc.\n\n\t\tCONFIG_CDP_CAPABILITIES\n\n\t\tA 32bit integer which indicates the device capabilities;\n\t\t0x00000010 for a normal host which does not forwards.\n\n\t\tCONFIG_CDP_VERSION\n\n\t\tAn ascii string containing the version of the software.\n\n\t\tCONFIG_CDP_PLATFORM\n\n\t\tAn ascii string containing the name of the platform.\n\n\t\tCONFIG_CDP_TRIGGER\n\n\t\tA 32bit integer sent on the trigger.\n\n\t\tCONFIG_CDP_POWER_CONSUMPTION\n\n\t\tA 16bit integer containing the power consumption of the\n\t\tdevice in .1 of milliwatts.\n\n\t\tCONFIG_CDP_APPLIANCE_VLAN_TYPE\n\n\t\tA byte containing the id of the VLAN.\n\n- Status LED:\tCONFIG_LED_STATUS\n\n\t\tSeveral configurations allow to display the current\n\t\tstatus using a LED. For instance, the LED will blink\n\t\tfast while running U-Boot code, stop blinking as\n\t\tsoon as a reply to a BOOTP request was received, and\n\t\tstart blinking slow once the Linux kernel is running\n\t\t(supported by a status LED driver in the Linux\n\t\tkernel). Defining CONFIG_LED_STATUS enables this\n\t\tfeature in U-Boot.\n\n\t\tAdditional options:\n\n\t\tCONFIG_LED_STATUS_GPIO\n\t\tThe status LED can be connected to a GPIO pin.\n\t\tIn such cases, the gpio_led driver can be used as a\n\t\tstatus LED backend implementation. Define CONFIG_LED_STATUS_GPIO\n\t\tto include the gpio_led driver in the U-Boot binary.\n\n\t\tCFG_GPIO_LED_INVERTED_TABLE\n\t\tSome GPIO connected LEDs may have inverted polarity in which\n\t\tcase the GPIO high value corresponds to LED off state and\n\t\tGPIO low value corresponds to LED on state.\n\t\tIn such cases CFG_GPIO_LED_INVERTED_TABLE may be defined\n\t\twith a list of GPIO LEDs that have inverted polarity.\n\n- I2C Support:\n\t\tCFG_SYS_NUM_I2C_BUSES\n\t\tHold the number of i2c buses you want to use.\n\n\t\tCFG_SYS_I2C_BUSES\n\t\thold a list of buses you want to use\n\n\t\t CFG_SYS_I2C_BUSES\t{{0, {I2C_NULL_HOP}}, \\\n\t\t\t\t\t{0, {{I2C_MUX_PCA9547, 0x70, 1}}}, \\\n\t\t\t\t\t{0, {{I2C_MUX_PCA9547, 0x70, 2}}}, \\\n\t\t\t\t\t{0, {{I2C_MUX_PCA9547, 0x70, 3}}}, \\\n\t\t\t\t\t{0, {{I2C_MUX_PCA9547, 0x70, 4}}}, \\\n\t\t\t\t\t{0, {{I2C_MUX_PCA9547, 0x70, 5}}}, \\\n\t\t\t\t\t{1, {I2C_NULL_HOP}}, \\\n\t\t\t\t\t{1, {{I2C_MUX_PCA9544, 0x72, 1}}}, \\\n\t\t\t\t\t{1, {{I2C_MUX_PCA9544, 0x72, 2}}}, \\\n\t\t\t\t\t}\n\n\t\twhich defines\n\t\t\tbus 0 on adapter 0 without a mux\n\t\t\tbus 1 on adapter 0 with a PCA9547 on address 0x70 port 1\n\t\t\tbus 2 on adapter 0 with a PCA9547 on address 0x70 port 2\n\t\t\tbus 3 on adapter 0 with a PCA9547 on address 0x70 port 3\n\t\t\tbus 4 on adapter 0 with a PCA9547 on address 0x70 port 4\n\t\t\tbus 5 on adapter 0 with a PCA9547 on address 0x70 port 5\n\t\t\tbus 6 on adapter 1 without a mux\n\t\t\tbus 7 on adapter 1 with a PCA9544 on address 0x72 port 1\n\t\t\tbus 8 on adapter 1 with a PCA9544 on address 0x72 port 2\n\n\t\tIf you do not have i2c muxes on your board, omit this define.\n\n- Legacy I2C Support:\n\t\tIf you use the software i2c interface (CONFIG_SYS_I2C_SOFT)\n\t\tthen the following macros need to be defined (examples are\n\t\tfrom include/configs/lwmon.h):\n\n\t\tI2C_INIT\n\n\t\t(Optional). Any commands necessary to enable the I2C\n\t\tcontroller or configure ports.\n\n\t\teg: #define I2C_INIT (immr->im_cpm.cp_pbdir |=\tPB_SCL)\n\n\t\tI2C_ACTIVE\n\n\t\tThe code necessary to make the I2C data line active\n\t\t(driven).  If the data line is open collector, this\n\t\tdefine can be null.\n\n\t\teg: #define I2C_ACTIVE (immr->im_cpm.cp_pbdir |=  PB_SDA)\n\n\t\tI2C_TRISTATE\n\n\t\tThe code necessary to make the I2C data line tri-stated\n\t\t(inactive).  If the data line is open collector, this\n\t\tdefine can be null.\n\n\t\teg: #define I2C_TRISTATE (immr->im_cpm.cp_pbdir &= ~PB_SDA)\n\n\t\tI2C_READ\n\n\t\tCode that returns true if the I2C data line is high,\n\t\tfalse if it is low.\n\n\t\teg: #define I2C_READ ((immr->im_cpm.cp_pbdat & PB_SDA) != 0)\n\n\t\tI2C_SDA(bit)\n\n\t\tIf <bit> is true, sets the I2C data line high. If it\n\t\tis false, it clears it (low).\n\n\t\teg: #define I2C_SDA(bit) \\\n\t\t\tif(bit) immr->im_cpm.cp_pbdat |=  PB_SDA; \\\n\t\t\telse\timmr->im_cpm.cp_pbdat &= ~PB_SDA\n\n\t\tI2C_SCL(bit)\n\n\t\tIf <bit> is true, sets the I2C clock line high. If it\n\t\tis false, it clears it (low).\n\n\t\teg: #define I2C_SCL(bit) \\\n\t\t\tif(bit) immr->im_cpm.cp_pbdat |=  PB_SCL; \\\n\t\t\telse\timmr->im_cpm.cp_pbdat &= ~PB_SCL\n\n\t\tI2C_DELAY\n\n\t\tThis delay is invoked four times per clock cycle so this\n\t\tcontrols the rate of data transfer.  The data rate thus\n\t\tis 1 / (I2C_DELAY * 4). Often defined to be something\n\t\tlike:\n\n\t\t#define I2C_DELAY  udelay(2)\n\n\t\tCONFIG_SOFT_I2C_GPIO_SCL / CONFIG_SOFT_I2C_GPIO_SDA\n\n\t\tIf your arch supports the generic GPIO framework (asm/gpio.h),\n\t\tthen you may alternatively define the two GPIOs that are to be\n\t\tused as SCL / SDA.  Any of the previous I2C_xxx macros will\n\t\thave GPIO-based defaults assigned to them as appropriate.\n\n\t\tYou should define these to the GPIO value as given directly to\n\t\tthe generic GPIO functions.\n\n\t\tCFG_SYS_I2C_NOPROBES\n\n\t\tThis option specifies a list of I2C devices that will be skipped\n\t\twhen the 'i2c probe' command is issued.\n\n\t\te.g.\n\t\t\t#define CFG_SYS_I2C_NOPROBES {0x50,0x68}\n\n\t\twill skip addresses 0x50 and 0x68 on a board with one I2C bus\n\n\t\tCONFIG_SOFT_I2C_READ_REPEATED_START\n\n\t\tdefining this will force the i2c_read() function in\n\t\tthe soft_i2c driver to perform an I2C repeated start\n\t\tbetween writing the address pointer and reading the\n\t\tdata.  If this define is omitted the default behaviour\n\t\tof doing a stop-start sequence will be used.  Most I2C\n\t\tdevices can use either method, but some require one or\n\t\tthe other.\n\n- SPI Support:\tCONFIG_SPI\n\n\t\tEnables SPI driver (so far only tested with\n\t\tSPI EEPROM, also an instance works with Crystal A/D and\n\t\tD/As on the SACSng board)\n\n\t\tCFG_SYS_SPI_MXC_WAIT\n\t\tTimeout for waiting until spi transfer completed.\n\t\tdefault: (CONFIG_SYS_HZ/100)     /* 10 ms */\n\n- FPGA Support: CONFIG_FPGA\n\n\t\tEnables FPGA subsystem.\n\n\t\tCONFIG_FPGA_<vendor>\n\n\t\tEnables support for specific chip vendors.\n\t\t(ALTERA, XILINX)\n\n\t\tCONFIG_FPGA_<family>\n\n\t\tEnables support for FPGA family.\n\t\t(SPARTAN2, SPARTAN3, VIRTEX2, CYCLONE2, ACEX1K, ACEX)\n\n\t\tCONFIG_SYS_FPGA_CHECK_BUSY\n\n\t\tEnable checks on FPGA configuration interface busy\n\t\tstatus by the configuration function. This option\n\t\twill require a board or device specific function to\n\t\tbe written.\n\n\t\tCFG_FPGA_DELAY\n\n\t\tIf defined, a function that provides delays in the FPGA\n\t\tconfiguration driver.\n\n\t\tCFG_SYS_FPGA_CHECK_ERROR\n\n\t\tCheck for configuration errors during FPGA bitfile\n\t\tloading. For example, abort during Virtex II\n\t\tconfiguration if the INIT_B line goes low (which\n\t\tindicated a CRC error).\n\n\t\tCFG_SYS_FPGA_WAIT_INIT\n\n\t\tMaximum time to wait for the INIT_B line to de-assert\n\t\tafter PROB_B has been de-asserted during a Virtex II\n\t\tFPGA configuration sequence. The default time is 500\n\t\tms.\n\n\t\tCFG_SYS_FPGA_WAIT_BUSY\n\n\t\tMaximum time to wait for BUSY to de-assert during\n\t\tVirtex II FPGA configuration. The default is 5 ms.\n\n\t\tCFG_SYS_FPGA_WAIT_CONFIG\n\n\t\tTime to wait after FPGA configuration. The default is\n\t\t200 ms.\n\n- Vendor Parameter Protection:\n\n\t\tU-Boot considers the values of the environment\n\t\tvariables \"serial#\" (Board Serial Number) and\n\t\t\"ethaddr\" (Ethernet Address) to be parameters that\n\t\tare set once by the board vendor / manufacturer, and\n\t\tprotects these variables from casual modification by\n\t\tthe user. Once set, these variables are read-only,\n\t\tand write or delete attempts are rejected. You can\n\t\tchange this behaviour:\n\n\t\tIf CONFIG_ENV_OVERWRITE is #defined in your config\n\t\tfile, the write protection for vendor parameters is\n\t\tcompletely disabled. Anybody can change or delete\n\t\tthese parameters.\n\n\t\tThe same can be accomplished in a more flexible way\n\t\tfor any variable by configuring the type of access\n\t\tto allow for those variables in the \".flags\" variable\n\t\tor define CFG_ENV_FLAGS_LIST_STATIC.\n\n- Protected RAM:\n\t\tCFG_PRAM\n\n\t\tDefine this variable to enable the reservation of\n\t\t\"protected RAM\", i. e. RAM which is not overwritten\n\t\tby U-Boot. Define CFG_PRAM to hold the number of\n\t\tkB you want to reserve for pRAM. You can overwrite\n\t\tthis default value by defining an environment\n\t\tvariable \"pram\" to the number of kB you want to\n\t\treserve. Note that the board info structure will\n\t\tstill show the full amount of RAM. If pRAM is\n\t\treserved, a new environment variable \"mem\" will\n\t\tautomatically be defined to hold the amount of\n\t\tremaining RAM in a form that can be passed as boot\n\t\targument to Linux, for instance like that:\n\n\t\t\tsetenv bootargs ... mem=\\${mem}\n\t\t\tsaveenv\n\n\t\tThis way you can tell Linux not to use this memory,\n\t\teither, which results in a memory region that will\n\t\tnot be affected by reboots.\n\n\t\t*WARNING* If your board configuration uses automatic\n\t\tdetection of the RAM size, you must make sure that\n\t\tthis memory test is non-destructive. So far, the\n\t\tfollowing board configurations are known to be\n\t\t\"pRAM-clean\":\n\n\t\t\tIVMS8, IVML24, SPD8xx,\n\t\t\tHERMES, IP860, RPXlite, LWMON,\n\t\t\tFLAGADM\n\n- Error Recovery:\n\tNote:\n\n\t\tIn the current implementation, the local variables\n\t\tspace and global environment variables space are\n\t\tseparated. Local variables are those you define by\n\t\tsimply typing `name=value'. To access a local\n\t\tvariable later on, you have write `$name' or\n\t\t`${name}'; to execute the contents of a variable\n\t\tdirectly type `$name' at the command prompt.\n\n\t\tGlobal environment variables are those you use\n\t\tsetenv/printenv to work with. To run a command stored\n\t\tin such a variable, you need to use the run command,\n\t\tand you must not use the '$' sign to access them.\n\n\t\tTo store commands and special characters in a\n\t\tvariable, please use double quotation marks\n\t\tsurrounding the whole text of the variable, instead\n\t\tof the backslashes before semicolons and special\n\t\tsymbols.\n\n- Default Environment:\n\t\tCFG_EXTRA_ENV_SETTINGS\n\n\t\tDefine this to contain any number of null terminated\n\t\tstrings (variable = value pairs) that will be part of\n\t\tthe default environment compiled into the boot image.\n\n\t\tFor example, place something like this in your\n\t\tboard's config file:\n\n\t\t#define CFG_EXTRA_ENV_SETTINGS \\\n\t\t\t\"myvar1=value1\\0\" \\\n\t\t\t\"myvar2=value2\\0\"\n\n\t\tWarning: This method is based on knowledge about the\n\t\tinternal format how the environment is stored by the\n\t\tU-Boot code. This is NOT an official, exported\n\t\tinterface! Although it is unlikely that this format\n\t\twill change soon, there is no guarantee either.\n\t\tYou better know what you are doing here.\n\n\t\tNote: overly (ab)use of the default environment is\n\t\tdiscouraged. Make sure to check other ways to preset\n\t\tthe environment like the \"source\" command or the\n\t\tboot command first.\n\n- Automatic software updates via TFTP server\n\t\tCONFIG_UPDATE_TFTP\n\t\tCONFIG_UPDATE_TFTP_CNT_MAX\n\t\tCONFIG_UPDATE_TFTP_MSEC_MAX\n\n\t\tThese options enable and control the auto-update feature;\n\t\tfor a more detailed description refer to doc/README.update.\n\n- MTD Support (mtdparts command, UBI support)\n\t\tCONFIG_MTD_UBI_WL_THRESHOLD\n\t\tThis parameter defines the maximum difference between the highest\n\t\terase counter value and the lowest erase counter value of eraseblocks\n\t\tof UBI devices. When this threshold is exceeded, UBI starts performing\n\t\twear leveling by means of moving data from eraseblock with low erase\n\t\tcounter to eraseblocks with high erase counter.\n\n\t\tThe default value should be OK for SLC NAND flashes, NOR flashes and\n\t\tother flashes which have eraseblock life-cycle 100000 or more.\n\t\tHowever, in case of MLC NAND flashes which typically have eraseblock\n\t\tlife-cycle less than 10000, the threshold should be lessened (e.g.,\n\t\tto 128 or 256, although it does not have to be power of 2).\n\n\t\tdefault: 4096\n\n\t\tCONFIG_MTD_UBI_BEB_LIMIT\n\t\tThis option specifies the maximum bad physical eraseblocks UBI\n\t\texpects on the MTD device (per 1024 eraseblocks). If the\n\t\tunderlying flash does not admit of bad eraseblocks (e.g. NOR\n\t\tflash), this value is ignored.\n\n\t\tNAND datasheets often specify the minimum and maximum NVM\n\t\t(Number of Valid Blocks) for the flashes' endurance lifetime.\n\t\tThe maximum expected bad eraseblocks per 1024 eraseblocks\n\t\tthen can be calculated as \"1024 * (1 - MinNVB / MaxNVB)\",\n\t\twhich gives 20 for most NANDs (MaxNVB is basically the total\n\t\tcount of eraseblocks on the chip).\n\n\t\tTo put it differently, if this value is 20, UBI will try to\n\t\treserve about 1.9% of physical eraseblocks for bad blocks\n\t\thandling. And that will be 1.9% of eraseblocks on the entire\n\t\tNAND chip, not just the MTD partition UBI attaches. This means\n\t\tthat if you have, say, a NAND flash chip admits maximum 40 bad\n\t\teraseblocks, and it is split on two MTD partitions of the same\n\t\tsize, UBI will reserve 40 eraseblocks when attaching a\n\t\tpartition.\n\n\t\tdefault: 20\n\n\t\tCONFIG_MTD_UBI_FASTMAP\n\t\tFastmap is a mechanism which allows attaching an UBI device\n\t\tin nearly constant time. Instead of scanning the whole MTD device it\n\t\tonly has to locate a checkpoint (called fastmap) on the device.\n\t\tThe on-flash fastmap contains all information needed to attach\n\t\tthe device. Using fastmap makes only sense on large devices where\n\t\tattaching by scanning takes long. UBI will not automatically install\n\t\ta fastmap on old images, but you can set the UBI parameter\n\t\tCONFIG_MTD_UBI_FASTMAP_AUTOCONVERT to 1 if you want so. Please note\n\t\tthat fastmap-enabled images are still usable with UBI implementations\n\t\twithout\tfastmap support. On typical flash devices the whole fastmap\n\t\tfits into one PEB. UBI will reserve PEBs to hold two fastmaps.\n\n\t\tCONFIG_MTD_UBI_FASTMAP_AUTOCONVERT\n\t\tSet this parameter to enable fastmap automatically on images\n\t\twithout a fastmap.\n\t\tdefault: 0\n\n\t\tCONFIG_MTD_UBI_FM_DEBUG\n\t\tEnable UBI fastmap debug\n\t\tdefault: 0\n\n- SPL framework\n\t\tCONFIG_SPL\n\t\tEnable building of SPL globally.\n\n\t\tCONFIG_SPL_PANIC_ON_RAW_IMAGE\n\t\tWhen defined, SPL will panic() if the image it has\n\t\tloaded does not have a signature.\n\t\tDefining this is useful when code which loads images\n\t\tin SPL cannot guarantee that absolutely all read errors\n\t\twill be caught.\n\t\tAn example is the LPC32XX MLC NAND driver, which will\n\t\tconsider that a completely unreadable NAND block is bad,\n\t\tand thus should be skipped silently.\n\n\t\tCONFIG_SPL_DISPLAY_PRINT\n\t\tFor ARM, enable an optional function to print more information\n\t\tabout the running system.\n\n\t\tCONFIG_SPL_MPC83XX_WAIT_FOR_NAND\n\t\tSet this for NAND SPL on PPC mpc83xx targets, so that\n\t\tstart.S waits for the rest of the SPL to load before\n\t\tcontinuing (the hardware starts execution after just\n\t\tloading the first page rather than the full 4K).\n\n\t\tCONFIG_SPL_UBI\n\t\tSupport for a lightweight UBI (fastmap) scanner and\n\t\tloader\n\n\t\tCONFIG_SYS_NAND_5_ADDR_CYCLE, CONFIG_SYS_NAND_PAGE_SIZE,\n\t\tCONFIG_SYS_NAND_OOBSIZE, CONFIG_SYS_NAND_BLOCK_SIZE,\n\t\tCONFIG_SYS_NAND_BAD_BLOCK_POS, CFG_SYS_NAND_ECCPOS,\n\t\tCFG_SYS_NAND_ECCSIZE, CFG_SYS_NAND_ECCBYTES\n\t\tDefines the size and behavior of the NAND that SPL uses\n\t\tto read U-Boot\n\n\t\tCFG_SYS_NAND_U_BOOT_DST\n\t\tLocation in memory to load U-Boot to\n\n\t\tCFG_SYS_NAND_U_BOOT_SIZE\n\t\tSize of image to load\n\n\t\tCFG_SYS_NAND_U_BOOT_START\n\t\tEntry point in loaded image to jump to\n\n\t\tCONFIG_SPL_RAM_DEVICE\n\t\tSupport for running image already present in ram, in SPL binary\n\n\t\tCONFIG_SPL_FIT_PRINT\n\t\tPrinting information about a FIT image adds quite a bit of\n\t\tcode to SPL. So this is normally disabled in SPL. Use this\n\t\toption to re-enable it. This will affect the output of the\n\t\tbootm command when booting a FIT image.\n\n- Interrupt support (PPC):\n\n\t\tThere are common interrupt_init() and timer_interrupt()\n\t\tfor all PPC archs. interrupt_init() calls interrupt_init_cpu()\n\t\tfor CPU specific initialization. interrupt_init_cpu()\n\t\tshould set decrementer_count to appropriate value. If\n\t\tCPU resets decrementer automatically after interrupt\n\t\t(ppc4xx) it should set decrementer_count to zero.\n\t\ttimer_interrupt() calls timer_interrupt_cpu() for CPU\n\t\tspecific handling. If board has watchdog / status_led\n\t\t/ other_activity_monitor it works automatically from\n\t\tgeneral timer_interrupt().\n\n\nBoard initialization settings:\n------------------------------\n\nDuring Initialization u-boot calls a number of board specific functions\nto allow the preparation of board specific prerequisites, e.g. pin setup\nbefore drivers are initialized. To enable these callbacks the\nfollowing configuration macros have to be defined. Currently this is\narchitecture specific, so please check arch/your_architecture/lib/board.c\ntypically in board_init_f() and board_init_r().\n\n- CONFIG_BOARD_EARLY_INIT_F: Call board_early_init_f()\n- CONFIG_BOARD_EARLY_INIT_R: Call board_early_init_r()\n- CONFIG_BOARD_LATE_INIT: Call board_late_init()\n\nConfiguration Settings:\n-----------------------\n\n- CONFIG_SYS_LONGHELP: Defined when you want long help messages included;\n\t\tundefine this when you're short of memory.\n\n- CFG_SYS_HELP_CMD_WIDTH: Defined when you want to override the default\n\t\twidth of the commands listed in the 'help' command output.\n\n- CONFIG_SYS_PROMPT:\tThis is what U-Boot prints on the console to\n\t\tprompt for user input.\n\n- CFG_SYS_BAUDRATE_TABLE:\n\t\tList of legal baudrate settings for this board.\n\n- CFG_SYS_MEM_RESERVE_SECURE\n\t\tOnly implemented for ARMv8 for now.\n\t\tIf defined, the size of CFG_SYS_MEM_RESERVE_SECURE memory\n\t\tis substracted from total RAM and won't be reported to OS.\n\t\tThis memory can be used as secure memory. A variable\n\t\tgd->arch.secure_ram is used to track the location. In systems\n\t\tthe RAM base is not zero, or RAM is divided into banks,\n\t\tthis variable needs to be recalcuated to get the address.\n\n- CFG_SYS_SDRAM_BASE:\n\t\tPhysical start address of SDRAM. _Must_ be 0 here.\n\n- CFG_SYS_FLASH_BASE:\n\t\tPhysical start address of Flash memory.\n\n- CONFIG_SYS_MALLOC_LEN:\n\t\tSize of DRAM reserved for malloc() use.\n\n- CFG_SYS_BOOTMAPSZ:\n\t\tMaximum size of memory mapped by the startup code of\n\t\tthe Linux kernel; all data that must be processed by\n\t\tthe Linux kernel (bd_info, boot arguments, FDT blob if\n\t\tused) must be put below this limit, unless \"bootm_low\"\n\t\tenvironment variable is defined and non-zero. In such case\n\t\tall data for the Linux kernel must be between \"bootm_low\"\n\t\tand \"bootm_low\" + CFG_SYS_BOOTMAPSZ.\t The environment\n\t\tvariable \"bootm_mapsize\" will override the value of\n\t\tCFG_SYS_BOOTMAPSZ.  If CFG_SYS_BOOTMAPSZ is undefined,\n\t\tthen the value in \"bootm_size\" will be used instead.\n\n- CONFIG_SYS_BOOT_GET_CMDLINE:\n\t\tEnables allocating and saving kernel cmdline in space between\n\t\t\"bootm_low\" and \"bootm_low\" + BOOTMAPSZ.\n\n- CONFIG_SYS_BOOT_GET_KBD:\n\t\tEnables allocating and saving a kernel copy of the bd_info in\n\t\tspace between \"bootm_low\" and \"bootm_low\" + BOOTMAPSZ.\n\n- CONFIG_SYS_FLASH_PROTECTION\n\t\tIf defined, hardware flash sectors protection is used\n\t\tinstead of U-Boot software protection.\n\n- CONFIG_SYS_FLASH_CFI:\n\t\tDefine if the flash driver uses extra elements in the\n\t\tcommon flash structure for storing flash geometry.\n\n- CONFIG_FLASH_CFI_DRIVER\n\t\tThis option also enables the building of the cfi_flash driver\n\t\tin the drivers directory\n\n- CONFIG_FLASH_CFI_MTD\n\t\tThis option enables the building of the cfi_mtd driver\n\t\tin the drivers directory. The driver exports CFI flash\n\t\tto the MTD layer.\n\n- CONFIG_SYS_FLASH_USE_BUFFER_WRITE\n\t\tUse buffered writes to flash.\n\n- CONFIG_ENV_FLAGS_LIST_DEFAULT\n- CFG_ENV_FLAGS_LIST_STATIC\n\tEnable validation of the values given to environment variables when\n\tcalling env set.  Variables can be restricted to only decimal,\n\thexadecimal, or boolean.  If CONFIG_CMD_NET is also defined,\n\tthe variables can also be restricted to IP address or MAC address.\n\n\tThe format of the list is:\n\t\ttype_attribute = [s|d|x|b|i|m]\n\t\taccess_attribute = [a|r|o|c]\n\t\tattributes = type_attribute[access_attribute]\n\t\tentry = variable_name[:attributes]\n\t\tlist = entry[,list]\n\n\tThe type attributes are:\n\t\ts - String (default)\n\t\td - Decimal\n\t\tx - Hexadecimal\n\t\tb - Boolean ([1yYtT|0nNfF])\n\t\ti - IP address\n\t\tm - MAC address\n\n\tThe access attributes are:\n\t\ta - Any (default)\n\t\tr - Read-only\n\t\to - Write-once\n\t\tc - Change-default\n\n\t- CONFIG_ENV_FLAGS_LIST_DEFAULT\n\t\tDefine this to a list (string) to define the \".flags\"\n\t\tenvironment variable in the default or embedded environment.\n\n\t- CFG_ENV_FLAGS_LIST_STATIC\n\t\tDefine this to a list (string) to define validation that\n\t\tshould be done if an entry is not found in the \".flags\"\n\t\tenvironment variable.  To override a setting in the static\n\t\tlist, simply add an entry for the same variable name to the\n\t\t\".flags\" variable.\n\n\tIf CONFIG_REGEX is defined, the variable_name above is evaluated as a\n\tregular expression. This allows multiple variables to define the same\n\tflags without explicitly listing them for each variable.\n\nThe following definitions that deal with the placement and management\nof environment data (variable area); in general, we support the\nfollowing configurations:\n\nBE CAREFUL! The first access to the environment happens quite early\nin U-Boot initialization (when we try to get the setting of for the\nconsole baudrate). You *MUST* have mapped your NVRAM area then, or\nU-Boot will hang.\n\nPlease note that even with NVRAM we still use a copy of the\nenvironment in RAM: we could work on NVRAM directly, but we want to\nkeep settings there always unmodified except somebody uses \"saveenv\"\nto save the current settings.\n\nBE CAREFUL! For some special cases, the local device can not use\n\"saveenv\" command. For example, the local device will get the\nenvironment stored in a remote NOR flash by SRIO or PCIE link,\nbut it can not erase, write this NOR flash by SRIO or PCIE interface.\n\n- CONFIG_NAND_ENV_DST\n\n\tDefines address in RAM to which the nand_spl code should copy the\n\tenvironment. If redundant environment is used, it will be copied to\n\tCONFIG_NAND_ENV_DST + CONFIG_ENV_SIZE.\n\nPlease note that the environment is read-only until the monitor\nhas been relocated to RAM and a RAM copy of the environment has been\ncreated; also, when using EEPROM you will have to use env_get_f()\nuntil then to read environment variables.\n\nThe environment is protected by a CRC32 checksum. Before the monitor\nis relocated into RAM, as a result of a bad CRC you will be working\nwith the compiled-in default environment - *silently*!!! [This is\nnecessary, because the first environment variable we need is the\n\"baudrate\" setting for the console - if we have a bad CRC, we don't\nhave any device yet where we could complain.]\n\nNote: once the monitor has been relocated, then it will complain if\nthe default environment is used; a new CRC is computed as soon as you\nuse the \"saveenv\" command to store a valid environment.\n\n- CONFIG_DISPLAY_BOARDINFO\n\t\tDisplay information about the board that U-Boot is running on\n\t\twhen U-Boot starts up. The board function checkboard() is called\n\t\tto do this.\n\n- CONFIG_DISPLAY_BOARDINFO_LATE\n\t\tSimilar to the previous option, but display this information\n\t\tlater, once stdio is running and output goes to the LCD, if\n\t\tpresent.\n\nLow Level (hardware related) configuration options:\n---------------------------------------------------\n\n- CONFIG_SYS_CACHELINE_SIZE:\n\t\tCache Line Size of the CPU.\n\n- CONFIG_SYS_CCSRBAR_DEFAULT:\n\t\tDefault (power-on reset) physical address of CCSR on Freescale\n\t\tPowerPC SOCs.\n\n- CFG_SYS_CCSRBAR:\n\t\tVirtual address of CCSR.  On a 32-bit build, this is typically\n\t\tthe same value as CONFIG_SYS_CCSRBAR_DEFAULT.\n\n- CFG_SYS_CCSRBAR_PHYS:\n\t\tPhysical address of CCSR.  CCSR can be relocated to a new\n\t\tphysical address, if desired.  In this case, this macro should\n\t\tbe set to that address.\t Otherwise, it should be set to the\n\t\tsame value as CONFIG_SYS_CCSRBAR_DEFAULT.  For example, CCSR\n\t\tis typically relocated on 36-bit builds.  It is recommended\n\t\tthat this macro be defined via the _HIGH and _LOW macros:\n\n\t\t#define CFG_SYS_CCSRBAR_PHYS ((CFG_SYS_CCSRBAR_PHYS_HIGH\n\t\t\t* 1ull) << 32 | CFG_SYS_CCSRBAR_PHYS_LOW)\n\n- CFG_SYS_CCSRBAR_PHYS_HIGH:\n\t\tBits 33-36 of CFG_SYS_CCSRBAR_PHYS.\tThis value is typically\n\t\teither 0 (32-bit build) or 0xF (36-bit build).\tThis macro is\n\t\tused in assembly code, so it must not contain typecasts or\n\t\tinteger size suffixes (e.g. \"ULL\").\n\n- CFG_SYS_CCSRBAR_PHYS_LOW:\n\t\tLower 32-bits of CFG_SYS_CCSRBAR_PHYS.  This macro is\n\t\tused in assembly code, so it must not contain typecasts or\n\t\tinteger size suffixes (e.g. \"ULL\").\n\n- CONFIG_SYS_IMMR:\tPhysical address of the Internal Memory.\n\t\tDO NOT CHANGE unless you know exactly what you're\n\t\tdoing! (11-4) [MPC8xx systems only]\n\n- CFG_SYS_INIT_RAM_ADDR:\n\n\t\tStart address of memory area that can be used for\n\t\tinitial data and stack; please note that this must be\n\t\twritable memory that is working WITHOUT special\n\t\tinitialization, i. e. you CANNOT use normal RAM which\n\t\twill become available only after programming the\n\t\tmemory controller and running certain initialization\n\t\tsequences.\n\n\t\tU-Boot uses the following memory types:\n\t\t- MPC8xx: IMMR (internal memory of the CPU)\n\n- CONFIG_SYS_SCCR:\tSystem Clock and reset Control Register (15-27)\n\n- CONFIG_SYS_OR_TIMING_SDRAM:\n\t\tSDRAM timing\n\n- CONFIG_SYS_SRIOn_MEM_VIRT:\n\t\tVirtual Address of SRIO port 'n' memory region\n\n- CONFIG_SYS_SRIOn_MEM_PHYxS:\n\t\tPhysical Address of SRIO port 'n' memory region\n\n- CONFIG_SYS_SRIOn_MEM_SIZE:\n\t\tSize of SRIO port 'n' memory region\n\n- CONFIG_SYS_NAND_BUSWIDTH_16BIT\n\t\tDefined to tell the NAND controller that the NAND chip is using\n\t\ta 16 bit bus.\n\t\tNot all NAND drivers use this symbol.\n\t\tExample of drivers that use it:\n\t\t- drivers/mtd/nand/raw/ndfc.c\n\t\t- drivers/mtd/nand/raw/mxc_nand.c\n\n- CONFIG_SYS_NDFC_EBC0_CFG\n\t\tSets the EBC0_CFG register for the NDFC. If not defined\n\t\ta default value will be used.\n\n- CONFIG_SYS_SPD_BUS_NUM\n\t\tIf SPD EEPROM is on an I2C bus other than the first\n\t\tone, specify here. Note that the value must resolve\n\t\tto something your driver can deal with.\n\n- CONFIG_FSL_DDR_INTERACTIVE\n\t\tEnable interactive DDR debugging. See doc/README.fsl-ddr.\n\n- CONFIG_FSL_DDR_SYNC_REFRESH\n\t\tEnable sync of refresh for multiple controllers.\n\n- CONFIG_FSL_DDR_BIST\n\t\tEnable built-in memory test for Freescale DDR controllers.\n\n- CONFIG_RMII\n\t\tEnable RMII mode for all FECs.\n\t\tNote that this is a global option, we can't\n\t\thave one FEC in standard MII mode and another in RMII mode.\n\n- CONFIG_CRC32_VERIFY\n\t\tAdd a verify option to the crc32 command.\n\t\tThe syntax is:\n\n\t\t=> crc32 -v <address> <count> <crc32>\n\n\t\tWhere address/count indicate a memory area\n\t\tand crc32 is the correct crc32 which the\n\t\tarea should have.\n\n- CONFIG_LOOPW\n\t\tAdd the \"loopw\" memory command. This only takes effect if\n\t\tthe memory commands are activated globally (CONFIG_CMD_MEMORY).\n\n- CONFIG_CMD_MX_CYCLIC\n\t\tAdd the \"mdc\" and \"mwc\" memory commands. These are cyclic\n\t\t\"md/mw\" commands.\n\t\tExamples:\n\n\t\t=> mdc.b 10 4 500\n\t\tThis command will print 4 bytes (10,11,12,13) each 500 ms.\n\n\t\t=> mwc.l 100 12345678 10\n\t\tThis command will write 12345678 to address 100 all 10 ms.\n\n\t\tThis only takes effect if the memory commands are activated\n\t\tglobally (CONFIG_CMD_MEMORY).\n\n- CONFIG_XPL_BUILD\n\t\tSet when the currently running compilation is for an artifact\n\t\tthat will end up in one of the 'xPL' builds, i.e. SPL, TPL or\n\t\tVPL. Code that needs phase-specific behaviour can check this,\n\t\tor (where possible) use xpl_phase() instead.\n\n- CONFIG_TPL_BUILD\n\t\tSet when the currently running compilation is for an artifact\n\t\tthat will end up in the TPL build (as opposed to SPL, VPL or\n\t\tU-Boot proper). Code that needs phase-specific behaviour can\n\t\tcheck this, or (where possible) use xpl_phase() instead.\n\n- CONFIG_VPL_BUILD\n\t\tSet when the currently running compilation is for an artifact\n\t\tthat will end up in the VPL build (as opposed to the SPL, TPL\n\t\tor U-Boot proper). Code that needs phase-specific behaviour can\n\t\tcheck this, or (where possible) use xpl_phase() instead.\n\n- CONFIG_ARCH_MAP_SYSMEM\n\t\tGenerally U-Boot (and in particular the md command) uses\n\t\teffective address. It is therefore not necessary to regard\n\t\tU-Boot address as virtual addresses that need to be translated\n\t\tto physical addresses. However, sandbox requires this, since\n\t\tit maintains its own little RAM buffer which contains all\n\t\taddressable memory. This option causes some memory accesses\n\t\tto be mapped through map_sysmem() / unmap_sysmem().\n\n- CONFIG_X86_RESET_VECTOR\n\t\tIf defined, the x86 reset vector code is included. This is not\n\t\tneeded when U-Boot is running from Coreboot.\n\nFreescale QE/FMAN Firmware Support:\n-----------------------------------\n\nThe Freescale QUICCEngine (QE) and Frame Manager (FMAN) both support the\nloading of \"firmware\", which is encoded in the QE firmware binary format.\nThis firmware often needs to be loaded during U-Boot booting, so macros\nare used to identify the storage device (NOR flash, SPI, etc) and the address\nwithin that device.\n\n- CONFIG_SYS_FMAN_FW_ADDR\n\tThe address in the storage device where the FMAN microcode is located.  The\n\tmeaning of this address depends on which CONFIG_SYS_QE_FMAN_FW_IN_xxx macro\n\tis also specified.\n\n- CONFIG_SYS_QE_FW_ADDR\n\tThe address in the storage device where the QE microcode is located.  The\n\tmeaning of this address depends on which CONFIG_SYS_QE_FMAN_FW_IN_xxx macro\n\tis also specified.\n\n- CONFIG_SYS_QE_FMAN_FW_LENGTH\n\tThe maximum possible size of the firmware.  The firmware binary format\n\thas a field that specifies the actual size of the firmware, but it\n\tmight not be possible to read any part of the firmware unless some\n\tlocal storage is allocated to hold the entire firmware first.\n\n- CONFIG_SYS_QE_FMAN_FW_IN_NOR\n\tSpecifies that QE/FMAN firmware is located in NOR flash, mapped as\n\tnormal addressable memory via the LBC.  CONFIG_SYS_FMAN_FW_ADDR is the\n\tvirtual address in NOR flash.\n\n- CONFIG_SYS_QE_FMAN_FW_IN_NAND\n\tSpecifies that QE/FMAN firmware is located in NAND flash.\n\tCONFIG_SYS_FMAN_FW_ADDR is the offset within NAND flash.\n\n- CONFIG_SYS_QE_FMAN_FW_IN_MMC\n\tSpecifies that QE/FMAN firmware is located on the primary SD/MMC\n\tdevice.  CONFIG_SYS_FMAN_FW_ADDR is the byte offset on that device.\n\n- CONFIG_SYS_QE_FMAN_FW_IN_REMOTE\n\tSpecifies that QE/FMAN firmware is located in the remote (master)\n\tmemory space.\tCONFIG_SYS_FMAN_FW_ADDR is a virtual address which\n\tcan be mapped from slave TLB->slave LAW->slave SRIO or PCIE outbound\n\twindow->master inbound window->master LAW->the ucode address in\n\tmaster's memory space.\n\nFreescale Layerscape Management Complex Firmware Support:\n---------------------------------------------------------\nThe Freescale Layerscape Management Complex (MC) supports the loading of\n\"firmware\".\nThis firmware often needs to be loaded during U-Boot booting, so macros\nare used to identify the storage device (NOR flash, SPI, etc) and the address\nwithin that device.\n\n- CONFIG_FSL_MC_ENET\n\tEnable the MC driver for Layerscape SoCs.\n\nFreescale Layerscape Debug Server Support:\n-------------------------------------------\nThe Freescale Layerscape Debug Server Support supports the loading of\n\"Debug Server firmware\" and triggering SP boot-rom.\nThis firmware often needs to be loaded during U-Boot booting.\n\n- CONFIG_SYS_MC_RSV_MEM_ALIGN\n\tDefine alignment of reserved memory MC requires\n\n\nBuilding the Software:\n======================\n\nBuilding U-Boot has been tested in several native build environments\nand in many different cross environments. Of course we cannot support\nall possibly existing versions of cross development tools in all\n(potentially obsolete) versions. In case of tool chain problems we\nrecommend to use the ELDK (see https://www.denx.de/wiki/DULG/ELDK)\nwhich is extensively used to build and test U-Boot.\n\nIf you are not using a native environment, it is assumed that you\nhave GNU cross compiling tools available in your path. In this case,\nyou must set the environment variable CROSS_COMPILE in your shell.\nNote that no changes to the Makefile or any other source files are\nnecessary. For example using the ELDK on a 4xx CPU, please enter:\n\n\t$ CROSS_COMPILE=ppc_4xx-\n\t$ export CROSS_COMPILE\n\nU-Boot is intended to be simple to build. After installing the\nsources you must configure U-Boot for one specific board type. This\nis done by typing:\n\n\tmake NAME_defconfig\n\nwhere \"NAME_defconfig\" is the name of one of the existing configu-\nrations; see configs/*_defconfig for supported names.\n\nNote: for some boards special configuration names may exist; check if\n      additional information is available from the board vendor; for\n      instance, the TQM823L systems are available without (standard)\n      or with LCD support. You can select such additional \"features\"\n      when choosing the configuration, i. e.\n\n      make TQM823L_defconfig\n\t- will configure for a plain TQM823L, i. e. no LCD support\n\n      make TQM823L_LCD_defconfig\n\t- will configure for a TQM823L with U-Boot console on LCD\n\n      etc.\n\n\nFinally, type \"make all\", and you should get some working U-Boot\nimages ready for download to / installation on your system:\n\n- \"u-boot.bin\" is a raw binary image\n- \"u-boot\" is an image in ELF binary format\n- \"u-boot.srec\" is in Motorola S-Record format\n\nUser specific CPPFLAGS, AFLAGS and CFLAGS can be passed to the compiler by\nsetting the according environment variables KCPPFLAGS, KAFLAGS and KCFLAGS.\nFor example to treat all compiler warnings as errors:\n\n\tmake KCFLAGS=-Werror\n\nPlease be aware that the Makefiles assume you are using GNU make, so\nfor instance on NetBSD you might need to use \"gmake\" instead of\nnative \"make\".\n\n\nIf the system board that you have is not listed, then you will need\nto port U-Boot to your hardware platform. To do this, follow these\nsteps:\n\n1.  Create a new directory to hold your board specific code. Add any\n    files you need. In your board directory, you will need at least\n    the \"Makefile\" and a \"<board>.c\".\n2.  Create a new configuration file \"include/configs/<board>.h\" for\n    your board.\n3.  If you're porting U-Boot to a new CPU, then also create a new\n    directory to hold your CPU specific code. Add any files you need.\n4.  Run \"make <board>_defconfig\" with your new name.\n5.  Type \"make\", and you should get a working \"u-boot.srec\" file\n    to be installed on your target system.\n6.  Debug and solve any problems that might arise.\n    [Of course, this last step is much harder than it sounds.]\n\n\nTesting of U-Boot Modifications, Ports to New Hardware, etc.:\n==============================================================\n\nIf you have modified U-Boot sources (for instance added a new board\nor support for new devices, a new CPU, etc.) you are expected to\nprovide feedback to the other developers. The feedback normally takes\nthe form of a \"patch\", i.e. a context diff against a certain (latest\nofficial or latest in the git repository) version of U-Boot sources.\n\nBut before you submit such a patch, please verify that your modifi-\ncation did not break existing code. At least make sure that *ALL* of\nthe supported boards compile WITHOUT ANY compiler warnings. To do so,\njust run the buildman script (tools/buildman/buildman), which will\nconfigure and build U-Boot for ALL supported system. Be warned, this\nwill take a while. Please see the buildman README, or run 'buildman -H'\nfor documentation.\n\n\nSee also \"U-Boot Porting Guide\" below.\n\n\nMonitor Commands - Overview:\n============================\n\ngo\t- start application at address 'addr'\nrun\t- run commands in an environment variable\nbootm\t- boot application image from memory\nbootp\t- boot image via network using BootP/TFTP protocol\nbootz   - boot zImage from memory\ntftpboot- boot image via network using TFTP protocol\n\t       and env variables \"ipaddr\" and \"serverip\"\n\t       (and eventually \"gatewayip\")\ntftpput - upload a file via network using TFTP protocol\nrarpboot- boot image via network using RARP/TFTP protocol\ndiskboot- boot from IDE devicebootd   - boot default, i.e., run 'bootcmd'\nloads\t- load S-Record file over serial line\nloadb\t- load binary file over serial line (kermit mode)\nloadm   - load binary blob from source address to destination address\nmd\t- memory display\nmm\t- memory modify (auto-incrementing)\nnm\t- memory modify (constant address)\nmw\t- memory write (fill)\nms\t- memory search\ncp\t- memory copy\ncmp\t- memory compare\ncrc32\t- checksum calculation\ni2c\t- I2C sub-system\nsspi\t- SPI utility commands\nbase\t- print or set address offset\nprintenv- print environment variables\npwm\t- control pwm channels\nseama   - load SEAMA NAND image\nsetenv\t- set environment variables\nsaveenv - save environment variables to persistent storage\nprotect - enable or disable FLASH write protection\nerase\t- erase FLASH memory\nflinfo\t- print FLASH memory information\nnand\t- NAND memory operations (see doc/README.nand)\nbdinfo\t- print Board Info structure\niminfo\t- print header information for application image\nconinfo - print console devices and informations\nide\t- IDE sub-system\nloop\t- infinite loop on address range\nloopw\t- infinite write loop on address range\nmtest\t- simple RAM test\nicache\t- enable or disable instruction cache\ndcache\t- enable or disable data cache\nreset\t- Perform RESET of the CPU\necho\t- echo args to console\nversion - print monitor version\nhelp\t- print online help\n?\t- alias for 'help'\n\n\nMonitor Commands - Detailed Description:\n========================================\n\nTODO.\n\nFor now: just type \"help <command>\".\n\n\nNote for Redundant Ethernet Interfaces:\n=======================================\n\nSome boards come with redundant Ethernet interfaces; U-Boot supports\nsuch configurations and is capable of automatic selection of a\n\"working\" interface when needed. MAC assignment works as follows:\n\nNetwork interfaces are numbered eth0, eth1, eth2, ... Corresponding\nMAC addresses can be stored in the environment as \"ethaddr\" (=>eth0),\n\"eth1addr\" (=>eth1), \"eth2addr\", ...\n\nIf the network interface stores some valid MAC address (for instance\nin SROM), this is used as default address if there is NO correspon-\nding setting in the environment; if the corresponding environment\nvariable is set, this overrides the settings in the card; that means:\n\no If the SROM has a valid MAC address, and there is no address in the\n  environment, the SROM's address is used.\n\no If there is no valid address in the SROM, and a definition in the\n  environment exists, then the value from the environment variable is\n  used.\n\no If both the SROM and the environment contain a MAC address, and\n  both addresses are the same, this MAC address is used.\n\no If both the SROM and the environment contain a MAC address, and the\n  addresses differ, the value from the environment is used and a\n  warning is printed.\n\no If neither SROM nor the environment contain a MAC address, an error\n  is raised. If CONFIG_NET_RANDOM_ETHADDR is defined, then in this case\n  a random, locally-assigned MAC is used.\n\nIf Ethernet drivers implement the 'write_hwaddr' function, valid MAC addresses\nwill be programmed into hardware as part of the initialization process.\t This\nmay be skipped by setting the appropriate 'ethmacskip' environment variable.\nThe naming convention is as follows:\n\"ethmacskip\" (=>eth0), \"eth1macskip\" (=>eth1) etc.\n\nImage Formats:\n==============\n\nU-Boot is capable of booting (and performing other auxiliary operations on)\nimages in two formats:\n\nNew uImage format (FIT)\n-----------------------\n\nFlexible and powerful format based on Flattened Image Tree -- FIT (similar\nto Flattened Device Tree). It allows the use of images with multiple\ncomponents (several kernels, ramdisks, etc.), with contents protected by\nSHA1, MD5 or CRC32. More details are found in the doc/uImage.FIT directory.\n\n\nOld uImage format\n-----------------\n\nOld image format is based on binary files which can be basically anything,\npreceded by a special header; see the definitions in include/image.h for\ndetails; basically, the header defines the following image properties:\n\n* Target Operating System (Provisions for OpenBSD, NetBSD, FreeBSD,\n  4.4BSD, Linux, SVR4, Esix, Solaris, Irix, SCO, Dell, NCR, VxWorks,\n  LynxOS, pSOS, QNX, RTEMS, INTEGRITY;\n  Currently supported: Linux, NetBSD, VxWorks, QNX, RTEMS, INTEGRITY).\n* Target CPU Architecture (Provisions for Alpha, ARM, Intel x86,\n  IA64, MIPS, Nios II, PowerPC, IBM S390, SuperH, Sparc, Sparc 64 Bit;\n  Currently supported: ARM, Intel x86, MIPS, Nios II, PowerPC).\n* Compression Type (uncompressed, gzip, bzip2)\n* Load Address\n* Entry Point\n* Image Name\n* Image Timestamp\n\nThe header is marked by a special Magic Number, and both the header\nand the data portions of the image are secured against corruption by\nCRC32 checksums.\n\n\nLinux Support:\n==============\n\nAlthough U-Boot should support any OS or standalone application\neasily, the main focus has always been on Linux during the design of\nU-Boot.\n\nU-Boot includes many features that so far have been part of some\nspecial \"boot loader\" code within the Linux kernel. Also, any\n\"initrd\" images to be used are no longer part of one big Linux image;\ninstead, kernel and \"initrd\" are separate images. This implementation\nserves several purposes:\n\n- the same features can be used for other OS or standalone\n  applications (for instance: using compressed images to reduce the\n  Flash memory footprint)\n\n- it becomes much easier to port new Linux kernel versions because\n  lots of low-level, hardware dependent stuff are done by U-Boot\n\n- the same Linux kernel image can now be used with different \"initrd\"\n  images; of course this also means that different kernel images can\n  be run with the same \"initrd\". This makes testing easier (you don't\n  have to build a new \"zImage.initrd\" Linux image when you just\n  change a file in your \"initrd\"). Also, a field-upgrade of the\n  software is easier now.\n\n\nLinux HOWTO:\n============\n\nPorting Linux to U-Boot based systems:\n---------------------------------------\n\nU-Boot cannot save you from doing all the necessary modifications to\nconfigure the Linux device drivers for use with your target hardware\n(no, we don't intend to provide a full virtual machine interface to\nLinux :-).\n\nBut now you can ignore ALL boot loader code (in arch/powerpc/mbxboot).\n\nJust make sure your machine specific header file (for instance\ninclude/asm-ppc/tqm8xx.h) includes the same definition of the Board\nInformation structure as we define in include/asm-<arch>/u-boot.h,\nand make sure that your definition of IMAP_ADDR uses the same value\nas your U-Boot configuration in CONFIG_SYS_IMMR.\n\nNote that U-Boot now has a driver model, a unified model for drivers.\nIf you are adding a new driver, plumb it into driver model. If there\nis no uclass available, you are encouraged to create one. See\ndoc/driver-model.\n\n\nConfiguring the Linux kernel:\n-----------------------------\n\nNo specific requirements for U-Boot. Make sure you have some root\ndevice (initial ramdisk, NFS) for your target system.\n\n\nBuilding a Linux Image:\n-----------------------\n\nWith U-Boot, \"normal\" build targets like \"zImage\" or \"bzImage\" are\nnot used. If you use recent kernel source, a new build target\n\"uImage\" will exist which automatically builds an image usable by\nU-Boot. Most older kernels also have support for a \"pImage\" target,\nwhich was introduced for our predecessor project PPCBoot and uses a\n100% compatible format.\n\nExample:\n\n\tmake TQM850L_defconfig\n\tmake oldconfig\n\tmake dep\n\tmake uImage\n\nThe \"uImage\" build target uses a special tool (in 'tools/mkimage') to\nencapsulate a compressed Linux kernel image with header\t information,\nCRC32 checksum etc. for use with U-Boot. This is what we are doing:\n\n* build a standard \"vmlinux\" kernel image (in ELF binary format):\n\n* convert the kernel into a raw binary image:\n\n\t${CROSS_COMPILE}-objcopy -O binary \\\n\t\t\t\t -R .note -R .comment \\\n\t\t\t\t -S vmlinux linux.bin\n\n* compress the binary image:\n\n\tgzip -9 linux.bin\n\n* package compressed binary image for U-Boot:\n\n\tmkimage -A ppc -O linux -T kernel -C gzip \\\n\t\t-a 0 -e 0 -n \"Linux Kernel Image\" \\\n\t\t-d linux.bin.gz uImage\n\n\nThe \"mkimage\" tool can also be used to create ramdisk images for use\nwith U-Boot, either separated from the Linux kernel image, or\ncombined into one file. \"mkimage\" encapsulates the images with a 64\nbyte header containing information about target architecture,\noperating system, image type, compression method, entry points, time\nstamp, CRC32 checksums, etc.\n\n\"mkimage\" can be called in two ways: to verify existing images and\nprint the header information, or to build new images.\n\nIn the first form (with \"-l\" option) mkimage lists the information\ncontained in the header of an existing U-Boot image; this includes\nchecksum verification:\n\n\ttools/mkimage -l image\n\t  -l ==> list image header information\n\nThe second form (with \"-d\" option) is used to build a U-Boot image\nfrom a \"data file\" which is used as image payload:\n\n\ttools/mkimage -A arch -O os -T type -C comp -a addr -e ep \\\n\t\t      -n name -d data_file image\n\t  -A ==> set architecture to 'arch'\n\t  -O ==> set operating system to 'os'\n\t  -T ==> set image type to 'type'\n\t  -C ==> set compression type 'comp'\n\t  -a ==> set load address to 'addr' (hex)\n\t  -e ==> set entry point to 'ep' (hex)\n\t  -n ==> set image name to 'name'\n\t  -d ==> use image data from 'datafile'\n\nRight now, all Linux kernels for PowerPC systems use the same load\naddress (0x00000000), but the entry point address depends on the\nkernel version:\n\n- 2.2.x kernels have the entry point at 0x0000000C,\n- 2.3.x and later kernels have the entry point at 0x00000000.\n\nSo a typical call to build a U-Boot image would read:\n\n\t-> tools/mkimage -n '2.4.4 kernel for TQM850L' \\\n\t> -A ppc -O linux -T kernel -C gzip -a 0 -e 0 \\\n\t> -d /opt/elsk/ppc_8xx/usr/src/linux-2.4.4/arch/powerpc/coffboot/vmlinux.gz \\\n\t> examples/uImage.TQM850L\n\tImage Name:   2.4.4 kernel for TQM850L\n\tCreated:      Wed Jul 19 02:34:59 2000\n\tImage Type:   PowerPC Linux Kernel Image (gzip compressed)\n\tData Size:    335725 Bytes = 327.86 kB = 0.32 MB\n\tLoad Address: 0x00000000\n\tEntry Point:  0x00000000\n\nTo verify the contents of the image (or check for corruption):\n\n\t-> tools/mkimage -l examples/uImage.TQM850L\n\tImage Name:   2.4.4 kernel for TQM850L\n\tCreated:      Wed Jul 19 02:34:59 2000\n\tImage Type:   PowerPC Linux Kernel Image (gzip compressed)\n\tData Size:    335725 Bytes = 327.86 kB = 0.32 MB\n\tLoad Address: 0x00000000\n\tEntry Point:  0x00000000\n\nNOTE: for embedded systems where boot time is critical you can trade\nspeed for memory and install an UNCOMPRESSED image instead: this\nneeds more space in Flash, but boots much faster since it does not\nneed to be uncompressed:\n\n\t-> gunzip /opt/elsk/ppc_8xx/usr/src/linux-2.4.4/arch/powerpc/coffboot/vmlinux.gz\n\t-> tools/mkimage -n '2.4.4 kernel for TQM850L' \\\n\t> -A ppc -O linux -T kernel -C none -a 0 -e 0 \\\n\t> -d /opt/elsk/ppc_8xx/usr/src/linux-2.4.4/arch/powerpc/coffboot/vmlinux \\\n\t> examples/uImage.TQM850L-uncompressed\n\tImage Name:   2.4.4 kernel for TQM850L\n\tCreated:      Wed Jul 19 02:34:59 2000\n\tImage Type:   PowerPC Linux Kernel Image (uncompressed)\n\tData Size:    792160 Bytes = 773.59 kB = 0.76 MB\n\tLoad Address: 0x00000000\n\tEntry Point:  0x00000000\n\n\nSimilar you can build U-Boot images from a 'ramdisk.image.gz' file\nwhen your kernel is intended to use an initial ramdisk:\n\n\t-> tools/mkimage -n 'Simple Ramdisk Image' \\\n\t> -A ppc -O linux -T ramdisk -C gzip \\\n\t> -d /LinuxPPC/images/SIMPLE-ramdisk.image.gz examples/simple-initrd\n\tImage Name:   Simple Ramdisk Image\n\tCreated:      Wed Jan 12 14:01:50 2000\n\tImage Type:   PowerPC Linux RAMDisk Image (gzip compressed)\n\tData Size:    566530 Bytes = 553.25 kB = 0.54 MB\n\tLoad Address: 0x00000000\n\tEntry Point:  0x00000000\n\nThe \"dumpimage\" tool can be used to disassemble or list the contents of images\nbuilt by mkimage. See dumpimage's help output (-h) for details.\n\nInstalling a Linux Image:\n-------------------------\n\nTo downloading a U-Boot image over the serial (console) interface,\nyou must convert the image to S-Record format:\n\n\tobjcopy -I binary -O srec examples/image examples/image.srec\n\nThe 'objcopy' does not understand the information in the U-Boot\nimage header, so the resulting S-Record file will be relative to\naddress 0x00000000. To load it to a given address, you need to\nspecify the target address as 'offset' parameter with the 'loads'\ncommand.\n\nExample: install the image to address 0x40100000 (which on the\nTQM8xxL is in the first Flash bank):\n\n\t=> erase 40100000 401FFFFF\n\n\t.......... done\n\tErased 8 sectors\n\n\t=> loads 40100000\n\t## Ready for S-Record download ...\n\t~>examples/image.srec\n\t1 2 3 4 5 6 7 8 9 10 11 12 13 ...\n\t...\n\t15989 15990 15991 15992\n\t[file transfer complete]\n\t[connected]\n\t## Start Addr = 0x00000000\n\n\nYou can check the success of the download using the 'iminfo' command;\nthis includes a checksum verification so you can be sure no data\ncorruption happened:\n\n\t=> imi 40100000\n\n\t## Checking Image at 40100000 ...\n\t   Image Name:\t 2.2.13 for initrd on TQM850L\n\t   Image Type:\t PowerPC Linux Kernel Image (gzip compressed)\n\t   Data Size:\t 335725 Bytes = 327 kB = 0 MB\n\t   Load Address: 00000000\n\t   Entry Point:\t 0000000c\n\t   Verifying Checksum ... OK\n\n\nBoot Linux:\n-----------\n\nThe \"bootm\" command is used to boot an application that is stored in\nmemory (RAM or Flash). In case of a Linux kernel image, the contents\nof the \"bootargs\" environment variable is passed to the kernel as\nparameters. You can check and modify this variable using the\n\"printenv\" and \"setenv\" commands:\n\n\n\t=> printenv bootargs\n\tbootargs=root=/dev/ram\n\n\t=> setenv bootargs root=/dev/nfs rw nfsroot=10.0.0.2:/LinuxPPC nfsaddrs=10.0.0.99:10.0.0.2\n\n\t=> printenv bootargs\n\tbootargs=root=/dev/nfs rw nfsroot=10.0.0.2:/LinuxPPC nfsaddrs=10.0.0.99:10.0.0.2\n\n\t=> bootm 40020000\n\t## Booting Linux kernel at 40020000 ...\n\t   Image Name:\t 2.2.13 for NFS on TQM850L\n\t   Image Type:\t PowerPC Linux Kernel Image (gzip compressed)\n\t   Data Size:\t 381681 Bytes = 372 kB = 0 MB\n\t   Load Address: 00000000\n\t   Entry Point:\t 0000000c\n\t   Verifying Checksum ... OK\n\t   Uncompressing Kernel Image ... OK\n\tLinux version 2.2.13 (wd@denx.local.net) (gcc version 2.95.2 19991024 (release)) #1 Wed Jul 19 02:35:17 MEST 2000\n\tBoot arguments: root=/dev/nfs rw nfsroot=10.0.0.2:/LinuxPPC nfsaddrs=10.0.0.99:10.0.0.2\n\ttime_init: decrementer frequency = 187500000/60\n\tCalibrating delay loop... 49.77 BogoMIPS\n\tMemory: 15208k available (700k kernel code, 444k data, 32k init) [c0000000,c1000000]\n\t...\n\nIf you want to boot a Linux kernel with initial RAM disk, you pass\nthe memory addresses of both the kernel and the initrd image (PPBCOOT\nformat!) to the \"bootm\" command:\n\n\t=> imi 40100000 40200000\n\n\t## Checking Image at 40100000 ...\n\t   Image Name:\t 2.2.13 for initrd on TQM850L\n\t   Image Type:\t PowerPC Linux Kernel Image (gzip compressed)\n\t   Data Size:\t 335725 Bytes = 327 kB = 0 MB\n\t   Load Address: 00000000\n\t   Entry Point:\t 0000000c\n\t   Verifying Checksum ... OK\n\n\t## Checking Image at 40200000 ...\n\t   Image Name:\t Simple Ramdisk Image\n\t   Image Type:\t PowerPC Linux RAMDisk Image (gzip compressed)\n\t   Data Size:\t 566530 Bytes = 553 kB = 0 MB\n\t   Load Address: 00000000\n\t   Entry Point:\t 00000000\n\t   Verifying Checksum ... OK\n\n\t=> bootm 40100000 40200000\n\t## Booting Linux kernel at 40100000 ...\n\t   Image Name:\t 2.2.13 for initrd on TQM850L\n\t   Image Type:\t PowerPC Linux Kernel Image (gzip compressed)\n\t   Data Size:\t 335725 Bytes = 327 kB = 0 MB\n\t   Load Address: 00000000\n\t   Entry Point:\t 0000000c\n\t   Verifying Checksum ... OK\n\t   Uncompressing Kernel Image ... OK\n\t## Loading RAMDisk Image at 40200000 ...\n\t   Image Name:\t Simple Ramdisk Image\n\t   Image Type:\t PowerPC Linux RAMDisk Image (gzip compressed)\n\t   Data Size:\t 566530 Bytes = 553 kB = 0 MB\n\t   Load Address: 00000000\n\t   Entry Point:\t 00000000\n\t   Verifying Checksum ... OK\n\t   Loading Ramdisk ... OK\n\tLinux version 2.2.13 (wd@denx.local.net) (gcc version 2.95.2 19991024 (release)) #1 Wed Jul 19 02:32:08 MEST 2000\n\tBoot arguments: root=/dev/ram\n\ttime_init: decrementer frequency = 187500000/60\n\tCalibrating delay loop... 49.77 BogoMIPS\n\t...\n\tRAMDISK: Compressed image found at block 0\n\tVFS: Mounted root (ext2 filesystem).\n\n\tbash#\n\nBoot Linux and pass a flat device tree:\n-----------\n\nFirst, U-Boot must be compiled with the appropriate defines. See the section\ntitled \"Linux Kernel Interface\" above for a more in depth explanation. The\nfollowing is an example of how to start a kernel and pass an updated\nflat device tree:\n\n=> print oftaddr\noftaddr=0x300000\n=> print oft\noft=oftrees/mpc8540ads.dtb\n=> tftp $oftaddr $oft\nSpeed: 1000, full duplex\nUsing TSEC0 device\nTFTP from server 192.168.1.1; our IP address is 192.168.1.101\nFilename 'oftrees/mpc8540ads.dtb'.\nLoad address: 0x300000\nLoading: #\ndone\nBytes transferred = 4106 (100a hex)\n=> tftp $loadaddr $bootfile\nSpeed: 1000, full duplex\nUsing TSEC0 device\nTFTP from server 192.168.1.1; our IP address is 192.168.1.2\nFilename 'uImage'.\nLoad address: 0x200000\nLoading:############\ndone\nBytes transferred = 1029407 (fb51f hex)\n=> print loadaddr\nloadaddr=200000\n=> print oftaddr\noftaddr=0x300000\n=> bootm $loadaddr - $oftaddr\n## Booting image at 00200000 ...\n   Image Name:\t Linux-2.6.17-dirty\n   Image Type:\t PowerPC Linux Kernel Image (gzip compressed)\n   Data Size:\t 1029343 Bytes = 1005.2 kB\n   Load Address: 00000000\n   Entry Point:\t 00000000\n   Verifying Checksum ... OK\n   Uncompressing Kernel Image ... OK\nBooting using flat device tree at 0x300000\nUsing MPC85xx ADS machine description\nMemory CAM mapping: CAM0=256Mb, CAM1=256Mb, CAM2=0Mb residual: 0Mb\n[snip]\n\n\nMore About U-Boot Image Types:\n------------------------------\n\nU-Boot supports the following image types:\n\n   \"Standalone Programs\" are directly runnable in the environment\n\tprovided by U-Boot; it is expected that (if they behave\n\twell) you can continue to work in U-Boot after return from\n\tthe Standalone Program.\n   \"OS Kernel Images\" are usually images of some Embedded OS which\n\twill take over control completely. Usually these programs\n\twill install their own set of exception handlers, device\n\tdrivers, set up the MMU, etc. - this means, that you cannot\n\texpect to re-enter U-Boot except by resetting the CPU.\n   \"RAMDisk Images\" are more or less just data blocks, and their\n\tparameters (address, size) are passed to an OS kernel that is\n\tbeing started.\n   \"Multi-File Images\" contain several images, typically an OS\n\t(Linux) kernel image and one or more data images like\n\tRAMDisks. This construct is useful for instance when you want\n\tto boot over the network using BOOTP etc., where the boot\n\tserver provides just a single image file, but you want to get\n\tfor instance an OS kernel and a RAMDisk image.\n\n\t\"Multi-File Images\" start with a list of image sizes, each\n\timage size (in bytes) specified by an \"uint32_t\" in network\n\tbyte order. This list is terminated by an \"(uint32_t)0\".\n\tImmediately after the terminating 0 follow the images, one by\n\tone, all aligned on \"uint32_t\" boundaries (size rounded up to\n\ta multiple of 4 bytes).\n\n   \"Firmware Images\" are binary images containing firmware (like\n\tU-Boot or FPGA images) which usually will be programmed to\n\tflash memory.\n\n   \"Script files\" are command sequences that will be executed by\n\tU-Boot's command interpreter; this feature is especially\n\tuseful when you configure U-Boot to use a real shell (hush)\n\tas command interpreter.\n\nBooting the Linux zImage:\n-------------------------\n\nOn some platforms, it's possible to boot Linux zImage. This is done\nusing the \"bootz\" command. The syntax of \"bootz\" command is the same\nas the syntax of \"bootm\" command.\n\nNote, defining the CONFIG_SUPPORT_RAW_INITRD allows user to supply\nkernel with raw initrd images. The syntax is slightly different, the\naddress of the initrd must be augmented by it's size, in the following\nformat: \"<initrd addres>:<initrd size>\".\n\n\nStandalone HOWTO:\n=================\n\nOne of the features of U-Boot is that you can dynamically load and\nrun \"standalone\" applications, which can use some resources of\nU-Boot like console I/O functions or interrupt services.\n\nTwo simple examples are included with the sources:\n\n\"Hello World\" Demo:\n-------------------\n\n'examples/hello_world.c' contains a small \"Hello World\" Demo\napplication; it is automatically compiled when you build U-Boot.\nIt's configured to run at address 0x00040004, so you can play with it\nlike that:\n\n\t=> loads\n\t## Ready for S-Record download ...\n\t~>examples/hello_world.srec\n\t1 2 3 4 5 6 7 8 9 10 11 ...\n\t[file transfer complete]\n\t[connected]\n\t## Start Addr = 0x00040004\n\n\t=> go 40004 Hello World! This is a test.\n\t## Starting application at 0x00040004 ...\n\tHello World\n\targc = 7\n\targv[0] = \"40004\"\n\targv[1] = \"Hello\"\n\targv[2] = \"World!\"\n\targv[3] = \"This\"\n\targv[4] = \"is\"\n\targv[5] = \"a\"\n\targv[6] = \"test.\"\n\targv[7] = \"<NULL>\"\n\tHit any key to exit ...\n\n\t## Application terminated, rc = 0x0\n\nAnother example, which demonstrates how to register a CPM interrupt\nhandler with the U-Boot code, can be found in 'examples/timer.c'.\nHere, a CPM timer is set up to generate an interrupt every second.\nThe interrupt service routine is trivial, just printing a '.'\ncharacter, but this is just a demo program. The application can be\ncontrolled by the following keys:\n\n\t? - print current values og the CPM Timer registers\n\tb - enable interrupts and start timer\n\te - stop timer and disable interrupts\n\tq - quit application\n\n\t=> loads\n\t## Ready for S-Record download ...\n\t~>examples/timer.srec\n\t1 2 3 4 5 6 7 8 9 10 11 ...\n\t[file transfer complete]\n\t[connected]\n\t## Start Addr = 0x00040004\n\n\t=> go 40004\n\t## Starting application at 0x00040004 ...\n\tTIMERS=0xfff00980\n\tUsing timer 1\n\t  tgcr @ 0xfff00980, tmr @ 0xfff00990, trr @ 0xfff00994, tcr @ 0xfff00998, tcn @ 0xfff0099c, ter @ 0xfff009b0\n\nHit 'b':\n\t[q, b, e, ?] Set interval 1000000 us\n\tEnabling timer\nHit '?':\n\t[q, b, e, ?] ........\n\ttgcr=0x1, tmr=0xff1c, trr=0x3d09, tcr=0x0, tcn=0xef6, ter=0x0\nHit '?':\n\t[q, b, e, ?] .\n\ttgcr=0x1, tmr=0xff1c, trr=0x3d09, tcr=0x0, tcn=0x2ad4, ter=0x0\nHit '?':\n\t[q, b, e, ?] .\n\ttgcr=0x1, tmr=0xff1c, trr=0x3d09, tcr=0x0, tcn=0x1efc, ter=0x0\nHit '?':\n\t[q, b, e, ?] .\n\ttgcr=0x1, tmr=0xff1c, trr=0x3d09, tcr=0x0, tcn=0x169d, ter=0x0\nHit 'e':\n\t[q, b, e, ?] ...Stopping timer\nHit 'q':\n\t[q, b, e, ?] ## Application terminated, rc = 0x0\n\n\nImplementation Internals:\n=========================\n\nThe following is not intended to be a complete description of every\nimplementation detail. However, it should help to understand the\ninner workings of U-Boot and make it easier to port it to custom\nhardware.\n\n\nInitial Stack, Global Data:\n---------------------------\n\nThe implementation of U-Boot is complicated by the fact that U-Boot\nstarts running out of ROM (flash memory), usually without access to\nsystem RAM (because the memory controller is not initialized yet).\nThis means that we don't have writable Data or BSS segments, and BSS\nis not initialized as zero. To be able to get a C environment working\nat all, we have to allocate at least a minimal stack. Implementation\noptions for this are defined and restricted by the CPU used: Some CPU\nmodels provide on-chip memory (like the IMMR area on MPC8xx and\nMPC826x processors), on others (parts of) the data cache can be\nlocked as (mis-) used as memory, etc.\n\n\tChris Hallinan posted a good summary of these issues to the\n\tU-Boot mailing list:\n\n\tSubject: RE: [U-Boot-Users] RE: More On Memory Bank x (nothingness)?\n\tFrom: \"Chris Hallinan\" <clh@net1plus.com>\n\tDate: Mon, 10 Feb 2003 16:43:46 -0500 (22:43 MET)\n\t...\n\n\tCorrect me if I'm wrong, folks, but the way I understand it\n\tis this: Using DCACHE as initial RAM for Stack, etc, does not\n\trequire any physical RAM backing up the cache. The cleverness\n\tis that the cache is being used as a temporary supply of\n\tnecessary storage before the SDRAM controller is setup. It's\n\tbeyond the scope of this list to explain the details, but you\n\tcan see how this works by studying the cache architecture and\n\toperation in the architecture and processor-specific manuals.\n\n\tOCM is On Chip Memory, which I believe the 405GP has 4K. It\n\tis another option for the system designer to use as an\n\tinitial stack/RAM area prior to SDRAM being available. Either\n\toption should work for you. Using CS 4 should be fine if your\n\tboard designers haven't used it for something that would\n\tcause you grief during the initial boot! It is frequently not\n\tused.\n\n\tCFG_SYS_INIT_RAM_ADDR should be somewhere that won't interfere\n\twith your processor/board/system design. The default value\n\tyou will find in any recent u-boot distribution in\n\twalnut.h should work for you. I'd set it to a value larger\n\tthan your SDRAM module. If you have a 64MB SDRAM module, set\n\tit above 400_0000. Just make sure your board has no resources\n\tthat are supposed to respond to that address! That code in\n\tstart.S has been around a while and should work as is when\n\tyou get the config right.\n\n\t-Chris Hallinan\n\tDS4.COM, Inc.\n\nIt is essential to remember this, since it has some impact on the C\ncode for the initialization procedures:\n\n* Initialized global data (data segment) is read-only. Do not attempt\n  to write it.\n\n* Do not use any uninitialized global data (or implicitly initialized\n  as zero data - BSS segment) at all - this is undefined, initiali-\n  zation is performed later (when relocating to RAM).\n\n* Stack space is very limited. Avoid big data buffers or things like\n  that.\n\nHaving only the stack as writable memory limits means we cannot use\nnormal global data to share information between the code. But it\nturned out that the implementation of U-Boot can be greatly\nsimplified by making a global data structure (gd_t) available to all\nfunctions. We could pass a pointer to this data as argument to _all_\nfunctions, but this would bloat the code. Instead we use a feature of\nthe GCC compiler (Global Register Variables) to share the data: we\nplace a pointer (gd) to the global data into a register which we\nreserve for this purpose.\n\nWhen choosing a register for such a purpose we are restricted by the\nrelevant  (E)ABI  specifications for the current architecture, and by\nGCC's implementation.\n\nFor PowerPC, the following registers have specific use:\n\tR1:\tstack pointer\n\tR2:\treserved for system use\n\tR3-R4:\tparameter passing and return values\n\tR5-R10: parameter passing\n\tR13:\tsmall data area pointer\n\tR30:\tGOT pointer\n\tR31:\tframe pointer\n\n\t(U-Boot also uses R12 as internal GOT pointer. r12\n\tis a volatile register so r12 needs to be reset when\n\tgoing back and forth between asm and C)\n\n    ==> U-Boot will use R2 to hold a pointer to the global data\n\n    Note: on PPC, we could use a static initializer (since the\n    address of the global data structure is known at compile time),\n    but it turned out that reserving a register results in somewhat\n    smaller code - although the code savings are not that big (on\n    average for all boards 752 bytes for the whole U-Boot image,\n    624 text + 127 data).\n\nOn ARM, the following registers are used:\n\n\tR0:\tfunction argument word/integer result\n\tR1-R3:\tfunction argument word\n\tR9:\tplatform specific\n\tR10:\tstack limit (used only if stack checking is enabled)\n\tR11:\targument (frame) pointer\n\tR12:\ttemporary workspace\n\tR13:\tstack pointer\n\tR14:\tlink register\n\tR15:\tprogram counter\n\n    ==> U-Boot will use R9 to hold a pointer to the global data\n\n    Note: on ARM, only R_ARM_RELATIVE relocations are supported.\n\nOn Nios II, the ABI is documented here:\n\thttps://www.altera.com/literature/hb/nios2/n2cpu_nii51016.pdf\n\n    ==> U-Boot will use gp to hold a pointer to the global data\n\n    Note: on Nios II, we give \"-G0\" option to gcc and don't use gp\n    to access small data sections, so gp is free.\n\nOn RISC-V, the following registers are used:\n\n\tx0: hard-wired zero (zero)\n\tx1: return address (ra)\n\tx2:\tstack pointer (sp)\n\tx3:\tglobal pointer (gp)\n\tx4:\tthread pointer (tp)\n\tx5:\tlink register (t0)\n\tx8:\tframe pointer (fp)\n\tx10-x11:\targuments/return values (a0-1)\n\tx12-x17:\targuments (a2-7)\n\tx28-31:\t temporaries (t3-6)\n\tpc:\tprogram counter (pc)\n\n    ==> U-Boot will use gp to hold a pointer to the global data\n\nSystem Initialization:\n----------------------\n\nIn the reset configuration, U-Boot starts at the reset entry point\n(on most PowerPC systems at address 0x00000100). Because of the reset\nconfiguration for CS0# this is a mirror of the on board Flash memory.\nTo be able to re-map memory U-Boot then jumps to its link address.\nTo be able to implement the initialization code in C, a (small!)\ninitial stack is set up in the internal Dual Ported RAM (in case CPUs\nwhich provide such a feature like), or in a locked part of the data\ncache. After that, U-Boot initializes the CPU core, the caches and\nthe SIU.\n\nNext, all (potentially) available memory banks are mapped using a\npreliminary mapping. For example, we put them on 512 MB boundaries\n(multiples of 0x20000000: SDRAM on 0x00000000 and 0x20000000, Flash\non 0x40000000 and 0x60000000, SRAM on 0x80000000). Then UPM A is\nprogrammed for SDRAM access. Using the temporary configuration, a\nsimple memory test is run that determines the size of the SDRAM\nbanks.\n\nWhen there is more than one SDRAM bank, and the banks are of\ndifferent size, the largest is mapped first. For equal size, the first\nbank (CS2#) is mapped first. The first mapping is always for address\n0x00000000, with any additional banks following immediately to create\ncontiguous memory starting from 0.\n\nThen, the monitor installs itself at the upper end of the SDRAM area\nand allocates memory for use by malloc() and for the global Board\nInfo data; also, the exception vector code is copied to the low RAM\npages, and the final stack is set up.\n\nOnly after this relocation will you have a \"normal\" C environment;\nuntil that you are restricted in several ways, mostly because you are\nrunning from ROM, and because the code will have to be relocated to a\nnew address in RAM.\n\n\nContributing\n============\n\nThe U-Boot projects depends on contributions from the user community.\nIf you want to participate, please, have a look at the 'General'\nsection of https://docs.u-boot.org/en/latest/develop/index.html\nwhere we describe coding standards and the patch submission process.\n",
      "stars_today": 4
    },
    {
      "id": 185861173,
      "name": "opentelemetry-collector",
      "full_name": "open-telemetry/opentelemetry-collector",
      "description": "OpenTelemetry Collector",
      "html_url": "https://github.com/open-telemetry/opentelemetry-collector",
      "stars": 6496,
      "forks": 1832,
      "language": "Go",
      "topics": [
        "metrics",
        "monitoring",
        "observability",
        "open-telemetry",
        "opentelemetry",
        "telemetry"
      ],
      "created_at": "2019-05-09T19:42:55Z",
      "updated_at": "2026-01-15T00:44:49Z",
      "pushed_at": "2026-01-14T23:57:41Z",
      "open_issues": 671,
      "owner": {
        "login": "open-telemetry",
        "avatar_url": "https://avatars.githubusercontent.com/u/49998002?v=4"
      },
      "readme": "---\n\n<p align=\"center\">\n  <strong>\n    <a href=\"https://opentelemetry.io/docs/collector/getting-started/\">Getting Started</a>\n    &nbsp;&nbsp;&bull;&nbsp;&nbsp;\n    <a href=\"CONTRIBUTING.md\">Getting Involved</a>\n    &nbsp;&nbsp;&bull;&nbsp;&nbsp;\n    <a href=\"https://cloud-native.slack.com/archives/C01N6P7KR6W\">Getting In Touch</a>\n  </strong>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/open-telemetry/opentelemetry-collector/actions/workflows/build-and-test.yml?query=branch%3Amain\">\n    <img alt=\"Build Status\" src=\"https://img.shields.io/github/actions/workflow/status/open-telemetry/opentelemetry-collector/build-and-test.yml?branch=main&style=for-the-badge\">\n  </a>\n  <a href=\"https://goreportcard.com/report/github.com/open-telemetry/opentelemetry-collector\">\n    <img alt=\"Go Report Card\" src=\"https://goreportcard.com/badge/github.com/open-telemetry/opentelemetry-collector?style=for-the-badge\">\n  </a>\n  <a href=\"https://codecov.io/gh/open-telemetry/opentelemetry-collector/branch/main/\">\n    <img alt=\"Codecov Status\" src=\"https://img.shields.io/codecov/c/github/open-telemetry/opentelemetry-collector?style=for-the-badge\">\n  </a>\n  <a href=\"https://github.com/open-telemetry/opentelemetry-collector/releases\">\n    <img alt=\"GitHub release (latest by date including pre-releases)\" src=\"https://img.shields.io/github/v/release/open-telemetry/opentelemetry-collector?include_prereleases&style=for-the-badge\">\n  </a>\n  </br>\n  <a href=\"https://www.bestpractices.dev/projects/8404\"><img src=\"https://www.bestpractices.dev/projects/8404/badge\">\n  </a>\n  <a href=\"https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:opentelemetry\">\n    <img alt=\"Fuzzing Status\" src=\"https://oss-fuzz-build-logs.storage.googleapis.com/badges/opentelemetry.svg\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <strong>\n    <a href=\"docs/vision.md\">Vision</a>\n    &nbsp;&nbsp;&bull;&nbsp;&nbsp;\n    <a href=\"https://opentelemetry.io/docs/collector/configuration/\">Configuration</a>\n    &nbsp;&nbsp;&bull;&nbsp;&nbsp;\n    <a href=\"https://opentelemetry.io/docs/collector/internal-telemetry/#use-internal-telemetry-to-monitor-the-collector\">Monitoring</a>\n    &nbsp;&nbsp;&bull;&nbsp;&nbsp;\n    <a href=\"docs/security-best-practices.md\">Security</a>\n    &nbsp;&nbsp;&bull;&nbsp;&nbsp;\n    <a href=\"https://pkg.go.dev/go.opentelemetry.io/collector\">Package</a>\n  </strong>\n</p>\n\n---\n\n# <img src=\"https://opentelemetry.io/img/logos/opentelemetry-logo-nav.png\" alt=\"OpenTelemetry Icon\" width=\"45\" height=\"\"> OpenTelemetry Collector\n\nThe OpenTelemetry Collector offers a vendor-agnostic implementation on how to\nreceive, process and export telemetry data. In addition, it removes the need\nto run, operate and maintain multiple agents/collectors in order to support\nopen-source telemetry data formats (e.g. Jaeger, Prometheus, etc.) to\nmultiple open-source or commercial back-ends.\n\nObjectives:\n\n- Usable: Reasonable default configuration, supports popular protocols, runs and collects out of the box.\n- Performant: Highly stable and performant under varying loads and configurations.\n- Observable: An exemplar of an observable service.\n- Extensible: Customizable without touching the core code.\n- Unified: Single codebase, deployable as an agent or collector with support for traces, metrics and logs.\n\n## Community\n\nThe OpenTelemetry Collector SIG is present at the [#otel-collector](https://cloud-native.slack.com/archives/C01N6P7KR6W)\nchannel on the CNCF Slack and [meets once a week](https://github.com/open-telemetry/community#implementation-sigs) via\nvideo calls. Everyone is invited to join those calls, which typically serves the following purposes:\n\n- meet the humans behind the project\n- get an opinion about specific proposals\n- look for a sponsor for a proposed component after trying already via GitHub and Slack\n- get attention to a specific pull-request that got stuck and is difficult to discuss asynchronously\n\nWe rotate our video calls between three time slots, in order to\nallow everyone to join at least once every three meetings. The rotation order is as follows:\n\nTuesday:\n\n- [17:00 PT](https://dateful.com/convert/pst-pdt-pacific-time?t=1700)\n\nWednesday:\n\n- [09:00 PT](https://dateful.com/convert/pst-pdt-pacific-time?t=0900)\n- [05:00 PT](https://dateful.com/convert/pst-pdt-pacific-time?t=0500)\n\nContributors to the project are also welcome to have ad-hoc meetings for synchronous discussions about specific points.\nPost a note in #otel-collector-dev on Slack inviting others, specifying the topic to be discussed. Unless there are strong\nreasons to keep the meeting private, please make it an open invitation for other contributors to join. Try also to\nidentify who would be the other contributors interested on that topic and in which timezones they are.\n\nRemember that our source of truth is GitHub: every decision made via Slack or video calls has to be recorded in the\nrelevant GitHub issue. Ideally, the agenda items from the meeting notes would include a link to the issue or pull\nrequest where a discussion is happening already. We acknowledge that not everyone can join Slack or the synchronous\ncalls and don't want them to feel excluded.\n\n## Supported OTLP version\n\nThis code base is currently built against using OTLP protocol v1.5.0,\nconsidered Stable. [See the OpenTelemetry Protocol Stability\ndefinition\nhere.](https://github.com/open-telemetry/opentelemetry-proto?tab=readme-ov-file#stability-definition)\n\n## Stability levels\n\nSee [Stability Levels and versioning](docs/component-stability.md) for more details.\n\n## Compatibility\n\nWhen used as a library, the OpenTelemetry Collector attempts to track the currently supported versions of Go, as [defined by the Go team](https://go.dev/doc/devel/release#policy).\nRemoving support for an unsupported Go version is not considered a breaking change.\n\nSupport for Go versions on the OpenTelemetry Collector is updated as follows:\n\n1. The first release after the release of a new Go minor version `N` will add build and tests steps for the new Go minor version.\n2. The first release after the release of a new Go minor version `N` will remove support for Go version `N-2`.\n\nOfficial OpenTelemetry Collector distro binaries will be built with a release in the latest Go minor version series.\n\n## Verifying the images signatures\n\n> [!NOTE]\n> To verify a signed artifact or blob, first [install Cosign](https://docs.sigstore.dev/cosign/system_config/installation/), then follow the instructions below.\n\nWe are signing the images `otel/opentelemetry-collector` and `otel/opentelemetry-collector-contrib` using [sigstore cosign](https://github.com/sigstore/cosign) tool and to verify the signatures you can run the following command:\n\n```console\n$ cosign verify \\\n  --certificate-identity=https://github.com/open-telemetry/opentelemetry-collector-releases/.github/workflows/base-release.yaml@refs/tags/<RELEASE_TAG> \\\n  --certificate-oidc-issuer=https://token.actions.githubusercontent.com \\\n  <OTEL_COLLECTOR_IMAGE>\n```\n\nwhere:\n\n- `<RELEASE_TAG>`: is the release that you want to validate\n- `<OTEL_COLLECTOR_IMAGE>`: is the image that you want to check\n\nExample:\n\n```console\n$ cosign verify --certificate-identity=https://github.com/open-telemetry/opentelemetry-collector-releases/.github/workflows/base-release.yaml@refs/tags/v0.98.0 --certificate-oidc-issuer=https://token.actions.githubusercontent.com ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.98.0\n\nVerification for ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.98.0 --\nThe following checks were performed on each of these signatures:\n  - The cosign claims were validated\n  - Existence of the claims in the transparency log was verified offline\n  - The code-signing certificate was verified using trusted certificate authority certificates\n\n[{\"critical\":{\"identity\":{\"docker-reference\":\"ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib\"},\"image\":{\"docker-manifest-digest\":\"sha256:5cea85bcbc734a3c0a641368e5a4ea9d31b472997e9f2feca57eeb4a147fcf1a\"},\"type\":\"cosign container image signature\"},\"optional\":{\"1.3.6.1.4.1.57264.1.1\":\"https://token.actions.githubusercontent.com\",\"1.3.6.1.4.1.57264.1.2\":\"push\",\"1.3.6.1.4.1.57264.1.3\":\"9e20bf5c142e53070ccb8320a20315fffb41469e\",\"1.3.6.1.4.1.57264.1.4\":\"Release Contrib\",\"1.3.6.1.4.1.57264.1.5\":\"open-telemetry/opentelemetry-collector-releases\",\"1.3.6.1.4.1.57264.1.6\":\"refs/tags/v0.98.0\",\"Bundle\":{\"SignedEntryTimestamp\":\"MEUCIQDdlmNeKXQrHnonwWiHLhLLwFDVDNoOBCn2sv85J9P8mgIgDQFssWJImo1hn38VlojvSCL7Qq5FMmtnGu0oLsNdOm8=\",\"Payload\":{\"body\":\"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\",\"integratedTime\":1712809120,\"logIndex\":84797936,\"logID\":\"c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\"}},\"Issuer\":\"https://token.actions.githubusercontent.com\",\"Subject\":\"https://github.com/open-telemetry/opentelemetry-collector-releases/.github/workflows/base-release.yaml@refs/tags/v0.98.0\",\"githubWorkflowName\":\"Release Contrib\",\"githubWorkflowRef\":\"refs/tags/v0.98.0\",\"githubWorkflowRepository\":\"open-telemetry/opentelemetry-collector-releases\",\"githubWorkflowSha\":\"9e20bf5c142e53070ccb8320a20315fffb41469e\",\"githubWorkflowTrigger\":\"push\"}},{\"critical\":{\"identity\":{\"docker-reference\":\"ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib\"},\"image\":{\"docker-manifest-digest\":\"sha256:5cea85bcbc734a3c0a641368e5a4ea9d31b472997e9f2feca57eeb4a147fcf1a\"},\"type\":\"cosign container image signature\"},\"optional\":{\"1.3.6.1.4.1.57264.1.1\":\"https://token.actions.githubusercontent.com\",\"1.3.6.1.4.1.57264.1.2\":\"push\",\"1.3.6.1.4.1.57264.1.3\":\"9e20bf5c142e53070ccb8320a20315fffb41469e\",\"1.3.6.1.4.1.57264.1.4\":\"Release Contrib\",\"1.3.6.1.4.1.57264.1.5\":\"open-telemetry/opentelemetry-collector-releases\",\"1.3.6.1.4.1.57264.1.6\":\"refs/tags/v0.98.0\",\"Bundle\":{\"SignedEntryTimestamp\":\"MEUCIQD1ehDnPO6fzoPIpeQ3KFuYHHBiX7RcEbpo9B2r7JAlzwIgZ1bsuQz7gAXbNU1IEdsTQgfAnRk3xVXO16GnKXM2sAQ=\",\"Payload\":{\"body\":\"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\",\"integratedTime\":1712809122,\"logIndex\":84797940,\"logID\":\"c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\"}},\"Issuer\":\"https://token.actions.githubusercontent.com\",\"Subject\":\"https://github.com/open-telemetry/opentelemetry-collector-releases/.github/workflows/base-release.yaml@refs/tags/v0.98.0\",\"githubWorkflowName\":\"Release Contrib\",\"githubWorkflowRef\":\"refs/tags/v0.98.0\",\"githubWorkflowRepository\":\"open-telemetry/opentelemetry-collector-releases\",\"githubWorkflowSha\":\"9e20bf5c142e53070ccb8320a20315fffb41469e\",\"githubWorkflowTrigger\":\"push\"}}]\n```\n\n> [!NOTE]\n> We started signing the images with release `v0.95.0`\n\n## Contributing\n\nSee the [Contributing Guide](CONTRIBUTING.md) for details.\n\nHere is a list of community roles with current and previous members:\n\n### Maintainers\n\n- [Alex Boten](https://github.com/codeboten), Honeycomb\n- [Bogdan Drutu](https://github.com/bogdandrutu), Snowflake\n- [Dmitrii Anoshin](https://github.com/dmitryax), Splunk\n- [Pablo Baeyens](https://github.com/mx-psi), DataDog\n\nFor more information about the maintainer role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#maintainer).\n\n### Approvers\n\n- [Andrew Wilkins](https://github.com/axw), Elastic\n- [Antoine Toulme](https://github.com/atoulme), Splunk\n- [Damien Mathieu](https://github.com/dmathieu), Elastic\n- [Evan Bradley](https://github.com/evan-bradley), Dynatrace\n- [Jade Guiton](https://github.com/jade-guiton-dd), Datadog\n- [Joshua MacDonald](https://github.com/jmacd), Microsoft\n- [Tyler Helmuth](https://github.com/TylerHelmuth), Honeycomb\n- [Yang Song](https://github.com/songy23), Datadog\n\nFor more information about the approver role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#approver).\n\nIn addition to what is described at the organization-level, the SIG Collector requires all core approvers to take part in rotating\nthe role of the [release manager](./docs/release.md#release-manager).\n\n### Triagers\n\n- [Andrzej Stencel](https://github.com/andrzej-stencel), Elastic\n- [Chao Weng](https://github.com/sincejune), AppDynamics\n- [Vihas Makwana](https://github.com/VihasMakwana), Elastic\n- Actively seeking contributors to triage issues\n\nFor more information about the triager role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#triager).\n\n### Emeritus Maintainers\n\n- [Paulo Janotti](https://github.com/pjanotti)\n- [Tigran Najaryan](https://github.com/tigrannajaryan)\n\nFor more information about the emeritus role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#emeritus-maintainerapprovertriager).\n\n### Emeritus Approvers\n\n- [Anthony Mirabella](https://github.com/Aneurysm9)\n- [Daniel Jaglowski](https://github.com/djaglowski)\n- [James Bebbington](https://github.com/james-bebbington)\n- [Jay Camp](https://github.com/jrcamp)\n- [Juraci PaixÃ£o KrÃ¶hling](https://github.com/jpkrohling)\n- [Nail Islamov](https://github.com/nilebox)\n- [Owais Lone](https://github.com/owais)\n- [Rahul Patel](https://github.com/rghetia)\n- [Steven Karis](https://github.com/sjkaris)\n\nFor more information about the emeritus role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#emeritus-maintainerapprovertriager).\n\n### Emeritus Triagers\n\n- [Alolita Sharma](https://github.com/alolita)\n- [Andrew Hsu](https://github.com/andrewhsu)\n- [Punya Biswal](https://github.com/punya)\n- [Steve Flanders](https://github.com/flands)\n\nFor more information about the emeritus role, see the [community repository](https://github.com/open-telemetry/community/blob/main/guides/contributor/membership.md#emeritus-maintainerapprovertriager).\n\n### Thanks to all of our contributors!\n\n<a href=\"https://github.com/open-telemetry/opentelemetry-collector/graphs/contributors\">\n  <img alt=\"Repo contributors\" src=\"https://contrib.rocks/image?repo=open-telemetry/opentelemetry-collector\" />\n</a>\n",
      "stars_today": 4
    },
    {
      "id": 38617,
      "name": "sqlcipher",
      "full_name": "sqlcipher/sqlcipher",
      "description": "SQLCipher is a standalone fork of SQLite that adds 256 bit AES encryption of database files and other security features.",
      "html_url": "https://github.com/sqlcipher/sqlcipher",
      "stars": 6967,
      "forks": 1357,
      "language": "C",
      "topics": [],
      "created_at": "2008-07-30T17:20:41Z",
      "updated_at": "2026-01-14T16:39:55Z",
      "pushed_at": "2025-12-08T15:25:58Z",
      "open_issues": 7,
      "owner": {
        "login": "sqlcipher",
        "avatar_url": "https://avatars.githubusercontent.com/u/649049?v=4"
      },
      "readme": "## SQLCipher\n\nSQLCipher is a standalone fork of the [SQLite](https://www.sqlite.org/) database library that adds 256 bit AES encryption of database files and other security features like:\n\n- on-the-fly encryption\n- tamper detection\n- memory sanitization\n- strong key derivation\n\nSQLCipher is based on SQLite and stable upstream release features are periodically integrated. While SQLCipher is maintained as a separate version of the source tree, the project minimizes alterations to core SQLite code whenever possible.\n\nSQLCipher is maintained by Zetetic, LLC, and additional information and documentation is available on the official [SQLCipher site](https://www.zetetic.net/sqlcipher/).\n\n## Features\n\n- Fast performance with as little as 5-15% overhead for encryption on many operations\n- 100% of data in the database file is encrypted\n- Good security practices (CBC mode, HMAC, key derivation)\n- Zero-configuration and application level cryptography\n- Support for multiple cryptographic providers\n\n## Compatibility\n\nSQLCipher maintains database format compatibility within the same major version number so an application on any platform can open databases created by any other application provided the major version of SQLCipher is the same between them. However, major version updates (e.g. from 3.x to 4.x) often include changes to default settings. This means that newer major versions of SQLCipher will not open databases created by older versions without using special settings. For example, SQLCipher 4 introduces many new performance and security enhancements. The new default algorithms, increased KDF iterations, and larger page size mean that SQLCipher 4 will not open databases created by SQLCipher 1.x, 2.x, or 3.x by default. Instead, an application would either need to migrate the older databases to use the new format or enable a special backwards-compatibility mode. The available options are described in SQLCipher's [upgrade documentation](https://discuss.zetetic.net/t/upgrading-to-sqlcipher-4/3283). \n\nSQLCipher is also compatible with standard SQLite databases. When a key is not provided, SQLCipher will behave just like the standard SQLite library. It is also possible to convert from a plaintext database (standard SQLite) to an encrypted SQLCipher database using [ATTACH and the sqlcipher_export() convenience function](https://discuss.zetetic.net/t/how-to-encrypt-a-plaintext-sqlite-database-to-use-sqlcipher-and-avoid-file-is-encrypted-or-is-not-a-database-errors/868).\n\n## Contributions\n\nThe SQLCipher team welcomes contributions to the core library. All contributions including pull requests and patches should be based on the `prerelease` branch, and must be accompanied by a [contributor agreement](https://www.zetetic.net/contributions/). We strongly encourage [discussion](https://discuss.zetetic.net/c/sqlcipher) of the proposed change prior to development and submission.\n\n## Compiling\n\nBuilding SQLCipher is similar to compiling a regular version of SQLite from source, with a few small exceptions. You must:\n\n 1. define `SQLITE_HAS_CODEC`\n 2. define `SQLITE_TEMP_STORE=2` or `SQLITE_TEMP_STORE=3` (or use `configure`'s --with-tempstore=yes option)\n 3. define `SQLITE_EXTRA_INIT=sqlcipher_extra_init` and `SQLITE_EXTRA_SHUTDOWN=sqlcipher_extra_shutdown`\n 4. define `SQLITE_THREADSAFE` to `1` or `2` (enabled automatically by `configure`)\n 2. compile and link with a supported cryptographic provider (OpenSSL, LibTomCrypt, CommonCrypto/Security.framework, or NSS)\n \nThe following examples demonstrate use of OpenSSL, which is a readily available provider on most Unix-like systems. Note that, in this example, `--with-tempstore=yes` is setting `SQLITE_TEMP_STORE=2` for the build, and `SQLITE_THREADSAFE` has a default value of `1`.\n\n```\n$ ./configure --with-tempstore=yes CFLAGS=\"-DSQLITE_HAS_CODEC -DSQLITE_EXTRA_INIT=sqlcipher_extra_init -DSQLITE_EXTRA_SHUTDOWN=sqlcipher_extra_shutdown\" \\\n\tLDFLAGS=\"-lcrypto\"\n$ make\n```\n\n## Testing\n\nThe full SQLite test suite will not complete successfully when using SQLCipher. In some cases encryption interferes with low-level tests that require access to database file data or features which are unsupported by SQLCipher. Those tests that are intended to support encryption are intended for non-SQLCipher implementations. In addition, because SQLite tests are not always isolated, if one test fails it can trigger a domino effect with other failures in later steps.\n\nAs a result, the SQLCipher package includes it's own independent tests that exercise and verify the core functionality of the SQLCipher extensions. This test suite is intended to provide an abbreviated verification of SQLCipher's internal logic; it does not perform an exhaustive test of the SQLite database system as a whole or verify functionality on specific platforms. Because SQLCipher is based on stable upstream builds of SQLite, it is considered a basic assumption that the core SQLite library code is operating properly (the SQLite core is almost untouched in SQLCipher). Thus, the additional SQLCipher-specific test provide the requisite verification that the library is operating as expected with SQLCipher's security features enabled.\n\nTo run SQLCipher specific tests, configure as described here and run the following to execute the tests and receive a report of the results:\n\n```\n$ ./configure --with-tempstore=yes --enable-fts5 CFLAGS=\"-DSQLITE_HAS_CODEC -DSQLITE_EXTRA_INIT=sqlcipher_extra_init -DSQLITE_EXTRA_SHUTDOWN=sqlcipher_extra_shutdown -DSQLCIPHER_TEST\" \\\n\tLDFLAGS=\"-lcrypto\"\n$ make testfixture\n$ ./testfixture test/sqlcipher.test\n```\n\n## Encrypting a database\n\nTo specify an encryption passphrase for the database via the SQL interface you \nuse a PRAGMA. The passphrase you enter is passed through PBKDF2 key derivation to\nobtain the encryption key for the database \n\n\tPRAGMA key = 'passphrase';\n\nAlternately, you can specify an exact byte sequence using a blob literal. If you\nuse this method it is your responsibility to ensure that the data you provide is a\n64 character hex string, which will be converted directly to 32 bytes (256 bits) of \nkey data without key derivation.\n\n\tPRAGMA key = \"x'2DD29CA851E7B56E4697B0E1F08507293D761A05CE4D1B628663F411A8086D99'\";\n\nTo encrypt a database programmatically you can use the `sqlite3_key` function. \nThe data provided in `pKey` is converted to an encryption key according to the \nsame rules as `PRAGMA key`. \n\n\tint sqlite3_key(sqlite3 *db, const void *pKey, int nKey);\n\n`PRAGMA key` or `sqlite3_key` should be called as the first operation when a database is open.\n\n## Changing a database key\n\nTo change the encryption passphrase for an existing database you may use the rekey PRAGMA\nafter you've supplied the correct database password;\n\n\tPRAGMA key = 'passphrase'; -- start with the existing database passphrase\n\tPRAGMA rekey = 'new-passphrase'; -- rekey will reencrypt with the new passphrase\n\nThe hex rekey pragma may be used to rekey to a specific binary value\n\n\tPRAGMA rekey = \"x'2DD29CA851E7B56E4697B0E1F08507293D761A05CE4D1B628663F411A8086D99'\";\n\nThis can be accomplished programmatically by using sqlite3_rekey;\n  \n\tsqlite3_rekey(sqlite3 *db, const void *pKey, int nKey)\n\n## Support\n\nThe primary source for complete documentation (design, API, platforms, usage) is the SQLCipher website:\n\nhttps://www.zetetic.net/sqlcipher/documentation\n\nThe primary avenue for support and discussions is the SQLCipher discuss site:\n\nhttps://discuss.zetetic.net/c/sqlcipher\n\nIssues or support questions on using SQLCipher should be entered into the \nGitHub Issue tracker:\n\nhttps://github.com/sqlcipher/sqlcipher/issues\n\nPlease DO NOT post issues, support questions, or other problems to blog \nposts about SQLCipher as we do not monitor them frequently.\n\nIf you are using SQLCipher in your own software please let us know at \nsupport@zetetic.net!\n\n## Community Edition Open Source License\n\nCopyright (c) 2025, ZETETIC LLC\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in the\n      documentation and/or other materials provided with the distribution.\n    * Neither the name of the ZETETIC LLC nor the\n      names of its contributors may be used to endorse or promote products\n      derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY ZETETIC LLC ''AS IS'' AND ANY\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL ZETETIC LLC BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n# Begin SQLite README.md\n\n<h1 align=\"center\">SQLite Source Repository</h1>\n\nThis repository contains the complete source code for the\n[SQLite database engine](https://sqlite.org/), including\nmany tests.  Additional tests and most documentation\nare managed separately.\n\nSee the [on-line documentation](https://sqlite.org/) for more information\nabout what SQLite is and how it works from a user's perspective.  This\nREADME file is about the source code that goes into building SQLite,\nnot about how SQLite is used.\n\n## Version Control\n\nSQLite sources are managed using\n[Fossil](https://fossil-scm.org/), a distributed version control system\nthat was specifically designed and written to support SQLite development.\nThe [Fossil repository](https://sqlite.org/src/timeline) contains the urtext.\n\nIf you are reading this on GitHub or some other Git repository or service,\nthen you are looking at a mirror.  The names of check-ins and\nother artifacts in a Git mirror are different from the official\nnames for those objects.  The official names for check-ins are\nfound in a footer on the check-in comment for authorized mirrors.\nThe official check-in name can also be seen in the `manifest.uuid` file\nin the root of the tree.  Always use the official name, not  the\nGit-name, when communicating about an SQLite check-in.\n\nIf you pulled your SQLite source code from a secondary source and want to\nverify its integrity, there are hints on how to do that in the\n[Verifying Code Authenticity](#vauth) section below.\n\n## Contacting The SQLite Developers\n\nThe preferred way to ask questions or make comments about SQLite or to\nreport bugs against SQLite is to visit the \n[SQLite Forum](https://sqlite.org/forum) at <https://sqlite.org/forum/>.\nAnonymous postings are permitted.\n\nIf you think you have found a bug that has security implications and\nyou do not want to report it on the public forum, you can send a private\nemail to drh at sqlite dot org.\n\n## Public Domain\n\nThe SQLite source code is in the public domain.  See\n<https://sqlite.org/copyright.html> for details. \n\nBecause SQLite is in the public domain, we do not normally accept pull\nrequests, because if we did take a pull request, the changes in that\npull request might carry a copyright and the SQLite source code would\nthen no longer be fully in the public domain.\n\n## Obtaining The SQLite Source Code\n\nSource code tarballs or ZIP archives are available at:\n\n  *  [Latest trunk check-in](https://sqlite.org/src/rchvdwnld/trunk).\n\n  *  [Latest release](https://sqlite.org/src/rchvdwnld/release)\n\n  *  For other check-ins, browse the\n     [project timeline](https://sqlite.org/src/timeline?y=ci) and\n     click on the check-in hash of the check-in you want to download.\n     On the resulting \"info\" page, click one of the options to the\n     right of the \"**Downloads:**\" label in the \"**Overview**\" section\n     near the top.\n\nTo access sources directly using [Fossil](https://fossil-scm.org/home),\nfirst install Fossil version 2.0 or later.\nSource tarballs and precompiled binaries for Fossil are available at\n<https://fossil-scm.org/home/uv/download.html>.  Fossil is\na stand-alone program.  To install, simply download or build the single\nexecutable file and put that file someplace on your $PATH or %PATH%.\nThen run commands like this:\n\n        mkdir -p ~/sqlite\n        cd ~/sqlite\n        fossil open https://sqlite.org/src\n\nThe initial \"fossil open\" command will take two or three minutes.  Afterwards,\nyou can do fast, bandwidth-efficient updates to the whatever versions\nof SQLite you like.  Some examples:\n\n        fossil update trunk             ;# latest trunk check-in\n        fossil update release           ;# latest official release\n        fossil update trunk:2024-01-01  ;# First trunk check-in after 2024-01-01\n        fossil update version-3.39.0    ;# Version 3.39.0\n\nOr type \"fossil ui\" to get a web-based user interface.\n\n## Compiling for Unix-like systems\n\nFirst create a directory in which to place\nthe build products.  It is recommended, but not required, that the\nbuild directory be separate from the source directory.  Cd into the\nbuild directory and then from the build directory run the configure\nscript found at the root of the source tree.  Then run \"make\".\n\nFor example:\n\n        apt install gcc make tcl-dev  ;#  Make sure you have all the necessary build tools\n        tar xzf sqlite.tar.gz         ;#  Unpack the source tree into \"sqlite\"\n        mkdir bld                     ;#  Build will occur in a sibling directory\n        cd bld                        ;#  Change to the build directory\n        ../sqlite/configure           ;#  Run the configure script\n        make sqlite3                  ;#  Builds the \"sqlite3\" command-line tool\n        make sqlite3.c                ;#  Build the \"amalgamation\" source file\n        make sqldiff                  ;#  Builds the \"sqldiff\" command-line tool\n        # Makefile targets below this point require tcl-dev\n        make tclextension-install     ;#  Build and install the SQLite TCL extension\n        make devtest                  ;#  Run development tests\n        make releasetest              ;#  Run full release tests\n        make sqlite3_analyzer         ;#  Builds the \"sqlite3_analyzer\" tool\n\nSee the makefile for additional targets.  For debugging builds, the\ncore developers typically run \"configure\" with options like this:\n\n        ../sqlite/configure --enable-all --enable-debug CFLAGS='-O0 -g'\n\nFor release builds, the core developers usually do:\n\n        ../sqlite/configure --enable-all\n\nAlmost all makefile targets require a \"tclsh\" TCL interpreter version 8.6 or\nlater.  The \"tclextension-install\" target and the test targets that follow\nall require TCL development libraries too.  (\"apt install tcl-dev\").  It is\nhelpful, but is not required, to install the SQLite TCL extension (the\n\"tclextension-install\" target) prior to running tests.  The \"releasetest\"\ntarget has additional requirements, such as \"valgrind\".\n\nOn \"make\" command-lines, one can add \"OPTIONS=...\" to specify additional\ncompile-time options over and above those set by ./configure.  For example,\nto compile with the SQLITE_OMIT_DEPRECATED compile-time option, one could say:\n\n        ./configure --enable-all\n        make OPTIONS=-DSQLITE_OMIT_DEPRECATED sqlite3\n\nThe configure script uses autoconf 2.61 and libtool.  If the configure\nscript does not work out for you, there is a generic makefile named\n\"Makefile.linux-gcc\" in the top directory of the source tree that you\ncan copy and edit to suit your needs.  Comments on the generic makefile\nshow what changes are needed.\n\n## Compiling for Windows Using MSVC\n\nOn Windows, everything can be compiled with MSVC.\nYou will also need a working installation of TCL if you want to run tests.\nTCL is not required if you just want to build SQLite itself.\nSee the [compile-for-windows.md](doc/compile-for-windows.md) document for\nadditional information about how to install MSVC and TCL and configure your\nbuild environment.\n\nIf you want to run tests, you need to let SQLite know the location of your\nTCL library, using a command like this:\n\n        set TCLDIR=c:\\Tcl\n\nSQLite uses \"tclsh.exe\" as part of the build process, and so that\nprogram will need to be somewhere on your %PATH%.  SQLite itself\ndoes not contain any TCL code, but it does use TCL to run tests.\nYou may need to install TCL development\nlibraries in order to successfully complete some makefile targets.\nIt is helpful, but is not required, to install the SQLite TCL extension\n(the \"tclextension-install\" target) prior to running tests.\n\nBuild using Makefile.msc.  Example:\n\n        nmake /f Makefile.msc sqlite3.exe\n        nmake /f Makefile.msc sqlite3.c\n        nmake /f Makefile.msc sqldiff.exe\n        # Makefile targets below this point require TCL development libraries\n        nmake /f Makefile.msc tclextension-install\n        nmake /f Makefile.msc devtest\n        nmake /f Makefile.msc releasetest\n        nmake /f Makefile.msc sqlite3_analyzer.exe\n \nThere are many other makefile targets.  See comments in Makefile.msc for\ndetails.\n\nAs with the unix Makefile, the OPTIONS=... argument can be passed on the nmake\ncommand-line to enable new compile-time options.  For example:\n\n        nmake /f Makefile.msc OPTIONS=-DSQLITE_OMIT_DEPRECATED sqlite3.exe\n\n## Source Tree Map\n\n  *  **src/** - This directory contains the primary source code for the\n     SQLite core.  For historical reasons, C-code used for testing is\n     also found here.  Source files intended for testing begin with \"`test`\".\n     The `tclsqlite3.c` and `tclsqlite3.h` files are the TCL interface\n     for SQLite and are also not part of the core.\n\n  *  **test/** - This directory and its subdirectories contains code used\n     for testing.  Files that end in \"`.test`\" are TCL scripts that run\n     tests using an augmented TCL interpreter named \"testfixture\".  Use\n     a command like \"`make testfixture`\" (unix) or \n     \"`nmake /f Makefile.msc testfixture.exe`\" (windows) to build that\n     augmented TCL interpreter, then run individual tests using commands like\n     \"`testfixture test/main.test`\".  This test/ subdirectory also contains\n     additional C code modules and scripts for other kinds of testing.\n\n  *  **tool/** - This directory contains programs and scripts used to\n     build some of the machine-generated code that goes into the SQLite\n     core, as well as to build and run tests and perform diagnostics.\n     The source code to [the Lemon parser generator](./doc/lemon.html) is\n     found here.  There are also TCL scripts used to build and/or transform\n     source code files.  For example, the tool/mksqlite3h.tcl script reads\n     the src/sqlite.h.in file and uses it as a template to construct\n     the deliverable \"sqlite3.h\" file that defines the SQLite interface.\n\n  *  **ext/** - Various extensions to SQLite are found under this\n     directory.  For example, the FTS5 subsystem is in \"ext/fts5/\".\n     Some of these extensions (ex: FTS3/4, FTS5, RTREE) might get built\n     into the SQLite amalgamation, but not all of them.  The\n     \"ext/misc/\" subdirectory contains an assortment of one-file extensions,\n     many of which are omitted from the SQLite core, but which are included\n     in the [SQLite CLI](https://sqlite.org/cli.html).\n     \n  *  **doc/** - Some documentation files about SQLite internals are found\n     here.  Note, however, that the primary documentation designed for\n     application developers and users of SQLite is in a completely separate\n     repository.  Note also that the primary API documentation is derived\n     from specially constructed comments in the src/sqlite.h.in file.\n\n### Generated Source Code Files\n\nSeveral of the C-language source files used by SQLite are generated from\nother sources rather than being typed in manually by a programmer.  This\nsection will summarize those automatically-generated files.  To create all\nof the automatically-generated files, simply run \"make target&#95;source\".\nThe \"target&#95;source\" make target will create a subdirectory \"tsrc/\" and\nfill it with all the source files needed to build SQLite, both\nmanually-edited files and automatically-generated files.\n\nThe SQLite interface is defined by the **sqlite3.h** header file, which is\ngenerated from src/sqlite.h.in, ./manifest.uuid, and ./VERSION.  The\n[Tcl script](https://www.tcl.tk) at tool/mksqlite3h.tcl does the conversion.\nThe manifest.uuid file contains the SHA3 hash of the particular check-in\nand is used to generate the SQLITE\\_SOURCE\\_ID macro.  The VERSION file\ncontains the current SQLite version number.  The sqlite3.h header is really\njust a copy of src/sqlite.h.in with the source-id and version number inserted\nat just the right spots. Note that comment text in the sqlite3.h file is\nused to generate much of the SQLite API documentation.  The Tcl scripts\nused to generate that documentation are in a separate source repository.\n\nThe SQL language parser is **parse.c** which is generated from a grammar in\nthe src/parse.y file.  The conversion of \"parse.y\" into \"parse.c\" is done\nby the [lemon](./doc/lemon.html) LALR(1) parser generator.  The source code\nfor lemon is at tool/lemon.c.  Lemon uses the tool/lempar.c file as a\ntemplate for generating its parser.\nLemon also generates the **parse.h** header file, at the same time it\ngenerates parse.c.\n\nThe **opcodes.h** header file contains macros that define the numbers\ncorresponding to opcodes in the \"VDBE\" virtual machine.  The opcodes.h\nfile is generated by scanning the src/vdbe.c source file.  The\nTcl script at ./mkopcodeh.tcl does this scan and generates opcodes.h.\nA second Tcl script, ./mkopcodec.tcl, then scans opcodes.h to generate\nthe **opcodes.c** source file, which contains a reverse mapping from\nopcode-number to opcode-name that is used for EXPLAIN output.\n\nThe **keywordhash.h** header file contains the definition of a hash table\nthat maps SQL language keywords (ex: \"CREATE\", \"SELECT\", \"INDEX\", etc.) into\nthe numeric codes used by the parse.c parser.  The keywordhash.h file is\ngenerated by a C-language program at tool mkkeywordhash.c.\n\nThe **pragma.h** header file contains various definitions used to parse\nand implement the PRAGMA statements.  The header is generated by a\nscript **tool/mkpragmatab.tcl**. If you want to add a new PRAGMA, edit\nthe **tool/mkpragmatab.tcl** file to insert the information needed by the\nparser for your new PRAGMA, then run the script to regenerate the\n**pragma.h** header file.\n\n### The Amalgamation\n\nAll of the individual C source code and header files (both manually-edited\nand automatically-generated) can be combined into a single big source file\n**sqlite3.c** called \"the amalgamation\".  The amalgamation is the recommended\nway of using SQLite in a larger application.  Combining all individual\nsource code files into a single big source code file allows the C compiler\nto perform more cross-procedure analysis and generate better code.  SQLite\nruns about 5% faster when compiled from the amalgamation versus when compiled\nfrom individual source files.\n\nThe amalgamation is generated from the tool/mksqlite3c.tcl Tcl script.\nFirst, all of the individual source files must be gathered into the tsrc/\nsubdirectory (using the equivalent of \"make target_source\") then the\ntool/mksqlite3c.tcl script is run to copy them all together in just the\nright order while resolving internal \"#include\" references.\n\nThe amalgamation source file is more than 200K lines long.  Some symbolic\ndebuggers (most notably MSVC) are unable to deal with files longer than 64K\nlines.  To work around this, a separate Tcl script, tool/split-sqlite3c.tcl,\ncan be run on the amalgamation to break it up into a single small C file\ncalled **sqlite3-all.c** that does #include on about seven other files\nnamed **sqlite3-1.c**, **sqlite3-2.c**, ..., **sqlite3-7.c**.  In this way,\nall of the source code is contained within a single translation unit so\nthat the compiler can do extra cross-procedure optimization, but no\nindividual source file exceeds 32K lines in length.\n\n## How It All Fits Together\n\nSQLite is modular in design.\nSee the [architectural description](https://sqlite.org/arch.html)\nfor details. Other documents that are useful in\nhelping to understand how SQLite works include the\n[file format](https://sqlite.org/fileformat2.html) description,\nthe [virtual machine](https://sqlite.org/opcode.html) that runs\nprepared statements, the description of\n[how transactions work](https://sqlite.org/atomiccommit.html), and\nthe [overview of the query planner](https://sqlite.org/optoverview.html).\n\nDecades of effort have gone into optimizing SQLite, both\nfor small size and high performance.  And optimizations tend to result in\ncomplex code.  So there is a lot of complexity in the current SQLite\nimplementation.  It will not be the easiest library in the world to hack.\n\n### Key source code files\n\n  *  **sqlite.h.in** - This file defines the public interface to the SQLite\n     library.  Readers will need to be familiar with this interface before\n     trying to understand how the library works internally.  This file is\n     really a template that is transformed into the \"sqlite3.h\" deliverable\n     using a script invoked by the makefile.\n\n  *  **sqliteInt.h** - this header file defines many of the data objects\n     used internally by SQLite.  In addition to \"sqliteInt.h\", some\n     subsystems inside of sQLite have their own header files.  These internal\n     interfaces are not for use by applications.  They can and do change\n     from one release of SQLite to the next.\n\n  *  **parse.y** - This file describes the LALR(1) grammar that SQLite uses\n     to parse SQL statements, and the actions that are taken at each step\n     in the parsing process.  The file is processed by the\n     [Lemon Parser Generator](./doc/lemon.html) to produce the actual C code\n     used for parsing.\n\n  *  **vdbe.c** - This file implements the virtual machine that runs\n     prepared statements.  There are various helper files whose names\n     begin with \"vdbe\".  The VDBE has access to the vdbeInt.h header file\n     which defines internal data objects.  The rest of SQLite interacts\n     with the VDBE through an interface defined by vdbe.h.\n\n  *  **where.c** - This file (together with its helper files named\n     by \"where*.c\") analyzes the WHERE clause and generates\n     virtual machine code to run queries efficiently.  This file is\n     sometimes called the \"query optimizer\".  It has its own private\n     header file, whereInt.h, that defines data objects used internally.\n\n  *  **btree.c** - This file contains the implementation of the B-Tree\n     storage engine used by SQLite.  The interface to the rest of the system\n     is defined by \"btree.h\".  The \"btreeInt.h\" header defines objects\n     used internally by btree.c and not published to the rest of the system.\n\n  *  **pager.c** - This file contains the \"pager\" implementation, the\n     module that implements transactions.  The \"pager.h\" header file\n     defines the interface between pager.c and the rest of the system.\n\n  *  **os_unix.c** and **os_win.c** - These two files implement the interface\n     between SQLite and the underlying operating system using the run-time\n     pluggable VFS interface.\n\n  *  **shell.c.in** - This file is not part of the core SQLite library.  This\n     is the file that, when linked against sqlite3.a, generates the\n     \"sqlite3.exe\" command-line shell.  The \"shell.c.in\" file is transformed\n     into \"shell.c\" as part of the build process.\n\n  *  **tclsqlite.c** - This file implements the Tcl bindings for SQLite.  It\n     is not part of the core SQLite library.  But as most of the tests in this\n     repository are written in Tcl, the Tcl language bindings are important.\n\n  *  **test\\*.c** - Files in the src/ folder that begin with \"test\" go into\n     building the \"testfixture.exe\" program.  The testfixture.exe program is\n     an enhanced Tcl shell.  The testfixture.exe program runs scripts in the\n     test/ folder to validate the core SQLite code.  The testfixture program\n     (and some other test programs too) is built and run when you type\n     \"make test\".\n\n  *  **VERSION**, **manifest**, and **manifest.uuid** - These files define\n     the current SQLite version number.  The \"VERSION\" file is human generated,\n     but the \"manifest\" and \"manifest.uuid\" files are automatically generated\n     by the [Fossil version control system](https://fossil-scm.org/).\n\nThere are many other source files.  Each has a succinct header comment that\ndescribes its purpose and role within the larger system.\n\n<a name=\"vauth\"></a>\n## Verifying Code Authenticity\n\nThe `manifest` file at the root directory of the source tree\ncontains either a SHA3-256 hash or a SHA1 hash\nfor every source file in the repository.\nThe name of the version of the entire source tree is just the\nSHA3-256 hash of the `manifest` file itself, possibly with the\nlast line of that file omitted if the last line begins with\n\"`# Remove this line`\".\nThe `manifest.uuid` file should contain the SHA3-256 hash of the\n`manifest` file. If all of the above hash comparisons are correct, then\nyou can be confident that your source tree is authentic and unadulterated.\nDetails on the format for the `manifest` files are available\n[on the Fossil website](https://fossil-scm.org/home/doc/trunk/www/fileformat.wiki#manifest).\n\nThe process of checking source code authenticity is automated by the \nmakefile:\n\n>   make verify-source\n\nOr on windows:\n\n>   nmake /f Makefile.msc verify-source\n\nUsing the makefile to verify source integrity is good for detecting\naccidental changes to the source tree, but malicious changes could be\nhidden by also modifying the makefiles.\n\n## Contacts\n\nThe main SQLite website is [https://sqlite.org/](https://sqlite.org/)\nwith geographically distributed backups at\n[https://www2.sqlite.org/](https://www2.sqlite.org) and\n[https://www3.sqlite.org/](https://www3.sqlite.org).\n\nContact the SQLite developers through the\n[SQLite Forum](https://sqlite.org/forum/).  In an emergency, you\ncan send private email to the lead developer at drh at sqlite dot org.\n",
      "stars_today": 4
    },
    {
      "id": 57452143,
      "name": "android",
      "full_name": "bitwarden/android",
      "description": "Bitwarden mobile apps (Password Manager and Authenticator) for Android.",
      "html_url": "https://github.com/bitwarden/android",
      "stars": 8260,
      "forks": 928,
      "language": "Kotlin",
      "topics": [
        "android",
        "bitwarden",
        "compose",
        "jetpack",
        "kotlin"
      ],
      "created_at": "2016-04-30T16:43:17Z",
      "updated_at": "2026-01-14T16:46:01Z",
      "pushed_at": "2026-01-14T22:39:47Z",
      "open_issues": 146,
      "owner": {
        "login": "bitwarden",
        "avatar_url": "https://avatars.githubusercontent.com/u/15990069?v=4"
      },
      "readme": "# Bitwarden Android\n\n## Contents\n\n- [Compatibility](#compatibility)\n- [Setup](#setup)\n- [Dependencies](#dependencies)\n\n## Compatibility\n\n- **Minimum SDK**: 29 (Android 10)\n- **Target SDK**: 36 (Android 16)\n- **Device Types Supported**: Phone and Tablet\n- **Orientations Supported**: Portrait and Landscape\n\n## Setup\n\n1. Clone the repository:\n\n    ```sh\n    $ git clone https://github.com/bitwarden/android\n    ```\n\n2. Create a `user.properties` file in the root directory of the project and add the following properties:\n\n    - `gitHubToken`: A \"classic\" Github Personal Access Token (PAT) with the `read:packages` scope (ex: `gitHubToken=gph_xx...xx`). These can be generated by going to the [Github tokens page](https://github.com/settings/tokens). See [the Github Packages user documentation concerning authentication](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-gradle-registry#authenticating-to-github-packages) for more details.\n    - `localSdk`: A boolean value to determine if the SDK should be loaded from the local maven artifactory (ex: `localSdk=true`). This is particularly useful when developing new SDK capabilities. Review [Linking SDK to clients](https://contributing.bitwarden.com/getting-started/sdk/#linking-the-sdk-to-clients) for more details.\n\n3. Setup the code style formatter:\n\n    All code must follow the guidelines described in the [Code Style Guidelines document](docs/STYLE_AND_BEST_PRACTICES.md). To aid in adhering to these rules, all contributors should apply `docs/bitwarden-style.xml` as their code style scheme. In IntelliJ / Android Studio:\n\n    - Navigate to `Preferences > Editor > Code Style`.\n    - Hit the `Manage` button next to `Scheme`.\n    - Select `Import`.\n    - Find the `bitwarden-style.xml` file in the project's `docs/` directory.\n    - Import \"from\" `BitwardenStyle` \"to\" `BitwardenStyle`.\n    - Hit `Apply` and `OK` to save the changes and exit Preferences.\n\n    Note that in some cases you may need to restart Android Studio for the changes to take effect.\n\n    All code should be formatted before submitting a pull request. This can be done manually but it can also be helpful to create a macro with a custom keyboard binding to auto-format when saving. In Android Studio on OS X:\n\n    - Select `Edit > Macros > Start Macro Recording`\n    - Select `Code > Optimize Imports`\n    - Select `Code > Reformat Code`\n    - Select `File > Save All`\n    - Select `Edit > Macros > Stop Macro Recording`\n\n    This can then be mapped to a set of keys by navigating to `Android Studio > Preferences` and editing the macro under `Keymap` (ex : shift + command + s).\n\n    Please avoid mixing formatting and logical changes in the same commit/PR. When possible, fix any large formatting issues in a separate PR before opening one to make logical changes to the same code. This helps others focus on the meaningful code changes when reviewing the code.\n\n4. Setup JDK `Version` `21`:\n\n    - Navigate to `Preferences > Build, Execution, Deployment > Build Tools > Gradle`.\n    - Hit the selected Gradle JDK next to `Gradle JDK:`.\n    - Select a `21.x` version or hit `Download JDK...` if not present.\n    - Select `Version` `21`.\n    - Select your preferred `Vendor`.\n    - Hit `Download`.\n    - Hit `Apply`.\n\n5. Setup `detekt` pre-commit hook (optional):\n\nRun the following script from the root of the repository to install the hook. This will overwrite any existing pre-commit hook if present.\n\n```shell\necho \"Writing detekt pre-commit hook...\"\ncat << 'EOL' > .git/hooks/pre-commit\n#!/usr/bin/env bash\n\necho \"Running detekt check...\"\nOUTPUT=\"/tmp/detekt-$(date +%s)\"\n./gradlew -Pprecommit=true detekt > $OUTPUT\nEXIT_CODE=$?\nif [ $EXIT_CODE -ne 0 ]; then\n  cat $OUTPUT\n  rm $OUTPUT\n  echo \"***********************************************\"\n  echo \"                 detekt failed                 \"\n  echo \" Please fix the above issues before committing \"\n  echo \"***********************************************\"\n  exit $EXIT_CODE\nfi\nrm $OUTPUT\nEOL\necho \"detekt pre-commit hook written to .git/hooks/pre-commit\"\necho \"Making the hook executable\"\nchmod +x .git/hooks/pre-commit\n\necho \"detekt pre-commit hook installed successfully to .git/hooks/pre-commit\"\n```\n\n## Dependencies\n\n### Application Dependencies\n\nThe following is a list of all third-party dependencies included as part of the application beyond the standard Android SDK.\n\n- **AndroidX Activity**\n    - https://developer.android.com/jetpack/androidx/releases/activity\n    - Purpose: Allows access composable APIs built on top of Activity.\n    - License: Apache 2.0\n\n- **AndroidX Appcompat**\n    - https://developer.android.com/jetpack/androidx/releases/appcompat\n    - Purpose: Allows access to new APIs on older API versions.\n    - License: Apache 2.0\n\n- **AndroidX Autofill**\n    - https://developer.android.com/jetpack/androidx/releases/autofill\n    - Purpose: Allows access to tools for building inline autofill UI.\n    - License: Apache 2.0\n\n- **AndroidX Biometrics**\n    - https://developer.android.com/jetpack/androidx/releases/biometric\n    - Purpose: Authenticate with biometrics or device credentials.\n    - License: Apache 2.0\n\n- **AndroidX Browser**\n    - https://developer.android.com/jetpack/androidx/releases/browser\n    - Purpose: Displays webpages with the user's default browser.\n    - License: Apache 2.0\n\n- **AndroidX Camera**\n    - https://developer.android.com/jetpack/androidx/releases/camera\n    - Purpose: Display and capture images for barcode scanning.\n    - License: Apache 2.0\n\n- **AndroidX Compose**\n    - https://developer.android.com/jetpack/androidx/releases/compose\n    - Purpose: A Kotlin-based declarative UI framework.\n    - License: Apache 2.0\n\n- **AndroidX Core**\n    - https://developer.android.com/jetpack/androidx/releases/core\n    - Purpose: Backwards compatible platform features and APIs.\n    - License: Apache 2.0\n\n- **AndroidX Credentials**\n    - https://developer.android.com/jetpack/androidx/releases/credentials\n    - Purpose: Unified access to user's credentials.\n    - License: Apache 2.0\n\n- **AndroidX Lifecycle**\n    - https://developer.android.com/jetpack/androidx/releases/lifecycle\n    - Purpose: Lifecycle aware components and tooling.\n    - License: Apache 2.0\n\n- **AndroidX Navigation**\n    - https://developer.android.com/jetpack/androidx/releases/navigation\n    - Purpose: Provides a consistent API for navigating between Android components.\n    - License: Apache 2.0\n\n- **AndroidX Room**\n    - https://developer.android.com/jetpack/androidx/releases/room\n    - Purpose: A convenient SQLite-based persistence layer for Android.\n    - License: Apache 2.0\n\n- **AndroidX Security**\n    - https://developer.android.com/jetpack/androidx/releases/security\n    - Purpose: Safely manage keys and encrypt files and sharedpreferences.\n    - License: Apache 2.0\n\n- **AndroidX WorkManager**\n    - https://developer.android.com/jetpack/androidx/releases/work\n    - Purpose: The WorkManager is used to schedule deferrable, asynchronous tasks that must be run reliably.\n    - License: Apache 2.0\n\n- **Dagger Hilt**\n    - https://github.com/google/dagger\n    - Purpose: Dependency injection framework.\n    - License: Apache 2.0\n\n- **Glide**\n    - https://github.com/bumptech/glide\n    - Purpose: Image loading and caching.\n    - License: BSD, part MIT and Apache 2.0\n\n- **kotlinx.collections.immutable**\n    - https://github.com/Kotlin/kotlinx.collections.immutable\n    - Purpose: Immutable collection interfaces and implementation prototypes for Kotlin.\n    - License: Apache 2.0\n\n- **kotlinx.coroutines**\n    - https://github.com/Kotlin/kotlinx.coroutines\n    - Purpose: Kotlin coroutines library for asynchronous and reactive code.\n    - License: Apache 2.0\n\n- **kotlinx.serialization**\n    - https://github.com/Kotlin/kotlinx.serialization/\n    - Purpose: JSON serialization library for Kotlin.\n    - License: Apache 2.0\n\n- **OkHttp 3**\n    - https://github.com/square/okhttp\n    - Purpose: An HTTP client used by the library to intercept and log traffic.\n    - License: Apache 2.0\n\n- **Retrofit 2**\n    - https://github.com/square/retrofit\n    - Purpose: A networking layer interface.\n    - License: Apache 2.0\n\n- **Timber**\n    - https://github.com/JakeWharton/timber\n    - Purpose: Extensible logging library for Android.\n    - License: Apache 2.0\n\n- **ZXing**\n    - https://github.com/zxing/zxing\n    - Purpose: Barcode scanning and generation.\n    - License: Apache 2.0\n\nThe following is an additional list of third-party dependencies that are only included in the non-F-Droid build variants of the application.\n\n- **Firebase Cloud Messaging**\n    - https://github.com/firebase/firebase-android-sdk\n    - Purpose: Allows for push notification support.\n    - License: Apache 2.0\n\n- **Firebase Crashlytics**\n    - https://github.com/firebase/firebase-android-sdk\n    - Purpose: SDK for crash and non-fatal error reporting.\n    - License: Apache 2.0\n\n- **Google Play Reviews**\n    - https://developer.android.com/reference/com/google/android/play/core/release-notes\n    - Purpose: On standard builds provide an interface to add a review for the password manager application in Google Play.\n    - License: Apache 2.0\n\n### Development Environment Dependencies\n\nThe following is a list of additional third-party dependencies used as part of the local development environment. This includes test-related artifacts as well as tools related to code quality and linting. These are not present in the final packaged application.\n\n- **detekt**\n    - https://github.com/detekt/detekt\n    - Purpose: A static code analysis tool for the Kotlin programming language.\n    - License: Apache 2.0\n\n- **JUnit 5**\n    - https://github.com/junit-team/junit5\n    - Purpose: Unit Testing framework for testing application code.\n    - License: Eclipse Public License 2.0\n\n- **MockK**\n    - https://github.com/mockk/mockk\n    - Purpose: Kotlin-friendly mocking library.\n    - License: Apache 2.0\n\n- **Robolectric**\n    - https://github.com/robolectric/robolectric\n    - Purpose: A unit testing framework for code directly depending on the Android framework.\n    - License: MIT\n\n- **Turbine**\n    - https://github.com/cashapp/turbine\n    - Purpose: A small testing library for kotlinx.coroutine's Flow.\n    - License: Apache 2.0\n\n### CI/CD Dependencies\n\nThe following is a list of additional third-party dependencies used as part of the CI/CD workflows. These are not present in the final packaged application.\n\n- **Fastlane**\n    - https://fastlane.tools/\n    - Purpose: Automates building, signing, and distributing applications.\n    - License: MIT\n\n- **Kover**\n    - https://github.com/Kotlin/kotlinx-kover\n    - Purpose: Kotlin code coverage toolset.\n    - License: Apache 2.0\n",
      "stars_today": 4
    },
    {
      "id": 46153892,
      "name": "nccl",
      "full_name": "NVIDIA/nccl",
      "description": "Optimized primitives for collective multi-GPU communication",
      "html_url": "https://github.com/NVIDIA/nccl",
      "stars": 4382,
      "forks": 1111,
      "language": "C++",
      "topics": [
        "communications",
        "cpp",
        "cuda",
        "deep-learning",
        "gpu",
        "nvidia"
      ],
      "created_at": "2015-11-14T00:12:04Z",
      "updated_at": "2026-01-14T15:09:55Z",
      "pushed_at": "2026-01-09T19:11:10Z",
      "open_issues": 270,
      "owner": {
        "login": "NVIDIA",
        "avatar_url": "https://avatars.githubusercontent.com/u/1728152?v=4"
      },
      "readme": "# NCCL\n\nOptimized primitives for inter-GPU communication.\n\n## Introduction\n\nNCCL (pronounced \"Nickel\") is a stand-alone library of standard communication routines for GPUs, implementing all-reduce, all-gather, reduce, broadcast, reduce-scatter, as well as any send/receive based communication pattern. It has been optimized to achieve high bandwidth on platforms using PCIe, NVLink, NVswitch, as well as networking using InfiniBand Verbs or TCP/IP sockets. NCCL supports an arbitrary number of GPUs installed in a single node or across multiple nodes, and can be used in either single- or multi-process (e.g., MPI) applications.\n\nFor more information on NCCL usage, please refer to the [NCCL documentation](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/index.html).\n\n## Build\n\nNote: the official and tested builds of NCCL can be downloaded from: https://developer.nvidia.com/nccl. You can skip the following build steps if you choose to use the official builds.\n\nTo build the library :\n\n```shell\n$ cd nccl\n$ make -j src.build\n```\n\nIf CUDA is not installed in the default /usr/local/cuda path, you can define the CUDA path with :\n\n```shell\n$ make src.build CUDA_HOME=<path to cuda install>\n```\n\nNCCL will be compiled and installed in `build/` unless `BUILDDIR` is set.\n\nBy default, NCCL is compiled for all supported architectures. To accelerate the compilation and reduce the binary size, consider redefining `NVCC_GENCODE` (defined in `makefiles/common.mk`) to only include the architecture of the target platform :\n```shell\n$ make -j src.build NVCC_GENCODE=\"-gencode=arch=compute_70,code=sm_70\"\n```\n\n## Install\n\nTo install NCCL on the system, create a package then install it as root.\n\nDebian/Ubuntu :\n```shell\n$ # Install tools to create debian packages\n$ sudo apt install build-essential devscripts debhelper fakeroot\n$ # Build NCCL deb package\n$ make pkg.debian.build\n$ ls build/pkg/deb/\n```\n\nRedHat/CentOS :\n```shell\n$ # Install tools to create rpm packages\n$ sudo yum install rpm-build rpmdevtools\n$ # Build NCCL rpm package\n$ make pkg.redhat.build\n$ ls build/pkg/rpm/\n```\n\nOS-agnostic tarball :\n```shell\n$ make pkg.txz.build\n$ ls build/pkg/txz/\n```\n\n## Tests\n\nTests for NCCL are maintained separately at https://github.com/nvidia/nccl-tests.\n\n```shell\n$ git clone https://github.com/NVIDIA/nccl-tests.git\n$ cd nccl-tests\n$ make\n$ ./build/all_reduce_perf -b 8 -e 256M -f 2 -g <ngpus>\n```\n\n## Copyright\n\nAll source code and accompanying documentation is copyright (c) 2015-2020, NVIDIA CORPORATION. All rights reserved.\n",
      "stars_today": 4
    },
    {
      "id": 633817517,
      "name": "pixi",
      "full_name": "prefix-dev/pixi",
      "description": "Package management made easy",
      "html_url": "https://github.com/prefix-dev/pixi",
      "stars": 6098,
      "forks": 404,
      "language": "Rust",
      "topics": [
        "conda",
        "conda-environment",
        "conda-packages",
        "package-management",
        "package-manager",
        "package-manager-tool",
        "python-virtual-environment",
        "rust",
        "rust-lang"
      ],
      "created_at": "2023-04-28T10:51:16Z",
      "updated_at": "2026-01-14T23:52:08Z",
      "pushed_at": "2026-01-14T22:10:59Z",
      "open_issues": 566,
      "owner": {
        "login": "prefix-dev",
        "avatar_url": "https://avatars.githubusercontent.com/u/111356225?v=4"
      },
      "readme": "<h1>\n  <a href=\"https://github.com/prefix-dev/pixi/\">\n    <picture>\n      <source srcset=\"https://github.com/user-attachments/assets/fb67afa5-1c2a-4f47-9b8e-d60648557bfc\" type=\"image/png\">\n      <source srcset=\"https://github.com/user-attachments/assets/fa2e98c2-0913-4098-9579-8f2efff7f814\" type=\"image/webp\">\n      <img src=\"https://github.com/user-attachments/assets/fb67afa5-1c2a-4f47-9b8e-d60648557bfc\" alt=\"banner\">\n    </picture>\n  </a>\n</h1>\n\n<h1 align=\"center\">\n\n![License][license-badge]\n[![Project Chat][chat-badge]][chat-url]\n[![Pixi Badge][pixi-badge]][pixi-url]\n\n\n[license-badge]: https://img.shields.io/badge/license-BSD--3--Clause-blue?style=flat-square\n[chat-badge]: https://img.shields.io/discord/1082332781146800168.svg?label=&logo=discord&logoColor=ffffff&color=7389D8&labelColor=6A7EC2&style=flat-square\n[chat-url]: https://discord.gg/kKV8ZxyzY4\n[pixi-badge]:https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/prefix-dev/pixi/main/assets/badge/v0.json&style=flat-square\n[pixi-url]: https://pixi.sh\n\n</h1>\n\n# Pixi: Package Management Made Easy\n\n## Overview\n\n`pixi` is a cross-platform, multi-language package manager and workflow tool built on the foundation of the conda ecosystem. It provides developers with an exceptional experience similar to popular package managers like [`cargo`](https://doc.rust-lang.org/cargo/) or [`npm`](https://docs.npmjs.com), but for any language.\n\nDeveloped with â¤ï¸ at [prefix.dev](https://prefix.dev).\n[![Real-time pixi_demo](https://github.com/prefix-dev/pixi/assets/12893423/0fc8f8c8-ac13-4c14-891b-dc613f25475b)](https://asciinema.org/a/636482)\n\n## Highlights\n\n- Supports **multiple languages** including Python, C++, and R using Conda packages. You can find available packages on [prefix.dev](https://prefix.dev).\n- Compatible with all major operating systems: Linux, Windows, macOS (including Apple Silicon).\n- Always includes an up-to-date [**lock file**](https://pixi.sh/latest/workspace/lockfile/).\n- Provides a clean and simple Cargo-like **command-line interface**.\n- Allows you to install tools **per-project** or **system-wide**.\n- Entirely written in **Rust** and built on top of the **[rattler](https://github.com/conda/rattler)** library.\n\n## Getting Started\n\n- âš¡ [Installation](#installation)\n- âš™ï¸ [Examples](/examples)\n- ğŸ“š [Documentation](https://pixi.sh/)\n- ğŸ˜ [Contributing](#contributing)\n- ğŸ”¨ [Built using Pixi](#built-using-pixi)\n- ğŸš€ [GitHub Action](https://github.com/prefix-dev/setup-pixi)\n\n## Status\n\nPixi is ready for production!\nWe are working hard to keep file-format changes compatible with the previous\nversions so that you can rely on Pixi with peace of mind.\n\nSome notable features we envision for upcoming releases are:\n\n- **Build and publish** your project as a Conda package.\n- Support for **dependencies from source**.\n- More powerful \"global installation\" of packages towards a deterministic setup of global packages on multiple machines.\n\n## Installation\n\n`pixi` can be installed on macOS, Linux, and Windows. The provided scripts will automatically download the latest version of `pixi`, extract it, and move the `pixi` binary to `~/.pixi/bin`. If this directory does not exist, the script will create it.\n\n### macOS and Linux\n\nTo install Pixi on macOS and Linux, open a terminal and run the following command:\n\n```bash\ncurl -fsSL https://pixi.sh/install.sh | sh\n# or with brew\nbrew install pixi\n```\n\nThe script will also update your `~/.bashrc` to include `~/.pixi/bin` in your `PATH`, allowing you to invoke the `pixi` command from anywhere.\nYou might need to restart your terminal or source your shell for the changes to take effect.\n\nStarting with macOS Catalina [zsh is the default login shell and interactive shell](https://support.apple.com/en-us/102360). Therefore, you might want to use `zsh` instead of `bash` in the install command:\n\n```zsh\ncurl -fsSL https://pixi.sh/install.sh | zsh\n```\n\nThe script will also update your `~/.zshrc` to include `~/.pixi/bin` in your `PATH`, allowing you to invoke the `pixi` command from anywhere.\n\n### Windows\n\nTo install Pixi on Windows, open a PowerShell terminal (you may need to run it as an administrator) and run the following command:\n\n```powershell\npowershell -ExecutionPolicy ByPass -c \"irm -useb https://pixi.sh/install.ps1 | iex\"\n```\nChanging the [execution policy](https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_execution_policies?view=powershell-7.4#powershell-execution-policies) allows running a script from the internet.\nCheck the script you would be running with:\n```powershell\npowershell -c \"irm -useb https://pixi.sh/install.ps1 | more\"\n```\n\nThe script will inform you once the installation is successful and add the `~/.pixi/bin` directory to your `PATH`, which will allow you to run the `pixi` command from any location.\nOr with `winget`\n\n```shell\nwinget install prefix-dev.pixi\n```\n\n### Autocompletion\n\nTo get autocompletion follow the instructions for your shell.\nAfterwards, restart the shell or source the shell config file.\n\n#### Bash (default on most Linux systems)\n\nAdd the following to the end of `~/.bashrc`:\n\n```bash\n# ~/.bashrc\n\neval \"$(pixi completion --shell bash)\"\n```\n#### Zsh (default on macOS)\n\nAdd the following to the end of `~/.zshrc`:\n\n\n```zsh\n# ~/.zshrc\n\neval \"$(pixi completion --shell zsh)\"\n```\n\n#### PowerShell (pre-installed on all Windows systems)\n\nAdd the following to the end of `Microsoft.PowerShell_profile.ps1`.\nYou can check the location of this file by querying the `$PROFILE` variable in PowerShell.\nTypically the path is `~\\Documents\\PowerShell\\Microsoft.PowerShell_profile.ps1` or\n`~/.config/powershell/Microsoft.PowerShell_profile.ps1` on -Nix.\n\n```pwsh\n(& pixi completion --shell powershell) | Out-String | Invoke-Expression\n```\n\n#### Fish\n\nAdd the following to the end of `~/.config/fish/config.fish`:\n\n```fish\n# ~/.config/fish/config.fish\n\npixi completion --shell fish | source\n```\n\n#### Nushell\n\nAdd the following to your Nushell config file (find it by running `$nu.config-path` in Nushell):\n\n```nushell\nmkdir $\"($nu.data-dir)/vendor/autoload\"\npixi completion --shell nushell | save --force $\"($nu.data-dir)/vendor/autoload/pixi-completions.nu\"\n```\n\n#### Elvish\n\nAdd the following to the end of `~/.elvish/rc.elv`:\n\n```elv\n# ~/.elvish/rc.elv\n\neval (pixi completion --shell elvish | slurp)\n```\n\n### Distro Packages\n\n[![Packaging status](https://repology.org/badge/vertical-allrepos/pixi.svg)](https://repology.org/project/pixi/versions)\n\n#### Arch Linux\n\nYou can install `pixi` from the [extra repository](https://archlinux.org/packages/extra/x86_64/pixi/) using [pacman](https://wiki.archlinux.org/title/Pacman):\n\n```shell\npacman -S pixi\n```\n\n#### Alpine Linux\n\n`pixi` is available for [Alpine Edge](https://pkgs.alpinelinux.org/packages?name=pixi&branch=edge). It can be installed via [apk](https://wiki.alpinelinux.org/wiki/Alpine_Package_Keeper) after enabling the [testing repository](https://wiki.alpinelinux.org/wiki/Repositories).\n\n```shell\napk add pixi\n```\n\n## Build/install from source\n\n`pixi` is 100% written in Rust and therefore it can be installed, built and tested with cargo.\nTo start using `pixi` from a source build run:\n\n```shell\ncargo install --locked --git https://github.com/prefix-dev/pixi.git pixi\n```\n\nWe don't publish to `crates.io` anymore, so you need to install it from the repository.\nThe reason for this is that we depend on some unpublished crates which disallows us to publish to `crates.io`.\n\nor when you want to make changes use:\n\n```shell\ncargo build\ncargo test\n```\n\nIf you have any issues building because of the dependency on `rattler` checkout\nit's [compile steps](https://github.com/conda/rattler/tree/main#give-it-a-try)\n\n## Uninstall\n\nTo uninstall, the Pixi binary should be removed.\nDelete `pixi` from the `$PIXI_DIR` which is default to `~/.pixi/bin/pixi`\n\nSo on Linux its:\n\n```shell\nrm ~/.pixi/bin/pixi\n```\n\nand on Windows:\n\n```shell\n$PIXI_BIN = \"$Env:LocalAppData\\pixi\\bin\\pixi\"; Remove-Item -Path $PIXI_BIN\n```\n\nAfter this command you can still use the tools you installed with `pixi`.\nTo remove these as well just remove the whole `~/.pixi` directory and remove the directory from your path.\n\n# Usage\n\nThe cli looks as follows:\n\n```bash\nâœ pixi\nPixi [version 0.59.0] - Developer Workflow and Environment Management for Multi-Platform, Language-Agnostic\nWorkspaces.\n\nPixi is a versatile developer workflow tool designed to streamline the management of your workspace's dependencies,\ntasks, and environments.\nBuilt on top of the Conda ecosystem, Pixi offers seamless integration with the PyPI ecosystem.\n\nBasic Usage:\n    Initialize pixi for a workspace:\n    $ pixi init\n    $ pixi add python numpy pytest\n\n    Run a task:\n    $ pixi task add test 'pytest -s'\n    $ pixi run test\n\nFound a Bug or Have a Feature Request?\nOpen an issue at: https://github.com/prefix-dev/pixi/issues\n\nNeed Help?\nAsk a question on the Prefix Discord server: https://discord.gg/kKV8ZxyzY4\n\nFor more information, see the documentation at: https://pixi.sh\n\nUsage: pixi [OPTIONS] [COMMAND]\n\nCommands:\n  add         Adds dependencies to the workspace [aliases: a]\n  auth        Login to prefix.dev or anaconda.org servers to access private channels\n  build       Workspace configuration\n  clean       Cleanup the environments\n  completion  Generates a completion script for a shell\n  config      Configuration management\n  exec        Run a command and install it in a temporary environment [aliases: x]\n  global      Subcommand for global package management actions [aliases: g]\n  info        Information about the system, workspace and environments for the current machine\n  init        Creates a new workspace\n  import      Imports a file into an environment in an existing workspace.\n  install     Install an environment, both updating the lockfile and installing the environment [aliases: i]\n  list        List the packages of the current workspace [aliases: ls]\n  lock        Solve environment and update the lock file without installing the environments\n  reinstall   Re-install an environment, both updating the lockfile and re-installing the environment\n  remove      Removes dependencies from the workspace [aliases: rm]\n  run         Runs task in the pixi environment [aliases: r]\n  search      Search a conda package\n  shell       Start a shell in a pixi environment, run `exit` to leave the shell [aliases: s]\n  shell-hook  Print the pixi environment activation script\n  task        Interact with tasks in the workspace\n  tree        Show a tree of workspace dependencies [aliases: t]\n  update      The `update` command checks if there are newer versions of the dependencies and updates the `pixi.lock`\n              file and environments accordingly\n  upgrade     Checks if there are newer versions of the dependencies and upgrades them in the lockfile and manifest\n              file\n  upload      Upload a conda package\n  workspace   Modify the workspace configuration file through the command line\n  help        Print this message or the help of the given subcommand(s)\n\nOptions:\n  -V, --version  Print version\n\nGlobal Options:\n  -h, --help           Display help information\n  -v, --verbose...     Increase logging verbosity (-v for warnings, -vv for info, -vvv for debug, -vvvv for trace)\n  -q, --quiet...       Decrease logging verbosity (quiet mode)\n      --color <COLOR>  Whether the log needs to be colored [env: PIXI_COLOR=] [default: auto] [possible values:\n                       always, never, auto]\n      --no-progress    Hide all progress bars, always turned on if stderr is not a terminal [env: PIXI_NO_PROGRESS=]\n      --list           List all installed commands (built-in and extensions)\n```\n\n## Creating a Pixi workspace\n\nInitialize a new workspace and navigate to the workspace directory\n\n```\npixi init myworkspace\ncd myworkspace\n```\n\nAdd the dependencies you want to use\n\n```\npixi add cowpy\n```\n\nRun the installed package in its environment\n\n```bash\npixi run cowpy \"Thanks for using pixi\"\n```\n\nActivate a shell in the environment\n\n```shell\npixi shell\ncowpy \"Thanks for using pixi\"\nexit\n```\n\nCheck out https://pixi.sh/dev/first_workspace/ for a more detailed introduction to workspaces.\n\n## Installing a conda package globally\n\nYou can also globally install conda packages into their own environment.\nThis behavior is similar to [`pipx`](https://github.com/pypa/pipx) or [`condax`](https://github.com/mariusvniekerk/condax).\n\n```bash\npixi global install cowpy\n```\n\n## Use in GitHub Actions\n\nYou can use Pixi in GitHub Actions to install dependencies and run commands.\nIt supports automatic caching of your environments.\n\n```yml\n- uses: prefix-dev/setup-pixi@v0.8.1\n- run: pixi exec cowpy \"Thanks for using pixi\"\n```\n\nSee the [documentation](https://pixi.sh/latest/advanced/github_actions) for more details.\n\n<a name=\"contributing\"></a>\n\n## Contributing ğŸ˜\n\nWe would absolutely love for you to contribute to Pixi!\nWhether you want to start an issue, fix a bug you encountered, or suggest an\nimprovement, every contribution is greatly appreciated.\n\nIf you're just getting started with our project or stepping into the Rust\necosystem for the first time, we've got your back!\nWe recommend beginning with issues labeled as `good first issue`.\nThese are carefully chosen tasks that provide a smooth entry point into\ncontributing.These issues are typically more straightforward and are a great way\nto get familiar with the project.\n\nGot questions or ideas, or just want to chat? Join our lively conversations on\nDiscord.\nWe're very active and would be happy to welcome you to our\ncommunity. [Join our discord server today!][chat-url]\n\n<a name=\"pixibuilt\"></a>\n\n## Built using Pixi\n\nTo see what's being built with `pixi` check out the [Community](/docs/misc/Community.md) page.\n",
      "stars_today": 4
    },
    {
      "id": 805199381,
      "name": "YuyanIme",
      "full_name": "gurecn/YuyanIme",
      "description": "è¯­ç‡•è¾“å…¥æ³•-ä¸€æ¬¾åŸºäºRimeå®šåˆ¶å¼€å‘çš„ä¹é”®ã€å…¨æ‹¼ã€åŒæ‹¼ã€æ‰‹å†™ã€ç«æ˜Ÿæ–‡ç­‰æ–¹æ¡ˆã€æ”¯æŒæ‚¬æµ®ã€å•æ‰‹ã€æ•°å­—è¡Œç­‰é”®ç›˜æ¨¡å¼çš„ä¸­æ–‡è¾“å…¥æ³•",
      "html_url": "https://github.com/gurecn/YuyanIme",
      "stars": 3000,
      "forks": 160,
      "language": "Kotlin",
      "topics": [
        "double-pinyin",
        "ime",
        "input-method",
        "keyboard",
        "pinyin",
        "qwerty",
        "rime",
        "t9"
      ],
      "created_at": "2024-05-24T04:53:19Z",
      "updated_at": "2026-01-15T00:55:53Z",
      "pushed_at": "2025-12-22T09:27:43Z",
      "open_issues": 346,
      "owner": {
        "login": "gurecn",
        "avatar_url": "https://avatars.githubusercontent.com/u/8704526?v=4"
      },
      "readme": "# è¯­ç‡•è¾“å…¥æ³•\né›¨ç‡•ä»¥å…¶æ•æ·ã€ä¼˜é›…çš„é£è¡Œå§¿æ€ï¼Œåœ¨é›¨å¤©ä¾ç„¶å¤Ÿé£ç¿”ã€ä¸ç•è‰°é™©ã€å‹‡å¾€ç›´å‰çš„é£ç¿”æ€åº¦ï¼Œè¢«èµ‹äºˆ**çµå·§ã€èªæ˜ã€ä¼¶ä¿ã€ç§¯æã€åˆ›æ–°**çš„å¯“æ„ã€‚è¯­è¨€æ˜¯äººç±»äº¤æµçš„åŸºæœ¬å·¥å…·ï¼Œæ˜¯æœ€é‡è¦çš„æ–‡åŒ–è½½ä½“ï¼Œè¾“å…¥æ–¹å¼æ˜¯è¯­è¨€äº¤æµå’Œä¿¡æ¯ä¼ é€’çš„é‡è¦ç¯èŠ‚ï¼Œä½¿è¯­è¨€äº¤æµå˜å¾—æ›´åŠ **é«˜æ•ˆã€ä¾¿æ·**ã€‚  \n[è¯­ç‡•è¾“å…¥æ³•](https://github.com/gurecn/YuyanIme)ç§‰æ‰¿è¿™äº›ç‰¹ç‚¹ï¼Œä»¥â€œ**æ˜“ç”¨ã€å¿«é€Ÿã€å‡†ç¡®**â€ä¸ºæ ¸å¿ƒç†å¿µï¼Œè¿½æ±‚æè‡´ã€å“è¶Šã€æµç•…çš„è¾“å…¥ä½“éªŒã€‚ åœ¨è®¾è®¡ä¸Šï¼Œè¯­ç‡•è¾“å…¥æ³•å€Ÿé‰´ä¸»æµçš„è°·æ­Œæ‹¼éŸ³ã€å¾®ä¿¡è¾“å…¥æ³•ç­‰ä¸»æµè¾“å…¥æ³•ç²¾åï¼Œè¿½æ±‚æ•´ä½“ç®€æ´å¤§æ–¹ï¼Œæ˜“äºä¸Šæ‰‹ã€‚æ”¯æŒå¤šç§è¾“å…¥æ–¹å¼ï¼Œè§„åˆ’åŒ…æ‹¬æ‹¼éŸ³ã€æ‰‹å†™ã€è¯­éŸ³ç­‰ï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·çš„è¾“å…¥éœ€æ±‚ã€‚æ”¯æŒä¸°å¯Œçš„ä¸ªæ€§åŒ–è®¾ç½®é€‰é¡¹ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½è¿›è¡Œè‡ªå®šä¹‰è®¾ç½®ï¼Œè®©è¾“å…¥æ›´åŠ ç¬¦åˆä¸ªäººä¹ æƒ¯ã€‚\n## å®‰è£…ä½¿ç”¨ï¼š\nåä¸ºåº”ç”¨å¸‚åœºï¼š[åº”ç”¨åœ°å€](https://appgallery.cloud.huawei.com/appDetail?pkgName=com.yuyan.pinyin.online.release)ï¼Œåº”ç”¨å®ï¼š[åº”ç”¨åœ°å€](https://sj.qq.com/appdetail/com.yuyan.pinyin.online.release)ï¼Œå·²ä¸Šæ¶è¯­ç‡•è¾“å…¥æ³•ï¼Œå¯ç›´æ¥æœç´¢å®‰è£…ã€‚\nä¹Ÿå¯ä»¥ç›´æ¥ç‚¹å‡»[Github Releases](https://github.com/gurecn/YuyanIme/releases)ï¼Œä¸‹è½½æœ€æ–°ç‰ˆæœ¬å®‰è£…åŒ…ç›´æ¥å®‰è£…ä½¿ç”¨ã€‚ \nå›½å†…è®¿é—®Githubæ…¢çš„è¯ï¼Œå¯ç‚¹å‡»[Gitee Releases](https://gitee.com/gurecn/YuyanIme/releases)ä¸‹è½½ã€‚\næ‰‹æœºæ‰«ç ä¸‹è½½åœ°å€ï¼š\n| Github                           | Gitee                          |\n|----------------------------------|--------------------------------|\n| ![github](./download/github.png) | ![gitee](./download/gitee.png) |\n\nä½¿ç”¨è¿‡ç¨‹ä¸­ä»»ä½•é—®é¢˜å¯ä»¥åˆ›å»ºissuesã€åº”ç”¨å†…åé¦ˆæˆ–é€šè¿‡é‚®ä»¶ç­‰æ–¹å¼åé¦ˆï¼Œæœ¬äººä¼šæ ¹æ®éœ€æ±‚åŠæ—¶ä¿®å¤ã€‚\n## è®¾è®¡åŸåˆ™ï¼š\n### çº¯è¾“å…¥åŠŸèƒ½ï¼Œä¸»æ‰“è½»å¿«ã€‚\nå–œæ¬¢ç®€æ´çš„æˆ‘çœ‹åˆ°ä¸€ä¸ªä¸ªæ‹¼éŸ³è¾“å…¥æ³•å·¥å…·è½¯ä»¶é€æ¸è¶‹å‘ç¹æ‚ï¼Œè½¯ä»¶å†…å„ç§çœ¼èŠ±ç¼­ä¹±çš„æ— ç”¨åŠŸèƒ½ä»¥åŠçƒ¦äººçš„å¹¿å‘Šè®©æˆ‘æ— æ³•å¿å—ã€‚  \n**å› æ­¤æˆ‘æƒ³è¦å®šåˆ¶å‡ºä¸€æ¬¾ç®€æ´ã€å®ç”¨ã€å¥½ç”¨çš„è¾“å…¥æ³•ï¼›**\n### æœ€å°ã€å¿…è¦çš„æƒé™åŸåˆ™ï¼Œæ›´å®‰å…¨ã€‚\nå½“å‰ä¸»æµè¾“å…¥æ³•è·å–å„ç±»éå¿…è¦æƒé™ï¼Œæ— è§†ç”¨æˆ·éšç§ï¼Œéšæ„ä¸Šä¼ ã€åˆ†æç”¨æˆ·æ•°æ®ã€‚è™½ç„¶å¤§æ•°æ®ä¸ä¼šåŒºåˆ«å¯¹å¾…ï¼Œä½†æˆ‘ä»ç„¶å¸Œæœ›è‡ªå·±çš„æ•°æ®åªåœ¨è‡ªå·±çš„æ‰‹æœºé‡Œï¼Œä¸è¦åœ¨æˆ‘ä¸çŸ¥æƒ…ã€æ— æ„è¯†çš„æƒ…å†µä¸‹ï¼ŒæŠŠæ‰€æœ‰æ•°æ®ä¸Šä¼ ã€‚    \n**è¯­ç‡•è¾“å…¥æ³•è°¨éµå¾ªå¿…è¦ã€æœ€å°åŒ–æƒé™ï¼Œåªä¸ºè¾“å…¥è€Œå­˜åœ¨ï¼Œçº¯å‡€ã€å®‰å…¨ã€æ›´é«˜æ•ˆã€‚**  \nè¯­ç‡•è¾“å…¥æ³•ä»…ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä¸ºè¾“å…¥æ³•å¼€å¯çš„`å‰ªè´´æ¿`ï¼ˆå‰ªè´´æ¿åŠŸèƒ½ï¼‰ã€`è®¾å¤‡è¿åŠ¨ä¸æ–¹å‘`ï¼ˆå±å¹•æ–¹å‘å˜æ›´ï¼‰ã€`åª’ä½“éŸ³æ§åˆ¶`ï¼ˆæŒ‰é”®éŸ³æ•ˆï¼‰ã€`æŒ¯åŠ¨`ï¼ˆæŒ‰é”®æŒ¯åŠ¨ï¼‰æƒé™ï¼Œä¸è·å–ç½‘ç»œã€å­˜å‚¨ã€ä½ç½®ã€è¾…åŠ©åŠŸèƒ½ç­‰å…¶ä»–æƒé™ï¼Œå®Œå…¨ç¦»çº¿ä¸ä¸Šä¼ äº‘ç«¯ï¼Œè¾“å…¥æ•°æ®ä¸é‡‡é›†ã€ä¸è®°å½•ï¼Œä¸è®¿é—®ä»»ä½•ä¸ªäººã€ç»ˆç«¯ã€ä½ç½®ã€å­˜å‚¨ç­‰ä¿¡æ¯ã€‚\n### åŸºäºRimeå¼•æ“ï¼Œä½†æ›´æ˜“ä¸Šæ‰‹ã€‚\nå½“å‰å¼€æ”¾çš„è¾“å…¥æ³•å¼•æ“ä¸­ï¼Œ[Rimeå¼•æ“](https://github.com/rime/librime)å·²ç»è¶‹å‘å®Œå–„ã€‚ç„¶åå¯¹äºå°ç™½ç”¨æˆ·æ¥è¯´ï¼Œä¸Šæ‰‹å´å¹¶ä¸å®¹æ˜“ï¼šå„ç§è¾“å…¥æ–¹æ¡ˆå®šåˆ¶åŠå…¼å®¹é—®é¢˜ï¼Œå„ç§é”®ç›˜çš„ç•Œé¢æ•ˆæœä¼˜åŒ–é—®é¢˜ã€‚  \n**å› æ­¤æˆ‘æƒ³è¦å®šåˆ¶å‡ºä¸€æ¬¾åŸºäºRimeå¼•æ“çš„å®‰è£…å³ç”¨ï¼Œå“ªæ€•æ²¡æ—¶é—´ç ”ç©¶ä¹Ÿèƒ½å¥½ç”¨çš„è¾“å…¥æ³•ï¼›**\n### è¾“å…¥æ¨¡å¼æ›´å®Œå–„ã€‚\næœ€æ—©æ¥è§¦å®‰å“å¹³å°çš„[åŒæ–‡è¾“å…¥æ³•](https://github.com/osfans)ï¼Œåé¢æ¥è§¦[å°ä¼é¹…è¾“å…¥æ³•](https://github.com/fcitx5-android/fcitx5-android)ï¼Œå‡é‡‡ç”¨Rimeæ–¹æ¡ˆè¿›è¡Œå®šåˆ¶ï¼Œåœ¨è¾“å…¥å±‚é¢å·²ç»æ»¡è¶³å¤§éƒ¨åˆ†éœ€æ±‚ã€‚ä½†æ˜¯å°ä¼é¹…è¾“å…¥æ³•ä¹å®«æ ¼é”®ç›˜ä¸æ”¯æŒï¼ŒåŒæ–‡è¾“å…¥æ³•å€™é€‰è¯é€‰æ‹©ä¸ä¾¿ä¸”æ— æ³•é€‰æ‹©æ‹¼éŸ³ç»„åˆï¼Œä½¿ç”¨èµ·æ¥ç¡®å®éœ€è¦å‹‡æ°”ã€‚  \nè¯­ç‡•è¾“å…¥æ³•å†…ç½®å¤šå¥—ä¼˜ç§€è¯åº“ï¼Œä¼˜åŒ–Rimeä¹å®«è¾“å…¥æ–¹æ¡ˆã€ä¹±åºè¾“å…¥æ–¹æ¡ˆï¼Œæ”¯æŒç»å¤§éƒ¨åˆ†è¾“å‡ºåœºæ™¯ï¼Œæå‡è¾“å…¥æ•ˆç‡ã€‚  \n**å› æ­¤æˆ‘æƒ³å®šåˆ¶å‡ºä¸€æ¬¾æ”¯æŒå¯¹å°ç™½ç”¨æˆ·æ¥è¯´ä½¿ç”¨æ›´æ™®åŠçš„ä¹å®«æ ¼ï¼ŒåŒæ—¶ç»“åˆå…¨é”®ã€åŒæ‹¼ã€æ‰‹å†™ã€è¯­éŸ³ç­‰å¤šç§æ–¹æ¡ˆçš„è¾“å…¥æ³•ã€‚**  \n### ä¸ªæ€§åŒ–å®šåˆ¶æ›´è´´å¿ƒã€‚\næ‰‹æœºå±å¹•è¶Šæ¥è¶Šå¤§ï¼Œä½†æ˜¯åœ¨èµ°è·¯æ—¶ï¼Œä¸€æ‰‹æä¸œè¥¿ï¼Œä¸€æ‰‹æ‰“å­—å›å¤æ¶ˆæ¯å¯¹æˆ‘æ¥è¯´æ˜¯ä¸ªå¤´ç–¼åœ°é—®é¢˜ï¼Œé€‰æ‹©å€™é€‰è¯å¤Ÿä¸åˆ°ã€é€‰æ‹©å‡ºé”™å±¡å±¡å‡ºç°ï¼Œå› æ­¤æˆ‘å®šåˆ¶äº†å•æ‰‹æ¨¡å¼ã€æ‚¬æµ®é”®ç›˜ã€‚  \nè¾“å…¥æ•°å­—è¦ä¹ˆåˆ‡æ¢åˆ°æ•°å­—é”®ç›˜ï¼Œè¦ä¹ˆé•¿æŒ‰æŒ‰é”®è¾“å…¥ï¼Œå¯¹è¾“å…¥æ¥è¯´éƒ½ä¸ä¾¿æ·ï¼Œå› æ­¤æˆ‘å®šåˆ¶äº†é”®ç›˜æ•°å­—è¡Œã€‚  \nå¤œé—´è¾“å…¥æ—¶ï¼Œå±å¹•åˆºçœ¼ï¼Œå› æ­¤æˆ‘å®šåˆ¶äº†æ·±è‰²ä¸»é¢˜è‡ªåŠ¨åˆ‡æ¢åŠŸèƒ½ã€‚æ›´å¤šè´´å¿ƒå®šåˆ¶é¡¹æ­£åœ¨è¿›è¡Œä¸­ã€‚\n\n## å®ç°åŠŸèƒ½ï¼š\n+ æ–¹æ¡ˆå†…ç½®ï¼šå…¨æ‹¼ï¼ˆä¹å®«æ ¼ã€å…¨é”®ï¼‰ã€åŒæ‹¼(å°é¹¤ã€æ™ºèƒ½ABCã€è‡ªç„¶ç ã€ç´«å…‰ã€å¾®è½¯ã€æœç‹—ã€ä¹±åº17)ã€æ‰‹å†™ã€äº”ç¬”ç”»ï¼›æ”¯æŒç®€æ‹¼ã€å…¨æ‹¼ï¼›\n+ è‹±æ–‡è¾“å…¥ï¼šæ™ºèƒ½å…¨é”®è‹±æ–‡è¾“å…¥ï¼›\n+ è¯åº“æ‹“å±•ï¼šæ”¯æŒé›¾å‡‡è¯åº“ã€ç™½éœœè¯åº“ç­‰å¤šç§è¯åº“æ‹“å±•ï¼Œè¾“å…¥ä½“éªŒè‰¯å¥½ï¼›\n+ ç¬¦å·è¾“å…¥ï¼šä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­¦ã€é¢œæ–‡å­—ã€EMOJIè¡¨æƒ…è¾“å…¥ã€å¾®ä¿¡ç‰¹æ•ˆè¡¨æƒ…ï¼›\n+ æ•°å­—è¾“å…¥ï¼šæ•°å­—é”®ç›˜è¾“å…¥ã€é”®ç›˜æ•°å­—è¡Œè¾“å…¥ï¼› \n+ é”®ç›˜è‡ªå®šä¹‰ï¼šè‡ªå®šä¹‰èœå•æ ã€ä¸»é¢˜ã€æ·±è‰²æ¨¡å¼ã€é”®ç›˜è°ƒèŠ‚ã€é”®ç›˜æ•°å­—è¡Œã€é”®ç›˜ä½ç½®ç§»åŠ¨ï¼› \n+ å•æ‰‹é”®ç›˜ï¼šå·¦ã€å³æ‰‹æ¨¡å¼åˆ‡æ¢ï¼›\n+ æ‚¬æµ®é”®ç›˜ï¼šæ‚¬æµ®é”®ç›˜æ¨¡å¼ï¼Œé”®ç›˜æ‹–æ‹½ã€ç§»åŠ¨ï¼›\n+ èŠ±æ¼¾å­—è¾“å…¥ï¼šç«æ˜Ÿæ–‡ï¼ˆç„±æš’å¦ï¼‰ã€ èŠ±è—¤å­—ï¼ˆÎ¶à¸±Í¡èŠ±à¸±Í¡è—¤à¸±Í¡å­—à¸±Í¡âœ¾ï¼‰ã€å‡Œä¹±å­—ï¼ˆ\"Ò‰Ò‰Ò‰å‡ŒÒ‰Ò‰Ò‰ä¹±Ò‰Ò‰Ò‰å­—Ò‰Ò‰Ò‰ï¼‰ã€å‘èŠ½å­—ï¼ˆå‘à½¼èŠ½à½¼å­—à½¼ï¼‰ã€é›¾éœ¾å­—ï¼ˆÒˆÒˆÒˆÒˆé›¾ÒˆÒˆÒˆÒˆéœ¾ÒˆÒˆÒˆÒˆå­—ÒˆÒˆÒˆÒˆï¼‰ã€ç¦æ­¢æŸ¥çœ‹ï¼ˆç¦âƒ æ­¢âƒ æŸ¥âƒ çœ‹âƒ ï¼‰ã€é•¿è‰å­—ï¼ˆ\"Òˆé•¿Ò‰Ò‰Òˆè‰Ò‰Ò‰Òˆå­—Ò‰ï¼‰ã€èµ·é£äº†ï¼ˆ=ÍŸÍŸÍÍé£=ÍŸÍŸÍÍå¤ª=ÍŸÍŸÍÍå¤§=ÍŸÍŸÍÍï¼‰èŠ±æ¼¾è¾“å…¥ï¼› \n+ æ‹¼éŸ³è¾“å…¥æ‰©å±•ï¼šæ”¯æŒç¹ä½“ã€ç®€ä½“ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··è¾“ï¼Œæ”¯æŒè¡¨æƒ…æè¿°è¾“å…¥ï¼›\n+ å‰ªåˆ‡æ¿ï¼šæ”¯æŒå‰ªåˆ‡æ¿è”æƒ³æ˜¾ç¤ºã€å‰ªåˆ‡æ¿åŠæ¸…ç©ºæ“ä½œï¼›\n+ å¸¸ç”¨è¯­ï¼šæ”¯æŒè‡ªå®šä¹‰å¸¸ç”¨è¯­ã€å¸¸ç”¨è¯­å¿«æ·è¾“å…¥ã€ç¼–è¾‘ã€åˆ é™¤ç­‰æ“ä½œï¼›\n+ å…¨é¢å±é”®ç›˜ä¼˜åŒ–ï¼šæ”¯æŒå…¨é¢å±é”®ç›˜ä¼˜åŒ–å¯¼èˆªæ åŠŸèƒ½ï¼›\n+ éšè—è¾“å…¥æ³•å›¾æ ‡ï¼šæ”¯æŒéšè—è¾“å…¥æ³•å›¾æ ‡åŠŸèƒ½ã€‚\n\n## å·²çŸ¥é—®é¢˜ï¼š\n* å°ç±³æ‰‹æœºä¸­é”®ç›˜èœå•ç‚¹å‡»è®¾ç½®ç­‰æ— ååº”:  \n  ç”±äºå°ç±³æ‰‹æœºä¸­é”®ç›˜è·³è½¬åº”ç”¨ç•Œé¢éœ€å€ŸåŠ©`åå°å¼¹å‡ºç•Œé¢`æƒé™ï¼Œè¯¥æƒé™éœ€ç”¨æˆ·æ‰‹åŠ¨å¼€å¯ï¼šè®¾ç½®-åº”ç”¨ç®¡ç†-è¯­ç‡•è¾“å…¥æ³•-æƒé™ç®¡ç†-å¼€å¯`åå°å¼¹å‡ºç•Œé¢`æƒé™å³å¯ã€‚\n* ä¸‰æ˜Ÿæ‰‹æœºæŒ‰é”®éŸ³é‡è°ƒèŠ‚æ— æ•ˆ:  \n  è¯­ç‡•è¾“å…¥æ³•ä½¿ç”¨ç³»ç»Ÿ`é€šçŸ¥`éŸ³é‡ä½œä¸ºæŒ‰é”®é»˜è®¤éŸ³é‡ï¼Œä½†ä¸åŒæ‰‹æœºè¡¨ç°ä¸åŒã€‚è¾“å…¥æ³•ä¼šä»¥æ‰‹æœºç³»ç»ŸéŸ³é‡è®¾ç½®ä¸ºå‰æï¼Œå½“æ‰‹æœºé™éŸ³æ—¶ï¼Œæ— è¾“å…¥æ³•æŒ‰é”®éŸ³ã€‚å½“æ‰‹æœºæœªé™éŸ³æ—¶ï¼Œä»¥`é€šçŸ¥`éŸ³é‡å¤§å°ä¸ºåŸºå‡†è¿›è¡Œè°ƒèŠ‚ã€‚åœ¨ä¸‰æ˜Ÿæ‰‹æœºä¸­ï¼ŒåŸºäº`ç³»ç»Ÿ`éŸ³é‡å¤§å°è¿›è¡Œè°ƒè§£ã€‚\n* åœ¨è¾“å…¥ä¸€åŠå†…å®¹æ—¶åˆ‡æ¢æ¨ªç«–å±ï¼Œè¾ƒå¤§æ¦‚ç‡å¯¼è‡´æ¨ªå±æ¨¡å¼å±å¹•è§¦æ‘¸æ— æ•ˆï¼Œä»…èƒ½ç‚¹å‡»é”®ç›˜æŒ‰é”®ã€‚\n  ä¸´æ—¶æ–¹æ¡ˆï¼šåˆ‡æ¢æ¨ªç«–å±å‰ï¼Œç¡®ä¿è¾“å…¥æ¡†å†…å®¹ä¸ºç©ºã€‚\n\n## å¼€å‘ç¯å¢ƒï¼š\n> Android SDK: minSdk 23, [app/build.gradle](./app/build.gradle)  \n> ç¬¬ä¸‰æ–¹åº“: [build.gradle](./build.gradle)  \n> JDK: OpenJDK version \"17.0.11\" 2024-04-16\n\n## æ„å»ºé¡¹ç›®ï¼š\n### 1. å…‹éš†æ­¤é¡¹ç›®å¹¶æ‹‰å–æ‰€æœ‰å­æ¨¡å—ã€‚\n```sh\ngit clone git@github.com:gurecn/YuyanIme.git\ngit submodule update --init --recursive\n```\n### 2. å¯¼å…¥Android Studio\nå»ºè®®ä½¿ç”¨æœ€æ–°ã€ç¨³å®šç‰ˆæœ¬ï¼Œæœ¬äººä½¿ç”¨`Android Studio Iguana | 2023.2.1 Patch 1`ç‰ˆæœ¬ï¼ŒæŒ‰ç…§å¸¸è§„é¡¹ç›®å¯¼å…¥å³å¯ï¼Œ`Android Studio`ä¼šè‡ªåŠ¨å®‰è£…å¹¶é…ç½® Android å¼€å‘ç¯å¢ƒã€‚\n\n## é”®ç›˜é¢„è§ˆï¼š\n| ä¹å®«é”®ç›˜ | å…¨æ‹¼é”®ç›˜ | ä¹±åº17 |\n| - | - | - |\n| ![ä¹å®«æ ¼æ‹¼éŸ³é”®ç›˜](./images/t9_pinyin.jpg) | ![å…¨é”®æ‹¼éŸ³é”®ç›˜](./images/qwerty_pinyin.jpg) | ![ä¹±åº17æ‹¼éŸ³](./images/double_lx17.jpg) |\n\n| åŒæ‹¼é”®ç›˜ | ç¬”ç”»é”®ç›˜ | æ‰‹å†™é”®ç›˜ |\n| - | - | - |\n| ![åŒæ‹¼é”®ç›˜](./images/double_pinyin.jpg) | ![ç¬”ç”»é”®ç›˜](./images/stroke_pinyin.jpg) | ![æ‰‹å†™é”®ç›˜](./images/writing_pinyin.jpg) |\n\n| è‹±è¯­é”®ç›˜ | æ•°å­—é”®ç›˜ | ç¼–è¾‘é”®ç›˜ |\n| - | - | - |\n| ![è‹±è¯­é”®ç›˜](./images/qwerty.jpg) |  ![æ•°å­—é”®ç›˜](./images/number.jpg) | ![ç¼–è¾‘é”®ç›˜](./images/textedit.jpg) |\n\n| å‰ªåˆ‡æ¿ | å•æ‰‹é”®ç›˜ | æ‚¬æµ®é”®ç›˜ |\n| - | - | - |\n| ![å‰ªåˆ‡æ¿](./images/clipboard.jpg) | ![å•æ‰‹é”®ç›˜](./images/onehand.jpg) | ![æ‚¬æµ®é”®ç›˜](./images/float.jpg) |\n\n| è¡¨æƒ…é”®ç›˜ | å¾®ä¿¡ç‰¹æ•ˆ | æ•°å­—è¡Œ |\n| - | - | - |\n| ![è¡¨æƒ…é”®ç›˜](./images/emoji.jpg) | ![å¾®ä¿¡ç‰¹æ•ˆ](./images/emoji_wechat.jpg) | ![æ•°å­—è¡Œ](./images/number_line.jpg) |\n\n| æ·±è‰²ä¸»é¢˜ | è®¾ç½®èœå• |\n| - | - |\n| ![æ·±è‰²ä¸»é¢˜](./images/dark.jpg) | ![è®¾ç½®èœå•](./images/setting.jpg) |\n\n## é¸£è°¢ï¼š\næ„Ÿè°¢ä»¥ä¸‹ä¼˜ç§€çš„å¼€æºç¤¾åŒºè´¡çŒ®ï¼š\n- [RIME](http://rime.im)\n- [åŒæ–‡è¾“å…¥æ³•](https://github.com/osfans)\n- [å°ä¼é¹…è¾“å…¥æ³•](https://github.com/fcitx5-android/fcitx5-android)\n- [é›¾å‡‡æ‹¼éŸ³](https://github.com/iDvel/rime-ice)\n- [ç™½éœœæ‹¼éŸ³](https://github.com/gaboolic/rime-frost)\n\n## è”ç³»ä½œè€…ï¼š\nè®¿é—®æˆ‘çš„èµ„æº: <a href=\"https://github.com/gurecn\">https://github.com/gurecn</a>  \n\nç»™æˆ‘å‘é€é‚®ç®±ï¼š[gurecn@163.com](mailto:gurecn@163.com)\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=gurecn/YuyanIme&type=Date)](https://star-history.com/#gurecn/YuyanIme&Date)\n\n\n\n\n",
      "stars_today": 4
    },
    {
      "id": 3484241,
      "name": "Weasis",
      "full_name": "nroduit/Weasis",
      "description": "Weasis is a web-based DICOM viewer for advanced medical imaging and seamless PACS integration.",
      "html_url": "https://github.com/nroduit/Weasis",
      "stars": 1142,
      "forks": 340,
      "language": "Java",
      "topics": [
        "dicom",
        "dicom-image",
        "dicom-image-viewer",
        "dicom-images",
        "dicom-pr",
        "dicom-rt",
        "dicom-seg",
        "dicom-viewer",
        "dicom-web-viewer",
        "dicomweb",
        "ecg",
        "export-dicom",
        "medical",
        "medical-imaging",
        "multiplanar-reconstruction",
        "viewer",
        "volume-rendering",
        "weasis"
      ],
      "created_at": "2012-02-19T08:18:11Z",
      "updated_at": "2026-01-14T22:55:03Z",
      "pushed_at": "2026-01-05T14:06:46Z",
      "open_issues": 46,
      "owner": {
        "login": "nroduit",
        "avatar_url": "https://avatars.githubusercontent.com/u/993975?v=4"
      },
      "readme": "[![License](https://img.shields.io/badge/License-EPL%202.0-blue.svg)](https://opensource.org/licenses/EPL-2.0) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) ![Maven Build](https://github.com/nroduit/weasis/workflows/Build/badge.svg) ![Github](https://img.shields.io/github/downloads/nroduit/weasis/total?classes=inline \"Github release downloads\")\n\n[![Sonar](https://sonarcloud.io/api/project_badges/measure?project=org.weasis%3Aweasis-framework&metric=ncloc)](https://sonarcloud.io/component_measures?id=org.weasis%3Aweasis-framework) [![Sonar](https://sonarcloud.io/api/project_badges/measure?project=org.weasis%3Aweasis-framework&metric=reliability_rating)](https://sonarcloud.io/component_measures?id=org.weasis%3Aweasis-framework) [![Sonar](https://sonarcloud.io/api/project_badges/measure?project=org.weasis%3Aweasis-framework&metric=sqale_rating)](https://sonarcloud.io/component_measures?id=org.weasis%3Aweasis-framework) [![Sonar](https://sonarcloud.io/api/project_badges/measure?project=org.weasis%3Aweasis-framework&metric=security_rating)](https://sonarcloud.io/component_measures?id=org.weasis%3Aweasis-framework) [![Sonar](https://sonarcloud.io/api/project_badges/measure?project=org.weasis%3Aweasis-framework&metric=alert_status)](https://sonarcloud.io/dashboard?id=org.weasis%3Aweasis-framework)\n\n## Table of Contents\n- [About Weasis](#about-weasis)\n- [Getting Started](#getting-started)\n- [Release History](#release-history)\n- [Build Weasis](#build-weasis)\n- [General Features](#general-features)\n- [Viewer Features](#viewer-features-see-also-tutorials)\n- [Community and Support](#community-and-support)\n- [License](#license)\n\n## About Weasis\n\n**Weasis** is a powerful, multifunctional, and open-source DICOM viewer designed for both standalone and web-based use. It excels in medical image visualization and is widely used by healthcare professionals and research institutions. Engineered for seamless integration with PACS and DICOM workflows, Weasis offers a reliable solution for modern medical imaging needs.\n\n## Getting Started\n\n* [General information](https://nroduit.github.io)\n* [Getting Started](https://nroduit.github.io/en/getting-started)\n* [Download binary releases](https://nroduit.github.io/en/getting-started/download-dicom-viewer)\n* [Live Demo with different datasets](https://nroduit.github.io/en/demo)\n\n## Release History\nSee [CHANGELOG](CHANGELOG.md)\n\n## Build Weasis\nSee [How to build Weasis](https://nroduit.github.io/en/getting-started/building-weasis)\n\n## General Features\n* Open source DICOM viewer under EPL 2 or Apache 2 license\n* Flexible integration with PACS, VNA, RIS, HIS, or EHR  (see [integration documentation](https://nroduit.github.io/en/basics/customize/integration/))\n* [Desktop distributions](https://nroduit.github.io/en/getting-started/download-dicom-viewer/) (Windows, macOS, and Linux)\n* Web access through [weasis protocol](https://nroduit.github.io/en/getting-started/weasis-protocol)\n* [Responsive user interface](https://nroduit.github.io/en/tutorials/theme/index.html#how-to-scale-the-user-interface) working well on high DPI screens\n* [Multi-language support](https://nroduit.github.io/en/getting-started/translating/)\n* [Configuration of preferences](https://nroduit.github.io/en/basics/customize/preferences/) on server-side and client-side\n* [API for building custom plug-ins](https://nroduit.github.io/en/basics/customize/build-plugins/)\n* DICOM Send (storeSCU and STOW-RS)\n* [DICOM Query/Retrieve](https://nroduit.github.io/en/tutorials/dicom-import/index.html#dicom-queryretrieve) (C-GET, C-MOVE and WADO-URI) and [DICOMWeb](https://nroduit.github.io/en/tutorials/dicomweb-config) (QUERY and RETRIEVE)\n* Dicomizer module to convert standard images into DICOM files\n* [Embedded DICOM viewer in CD/DVD](https://nroduit.github.io/en/tutorials/dicom-export/index.html#cddvd-image) or other portable media\n\n## Viewer Features (see also [Tutorials](https://nroduit.github.io/en/tutorials/))\n\n* Data type support\n  * Display most DICOM files including multi-frame, enhanced, MPEG-2, MPEG-4, MIME Encapsulation, DOC, SR, PR, KOS, SEG, AU, RT, and ECG\n  * Display DICOM image containing float or double data (Parametric Map)\n  * Import and export DICOM CD/DVD with DICOMDIR\n  * Import and export DICOM ZIP files\n  * Viewer for common image formats (TIFF, BMP, GIF, JPEG, PNG, RAS, HDR, and PNM)\n\n* Exporting data\n  * Export DICOM files locally with several options (DICOMDIR, ZIP, ISO image file with Weasis, TIFF, JPEG, PNG...)\n  * Send DICOM files to a remote PACS or DICOMWeb server (C-STORE or STOW-RS)\n  * Save measurements and annotations in DICOM Presentation States or XML file\n\n* Viewing and image rendering\n  * Support of several screens with different calibration, support of HiDPI (High Dots Per Inch) monitors, full-screen mode\n  * Image manipulation with mouse buttons  (pan, zoom, windowing, rotation, scroll, crosshair)\n  * Support of DICOM Modality LUTs, VOI LUTs, LUT Shapes, and Presentation LUTs (even non-linear)\n  * Apply DICOM Presentation States (GSPS) and display graphics as overlays\n  * Support DICOM Overlays, Shutters, and DICOM Pixel Padding\n  * Volume rendering with 3D presets\n  * Layouts for comparing series or studies\n  * Advanced series synchronization options\n  * Display cross-lines\n  * 3D cursor\n  * Oblique Multi-planar Reconstruction (MPR)\n  * Maximum Intensity Projection\n  * Persistent magnifier glass\n\n* Measurement and annotation tools\n  * Length, area, and angle measurement\n  * Region statistics of pixels (Min, Max, Mean, StDev, Skewness, Kurtosis, Entropy)\n  * Histogram of modality values\n  * SUV measurement\n\n* Specific viewers\n  * DICOM ECG: display all the DICOM waveforms and allow to make some measurements\n  * DICOM SR: structured report viewer with hyperlinks to images and associated graphics\n  * DICOM AU: audio player (allow to export to WAV files)\n\n* Other tools\n  * Printing views to DICOM and system printers\n  * Apply and Create DICOM Key Object Selection by selecting images with the star button\n  * Display and search into all DICOM attributes\n  * DICOM RT tools for radiotherapy: display RT structure set, dose, and DVH chart\n\n<img src='./weasis.jpg'>\n\n## Community and Support\nWeasis encourages community participation. Whether youâ€™re reporting bugs, suggesting features, or seeking help, you can connect with others here:\n* [GitHub Issues](https://github.com/nroduit/Weasis/issues)\n* Forum: [Google group](https://groups.google.com/forum/#!forum/dcm4che) or [GitHub Discussions](https://github.com/nroduit/Weasis/discussions)\n* [Frequently Asked Questions](https://nroduit.github.io/en/faq/)\n\n## License\nWeasis is dual-licensed under the [EPL 2.0](https://opensource.org/licenses/EPL-2.0) and [Apache 2.0](https://opensource.org/licenses/Apache-2.0).",
      "stars_today": 4
    },
    {
      "id": 982577878,
      "name": "nav3-recipes",
      "full_name": "android/nav3-recipes",
      "description": "Implement common use cases with Jetpack Navigation 3",
      "html_url": "https://github.com/android/nav3-recipes",
      "stars": 1105,
      "forks": 116,
      "language": "Kotlin",
      "topics": [
        "android",
        "compose",
        "navigation"
      ],
      "created_at": "2025-05-13T05:23:51Z",
      "updated_at": "2026-01-14T22:33:14Z",
      "pushed_at": "2026-01-14T19:49:29Z",
      "open_issues": 54,
      "owner": {
        "login": "android",
        "avatar_url": "https://avatars.githubusercontent.com/u/32689599?v=4"
      },
      "readme": "# Navigation 3 - Code recipes\n[Jetpack Navigation 3](https://goo.gle/nav3) is a library for app navigation. This repository contains recipes for how to \nuse its APIs to implement common navigation use cases. Each recipe introduces a single concept. Instead\nof making existing recipes more complex, there should be a new recipe for that particular concept.\n\nEvery Navigation 3 release will be an opportunity for patterns you see in recipes to \"graduate\" and become\n(optional) helpers in the library itself. Then we'll update the recipe to use that prebuilt helper, thus\nensuring that the recipes continue to be a good way to approach these kinds of problems.\n\nRecipes on the `main` branch use the **latest** (which may be an alpha or snapshot) version of Nav3. For recipes that use **stable** versions, check the [releases page](https://github.com/android/nav3-recipes/releases).\n\n## Recipes\nThese are the recipes and what they demonstrate. \n\n### Basic API examples\n- **[Basic](app/src/main/java/com/example/nav3recipes/basic)**: Shows most basic API usage.\n- **[Saveable back stack](app/src/main/java/com/example/nav3recipes/basicsaveable)**: As above, with a persistent back stack.\n- **[Entry provider DSL](app/src/main/java/com/example/nav3recipes/basicdsl)**: As above, using the entryProvider DSL.\n\n### Deep links\nRead the [guide to deeplinking](docs/deeplink-guide.md). Upvote [this issue](https://issuetracker.google.com/470282247) if you would like an API for deeplinks.\n- **[Basic](app/src/main/java/com/example/nav3recipes/deeplink/basic)**: Shows how to parse a deep link URL from an Android Intent into a navigation key.\n- **[Advanced](app/src/main/java/com/example/nav3recipes/deeplink/advanced)**: Shows how to handle deep links with a synthetic back stack and correct \"Up\" navigation behavior.\n\n### Layouts using Scenes\n- **[List-Detail Scene](app/src/main/java/com/example/nav3recipes/scenes/listdetail)**: Shows how to create a custom, list-detail layout using a `Scene` and `SceneStrategy` (see video of UI behavior below).\n- **[Two pane Scene](app/src/main/java/com/example/nav3recipes/scenes/twopane)**: Shows how to create a custom, 2-pane layout.\n- **[BottomSheet](app/src/main/java/com/example/nav3recipes/bottomsheet)**: Shows how to create a BottomSheet destination.\n- **[Dialog](app/src/main/java/com/example/nav3recipes/dialog)**: Shows how to create a Dialog.\n\n### Material adaptive layouts\nExamples showing how to use the layouts provided by the [Compose Material3 Adaptive Navigation3 library](https://developer.android.com/jetpack/androidx/releases/compose-material3-adaptive#compose_material3_adaptive_navigation3_version_10_2)\n- **[List-Detail](app/src/main/java/com/example/nav3recipes/material/listdetail)**: Shows how to use a Material adaptive list-detail layout.\n- **[Supporting Pane](app/src/main/java/com/example/nav3recipes/material/supportingpane)**: Shows how to use a Material adaptive supporting pane layout.\n\nNote: If you find a bug or have a feature request for Material3 Adaptive Scenes [please file it here](https://issuetracker.google.com/issues/new?component=1467081). Don't file an issue on this repository.\n\n### Animations\n- **[Animations](app/src/main/java/com/example/nav3recipes/animations)**: Shows how to override the default animations for all destinations and a single destination.\n\n### Common use cases\n- **[Common navigation UI](app/src/main/java/com/example/nav3recipes/commonui)**: A common navigation toolbar where each item in the toolbar navigates to a top level destination.  \n- **[Multiple back stacks](app/src/main/java/com/example/nav3recipes/multiplestacks)**: Shows how to create multiple top level routes, each with its own back stack. Top level routes are displayed in a navigation bar allowing users to switch between them. State is retained for each top level route, and the navigation state persists config changes and process death.  \n- **[Conditional navigation](app/src/main/java/com/example/nav3recipes/conditional)**: Switch to a different navigation flow when a condition is met. For example, for authentication or first-time user onboarding.\n\n### Architecture\n- **[Hilt - Modularized navigation code](app/src/main/java/com/example/nav3recipes/modular/hilt)**: Demonstrates how to decouple navigation code into separate modules (uses Dagger/Hilt for DI). \n- **[Koin - Modularized navigation code](app/src/main/java/com/example/nav3recipes/modular/koin)**: Demonstrates how to decouple navigation code into separate modules (uses Koin for DI).\n\n### Passing navigation arguments to ViewModels\n- **[Basic ViewModel](app/src/main/java/com/example/nav3recipes/passingarguments/viewmodels/basic)**: Navigation arguments are passed to a ViewModel constructed using `viewModel()`\n- **[Hilt injected ViewModel](app/src/main/java/com/example/nav3recipes/passingarguments/viewmodels/hilt)**: Navigation arguments are passed to a ViewModel constructed using `hiltViewModel()`\n- **[Koin injected ViewModel](app/src/main/java/com/example/nav3recipes/passingarguments/viewmodels/koin)**: Navigation arguments are passed to a ViewModel constructed using `koinViewModel()`\n\n### Returning Results\n- **[Returning Results as Events](app/src/main/java/com/example/nav3recipes/results/event)**: Returning results as events to content in another NavEntry.\n- **[Returning Results as State](app/src/main/java/com/example/nav3recipes/results/state)**: Returning results as state stored in a CompositionLocal.\n\n### Future recipes\nThe most upvoted [recipe requests]([url](https://github.com/android/nav3-recipes/issues?q=is%3Aissue%20state%3Aopen%20label%3Arecipe-request)) will be considered for implementation. Don't see your recipe? [File a request for one here](https://github.com/android/nav3-recipes/issues/new?template=1-recipe-request.md)\n\n## Custom layout example\nThe following is a screen recording showing the navigation behavior of a [custom, list-detail Scene](app/src/main/java/com/example/nav3recipes/scenes/listdetail).\n\n![Custom layout example](/docs/images/ListDetailScene.gif)\n\n## Instructions\nClone this repository and open the root folder in [Android Studio](https://developer.android.com/studio). Each recipe is contained in its own package with its own `Activity`.\n\n## Found an issue?\nIf the issue is _directly related to this project_, as in, it's reproducible without modifying this project's source code, then please [file an issue on github](https://github.com/android/nav3-recipes/issues/new?template=2-bug-report.md). If you've found an issue with the Jetpack Navigation 3 library, please [file an issue on the issue tracker](https://issuetracker.google.com/issues/new?component=1750212&template=2102223).\n\n## Contributing\nWe'd love to accept your contributions. Please follow [these instructions](CONTRIBUTING.md).\n\n## Compose Multiplatform Recipes\nCMP recipes can be found [here](https://github.com/terrakok/nav3-recipes).\n\n## License\n```\nCopyright 2025 The Android Open Source Project\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n",
      "stars_today": 4
    },
    {
      "id": 506113888,
      "name": "elide",
      "full_name": "elide-dev/elide",
      "description": "fast, all-in-one, AI-native, multi-lang, runtime",
      "html_url": "https://github.com/elide-dev/elide",
      "stars": 376,
      "forks": 39,
      "language": "Kotlin",
      "topics": [
        "graalvm",
        "javascript",
        "jvm",
        "kotlin",
        "nodejs",
        "python",
        "runtime",
        "typescript",
        "wintertc"
      ],
      "created_at": "2022-06-22T05:48:04Z",
      "updated_at": "2026-01-14T11:31:06Z",
      "pushed_at": "2026-01-12T00:03:18Z",
      "open_issues": 134,
      "owner": {
        "login": "elide-dev",
        "avatar_url": "https://avatars.githubusercontent.com/u/101684957?v=4"
      },
      "readme": "<p align=\"center\">\n  <a href=\"https://github.com/elide-dev\">\n    <img src=\"https://static.elide.dev/assets/org-profile/creative/elide-banner-purple.png\" alt=\"Elide\" />\n  </a>\n</p>\n\n<p align=\"center\">\n<b>Elide is a fast batteries-included runtime, combining support for Kotlin, JavaScript, TypeScript, and Python.</b>\n<br />\n<br />\n<i>elide: verb. to omit (a sound or syllable) when speaking. to join together; to merge.</i>\n<br />\n<br />\n</p>\n\n<hr />\n\n<p align=\"center\">\n  <a href=\"https://github.com/elide-dev/elide/actions/workflows/build.ci.yml\"><img src=\"https://github.com/elide-dev/elide/actions/workflows/on.push.yml/badge.svg\" /></a>\n  <a href=\"https://codecov.io/gh/elide-dev/elide\"><img src=\"https://codecov.io/gh/elide-dev/elide/branch/main/graph/badge.svg?token=FXxhJlpKG3\" /></a>\n  <a href=\"https://bestpractices.coreinfrastructure.org/projects/7690\"><img src=\"https://bestpractices.coreinfrastructure.org/projects/7690/badge\" /></a>\n  <a href=\"https://github.com/elide-dev/elide\"><img src=\"https://img.shields.io/badge/Contributor%20Covenant-v1.4-ff69b4.svg\" alt=\"Code of Conduct\" /></a>\n  <br />\n  <a href=\"https://elide.dev/discord\"><img src=\"https://img.shields.io/discord/1119121740161884252?b1&logo=discord&logoColor=white&label=Discord\" /></a>\n  <a href=\"https://262.ecma-international.org/13.0/\"><img src=\"https://img.shields.io/badge/-ECMA2024-blue.svg?logo=javascript&logoColor=white\" /></a>\n  <a href=\"https://typescriptlang.org\"><img src=\"https://img.shields.io/badge/-TypeScript-blue.svg?logo=typescript&logoColor=white\" /></a>\n  <img alt=\"Python 3.11.x\" src=\"https://img.shields.io/badge/Python%203.11.x-green?style=flat&logo=python&logoColor=white&color=blue\">\n  <a href=\"https://pkl-lang.org\"><img src=\"https://img.shields.io/badge/-Pkl-blue.svg?logo=apple&logoColor=white\" /></a>\n  <a href=\"https://kotlinlang.org\"><img src=\"https://img.shields.io/badge/-Kotlin-blue.svg?logo=kotlin&logoColor=white\" /></a>\n</p>\n\n<p align=\"center\">\nLatest: <code>1.0.0-beta10</code>\n</p>\n<p align=\"center\">\n  Learn more at <a href=\"https://elide.dev\">elide.dev</a> | <a href=\"https://docs.elide.dev\">Docs, Guides, and Samples</a>\n</p>\n\n<hr />\n\n> [!IMPORTANT]\n> Careful! Elide is in beta.\n\n## Usage\n\nElide is like Node or Python. Use it to run things:\n```shell\n> elide ./my-code.{ts,js,py,kts,kt}\n```\n\nYou can use Node APIs. You can even mix languages:\n```typescript\n// sample.mts\n\n// use node apis\nimport { readFileSync } from \"node:fs\"\n\n// interoperate across languages \nimport sample from \"./sample.py\"\n\n// this is typescript - no build step needed first, like deno or bun\nconst x: number = 42;\n\nconsole.log(sample.greeting() + ` The answer is ${x}`);\n```\n```python\n# sample.py\n\ndef greeting(name = \"Elide\"):\n  return f\"Hello, {name}!\"\n```\n\n```shell\n> elide ./sample.mts\nHello, Elide! The answer is 42\n```\n\n### Kotlin as a first-class citizen\n\nElide can run Kotlin with no prior build step, can build Java code identically to `javac`, and can build Kotlin code identically to `kotlinc`.\n\n![elide-projects](./project/gifs/init-build-test.gif)\n\n- KotlinX is supported out of the box with no need to install dependencies\n- Build Kotlin to JVM bytecode, run tests, and install from Maven, all without verbose configuration\n\n### Pkl as a manifest format\n\nElide uses [Apple's Pkl](https://pkl-lang.org) as a dialect for project manifests. This is like Elide's equivalent of `package.json` or `pom.xml`. Here's an example:\n\n```pkl\namends \"elide:project.pkl\"\n\nname = \"elide-test-ktjvm\"\ndescription = \"Example project using Elide with Kotlin/JVM.\"\n\ndependencies {\n  maven {\n    packages {\n      // Guava\n      \"com.google.guava:guava:33.4.8-jre\"\n    }\n  }\n}\n```\n\nThis is the manifest used above :point_up: in the _Kotlin as a first-class citizen_ sample.\n\n> [!NOTE]\n> See the full sources for the `ktjvm` sample [here](https://github.com/elide-dev/elide/tree/main/packages/cli/src/projects/ktjvm)\n\nRead more about Elide's [feature highlights](https://elide.dev)\n\n### Support for End-User Binaries + Containers\n\nElide has early support for building _your_ apps into native binaries, too! You can even wrap these in containers,\nwithout the need for Docker.\n\nAdding to the _Kotlin as a first-class-citizen_ example above:\n```pkl\nartifacts {\n  // Build a JAR from our Kotlin code.\n  [\"jar\"] = build.jar()\n\n  // Build a native image from our JAR and classpath.\n  [\"native\"] = build.nativeImage(\"jar\")\n\n  // Wrap the native image in a container image.\n  [\"container\"] = (build.containerImage(\"native\")) {\n    // Set this property to a remote image. This is the target image.\n    image = \"ghcr.io/elide-dev/samples/containers\"\n  }\n}\n```\n\nNow, `elide build` produces a JAR, a native image, and a container image, and then pushes it directly up to the registry\nlisted in the config:\n\n![elide-containers](./project/gifs/containers.gif)\n\n> [!NOTE]\n> See the full sources for the `containers` sample [here](https://github.com/elide-dev/elide/tree/main/packages/cli/src/projects/containers)\n\n## Installation\n\nYou can install Elide in several ways:\n\n### Script Install (Linux amd64 or macOS arm64)\n\n```shell\ncurl -sSL --tlsv1.2 elide.sh | bash -s -\n```\n\n### Homebrew (macOS)\n```shell\nbrew tap elide-dev/elide\nbrew install elide\n```\n\nAfter installation, you can run `elide --help` or `elide info` to see more information.\n\n> [!NOTE]\n> If you need a binary for a different architecture, please file an issue.\n\n### Using Elide via Docker\n\nWe provide a container image, hosted on GitHub:\n\n```\ndocker run --rm -it ghcr.io/elide-dev/elide\n```\n\n### Using Elide in GitHub Actions\n\nWe provide a [setup action](https://github.com/marketplace/actions/setup-elide):\n\n```yaml\n- name: \"Setup: Elide\"\n  uses: elide-dev/setup-elide@v3\n  with:\n    # any tag from the `elide-dev/elide` repo; omit for latest\n    version: 1.0.0-beta10\n```\n\n### Using Elide from Gradle\n\nWe provide an experimental [Gradle plugin](https://github.com/elide-dev/gradle) which can:\n\n- Accelerate `javac` compilations by up to 20x (drop-in!) with identical inputs and outputs\n- Accelerate downloading of Maven dependencies\n\nThe plugin documentation explains how it works. By native-image compiling tools like `javac`, JIT warmup is skipped, potentially yielding significant performance gains for projects under 10,000 classes.\n\n[Installation in Gradle](https://github.com/elide-dev/gradle)\n```kotlin\nplugins {\n  alias(elideRuntime.plugins.elide)\n}\n```\n\n### Using Elide via GitHub Codespaces\n\nWe provide a [GitHub Codespace](https://github.com/features/codespaces) with Elide pre-installed. You can click below to try it out, right from your browser:\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/elide-dev/elide?devcontainer_path=.devcontainer%2Fdevcontainer.json)\n\n## Contributing\n\nIssue reports and pull requests are welcome! See our [contribution guidelines](CONTRIBUTING.md) or join our [discord community](https://elide.dev/discord) and let us know which features you would like to see implemented, or simply participate in the discussions to help shape the future of the project.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=elide-dev/elide&type=Date)](https://star-history.com/#elide-dev/elide)\n\n[1]: https://kotlinlang.org/\n[2]: https://graalvm.org/\n[3]: https://micronaut.io/\n[4]: https://reactjs.org/\n[5]: https://developers.google.com/protocol-buffers\n[6]: https://grpc.io/\n[7]: https://developers.google.com/closure\n[8]: https://bazel.build/\n[9]: https://gradle.org/\n[10]: https://developers.google.com/speed/pagespeed/module\n[11]: https://github.com/sgammon/elide/tree/master\n[12]: https://github.com/sgammon/elide\n[13]: https://buf.build\n[14]: https://esbuild.github.io/\n",
      "stars_today": 4
    },
    {
      "id": 1357796,
      "name": "emscripten",
      "full_name": "emscripten-core/emscripten",
      "description": "Emscripten: An LLVM-to-WebAssembly Compiler",
      "html_url": "https://github.com/emscripten-core/emscripten",
      "stars": 27117,
      "forks": 3477,
      "language": "C++",
      "topics": [
        "emscripten",
        "hacktoberfest",
        "wasm",
        "webassembly"
      ],
      "created_at": "2011-02-12T05:23:30Z",
      "updated_at": "2026-01-14T23:42:59Z",
      "pushed_at": "2026-01-15T00:28:12Z",
      "open_issues": 2386,
      "owner": {
        "login": "emscripten-core",
        "avatar_url": "https://avatars.githubusercontent.com/u/46011144?v=4"
      },
      "readme": "![emscripten logo](media/switch_logo.png)\n\nMain project page: <https://emscripten.org>\n\nGitHub CI status: [![CircleCI](https://circleci.com/gh/emscripten-core/emscripten.svg?style=svg)](https://circleci.com/gh/emscripten-core/emscripten/tree/main)\n\nChromium builder status: [emscripten-releases](https://ci.chromium.org/p/emscripten-releases)\n\nOverview\n--------\n\nEmscripten compiles C and C++ to [WebAssembly](https://webassembly.org/) using\n[LLVM](https://en.wikipedia.org/wiki/LLVM) and\n[Binaryen](https://github.com/WebAssembly/binaryen/). Emscripten output can run\non the Web, in Node.js, and in\n[wasm runtimes](https://v8.dev/blog/emscripten-standalone-wasm#running-in-wasm-runtimes).\n\nEmscripten provides Web support for popular portable APIs such as OpenGL and\nSDL2, allowing complex graphical native applications to be ported, such as\nthe [Unity game engine](https://docs.unity3d.com/Manual/webgl-gettingstarted.html)\nand [Google Earth](https://blog.chromium.org/2019/06/webassembly-brings-google-earth-to-more.html).\nIt can probably port your codebase, too!\n\nWhile Emscripten mostly focuses on compiling C and C++ using\n[Clang](https://clang.llvm.org/), it can be integrated with other LLVM-using\ncompilers (for example, Rust has Emscripten integration, with the\n`wasm32-unknown-emscripten` target).\n\nLicense\n-------\n\nEmscripten is available under 2 licenses, the MIT license and the\nUniversity of Illinois/NCSA Open Source License.\n\nBoth are permissive open source licenses, with little if any\npractical difference between them.\n\nThe reason for offering both is that (1) the MIT license is\nwell-known and suitable for a compiler toolchain, while\n(2) LLVM's original license, the University of Illinois/NCSA Open Source\nLicense, was also offered to allow Emscripten's code to be integrated\nupstream into LLVM. The second reason became less important after\nEmscripten switched to the LLVM wasm backend, at which point there\nisn't any code we expect to move back and forth between the projects;\nalso, LLVM relicensed to Apache 2.0 + exceptions meanwhile. In practice you\ncan just consider Emscripten as MIT licensed (which allows\nyou to do pretty much anything you want with a compiler, including\ncommercial and non-commercial use).\n\nSee `LICENSE` for the full content of the licenses.\n",
      "stars_today": 3
    },
    {
      "id": 33486016,
      "name": "Kingfisher",
      "full_name": "onevcat/Kingfisher",
      "description": "A lightweight, pure-Swift library for downloading and caching images from the web.",
      "html_url": "https://github.com/onevcat/Kingfisher",
      "stars": 24237,
      "forks": 2746,
      "language": "Swift",
      "topics": [
        "cache",
        "filters",
        "image",
        "image-processor",
        "ios",
        "kingfisher",
        "macos",
        "swift",
        "xcode"
      ],
      "created_at": "2015-04-06T14:26:21Z",
      "updated_at": "2026-01-14T19:43:30Z",
      "pushed_at": "2026-01-05T14:19:51Z",
      "open_issues": 173,
      "owner": {
        "login": "onevcat",
        "avatar_url": "https://avatars.githubusercontent.com/u/1019875?v=4"
      },
      "readme": "<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/onevcat/Kingfisher/master/images/logo.png\" alt=\"Kingfisher\" title=\"Kingfisher\" width=\"557\"/>\n</p>\n\n<p align=\"center\">\n<a href=\"https://github.com/onevcat/Kingfisher/actions?query=workflow%3Abuild\"><img src=\"https://github.com/onevcat/kingfisher/workflows/build/badge.svg?branch=master\"></a>\n<a href=\"https://swiftpackageindex.com/onevcat/Kingfisher/master/documentation/kingfisher\"><img src=\"https://img.shields.io/badge/Swift-Doc-DE5C43.svg?style=flat\"></a>\n<a href=\"https://cocoapods.org/pods/Kingfisher\"><img src=\"https://img.shields.io/github/v/tag/onevcat/Kingfisher.svg?color=blue&include_prereleases=&sort=semver\"></a>\n<a href=\"https://swift.org/package-manager/\"><img src=\"https://img.shields.io/badge/SPM-supported-DE5C43.svg?style=flat\"></a>\n<a href=\"https://raw.githubusercontent.com/onevcat/Kingfisher/master/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-black\"></a>\n</p>\n\nKingfisher is a powerful, pure-Swift library for downloading and caching images from the web. It provides you a chance to use a pure-Swift way to work with remote images in your next app.\n\n## Features\n\n- [x] Asynchronous image downloading and caching.\n- [x] Loading image from either `URLSession`-based networking or local provided data.\n- [x] Useful image processors and filters provided.\n- [x] Multiple-layer hybrid cache for both memory and disk.\n- [x] Fine control on cache behavior. Customizable expiration date and size limit.\n- [x] Cancelable downloading and auto-reusing previous downloaded content to improve performance.\n- [x] Independent components. Use the downloader, caching system, and image processors separately as you need.\n- [x] Prefetching images and showing them from the cache to boost your app.\n- [x] Extensions for `UIImageView`, `NSImageView`, `NSButton`, `UIButton`, `NSTextAttachment`, `WKInterfaceImage`, `TVMonogramView` and `CPListItem` to directly set an image from a URL.\n- [x] Built-in transition animation when setting images.\n- [x] Customizable placeholder and indicator while loading images.\n- [x] Extensible image processing and image format easily.\n- [x] Low Data Mode support.\n- [x] SwiftUI support.\n- [x] Swift 6 & Swift Concurrency (strict mode) prepared.\n- [x] Load & cache for Live Photo.\n\n### Kingfisher 101\n\nThe simplest use-case is setting an image to an image view with the `UIImageView` extension:\n\n```swift\nimport Kingfisher\n\nlet url = URL(string: \"https://example.com/image.png\")\nimageView.kf.setImage(with: url)\n```\n\nKingfisher will download the image from `url`, send it to both memory cache and disk cache, and display it in `imageView`. \nWhen you set it with the same URL later, the image will be retrieved from the cache and shown immediately.\n\nIt also works if you use SwiftUI:\n\n```swift\nvar body: some View {\n    KFImage(URL(string: \"https://example.com/image.png\")!)\n}\n```\n\n### A More Advanced Example\n\nWith the powerful options, you can do hard tasks with Kingfisher in a simple way. For example, the code below: \n\n1. Downloads a high-resolution image.\n2. Downsamples it to match the image view size.\n3. Makes it round cornered with a given radius.\n4. Shows a system indicator and a placeholder image while downloading.\n5. When prepared, it animates the small thumbnail image with a \"fade in\" effect. \n6. The original large image is also cached to disk for later use, to get rid of downloading it again in a detail view.\n7. A console log is printed when the task finishes, either for success or failure.\n\n```swift\nlet url = URL(string: \"https://example.com/high_resolution_image.png\")\nlet processor = DownsamplingImageProcessor(size: imageView.bounds.size)\n             |> RoundCornerImageProcessor(cornerRadius: 20)\nimageView.kf.indicatorType = .activity\nimageView.kf.setImage(\n    with: url,\n    placeholder: UIImage(named: \"placeholderImage\"),\n    options: [\n        .processor(processor),\n        .scaleFactor(UIScreen.main.scale),\n        .transition(.fade(1)),\n        .cacheOriginalImage\n    ])\n{\n    result in\n    switch result {\n    case .success(let value):\n        print(\"Task done for: \\(value.source.url?.absoluteString ?? \"\")\")\n    case .failure(let error):\n        print(\"Job failed: \\(error.localizedDescription)\")\n    }\n}\n```\n\nIt is a common situation I can meet in my daily work. Think about how many lines you need to write without\nKingfisher!\n\n### Method Chaining\n\nIf you are not a fan of the `kf` extension, you can also prefer to use the `KF` builder and chained the method \ninvocations. The code below is doing the same thing:\n\n```swift\n// Use `kf` extension\nimageView.kf.setImage(\n    with: url,\n    placeholder: placeholderImage,\n    options: [\n        .processor(processor),\n        .loadDiskFileSynchronously,\n        .cacheOriginalImage,\n        .transition(.fade(0.25)),\n        .lowDataMode(.network(lowResolutionURL))\n    ],\n    progressBlock: { receivedSize, totalSize in\n        // Progress updated\n    },\n    completionHandler: { result in\n        // Done\n    }\n)\n\n// Use `KF` builder\nKF.url(url)\n  .placeholder(placeholderImage)\n  .setProcessor(processor)\n  .loadDiskFileSynchronously()\n  .cacheMemoryOnly()\n  .fade(duration: 0.25)\n  .lowDataModeSource(.network(lowResolutionURL))\n  .onProgress { receivedSize, totalSize in  }\n  .onSuccess { result in  }\n  .onFailure { error in }\n  .set(to: imageView)\n```\n\nAnd even better, if later you want to switch to SwiftUI, just change the `KF` above to `KFImage`, and you've done:\n\n```swift\nstruct ContentView: View {\n    var body: some View {\n        KFImage.url(url)\n          .placeholder(placeholderImage)\n          .setProcessor(processor)\n          .loadDiskFileSynchronously()\n          .cacheMemoryOnly()\n          .fade(duration: 0.25)\n          .lowDataModeSource(.network(lowResolutionURL))\n          .onProgress { receivedSize, totalSize in  }\n          .onSuccess { result in  }\n          .onFailure { error in }\n    }\n}\n```\n\n## Requirements\n\n### Kingfisher 8.0\n\n- (UIKit/AppKit) iOS 13.0+ / macOS 10.15+ / tvOS 13.0+ / watchOS 6.0+ / visionOS 1.0+\n- (SwiftUI) iOS 14.0+ / macOS 11.0+ / tvOS 14.0+ / watchOS 7.0+ / visionOS 1.0+\n- Swift 5.9+\n\n### Kingfisher 7.0\n\n- (UIKit/AppKit) iOS 12.0+ / macOS 10.14+ / tvOS 12.0+ / watchOS 5.0+ / visionOS 1.0+\n- (SwiftUI) iOS 14.0+ / macOS 11.0+ / tvOS 14.0+ / watchOS 7.0+ / visionOS 1.0+\n- Swift 5.0+\n\n### Installation\n\nRefer to one of the following tutorials to install and use the framework:\n\n- [UIKit Tutorial](https://swiftpackageindex.com/onevcat/kingfisher/master/tutorials/kingfisher/gettingstarteduikit)\n- [SwiftUI Tutorial](https://swiftpackageindex.com/onevcat/kingfisher/master/tutorials/kingfisher/gettingstartedswiftui)\n\nAlternatively, you can follow either of the methods below.\n\n#### Swift Package Manager\n\n- File > Swift Packages > Add Package Dependency\n- Add `https://github.com/onevcat/Kingfisher.git`\n- Select \"Up to Next Major\" with \"8.0.0\"\n\n#### CocoaPods\n\n```ruby\nsource 'https://github.com/CocoaPods/Specs.git'\nplatform :ios, '13.0'\nuse_frameworks!\n\ntarget 'MyApp' do\n  pod 'Kingfisher', '~> 8.0'\nend\n```\n\n#### Pre-built Framework\n\n1. Open the release page, download the latest version of Kingfisher from the assets section. \n2. Drag the `Kingfisher.xcframework` into your project and add it to the target (usually the app target).\n3. Select your target, in the \"General\" Tab, find the \"Frameworks, Libraries, and Embedded Content\" section, set the `Embed Without Signing` to Kingfisher.\n\n## Documentation\n\nCheck the documentation and tutorials:\n\n- [Documentation Home](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher)\n- [Getting Started](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher/gettingstarted)\n    - [UIKit Tutorial](https://swiftpackageindex.com/onevcat/kingfisher/master/tutorials/kingfisher/gettingstarteduikit)\n    - [SwiftUI Tutorial](https://swiftpackageindex.com/onevcat/kingfisher/master/tutorials/kingfisher/gettingstartedswiftui)\n- [Common Tasks - General](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher/commontasks)\n    - [Common Tasks - Cache](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher/commontasks_cache)\n    - [Common Tasks - Downloader](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher/commontasks_downloader)\n    - [Common tasks - Processor](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher/commontasks_processor)\n\n### Migrating\n\n- [Kingfisher 8.0 Migration](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher/migration-to-8)\n- [Kingfisher 7.0 Migration](https://github.com/onevcat/Kingfisher/wiki/Kingfisher-7.0-Migration-Guide)\n\nIf you are using an even earlier version, see the guides below to know the steps for migrating.\n\n## Other\n\n### Future of Kingfisher\n\nI want to keep Kingfisher lightweight. This framework focuses on providing a simple solution for downloading and caching images. This doesnâ€™t mean the framework canâ€™t be improved. Kingfisher is far from perfect, so necessary and useful updates will be made to make it better.\n\n### Developments and Tests\n\nAny contributing and pull requests are warmly welcome. However, before you plan to implement some features or try to fix an uncertain issue, it is recommended to open a discussion first. It would be appreciated if your pull requests could build with all tests green. :)\n\n### About the logo\n\nThe logo of Kingfisher is inspired by [Tangram (ä¸ƒå·§æ¿)](http://en.wikipedia.org/wiki/Tangram), a dissection puzzle consisting of seven flat shapes from China. I believe she's a kingfisher bird instead of a swift, but someone insists that she is a pigeon. I guess I should give her a name. Hi, guys, do you have any suggestions?\n\n### Contact\n\nFollow and contact me on [Twitter](http://twitter.com/onevcat) or [Sina Weibo](http://weibo.com/onevcat). If you find an issue, [open a ticket](https://github.com/onevcat/Kingfisher/issues/new). Pull requests are warmly welcome as well.\n\n## Backers & Sponsors\n\nOpen-source projects cannot live long without your help. If you find Kingfisher to be useful, please consider supporting this \nproject by becoming a sponsor. Your user icon or company logo shows up [on my blog](https://onevcat.com/tabs/about/) with a link to your home page. \n\nBecome a sponsor through [GitHub Sponsors](https://github.com/sponsors/onevcat). :heart:\n\nSpecial thanks to:\n\n[![imgly](https://user-images.githubusercontent.com/1812216/106253726-271ed000-6218-11eb-98e0-c9c681925770.png)](https://img.ly/)\n\n[![emergetools](https://github-production-user-asset-6210df.s3.amazonaws.com/1019875/254794187-d44f6f50-993f-42e3-b79c-960f69c4adc1.png)](https://www.emergetools.com)\n\n\n\n### License\n\nKingfisher is released under the MIT license. See LICENSE for details.\n",
      "stars_today": 3
    },
    {
      "id": 253044228,
      "name": "nuclei-templates",
      "full_name": "projectdiscovery/nuclei-templates",
      "description": "Community curated list of templates for the nuclei engine to find security vulnerabilities.",
      "html_url": "https://github.com/projectdiscovery/nuclei-templates",
      "stars": 11789,
      "forks": 3298,
      "language": "JavaScript",
      "topics": [
        "bugbounty",
        "exploit-development",
        "exploits",
        "fingerprint",
        "hacktoberfest",
        "nuclei",
        "nuclei-checks",
        "nuclei-templates",
        "security",
        "vulnerability-detection"
      ],
      "created_at": "2020-04-04T16:21:34Z",
      "updated_at": "2026-01-15T00:04:32Z",
      "pushed_at": "2026-01-15T00:27:43Z",
      "open_issues": 156,
      "owner": {
        "login": "projectdiscovery",
        "avatar_url": "https://avatars.githubusercontent.com/u/50994705?v=4"
      },
      "readme": "\n\n<h1 align=\"center\">\nNuclei Templates\n</h1>\n<h4 align=\"center\">Community curated list of templates for the nuclei engine to find security vulnerabilities in applications.</h4>\n\n\n<p align=\"center\">\n<a href=\"https://github.com/projectdiscovery/nuclei-templates/issues\"><img src=\"https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\"></a>\n<a href=\"https://github.com/projectdiscovery/nuclei-templates/releases\"><img src=\"https://img.shields.io/github/release/projectdiscovery/nuclei-templates\"></a>\n<a href=\"https://twitter.com/pdnuclei\"><img src=\"https://img.shields.io/twitter/follow/pdnuclei.svg?logo=twitter\"></a>\n<a href=\"https://discord.gg/projectdiscovery\"><img src=\"https://img.shields.io/discord/695645237418131507.svg?logo=discord\"></a>\n</p>\n      \n<p align=\"center\">\n  <a href=\"https://docs.projectdiscovery.io/templates/introduction\">Documentation</a> â€¢\n  <a href=\"#-contributions\">Contributions</a> â€¢\n  <a href=\"#-discussion\">Discussion</a> â€¢\n  <a href=\"#-community\">Community</a> â€¢\n  <a href=\"https://docs.projectdiscovery.io/templates/faq\">FAQs</a> â€¢\n  <a href=\"https://discord.gg/projectdiscovery\">Join Discord</a>\n</p>\n\n----\n\nTemplates are the core of the [nuclei scanner](https://github.com/projectdiscovery/nuclei) which powers the actual scanning engine.\nThis repository stores and houses various templates for the scanner provided by our team, as well as contributed by the community.\nWe hope that you also contribute by sending templates via **pull requests** or [Github issues](https://github.com/projectdiscovery/nuclei-templates/issues/new?assignees=&labels=&template=submit-template.md&title=%5Bnuclei-template%5D+) to grow the list.\n\n\n## Nuclei Templates overview\n\n\nAn overview of the nuclei template project, including statistics on unique tags, author, directory, severity, and type of templates. The table below contains the top ten statistics for each matrix; an expanded version of this is [available here](TEMPLATES-STATS.md), and also available in [JSON](TEMPLATES-STATS.json) format for integration.\n\n<table>\n<tr>\n<td>\n\n### ğŸš¨ Known Exploited Vulnerabilities (KEV) Coverage\n\nNuclei templates provide coverage for vulnerabilities actively exploited in the wild:\n\n| **KEV Source** | **Templates** | **Description** |\n|----------------|---------------|-----------------|\n| ğŸ”´ **CISA KEV** | **454** | [CISA Known Exploited Vulnerabilities Catalog](https://www.cisa.gov/known-exploited-vulnerabilities-catalog) |\n| ğŸŸ  **VulnCheck KEV** | **1449** | [VulnCheck KEV](https://vulncheck.com/kev) - Enhanced vulnerability intelligence |\n| ğŸŸ¢ **Both Sources** | **407** | Templates covering vulnerabilities in both catalogs |\n\n> ğŸ’¡ **Total unique KEV templates: 1496** - Use `nuclei -tags kev,vkev` to scan for actively exploited vulnerabilities\n\n---\n\n## Nuclei Templates Top 10 statistics\n\n|    TAG    | COUNT |    AUTHOR     | COUNT | DIRECTORY  | COUNT | SEVERITY | COUNT | TYPE | COUNT |\n|-----------|-------|---------------|-------|------------|-------|----------|-------|------|-------|\n| vuln      |  6468 | dhiyaneshdk   |  1894 | http       |  9281 | info     |  4353 | file |   436 |\n| cve       |  3587 | daffainfo     |   905 | cloud      |   659 | high     |  2552 | dns  |    26 |\n| discovery |  3265 | princechaddha |   854 | file       |   436 | medium   |  2457 |      |       |\n| vkev      |  1394 | dwisiswant0   |   805 | network    |   259 | critical |  1555 |      |       |\n| panel     |  1365 | ritikchaddha  |   678 | code       |   251 | low      |   330 |      |       |\n| xss       |  1269 | pussycat0x    |   675 | dast       |   240 | unknown  |    54 |      |       |\n| wordpress |  1261 | pikpikcu      |   353 | workflows  |   205 |          |       |      |       |\n| exposure  |  1141 | pdteam        |   314 | javascript |    92 |          |       |      |       |\n| wp-plugin |  1103 | pdresearch    |   275 | ssl        |    38 |          |       |      |       |\n| osint     |   848 | iamnoooob     |   263 | dns        |    23 |          |       |      |       |\n\n**873 directories, 11997 files**.\n\n</td>\n</tr>\n</table>\n\nğŸ“– Documentation\n-----\n\nPlease navigate to https://nuclei.projectdiscovery.io for detailed documentation to **build** new or your own **custom** templates.\nWe have also added a set of templates to help you understand how things work.\n\nğŸ’ª Contributions\n-----\n\nNuclei-templates is powered by major contributions from the community.\n[Template contributions ](https://github.com/projectdiscovery/nuclei-templates/issues/new?assignees=&labels=&template=submit-template.md&title=%5Bnuclei-template%5D+), [Feature Requests](https://github.com/projectdiscovery/nuclei-templates/issues/new?assignees=&labels=&template=feature_request.md&title=%5BFeature%5D+) and [Bug Reports](https://github.com/projectdiscovery/nuclei-templates/issues/new?assignees=&labels=&template=bug_report.md&title=%5BBug%5D+) are more than welcome.\n\n![Alt](https://repobeats.axiom.co/api/embed/55ee65543bb9a0f9c797626c4e66d472a517d17c.svg \"Repobeats analytics image\")\n\nğŸ’¬ Discussion\n-----\n\nHave questions / doubts / ideas to discuss?\nFeel free to open a discussion on [Github discussions](https://github.com/projectdiscovery/nuclei-templates/discussions) board.\n\nğŸ‘¨â€ğŸ’» Community\n-----\n\nYou are welcome to join the active [Discord Community](https://discord.gg/projectdiscovery) to discuss directly with project maintainers and share things with others around security and automation.\nAdditionally, you may follow us on [Twitter](https://twitter.com/pdnuclei) to be updated on all the things about Nuclei.\n\n\n<p align=\"center\">\n<a href=\"https://github.com/projectdiscovery/nuclei-templates/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=projectdiscovery/nuclei-templates&max=300\">\n</a>\n</p>\n\n\nThanks again for your contribution and keeping this community vibrant. :heart:\n",
      "stars_today": 3
    },
    {
      "id": 54378638,
      "name": "gh-ost",
      "full_name": "github/gh-ost",
      "description": "GitHub's Online Schema-migration Tool for MySQL",
      "html_url": "https://github.com/github/gh-ost",
      "stars": 13143,
      "forks": 1350,
      "language": "Go",
      "topics": [
        "mysql",
        "schema-migrations"
      ],
      "created_at": "2016-03-21T10:08:52Z",
      "updated_at": "2026-01-14T15:28:08Z",
      "pushed_at": "2026-01-12T21:22:56Z",
      "open_issues": 317,
      "owner": {
        "login": "github",
        "avatar_url": "https://avatars.githubusercontent.com/u/9919?v=4"
      },
      "readme": "# gh-ost\n\n[![ci](https://github.com/github/gh-ost/actions/workflows/ci.yml/badge.svg)](https://github.com/github/gh-ost/actions/workflows/ci.yml) [![replica-tests](https://github.com/github/gh-ost/actions/workflows/replica-tests.yml/badge.svg)](https://github.com/github/gh-ost/actions/workflows/replica-tests.yml) [![downloads](https://img.shields.io/github/downloads/github/gh-ost/total.svg)](https://github.com/github/gh-ost/releases) [![release](https://img.shields.io/github/release/github/gh-ost.svg)](https://github.com/github/gh-ost/releases)\n\n#### GitHub's online schema migration for MySQL <img src=\"doc/images/gh-ost-logo-light-160.png\" align=\"right\">\n\n `gh-ost` is a triggerless online schema migration solution for MySQL. It is testable and provides pausability, dynamic control/reconfiguration, auditing, and many operational perks.\n\n`gh-ost` produces a light workload on the master throughout the migration, decoupled from the existing workload on the migrated table.\n\nIt has been designed based on years of experience with existing solutions, and changes the paradigm of table migrations.\n\n\n\n## How?\n\nAll existing online-schema-change tools operate in similar manner: they create a _ghost_ table in the likeness of your original table, migrate that table while empty, slowly and incrementally copy data from your original table to the _ghost_ table, meanwhile propagating ongoing changes (any `INSERT`, `DELETE`, `UPDATE` applied to your table) to the _ghost_ table. Finally, at the right time, they replace your original table with the _ghost_ table.\n\n`gh-ost` uses the same pattern. However it differs from all existing tools by not using triggers. We have recognized the triggers to be the source of [many limitations and risks](doc/why-triggerless.md).\n\nInstead, `gh-ost` [uses the binary log stream](doc/triggerless-design.md) to capture table changes, and asynchronously applies them onto the _ghost_ table. `gh-ost` takes upon itself some tasks that other tools leave for the database to perform. As result, `gh-ost` has greater control over the migration process; can truly suspend it; can truly decouple the migration's write load from the master's workload.\n\nIn addition, it offers many [operational perks](doc/perks.md) that make it safer, trustworthy and fun to use.\n\n![gh-ost general flow](doc/images/gh-ost-general-flow.png)\n\n## Highlights\n\n- Build your trust in `gh-ost` by testing it on replicas. `gh-ost` will issue same flow as it would have on the master, to migrate a table on a replica, without actually replacing the original table, leaving the replica with two tables you can then compare and satisfy yourself that the tool operates correctly. This is how we continuously test `gh-ost` in production.\n- True pause: when `gh-ost` [throttles](doc/throttle.md), it truly ceases writes on master: no row copies and no ongoing events processing. By throttling, you return your master to its original workload\n- Dynamic control: you can [interactively](doc/interactive-commands.md) reconfigure `gh-ost`, even as migration still runs. You may forcibly initiate throttling.\n- Auditing: you may query `gh-ost` for status. `gh-ost` listens on unix socket or TCP.\n- Control over cut-over phase: `gh-ost` can be instructed to postpone what is probably the most critical step: the swap of tables, until such time that you're comfortably available. No need to worry about ETA being outside office hours.\n- External [hooks](doc/hooks.md) can couple `gh-ost` with your particular environment.\n\nPlease refer to the [docs](doc) for more information. No, really, read the [docs](doc).\n\n## Usage\n\nThe [cheatsheet](doc/cheatsheet.md) has it all. You may be interested in invoking `gh-ost` in various modes:\n\n- a _noop_ migration (merely testing that the migration is valid and good to go)\n- a real migration, utilizing a replica (the migration runs on the master; `gh-ost` figures out identities of servers involved. Required mode if your master uses Statement Based Replication)\n- a real migration, run directly on the master (but `gh-ost` prefers the former)\n- a real migration on a replica (master untouched)\n- a test migration on a replica, the way for you to build trust with `gh-ost`'s operation.\n\nOur tips:\n\n- [Testing above all](doc/testing-on-replica.md), try out `--test-on-replica` first few times. Better yet, make it continuous. We have multiple replicas where we iterate our entire fleet of production tables, migrating them one by one, checksumming the results, verifying migration is good.\n- For each master migration, first issue a _noop_\n- Then issue the real thing via `--execute`.\n\nMore tips:\n\n- Use `--exact-rowcount` for accurate progress indication\n- Use `--postpone-cut-over-flag-file` to gain control over cut-over timing\n- Get familiar with the [interactive commands](doc/interactive-commands.md)\n\nAlso see:\n\n- [requirements and limitations](doc/requirements-and-limitations.md)\n- [common questions](doc/questions.md)\n- [what if?](doc/what-if.md)\n- [the fine print](doc/the-fine-print.md)\n- [Community questions](https://github.com/github/gh-ost/issues?q=label%3Aquestion)\n- [Using `gh-ost` on AWS RDS](doc/rds.md)\n- [Using `gh-ost` on Azure Database for MySQL](doc/azure.md)\n\n## What's in a name?\n\nOriginally this was named `gh-osc`: GitHub Online Schema Change, in the likes of [Facebook online schema change](https://www.facebook.com/notes/mysql-at-facebook/online-schema-change-for-mysql/430801045932/) and [pt-online-schema-change](https://www.percona.com/doc/percona-toolkit/2.2/pt-online-schema-change.html).\n\nBut then a rare genetic mutation happened, and the `c` transformed into `t`. And that sent us down the path of trying to figure out a new acronym. `gh-ost` (pronounce: _Ghost_), stands for GitHub's Online Schema Transmogrifier/Translator/Transformer/Transfigurator\n\n## License\n\n`gh-ost` is licensed under the [MIT license](https://github.com/github/gh-ost/blob/master/LICENSE)\n\n`gh-ost` uses 3rd party libraries, each with their own license. These are found [here](https://github.com/github/gh-ost/tree/master/vendor).\n\n## Community\n\n`gh-ost` is released at a stable state, but with mileage to go. We are [open to pull requests](https://github.com/github/gh-ost/blob/master/.github/CONTRIBUTING.md). Please first discuss your intentions via [Issues](https://github.com/github/gh-ost/issues).\n\nWe develop `gh-ost` at GitHub and for the community. We may have different priorities than others. From time to time we may suggest a contribution that is not on our immediate roadmap but which may appeal to others.\n\nPlease see [Coding gh-ost](doc/coding-ghost.md) for a guide to getting started developing with gh-ost.\n\n## Download/binaries/source\n\n`gh-ost` is now GA and stable.\n\n`gh-ost` is available in binary format for Linux and Mac OS/X\n\n[Download latest release here](https://github.com/github/gh-ost/releases/latest)\n\n`gh-ost` is a Go project; it is built with Go `1.15` and above. To build on your own, use either:\n- [script/build](https://github.com/github/gh-ost/blob/master/script/build) - this is the same build script used by CI hence the authoritative; artifact is `./bin/gh-ost` binary.\n- [build.sh](https://github.com/github/gh-ost/blob/master/build.sh) for building `tar.gz` artifacts in `/tmp/gh-ost-release`\n\nGenerally speaking, `master` branch is stable, but only [releases](https://github.com/github/gh-ost/releases) are to be used in production.\n\n## Authors\n\n`gh-ost` is designed, authored, reviewed and tested by the database infrastructure team at GitHub:\n- [@jonahberquist](https://github.com/jonahberquist)\n- [@ggunson](https://github.com/ggunson)\n- [@tomkrouper](https://github.com/tomkrouper)\n- [@shlomi-noach](https://github.com/shlomi-noach)\n- [@jessbreckenridge](https://github.com/jessbreckenridge)\n- [@gtowey](https://github.com/gtowey)\n- [@timvaillancourt](https://github.com/timvaillancourt)\n",
      "stars_today": 3
    },
    {
      "id": 19438,
      "name": "ggplot2",
      "full_name": "tidyverse/ggplot2",
      "description": "An implementation of the Grammar of Graphics in R",
      "html_url": "https://github.com/tidyverse/ggplot2",
      "stars": 6856,
      "forks": 2117,
      "language": "R",
      "topics": [
        "data-visualisation",
        "r",
        "visualisation"
      ],
      "created_at": "2008-05-25T01:21:32Z",
      "updated_at": "2026-01-14T22:08:36Z",
      "pushed_at": "2025-12-18T13:22:01Z",
      "open_issues": 91,
      "owner": {
        "login": "tidyverse",
        "avatar_url": "https://avatars.githubusercontent.com/u/22032646?v=4"
      },
      "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# ggplot2 <a href=\"https://ggplot2.tidyverse.org\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" alt=\"ggplot2 website\" /></a>\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/tidyverse/ggplot2/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidyverse/ggplot2/actions/workflows/R-CMD-check.yaml)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/ggplot2)](https://cran.r-project.org/package=ggplot2)\n[![Codecov test\ncoverage](https://codecov.io/gh/tidyverse/ggplot2/graph/badge.svg)](https://app.codecov.io/gh/tidyverse/ggplot2)\n<!-- badges: end -->\n\n## Overview\n\nggplot2 is a system for declaratively creating graphics, based on [The\nGrammar of\nGraphics](https://link.springer.com/book/10.1007/0-387-28695-0). You\nprovide the data, tell ggplot2 how to map variables to aesthetics, what\ngraphical primitives to use, and it takes care of the details.\n\n## Installation\n\n``` r\n# The easiest way to get ggplot2 is to install the whole tidyverse:\ninstall.packages(\"tidyverse\")\n\n# Alternatively, install just ggplot2:\ninstall.packages(\"ggplot2\")\n\n# Or the development version from GitHub:\n# install.packages(\"pak\")\npak::pak(\"tidyverse/ggplot2\")\n```\n\n## Cheatsheet\n\n<a href=\"https://github.com/rstudio/cheatsheets/blob/main/data-visualization.pdf\"><img src=\"https://raw.githubusercontent.com/rstudio/cheatsheets/main/pngs/thumbnails/data-visualization-cheatsheet-thumbs.png\" width=\"630\" height=\"252\" alt=\"ggplot2 cheatsheet\" /></a>\n\n## Usage\n\nItâ€™s hard to succinctly describe how ggplot2 works because it embodies a\ndeep philosophy of visualisation. However, in most cases you start with\n`ggplot()`, supply a dataset and aesthetic mapping (with `aes()`). You\nthen add on layers (like `geom_point()` or `geom_histogram()`), scales\n(like `scale_colour_brewer()`), faceting specifications (like\n`facet_wrap()`) and coordinate systems (like `coord_flip()`).\n\n``` r\nlibrary(ggplot2)\n\nggplot(mpg, aes(displ, hwy, colour = class)) +\n  geom_point()\n```\n\n<img src=\"man/figures/README-example-1.png\" alt=\"Scatterplot of engine displacement versus highway miles per gallon, for 234 cars coloured by 7 'types' of car. The displacement and miles per gallon are inversely correlated.\"  />\n\n## Lifecycle\n\n[![lifecycle](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n\nggplot2 is now 18 years old and is used by hundreds of thousands of\npeople to make millions of plots. That means, by-and-large, ggplot2\nitself changes relatively little. When we do make changes, they will be\ngenerally to add new functions or arguments rather than changing the\nbehaviour of existing functions, and if we do make changes to existing\nbehaviour we will do them for compelling reasons.\n\nIf you are looking for innovation, look to ggplot2â€™s rich ecosystem of\nextensions. See a community maintained list at\n<https://exts.ggplot2.tidyverse.org/gallery/>.\n\n## Learning ggplot2\n\nIf you are new to ggplot2 you are better off starting with a systematic\nintroduction, rather than trying to learn from reading individual\ndocumentation pages. Currently, there are several good places to start:\n\n1.  The [Data Visualization](https://r4ds.hadley.nz/data-visualize) and\n    [Communication](https://r4ds.hadley.nz/communication) chapters in [R\n    for Data Science](https://r4ds.hadley.nz). R for Data Science is\n    designed to give you a comprehensive introduction to the\n    [tidyverse](https://tidyverse.org/), and these two chapters will get\n    you up to speed with the essentials of ggplot2 as quickly as\n    possible.\n\n2.  If youâ€™d like to take an online course, try [Data Visualization in R\n    With\n    ggplot2](https://learning.oreilly.com/videos/data-visualization-in/9781491963661/)\n    by Kara Woo.\n\n3.  If youâ€™d like to follow a webinar, try [Plotting Anything with\n    ggplot2](https://youtu.be/h29g21z0a68) by Thomas Lin Pedersen.\n\n4.  If you want to dive into making common graphics as quickly as\n    possible, I recommend [The R Graphics\n    Cookbook](https://r-graphics.org) by Winston Chang. It provides a\n    set of recipes to solve common graphics problems.\n\n5.  If youâ€™ve mastered the basics and want to learn more, read [ggplot2:\n    Elegant Graphics for Data Analysis](https://ggplot2-book.org). It\n    describes the theoretical underpinnings of ggplot2 and shows you how\n    all the pieces fit together. This book helps you understand the\n    theory that underpins ggplot2, and will help you create new types of\n    graphics specifically tailored to your needs.\n\n6.  For articles about announcements and deep-dives you can visit the\n    [tidyverse blog](https://tidyverse.org/tags/ggplot2/).\n\n## Getting help\n\nThere are two main places to get help with ggplot2:\n\n1.  The [Posit Community](https://forum.posit.co/) (formerly RStudio\n    Community) is a friendly place to ask any questions about ggplot2.\n\n2.  [Stack\n    Overflow](https://stackoverflow.com/questions/tagged/ggplot2?sort=frequent&pageSize=50)\n    is a great source of answers to common ggplot2 questions. It is also\n    a great place to get help, once you have created a reproducible\n    example that illustrates your problem.\n",
      "stars_today": 3
    },
    {
      "id": 4930716,
      "name": "zeek",
      "full_name": "zeek/zeek",
      "description": "Zeek is a powerful network analysis framework that is much different from the typical IDS you may know.",
      "html_url": "https://github.com/zeek/zeek",
      "stars": 7411,
      "forks": 1314,
      "language": "C++",
      "topics": [
        "bro",
        "dfir",
        "ndr",
        "network-monitoring",
        "nsm",
        "pcap",
        "security",
        "zeek"
      ],
      "created_at": "2012-07-06T20:30:16Z",
      "updated_at": "2026-01-14T20:27:44Z",
      "pushed_at": "2026-01-15T00:41:09Z",
      "open_issues": 224,
      "owner": {
        "login": "zeek",
        "avatar_url": "https://avatars.githubusercontent.com/u/1194067?v=4"
      },
      "readme": "<h1 align=\"center\">\n\n[![Zeek Logo](https://zeek.org/wp-content/uploads/2020/04/zeek-logo-without-text.png)](https://www.zeek.org)\n\nThe Zeek Network Security Monitor\n\n</h1><h4 align=\"center\">\n\nA [powerful](https://old.zeek.org/why_choose_zeek.pdf) framework for network\ntraffic analysis and security monitoring.\n\n[_Key Features_](#key-features) â€”\n[_Documentation_](https://docs.zeek.org/en/stable/index.html) â€”\n[_Getting Started_](#getting-started) â€”\n[_Development_](#development) â€”\n[_License_](#license)\n\n[![Coverage Status](https://coveralls.io/repos/github/zeek/zeek/badge.svg?branch=master)](https://coveralls.io/github/zeek/zeek?branch=master)\n[![Build Status](https://img.shields.io/cirrus/github/zeek/zeek)](https://cirrus-ci.com/github/zeek/zeek)\n\n[![Slack](https://img.shields.io/badge/slack-@zeek-brightgreen.svg?logo=slack)](https://zeek.org/slack)\n[![Discourse](https://img.shields.io/discourse/status?server=https%3A%2F%2Fcommunity.zeek.org)](https://community.zeek.org)\n\n[![Mastodon](https://img.shields.io/badge/mastodon-@zeek@infosec.exchange-brightgreen.svg?logo=mastodon)](https://infosec.exchange/@zeek)\n[![Bluesky](https://img.shields.io/badge/bluesky-@zeek-brightgreen.svg?logo=bluesky)](https://bsky.app/profile/zeek.org)\n\n</h4>\n\n\nKey Features\n--------------\n\n* __In-depth Analysis__\n\tZeek ships with analyzers for many protocols, enabling high-level semantic\n  analysis at the application layer.\n\n* __Adaptable and Flexible__\n\tZeek's domain-specific scripting language enables site-specific monitoring\n  policies and means that it is not restricted to any particular detection\n  approach.\n\n* __Efficient__\n\tZeek targets high-performance networks and is used operationally at a variety\n  of large sites.\n\n* __Highly Stateful__\n\tZeek keeps extensive application-layer state about the network it monitors\n  and provides a high-level archive of a network's activity.\n\nGetting Started\n---------------\n\nThe best place to find information about getting started with Zeek is\nour web site [www.zeek.org](https://www.zeek.org), specifically the\n[documentation](https://docs.zeek.org/en/stable/index.html) section\nthere. On the web site you can also find downloads for stable\nreleases, tutorials on getting Zeek set up, and many other useful\nresources.\n\nYou can find release notes in [NEWS](https://github.com/zeek/zeek/blob/master/NEWS),\nand a complete record of all changes in [CHANGES](https://github.com/zeek/zeek/blob/master/CHANGES).\n\nTo work with the most recent code from the development branch of Zeek,\nclone the master git repository:\n\n`git clone --recursive https://github.com/zeek/zeek`\n\nWith all [dependencies](https://docs.zeek.org/en/stable/install/install.html#prerequisites)\nin place, build and install:\n\n`./configure && make && sudo make install`\n\nWrite your first Zeek script:\n\n```zeek\n# File \"hello.zeek\"\n\nevent zeek_init()\n    {\n    print \"Hello World!\";\n    }\n```\n\nAnd run it:\n\n`zeek hello.zeek`\n\nFor learning more about the Zeek scripting\nlanguage, [try.zeek.org](http://try.zeek.org) is a great resource.\n\nDevelopment\n-----------\n\nZeek is developed on GitHub by its community. We welcome\ncontributions. Working on an open source project like Zeek can be an\nincredibly rewarding experience and, packet by packet, makes the\nInternet a little safer. Today, as a result of countless\ncontributions, Zeek is used operationally around the world by major\ncompanies and educational and scientific institutions alike for\nsecuring their cyber infrastructure.\n\nIf you're interested in getting involved, we collect feature requests\nand issues on GitHub [here](https://github.com/zeek/zeek/issues) and\nyou might find\n[these](https://github.com/zeek/zeek/labels/good%20first%20issue)\nto be a good place to get started. More information on Zeek's\ndevelopment can be found\n[here](https://docs.zeek.org/en/current/devel/index.html), and information\nabout its community and mailing lists (which are fairly active) can be\nfound [here](https://www.zeek.org/community/).\n\nLicense\n-------\n\nZeek comes with a BSD license, allowing for free use with virtually no\nrestrictions. You can find it [here](https://github.com/zeek/zeek/blob/master/COPYING).\n\n\nTooling\n-------\n\nWe use the following tooling to help discover issues to fix, amongst a number of\nothers.\n\n- [Clang-Tidy](https://clang.llvm.org/extra/clang-tidy/)\n- [Coverity](https://scan.coverity.com/projects/bro)\n- [PVS-Studio](https://pvs-studio.com/en/pvs-studio/?utm_source=github&utm_medium=organic&utm_campaign=open_source) - static analyzer for C, C++, C#, and Java code.\n",
      "stars_today": 3
    },
    {
      "id": 38304949,
      "name": "GRDB.swift",
      "full_name": "groue/GRDB.swift",
      "description": "A toolkit for SQLite databases, with a focus on application development",
      "html_url": "https://github.com/groue/GRDB.swift",
      "stars": 8102,
      "forks": 827,
      "language": "Swift",
      "topics": [
        "database",
        "database-observation",
        "grdb",
        "spm",
        "sql",
        "sql-builder",
        "sqlite",
        "sqlite-databases"
      ],
      "created_at": "2015-06-30T11:17:06Z",
      "updated_at": "2026-01-15T00:55:06Z",
      "pushed_at": "2026-01-09T15:30:58Z",
      "open_issues": 12,
      "owner": {
        "login": "groue",
        "avatar_url": "https://avatars.githubusercontent.com/u/54219?v=4"
      },
      "readme": "<picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/groue/GRDB.swift/master/GRDB~dark.png\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/groue/GRDB.swift/master/GRDB.png\">\n    <img alt=\"GRDB: A toolkit for SQLite databases, with a focus on application development.\" src=\"https://raw.githubusercontent.com/groue/GRDB.swift/master/GRDB.png\">\n</picture>\n\n<p align=\"center\">\n    <strong>A toolkit for SQLite databases, with a focus on application development</strong><br>\n    Proudly serving the community since 2015\n</p>\n\n<p align=\"center\">\n    <a href=\"https://developer.apple.com/swift/\"><img alt=\"Swift 6.1\" src=\"https://img.shields.io/badge/swift-6.1-orange.svg?style=flat\"></a>\n    <a href=\"https://github.com/groue/GRDB.swift/blob/master/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/github/license/groue/GRDB.swift.svg?maxAge=2592000\"></a>\n    <a href=\"https://github.com/groue/GRDB.swift/actions/workflows/CI.yml\"><img alt=\"CI Status\" src=\"https://github.com/groue/GRDB.swift/actions/workflows/CI.yml/badge.svg?branch=master\"></a>\n</p>\n\n**Latest release**: December 13, 2025 â€¢ [version 7.9.0](https://github.com/groue/GRDB.swift/tree/v7.9.0) â€¢ [CHANGELOG](CHANGELOG.md) â€¢ [Migrating From GRDB 6 to GRDB 7](Documentation/GRDB7MigrationGuide.md)\n\n**Requirements**: iOS 13.0+ / macOS 10.15+ / tvOS 13.0+ / watchOS 7.0+ &bull; SQLite 3.20.0+ &bull; Swift 6.1+ / Xcode 16.3+\n\n**Contact**:\n\n- Release announcements and usage tips: follow [@groue@hachyderm.io](https://hachyderm.io/@groue) on Mastodon.\n- Report bugs in a [Github issue](https://github.com/groue/GRDB.swift/issues/new). Make sure you check the [existing issues](https://github.com/groue/GRDB.swift/issues?q=is%3Aopen) first.\n- A question? Looking for advice? Do you wonder how to contribute? Fancy a chat? Go to the [GitHub discussions](https://github.com/groue/GRDB.swift/discussions), or the [GRDB forums](https://forums.swift.org/c/related-projects/grdb).\n\n\n## What is GRDB?\n\nUse this library to save your applicationâ€™s permanent data into SQLite databases. It comes with built-in tools that address common needs:\n\n- **SQL Generation**\n    \n    Enhance your application models with persistence and fetching methods, so that you don't have to deal with SQL and raw database rows when you don't want to.\n\n- **Database Observation**\n    \n    Get notifications when database values are modified. \n\n- **Robust Concurrency**\n    \n    Multi-threaded applications can efficiently use their databases, including WAL databases that support concurrent reads and writes. \n\n- **Migrations**\n    \n    Evolve the schema of your database as you ship new versions of your application.\n    \n- **Leverage your SQLite skills**\n\n    Not all developers need advanced SQLite features. But when you do, GRDB is as sharp as you want it to be. Come with your SQL and SQLite skills, or learn new ones as you go!\n\n---\n\n<p align=\"center\">\n    <a href=\"#usage\">Usage</a> &bull;\n    <a href=\"#documentation\">Documentation</a> &bull;\n    <a href=\"#installation\">Installation</a> &bull;\n    <a href=\"#faq\">FAQ</a>\n</p>\n\n---\n\n## Usage\n\n<details open>\n  <summary>Start using the database in four steps</summary>\n\n```swift\nimport GRDB\n\n// 1. Open a database connection\nlet dbQueue = try DatabaseQueue(path: \"/path/to/database.sqlite\")\n\n// 2. Define the database schema\ntry dbQueue.write { db in\n    try db.create(table: \"player\") { t in\n        t.primaryKey(\"id\", .text)\n        t.column(\"name\", .text).notNull()\n        t.column(\"score\", .integer).notNull()\n    }\n}\n\n// 3. Define a record type\nstruct Player: Codable, Identifiable, FetchableRecord, PersistableRecord {\n    var id: String\n    var name: String\n    var score: Int\n    \n    enum Columns {\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n}\n\n// 4. Write and read in the database\ntry dbQueue.write { db in\n    try Player(id: \"1\", name: \"Arthur\", score: 100).insert(db)\n    try Player(id: \"2\", name: \"Barbara\", score: 1000).insert(db)\n}\n\ntry dbQueue.read { db in\n    let player = try Player.find(db, id: \"1\"))\n    \n    let bestPlayers = try Player\n        .order(\\.score.desc)\n        .limit(10)\n        .fetchAll(db)\n}\n```\n\n</details>\n\n<details>\n    <summary>Access to raw SQL</summary>\n\n```swift\ntry dbQueue.write { db in\n    try db.execute(sql: \"\"\"\n        CREATE TABLE player (\n          id TEXT PRIMARY KEY,\n          name TEXT NOT NULL,\n          score INT NOT NULL)\n        \"\"\")\n    \n    try db.execute(sql: \"\"\"\n        INSERT INTO player (id, name, score)\n        VALUES (?, ?, ?)\n        \"\"\", arguments: [\"1\", \"Arthur\", 100])\n    \n    // Avoid SQL injection with SQL interpolation\n    let id = \"2\"\n    let name = \"O'Brien\"\n    let score = 1000\n    try db.execute(literal: \"\"\"\n        INSERT INTO player (id, name, score)\n        VALUES (\\(id), \\(name), \\(score))\n        \"\"\")\n}\n```\n\nSee [Executing Updates](#executing-updates)\n\n</details>\n\n<details>\n    <summary>Access to raw database rows and values</summary>\n\n```swift\ntry dbQueue.read { db in\n    // Fetch database rows\n    let rows = try Row.fetchCursor(db, sql: \"SELECT * FROM player\")\n    while let row = try rows.next() {\n        let id: String = row[\"id\"]\n        let name: String = row[\"name\"]\n        let score: Int = row[\"score\"]\n    }\n    \n    // Fetch values\n    let playerCount = try Int.fetchOne(db, sql: \"SELECT COUNT(*) FROM player\")! // Int\n    let playerNames = try String.fetchAll(db, sql: \"SELECT name FROM player\") // [String]\n}\n\nlet playerCount = try dbQueue.read { db in\n    try Int.fetchOne(db, sql: \"SELECT COUNT(*) FROM player\")!\n}\n```\n\nSee [Fetch Queries](#fetch-queries)\n\n</details>\n\n<details>\n    <summary>Database model types aka \"records\"</summary>\n\n```swift\nstruct Player: Codable, Identifiable, FetchableRecord, PersistableRecord {\n    var id: String\n    var name: String\n    var score: Int\n    \n    enum Columns {\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n}\n\ntry dbQueue.write { db in\n    // Create database table\n    try db.create(table: \"player\") { t in\n        t.primaryKey(\"id\", .text)\n        t.column(\"name\", .text).notNull()\n        t.column(\"score\", .integer).notNull()\n    }\n    \n    // Insert a record\n    var player = Player(id: \"1\", name: \"Arthur\", score: 100)\n    try player.insert(db)\n    \n    // Update a record\n    player.score += 10\n    try score.update(db)\n    \n    try player.updateChanges { $0.score += 10 }\n    \n    // Delete a record\n    try player.delete(db)\n}\n```\n\nSee [Records](#records)\n\n</details>\n\n<details>\n    <summary>Query the database with the Swift query interface</summary>\n\n```swift\ntry dbQueue.read { db in\n    // Player\n    let player = try Player.find(db, id: \"1\")\n    \n    // Player?\n    let arthur = try Player.filter { $0.name == \"Arthur\" }.fetchOne(db)\n    \n    // [Player]\n    let bestPlayers = try Player.order(\\.score.desc).limit(10).fetchAll(db)\n    \n    // Int\n    let playerCount = try Player.fetchCount(db)\n    \n    // SQL is always welcome\n    let players = try Player.fetchAll(db, sql: \"SELECT * FROM player\")\n}\n```\n\nSee the [Query Interface](#the-query-interface)\n\n</details>\n\n<details>\n    <summary>Database changes notifications</summary>\n\n```swift\n// Define the observed value\nlet observation = ValueObservation.tracking { db in\n    try Player.fetchAll(db)\n}\n\n// Start observation\nlet cancellable = observation.start(\n    in: dbQueue,\n    onError: { error in ... },\n    onChange: { (players: [Player]) in print(\"Fresh players: \\(players)\") })\n```\n\nReady-made support for Combine and RxSwift:\n\n```swift\n// Swift concurrency\nfor try await players in observation.values(in: dbQueue) {\n    print(\"Fresh players: \\(players)\")\n}\n\n// Combine\nlet cancellable = observation.publisher(in: dbQueue).sink(\n    receiveCompletion: { completion in ... },\n    receiveValue: { (players: [Player]) in print(\"Fresh players: \\(players)\") })\n\n// RxSwift\nlet disposable = observation.rx.observe(in: dbQueue).subscribe(\n    onNext: { (players: [Player]) in print(\"Fresh players: \\(players)\") },\n    onError: { error in ... })\n```\n\nSee [Database Observation], [Combine Support], [RxGRDB].\n\n</details>\n\nDocumentation\n=============\n\n**GRDB runs on top of SQLite**: you should get familiar with the [SQLite FAQ](http://www.sqlite.org/faq.html). For general and detailed information, jump to the [SQLite Documentation](http://www.sqlite.org/docs.html).\n\n\n#### Demo Applications & Frequently Asked Questions\n\n- [Demo Applications]\n- [FAQ]\n\n#### Reference\n\n- ğŸ“– [GRDB Reference](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/)\n\n#### Getting Started\n\n- [Installation](#installation)\n- [Database Connections]: Connect to SQLite databases\n\n#### SQLite and SQL\n\n- [SQLite API](#sqlite-api): The low-level SQLite API &bull; [executing updates](#executing-updates) &bull; [fetch queries](#fetch-queries) &bull; [SQL Interpolation]\n\n#### Records and the Query Interface\n\n- [Records](#records): Fetching and persistence methods for your custom structs and class hierarchies\n- [Query Interface](#the-query-interface): A swift way to generate SQL &bull; [create tables, indexes, etc](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseschema) &bull; [requests](#requests) â€¢ [associations between record types](Documentation/AssociationsBasics.md)\n\n#### Application Tools\n\n- [Migrations]: Transform your database as your application evolves.\n- [Full-Text Search]: Perform efficient and customizable full-text searches.\n- [Database Observation]: Observe database changes and transactions.\n- [Encryption](#encryption): Encrypt your database with SQLCipher.\n- [Backup](#backup): Dump the content of a database to another.\n- [Interrupt a Database](#interrupt-a-database): Abort any pending database operation.\n- [Sharing a Database]: How to share an SQLite database between multiple processes - recommendations for App Group containers, App Extensions, App Sandbox, and file coordination.\n\n#### Good to Know\n\n- [Concurrency]: How to access databases in a multi-threaded application.\n- [Combine](Documentation/Combine.md): Access and observe the database with Combine publishers.\n- [Avoiding SQL Injection](#avoiding-sql-injection)\n- [Error Handling](#error-handling)\n- [Unicode](#unicode)\n- [Memory Management](#memory-management)\n- [Data Protection](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseconnections)\n- :bulb: [Migrating From GRDB 6 to GRDB 7](Documentation/GRDB7MigrationGuide.md)\n- :bulb: [Why Adopt GRDB?](Documentation/WhyAdoptGRDB.md)\n- :bulb: [Recommended Practices for Designing Record Types](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/recordrecommendedpractices)\n\n#### Companion Libraries\n\n- [GRDBQuery](https://github.com/groue/GRDBQuery): Access and observe the database from your SwiftUI views.\n- [GRDBSnapshotTesting](https://github.com/groue/GRDBSnapshotTesting): Test your database. \n\n**[FAQ]**\n\n**[Sample Code](#sample-code)**\n\n\nInstallation\n============\n\n**The installation procedures below have GRDB use the version of SQLite that ships with the target operating system.**\n\nSee [Encryption](#encryption) for the installation procedure of GRDB with SQLCipher.\n\nSee [Custom SQLite builds](Documentation/CustomSQLiteBuilds.md) for the installation procedure of GRDB with a customized build of SQLite.\n\n\n## Swift Package Manager\n\nThe [Swift Package Manager](https://swift.org/package-manager/) automates the distribution of Swift code. To use GRDB with SPM, add a dependency to `https://github.com/groue/GRDB.swift.git`\n\nGRDB offers two libraries, `GRDB` and `GRDB-dynamic`. Pick only one. When in doubt, prefer `GRDB`. The `GRDB-dynamic` library can reveal useful if you are going to link it with multiple targets within your app and only wish to link to a shared, dynamic framework once. See [How to link a Swift Package as dynamic](https://forums.swift.org/t/how-to-link-a-swift-package-as-dynamic/32062) for more information.\n\n> **Note**: Linux support is provided by contributors. It is not automatically tested, and not officially maintained. If you notice a build or runtime failure on Linux, please open a pull request with the necessary fix, thank you!\n\n\n## CocoaPods\n\n[CocoaPods](http://cocoapods.org/) is a dependency manager for Xcode projects. To use GRDB with CocoaPods (version 1.2 or higher), specify in your `Podfile`:\n\n```ruby\npod 'GRDB.swift'\n```\n\nGRDB can be installed as a framework, or a static library.\n\n**Important Note for CocoaPods installation**\n\nDue to an [issue](https://github.com/CocoaPods/CocoaPods/issues/11839) in CocoaPods, it is currently not possible to deploy new versions of GRDB to CocoaPods. The last version available on CocoaPods is 6.24.1. To install later versions of GRDB using CocoaPods, use one of the following workarounds:\n\n- Depend on the `GRDB7` branch. This is more or less equivalent to what `pod 'GRDB.swift', '~> 7.0'` would normally do, if CocoaPods would accept new GRDB versions to be published:\n\n    ```ruby\n    # Can't use semantic versioning due to https://github.com/CocoaPods/CocoaPods/issues/11839\n    pod 'GRDB.swift', git: 'https://github.com/groue/GRDB.swift.git', branch: 'GRDB7'\n    ```\n\n- Depend on a specific version explicitly (Replace the tag with the version you want to use):\n\n    ```ruby\n    # Can't use semantic versioning due to https://github.com/CocoaPods/CocoaPods/issues/11839\n    # Replace the tag with the tag that you want to use.\n    pod 'GRDB.swift', git: 'https://github.com/groue/GRDB.swift.git', tag: 'v6.29.0' \n    ```\n\n## Carthage\n\n[Carthage](https://github.com/Carthage/Carthage) is **unsupported**. For some context about this decision, see [#433](https://github.com/groue/GRDB.swift/issues/433).\n\n\n## Manually\n\n1. [Download](https://github.com/groue/GRDB.swift/releases) a copy of GRDB, or clone its repository and make sure you checkout the latest tagged version.\n\n2. Embed the `GRDB.xcodeproj` project in your own project.\n\n3. Add the `GRDB` target in the **Target Dependencies** section of the **Build Phases** tab of your application target (extension target for WatchOS).\n\n4. Add the `GRDB.framework` to the **Embedded Binaries** section of the **General**  tab of your application target (extension target for WatchOS).\n\n\nDatabase Connections\n====================\n\nGRDB provides two classes for accessing SQLite databases: [`DatabaseQueue`] and [`DatabasePool`]:\n\n```swift\nimport GRDB\n\n// Pick one:\nlet dbQueue = try DatabaseQueue(path: \"/path/to/database.sqlite\")\nlet dbPool = try DatabasePool(path: \"/path/to/database.sqlite\")\n```\n\nThe differences are:\n\n- Database pools allow concurrent database accesses (this can improve the performance of multithreaded applications).\n- Database pools open your SQLite database in the [WAL mode](https://www.sqlite.org/wal.html) (unless read-only).\n- Database queues support [in-memory databases](https://www.sqlite.org/inmemorydb.html).\n\n**If you are not sure, choose [`DatabaseQueue`].** You will always be able to switch to [`DatabasePool`] later.\n\nFor more information and tips when opening connections, see [Database Connections](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseconnections).\n\n\nSQLite API\n==========\n\n**In this section of the documentation, we will talk SQL.** Jump to the [query interface](#the-query-interface) if SQL is not your cup of tea.\n\n- [Executing Updates](#executing-updates)\n- [Fetch Queries](#fetch-queries)\n    - [Fetching Methods](#fetching-methods)\n    - [Row Queries](#row-queries)\n    - [Value Queries](#value-queries)\n- [Values](#values)\n    - [Data](#data-and-memory-savings)\n    - [Date and DateComponents](#date-and-datecomponents)\n    - [NSNumber, NSDecimalNumber, and Decimal](#nsnumber-nsdecimalnumber-and-decimal)\n    - [Swift enums](#swift-enums)\n    - [`DatabaseValueConvertible`]: the protocol for custom value types\n- [Transactions and Savepoints]\n- [SQL Interpolation]\n\nAdvanced topics:\n\n- [Prepared Statements]\n- [Custom SQL Functions and Aggregates](#custom-sql-functions-and-aggregates)\n- [Database Schema Introspection](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseschemaintrospection)\n- [Row Adapters](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/rowadapter)\n- [Raw SQLite Pointers](#raw-sqlite-pointers)\n\n\n## Executing Updates\n\nOnce granted with a [database connection], the [`execute(sql:arguments:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/database/execute(sql:arguments:)) method executes the SQL statements that do not return any database row, such as `CREATE TABLE`, `INSERT`, `DELETE`, `ALTER`, etc.\n\nFor example:\n\n```swift\ntry dbQueue.write { db in\n    try db.execute(sql: \"\"\"\n        CREATE TABLE player (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            name TEXT NOT NULL,\n            score INT)\n        \"\"\")\n    \n    try db.execute(\n        sql: \"INSERT INTO player (name, score) VALUES (?, ?)\",\n        arguments: [\"Barbara\", 1000])\n    \n    try db.execute(\n        sql: \"UPDATE player SET score = :score WHERE id = :id\",\n        arguments: [\"score\": 1000, \"id\": 1])\n    }\n}\n```\n\nThe `?` and colon-prefixed keys like `:score` in the SQL query are the **statements arguments**. You pass arguments with arrays or dictionaries, as in the example above. See [Values](#values) for more information on supported arguments types (Bool, Int, String, Date, Swift enums, etc.), and [`StatementArguments`] for a detailed documentation of SQLite arguments.\n\nYou can also embed query arguments right into your SQL queries, with [`execute(literal:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/database/execute(literal:)), as in the example below. See [SQL Interpolation] for more details.\n\n```swift\ntry dbQueue.write { db in\n    let name = \"O'Brien\"\n    let score = 550\n    try db.execute(literal: \"\"\"\n        INSERT INTO player (name, score) VALUES (\\(name), \\(score))\n        \"\"\")\n}\n```\n\n**Never ever embed values directly in your raw SQL strings**. See [Avoiding SQL Injection](#avoiding-sql-injection) for more information:\n\n```swift\n// WRONG: don't embed values in raw SQL strings\nlet id = 123\nlet name = textField.text\ntry db.execute(\n    sql: \"UPDATE player SET name = '\\(name)' WHERE id = \\(id)\")\n\n// CORRECT: use arguments dictionary\ntry db.execute(\n    sql: \"UPDATE player SET name = :name WHERE id = :id\",\n    arguments: [\"name\": name, \"id\": id])\n\n// CORRECT: use arguments array\ntry db.execute(\n    sql: \"UPDATE player SET name = ? WHERE id = ?\",\n    arguments: [name, id])\n\n// CORRECT: use SQL Interpolation\ntry db.execute(\n    literal: \"UPDATE player SET name = \\(name) WHERE id = \\(id)\")\n```\n\n**Join multiple statements with a semicolon**:\n\n```swift\ntry db.execute(sql: \"\"\"\n    INSERT INTO player (name, score) VALUES (?, ?);\n    INSERT INTO player (name, score) VALUES (?, ?);\n    \"\"\", arguments: [\"Arthur\", 750, \"Barbara\", 1000])\n\ntry db.execute(literal: \"\"\"\n    INSERT INTO player (name, score) VALUES (\\(\"Arthur\"), \\(750));\n    INSERT INTO player (name, score) VALUES (\\(\"Barbara\"), \\(1000));\n    \"\"\")\n```\n\nWhen you want to make sure that a single statement is executed, use a prepared [`Statement`].\n\n**After an INSERT statement**, you can get the row ID of the inserted row with [`lastInsertedRowID`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/database/lastinsertedrowid):\n\n```swift\ntry db.execute(\n    sql: \"INSERT INTO player (name, score) VALUES (?, ?)\",\n    arguments: [\"Arthur\", 1000])\nlet playerId = db.lastInsertedRowID\n```\n\nDon't miss [Records](#records), that provide classic **persistence methods**:\n\n```swift\nvar player = Player(name: \"Arthur\", score: 1000)\ntry player.insert(db)\nlet playerId = player.id\n```\n\n\n## Fetch Queries\n\n[Database connections] let you fetch database rows, plain values, and custom models aka \"records\".\n\n**Rows** are the raw results of SQL queries:\n\n```swift\ntry dbQueue.read { db in\n    if let row = try Row.fetchOne(db, sql: \"SELECT * FROM wine WHERE id = ?\", arguments: [1]) {\n        let name: String = row[\"name\"]\n        let color: Color = row[\"color\"]\n        print(name, color)\n    }\n}\n```\n\n\n**Values** are the Bool, Int, String, Date, Swift enums, etc. stored in row columns:\n\n```swift\ntry dbQueue.read { db in\n    let urls = try URL.fetchCursor(db, sql: \"SELECT url FROM wine\")\n    while let url = try urls.next() {\n        print(url)\n    }\n}\n```\n\n\n**Records** are your application objects that can initialize themselves from rows:\n\n```swift\nlet wines = try dbQueue.read { db in\n    try Wine.fetchAll(db, sql: \"SELECT * FROM wine\")\n}\n```\n\n- [Fetching Methods](#fetching-methods) and [Cursors](#cursors)\n- [Row Queries](#row-queries)\n- [Value Queries](#value-queries)\n- [Records](#records)\n\n\n### Fetching Methods\n\n**Throughout GRDB**, you can always fetch *cursors*, *arrays*, *sets*, or *single values* of any fetchable type (database [row](#row-queries), simple [value](#value-queries), or custom [record](#records)):\n\n```swift\ntry Row.fetchCursor(...) // A Cursor of Row\ntry Row.fetchAll(...)    // [Row]\ntry Row.fetchSet(...)    // Set<Row>\ntry Row.fetchOne(...)    // Row?\n```\n\n- `fetchCursor` returns a **[cursor](#cursors)** over fetched values:\n    \n    ```swift\n    let rows = try Row.fetchCursor(db, sql: \"SELECT ...\") // A Cursor of Row\n    ```\n    \n- `fetchAll` returns an **array**:\n    \n    ```swift\n    let players = try Player.fetchAll(db, sql: \"SELECT ...\") // [Player]\n    ```\n\n- `fetchSet` returns a **set**:\n    \n    ```swift\n    let names = try String.fetchSet(db, sql: \"SELECT ...\") // Set<String>\n    ```\n\n- `fetchOne` returns a **single optional value**, and consumes a single database row (if any).\n    \n    ```swift\n    let count = try Int.fetchOne(db, sql: \"SELECT COUNT(*) ...\") // Int?\n    ```\n\n**All those fetching methods require an SQL string that contains a single SQL statement.** When you want to fetch from multiple statements joined with a semicolon, iterate the multiple [prepared statements] found in the SQL string.\n\n### Cursors\n\nğŸ“– [`Cursor`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/cursor)\n\n**Whenever you consume several rows from the database, you can fetch an Array, a Set, or a Cursor**.\n\nThe `fetchAll()` and `fetchSet()` methods return regular Swift array and sets, that you iterate like all other arrays and sets:\n\n```swift\ntry dbQueue.read { db in\n    // [Player]\n    let players = try Player.fetchAll(db, sql: \"SELECT ...\")\n    for player in players {\n        // use player\n    }\n}\n```\n\nUnlike arrays and sets, cursors returned by `fetchCursor()` load their results step after step:\n\n```swift\ntry dbQueue.read { db in\n    // Cursor of Player\n    let players = try Player.fetchCursor(db, sql: \"SELECT ...\")\n    while let player = try players.next() {\n        // use player\n    }\n}\n```\n\n- **Cursors can not be used on any thread**: you must consume a cursor on the dispatch queue it was created in. Particularly, don't extract a cursor out of a database access method:\n    \n    ```swift\n    // Wrong\n    let cursor = try dbQueue.read { db in\n        try Player.fetchCursor(db, ...)\n    }\n    while let player = try cursor.next() { ... }\n    ```\n    \n    Conversely, arrays and sets may be consumed on any thread:\n    \n    ```swift\n    // OK\n    let array = try dbQueue.read { db in\n        try Player.fetchAll(db, ...)\n    }\n    for player in array { ... }\n    ```\n    \n- **Cursors can be iterated only one time.** Arrays and sets can be iterated many times.\n\n- **Cursors iterate database results in a lazy fashion**, and don't consume much memory. Arrays and sets contain copies of database values, and may take a lot of memory when there are many fetched results.\n\n- **Cursors are granted with direct access to SQLite,** unlike arrays and sets that have to take the time to copy database values. If you look after extra performance, you may prefer cursors.\n\n- **Cursors can feed Swift collections.**\n    \n    You will most of the time use `fetchAll` or `fetchSet` when you want an array or a set. For more specific needs, you may prefer one of the initializers below. All of them accept an extra optional `minimumCapacity` argument which helps optimizing your app when you have an idea of the number of elements in a cursor (the built-in `fetchAll` and `fetchSet` do not perform such an optimization).\n    \n    **Arrays** and all types conforming to `RangeReplaceableCollection`:\n    \n    ```swift\n    // [String]\n    let cursor = try String.fetchCursor(db, ...)\n    let array = try Array(cursor)\n    ```\n    \n    **Sets**:\n    \n    ```swift\n    // Set<Int>\n    let cursor = try Int.fetchCursor(db, ...)\n    let set = try Set(cursor)\n    ```\n    \n    **Dictionaries**:\n    \n    ```swift\n    // [Int64: [Player]]\n    let cursor = try Player.fetchCursor(db)\n    let dictionary = try Dictionary(grouping: cursor, by: { $0.teamID })\n    \n    // [Int64: Player]\n    let cursor = try Player.fetchCursor(db).map { ($0.id, $0) }\n    let dictionary = try Dictionary(uniqueKeysWithValues: cursor)\n    ```\n\n- **Cursors adopt the [Cursor](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/cursor) protocol, which looks a lot like standard [lazy sequences](https://developer.apple.com/reference/swift/lazysequenceprotocol) of Swift.** As such, cursors come with many convenience methods: `compactMap`, `contains`, `dropFirst`, `dropLast`, `drop(while:)`, `enumerated`, `filter`, `first`, `flatMap`, `forEach`, `joined`, `joined(separator:)`, `max`, `max(by:)`, `min`, `min(by:)`, `map`, `prefix`, `prefix(while:)`, `reduce`, `reduce(into:)`, `suffix`:\n    \n    ```swift\n    // Prints all Github links\n    try URL\n        .fetchCursor(db, sql: \"SELECT url FROM link\")\n        .filter { url in url.host == \"github.com\" }\n        .forEach { url in print(url) }\n    \n    // An efficient cursor of coordinates:\n    let locations = try Row.\n        .fetchCursor(db, sql: \"SELECT latitude, longitude FROM place\")\n        .map { row in\n            CLLocationCoordinate2D(latitude: row[0], longitude: row[1])\n        }\n    ```\n\n- **Cursors are not Swift sequences.** That's because Swift sequences can't handle iteration errors, when reading SQLite results may fail at any time.\n\n- **Cursors require a little care**:\n    \n    - Don't modify the results during a cursor iteration:\n        \n        ```swift\n        // Undefined behavior\n        while let player = try players.next() {\n            try db.execute(sql: \"DELETE ...\")\n        }\n        ```\n    \n    - Don't turn a cursor of `Row` into an array or a set. You would not get the distinct rows you expect. To get a array of rows, use `Row.fetchAll(...)`. To get a set of rows, use `Row.fetchSet(...)`. Generally speaking, make sure you copy a row whenever you extract it from a cursor for later use: `row.copy()`.\n\nIf you don't see, or don't care about the difference, use arrays. If you care about memory and performance, use cursors when appropriate.\n\n\n### Row Queries\n\n- [Fetching Rows](#fetching-rows)\n- [Column Values](#column-values)\n- [DatabaseValue](#databasevalue)\n- [Rows as Dictionaries](#rows-as-dictionaries)\n- ğŸ“– [`Row`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/row)\n\n\n#### Fetching Rows\n\nFetch **cursors** of rows, **arrays**, **sets**, or **single** rows (see [fetching methods](#fetching-methods)):\n\n```swift\ntry dbQueue.read { db in\n    try Row.fetchCursor(db, sql: \"SELECT ...\", arguments: ...) // A Cursor of Row\n    try Row.fetchAll(db, sql: \"SELECT ...\", arguments: ...)    // [Row]\n    try Row.fetchSet(db, sql: \"SELECT ...\", arguments: ...)    // Set<Row>\n    try Row.fetchOne(db, sql: \"SELECT ...\", arguments: ...)    // Row?\n    \n    let rows = try Row.fetchCursor(db, sql: \"SELECT * FROM wine\")\n    while let row = try rows.next() {\n        let name: String = row[\"name\"]\n        let color: Color = row[\"color\"]\n        print(name, color)\n    }\n}\n\nlet rows = try dbQueue.read { db in\n    try Row.fetchAll(db, sql: \"SELECT * FROM player\")\n}\n```\n\nArguments are optional arrays or dictionaries that fill the positional `?` and colon-prefixed keys like `:name` in the query:\n\n```swift\nlet rows = try Row.fetchAll(db,\n    sql: \"SELECT * FROM player WHERE name = ?\",\n    arguments: [\"Arthur\"])\n\nlet rows = try Row.fetchAll(db,\n    sql: \"SELECT * FROM player WHERE name = :name\",\n    arguments: [\"name\": \"Arthur\"])\n```\n\nSee [Values](#values) for more information on supported arguments types (Bool, Int, String, Date, Swift enums, etc.), and [`StatementArguments`] for a detailed documentation of SQLite arguments.\n\nUnlike row arrays that contain copies of the database rows, row cursors are close to the SQLite metal, and require a little care:\n\n> **Note**: **Don't turn a cursor of `Row` into an array or a set**. You would not get the distinct rows you expect. To get a array of rows, use `Row.fetchAll(...)`. To get a set of rows, use `Row.fetchSet(...)`. Generally speaking, make sure you copy a row whenever you extract it from a cursor for later use: `row.copy()`.\n\n\n#### Column Values\n\n**Read column values** by index or column name:\n\n```swift\nlet name: String = row[0]      // 0 is the leftmost column\nlet name: String = row[\"name\"] // Leftmost matching column - lookup is case-insensitive\nlet name: String = row[Column(\"name\")] // Using query interface's Column\n```\n\nMake sure to ask for an optional when the value may be NULL:\n\n```swift\nlet name: String? = row[\"name\"]\n```\n\nThe `row[]` subscript returns the type you ask for. See [Values](#values) for more information on supported value types:\n\n```swift\nlet bookCount: Int     = row[\"bookCount\"]\nlet bookCount64: Int64 = row[\"bookCount\"]\nlet hasBooks: Bool     = row[\"bookCount\"] // false when 0\n\nlet string: String     = row[\"date\"]      // \"2015-09-11 18:14:15.123\"\nlet date: Date         = row[\"date\"]      // Date\nself.date = row[\"date\"] // Depends on the type of the property.\n```\n\nYou can also use the `as` type casting operator:\n\n```swift\nrow[...] as Int\nrow[...] as Int?\n```\n\nThrowing accessors exist as well. Their use is not encouraged, because a database decoding error is a programming error. If an application stores invalid data in the database file, that is a bug that needs to be fixed:\n\n```swift\nlet name = try row.decode(String.self, atIndex: 0)\nlet bookCount = try row.decode(Int.self, forColumn: \"bookCount\")\n```\n\n> **Warning**: avoid the `as!` and `as?` operators:\n> \n> ```swift\n> if let int = row[...] as? Int { ... } // BAD - doesn't work\n> if let int = row[...] as Int? { ... } // GOOD\n> ```\n\n> **Warning**: avoid nil-coalescing row values, and prefer the `coalesce` method instead:\n>\n> ```swift\n> let name: String? = row[\"nickname\"] ?? row[\"name\"]     // BAD - doesn't work\n> let name: String? = row.coalesce([\"nickname\", \"name\"]) // GOOD\n> ```\n\nGenerally speaking, you can extract the type you need, provided it can be converted from the underlying SQLite value:\n\n- **Successful conversions include:**\n    \n    - All numeric SQLite values to all numeric Swift types, and Bool (zero is the only false boolean).\n    - Text SQLite values to Swift String.\n    - Blob SQLite values to Foundation Data.\n    \n    See [Values](#values) for more information on supported types (Bool, Int, String, Date, Swift enums, etc.)\n    \n- **NULL returns nil.**\n    \n    ```swift\n    let row = try Row.fetchOne(db, sql: \"SELECT NULL\")!\n    row[0] as Int? // nil\n    row[0] as Int  // fatal error: could not convert NULL to Int.\n    ```\n    \n    There is one exception, though: the [DatabaseValue](#databasevalue) type:\n    \n    ```swift\n    row[0] as DatabaseValue // DatabaseValue.null\n    ```\n    \n- **Missing columns return nil.**\n    \n    ```swift\n    let row = try Row.fetchOne(db, sql: \"SELECT 'foo' AS foo\")!\n    row[\"missing\"] as String? // nil\n    row[\"missing\"] as String  // fatal error: no such column: missing\n    ```\n    \n    You can explicitly check for a column presence with the `hasColumn` method.\n\n- **Invalid conversions throw a fatal error.**\n    \n    ```swift\n    let row = try Row.fetchOne(db, sql: \"SELECT 'Momâ€™s birthday'\")!\n    row[0] as String // \"Momâ€™s birthday\"\n    row[0] as Date?  // fatal error: could not convert \"Momâ€™s birthday\" to Date.\n    row[0] as Date   // fatal error: could not convert \"Momâ€™s birthday\" to Date.\n    \n    let row = try Row.fetchOne(db, sql: \"SELECT 256\")!\n    row[0] as Int    // 256\n    row[0] as UInt8? // fatal error: could not convert 256 to UInt8.\n    row[0] as UInt8  // fatal error: could not convert 256 to UInt8.\n    ```\n    \n    Those conversion fatal errors can be avoided with the [DatabaseValue](#databasevalue) type:\n    \n    ```swift\n    let row = try Row.fetchOne(db, sql: \"SELECT 'Momâ€™s birthday'\")!\n    let dbValue: DatabaseValue = row[0]\n    if dbValue.isNull {\n        // Handle NULL\n    } else if let date = Date.fromDatabaseValue(dbValue) {\n        // Handle valid date\n    } else {\n        // Handle invalid date\n    }\n    ```\n    \n    This extra verbosity is the consequence of having to deal with an untrusted database: you may consider fixing the content of your database instead. See [Fatal Errors](#fatal-errors) for more information.\n    \n- **SQLite has a weak type system, and provides [convenience conversions](https://www.sqlite.org/c3ref/column_blob.html) that can turn String to Int, Double to Blob, etc.**\n    \n    GRDB will sometimes let those conversions go through:\n    \n    ```swift\n    let rows = try Row.fetchCursor(db, sql: \"SELECT '20 small cigars'\")\n    while let row = try rows.next() {\n        row[0] as Int   // 20\n    }\n    ```\n    \n    Don't freak out: those conversions did not prevent SQLite from becoming the immensely successful database engine you want to use. And GRDB adds safety checks described just above. You can also prevent those convenience conversions altogether by using the [DatabaseValue](#databasevalue) type.\n\n\n#### DatabaseValue\n\nğŸ“– [`DatabaseValue`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasevalue)\n\n**`DatabaseValue` is an intermediate type between SQLite and your values, which gives information about the raw value stored in the database.**\n\nYou get `DatabaseValue` just like other value types:\n\n```swift\nlet dbValue: DatabaseValue = row[0]\nlet dbValue: DatabaseValue? = row[\"name\"] // nil if and only if column does not exist\n\n// Check for NULL:\ndbValue.isNull // Bool\n\n// The stored value:\ndbValue.storage.value // Int64, Double, String, Data, or nil\n\n// All the five storage classes supported by SQLite:\nswitch dbValue.storage {\ncase .null:                 print(\"NULL\")\ncase .int64(let int64):     print(\"Int64: \\(int64)\")\ncase .double(let double):   print(\"Double: \\(double)\")\ncase .string(let string):   print(\"String: \\(string)\")\ncase .blob(let data):       print(\"Data: \\(data)\")\n}\n```\n\nYou can extract regular [values](#values) (Bool, Int, String, Date, Swift enums, etc.) from `DatabaseValue` with the [fromDatabaseValue()](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasevalueconvertible/fromdatabasevalue(_:)-21zzv) method:\n\n```swift\nlet dbValue: DatabaseValue = row[\"bookCount\"]\nlet bookCount   = Int.fromDatabaseValue(dbValue)   // Int?\nlet bookCount64 = Int64.fromDatabaseValue(dbValue) // Int64?\nlet hasBooks    = Bool.fromDatabaseValue(dbValue)  // Bool?, false when 0\n\nlet dbValue: DatabaseValue = row[\"date\"]\nlet string = String.fromDatabaseValue(dbValue)     // \"2015-09-11 18:14:15.123\"\nlet date   = Date.fromDatabaseValue(dbValue)       // Date?\n```\n\n`fromDatabaseValue` returns nil for invalid conversions:\n\n```swift\nlet row = try Row.fetchOne(db, sql: \"SELECT 'Momâ€™s birthday'\")!\nlet dbValue: DatabaseValue = row[0]\nlet string = String.fromDatabaseValue(dbValue) // \"Momâ€™s birthday\"\nlet int    = Int.fromDatabaseValue(dbValue)    // nil\nlet date   = Date.fromDatabaseValue(dbValue)   // nil\n```\n\n\n#### Rows as Dictionaries\n\nRow adopts the standard [RandomAccessCollection](https://developer.apple.com/documentation/swift/randomaccesscollection) protocol, and can be seen as a dictionary of [DatabaseValue](#databasevalue):\n\n```swift\n// All the (columnName, dbValue) tuples, from left to right:\nfor (columnName, dbValue) in row {\n    ...\n}\n```\n\n**You can build rows from dictionaries** (standard Swift dictionaries and NSDictionary). See [Values](#values) for more information on supported types:\n\n```swift\nlet row: Row = [\"name\": \"foo\", \"date\": nil]\nlet row = Row([\"name\": \"foo\", \"date\": nil])\nlet row = Row(/* [AnyHashable: Any] */) // nil if invalid dictionary\n```\n\nYet rows are not real dictionaries: they may contain duplicate columns:\n\n```swift\nlet row = try Row.fetchOne(db, sql: \"SELECT 1 AS foo, 2 AS foo\")!\nrow.columnNames    // [\"foo\", \"foo\"]\nrow.databaseValues // [1, 2]\nrow[\"foo\"]         // 1 (leftmost matching column)\nfor (columnName, dbValue) in row { ... } // (\"foo\", 1), (\"foo\", 2)\n```\n\n**When you build a dictionary from a row**, you have to disambiguate identical columns, and choose how to present database values. For example:\n\n- A `[String: DatabaseValue]` dictionary that keeps leftmost value in case of duplicated column name:\n\n    ```swift\n    let dict = Dictionary(row, uniquingKeysWith: { (left, _) in left })\n    ```\n\n- A `[String: AnyObject]` dictionary which keeps rightmost value in case of duplicated column name. This dictionary is identical to FMResultSet's resultDictionary from FMDB. It contains NSNull values for null columns, and can be shared with Objective-C:\n\n    ```swift\n    let dict = Dictionary(\n        row.map { (column, dbValue) in\n            (column, dbValue.storage.value as AnyObject)\n        },\n        uniquingKeysWith: { (_, right) in right })\n    ```\n\n- A `[String: Any]` dictionary that can feed, for example, JSONSerialization:\n    \n    ```swift\n    let dict = Dictionary(\n        row.map { (column, dbValue) in\n            (column, dbValue.storage.value)\n        },\n        uniquingKeysWith: { (left, _) in left })\n    ```\n\nSee the documentation of [`Dictionary.init(_:uniquingKeysWith:)`](https://developer.apple.com/documentation/swift/dictionary/2892961-init) for more information.\n\n\n### Value Queries\n\nğŸ“– [`DatabaseValueConvertible`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasevalueconvertible)\n\n**Instead of rows, you can directly fetch values.** There are many supported [value types](#values) (Bool, Int, String, Date, Swift enums, etc.).\n\nLike rows, fetch values as **cursors**, **arrays**, **sets**, or **single** values (see [fetching methods](#fetching-methods)). Values are extracted from the leftmost column of the SQL queries:\n\n```swift\ntry dbQueue.read { db in\n    try Int.fetchCursor(db, sql: \"SELECT ...\", arguments: ...) // A Cursor of Int\n    try Int.fetchAll(db, sql: \"SELECT ...\", arguments: ...)    // [Int]\n    try Int.fetchSet(db, sql: \"SELECT ...\", arguments: ...)    // Set<Int>\n    try Int.fetchOne(db, sql: \"SELECT ...\", arguments: ...)    // Int?\n    \n    let maxScore = try Int.fetchOne(db, sql: \"SELECT MAX(score) FROM player\") // Int?\n    let names = try String.fetchAll(db, sql: \"SELECT name FROM player\")       // [String]\n}\n```\n\n`Int.fetchOne` returns nil in two cases: either the SELECT statement yielded no row, or one row with a NULL value:\n\n```swift\n// No row:\ntry Int.fetchOne(db, sql: \"SELECT 42 WHERE FALSE\") // nil\n\n// One row with a NULL value:\ntry Int.fetchOne(db, sql: \"SELECT NULL\")           // nil\n\n// One row with a non-NULL value:\ntry Int.fetchOne(db, sql: \"SELECT 42\")             // 42\n```\n\nFor requests which may contain NULL, fetch optionals:\n\n```swift\ntry dbQueue.read { db in\n    try Optional<Int>.fetchCursor(db, sql: \"SELECT ...\", arguments: ...) // A Cursor of Int?\n    try Optional<Int>.fetchAll(db, sql: \"SELECT ...\", arguments: ...)    // [Int?]\n    try Optional<Int>.fetchSet(db, sql: \"SELECT ...\", arguments: ...)    // Set<Int?>\n}\n```\n\n> :bulb: **Tip**: One advanced use case, when you fetch one value, is to distinguish the cases of a statement that yields no row, or one row with a NULL value. To do so, use `Optional<Int>.fetchOne`, which returns a double optional `Int??`:\n> \n> ```swift\n> // No row:\n> try Optional<Int>.fetchOne(db, sql: \"SELECT 42 WHERE FALSE\") // .none\n> // One row with a NULL value:\n> try Optional<Int>.fetchOne(db, sql: \"SELECT NULL\")           // .some(.none)\n> // One row with a non-NULL value:\n> try Optional<Int>.fetchOne(db, sql: \"SELECT 42\")             // .some(.some(42))\n> ```\n\nThere are many supported value types (Bool, Int, String, Date, Swift enums, etc.). See [Values](#values) for more information.\n\n\n## Values\n\nGRDB ships with built-in support for the following value types:\n\n- **Swift Standard Library**: Bool, Double, Float, all signed and unsigned integer types, String, [Swift enums](#swift-enums).\n    \n- **Foundation**: [Data](#data-and-memory-savings), [Date](#date-and-datecomponents), [DateComponents](#date-and-datecomponents), [Decimal](#nsnumber-nsdecimalnumber-and-decimal), NSNull, [NSNumber](#nsnumber-nsdecimalnumber-and-decimal), NSString, URL, [UUID](#uuid).\n    \n- **CoreGraphics**: CGFloat.\n\n- **[DatabaseValue](#databasevalue)**, the type which gives information about the raw value stored in the database.\n\n- **Full-Text Patterns**: [FTS3Pattern](Documentation/FullTextSearch.md#fts3pattern) and [FTS5Pattern](Documentation/FullTextSearch.md#fts5pattern).\n\n- Generally speaking, all types that adopt the [`DatabaseValueConvertible`] protocol.\n\nValues can be used as [statement arguments](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statementarguments):\n\n```swift\nlet url: URL = ...\nlet verified: Bool = ...\ntry db.execute(\n    sql: \"INSERT INTO link (url, verified) VALUES (?, ?)\",\n    arguments: [url, verified])\n```\n\nValues can be [extracted from rows](#column-values):\n\n```swift\nlet rows = try Row.fetchCursor(db, sql: \"SELECT * FROM link\")\nwhile let row = try rows.next() {\n    let url: URL = row[\"url\"]\n    let verified: Bool = row[\"verified\"]\n}\n```\n\nValues can be [directly fetched](#value-queries):\n\n```swift\nlet urls = try URL.fetchAll(db, sql: \"SELECT url FROM link\")  // [URL]\n```\n\nUse values in [Records](#records):\n\n```swift\nstruct Link: FetchableRecord {\n    var url: URL\n    var isVerified: Bool\n    \n    init(row: Row) {\n        url = row[\"url\"]\n        isVerified = row[\"verified\"]\n    }\n}\n```\n\nUse values in the [query interface](#the-query-interface):\n\n```swift\nlet url: URL = ...\nlet link = try Link.filter { $0.url == url }.fetchOne(db)\n```\n\n\n### Data (and Memory Savings)\n\n**Data** suits the BLOB SQLite columns. It can be stored and fetched from the database just like other [values](#values):\n\n```swift\nlet rows = try Row.fetchCursor(db, sql: \"SELECT data, ...\")\nwhile let row = try rows.next() {\n    let data: Data = row[\"data\"]\n}\n```\n\nAt each step of the request iteration, the `row[]` subscript creates *two copies* of the database bytes: one fetched by SQLite, and another, stored in the Swift Data value.\n\n**You have the opportunity to save memory** by not copying the data fetched by SQLite:\n\n```swift\nwhile let row = try rows.next() {\n    try row.withUnsafeData(name: \"data\") { (data: Data?) in\n        ...\n    }\n}\n```\n\nThe non-copied data does not live longer than the iteration step: make sure that you do not use it past this point.\n\n\n### Date and DateComponents\n\n[**Date**](#date) and [**DateComponents**](#datecomponents) can be stored and fetched from the database.\n\nHere is how GRDB supports the various [date formats](https://www.sqlite.org/lang_datefunc.html) supported by SQLite:\n\n| SQLite format                | Date               | DateComponents |\n|:---------------------------- |:------------------:|:--------------:|\n| YYYY-MM-DD                   |       Read Â¹       | Read / Write   |\n| YYYY-MM-DD HH:MM             |       Read Â¹ Â²     | Read Â² / Write |\n| YYYY-MM-DD HH:MM:SS          |       Read Â¹ Â²     | Read Â² / Write |\n| YYYY-MM-DD HH:MM:SS.SSS      | Read Â¹ Â² / Write Â¹ | Read Â² / Write |\n| YYYY-MM-DD**T**HH:MM         |       Read Â¹ Â²     |      Read Â²    |\n| YYYY-MM-DD**T**HH:MM:SS      |       Read Â¹ Â²     |      Read Â²    |\n| YYYY-MM-DD**T**HH:MM:SS.SSS  |       Read Â¹ Â²     |      Read Â²    |\n| HH:MM                        |                    | Read Â² / Write |\n| HH:MM:SS                     |                    | Read Â² / Write |\n| HH:MM:SS.SSS                 |                    | Read Â² / Write |\n| Timestamps since unix epoch  |       Read Â³       |                |\n| `now`                        |                    |                |\n\nÂ¹ Missing components are assumed to be zero. Dates are stored and read in the UTC time zone, unless the format is followed by a timezone indicator â½Â²â¾.\n\nÂ² This format may be optionally followed by a timezone indicator of the form `[+-]HH:MM` or just `Z`.\n\nÂ³ GRDB 2+ interprets numerical values as timestamps that fuel `Date(timeIntervalSince1970:)`. Previous GRDB versions used to interpret numbers as [julian days](https://en.wikipedia.org/wiki/Julian_day). Julian days are still supported, with the `Date(julianDay:)` initializer.\n\n> **Warning**: the range of valid years in the SQLite date formats is 0000-9999. You will need to pick another date format when your application needs to process years outside of this range. See the following chapters.\n\n\n#### Date\n\n**Date** can be stored and fetched from the database just like other [values](#values):\n\n```swift\ntry db.execute(\n    sql: \"INSERT INTO player (creationDate, ...) VALUES (?, ...)\",\n    arguments: [Date(), ...])\n\nlet row = try Row.fetchOne(db, ...)!\nlet creationDate: Date = row[\"creationDate\"]\n```\n\nDates are stored using the format \"YYYY-MM-DD HH:MM:SS.SSS\" in the UTC time zone. It is precise to the millisecond.\n\n> **Note**: this format was chosen because it is the only format that is:\n> \n> - Comparable (`ORDER BY date` works)\n> - Comparable with the SQLite keyword CURRENT_TIMESTAMP (`WHERE date > CURRENT_TIMESTAMP` works)\n> - Able to feed [SQLite date & time functions](https://www.sqlite.org/lang_datefunc.html)\n> - Precise enough\n>\n> **Warning**: the range of valid years in the SQLite date format is 0000-9999. You will experience problems with years outside of this range, such as decoding errors, or invalid date computations with [SQLite date & time functions](https://www.sqlite.org/lang_datefunc.html).\n\nSome applications may prefer another date format:\n\n- Some may prefer ISO-8601, with a `T` separator.\n- Some may prefer ISO-8601, with a time zone.\n- Some may need to store years beyond the 0000-9999 range.\n- Some may need sub-millisecond precision.\n- Some may need exact `Date` roundtrip.\n- Etc.\n\n**You should think twice before choosing a different date format:**\n\n- ISO-8601 is about *exchange and communication*, when SQLite is about *storage and data manipulation*. Sharing the same representation in your database and in JSON files only provides a superficial convenience, and should be the least of your priorities. Don't store dates as ISO-8601 without understanding what you lose. For example, ISO-8601 time zones forbid database-level date comparison. \n- Sub-millisecond precision and exact `Date` roundtrip are not as obvious needs as it seems at first sight. Dates generally don't precisely roundtrip as soon as they leave your application anyway, because the other systems your app communicates with use their own date representation (the Android version of your app, the server your application is talking to, etc.) On top of that, `Date` comparison is at least as hard and nasty as [floating point comparison](https://www.google.com/search?q=floating+point+comparison+is+hard).\n\nThe customization of date format is explicit. For example:\n\n```swift\nlet date = Date()\nlet timeInterval = date.timeIntervalSinceReferenceDate\ntry db.execute(\n    sql: \"INSERT INTO player (creationDate, ...) VALUES (?, ...)\",\n    arguments: [timeInterval, ...])\n\nif let row = try Row.fetchOne(db, ...) {\n    let timeInterval: TimeInterval = row[\"creationDate\"]\n    let creationDate = Date(timeIntervalSinceReferenceDate: timeInterval)\n}\n```\n\nSee also [Codable Records] for more date customization options, and [`DatabaseValueConvertible`] if you want to define a Date-wrapping type with customized database representation.\n\n\n#### DateComponents\n\nDateComponents is indirectly supported, through the **DatabaseDateComponents** helper type.\n\nDatabaseDateComponents reads date components from all [date formats supported by SQLite](https://www.sqlite.org/lang_datefunc.html), and stores them in the format of your choice, from HH:MM to YYYY-MM-DD HH:MM:SS.SSS.\n\n> **Warning**: the range of valid years is 0000-9999. You will experience problems with years outside of this range, such as decoding errors, or invalid date computations with [SQLite date & time functions](https://www.sqlite.org/lang_datefunc.html). See [Date](#date) for more information.\n\nDatabaseDateComponents can be stored and fetched from the database just like other [values](#values):\n\n```swift\nlet components = DateComponents()\ncomponents.year = 1973\ncomponents.month = 9\ncomponents.day = 18\n\n// Store \"1973-09-18\"\nlet dbComponents = DatabaseDateComponents(components, format: .YMD)\ntry db.execute(\n    sql: \"INSERT INTO player (birthDate, ...) VALUES (?, ...)\",\n    arguments: [dbComponents, ...])\n\n// Read \"1973-09-18\"\nlet row = try Row.fetchOne(db, sql: \"SELECT birthDate ...\")!\nlet dbComponents: DatabaseDateComponents = row[\"birthDate\"]\ndbComponents.format         // .YMD (the actual format found in the database)\ndbComponents.dateComponents // DateComponents\n```\n\n\n### NSNumber, NSDecimalNumber, and Decimal\n\n**NSNumber** and **Decimal** can be stored and fetched from the database just like other [values](#values).\n\nHere is how GRDB supports the various data types supported by SQLite:\n\n|                 |    Integer   |     Double   |    String    |\n|:--------------- |:------------:|:------------:|:------------:|\n| NSNumber        | Read / Write | Read / Write |     Read     |\n| NSDecimalNumber | Read / Write | Read / Write |     Read     |\n| Decimal         |     Read     |     Read     | Read / Write |\n\n- All three types can decode database integers and doubles:\n\n    ```swift\n    let number = try NSNumber.fetchOne(db, sql: \"SELECT 10\")            // NSNumber\n    let number = try NSDecimalNumber.fetchOne(db, sql: \"SELECT 1.23\")   // NSDecimalNumber\n    let number = try Decimal.fetchOne(db, sql: \"SELECT -100\")           // Decimal\n    ```\n    \n- All three types decode database strings as decimal numbers:\n\n    ```swift\n    let number = try NSNumber.fetchOne(db, sql: \"SELECT '10'\")          // NSDecimalNumber (sic)\n    let number = try NSDecimalNumber.fetchOne(db, sql: \"SELECT '1.23'\") // NSDecimalNumber\n    let number = try Decimal.fetchOne(db, sql: \"SELECT '-100'\")         // Decimal\n    ```\n\n- `NSNumber` and `NSDecimalNumber` send 64-bit signed integers and doubles in the database:\n\n    ```swift\n    // INSERT INTO transfer VALUES (10)\n    try db.execute(sql: \"INSERT INTO transfer VALUES (?)\", arguments: [NSNumber(value: 10)])\n    \n    // INSERT INTO transfer VALUES (10.0)\n    try db.execute(sql: \"INSERT INTO transfer VALUES (?)\", arguments: [NSNumber(value: 10.0)])\n    \n    // INSERT INTO transfer VALUES (10)\n    try db.execute(sql: \"INSERT INTO transfer VALUES (?)\", arguments: [NSDecimalNumber(string: \"10.0\")])\n    \n    // INSERT INTO transfer VALUES (10.5)\n    try db.execute(sql: \"INSERT INTO transfer VALUES (?)\", arguments: [NSDecimalNumber(string: \"10.5\")])\n    ```\n    \n    > **Warning**: since SQLite does not support decimal numbers, sending a non-integer `NSDecimalNumber` can result in a loss of precision during the conversion to double.\n    >\n    > Instead of sending non-integer `NSDecimalNumber` to the database, you may prefer:\n    >\n    > - Send `Decimal` instead (those store decimal strings in the database).\n    > - Send integers instead (for example, store amounts of cents instead of amounts of Euros).\n\n- `Decimal` sends decimal strings in the database:\n\n    ```swift\n    // INSERT INTO transfer VALUES ('10')\n    try db.execute(sql: \"INSERT INTO transfer VALUES (?)\", arguments: [Decimal(10)])\n    \n    // INSERT INTO transfer VALUES ('10.5')\n    try db.execute(sql: \"INSERT INTO transfer VALUES (?)\", arguments: [Decimal(string: \"10.5\")!])\n    ```\n\n\n### UUID\n\n**UUID** can be stored and fetched from the database just like other [values](#values).\n\nGRDB stores uuids as 16-bytes data blobs, and decodes them from both 16-bytes data blobs and strings such as \"E621E1F8-C36C-495A-93FC-0C247A3E6E5F\".\n\n\n### Swift Enums\n\n**Swift enums** and generally all types that adopt the [RawRepresentable](https://developer.apple.com/library/tvos/documentation/Swift/Reference/Swift_RawRepresentable_Protocol/index.html) protocol can be stored and fetched from the database just like their raw [values](#values):\n\n```swift\nenum Color : Int {\n    case red, white, rose\n}\n\nenum Grape : String {\n    case chardonnay, merlot, riesling\n}\n\n// Declare empty DatabaseValueConvertible adoption\nextension Color : DatabaseValueConvertible { }\nextension Grape : DatabaseValueConvertible { }\n\n// Store\ntry db.execute(\n    sql: \"INSERT INTO wine (grape, color) VALUES (?, ?)\",\n    arguments: [Grape.merlot, Color.red])\n\n// Read\nlet rows = try Row.fetchCursor(db, sql: \"SELECT * FROM wine\")\nwhile let row = try rows.next() {\n    let grape: Grape = row[\"grape\"]\n    let color: Color = row[\"color\"]\n}\n```\n\n**When a database value does not match any enum case**, you get a fatal error. This fatal error can be avoided with the [DatabaseValue](#databasevalue) type:\n\n```swift\nlet row = try Row.fetchOne(db, sql: \"SELECT 'syrah'\")!\n\nrow[0] as String  // \"syrah\"\nrow[0] as Grape?  // fatal error: could not convert \"syrah\" to Grape.\nrow[0] as Grape   // fatal error: could not convert \"syrah\" to Grape.\n\nlet dbValue: DatabaseValue = row[0]\nif dbValue.isNull {\n    // Handle NULL\n} else if let grape = Grape.fromDatabaseValue(dbValue) {\n    // Handle valid grape\n} else {\n    // Handle unknown grape\n}\n```\n\n\n## Custom SQL Functions and Aggregates\n\n**SQLite lets you define SQL functions and aggregates.**\n\nA custom SQL function or aggregate extends SQLite:\n\n```sql\nSELECT reverse(name) FROM player;   -- custom function\nSELECT maxLength(name) FROM player; -- custom aggregate\n```\n\n- [Custom SQL Functions](#custom-sql-functions)\n- [Custom Aggregates](#custom-aggregates)\n\n\n### Custom SQL Functions\n\nğŸ“– [`DatabaseFunction`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasefunction)\n\nA *function* argument takes an array of [DatabaseValue](#databasevalue), and returns any valid [value](#values) (Bool, Int, String, Date, Swift enums, etc.) The number of database values is guaranteed to be *argumentCount*.\n\nSQLite has the opportunity to perform additional optimizations when functions are \"pure\", which means that their result only depends on their arguments. So make sure to set the *pure* argument to true when possible.\n\n```swift\nlet reverse = DatabaseFunction(\"reverse\", argumentCount: 1, pure: true) { (values: [DatabaseValue]) in\n    // Extract string value, if any...\n    guard let string = String.fromDatabaseValue(values[0]) else {\n        return nil\n    }\n    // ... and return reversed string:\n    return String(string.reversed())\n}\n```\n\nYou make a function available to a database connection through its configuration:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    db.add(function: reverse)\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n\ntry dbQueue.read { db in\n    // \"oof\"\n    try String.fetchOne(db, sql: \"SELECT reverse('foo')\")!\n}\n```\n\n\n**Functions can take a variable number of arguments:**\n\nWhen you don't provide any explicit *argumentCount*, the function can take any number of arguments:\n\n```swift\nlet averageOf = DatabaseFunction(\"averageOf\", pure: true) { (values: [DatabaseValue]) in\n    let doubles = values.compactMap { Double.fromDatabaseValue($0) }\n    return doubles.reduce(0, +) / Double(doubles.count)\n}\ndb.add(function: averageOf)\n\n// 2.0\ntry Double.fetchOne(db, sql: \"SELECT averageOf(1, 2, 3)\")!\n```\n\n\n**Functions can throw:**\n\n```swift\nlet sqrt = DatabaseFunction(\"sqrt\", argumentCount: 1, pure: true) { (values: [DatabaseValue]) in\n    guard let double = Double.fromDatabaseValue(values[0]) else {\n        return nil\n    }\n    guard double >= 0 else {\n        throw DatabaseError(message: \"invalid negative number\")\n    }\n    return sqrt(double)\n}\ndb.add(function: sqrt)\n\n// SQLite error 1 with statement `SELECT sqrt(-1)`: invalid negative number\ntry Double.fetchOne(db, sql: \"SELECT sqrt(-1)\")!\n```\n\n\n**Use custom functions in the [query interface](#the-query-interface):**\n\n```swift\n// SELECT reverseString(\"name\") FROM player\nPlayer.select { reverseString($0.name) }\n```\n\n\n**GRDB ships with built-in SQL functions that perform unicode-aware string transformations.** See [Unicode](#unicode).\n\n\n### Custom Aggregates\n\nğŸ“– [`DatabaseFunction`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasefunction), [`DatabaseAggregate`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseaggregate)\n\nBefore registering a custom aggregate, you need to define a type that adopts the `DatabaseAggregate` protocol:\n\n```swift\nprotocol DatabaseAggregate {\n    // Initializes an aggregate\n    init()\n    \n    // Called at each step of the aggregation\n    mutating func step(_ dbValues: [DatabaseValue]) throws\n    \n    // Returns the final result\n    func finalize() throws -> DatabaseValueConvertible?\n}\n```\n\nFor example:\n\n```swift\nstruct MaxLength : DatabaseAggregate {\n    var maxLength: Int = 0\n    \n    mutating func step(_ dbValues: [DatabaseValue]) {\n        // At each step, extract string value, if any...\n        guard let string = String.fromDatabaseValue(dbValues[0]) else {\n            return\n        }\n        // ... and update the result\n        let length = string.count\n        if length > maxLength {\n            maxLength = length\n        }\n    }\n    \n    func finalize() -> DatabaseValueConvertible? {\n        maxLength\n    }\n}\n\nlet maxLength = DatabaseFunction(\n    \"maxLength\",\n    argumentCount: 1,\n    pure: true,\n    aggregate: MaxLength.self)\n```\n\nLike [custom SQL Functions](#custom-sql-functions), you make an aggregate function available to a database connection through its configuration:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    db.add(function: maxLength)\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n\ntry dbQueue.read { db in\n    // Some Int\n    try Int.fetchOne(db, sql: \"SELECT maxLength(name) FROM player\")!\n}\n```\n\nThe `step` method of the aggregate takes an array of [DatabaseValue](#databasevalue). This array contains as many values as the *argumentCount* parameter (or any number of values, when *argumentCount* is omitted).\n\nThe `finalize` method of the aggregate returns the final aggregated [value](#values) (Bool, Int, String, Date, Swift enums, etc.).\n\nSQLite has the opportunity to perform additional optimizations when aggregates are \"pure\", which means that their result only depends on their inputs. So make sure to set the *pure* argument to true when possible.\n\n\n**Use custom aggregates in the [query interface](#the-query-interface):**\n\n```swift\n// SELECT maxLength(\"name\") FROM player\nlet request = Player.select { maxLength($0.name) }\ntry Int.fetchOne(db, request) // Int?\n```\n\n\n## Raw SQLite Pointers\n\n**If not all SQLite APIs are exposed in GRDB, you can still use the [SQLite C Interface](https://www.sqlite.org/c3ref/intro.html) and call [SQLite C functions](https://www.sqlite.org/c3ref/funclist.html).**\n\nTo access the C SQLite functions from SQLCipher or the system SQLite, you need to perform an extra import:\n\n```swift\nimport SQLite3   // System SQLite\nimport SQLCipher // SQLCipher\n\nlet sqliteVersion = String(cString: sqlite3_libversion())\n```\n\nRaw pointers to database connections and statements are available through the `Database.sqliteConnection` and `Statement.sqliteStatement` properties:\n\n```swift\ntry dbQueue.read { db in\n    // The raw pointer to a database connection:\n    let sqliteConnection = db.sqliteConnection\n\n    // The raw pointer to a statement:\n    let statement = try db.makeStatement(sql: \"SELECT ...\")\n    let sqliteStatement = statement.sqliteStatement\n}\n```\n\n> **Note**\n>\n> - Those pointers are owned by GRDB: don't close connections or finalize statements created by GRDB.\n> - GRDB opens SQLite connections in the \"[multi-thread mode](https://www.sqlite.org/threadsafe.html)\", which (oddly) means that **they are not thread-safe**. Make sure you touch raw databases and statements inside their dedicated dispatch queues.\n> - Use the raw SQLite C Interface at your own risk. GRDB won't prevent you from shooting yourself in the foot.\n\n\nRecords\n=======\n\n**On top of the [SQLite API](#sqlite-api), GRDB provides protocols** that help manipulating database rows as regular objects named \"records\":\n\n```swift\ntry dbQueue.write { db in\n    if var place = try Player.fetchOne(db, id: 1) {\n        player.score += 10\n        try player.update(db)\n    }\n}\n```\n\nOf course, you need to open a [database connection], and [create database tables](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseschema) first.\n\nTo define a record type, define a type and extend it with database protocols:\n\n- `FetchableRecord` makes it possible to fetch instances from the database.\n- `PersistableRecord` makes it possible to save instances into the database.\n- `Codable` (not mandatory) provides ready-made serialization to and from database rows.\n- `Identifiable` (not mandatory) provides extra convenience database methods.\n\nTo make it easier to customize database requests, also nest a `Columns` enum: \n\n```swift\nstruct Player: Codable, Identifiable {\n    var id: Int64\n    var name: String\n    var score: Int\n    var team: String?\n}\n\n// Add database support\nextension Player: FetchableRecord, PersistableRecord {\n    enum Columns {\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n        static let team = Column(CodingKeys.team)\n    }\n}\n```\n\nSee more [examples of record definitions](#examples-of-record-definitions) below.\n\n> Note: if you are familiar with Core Data's NSManagedObject or Realm's Object, you may experience a cultural shock: GRDB records are not uniqued, do not auto-update, and do not lazy-load. This is both a purpose, and a consequence of protocol-oriented programming.\n>\n> Tip: The [Recommended Practices for Designing Record Types](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/recordrecommendedpractices) guide provides general guidance..\n>\n> Tip: See the [Demo Applications] for sample apps that uses records.\n\n**Overview**\n\n- [Inserting Records](#inserting-records)\n- [Fetching Records](#fetching-records)\n- [Updating Records](#updating-records)\n- [Deleting Records](#deleting-records)\n- [Counting Records](#counting-records)\n\n**Protocols and the Record Class**\n\n- [Record Protocols Overview](#record-protocols-overview)\n- [FetchableRecord Protocol](#fetchablerecord-protocol)\n- [TableRecord Protocol](#tablerecord-protocol)\n- [PersistableRecord Protocol](#persistablerecord-protocol)\n    - [Persistence Methods]\n    - [Persistence Methods and the `RETURNING` clause]\n    - [Persistence Callbacks]\n- [Identifiable Records]\n- [Codable Records]\n- [Record Comparison]\n- [Record Customization Options]\n- [Record Timestamps and Transaction Date]\n\n\n### Inserting Records\n\nTo insert a record in the database, call the `insert` method:\n\n```swift\nlet player = Player(id: 1, name: \"Arthur\", score: 1000)\ntry player.insert(db)\n```\n\n:point_right: `insert` is available for types that adopt the [PersistableRecord] protocol.\n\n\n### Fetching Records\n\nTo fetch records from the database, call a [fetching method](#fetching-methods):\n\n```swift\nlet arthur = try Player.fetchOne(db,            // Player?\n    sql: \"SELECT * FROM players WHERE name = ?\",\n    arguments: [\"Arthur\"])\n\nlet bestPlayers = try Player                    // [Player]\n    .order(\\.score.desc)\n    .limit(10)\n    .fetchAll(db)\n    \nlet spain = try Country.fetchOne(db, id: \"ES\")  // Country?\nlet italy = try Country.find(db, id: \"IT\")      // Country\n```\n\n:point_right: Fetching from raw SQL is available for types that adopt the [FetchableRecord] protocol.\n\n:point_right: Fetching without SQL, using the [query interface](#the-query-interface), is available for types that adopt both [FetchableRecord] and [TableRecord] protocol.\n\n\n### Updating Records\n\nTo update a record in the database, call the `update` method:\n\n```swift\nvar player: Player = ...\nplayer.score = 1000\ntry player.update(db)\n```\n\nIt is possible to [avoid useless updates](#record-comparison):\n\n```swift\n// does not hit the database if score has not changed\ntry player.updateChanges(db) {\n    $0.score = 1000\n}\n```\n\nSee the [query interface](#the-query-interface) for batch updates:\n\n```swift\ntry Player\n    .filter { $0.team == \"red\" }\n    .updateAll(db) { $0.score += 1 }\n```\n\n:point_right: update methods are available for types that adopt the [PersistableRecord] protocol. Batch updates are available on the [TableRecord] protocol.\n\n\n### Deleting Records\n\nTo delete a record in the database, call the `delete` method:\n\n```swift\nlet player: Player = ...\ntry player.delete(db)\n```\n\nYou can also delete by primary key, unique key, or perform batch deletes (see [Delete Requests](#delete-requests)):\n\n```swift\ntry Player.deleteOne(db, id: 1)\ntry Player.deleteOne(db, key: [\"email\": \"arthur@example.com\"])\ntry Country.deleteAll(db, ids: [\"FR\", \"US\"])\ntry Player\n    .filter { $0.email == nil }\n    .deleteAll(db)\n```\n\n:point_right: delete methods are available for types that adopt the [PersistableRecord] protocol. Batch deletes are available on the [TableRecord] protocol.\n\n\n### Counting Records\n\nTo count records, call the `fetchCount` method:\n\n```swift\nlet playerCount: Int = try Player.fetchCount(db)\n\nlet playerWithEmailCount: Int = try Player\n    .filter { $0.email == nil }\n    .fetchCount(db)\n```\n\n:point_right: `fetchCount` is available for types that adopt the [TableRecord] protocol.\n\n\nDetails follow:\n\n- [Record Protocols Overview](#record-protocols-overview)\n- [FetchableRecord Protocol](#fetchablerecord-protocol)\n- [TableRecord Protocol](#tablerecord-protocol)\n- [PersistableRecord Protocol](#persistablerecord-protocol)\n- [Identifiable Records]\n- [Codable Records]\n- [Record Comparison]\n- [Record Customization Options]\n- [Examples of Record Definitions](#examples-of-record-definitions)\n\n\n## Record Protocols Overview\n\n**GRDB ships with three record protocols**. Your own types will adopt one or several of them, according to the abilities you want to extend your types with.\n\n- [FetchableRecord] is able to **decode database rows**.\n    \n    ```swift\n    struct Place: FetchableRecord { ... }\n    \n    let places = try dbQueue.read { db in\n        try Place.fetchAll(db, sql: \"SELECT * FROM place\")\n    }\n    ```\n    \n    > :bulb: **Tip**: `FetchableRecord` can derive its implementation from the standard `Decodable` protocol. See [Codable Records] for more information.\n    \n    `FetchableRecord` can decode database rows, but it is not able to build SQL requests for you. For that, you also need `TableRecord`:\n    \n- [TableRecord] is able to **generate SQL queries**:\n    \n    ```swift\n    struct Place: TableRecord { ... }\n    \n    let placeCount = try dbQueue.read { db in\n        // Generates and runs `SELECT COUNT(*) FROM place`\n        try Place.fetchCount(db)\n    }\n    ```\n    \n    When a type adopts both `TableRecord` and `FetchableRecord`, it can load from those requests:\n    \n    ```swift\n    struct Place: TableRecord, FetchableRecord { ... }\n    \n    try dbQueue.read { db in\n        let places = try Place.order(\\.title).fetchAll(db)\n        let paris = try Place.fetchOne(id: 1)\n    }\n    ```\n\n- [PersistableRecord] is able to **write**: it can create, update, and delete rows in the database:\n    \n    ```swift\n    struct Place : PersistableRecord { ... }\n    \n    try dbQueue.write { db in\n        try Place.delete(db, id: 1)\n        try Place(...).insert(db)\n    }\n    ```\n    \n    A persistable record can also [compare](#record-comparison) itself against other records, and avoid useless database updates.\n    \n    > :bulb: **Tip**: `PersistableRecord` can derive its implementation from the standard `Encodable` protocol. See [Codable Records] for more information.\n\n\n## FetchableRecord Protocol\n\nğŸ“– [`FetchableRecord`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/fetchablerecord)\n\n**The FetchableRecord protocol grants fetching methods to any type** that can be built from a database row:\n\n```swift\nprotocol FetchableRecord {\n    /// Row initializer\n    init(row: Row) throws\n}\n```\n\nFor example:\n\n```swift\nstruct Place {\n    var id: Int64?\n    var title: String\n    var coordinate: CLLocationCoordinate2D\n}\n\nextension Place: FetchableRecord {\n    enum Columns {\n        static let id = Column(\"id\")\n        static let title = Column(\"title\")\n        static let latitude = Column(\"latitude\")\n        static let longitude = Column(\"longitude\")\n    }\n    \n    init(row: Row) {\n        id = row[Columns.id]\n        title = row[Columns.title]\n        coordinate = CLLocationCoordinate2D(\n            latitude: row[Columns.latitude],\n            longitude: row[Columns.longitude])\n    }\n}\n```\n\nSee [column values](#column-values) for more information about the `row[]` subscript.\n\nWhen your record type adopts the standard Decodable protocol, you don't have to provide the implementation for `init(row:)`. See [Codable Records] for more information:\n\n```swift\n// That's all\nstruct Player: Decodable, FetchableRecord {\n    var id: Int64\n    var name: String\n    var score: Int\n    \n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n}\n```\n\nFetchableRecord allows adopting types to be fetched from SQL queries:\n\n```swift\ntry Place.fetchCursor(db, sql: \"SELECT ...\", arguments:...) // A Cursor of Place\ntry Place.fetchAll(db, sql: \"SELECT ...\", arguments:...)    // [Place]\ntry Place.fetchSet(db, sql: \"SELECT ...\", arguments:...)    // Set<Place>\ntry Place.fetchOne(db, sql: \"SELECT ...\", arguments:...)    // Place?\n```\n\nSee [fetching methods](#fetching-methods) for information about the `fetchCursor`, `fetchAll`, `fetchSet` and `fetchOne` methods. See [`StatementArguments`] for more information about the query arguments.\n\n> **Note**: for performance reasons, the same row argument to `init(row:)` is reused during the iteration of a fetch query. If you want to keep the row for later use, make sure to store a copy: `self.row = row.copy()`.\n\n> **Note**: The `FetchableRecord.init(row:)` initializer fits the needs of most applications. But some application are more demanding than others. When FetchableRecord does not exactly provide the support you need, have a look at the [Beyond FetchableRecord] chapter.\n\n\n## TableRecord Protocol\n\nğŸ“– [`TableRecord`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerecord)\n\n**The TableRecord protocol** generates SQL for you:\n\n```swift\nprotocol TableRecord {\n    static var databaseTableName: String { get }\n    static var databaseSelection: [any SQLSelectable] { get }\n}\n```\n\nThe `databaseSelection` type property is optional, and documented in the [Columns Selected by a Request] chapter.\n\nThe `databaseTableName` type property is the name of a database table. By default, it is derived from the type name:\n\n```swift\nstruct Place: TableRecord { }\n\nprint(Place.databaseTableName) // prints \"place\"\n```\n\nFor example:\n\n- Place: `place`\n- Country: `country`\n- PostalAddress: `postalAddress`\n- HTTPRequest: `httpRequest`\n- TOEFL: `toefl`\n\nYou can still provide a custom table name:\n\n```swift\nstruct Place: TableRecord {\n    static let databaseTableName = \"location\"\n}\n\nprint(Place.databaseTableName) // prints \"location\"\n```\n\nWhen a type adopts both TableRecord and [FetchableRecord](#fetchablerecord-protocol), it can be fetched using the [query interface](#the-query-interface):\n\n```swift\n// SELECT * FROM place WHERE name = 'Paris'\nlet paris = try Place.filter { $0.name == \"Paris\" }.fetchOne(db)\n```\n\nTableRecord can also fetch deal with primary and unique keys: see [Fetching by Key](#fetching-by-key) and [Testing for Record Existence](#testing-for-record-existence).\n\n\n## PersistableRecord Protocol\n\nğŸ“– [`EncodableRecord`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/encodablerecord), [`MutablePersistableRecord`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/mutablepersistablerecord), [`PersistableRecord`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/persistablerecord)\n\n**GRDB record types can create, update, and delete rows in the database.**\n\nThose abilities are granted by three protocols:\n\n```swift\n// Defines how a record encodes itself into the database\nprotocol EncodableRecord {\n    /// Defines the values persisted in the database\n    func encode(to container: inout PersistenceContainer) throws\n}\n\n// Adds persistence methods\nprotocol MutablePersistableRecord: TableRecord, EncodableRecord {\n    /// Optional method that lets your adopting type store its rowID upon\n    /// successful insertion. Don't call it directly: it is called for you.\n    mutating func didInsert(_ inserted: InsertionSuccess)\n}\n\n// Adds immutability\nprotocol PersistableRecord: MutablePersistableRecord {\n    /// Non-mutating version of the optional didInsert(_:)\n    func didInsert(_ inserted: InsertionSuccess)\n}\n```\n\nYes, three protocols instead of one. Here is how you pick one or the other:\n\n- **If your type is a class**, choose `PersistableRecord`. On top of that, implement `didInsert(_:)` if the database table has an auto-incremented primary key.\n\n- **If your type is a struct, and the database table has an auto-incremented primary key**, choose `MutablePersistableRecord`, and implement `didInsert(_:)`.\n\n- **Otherwise**, choose `PersistableRecord`, and ignore `didInsert(_:)`.\n\nThe `encode(to:)` method defines which [values](#values) (Bool, Int, String, Date, Swift enums, etc.) are assigned to database columns.\n\nThe optional `didInsert` method lets the adopting type store its rowID after successful insertion, and is only useful for tables that have an auto-incremented primary key. It is called from a protected dispatch queue, and serialized with all database updates.\n\nFor example:\n\n```swift\nextension Place: MutablePersistableRecord {\n    enum Columns {\n        static let id = Column(\"id\")\n        static let title = Column(\"title\")\n        static let latitude = Column(\"latitude\")\n        static let longitude = Column(\"longitude\")\n    }\n    \n    /// The values persisted in the database\n    func encode(to container: inout PersistenceContainer) {\n        container[Columns.id] = id\n        container[Columns.title] = title\n        container[Columns.latitude] = coordinate.latitude\n        container[Columns.longitude] = coordinate.longitude\n    }\n    \n    // Update auto-incremented id upon successful insertion\n    mutating func didInsert(_ inserted: InsertionSuccess) {\n        id = inserted.rowID\n    }\n}\n\nvar paris = Place(\n    id: nil,\n    title: \"Paris\",\n    coordinate: CLLocationCoordinate2D(latitude: 48.8534100, longitude: 2.3488000))\n\ntry paris.insert(db)\nparis.id   // some value\n```\n\nWhen your record type adopts the standard Encodable protocol, you don't have to provide the implementation for `encode(to:)`. See [Codable Records] for more information:\n\n```swift\n// That's all\nstruct Player: Encodable, MutablePersistableRecord {\n    var id: Int64?\n    var name: String\n    var score: Int\n    \n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n    \n    // Update auto-incremented id upon successful insertion\n    mutating func didInsert(_ inserted: InsertionSuccess) {\n        id = inserted.rowID\n    }\n}\n```\n\n\n### Persistence Methods\n\nTypes that adopt the [PersistableRecord] protocol are given methods that insert, update, and delete:\n\n```swift\n// INSERT\ntry place.insert(db)\nlet insertedPlace = try place.inserted(db) // non-mutating\n\n// UPDATE\ntry place.update(db)\ntry place.update(db, columns: [\"title\"])\n\n// Maybe UPDATE\ntry place.updateChanges(db, from: otherPlace)\ntry place.updateChanges(db) { $0.isFavorite = true }\n\n// INSERT or UPDATE\ntry place.save(db)\nlet savedPlace = place.saved(db) // non-mutating\n\n// UPSERT\ntry place.upsert(db)\nlet insertedPlace = place.upsertAndFetch(db)\n\n// DELETE\ntry place.delete(db)\n\n// EXISTENCE CHECK\nlet exists = try place.exists(db)\n```\n\nSee [Upsert](#upsert) below for more information about upserts.\n\n**The [TableRecord] protocol comes with batch operations**:\n\n```swift\n// UPDATE\ntry Place.updateAll(db, ...)\n\n// DELETE\ntry Place.deleteAll(db)\ntry Place.deleteAll(db, ids:...)\ntry Place.deleteAll(db, keys:...)\ntry Place.deleteOne(db, id:...)\ntry Place.deleteOne(db, key:...)\n```\n\nFor more information about batch updates, see [Update Requests](#update-requests).\n\n- All persistence methods can throw a [DatabaseError](#error-handling).\n\n- `update` and `updateChanges` throw [RecordError] if the database does not contain any row for the primary key of the record.\n\n- `save` makes sure your values are stored in the database. It performs an UPDATE if the record has a non-null primary key, and then, if no row was modified, an INSERT. It directly performs an INSERT if the record has no primary key, or a null primary key.\n\n- `delete` and `deleteOne` returns whether a database row was deleted or not. `deleteAll` returns the number of deleted rows. `updateAll` returns the number of updated rows. `updateChanges` returns whether a database row was updated or not.\n\n**All primary keys are supported**, including composite primary keys that span several columns, and the [hidden `rowid` column](https://www.sqlite.org/rowidtable.html).\n\n**To customize persistence methods**, you provide [Persistence Callbacks], described below. Do not attempt at overriding the ready-made persistence methods.\n\n### Upsert\n\n[UPSERT](https://www.sqlite.org/lang_UPSERT.html) is an SQLite feature that causes an INSERT to behave as an UPDATE or a no-op if the INSERT would violate a uniqueness constraint (primary key or unique index).\n\n> **Note**: Upsert apis are available from SQLite 3.35.0+: iOS 15.0+, macOS 12.0+, tvOS 15.0+, watchOS 8.0+, or with a [custom SQLite build] or [SQLCipher](#encryption).\n>\n> **Note**: With regard to [persistence callbacks](#available-callbacks), an upsert behaves exactly like an insert. In particular: the `aroundInsert(_:)` and `didInsert(_:)` callbacks reports the rowid of the inserted or updated row; `willUpdate`, `aroundUpdate`, `didUpdate` are not called.\n\n[PersistableRecord] provides three upsert methods:\n\n- `upsert(_:)`\n    \n    Inserts or updates a record.\n    \n    The upsert behavior is triggered by a violation of any uniqueness constraint on the table (primary key or unique index). In case of conflict, all columns but the primary key are overwritten with the inserted values:\n    \n    ```swift\n    struct Player: Encodable, PersistableRecord {\n        var id: Int64\n        var name: String\n        var score: Int\n    }\n    \n    // INSERT INTO player (id, name, score)\n    // VALUES (1, 'Arthur', 1000)\n    // ON CONFLICT DO UPDATE SET\n    //   name = excluded.name,\n    //   score = excluded.score\n    let player = Player(id: 1, name: \"Arthur\", score: 1000)\n    try player.upsert(db)\n    ```\n\n- `upsertAndFetch(_:onConflict:doUpdate:)` (requires [FetchableRecord] conformance)\n\n    Inserts or updates a record, and returns the upserted record.\n    \n    The `onConflict` and `doUpdate` arguments let you further control the upsert behavior. Make sure you check the [SQLite UPSERT documentation](https://www.sqlite.org/lang_UPSERT.html) for detailed information.\n    \n    - `onConflict`: the \"conflict target\" is the array of columns in the uniqueness constraint (primary key or unique index) that triggers the upsert.\n        \n        If empty (the default), all uniqueness constraint are considered.\n    \n    - `doUpdate`: a closure that returns columns assignments to perform in case of conflict. Other columns are overwritten with the inserted values.\n        \n        By default, all inserted columns but the primary key and the conflict target are overwritten.\n    \n    In the example below, we upsert the new vocabulary word \"jovial\". It is inserted if that word is not already in the dictionary. Otherwise, `count` is incremented, `isTainted` is not overwritten, and `kind` is overwritten:\n    \n    ```swift\n    // CREATE TABLE vocabulary(\n    //   word TEXT NOT NULL PRIMARY KEY,\n    //   kind TEXT NOT NULL,\n    //   isTainted BOOLEAN DEFAULT 0,\n    //   count INT DEFAULT 1))\n    struct Vocabulary: Encodable, PersistableRecord {\n        var word: String\n        var kind: String\n        var isTainted: Bool\n    }\n    \n    // INSERT INTO vocabulary(word, kind, isTainted)\n    // VALUES('jovial', 'adjective', 0)\n    // ON CONFLICT(word) DO UPDATE SET \\\n    //   count = count + 1,   -- on conflict, count is incremented\n    //   kind = excluded.kind -- on conflict, kind is overwritten\n    // RETURNING *\n    let vocabulary = Vocabulary(word: \"jovial\", kind: \"adjective\", isTainted: false)\n    let upserted = try vocabulary.upsertAndFetch(\n        db, onConflict: [\"word\"],\n        doUpdate: { _ in\n            [Column(\"count\") += 1,            // on conflict, count is incremented\n             Column(\"isTainted\").noOverwrite] // on conflict, isTainted is NOT overwritten\n        })\n    ```\n    \n    The `doUpdate` closure accepts an `excluded` TableAlias argument that refers to the inserted values that trigger the conflict. You can use it to specify an explicit overwrite, or to perform a computation. In the next example, the upsert keeps the maximum date in case of conflict:\n    \n    ```swift\n    // INSERT INTO message(id, text, date)\n    // VALUES(...)\n    // ON CONFLICT DO UPDATE SET \\\n    //   text = excluded.text,\n    //   date = MAX(date, excluded.date)\n    // RETURNING *\n    let upserted = try message.upsertAndFetch(doUpdate: { excluded in\n        // keep the maximum date in case of conflict\n        [Column(\"date\").set(to: max(Column(\"date\"), excluded[\"date\"]))]\n    })\n    ```\n\n- `upsertAndFetch(_:as:onConflict:doUpdate:)` (does not require [FetchableRecord] conformance)\n\n    This method is identical to `upsertAndFetch(_:onConflict:doUpdate:)` described above, but you can provide a distinct [FetchableRecord] record type as a result, in order to specify the returned columns.\n\n### Persistence Methods and the `RETURNING` clause\n\nSQLite is able to return values from a inserted, updated, or deleted row, with the [`RETURNING` clause](https://www.sqlite.org/lang_returning.html).\n\n> **Note**: Support for the `RETURNING` clause is available from SQLite 3.35.0+: iOS 15.0+, macOS 12.0+, tvOS 15.0+, watchOS 8.0+, or with a [custom SQLite build] or [SQLCipher](#encryption).\n\nThe `RETURNING` clause helps dealing with database features such as auto-incremented ids, default values, and [generated columns](https://sqlite.org/gencol.html). You can, for example, insert a few columns and fetch the default or generated ones in one step.\n\nGRDB uses the `RETURNING` clause in all persistence methods that contain `AndFetch` in their name.\n\nFor example, given a database table with an auto-incremented primary key and a default score:\n\n```swift\ntry dbQueue.write { db in\n    try db.execute(sql: \"\"\"\n        CREATE TABLE player(\n          id INTEGER PRIMARY KEY AUTOINCREMENT,\n          name TEXT NOT NULL,\n          score INTEGER NOT NULL DEFAULT 1000)\n        \"\"\")\n}\n```\n\nYou can define a record type with full database information, and another partial record type that deals with a subset of columns:\n\n```swift\n// A player with full database information\nstruct Player: Codable, PersistableRecord, FetchableRecord {\n    var id: Int64\n    var name: String\n    var score: Int\n    \n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n}\n\n// A partial player\nstruct PartialPlayer: Encodable, PersistableRecord {\n    static let databaseTableName = \"player\"\n    var name: String\n    \n    typealias Columns = Player.Columns\n}\n```\n\nAnd now you can get a full player by inserting a partial one:\n\n```swift\ntry dbQueue.write { db in\n    let partialPlayer = PartialPlayer(name: \"Alice\")\n    \n    // INSERT INTO player (name) VALUES ('Alice') RETURNING *\n    let player = try partialPlayer.insertAndFetch(db, as: Player.self)\n    print(player.id)    // The inserted id\n    print(player.name)  // The inserted name\n    print(player.score) // The default score\n}\n```\n\nFor extra precision, you can select only the columns you need, and fetch the desired value from the provided prepared [`Statement`]:\n\n```swift\ntry dbQueue.write { db in\n    let partialPlayer = PartialPlayer(name: \"Alice\")\n    \n    // INSERT INTO player (name) VALUES ('Alice') RETURNING score\n    let score = try partialPlayer.insertAndFetch(db) { statement in\n        try Int.fetchOne(statement)\n    } select: {\n        [$0.score]\n    }\n    print(score) // Prints 1000, the default score\n}\n```\n\nThere are other similar persistence methods, such as `upsertAndFetch`, `saveAndFetch`, `updateAndFetch`, `updateChangesAndFetch`, etc. They all behave like `upsert`, `save`, `update`, `updateChanges`, except that they return saved values. For example:\n\n```swift\n// Save and return the saved player\nlet savedPlayer = try player.saveAndFetch(db)\n```\n\nSee [Persistence Methods], [Upsert](#upsert), and [`updateChanges` methods](#the-updatechanges-methods) for more information.\n\n**Batch operations** can return updated or deleted values:\n\n> **Warning**: Make sure you check the [documentation of the `RETURNING` clause](https://www.sqlite.org/lang_returning.html#limitations_and_caveats), which describes important limitations and caveats for batch operations.\n\n```swift\nlet request = Player.filter(...)...\n\n// Fetch all deleted players\n// DELETE FROM player RETURNING *\nlet deletedPlayers = try request.deleteAndFetchAll(db) // [Player]\n\n// Fetch a selection of columns from the deleted rows\n// DELETE FROM player RETURNING name\nlet statement = try request.deleteAndFetchStatement(db) { [$0.name] }\nlet deletedNames = try String.fetchSet(statement)\n\n// Fetch all updated players\n// UPDATE player SET score = score + 10 RETURNING *\nlet updatedPlayers = try request.updateAndFetchAll(db) { [$0.score += 10] } // [Player]\n\n// Fetch a selection of columns from the updated rows\n// UPDATE player SET score = score + 10 RETURNING score\nlet statement = try request.updateAndFetchStatement(db) {\n    [$0.score += 10]\n} select: {\n    [$0.score]\n}\nlet updatedScores = try Int.fetchAll(statement)\n```\n\n\n### Persistence Callbacks\n\nYour custom type may want to perform extra work when the persistence methods are invoked.\n\nTo this end, your record type can implement **persistence callbacks**. Callbacks are methods that get called at certain moments of a record's life cycle. With callbacks it is possible to write code that will run whenever an record is inserted, updated, or deleted.\n\nIn order to use a callback method, you need to provide its implementation. For example, a frequently used callback is `didInsert`, in the case of auto-incremented database ids:\n\n```swift\nstruct Player: MutablePersistableRecord {\n    var id: Int64?\n    \n    // Update auto-incremented id upon successful insertion\n    mutating func didInsert(_ inserted: InsertionSuccess) {\n        id = inserted.rowID\n    }\n}\n\ntry dbQueue.write { db in\n    var player = Player(id: nil, ...)\n    try player.insert(db)\n    print(player.id) // didInsert was called: prints some non-nil id\n}\n```\n\nCallbacks can also help implementing record validation:\n\n```swift\nstruct Link: PersistableRecord {\n    var url: URL\n    \n    func willSave(_ db: Database) throws {\n        if url.host == nil {\n            throw ValidationError(\"url must be absolute.\")\n        }\n    }\n}\n\ntry link.insert(db) // Calls the willSave callback\ntry link.update(db) // Calls the willSave callback\ntry link.save(db)   // Calls the willSave callback\ntry link.upsert(db) // Calls the willSave callback\n```\n\n#### Available Callbacks\n\nHere is a list with all the available [persistence callbacks], listed in the same order in which they will get called during the respective operations:\n\n- Inserting a record (all `record.insert` and `record.upsert` methods)\n    - `willSave`\n    - `aroundSave`\n    - `willInsert`\n    - `aroundInsert`\n    - `didInsert`\n    - `didSave`\n    \n- Updating a record (all `record.update` methods)\n    - `willSave`\n    - `aroundSave`\n    - `willUpdate`\n    - `aroundUpdate`\n    - `didUpdate`\n    - `didSave`\n    \n- Deleting a record (only the `record.delete(_:)` method)\n    - `willDelete`\n    - `aroundDelete`\n    - `didDelete`\n\nFor detailed information about each callback, check the [reference](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/mutablepersistablerecord/).\n\nIn the `MutablePersistableRecord` protocol, `willInsert` and `didInsert` are mutating methods. In `PersistableRecord`, they are not mutating.\n\n> **Note**: The `record.save(_:)` method performs an UPDATE if the record has a non-null primary key, and then, if no row was modified, an INSERT. It directly performs an INSERT if the record has no primary key, or a null primary key. It triggers update and/or insert callbacks accordingly.\n>\n> **Warning**: Callbacks are only invoked from persistence methods called on record instances. Callbacks are not invoked when you call a type method, perform a batch operations, or execute raw SQL.\n>\n> **Warning**: When a `did***` callback is invoked, do not assume that the change is actually persisted on disk, because the database may still be inside an uncommitted transaction. When you need to handle transaction completions, use the [afterNextTransaction(onCommit:onRollback:)](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/database/afternexttransaction(oncommit:onrollback:)). For example:\n>\n> ```swift\n> struct PictureFile: PersistableRecord {\n>     var path: String\n>     \n>     func willDelete(_ db: Database) {\n>         db.afterNextTransaction { _ in\n>             try? deleteFileOnDisk()\n>         }\n>     }\n> }\n> ```\n\n\n## Identifiable Records\n\n**When a record type maps a table with a single-column primary key, it is recommended to have it adopt the standard [Identifiable] protocol.**\n\n```swift\nstruct Player: Identifiable, FetchableRecord, PersistableRecord {\n    var id: Int64 // fulfills the Identifiable requirement\n    var name: String\n    var score: Int\n}\n```\n\nWhen `id` has a [database-compatible type](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasevalueconvertible) (Int64, Int, String, UUID, ...), the `Identifiable` conformance unlocks type-safe record and request methods:\n\n```swift\nlet player = try Player.find(db, id: 1)               // Player\nlet player = try Player.fetchOne(db, id: 1)           // Player?\nlet players = try Player.fetchAll(db, ids: [1, 2, 3]) // [Player]\nlet players = try Player.fetchSet(db, ids: [1, 2, 3]) // Set<Player>\n\nlet request = Player.filter(id: 1)\nlet request = Player.filter(ids: [1, 2, 3])\n\ntry Player.deleteOne(db, id: 1)\ntry Player.deleteAll(db, ids: [1, 2, 3])\n```\n\n> **Note**: Not all record types can be made `Identifiable`, and not all tables have a single-column primary key. GRDB provides other methods that deal with primary and unique keys, but they won't check the type of their arguments:\n> \n> ```swift\n> // Available on non-Identifiable types\n> try Player.fetchOne(db, key: 1)\n> try Player.fetchOne(db, key: [\"email\": \"arthur@example.com\"])\n> try Country.fetchAll(db, keys: [\"FR\", \"US\"])\n> try Citizenship.fetchOne(db, key: [\"citizenId\": 1, \"countryCode\": \"FR\"])\n> \n> let request = Player.filter(key: 1)\n> let request = Player.filter(keys: [1, 2, 3])\n> \n> try Player.deleteOne(db, key: 1)\n> try Player.deleteAll(db, keys: [1, 2, 3])\n> ```\n\n> **Note**: It is not recommended to use `Identifiable` on record types that use an auto-incremented primary key:\n>\n> ```swift\n> // AVOID declaring Identifiable conformance when key is auto-incremented\n> struct Player {\n>     var id: Int64? // Not an id suitable for Identifiable\n>     var name: String\n>     var score: Int\n> }\n> \n> extension Player: FetchableRecord, MutablePersistableRecord {\n>     // Update auto-incremented id upon successful insertion\n>     mutating func didInsert(_ inserted: InsertionSuccess) {\n>         id = inserted.rowID\n>     }\n> }\n> ```\n>\n> For a detailed rationale, please see [issue #1435](https://github.com/groue/GRDB.swift/issues/1435#issuecomment-1740857712).\n\nSome database tables have a single-column primary key which is not called \"id\":\n\n```swift\ntry db.create(table: \"country\") { t in\n    t.primaryKey(\"isoCode\", .text)\n    t.column(\"name\", .text).notNull()\n    t.column(\"population\", .integer).notNull()\n}\n```\n\nIn this case, `Identifiable` conformance can be achieved, for example, by returning the primary key column from the `id` property:\n\n```swift\nstruct Country: Identifiable, FetchableRecord, PersistableRecord {\n    var isoCode: String\n    var name: String\n    var population: Int\n    \n    // Fulfill the Identifiable requirement\n    var id: String { isoCode }\n}\n\nlet france = try dbQueue.read { db in\n    try Country.fetchOne(db, id: \"FR\")\n}\n```\n\n\n## Codable Records\n\nRecord types that adopt an archival protocol ([Codable, Encodable or Decodable](https://developer.apple.com/documentation/foundation/archives_and_serialization/encoding_and_decoding_custom_types)) get free database support just by declaring conformance to the desired [record protocols](#record-protocols-overview):\n\n```swift\n// Declare a record...\nstruct Player: Codable, FetchableRecord, PersistableRecord {\n    var id: Int64\n    var name: String\n    var score: Int\n    \n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n}\n\n// ...and there you go:\ntry dbQueue.write { db in\n    try Player(id: 1, name: \"Arthur\", score: 100).insert(db)\n    let players = try Player.order(\\.score.desc).fetchAll(db)\n}\n```\n\nCodable records encode and decode their properties according to their own implementation of the Encodable and Decodable protocols. Yet databases have specific requirements:\n\n- Properties are always coded according to their preferred database representation, when they have one (all [values](#values) that adopt the [`DatabaseValueConvertible`] protocol).\n- You can customize the encoding and decoding of dates and uuids.\n- Complex properties (arrays, dictionaries, nested structs, etc.) are stored as JSON.\n\nFor more information about Codable records, see:\n\n- [JSON Columns]\n- [Column Names Coding Strategies]\n- [Data, Date, and UUID Coding Strategies]\n- [The userInfo Dictionary]\n- [Tip: Derive Columns from Coding Keys](#tip-derive-columns-from-coding-keys)\n\n> :bulb: **Tip**: see the [Demo Applications] for sample code that uses Codable records.\n\n\n### JSON Columns\n\nWhen a [Codable record](#codable-records) contains a property that is not a simple [value](#values) (Bool, Int, String, Date, Swift enums, etc.), that value is encoded and decoded as a **JSON string**. For example:\n\n```swift\nenum AchievementColor: String, Codable {\n    case bronze, silver, gold\n}\n\nstruct Achievement: Codable {\n    var name: String\n    var color: AchievementColor\n}\n\nstruct Player: Codable, FetchableRecord, PersistableRecord {\n    var name: String\n    var score: Int\n    var achievements: [Achievement] // stored in a JSON column\n}\n\ntry dbQueue.write { db in\n    // INSERT INTO player (name, score, achievements)\n    // VALUES (\n    //   'Arthur',\n    //   100,\n    //   '[{\"color\":\"gold\",\"name\":\"Use Codable Records\"}]')\n    let achievement = Achievement(name: \"Use Codable Records\", color: .gold)\n    let player = Player(name: \"Arthur\", score: 100, achievements: [achievement])\n    try player.insert(db)\n}\n```\n\nGRDB uses the standard [JSONDecoder](https://developer.apple.com/documentation/foundation/jsondecoder) and [JSONEncoder](https://developer.apple.com/documentation/foundation/jsonencoder) from Foundation. By default, Data values are handled with the `.base64` strategy, Date with the `.millisecondsSince1970` strategy, and non conforming floats with the `.throw` strategy.\n\nYou can customize the JSON format by implementing those methods:\n\n```swift\nprotocol FetchableRecord {\n    static func databaseJSONDecoder(for column: String) -> JSONDecoder\n}\n\nprotocol EncodableRecord {\n    static func databaseJSONEncoder(for column: String) -> JSONEncoder\n}\n```\n\n> :bulb: **Tip**: Make sure you set the JSONEncoder `sortedKeys` option. This option makes sure that the JSON output is stable. This stability is required for [Record Comparison] to work as expected, and database observation tools such as [ValueObservation] to accurately recognize changed records.\n\n\n### Column Names Coding Strategies\n\nBy default, [Codable Records] store their values into database columns that match their coding keys: the `teamID` property is stored into the `teamID` column.\n\nThis behavior can be overridden, so that you can, for example, store the `teamID` property into the `team_id` column:\n\n```swift\nprotocol FetchableRecord {\n    static var databaseColumnDecodingStrategy: DatabaseColumnDecodingStrategy { get }\n}\n\nprotocol EncodableRecord {\n    static var databaseColumnEncodingStrategy: DatabaseColumnEncodingStrategy { get }\n}\n```\n\nSee [DatabaseColumnDecodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasecolumndecodingstrategy) and [DatabaseColumnEncodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasecolumnencodingstrategy/) to learn about all available strategies.\n\n\n### Data, Date, and UUID Coding Strategies\n\nBy default, [Codable Records] encode and decode their Data properties as blobs, and Date and UUID properties as described in the general [Date and DateComponents](#date-and-datecomponents) and [UUID](#uuid) chapters.\n\nTo sum up: dates encode themselves in the \"YYYY-MM-DD HH:MM:SS.SSS\" format, in the UTC time zone, and decode a variety of date formats and timestamps. UUIDs encode themselves as 16-bytes data blobs, and decode both 16-bytes data blobs and strings such as \"E621E1F8-C36C-495A-93FC-0C247A3E6E5F\".\n\nThose behaviors can be overridden:\n\n```swift\nprotocol FetchableRecord {\n    static func databaseDataDecodingStrategy(for column: String) -> DatabaseDataDecodingStrategy\n    static func databaseDateDecodingStrategy(for column: String) -> DatabaseDateDecodingStrategy\n}\n\nprotocol EncodableRecord {\n    static func databaseDataEncodingStrategy(for column: String) -> DatabaseDataEncodingStrategy\n    static func databaseDateEncodingStrategy(for column: String) -> DatabaseDateEncodingStrategy\n    static func databaseUUIDEncodingStrategy(for column: String) -> DatabaseUUIDEncodingStrategy\n}\n```\n\nSee [DatabaseDataDecodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasedatadecodingstrategy/), [DatabaseDateDecodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasedatedecodingstrategy/), [DatabaseDataEncodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasedataencodingstrategy/), [DatabaseDateEncodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasedateencodingstrategy/), and [DatabaseUUIDEncodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseuuidencodingstrategy/) to learn about all available strategies.\n\nThere is no customization of uuid decoding, because UUID can already decode all its encoded variants (16-bytes blobs and uuid strings, both uppercase and lowercase).\n\nCustomized coding strategies apply:\n\n- When encoding and decoding database rows to and from records (fetching and persistence methods).\n- In requests by single-column primary key: `fetchOne(_:id:)`, `filter(id:)`, `deleteAll(_:keys:)`, etc.\n\n*They do not apply* in other requests based on data, date, or uuid values.\n\nSo make sure that those are properly encoded in your requests. For example:\n\n```swift\nstruct Player: Codable, FetchableRecord, PersistableRecord, Identifiable {\n    // UUIDs are stored as strings\n    static func databaseUUIDEncodingStrategy(for column: String) -> DatabaseUUIDEncodingStrategy {\n        .uppercaseString\n    }\n    \n    var id: UUID\n    ...\n}\n\ntry dbQueue.write { db in\n    let uuid = UUID()\n    let player = Player(id: uuid, ...)\n    \n    // OK: inserts a player in the database, with a string uuid\n    try player.insert(db)\n    \n    // OK: performs a string-based query, finds the inserted player\n    _ = try Player.filter(id: uuid).fetchOne(db)\n\n    // NOT OK: performs a blob-based query, fails to find the inserted player\n    _ = try Player.filter { $0.id == uuid }.fetchOne(db)\n    \n    // OK: performs a string-based query, finds the inserted player\n    _ = try Player.filter { $0.id == uuid.uuidString }.fetchOne(db)\n}\n```\n\n\n### The userInfo Dictionary\n\nYour [Codable Records] can be stored in the database, but they may also have other purposes. In this case, you may need to customize their implementations of `Decodable.init(from:)` and `Encodable.encode(to:)`, depending on the context.\n\nThe standard way to provide such context is the `userInfo` dictionary. Implement those properties:\n\n```swift\nprotocol FetchableRecord {\n    static var databaseDecodingUserInfo: [CodingUserInfoKey: Any] { get }\n}\n\nprotocol EncodableRecord {\n    static var databaseEncodingUserInfo: [CodingUserInfoKey: Any] { get }\n}\n```\n\nFor example, here is a Player type that customizes its decoding:\n\n```swift\n// A key that holds a decoder's name\nlet decoderName = CodingUserInfoKey(rawValue: \"decoderName\")!\n\nstruct Player: FetchableRecord, Decodable {\n    init(from decoder: Decoder) throws {\n        // Print the decoder name\n        let decoderName = decoder.userInfo[decoderName] as? String\n        print(\"Decoded from \\(decoderName ?? \"unknown decoder\")\")\n        ...\n    }\n}\n```\n\nYou can have a specific decoding from JSON...\n\n```swift\n// prints \"Decoded from JSON\"\nlet decoder = JSONDecoder()\ndecoder.userInfo = [decoderName: \"JSON\"]\nlet player = try decoder.decode(Player.self, from: jsonData)\n```\n\n... and another one from database rows:\n\n```swift\nextension Player: FetchableRecord {\n    static var databaseDecodingUserInfo: [CodingUserInfoKey: Any] {\n        [decoderName: \"database row\"]\n    }\n}\n\n// prints \"Decoded from database row\"\nlet player = try Player.fetchOne(db, ...)\n```\n\n> **Note**: make sure the `databaseDecodingUserInfo` and `databaseEncodingUserInfo` properties are explicitly declared as `[CodingUserInfoKey: Any]`. If they are not, the Swift compiler may silently miss the protocol requirement, resulting in sticky empty userInfo.\n\n\n### Tip: Derive Columns from Coding Keys\n\nCodable types are granted with a [CodingKeys](https://developer.apple.com/documentation/foundation/archives_and_serialization/encoding_and_decoding_custom_types) enum. You can use them to safely define database columns:\n\n```swift\nstruct Player: Codable {\n    var id: Int64\n    var name: String\n    var score: Int\n}\n\nextension Player: FetchableRecord, PersistableRecord {\n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n}\n```\n\nSee the [query interface](#the-query-interface) and [Recommended Practices for Designing Record Types](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/recordrecommendedpractices) for further information.\n\n\n## Record Comparison\n\n**Records that adopt the [EncodableRecord] protocol can compare against other records, or against previous versions of themselves.**\n\nThis helps avoiding costly UPDATE statements when a record has not been edited.\n\n- [The `updateChanges` Methods](#the-updatechanges-methods)\n- [The `databaseEquals` Method](#the-databaseequals-method)\n- [The `databaseChanges` and `hasDatabaseChanges` Methods](#the-databasechanges-and-hasdatabasechanges-methods)\n\n\n### The `updateChanges` Methods\n\nThe `updateChanges` methods perform a database update of the changed columns only (and does nothing if record has no change).\n\n- `updateChanges(_:from:)`\n\n    This method lets you compare two records:\n\n    ```swift\n    if let oldPlayer = try Player.fetchOne(db, id: 42) {\n        var newPlayer = oldPlayer\n        newPlayer.score = 100\n        if try newPlayer.updateChanges(db, from: oldPlayer) {\n            print(\"player was modified, and updated in the database\")\n        } else {\n            print(\"player was not modified, and database was not hit\")\n        }\n    }\n    ```\n\n- `updateChanges(_:modify:)`\n    \n    This method lets you update a record in place:\n    \n    ```swift\n    if var player = try Player.fetchOne(db, id: 42) {\n        let modified = try player.updateChanges(db) {\n            $0.score = 100\n        }\n        if modified {\n            print(\"player was modified, and updated in the database\")\n        } else {\n            print(\"player was not modified, and database was not hit\")\n        }\n    }\n    ```\n\n### The `databaseEquals` Method\n\nThis method returns whether two records have the same database representation:\n\n```swift\nlet oldPlayer: Player = ...\nvar newPlayer: Player = ...\nif newPlayer.databaseEquals(oldPlayer) == false {\n    try newPlayer.save(db)\n}\n```\n\n> **Note**: The comparison is performed on the database representation of records. As long as your record type adopts the EncodableRecord protocol, you don't need to care about Equatable.\n\n\n### The `databaseChanges` and `hasDatabaseChanges` Methods\n\n`databaseChanges(from:)` returns a dictionary of differences between two records:\n\n```swift\nlet oldPlayer = Player(id: 1, name: \"Arthur\", score: 100)\nlet newPlayer = Player(id: 1, name: \"Arthur\", score: 1000)\nfor (column, oldValue) in try newPlayer.databaseChanges(from: oldPlayer) {\n    print(\"\\(column) was \\(oldValue)\")\n}\n// prints \"score was 100\"\n```\n\nFor an efficient algorithm which synchronizes the content of a database table with a JSON payload, check [groue/SortedDifference](https://github.com/groue/SortedDifference).\n\n\n## Record Customization Options\n\nGRDB records come with many default behaviors, that are designed to fit most situations. Many of those defaults can be customized for your specific needs:\n\n- [Persistence Callbacks]: define what happens when you call a persistence method such as `player.insert(db)`\n- [Conflict Resolution]: Run `INSERT OR REPLACE` queries, and generally define what happens when a persistence method violates a unique index.\n- [Columns Selected by a Request]: define which columns are selected by requests such as `Player.fetchAll(db)`.\n- [Beyond FetchableRecord]: the FetchableRecord protocol is not the end of the story.\n\n[Codable Records] have a few extra options:\n\n- [JSON Columns]: control the format of JSON columns.\n- [Column Names Coding Strategies]: control how coding keys are turned into column names\n- [Date and UUID Coding Strategies]: control the format of Date and UUID properties in your Codable records.\n- [The userInfo Dictionary]: adapt your Codable implementation for the database.\n\n\n### Conflict Resolution\n\n**Insertions and updates can create conflicts**: for example, a query may attempt to insert a duplicate row that violates a unique index.\n\nThose conflicts normally end with an error. Yet SQLite let you alter the default behavior, and handle conflicts with specific policies. For example, the `INSERT OR REPLACE` statement handles conflicts with the \"replace\" policy which replaces the conflicting row instead of throwing an error.\n\nThe [five different policies](https://www.sqlite.org/lang_conflict.html) are: abort (the default), replace, rollback, fail, and ignore.\n\n**SQLite let you specify conflict policies at two different places:**\n\n- In the definition of the database table:\n    \n    ```swift\n    // CREATE TABLE player (\n    //     id INTEGER PRIMARY KEY AUTOINCREMENT,\n    //     email TEXT UNIQUE ON CONFLICT REPLACE\n    // )\n    try db.create(table: \"player\") { t in\n        t.autoIncrementedPrimaryKey(\"id\")\n        t.column(\"email\", .text).unique(onConflict: .replace) // <--\n    }\n    \n    // Despite the unique index on email, both inserts succeed.\n    // The second insert replaces the first row:\n    try db.execute(sql: \"INSERT INTO player (email) VALUES (?)\", arguments: [\"arthur@example.com\"])\n    try db.execute(sql: \"INSERT INTO player (email) VALUES (?)\", arguments: [\"arthur@example.com\"])\n    ```\n    \n- In each modification query:\n    \n    ```swift\n    // CREATE TABLE player (\n    //     id INTEGER PRIMARY KEY AUTOINCREMENT,\n    //     email TEXT UNIQUE\n    // )\n    try db.create(table: \"player\") { t in\n        t.autoIncrementedPrimaryKey(\"id\")\n        t.column(\"email\", .text).unique()\n    }\n    \n    // Again, despite the unique index on email, both inserts succeed.\n    try db.execute(sql: \"INSERT OR REPLACE INTO player (email) VALUES (?)\", arguments: [\"arthur@example.com\"])\n    try db.execute(sql: \"INSERT OR REPLACE INTO player (email) VALUES (?)\", arguments: [\"arthur@example.com\"])\n    ```\n\nWhen you want to handle conflicts at the query level, specify a custom `persistenceConflictPolicy` in your type that adopts the PersistableRecord protocol. It will alter the INSERT and UPDATE queries run by the `insert`, `update` and `save` [persistence methods]:\n\n```swift\nprotocol MutablePersistableRecord {\n    /// The policy that handles SQLite conflicts when records are\n    /// inserted or updated.\n    ///\n    /// This property is optional: its default value uses the ABORT\n    /// policy for both insertions and updates, so that GRDB generate\n    /// regular INSERT and UPDATE queries.\n    static var persistenceConflictPolicy: PersistenceConflictPolicy { get }\n}\n\nstruct Player : MutablePersistableRecord {\n    static let persistenceConflictPolicy = PersistenceConflictPolicy(\n        insert: .replace,\n        update: .replace)\n}\n\n// INSERT OR REPLACE INTO player (...) VALUES (...)\ntry player.insert(db)\n```\n\n> **Note**: If you specify the `ignore` policy for inserts, the [`didInsert` callback](#persistence-callbacks) will be called with some random id in case of failed insert. You can detect failed insertions with `insertAndFetch`:\n>     \n> ```swift\n> // How to detect failed `INSERT OR IGNORE`:\n> // INSERT OR IGNORE INTO player ... RETURNING *\n> do {\n>     let insertedPlayer = try player.insertAndFetch(db) {\n>     // Successful insertion\n> catch RecordError.recordNotFound {\n>     // Failed insertion due to IGNORE policy\n> }\n> ```\n>\n> **Note**: The `replace` policy may have to delete rows so that inserts and updates can succeed. Those deletions are not reported to [transaction observers](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/transactionobserver) (this might change in a future release of SQLite).\n\n### Beyond FetchableRecord\n\n**Some GRDB users eventually discover that the [FetchableRecord] protocol does not fit all situations.** Use cases that are not well handled by FetchableRecord include:\n\n- Your application needs polymorphic row decoding: it decodes some type or another, depending on the values contained in a database row.\n\n- Your application needs to decode rows with a context: each decoded value should be initialized with some extra value that does not come from the database.\n\nSince those use cases are not well handled by FetchableRecord, don't try to implement them on top of this protocol: you'll just fight the framework.\n\n\n## Examples of Record Definitions\n\nWe will show below how to declare a record type for the following database table:\n\n```swift\ntry dbQueue.write { db in\n    try db.create(table: \"place\") { t in\n        t.autoIncrementedPrimaryKey(\"id\")\n        t.column(\"title\", .text).notNull()\n        t.column(\"isFavorite\", .boolean).notNull().defaults(to: false)\n        t.column(\"longitude\", .double).notNull()\n        t.column(\"latitude\", .double).notNull()\n    }\n}\n```\n\nEach one of the three examples below is correct. You will pick one or the other depending on your personal preferences and the requirements of your application:\n\n<details>\n  <summary>Define a Codable struct, and adopt the record protocols you need</summary>\n\nThis is the shortest way to define a record type.\n\nSee the [Record Protocols Overview](#record-protocols-overview), and [Codable Records] for more information.\n\n```swift\nstruct Place: Codable {\n    var id: Int64?\n    var title: String\n    var isFavorite: Bool\n    private var latitude: CLLocationDegrees\n    private var longitude: CLLocationDegrees\n    \n    var coordinate: CLLocationCoordinate2D {\n        get {\n            CLLocationCoordinate2D(\n                latitude: latitude,\n                longitude: longitude)\n        }\n        set {\n            latitude = newValue.latitude\n            longitude = newValue.longitude\n        }\n    }\n}\n\n// SQL generation\nextension Place: TableRecord {\n    /// The table columns\n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let title = Column(CodingKeys.title)\n        static let isFavorite = Column(CodingKeys.isFavorite)\n        static let latitude = Column(CodingKeys.latitude)\n        static let longitude = Column(CodingKeys.longitude)\n    }\n}\n\n// Fetching methods\nextension Place: FetchableRecord { }\n\n// Persistence methods\nextension Place: MutablePersistableRecord {\n    // Update auto-incremented id upon successful insertion\n    mutating func didInsert(_ inserted: InsertionSuccess) {\n        id = inserted.rowID\n    }\n}\n```\n\n</details>\n\n<details>\n  <summary>Define a plain struct, and adopt the record protocols you need</summary>\n\nSee the [Record Protocols Overview](#record-protocols-overview) for more information.\n    \n```swift\nstruct Place {\n    var id: Int64?\n    var title: String\n    var isFavorite: Bool\n    var coordinate: CLLocationCoordinate2D\n}\n\n// SQL generation\nextension Place: TableRecord {\n    /// The table columns\n    enum Columns {\n        static let id = Column(\"id\")\n        static let title = Column(\"title\")\n        static let isFavorite = Column(\"isFavorite\")\n        static let latitude = Column(\"latitude\")\n        static let longitude = Column(\"longitude\")\n    }\n}\n\n// Fetching methods\nextension Place: FetchableRecord {\n    /// Creates a record from a database row\n    init(row: Row) {\n        id = row[Columns.id]\n        title = row[Columns.title]\n        isFavorite = row[Columns.isFavorite]\n        coordinate = CLLocationCoordinate2D(\n            latitude: row[Columns.latitude],\n            longitude: row[Columns.longitude])\n    }\n}\n\n// Persistence methods\nextension Place: MutablePersistableRecord {\n    /// The values persisted in the database\n    func encode(to container: inout PersistenceContainer) {\n        container[Columns.id] = id\n        container[Columns.title] = title\n        container[Columns.isFavorite] = isFavorite\n        container[Columns.latitude] = coordinate.latitude\n        container[Columns.longitude] = coordinate.longitude\n    }\n    \n    // Update auto-incremented id upon successful insertion\n    mutating func didInsert(_ inserted: InsertionSuccess) {\n        id = inserted.rowID\n    }\n}\n```\n\n</details>\n\n<details>\n  <summary>Define a plain struct optimized for fetching performance</summary>\n\nThis struct derives its persistence methods from the standard Encodable protocol (see [Codable Records]), but performs optimized row decoding by accessing database columns with numeric indexes.\n\nSee the [Record Protocols Overview](#record-protocols-overview) for more information.\n    \n```swift\nstruct Place: Encodable {\n    var id: Int64?\n    var title: String\n    var isFavorite: Bool\n    private var latitude: CLLocationDegrees\n    private var longitude: CLLocationDegrees\n    \n    var coordinate: CLLocationCoordinate2D {\n        get {\n            CLLocationCoordinate2D(\n                latitude: latitude,\n                longitude: longitude)\n        }\n        set {\n            latitude = newValue.latitude\n            longitude = newValue.longitude\n        }\n    }\n}\n\n// SQL generation\nextension Place: TableRecord {\n    /// The table columns\n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let title = Column(CodingKeys.title)\n        static let isFavorite = Column(CodingKeys.isFavorite)\n        static let latitude = Column(CodingKeys.latitude)\n        static let longitude = Column(CodingKeys.longitude)\n    }\n    \n    /// Arrange the selected columns and lock their order\n    static var databaseSelection: [any SQLSelectable] {\n        [\n            Columns.id,\n            Columns.title,\n            Columns.favorite,\n            Columns.latitude,\n            Columns.longitude,\n        ]\n    }\n}\n\n// Fetching methods\nextension Place: FetchableRecord {\n    /// Creates a record from a database row\n    init(row: Row) {\n        // For high performance, use numeric indexes that match the\n        // order of Place.databaseSelection\n        id = row[0]\n        title = row[1]\n        isFavorite = row[2]\n        coordinate = CLLocationCoordinate2D(\n            latitude: row[3],\n            longitude: row[4])\n    }\n}\n\n// Persistence methods\nextension Place: MutablePersistableRecord {\n    // Update auto-incremented id upon successful insertion\n    mutating func didInsert(_ inserted: InsertionSuccess) {\n        id = inserted.rowID\n    }\n}\n```\n\n</details>\n\n\nThe Query Interface\n===================\n\n**The query interface lets you write pure Swift instead of SQL:**\n\n```swift\ntry dbQueue.write { db in\n    // Update database schema\n    try db.create(table: \"player\") { t in ... }\n    \n    // Fetch records\n    let bestPlayers = try Player\n        .order(\\.score.desc)\n        .limit(10)\n        .fetchAll(db)\n    \n    // Count\n    let count = try Player\n        .filter { $0.score >= 1000 }\n        .fetchCount(db)\n    \n    // Batch update\n    try Player\n        .filter { $0.team == \"Reds\" }\n        .updateAll(db) { $0.score += 100 }\n    \n    // Batch delete\n    try Player\n        .filter { $0.score == 0 }\n        .deleteAll(db)\n}\n```\n\nYou need to open a [database connection] before you can query the database.\n\nPlease bear in mind that the query interface can not generate all possible SQL queries. You may also *prefer* writing SQL, and this is just OK. From little snippets to full queries, your SQL skills are welcome:\n\n```swift\ntry dbQueue.write { db in\n    // Update database schema (with SQL)\n    try db.execute(sql: \"CREATE TABLE player (...)\")\n    \n    // Fetch records (with SQL)\n    let bestPlayers = try Player.fetchAll(db, sql: \"\"\"\n        SELECT * FROM player ORDER BY score DESC LIMIT 10\n        \"\"\")\n    \n    // Count (with an SQL snippet)\n    let minScore = 1000\n    let count = try Player\n        .filter(sql: \"score >= ?\", arguments: [minScore])\n        .fetchCount(db)\n    \n    // Update (with SQL)\n    try db.execute(sql: \"UPDATE player SET score = score + 100 WHERE team = 'Reds'\")\n    \n    // Delete (with SQL)\n    try db.execute(sql: \"DELETE FROM player WHERE score = 0\")\n}\n```\n\nSo don't miss the [SQL API](#sqlite-api).\n\n> **Note**: the generated SQL may change between GRDB releases, without notice: don't have your application rely on any specific SQL output.\n\n- [The Database Schema](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseschema)\n- [Requests](#requests)\n- [Expressions](#expressions)\n    - [SQL Operators](#sql-operators)\n    - [SQL Functions](#sql-functions)\n- [Embedding SQL in Query Interface Requests]\n- [Fetching from Requests]\n- [Fetching by Key](#fetching-by-key)\n- [Testing for Record Existence](#testing-for-record-existence)\n- [Fetching Aggregated Values](#fetching-aggregated-values)\n- [Delete Requests](#delete-requests)\n- [Update Requests](#update-requests)\n- [Custom Requests](#custom-requests)\n- :blue_book: [Associations and Joins](Documentation/AssociationsBasics.md)\n- :blue_book: [Common Table Expressions]\n- :blue_book: [Query Interface Organization]\n\n## Requests\n\nğŸ“– [`QueryInterfaceRequest`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterfacerequest), [`Table`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/table)\n\n**The query interface requests** let you fetch values from the database:\n\n```swift\nlet request = Player.filter { $0.email != nil }.order(\\.name)\nlet players = try request.fetchAll(db)  // [Player]\nlet count = try request.fetchCount(db)  // Int\n```\n\nQuery interface requests usually start from **a type** that adopts the `TableRecord` protocol:\n\n```swift\nstruct Player: TableRecord { ... }\n\n// The request for all players:\nlet request = Player.all()\nlet players = try request.fetchAll(db) // [Player]\n```\n\nWhen you can not use a record type, use `Table`:\n\n```swift\n// The request for all rows from the player table:\nlet table = Table(\"player\")\nlet request = table.all()\nlet rows = try request.fetchAll(db)    // [Row]\n\n// The request for all players from the player table:\nlet table = Table<Player>(\"player\")\nlet request = table.all()\nlet players = try request.fetchAll(db) // [Player]\n```\n\n> **Note**: all examples in the documentation below use a record type, but you can always substitute a `Table` instead.\n\nNext, declare the table **columns** that you want to use for filtering, or sorting, in a nested type named `Columns`:\n\n```swift\nextension Player {\n    enum Columns {\n        static let id = Column(\"id\")\n        static let name = Column(\"name\")\n    }\n}\n```\n\nWhen `Player` is `Codable`, you'll prefer defining columns from coding keys:\n\n```swift\nextension Player {\n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let name = Column(CodingKeys.name)\n    }\n}\n```\n\nYou can now build requests with the following methods: `all`, `none`, `select`, `distinct`, `filter`, `matching`, `group`, `having`, `order`, `reversed`, `limit`, `joining`, `including`, `with`. All those methods return another request, which you can further refine by applying another method: `Player.select(...).filter(...).order(...)`.\n\n- [`all()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerecord/all()), [`none()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerecord/none()): the requests for all rows, or no row.\n\n    ```swift\n    // SELECT * FROM player\n    Player.all()\n    ```\n    \n    By default, all columns are selected. See [Columns Selected by a Request].\n\n- [`select(...)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/selectionrequest/select(_:)-ruzy) and [`select(..., as:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterfacerequest/select(_:as:)-58954) define the selected columns. See [Columns Selected by a Request].\n    \n    ```swift\n    // SELECT name FROM player\n    Player.select(\\.name, as: String.self)\n    ```\n\n- [`selectID()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterfacerequest/selectID()) is available on [Identifiable Records]. It supports all tables that have a single-column primary key:\n\n    ```swift\n    // SELECT id FROM player\n    Player.selectID()\n    \n    // SELECT id FROM player WHERE name IS NOT NULL\n    Player.filter { $0.name != nil }.selectID()\n    ```\n\n- [`annotated(with: expression...)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/selectionrequest/annotated(with:)-1satx) extends the selection.\n\n    ```swift\n    // SELECT *, (score + bonus) AS total FROM player\n    Player.annotated { ($0.score + $0.bonus).forKey(\"total\") }\n    ```\n\n- [`annotated(with: aggregate)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/derivablerequest/annotated(with:)-74xfs) extends the selection with [association aggregates](Documentation/AssociationsBasics.md#association-aggregates).\n    \n    ```swift\n    // SELECT team.*, COUNT(DISTINCT player.id) AS playerCount\n    // FROM team\n    // LEFT JOIN player ON player.teamId = team.id\n    // GROUP BY team.id\n    Team.annotated(with: Team.players.count)\n    ```\n\n- [`annotated(withRequired: association)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/annotated(withrequired:)) and [`annotated(withOptional: association)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/annotated(withoptional:)) extends the selection with [Associations].\n    \n    ```swift\n    // SELECT player.*, team.color\n    // FROM player\n    // JOIN team ON team.id = player.teamId\n    Player.annotated(withRequired: Player.team.select(\\.color))\n    ```\n\n- [`distinct()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/derivablerequest/distinct()) performs uniquing.\n    \n    ```swift\n    // SELECT DISTINCT name FROM player\n    Player.select(\\.name, as: String.self).distinct()\n    ```\n\n- [`filter(expression)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/filteredrequest/filter(_:)-6xr3d) applies conditions.\n    \n    ```swift\n    // SELECT * FROM player WHERE id IN (1, 2, 3)\n    Player.filter { [1,2,3].contains($0.id) }\n    \n    // SELECT * FROM player WHERE (name IS NOT NULL) AND (height > 1.75)\n    Player.filter { $0.name != nil && $0.height > 1.75 }\n    ```\n\n- [`filter(id:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/filter(id:)) and [`filter(ids:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/filter(ids:)) are type-safe methods available on [Identifiable Records]:\n    \n    ```swift\n    // SELECT * FROM player WHERE id = 1\n    Player.filter(id: 1)\n    \n    // SELECT * FROM country WHERE isoCode IN ('FR', 'US')\n    Country.filter(ids: [\"FR\", \"US\"])\n    ```\n    \n- [`filter(key:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/filter(key:)-1p9sq) and [`filter(keys:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/filter(keys:)-6ggt1) apply conditions on primary and unique keys:\n    \n    ```swift\n    // SELECT * FROM player WHERE id = 1\n    Player.filter(key: 1)\n    \n    // SELECT * FROM country WHERE isoCode IN ('FR', 'US')\n    Country.filter(keys: [\"FR\", \"US\"])\n    \n    // SELECT * FROM citizenship WHERE citizenId = 1 AND countryCode = 'FR'\n    Citizenship.filter(key: [\"citizenId\": 1, \"countryCode\": \"FR\"])\n    \n    // SELECT * FROM player WHERE email = 'arthur@example.com'\n    Player.filter(key: [\"email\": \"arthur@example.com\"])\n    ```\n\n- `matching(pattern)` ([FTS3](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/matching(_:)-3s3zr), [FTS5](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/matching(_:)-7c1e8)) performs [full-text search](Documentation/FullTextSearch.md).\n    \n    ```swift\n    // SELECT * FROM document WHERE document MATCH 'sqlite database'\n    let pattern = FTS3Pattern(matchingAllTokensIn: \"SQLite database\")\n    Document.matching(pattern)\n    ```\n    \n    When the pattern is nil, no row will match.\n\n- [`group(expression, ...)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/aggregatingrequest/group(_:)-2g7br) groups rows.\n    \n    ```swift\n    // SELECT name, MAX(score) FROM player GROUP BY name\n    Player\n        .select { [$0.name, max($0.score)] }\n        .group(\\.name)\n    ```\n\n- [`having(expression)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/aggregatingrequest/having(_:)-2oggh) applies conditions on grouped rows.\n    \n    ```swift\n    // SELECT team, MAX(score) FROM player GROUP BY team HAVING MIN(score) >= 1000\n    Player\n        .select { [$0.team, max($0.score)] }\n        .group(\\.team)\n        .having { min($0.score) >= 1000 }\n    ```\n\n- [`having(aggregate)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/derivablerequest/having(_:)) applies conditions on grouped rows, according to an [association aggregate](Documentation/AssociationsBasics.md#association-aggregates).\n    \n    ```swift\n    // SELECT team.*\n    // FROM team\n    // LEFT JOIN player ON player.teamId = team.id\n    // GROUP BY team.id\n    // HAVING COUNT(DISTINCT player.id) >= 5\n    Team.having(Team.players.count >= 5)\n    ```\n\n- [`order(ordering, ...)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/orderedrequest/order(_:)-9d0hr) sorts.\n    \n    ```swift\n    // SELECT * FROM player ORDER BY name\n    Player.order(\\.name)\n    \n    // SELECT * FROM player ORDER BY score DESC\n    Player.order(\\.score.desc)\n    \n    // SELECT * FROM player ORDER BY score DESC, name\n    Player.order { [$0.score.desc, $0.name] }\n    ```\n    \n    SQLite considers NULL values to be smaller than any other values for sorting purposes. Hence, NULLs naturally appear at the beginning of an ascending ordering and at the end of a descending ordering. With a [custom SQLite build], this can be changed using `.ascNullsLast` and `.descNullsFirst`:\n    \n    ```swift\n    // SELECT * FROM player ORDER BY score ASC NULLS LAST\n    Player.order(\\.name.ascNullsLast)\n    ```\n    \n    Each `order` call clears any previous ordering:\n    \n    ```swift\n    // SELECT * FROM player ORDER BY name\n    Player.order(\\.score).order(\\.name)\n    ```\n\n- [`reversed()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/orderedrequest/reversed()) reverses the eventual orderings.\n    \n    ```swift\n    // SELECT * FROM player ORDER BY score ASC, name DESC\n    Player.order { [$0.score.desc, $0.name] }.reversed()\n    ```\n    \n    If no ordering was already specified, this method has no effect:\n    \n    ```swift\n    // SELECT * FROM player\n    Player.all().reversed()\n    ```\n\n- [`limit(limit, offset: offset)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterfacerequest/limit(_:offset:)) limits and pages results.\n    \n    ```swift\n    // SELECT * FROM player LIMIT 5\n    Player.limit(5)\n    \n    // SELECT * FROM player LIMIT 5 OFFSET 10\n    Player.limit(5, offset: 10)\n    ```\n\n- [`joining(required:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/joining(required:)), [`joining(optional:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/joining(optional:)), [`including(required:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/including(required:)), [`including(optional:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/including(optional:)), and [`including(all:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/including(all:)) fetch and join records through [Associations].\n    \n    ```swift\n    // SELECT player.*, team.*\n    // FROM player\n    // JOIN team ON team.id = player.teamId\n    Player.including(required: Player.team)\n    ```\n\n- [`with(cte)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/derivablerequest/with(_:)) embeds a [common table expression]:\n    \n    ```swift\n    // WITH ... SELECT * FROM player\n    let cte = CommonTableExpression(...)\n    Player.with(cte)\n    ```\n\n- Other requests that involve the primary key:\n    \n    - [`selectPrimaryKey(as:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterfacerequest/selectprimarykey(as:)) selects the primary key.\n    \n        ```swift\n        // SELECT id FROM player\n        Player.selectPrimaryKey(as: Int64.self)    // QueryInterfaceRequest<Int64>\n        \n        // SELECT code FROM country\n        Country.selectPrimaryKey(as: String.self)  // QueryInterfaceRequest<String>\n        \n        // SELECT citizenId, countryCode FROM citizenship\n        Citizenship.selectPrimaryKey(as: Row.self) // QueryInterfaceRequest<Row>\n        ```\n        \n    - [`orderByPrimaryKey()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/orderbyprimarykey()) sorts by primary key.\n        \n        ```swift\n        // SELECT * FROM player ORDER BY id\n        Player.orderByPrimaryKey()\n        \n        // SELECT * FROM country ORDER BY code\n        Country.orderByPrimaryKey()\n        \n        // SELECT * FROM citizenship ORDER BY citizenId, countryCode\n        Citizenship.orderByPrimaryKey()\n        ```\n    \n    - [`groupByPrimaryKey()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/groupbyprimarykey()) groups rows by primary key.\n\n\nYou can refine requests by chaining those methods:\n\n```swift\n// SELECT * FROM player WHERE (email IS NOT NULL) ORDER BY name\nPlayer.order(\\.name).filter { $0.email != nil }\n```\n\nThe `select`, `order`, `group`, and `limit` methods ignore and replace previously applied selection, orderings, grouping, and limits. On the opposite, `filter`, `matching`, and `having` methods extend the query:\n\n```swift\nPlayer                          // SELECT * FROM player\n    .filter { $0.name != nil }  // WHERE (name IS NOT NULL)\n    .filter { $0.email != nil } //        AND (email IS NOT NULL)\n    .order(\\.name)              // - ignored -\n    .reversed()                 // - ignored -\n    .order(\\.score)             // ORDER BY score\n    .limit(20, offset: 40)      // - ignored -\n    .limit(10)                  // LIMIT 10\n```\n\n\nRaw SQL snippets are also accepted, with eventual [arguments](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statementarguments):\n\n```swift\n// SELECT DATE(creationDate), COUNT(*) FROM player WHERE name = 'Arthur' GROUP BY date(creationDate)\nPlayer\n    .select(sql: \"DATE(creationDate), COUNT(*)\")\n    .filter(sql: \"name = ?\", arguments: [\"Arthur\"])\n    .group(sql: \"DATE(creationDate)\")\n```\n\n\n### Columns Selected by a Request\n\nBy default, query interface requests select all columns:\n\n```swift\n// SELECT * FROM player\nstruct Player: TableRecord { ... }\nlet request = Player.all()\n\n// SELECT * FROM player\nlet table = Table(\"player\")\nlet request = table.all()\n```\n\n**The selection can be changed for each individual requests, or in the case of record-based requests, for all requests built from this record type.**\n\nThe `select(...)` and `select(..., as:)` methods change the selection of a single request (see [Fetching from Requests] for detailed information):\n\n```swift\nlet request = Player.select { max($0.score) }\nlet maxScore = try Int.fetchOne(db, request) // Int?\n\nlet request = Player.select({ max($0.score) }, as: Int.self)\nlet maxScore = try request.fetchOne(db)      // Int?\n```\n\nThe default selection for a record type is controlled by the `databaseSelection` property. For example:\n\n```swift\n// Select a limited set of columns\nstruct RestrictedPlayer: TableRecord {\n    static let databaseTableName = \"player\"\n    \n    enum Columns {\n        static let id = Column(\"id\")\n        static let name = Column(\"name\")\n    }\n    \n    static var databaseSelection: [any SQLSelectable] {\n        [Columns.id, Columns.name]\n    }\n}\n\n// SELECT id, name FROM player\nlet request = RestrictedPlayer.all()\n```\n\n```swift\n// Select all but a few columns\nstruct Player : TableRecord {\n    static var databaseSelection: [any SQLSelectable] { \n        [.allColumns(excluding: [\"generatedColumn\"])]\n    }\n}\n\n// SELECT id, name FROM player\nlet request = RestrictedPlayer.all()\n```\n\n```swift\n// Select all columns and more\nstruct ExtendedPlayer : TableRecord {\n    static let databaseTableName = \"player\"\n    static var databaseSelection: [any SQLSelectable] {\n        [.allColumns, .rowID]\n    }\n}\n\n// SELECT *, rowid FROM player\nlet request = ExtendedPlayer.all()\n```\n\n> **Note**: make sure the `databaseSelection` property is explicitly declared as `[any SQLSelectable]`. If it is not, the Swift compiler may silently miss the protocol requirement, resulting in sticky `SELECT *` requests. To verify your setup, see the [How do I print a request as SQL?](#how-do-i-print-a-request-as-sql) FAQ.\n\n\n## Expressions\n\nFeed [requests](#requests) with SQL expressions built from your Swift code:\n\n\n### SQL Operators\n\nğŸ“– [`SQLSpecificExpressible`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/sqlspecificexpressible)\n\nGRDB comes with a Swift version of many SQLite [built-in operators](https://sqlite.org/lang_expr.html#operators), listed below. But not all: see [Embedding SQL in Query Interface Requests] for a way to add support for missing SQL operators.\n\n- `=`, `<>`, `<`, `<=`, `>`, `>=`, `IS`, `IS NOT`\n    \n    Comparison operators are based on the Swift operators `==`, `!=`, `===`, `!==`, `<`, `<=`, `>`, `>=`:\n    \n    ```swift\n    // SELECT * FROM player WHERE (name = 'Arthur')\n    Player.filter { $0.name == \"Arthur\" }\n    \n    // SELECT * FROM player WHERE (name IS NULL)\n    Player.filter { $0.name == nil }\n    \n    // SELECT * FROM player WHERE (score IS 1000)\n    Player.filter { $0.score === 1000 }\n    \n    // SELECT * FROM rectangle WHERE width < height\n    Rectangle.filter { $0.width < $0.height }\n    ```\n    \n    Subqueries are supported:\n    \n    ```swift\n    // SELECT * FROM player WHERE score = (SELECT max(score) FROM player)\n    let maximumScore = Player.select { max($0.score) }\n    Player.filter { $0.score == maximumScore }\n    \n    // SELECT * FROM player WHERE score = (SELECT max(score) FROM player)\n    let maximumScore = SQLRequest(\"SELECT max(score) FROM player\")\n    Player.filter { $0.score == maximumScore }\n    ```\n    \n    > **Note**: SQLite string comparison, by default, is case-sensitive and not Unicode-aware. See [string comparison](#string-comparison) if you need more control.\n\n- `*`, `/`, `+`, `-`\n    \n    SQLite arithmetic operators are derived from their Swift equivalent:\n    \n    ```swift\n    // SELECT ((temperature * 1.8) + 32) AS fahrenheit FROM planet\n    Planet.select { ($0.temperature * 1.8 + 32).forKey(\"fahrenheit\") }\n    ```\n    \n    > **Note**: an expression like `nameColumn + \"rrr\"` will be interpreted by SQLite as a numerical addition (with funny results), not as a string concatenation. See the `concat` operator below.\n    \n    When you want to join a sequence of expressions with the `+` or `*` operator, use `joined(operator:)`:\n    \n    ```swift\n    // SELECT score + bonus + 1000 FROM player\n    Player.select {\n        [$0.score, $0.bonus, 1000.databaseValue].joined(operator: .add)\n    }\n    ```\n    \n    Note in the example above how you concatenate raw values: `1000.databaseValue`. A plain `1000` would not compile.\n    \n    When the sequence is empty, `joined(operator: .add)` returns 0, and `joined(operator: .multiply)` returns 1.\n\n- `&`, `|`, `~`, `<<`, `>>`\n    \n    Bitwise operations (bitwise and, or, not, left shift, right shift) are derived from their Swift equivalent:\n    \n    ```swift\n    // SELECT mask & 2 AS isRocky FROM planet\n    Planet.select { ($0.mask & 2).forKey(\"isRocky\") }\n    ```\n\n- `||`\n    \n    Concatenate several strings:\n    \n    ```swift\n    // SELECT firstName || ' ' || lastName FROM player\n    Player.select {\n        [$0.firstName, \" \".databaseValue, $0.lastName].joined(operator: .concat)\n    }\n    ```\n    \n    Note in the example above how you concatenate raw strings: `\" \".databaseValue`. A plain `\" \"` would not compile.\n    \n    When the sequence is empty, `joined(operator: .concat)` returns the empty string.\n\n- `AND`, `OR`, `NOT`\n    \n    The SQL logical operators are derived from the Swift `&&`, `||` and `!`:\n    \n    ```swift\n    // SELECT * FROM player WHERE ((NOT isVerified) OR (score < 1000))\n    Player.filter { !$0.isVerified || $0.score < 1000 }\n    ```\n    \n    When you want to join a sequence of expressions with the `AND` or `OR` operator, use `joined(operator:)`:\n    \n    ```swift\n    // SELECT * FROM player WHERE (isVerified AND (score >= 1000) AND (name IS NOT NULL))\n    Player.filter {\n        [$0.isVerified, $0.score >= 1000, $0.name != nil].joined(operator: .and)\n    }\n    ```\n    \n    When the sequence is empty, `joined(operator: .and)` returns true, and `joined(operator: .or)` returns false:\n\n- `BETWEEN`, `IN`, `NOT IN`\n    \n    To check inclusion in a Swift sequence (array, set, rangeâ€¦), call the `contains` method:\n    \n    ```swift\n    // SELECT * FROM player WHERE id IN (1, 2, 3)\n    Player.filter { [1, 2, 3].contains($0.id) }\n    \n    // SELECT * FROM player WHERE id NOT IN (1, 2, 3)\n    Player.filter { ![1, 2, 3].contains($0.id) }\n    \n    // SELECT * FROM player WHERE score BETWEEN 0 AND 1000\n    Player.filter { (0...1000).contains($0.score) }\n    \n    // SELECT * FROM player WHERE (score >= 0) AND (score < 1000)\n    Player.filter { (0..<1000).contains($0.score) }\n    \n    // SELECT * FROM player WHERE initial BETWEEN 'A' AND 'N'\n    Player.filter { (\"A\"...\"N\").contains($0.initial) }\n    \n    // SELECT * FROM player WHERE (initial >= 'A') AND (initial < 'N')\n    Player.filter { (\"A\"..<\"N\").contains($0.initial) }\n    ```\n    \n    To check inclusion inside a subquery, call the `contains` method as well:\n    \n    ```swift\n    // SELECT * FROM player WHERE id IN (SELECT playerId FROM playerSelection)\n    let selectedPlayerIds = PlayerSelection.select(\\.playerId)\n    Player.filter { selectedPlayerIds.contains($0.id) }\n    \n    // SELECT * FROM player WHERE id IN (SELECT playerId FROM playerSelection)\n    let selectedPlayerIds = SQLRequest(\"SELECT playerId FROM playerSelection\")\n    Player.filter { selectedPlayerIds.contains($0.id) }\n    ```\n    \n    To check inclusion inside a [common table expression], call the `contains` method as well:\n    \n    ```swift\n    // WITH selectedName AS (...)\n    // SELECT * FROM player WHERE name IN selectedName\n    let cte = CommonTableExpression(named: \"selectedName\", ...)\n    Player\n        .with(cte)\n        .filter { cte.contains($0.name) }\n    ```\n    \n    > **Note**: SQLite string comparison, by default, is case-sensitive and not Unicode-aware. See [string comparison](#string-comparison) if you need more control.\n\n- `EXISTS`, `NOT EXISTS`\n    \n    To check if a subquery would return rows, call the `exists` method:\n    \n    ```swift\n    // Teams that have at least one other player\n    //\n    //  SELECT * FROM team\n    //  WHERE EXISTS (SELECT * FROM player WHERE teamId = team.id)\n    let teamAlias = TableAlias<Team>()\n    let player = Player.filter { $0.teamId == teamAlias.id }\n    let teams = Team.aliased(teamAlias).filter(player.exists())\n    \n    // Teams that have no player\n    //\n    //  SELECT * FROM team\n    //  WHERE NOT EXISTS (SELECT * FROM player WHERE teamId = team.id)\n    let teams = Team.aliased(teamAlias).filter(!player.exists())\n    ```\n    \n    In the above example, you use a `TableAlias` in order to let a subquery refer to a column from another table.\n    \n    In the next example, which involves the same table twice, the table alias requires an explicit disambiguation with `TableAlias(name:)`:\n    \n    ```swift    \n    // Players who coach at least one other player\n    //\n    //  SELECT coach.* FROM player coach\n    //  WHERE EXISTS (SELECT * FROM player WHERE coachId = coach.id)\n    let coachAlias = TableAlias<Player>(name: \"coach\")\n    let coachedPlayer = Player.filter { $0.coachId == coachAlias.id }\n    let coaches = Player.aliased(coachAlias).filter(coachedPlayer.exists())\n    ```\n    \n    Finally, subqueries can also be expressed as SQL, with [SQL Interpolation]:\n    \n    ```swift\n    // SELECT coach.* FROM player coach\n    // WHERE EXISTS (SELECT * FROM player WHERE coachId = coach.id)\n    let coachedPlayer = SQLRequest(\"SELECT * FROM player WHERE coachId = \\(coachAlias.id)\")\n    let coaches = Player.aliased(coachAlias).filter(coachedPlayer.exists())\n    ```\n    \n- `LIKE`\n    \n    The SQLite LIKE operator is available as the `like` method:\n    \n    ```swift\n    // SELECT * FROM player WHERE (email LIKE '%@example.com')\n    Player.filter { $0.email.like(\"%@example.com\") }\n    \n    // SELECT * FROM book WHERE (title LIKE '%10\\%%' ESCAPE '\\')\n    Player.filter { $0.email.like(\"%10\\\\%%\", escape: \"\\\\\") }\n    ```\n    \n    > **Note**: the SQLite LIKE operator is case-insensitive but not Unicode-aware. For example, the expression `'a' LIKE 'A'` is true but `'Ã¦' LIKE 'Ã†'` is false.\n\n- `MATCH`\n    \n    The full-text MATCH operator is available through [FTS3Pattern](Documentation/FullTextSearch.md#fts3pattern) (for FTS3 and FTS4 tables) and [FTS5Pattern](Documentation/FullTextSearch.md#fts5pattern) (for FTS5):\n    \n    FTS3 and FTS4:\n    \n    ```swift\n    let pattern = FTS3Pattern(matchingAllTokensIn: \"SQLite database\")\n    \n    // SELECT * FROM document WHERE document MATCH 'sqlite database'\n    Document.matching(pattern)\n    \n    // SELECT * FROM document WHERE content MATCH 'sqlite database'\n    Document.filter { $0.content.match(pattern) }\n    ```\n    \n    FTS5:\n    \n    ```swift\n    let pattern = FTS5Pattern(matchingAllTokensIn: \"SQLite database\")\n    \n    // SELECT * FROM document WHERE document MATCH 'sqlite database'\n    Document.matching(pattern)\n    ```\n- `AS`\n    \n    To give an alias to an expression, use the `forKey` method:\n    \n    ```swift\n    // SELECT (score + bonus) AS total\n    // FROM player\n    Player.select { ($0.score + $0.bonus).forKey(\"total\") }\n    ```\n    \n    If you need to refer to this aliased column in another place of the request, use a detached column:\n    \n    ```swift\n    // SELECT (score + bonus) AS total\n    // FROM player \n    // ORDER BY total\n    Player\n        .select { ($0.score + $0.bonus).forKey(\"total\") }\n        .order(Column(\"total\").detached)\n    ```\n    \n    The detached column `Column(\"total\").detached` is not considered as a part of the \"player\" table, so it is always rendered as `total` in the generated SQL, even when the request involves other tables via an [association](Documentation/AssociationsBasics.md) or a [common table expression].\n\n\n### SQL Functions\n\nğŸ“– [`SQLSpecificExpressible`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/sqlspecificexpressible)\n\nGRDB comes with a Swift version of many SQLite [built-in functions](https://sqlite.org/lang_corefunc.html), listed below. But not all: see [Embedding SQL in Query Interface Requests] for a way to add support for missing SQL functions.\n\n- `ABS`, `AVG`, `COALESCE`, `COUNT`, `DATETIME`, `JULIANDAY`, `LENGTH`, `MAX`, `MIN`, `SUM`, `TOTAL`:\n    \n    Those are based on the `abs`, `average`, `coalesce`, `count`, `dateTime`, `julianDay`, `length`, `max`, `min`, `sum`, and `total` Swift functions:\n    \n    ```swift\n    // SELECT MIN(score), MAX(score) FROM player\n    Player.select { [min($0.score), max($0.score)] }\n    \n    // SELECT COUNT(name) FROM player\n    Player.select { count($0.name) }\n    \n    // SELECT COUNT(DISTINCT name) FROM player\n    Player.select { count(distinct: $0.name) }\n    \n    // SELECT JULIANDAY(date, 'start of year') FROM game\n    Game.select { julianDay($0.date, .startOfYear) }\n    ```\n    \n    For more information about the functions `dateTime` and `julianDay`, see [Date And Time Functions](https://www.sqlite.org/lang_datefunc.html).\n\n- `CAST`\n\n    Use the `cast` Swift function:\n    \n    ```swift\n    // SELECT (CAST(wins AS REAL) / games) AS successRate FROM player\n    Player.select { (cast($0.wins, as: .real) / $0.games).forKey(\"successRate\") }\n    ```\n    \n    See [CAST expressions](https://www.sqlite.org/lang_expr.html#castexpr) for more information about SQLite conversions.\n\n- `IFNULL`\n    \n    Use the Swift `??` operator:\n    \n    ```swift\n    // SELECT IFNULL(name, 'Anonymous') FROM player\n    Player.select { $0.name ?? \"Anonymous\" }\n    \n    // SELECT IFNULL(name, email) FROM player\n    Player.select { $0.name ?? $0.email }\n    ```\n\n- `LOWER`, `UPPER`\n    \n    The query interface does not give access to those SQLite functions. Nothing against them, but they are not unicode aware.\n    \n    Instead, GRDB extends SQLite with SQL functions that call the Swift built-in string functions `capitalized`, `lowercased`, `uppercased`, `localizedCapitalized`, `localizedLowercased` and `localizedUppercased`:\n    \n    ```swift\n    Player.select { $0.name.uppercased() }\n    ```\n    \n    > **Note**: When *comparing* strings, you'd rather use a [collation](#string-comparison):\n    >\n    > ```swift\n    > let name: String = ...\n    >\n    > // Not recommended\n    > Player.filter { $0.name.uppercased() == name.uppercased() }\n    >\n    > // Better\n    > Player.filter { $0.name.collating(.caseInsensitiveCompare) == name }\n    > ```\n\n- Custom SQL functions and aggregates\n    \n    You can apply your own [custom SQL functions and aggregates](#custom-functions-):\n    \n    ```swift\n    let myFunction = DatabaseFunction(\"myFunction\", ...)\n    \n    // SELECT myFunction(name) FROM player\n    Player.select { myFunction($0.name) }\n    ```\n\n## Embedding SQL in Query Interface Requests\n\nYou will sometimes want to extend your query interface requests with SQL snippets. This can happen because GRDB does not provide a Swift interface for some SQL function or operator, or because you want to use an SQLite construct that GRDB does not support.\n\nSupport for extensibility is large, but not unlimited. All the SQL queries built by the query interface request have the shape below. _If you need something else, you'll have to use [raw SQL requests](#sqlite-api)._\n\n```sql\nWITH ...     -- 1\nSELECT ...   -- 2\nFROM ...     -- 3\nJOIN ...     -- 4\nWHERE ...    -- 5\nGROUP BY ... -- 6\nHAVING ...   -- 7\nORDER BY ... -- 8\nLIMIT ...    -- 9\n```\n\n1. `WITH ...`: see [Common Table Expressions].\n\n2. `SELECT ...`\n\n    The selection can be provided as raw SQL:\n    \n    ```swift\n    // SELECT IFNULL(name, 'O''Brien'), score FROM player\n    let request = Player.select(sql: \"IFNULL(name, 'O''Brien'), score\")\n    \n    // SELECT IFNULL(name, 'O''Brien'), score FROM player\n    let defaultName = \"O'Brien\"\n    let request = Player.select(sql: \"IFNULL(name, ?), score\", arguments: [suffix])\n    ```\n\n    The selection can be provided with [SQL Interpolation]:\n    \n    ```swift\n    // SELECT IFNULL(name, 'O''Brien'), score FROM player\n    let defaultName = \"O'Brien\"\n    let request = Player.select(literal: \"IFNULL(name, \\(defaultName)), score\")\n    ```\n    \n    The selection can be provided with a mix of Swift and [SQL Interpolation]:\n    \n    ```swift\n    // SELECT IFNULL(name, 'O''Brien') AS displayName, score FROM player\n    let defaultName = \"O'Brien\"\n    let request = Player.select {\n        let displayName: SQL = \"IFNULL(\\($0.name), \\(defaultName)) AS displayName\"\n        return [displayName, $0.score]\n    }\n    ```\n    \n    When the custom SQL snippet should behave as a full-fledged expression, with support for the `+` Swift operator, the `forKey` aliasing method, and all other [SQL Operators](#sql-operators), build an _expression literal_ with the `SQL.sqlExpression` method:\n    \n    ```swift\n    // SELECT IFNULL(name, 'O''Brien') AS displayName, score FROM player\n    let defaultName = \"O'Brien\"\n    let request = Player.select {\n        let displayName = SQL(\"IFNULL(\\($0.name), \\(defaultName))\").sqlExpression\n        return [displayName.forKey(\"displayName\"), $0.score]\n    }\n    ```\n    \n    Such expression literals allow you to build a reusable support library of SQL functions or operators that are missing from the query interface. For example, you can define a Swift `date` function:\n    \n    ```swift\n    func date(_ value: some SQLSpecificExpressible) -> SQLExpression {\n        SQL(\"DATE(\\(value))\").sqlExpression\n    }\n    \n    // SELECT * FROM \"player\" WHERE DATE(\"createdAt\") = '2020-01-23'\n    let request = Player.filter { date($0.createdAt) == \"2020-01-23\" }\n    ```\n    \n    See the [Query Interface Organization] for more information about `SQLSpecificExpressible` and `SQLExpression`.\n    \n3. `FROM ...`: only one table is supported here. You can not customize this SQL part.\n\n4. `JOIN ...`: joins are fully controlled by [Associations]. You can not customize this SQL part.\n\n5. `WHERE ...`\n    \n    The WHERE clause can be provided as raw SQL:\n    \n    ```swift\n    // SELECT * FROM player WHERE score >= 1000\n    let request = Player.filter(sql: \"score >= 1000\")\n    \n    // SELECT * FROM player WHERE score >= 1000\n    let minScore = 1000\n    let request = Player.filter(sql: \"score >= ?\", arguments: [minScore])\n    ```\n\n    The WHERE clause can be provided with [SQL Interpolation]:\n    \n    ```swift\n    // SELECT * FROM player WHERE score >= 1000\n    let minScore = 1000\n    let request = Player.filter(literal: \"score >= \\(minScore)\")\n    ```\n    \n    The WHERE clause can be provided with a mix of Swift and [SQL Interpolation]:\n    \n    ```swift\n    // SELECT * FROM player WHERE (score >= 1000) AND (team = 'red')\n    let minScore = 1000\n    let request = Player.filter { \n        let scoreCondition: SQL = \"\\($0.score) >= \\(minScore)\"\n        return scoreCondition && $0.team == \"red\"\n    }\n    ```\n    \n    See `SELECT ...` above for more SQL Interpolation examples.\n    \n6. `GROUP BY ...`\n\n    The GROUP BY clause can be provided as raw SQL, SQL Interpolation, or a mix of Swift and SQL Interpolation, just as the selection and the WHERE clause (see above).\n    \n7. `HAVING ...`\n\n    The HAVING clause can be provided as raw SQL, SQL Interpolation, or a mix of Swift and SQL Interpolation, just as the selection and the WHERE clause (see above).\n    \n8. `ORDER BY ...`\n\n    The ORDER BY clause can be provided as raw SQL, SQL Interpolation, or a mix of Swift and SQL Interpolation, just as the selection and the WHERE clause (see above).\n    \n    In order to support the `desc` and `asc` query interface operators, and the `reversed()` query interface method, you must provide your orderings as _expression literals_ with the `SQL.sqlExpression` method:\n    \n    ```swift\n    // SELECT * FROM \"player\" \n    // ORDER BY (score + bonus) ASC, name DESC\n    let request = Player\n        .order {\n            let total = SQL(\"(\\($0.score) + \\($0.bonus))\").sqlExpression\n            return [total.desc, $0.name]\n        }\n        .reversed()\n    ```\n    \n9. `LIMIT ...`: use the `limit(_:offset:)` method. You can not customize this SQL part.\n\n\n## Fetching from Requests\n\nOnce you have a request, you can fetch the records at the origin of the request:\n\n```swift\n// Some request based on `Player`\nlet request = Player.filter { ... }... // QueryInterfaceRequest<Player>\n\n// Fetch players:\ntry request.fetchCursor(db) // A Cursor of Player\ntry request.fetchAll(db)    // [Player]\ntry request.fetchSet(db)    // Set<Player>\ntry request.fetchOne(db)    // Player?\n```\n\nFor example:\n\n```swift\nlet allPlayers = try Player.fetchAll(db)                            // [Player]\nlet arthur = try Player.filter { $0.name == \"Arthur\" }.fetchOne(db) // Player?\n```\n\nSee [fetching methods](#fetching-methods) for information about the `fetchCursor`, `fetchAll`, `fetchSet` and `fetchOne` methods.\n\n**You sometimes want to fetch other values**.\n\nThe simplest way is to use the request as an argument to a fetching method of the desired type:\n\n```swift\n// Fetch an Int\nlet request = Player.select { max($0.score) }\nlet maxScore = try Int.fetchOne(db, request) // Int?\n\n// Fetch a Row\nlet request = Player.select { [min($0.score), max($0.score)] }\nlet row = try Row.fetchOne(db, request)!     // Row\nlet minScore = row[0] as Int?\nlet maxScore = row[1] as Int?\n```\n\nYou can also change the request so that it knows the type it has to fetch:\n\n- With `asRequest(of:)`, useful when you use [Associations]:\n    \n    ```swift\n    struct BookInfo: FetchableRecord, Decodable {\n        var book: Book\n        var author: Author\n    }\n    \n    // A request of BookInfo\n    let request = Book\n        .including(required: Book.author)\n        .asRequest(of: BookInfo.self)\n    \n    let bookInfos = try dbQueue.read { db in\n        try request.fetchAll(db) // [BookInfo]\n    }\n    ```\n    \n- With `select(..., as:)`, which is handy when you change the selection:\n    \n    ```swift\n    // A request of Int\n    let request = Player.select({ max($0.score) }, as: Int.self)\n    \n    let maxScore = try dbQueue.read { db in\n        try request.fetchOne(db) // Int?\n    }\n    ```\n\n\n## Fetching by Key\n\n**Fetching records according to their primary key** is a common task.\n\n[Identifiable Records] can use the type-safe methods `find(_:id:)`, `fetchOne(_:id:)`, `fetchAll(_:ids:)` and `fetchSet(_:ids:)`:\n\n```swift\ntry Player.find(db, id: 1)                   // Player\ntry Player.fetchOne(db, id: 1)               // Player?\ntry Country.fetchAll(db, ids: [\"FR\", \"US\"])  // [Countries]\n```\n\nAll record types can use `find(_:key:)`, `fetchOne(_:key:)`, `fetchAll(_:keys:)` and `fetchSet(_:keys:)` that apply conditions on primary and unique keys:\n\n```swift\ntry Player.find(db, key: 1)                  // Player\ntry Player.fetchOne(db, key: 1)              // Player?\ntry Country.fetchAll(db, keys: [\"FR\", \"US\"]) // [Country]\ntry Player.fetchOne(db, key: [\"email\": \"arthur@example.com\"])            // Player?\ntry Citizenship.fetchOne(db, key: [\"citizenId\": 1, \"countryCode\": \"FR\"]) // Citizenship?\n```\n\nWhen the table has no explicit primary key, GRDB uses the [hidden `rowid` column](https://www.sqlite.org/rowidtable.html):\n\n```swift\n// SELECT * FROM document WHERE rowid = 1\ntry Document.fetchOne(db, key: 1)            // Document?\n```\n\n**When you want to build a request and plan to fetch from it later**, use a `filter` method:\n\n```swift\nlet request = Player.filter(id: 1)\nlet request = Country.filter(ids: [\"FR\", \"US\"])\nlet request = Player.filter(key: [\"email\": \"arthur@example.com\"])\nlet request = Citizenship.filter(key: [\"citizenId\": 1, \"countryCode\": \"FR\"])\n```\n\n\n## Testing for Record Existence\n\n**You can check if a request has matching rows in the database.**\n\n```swift\n// Some request based on `Player`\nlet request = Player.filter { ... }...\n\n// Check for player existence:\nlet noSuchPlayer = try request.isEmpty(db) // Bool\n```\n\nYou should check for emptiness instead of counting:\n\n```swift\n// Correct\nlet noSuchPlayer = try request.fetchCount(db) == 0\n// Even better\nlet noSuchPlayer = try request.isEmpty(db)\n```\n\n**You can also check if a given primary or unique key exists in the database.**\n\n[Identifiable Records] can use the type-safe method `exists(_:id:)`:\n\n```swift\ntry Player.exists(db, id: 1)\ntry Country.exists(db, id: \"FR\")\n```\n\nAll record types can use `exists(_:key:)` that can check primary and unique keys:\n\n```swift\ntry Player.exists(db, key: 1)\ntry Country.exists(db, key: \"FR\")\ntry Player.exists(db, key: [\"email\": \"arthur@example.com\"])\ntry Citizenship.exists(db, key: [\"citizenId\": 1, \"countryCode\": \"FR\"])\n```\n\nYou should check for key existence instead of fetching a record and checking for nil:\n\n```swift\n// Correct\nlet playerExists = try Player.fetchOne(db, id: 1) != nil\n// Even better\nlet playerExists = try Player.exists(db, id: 1)\n```\n\n\n## Fetching Aggregated Values\n\n**Requests can count.** The `fetchCount()` method returns the number of rows that would be returned by a fetch request:\n\n```swift\n// SELECT COUNT(*) FROM player\nlet count = try Player.fetchCount(db) // Int\n\n// SELECT COUNT(*) FROM player WHERE email IS NOT NULL\nlet count = try Player.filter { $0.email != nil }.fetchCount(db)\n\n// SELECT COUNT(DISTINCT name) FROM player\nlet count = try Player.select(\\.name).distinct().fetchCount(db)\n\n// SELECT COUNT(*) FROM (SELECT DISTINCT name, score FROM player)\nlet count = try Player.select { [$0.name, $0.score] }.distinct().fetchCount(db)\n```\n\n\n**Other aggregated values** can also be selected and fetched (see [SQL Functions](#sql-functions)):\n\n```swift\nlet request = Player.select { max($0.score) }\nlet maxScore = try Int.fetchOne(db, request) // Int?\n\nlet request = Player.select { [min($0.score), max($0.score)] }\nlet row = try Row.fetchOne(db, request)!     // Row\nlet minScore = row[0] as Int?\nlet maxScore = row[1] as Int?\n```\n\n\n## Delete Requests\n\n**Requests can delete records**, with the `deleteAll()` method:\n\n```swift\n// DELETE FROM player\ntry Player.deleteAll(db)\n\n// DELETE FROM player WHERE team = 'Reds'\ntry Player\n    .filter { $0.team == \"Reds\" }\n    .deleteAll(db)\n\n// DELETE FROM player ORDER BY score LIMIT 10\ntry Player\n    .order(\\.score)\n    .limit(10)\n    .deleteAll(db)\n```\n\n> **Note** Deletion methods are available on types that adopt the [TableRecord] protocol, and `Table`:\n>\n> ```swift\n> struct Player: TableRecord { ... }\n> try Player.deleteAll(db)          // Fine\n> try Table(\"player\").deleteAll(db) // Just as fine\n> ```\n\n**Deleting records according to their primary key** is a common task.\n\n[Identifiable Records] can use the type-safe methods `deleteOne(_:id:)` and `deleteAll(_:ids:)`:\n\n```swift\ntry Player.deleteOne(db, id: 1)\ntry Country.deleteAll(db, ids: [\"FR\", \"US\"])\n```\n\nAll record types can use `deleteOne(_:key:)` and `deleteAll(_:keys:)` that apply conditions on primary and unique keys:\n\n```swift\ntry Player.deleteOne(db, key: 1)\ntry Country.deleteAll(db, keys: [\"FR\", \"US\"])\ntry Player.deleteOne(db, key: [\"email\": \"arthur@example.com\"])\ntry Citizenship.deleteOne(db, key: [\"citizenId\": 1, \"countryCode\": \"FR\"])\n```\n\nWhen the table has no explicit primary key, GRDB uses the [hidden `rowid` column](https://www.sqlite.org/rowidtable.html):\n\n```swift\n// DELETE FROM document WHERE rowid = 1\ntry Document.deleteOne(db, id: 1)             // Document?\n```\n\n\n## Update Requests\n\n**Requests can batch update records**. The `updateAll()` method accepts *column assignments* defined with the `set(to:)` method:\n\n```swift\n// UPDATE player SET score = 0, isHealthy = 1, bonus = NULL\ntry Player.updateAll(db) { [\n    $0.score.set(to: 0), \n    $0.isHealthy.set(to: true), \n    $0.bonus.set(to: nil),\n] }\n\n// UPDATE player SET score = 0 WHERE team = 'Reds'\ntry Player\n    .filter { $0.team == \"Reds\" }\n    .updateAll(db) { $0.score.set(to: 0) }\n\n// UPDATE player SET isGreat = 1 ORDER BY score DESC LIMIT 10\ntry Player\n    .order(\\.score.desc)\n    .limit(10)\n    .updateAll(db) { $0.isGreat.set(to: true) }\n\n// UPDATE country SET population = 67848156 WHERE id = 'FR'\ntry Country\n    .filter(id: \"FR\")\n    .updateAll(db) { $0.population.set(to: 67_848_156) }\n```\n\nColumn assignments accept any expression:\n\n```swift\n// UPDATE player SET score = score + (bonus * 2)\ntry Player.updateAll(db) {\n    $0.score.set(to: $0.score + $0.bonus * 2)\n}\n```\n\nAs a convenience, you can also use the `+=`, `-=`, `*=`, or `/=` operators:\n\n```swift\n// UPDATE player SET score = score + (bonus * 2)\ntry Player.updateAll(db) { $0.score += $0.bonus * 2 }\n```\n\nDefault [Conflict Resolution] rules apply, and you may also provide a specific one:\n\n```swift\n// UPDATE OR IGNORE player SET ...\ntry Player.updateAll(db, onConflict: .ignore) { /* assignments... */ }\n```\n\n> **Note** The `updateAll` method is available on types that adopt the [TableRecord] protocol, and `Table`:\n>\n> ```swift\n> struct Player: TableRecord { ... }\n> try Player.updateAll(db, ...)          // Fine\n> try Table(\"player\").updateAll(db, ...) // Just as fine\n> ```\n\n\n## Custom Requests\n\nUntil now, we have seen [requests](#requests) created from any type that adopts the [TableRecord] protocol:\n\n```swift\nlet request = Player.all()  // QueryInterfaceRequest<Player>\n```\n\nThose requests of type `QueryInterfaceRequest` can fetch and count:\n\n```swift\ntry request.fetchCursor(db) // A Cursor of Player\ntry request.fetchAll(db)    // [Player]\ntry request.fetchSet(db)    // Set<Player>\ntry request.fetchOne(db)    // Player?\ntry request.fetchCount(db)  // Int\n```\n\n**When the query interface can not generate the SQL you need**, you can still fallback to [raw SQL](#fetch-queries):\n\n```swift\n// Custom SQL is always welcome\ntry Player.fetchAll(db, sql: \"SELECT ...\")   // [Player]\n```\n\nBut you may prefer to bring some elegance back in, and build custom requests:\n\n```swift\n// No custom SQL in sight\ntry Player.customRequest().fetchAll(db) // [Player]\n```\n\n**To build custom requests**, you can use one of the built-in requests or derive requests from other requests.\n\n- [SQLRequest] is a fetch request built from raw SQL. For example:\n    \n    ```swift\n    extension Player {\n        static func filter(color: Color) -> SQLRequest<Player> {\n            SQLRequest<Player>(\n                sql: \"SELECT * FROM player WHERE color = ?\"\n                arguments: [color])\n        }\n    }\n    \n    // [Player]\n    try Player.filter(color: .red).fetchAll(db)\n    ```\n    \n    SQLRequest supports [SQL Interpolation]:\n    \n    ```swift\n    extension Player {\n        static func filter(color: Color) -> SQLRequest<Player> {\n            \"SELECT * FROM player WHERE color = \\(color)\"\n        }\n    }\n    ```\n    \n- The [`asRequest(of:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterfacerequest/asrequest(of:)) method changes the type fetched by the request. It is useful, for example, when you use [Associations]:\n\n    ```swift\n    struct BookInfo: FetchableRecord, Decodable {\n        var book: Book\n        var author: Author\n    }\n    \n    let request = Book\n        .including(required: Book.author)\n        .asRequest(of: BookInfo.self)\n    \n    // [BookInfo]\n    try request.fetchAll(db)\n    ```\n\n- The [`adapted(_:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/fetchrequest/adapted(_:)) method eases the consumption of complex rows with row adapters. See [`RowAdapter`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/rowadapter) and [`splittingRowAdapters(columnCounts:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/splittingrowadapters(columncounts:)) for a sample code that uses `adapted(_:)`.\n\n\nEncryption\n==========\n\n**GRDB can encrypt your database with [SQLCipher](http://sqlcipher.net) v3.4+.**\n\nUse [CocoaPods](http://cocoapods.org/), and specify in your `Podfile`:\n\n```ruby\n# GRDB with SQLCipher 4\npod 'GRDB.swift/SQLCipher'\npod 'SQLCipher', '~> 4.0'\n\n# GRDB with SQLCipher 3\npod 'GRDB.swift/SQLCipher'\npod 'SQLCipher', '~> 3.4'\n```\n\nMake sure you remove any existing `pod 'GRDB.swift'` from your Podfile. `GRDB.swift/SQLCipher` must be the only active GRDB pod in your whole project, or you will face linker or runtime errors, due to the conflicts between SQLCipher and the system SQLite.\n\n- [Creating or Opening an Encrypted Database](#creating-or-opening-an-encrypted-database)\n- [Changing the Passphrase of an Encrypted Database](#changing-the-passphrase-of-an-encrypted-database)\n- [Exporting a Database to an Encrypted Database](#exporting-a-database-to-an-encrypted-database)\n- [Security Considerations](#security-considerations)\n\n\n### Creating or Opening an Encrypted Database\n\n**You create and open an encrypted database** by providing a passphrase to your [database connection]:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    try db.usePassphrase(\"secret\")\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\nIt is also in `prepareDatabase` that you perform other [SQLCipher configuration steps](https://www.zetetic.net/sqlcipher/sqlcipher-api/) that must happen early in the lifetime of a SQLCipher connection. For example:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    try db.usePassphrase(\"secret\")\n    try db.execute(sql: \"PRAGMA cipher_page_size = ...\")\n    try db.execute(sql: \"PRAGMA kdf_iter = ...\")\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\nWhen you want to open an existing SQLCipher 3 database with SQLCipher 4, you may want to run the `cipher_compatibility` pragma:\n\n```swift\n// Open an SQLCipher 3 database with SQLCipher 4\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    try db.usePassphrase(\"secret\")\n    try db.execute(sql: \"PRAGMA cipher_compatibility = 3\")\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\nSee [SQLCipher 4.0.0 Release](https://www.zetetic.net/blog/2018/11/30/sqlcipher-400-release/) and [Upgrading to SQLCipher 4](https://discuss.zetetic.net/t/upgrading-to-sqlcipher-4/3283) for more information.\n\n\n### Changing the Passphrase of an Encrypted Database\n\n**You can change the passphrase** of an already encrypted database.\n\nWhen you use a [database queue](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasequeue), open the database with the old passphrase, and then apply the new passphrase:\n\n```swift\ntry dbQueue.write { db in\n    try db.changePassphrase(\"newSecret\")\n}\n```\n\nWhen you use a [database pool](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasepool), make sure that no concurrent read can happen by changing the passphrase within the `barrierWriteWithoutTransaction` block. You must also ensure all future reads open a new database connection by calling the `invalidateReadOnlyConnections` method:\n\n```swift\ntry dbPool.barrierWriteWithoutTransaction { db in\n    try db.changePassphrase(\"newSecret\")\n    dbPool.invalidateReadOnlyConnections()\n}\n```\n\n> **Note**: When an application wants to keep on using a database queue or pool after the passphrase has changed, it is responsible for providing the correct passphrase to the `usePassphrase` method called in the database preparation function. Consider:\n>\n> ```swift\n> // WRONG: this won't work across a passphrase change\n> let passphrase = try getPassphrase()\n> var config = Configuration()\n> config.prepareDatabase { db in\n>     try db.usePassphrase(passphrase)\n> }\n>\n> // CORRECT: get the latest passphrase when it is needed\n> var config = Configuration()\n> config.prepareDatabase { db in\n>     let passphrase = try getPassphrase()\n>     try db.usePassphrase(passphrase)\n> }\n> ```\n\n> **Note**: The `DatabasePool.barrierWriteWithoutTransaction` method does not prevent [database snapshots](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasesnapshot) from accessing the database during the passphrase change, or after the new passphrase has been applied to the database. Those database accesses may throw errors. Applications should provide their own mechanism for invalidating open snapshots before the passphrase is changed.\n\n> **Note**: Instead of changing the passphrase \"in place\" as described here, you can also export the database in a new encrypted database that uses the new passphrase. See [Exporting a Database to an Encrypted Database](#exporting-a-database-to-an-encrypted-database).\n\n\n### Exporting a Database to an Encrypted Database\n\nProviding a passphrase won't encrypt a clear-text database that already exists, though. SQLCipher can't do that, and you will get an error instead: `SQLite error 26: file is encrypted or is not a database`.\n\nInstead, create a new encrypted database, at a distinct location, and export the content of the existing database. This can both encrypt a clear-text database, or change the passphrase of an encrypted database.\n\nThe technique to do that is [documented](https://discuss.zetetic.net/t/how-to-encrypt-a-plaintext-sqlite-database-to-use-sqlcipher-and-avoid-file-is-encrypted-or-is-not-a-database-errors/868/1) by SQLCipher.\n\nWith GRDB, it gives:\n\n```swift\n// The existing database\nlet existingDBQueue = try DatabaseQueue(path: \"/path/to/existing.db\")\n\n// The new encrypted database, at some distinct location:\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    try db.usePassphrase(\"secret\")\n}\nlet newDBQueue = try DatabaseQueue(path: \"/path/to/new.db\", configuration: config)\n\ntry existingDBQueue.inDatabase { db in\n    try db.execute(\n        sql: \"\"\"\n            ATTACH DATABASE ? AS encrypted KEY ?;\n            SELECT sqlcipher_export('encrypted');\n            DETACH DATABASE encrypted;\n            \"\"\",\n        arguments: [newDBQueue.path, \"secret\"])\n}\n\n// Now the export is completed, and the existing database can be deleted.\n```\n\n\n### Security Considerations\n\n#### Managing the lifetime of the passphrase string\n\nIt is recommended to avoid keeping the passphrase in memory longer than necessary. To do this, make sure you load the passphrase from the `prepareDatabase` method:\n\n```swift\n// NOT RECOMMENDED: this keeps the passphrase in memory longer than necessary\nlet passphrase = try getPassphrase()\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    try db.usePassphrase(passphrase)\n}\n\n// RECOMMENDED: only load the passphrase when it is needed\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    let passphrase = try getPassphrase()\n    try db.usePassphrase(passphrase)\n}\n```\n\nThis technique helps manages the lifetime of the passphrase, although keep in mind that the content of a String may remain intact in memory long after the object has been released.\n\nFor even better control over the lifetime of the passphrase in memory, use a Data object which natively provides the `resetBytes` function.\n\n```swift\n// RECOMMENDED: only load the passphrase when it is needed and reset its content immediately after use\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    var passphraseData = try getPassphraseData() // Data\n    defer {\n        passphraseData.resetBytes(in: 0..<passphraseData.count)\n    }\n    try db.usePassphrase(passphraseData)\n}\n```\n\nSome demanding users will want to go further, and manage the lifetime of the raw passphrase bytes. See below.\n\n\n#### Managing the lifetime of the passphrase bytes\n\nGRDB offers convenience methods for providing the database passphrases as Swift strings: `usePassphrase(_:)` and `changePassphrase(_:)`. Those methods don't keep the passphrase String in memory longer than necessary. But they are as secure as the standard String type: the lifetime of actual passphrase bytes in memory is not under control.\n\nWhen you want to precisely manage the passphrase bytes, talk directly to SQLCipher, using its raw C functions.\n\nFor example:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    ... // Carefully load passphrase bytes\n    let code = sqlite3_key(db.sqliteConnection, /* passphrase bytes */)\n    ... // Carefully dispose passphrase bytes\n    guard code == SQLITE_OK else {\n        throw DatabaseError(\n            resultCode: ResultCode(rawValue: code), \n            message: db.lastErrorMessage)\n    }\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\n#### Passphrase availability vs. Database availability\n\nWhen the passphrase is securely stored in the system keychain, your application can protect it using the [`kSecAttrAccessible`](https://developer.apple.com/documentation/security/ksecattraccessible) attribute.\n\nSuch protection prevents GRDB from creating SQLite connections when the passphrase is not available:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    let passphrase = try loadPassphraseFromSystemKeychain()\n    try db.usePassphrase(passphrase)\n}\n\n// Success if and only if the passphrase is available\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\nFor the same reason, [database pools], which open SQLite connections on demand, may fail at any time as soon as the passphrase becomes unavailable:\n\n```swift\n// Success if and only if the passphrase is available\nlet dbPool = try DatabasePool(path: dbPath, configuration: config)\n\n// May fail if passphrase has turned unavailable\ntry dbPool.read { ... }\n\n// May trigger value observation failure if passphrase has turned unavailable\ntry dbPool.write { ... }\n```\n\nBecause DatabasePool maintains a pool of long-lived SQLite connections, some database accesses will use an existing connection, and succeed. And some other database accesses will fail, as soon as the pool wants to open a new connection. It is impossible to predict which accesses will succeed or fail.\n\nFor the same reason, a database queue, which also maintains a long-lived SQLite connection, will remain available even after the passphrase has turned unavailable.\n\nApplications are thus responsible for protecting database accesses when the passphrase is unavailable. To this end, they can use [Data Protection](https://developer.apple.com/documentation/uikit/protecting_the_user_s_privacy/encrypting_your_app_s_files). They can also destroy their instances of database queue or pool when the passphrase becomes unavailable.\n\n\n## Backup\n\n**You can backup (copy) a database into another.**\n\nBackups can for example help you copying an in-memory database to and from a database file when you implement NSDocument subclasses.\n\n```swift\nlet source: DatabaseQueue = ...      // or DatabasePool\nlet destination: DatabaseQueue = ... // or DatabasePool\ntry source.backup(to: destination)\n```\n\nThe `backup` method blocks the current thread until the destination database contains the same contents as the source database.\n\nWhen the source is a [database pool](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasepool), concurrent writes can happen during the backup. Those writes may, or may not, be reflected in the backup, but they won't trigger any error.\n\n`Database` has an analogous `backup` method.\n\n```swift\nlet source: DatabaseQueue = ...      // or DatabasePool\nlet destination: DatabaseQueue = ... // or DatabasePool\ntry source.write { sourceDb in\n    try destination.barrierWriteWithoutTransaction { destDb in\n        try sourceDb.backup(to: destDb)\n    }\n}\n```\n\nThis method allows for the choice of source and destination `Database` handles with which to backup the database.\n\n### Backup Progress Reporting\n\nThe `backup` methods take optional `pagesPerStep` and `progress` parameters. Together these parameters can be used to track a database backup in progress and abort an incomplete backup.\n\nWhen `pagesPerStep` is provided, the database backup is performed in _steps_. At each step, no more than `pagesPerStep` database pages are copied from the source to the destination. The backup proceeds one step at a time until all pages have been copied.\n\nWhen a `progress` callback is provided, `progress` is called after every backup step, including the last. Even if a non-default `pagesPerStep` is specified or the backup is otherwise completed in a single step, the `progress` callback will be called.\n\n```swift\ntry source.backup(\n    to: destination,\n    pagesPerStep: ...)\n    { backupProgress in\n       print(\"Database backup progress:\", backupProgress)\n    }\n```\n\n### Aborting an Incomplete Backup\n\nIf a call to `progress` throws when `backupProgress.isComplete == false`, the backup will be aborted and the error rethrown. However, if a call to `progress` throws when `backupProgress.isComplete == true`, the backup is unaffected and the error is silently ignored.\n\n> **Warning**: Passing non-default values of `pagesPerStep` or `progress` to the backup methods is an advanced API intended to provide additional capabilities to expert users. GRDB's backup API provides a faithful, low-level wrapper to the underlying SQLite online backup API. GRDB's documentation is not a comprehensive substitute for the official SQLite [documentation of their backup API](https://www.sqlite.org/c3ref/backup_finish.html).\n\n## Interrupt a Database\n\n**The `interrupt()` method** causes any pending database operation to abort and return at its earliest opportunity.\n\nIt can be called from any thread.\n\n```swift\ndbQueue.interrupt()\ndbPool.interrupt()\n```\n\nA call to `interrupt()` that occurs when there are no running SQL statements is a no-op and has no effect on SQL statements that are started after `interrupt()` returns.\n\nA database operation that is interrupted will throw a DatabaseError with code `SQLITE_INTERRUPT`. If the interrupted SQL operation is an INSERT, UPDATE, or DELETE that is inside an explicit transaction, then the entire transaction will be rolled back automatically. If the rolled back transaction was started by a transaction-wrapping method such as `DatabaseWriter.write` or `Database.inTransaction`, then all database accesses will throw a DatabaseError with code `SQLITE_ABORT` until the wrapping method returns.\n\nFor example:\n\n```swift\ntry dbQueue.write { db in\n    try Player(...).insert(db)     // throws SQLITE_INTERRUPT\n    try Player(...).insert(db)     // not executed\n}                                  // throws SQLITE_INTERRUPT\n\ntry dbQueue.write { db in\n    do {\n        try Player(...).insert(db) // throws SQLITE_INTERRUPT\n    } catch { }\n}                                  // throws SQLITE_ABORT\n\ntry dbQueue.write { db in\n    do {\n        try Player(...).insert(db) // throws SQLITE_INTERRUPT\n    } catch { }\n    try Player(...).insert(db)     // throws SQLITE_ABORT\n}                                  // throws SQLITE_ABORT\n```\n\nYou can catch both `SQLITE_INTERRUPT` and `SQLITE_ABORT` errors:\n\n```swift\ndo {\n    try dbPool.write { db in ... }\n} catch DatabaseError.SQLITE_INTERRUPT, DatabaseError.SQLITE_ABORT {\n    // Oops, the database was interrupted.\n}\n```\n\nFor more information, see [Interrupt A Long-Running Query](https://www.sqlite.org/c3ref/interrupt.html).\n\n\n## Avoiding SQL Injection\n\nSQL injection is a technique that lets an attacker nuke your database.\n\n> ![XKCD: Exploits of a Mom](https://imgs.xkcd.com/comics/exploits_of_a_mom.png)\n>\n> https://xkcd.com/327/\n\nHere is an example of code that is vulnerable to SQL injection:\n\n```swift\n// BAD BAD BAD\nlet id = 1\nlet name = textField.text\ntry dbQueue.write { db in\n    try db.execute(sql: \"UPDATE students SET name = '\\(name)' WHERE id = \\(id)\")\n}\n```\n\nIf the user enters a funny string like `Robert'; DROP TABLE students; --`, SQLite will see the following SQL, and drop your database table instead of updating a name as intended:\n\n```sql\nUPDATE students SET name = 'Robert';\nDROP TABLE students;\n--' WHERE id = 1\n```\n\nTo avoid those problems, **never embed raw values in your SQL queries**. The only correct technique is to provide [arguments](#executing-updates) to your raw SQL queries:\n\n```swift\nlet name = textField.text\ntry dbQueue.write { db in\n    // Good\n    try db.execute(\n        sql: \"UPDATE students SET name = ? WHERE id = ?\",\n        arguments: [name, id])\n    \n    // Just as good\n    try db.execute(\n        sql: \"UPDATE students SET name = :name WHERE id = :id\",\n        arguments: [\"name\": name, \"id\": id])\n}\n```\n\nWhen you use [records](#records) and the [query interface](#the-query-interface), GRDB always prevents SQL injection for you:\n\n```swift\nlet id = 1\nlet name = textField.text\ntry dbQueue.write { db in\n    if var student = try Student.fetchOne(db, id: id) {\n        student.name = name\n        try student.update(db)\n    }\n}\n```\n\n\n## Error Handling\n\nGRDB can throw [DatabaseError](#databaseerror), [RecordError], [RowDecodingError], or crash your program with a [fatal error](#fatal-errors).\n\nConsidering that a local database is not some JSON loaded from a remote server, GRDB focuses on **trusted databases**. Dealing with [untrusted databases](#how-to-deal-with-untrusted-inputs) requires extra care.\n\n- [DatabaseError](#databaseerror)\n- [RecordError]\n- [RowDecodingError]\n- [Fatal Errors](#fatal-errors)\n- [How to Deal with Untrusted Inputs](#how-to-deal-with-untrusted-inputs)\n- [Error Log](#error-log)\n\n\n### DatabaseError\n\nğŸ“– [`DatabaseError`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseerror)\n\n**DatabaseError** are thrown on SQLite errors:\n\n```swift\ndo {\n    try Pet(masterId: 1, name: \"Bobby\").insert(db)\n} catch let error as DatabaseError {\n    // The SQLite error code: 19 (SQLITE_CONSTRAINT)\n    error.resultCode\n    \n    // The extended error code: 787 (SQLITE_CONSTRAINT_FOREIGNKEY)\n    error.extendedResultCode\n    \n    // The eventual SQLite message: FOREIGN KEY constraint failed\n    error.message\n    \n    // The eventual erroneous SQL query\n    // \"INSERT INTO pet (masterId, name) VALUES (?, ?)\"\n    error.sql\n    \n    // The eventual SQL arguments\n    // [1, \"Bobby\"]\n    error.arguments\n    \n    // Full error description\n    // > SQLite error 19: FOREIGN KEY constraint failed -\n    // > while executing `INSERT INTO pet (masterId, name) VALUES (?, ?)`\n    error.description\n}\n```\n\nIf you want to see statement arguments in the error description, [make statement arguments public](https://swiftpackageindex.com/groue/GRDB.swift/configuration/publicstatementarguments).\n\n**SQLite uses [results codes](https://www.sqlite.org/rescode.html) to distinguish between various errors**.\n\nYou can catch a DatabaseError and match on result codes:\n\n```swift\ndo {\n    try ...\n} catch let error as DatabaseError {\n    switch error {\n    case DatabaseError.SQLITE_CONSTRAINT_FOREIGNKEY:\n        // foreign key constraint error\n    case DatabaseError.SQLITE_CONSTRAINT:\n        // any other constraint error\n    default:\n        // any other database error\n    }\n}\n```\n\nYou can also directly match errors on result codes:\n\n```swift\ndo {\n    try ...\n} catch DatabaseError.SQLITE_CONSTRAINT_FOREIGNKEY {\n    // foreign key constraint error\n} catch DatabaseError.SQLITE_CONSTRAINT {\n    // any other constraint error\n} catch {\n    // any other database error\n}\n```\n\nEach DatabaseError has two codes: an `extendedResultCode` (see [extended result code](https://www.sqlite.org/rescode.html#extended_result_code_list)), and a less precise `resultCode` (see [primary result code](https://www.sqlite.org/rescode.html#primary_result_code_list)). Extended result codes are refinements of primary result codes, as `SQLITE_CONSTRAINT_FOREIGNKEY` is to `SQLITE_CONSTRAINT`, for example.\n\n> **Warning**: SQLite has progressively introduced extended result codes across its versions. The [SQLite release notes](http://www.sqlite.org/changes.html) are unfortunately not quite clear about that: write your handling of extended result codes with care.\n\n\n### RecordError\n\nğŸ“– [`RecordError`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/recorderror)\n\n**RecordError** is thrown by the [PersistableRecord] protocol when the `update` method could not find any row to update:\n\n```swift\ndo {\n    try player.update(db)\n} catch let RecordError.recordNotFound(databaseTableName: table, key: key) {\n    print(\"Key \\(key) was not found in table \\(table).\")\n}\n```\n\n**RecordError** is also thrown by the [FetchableRecord] protocol when the `find` method does not find any record:\n\n```swift\ndo {\n    let player = try Player.find(db, id: 42)\n} catch let RecordError.recordNotFound(databaseTableName: table, key: key) {\n    print(\"Key \\(key) was not found in table \\(table).\")\n}\n```\n\n\n### RowDecodingError\n\nğŸ“– [`RowDecodingError`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/rowdecodingerror)\n\n**RowDecodingError** is thrown when the application can not decode a value from a database row. For example:\n\n```swift\nlet row = try Row.fetchOne(db, sql: \"SELECT NULL AS name\")!\n// RowDecodingError: could not decode String from database value NULL.\nlet name = try row.decode(String.self, forColumn: \"name\")\n```\n\n### Fatal Errors\n\n**Fatal errors notify that the program, or the database, has to be changed.**\n\nThey uncover programmer errors, false assumptions, and prevent misuses. Here are a few examples:\n\n- **The code asks for a non-optional value, when the database contains NULL:**\n    \n    ```swift\n    // fatal error: could not convert NULL to String.\n    let name: String = row[\"name\"]\n    ```\n    \n    Solution: fix the contents of the database, use [NOT NULL constraints](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/columndefinition/notnull(onconflict:)), or load an optional:\n    \n    ```swift\n    let name: String? = row[\"name\"]\n    ```\n\n- **Conversion from database value to Swift type fails:**\n    \n    ```swift\n    // fatal error: could not convert \"Momâ€™s birthday\" to Date.\n    let date: Date = row[\"date\"]\n    \n    // fatal error: could not convert \"\" to URL.\n    let url: URL = row[\"url\"]\n    ```\n    \n    Solution: fix the contents of the database, or use [DatabaseValue](#databasevalue) to handle all possible cases:\n    \n    ```swift\n    let dbValue: DatabaseValue = row[\"date\"]\n    if dbValue.isNull {\n        // Handle NULL\n    } else if let date = Date.fromDatabaseValue(dbValue) {\n        // Handle valid date\n    } else {\n        // Handle invalid date\n    }\n    ```\n\n- **The database can't guarantee that the code does what it says:**\n\n    ```swift\n    // fatal error: table player has no unique index on column email\n    try Player.deleteOne(db, key: [\"email\": \"arthur@example.com\"])\n    ```\n    \n    Solution: add a unique index to the player.email column, or use the `deleteAll` method to make it clear that you may delete more than one row:\n    \n    ```swift\n    try Player.filter { $0.email == \"arthur@example.com\" }.deleteAll(db)\n    ```\n\n- **Database connections are not reentrant:**\n    \n    ```swift\n    // fatal error: Database methods are not reentrant.\n    dbQueue.write { db in\n        dbQueue.write { db in\n            ...\n        }\n    }\n    ```\n    \n    Solution: avoid reentrancy, and instead pass a database connection along.\n\n\n### How to Deal with Untrusted Inputs\n\nLet's consider the code below:\n\n```swift\nlet sql = \"SELECT ...\"\n\n// Some untrusted arguments for the query\nlet arguments: [String: Any] = ...\nlet rows = try Row.fetchCursor(db, sql: sql, arguments: StatementArguments(arguments))\n\nwhile let row = try rows.next() {\n    // Some untrusted database value:\n    let date: Date? = row[0]\n}\n```\n\nIt has two opportunities to throw fatal errors:\n\n- **Untrusted arguments**: The dictionary may contain values that do not conform to the [DatabaseValueConvertible protocol](#values), or may miss keys required by the statement.\n- **Untrusted database content**: The row may contain a non-null value that can't be turned into a date.\n\nIn such a situation, you can still avoid fatal errors by exposing and handling each failure point, one level down in the GRDB API:\n\n```swift\n// Untrusted arguments\nif let arguments = StatementArguments(arguments) {\n    let statement = try db.makeStatement(sql: sql)\n    try statement.setArguments(arguments)\n    \n    var cursor = try Row.fetchCursor(statement)\n    while let row = try iterator.next() {\n        // Untrusted database content\n        let dbValue: DatabaseValue = row[0]\n        if dbValue.isNull {\n            // Handle NULL\n        if let date = Date.fromDatabaseValue(dbValue) {\n            // Handle valid date\n        } else {\n            // Handle invalid date\n        }\n    }\n}\n```\n\nSee [`Statement`] and [DatabaseValue](#databasevalue) for more information.\n\n\n### Error Log\n\n**SQLite can be configured to invoke a callback function containing an error code and a terse error message whenever anomalies occur.**\n\nThis global error callback must be configured early in the lifetime of your application:\n\n```swift\nDatabase.logError = { (resultCode, message) in\n    NSLog(\"%@\", \"SQLite error \\(resultCode): \\(message)\")\n}\n```\n\n> **Warning**: Database.logError must be set before any database connection is opened. This includes the connections that your application opens with GRDB, but also connections opened by other tools, such as third-party libraries. Setting it after a connection has been opened is an SQLite misuse, and has no effect.\n\nSee [The Error And Warning Log](https://sqlite.org/errlog.html) for more information.\n\n\n## Unicode\n\nSQLite lets you store unicode strings in the database.\n\nHowever, SQLite does not provide any unicode-aware string transformations or comparisons.\n\n\n### Unicode functions\n\nThe `UPPER` and `LOWER` built-in SQLite functions are not unicode-aware:\n\n```swift\n// \"JÃ©RÃ´ME\"\ntry String.fetchOne(db, sql: \"SELECT UPPER('JÃ©rÃ´me')\")\n```\n\nGRDB extends SQLite with [SQL functions](#custom-sql-functions-and-aggregates) that call the Swift built-in string functions `capitalized`, `lowercased`, `uppercased`, `localizedCapitalized`, `localizedLowercased` and `localizedUppercased`:\n\n```swift\n// \"JÃ‰RÃ”ME\"\nlet uppercased = DatabaseFunction.uppercase\ntry String.fetchOne(db, sql: \"SELECT \\(uppercased.name)('JÃ©rÃ´me')\")\n```\n\nThose unicode-aware string functions are also readily available in the [query interface](#sql-functions):\n\n```swift\nPlayer.select { $0.name.uppercased }\n```\n\n\n### String Comparison\n\nSQLite compares strings in many occasions: when you sort rows according to a string column, or when you use a comparison operator such as `=` and `<=`.\n\nThe comparison result comes from a *collating function*, or *collation*. SQLite comes with three built-in collations that do not support Unicode: [binary, nocase, and rtrim](https://www.sqlite.org/datatype3.html#collation).\n\nGRDB comes with five extra collations that leverage unicode-aware comparisons based on the standard Swift String comparison functions and operators:\n\n- `unicodeCompare` (uses the built-in `<=` and `==` Swift operators)\n- `caseInsensitiveCompare`\n- `localizedCaseInsensitiveCompare`\n- `localizedCompare`\n- `localizedStandardCompare`\n\nA collation can be applied to a table column. All comparisons involving this column will then automatically trigger the comparison function:\n    \n```swift\ntry db.create(table: \"player\") { t in\n    // Guarantees case-insensitive email unicity\n    t.column(\"email\", .text).unique().collate(.nocase)\n    \n    // Sort names in a localized case insensitive way\n    t.column(\"name\", .text).collate(.localizedCaseInsensitiveCompare)\n}\n\n// Players are sorted in a localized case insensitive way:\nlet players = try Player.order(\\.name).fetchAll(db)\n```\n\n> **Warning**: SQLite *requires* host applications to provide the definition of any collation other than binary, nocase and rtrim. When a database file has to be shared or migrated to another SQLite library of platform (such as the Android version of your application), make sure you provide a compatible collation.\n\nIf you can't or don't want to define the comparison behavior of a column (see warning above), you can still use an explicit collation in SQL requests and in the [query interface](#the-query-interface):\n\n```swift\nlet collation = DatabaseCollation.localizedCaseInsensitiveCompare\nlet players = try Player.fetchAll(db,\n    sql: \"SELECT * FROM player ORDER BY name COLLATE \\(collation.name))\")\nlet players = try Player.order { $0.name.collating(collation) }.fetchAll(db)\n```\n\n\n**You can also define your own collations**:\n\n```swift\nlet collation = DatabaseCollation(\"customCollation\") { (lhs, rhs) -> NSComparisonResult in\n    // return the comparison of lhs and rhs strings.\n}\n\n// Make the collation available to a database connection\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    db.add(collation: collation)\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\n\n\n## Memory Management\n\nBoth SQLite and GRDB use non-essential memory that help them perform better.\n\nYou can reclaim this memory with the `releaseMemory` method:\n\n```swift\n// Release as much memory as possible.\ndbQueue.releaseMemory()\ndbPool.releaseMemory()\n```\n\nThis method blocks the current thread until all current database accesses are completed, and the memory collected.\n\n> **Warning**: If `DatabasePool.releaseMemory()` is called while a long read is performed concurrently, then no other read access will be possible until this long read has completed, and the memory has been released. If this does not suit your application needs, look for the asynchronous options below:\n\nYou can release memory in an asynchronous way as well:\n\n```swift\n// On a DatabaseQueue\ndbQueue.asyncWriteWithoutTransaction { db in\n    db.releaseMemory()\n}\n\n// On a DatabasePool\ndbPool.releaseMemoryEventually()\n```\n\n`DatabasePool.releaseMemoryEventually()` does not block the current thread, and does not prevent concurrent database accesses. In exchange for this convenience, you don't know when memory has been freed.\n\n\n### Memory Management on iOS\n\n**The iOS operating system likes applications that do not consume much memory.**\n\n[Database queues] and [pools](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasepool) automatically free non-essential memory when the application receives a memory warning, and when the application enters background.\n\nYou can opt out of this automatic memory management:\n\n```swift\nvar config = Configuration()\nconfig.automaticMemoryManagement = false\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config) // or DatabasePool\n```\n\nFAQ\n===\n\n**[FAQ: Opening Connections](#faq-opening-connections)**\n\n- [How do I create a database in my application?](#how-do-i-create-a-database-in-my-application)\n- [How do I open a database stored as a resource of my application?](#how-do-i-open-a-database-stored-as-a-resource-of-my-application)\n- [How do I close a database connection?](#how-do-i-close-a-database-connection)\n\n**[FAQ: SQL](#faq-sql)**\n\n- [How do I print a request as SQL?](#how-do-i-print-a-request-as-sql)\n\n**[FAQ: General](#faq-general)**\n\n- [How do I monitor the duration of database statements execution?](#how-do-i-monitor-the-duration-of-database-statements-execution)\n- [What Are Experimental Features?](#what-are-experimental-features)\n- [Does GRDB support library evolution and ABI stability?](#does-grdb-support-library-evolution-and-abi-stability)\n\n**[FAQ: Associations](#faq-associations)**\n\n- [How do I filter records and only keep those that are associated to another record?](#how-do-i-filter-records-and-only-keep-those-that-are-associated-to-another-record)\n- [How do I filter records and only keep those that are NOT associated to another record?](#how-do-i-filter-records-and-only-keep-those-that-are-not-associated-to-another-record)\n- [How do I select only one column of an associated record?](#how-do-i-select-only-one-column-of-an-associated-record)\n\n**[FAQ: ValueObservation](#faq-valueobservation)**\n\n- [Why is ValueObservation not publishing value changes?](#why-is-valueobservation-not-publishing-value-changes)\n\n**[FAQ: Errors](#faq-errors)**\n\n- [Generic parameter 'T' could not be inferred](#generic-parameter-t-could-not-be-inferred)\n- [Mutation of captured var in concurrently-executing code](#mutation-of-captured-var-in-concurrently-executing-code)\n- [SQLite error 1 \"no such column\"](#sqlite-error-1-no-such-column)\n- [SQLite error 10 \"disk I/O error\", SQLite error 23 \"not authorized\"](#sqlite-error-10-disk-io-error-sqlite-error-23-not-authorized)\n- [SQLite error 21 \"wrong number of statement arguments\" with LIKE queries](#sqlite-error-21-wrong-number-of-statement-arguments-with-like-queries)\n\n\n## FAQ: Opening Connections\n\n- :arrow_up: [FAQ]\n- [How do I create a database in my application?](#how-do-i-create-a-database-in-my-application)\n- [How do I open a database stored as a resource of my application?](#how-do-i-open-a-database-stored-as-a-resource-of-my-application)\n- [How do I close a database connection?](#how-do-i-close-a-database-connection)\n\n### How do I create a database in my application?\n\nFirst choose a proper location for the database file. Document-based applications will let the user pick a location. Apps that use the database as a global storage will prefer the Application Support directory.\n\nThe sample code below creates or opens a database file inside its dedicated directory (a [recommended practice](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseconnections)). On the first run, a new empty database file is created. On subsequent runs, the database file already exists, so it just opens a connection:\n\n```swift\n// HOW TO create an empty database, or open an existing database file\n\n// Create the \"Application Support/MyDatabase\" directory\nlet fileManager = FileManager.default\nlet appSupportURL = try fileManager.url(\n    for: .applicationSupportDirectory, in: .userDomainMask,\n    appropriateFor: nil, create: true) \nlet directoryURL = appSupportURL.appendingPathComponent(\"MyDatabase\", isDirectory: true)\ntry fileManager.createDirectory(at: directoryURL, withIntermediateDirectories: true)\n\n// Open or create the database\nlet databaseURL = directoryURL.appendingPathComponent(\"db.sqlite\")\nlet dbQueue = try DatabaseQueue(path: databaseURL.path)\n```\n\n### How do I open a database stored as a resource of my application?\n\nOpen a read-only connection to your resource:\n\n```swift\n// HOW TO open a read-only connection to a database resource\n\n// Get the path to the database resource.\nif let dbPath = Bundle.main.path(forResource: \"db\", ofType: \"sqlite\") {\n    // If the resource exists, open a read-only connection.\n    // Writes are disallowed because resources can not be modified. \n    var config = Configuration()\n    config.readonly = true\n    let dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n} else {\n    // The database resource can not be found.\n    // Fix your setup, or report the problem to the user. \n}\n```\n\n### How do I close a database connection?\n\nDatabase connections are automatically closed when `DatabaseQueue` or `DatabasePool` instances are deinitialized.\n\nIf the correct execution of your program depends on precise database closing, perform an explicit call to [`close()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasereader/close()). This method may fail and create zombie connections, so please check its detailed documentation.\n\n## FAQ: SQL\n\n- :arrow_up: [FAQ]\n- [How do I print a request as SQL?](#how-do-i-print-a-request-as-sql)\n\n### How do I print a request as SQL?\n\nWhen you want to debug a request that does not deliver the expected results, you may want to print the SQL that is actually executed.\n\nYou can compile the request into a prepared [`Statement`]:\n\n```swift\ntry dbQueue.read { db in\n    let request = Player.filter { $0.email == \"arthur@example.com\" }\n    let statement = try request.makePreparedRequest(db).statement\n    print(statement) // SELECT * FROM player WHERE email = ?\n    print(statement.arguments) // [\"arthur@example.com\"]\n}\n```\n\nAnother option is to setup a tracing function that prints out the executed SQL requests. For example, provide a tracing function when you connect to the database:\n\n```swift\n// Prints all SQL statements\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    db.trace { print($0) }\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n\ntry dbQueue.read { db in\n    // Prints \"SELECT * FROM player WHERE email = ?\"\n    let players = try Player.filter { $0.email == \"arthur@example.com\" }.fetchAll(db)\n}\n```\n\nIf you want to see statement arguments such as `'arthur@example.com'` in the logged statements, [make statement arguments public](https://swiftpackageindex.com/groue/GRDB.swift/configuration/publicstatementarguments).\n\n> **Note**: the generated SQL may change between GRDB releases, without notice: don't have your application rely on any specific SQL output.\n\n\n## FAQ: General\n\n- :arrow_up: [FAQ]\n- [How do I monitor the duration of database statements execution?](#how-do-i-monitor-the-duration-of-database-statements-execution)\n- [What Are Experimental Features?](#what-are-experimental-features)\n- [Does GRDB support library evolution and ABI stability?](#does-grdb-support-library-evolution-and-abi-stability)\n\n### How do I monitor the duration of database statements execution?\n\nUse the `trace(options:_:)` method, with the `.profile` option:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    db.trace(options: .profile) { event in\n        // Prints all SQL statements with their duration\n        print(event)\n        \n        // Access to detailed profiling information\n        if case let .profile(statement, duration) = event, duration > 0.5 {\n            print(\"Slow query: \\(statement.sql)\")\n        }\n    }\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n\ntry dbQueue.read { db in\n    let players = try Player.filter { $0.email == \"arthur@example.com\" }.fetchAll(db)\n    // Prints \"0.003s SELECT * FROM player WHERE email = ?\"\n}\n```\n\nIf you want to see statement arguments such as `'arthur@example.com'` in the logged statements, [make statement arguments public](https://swiftpackageindex.com/groue/GRDB.swift/configuration/publicstatementarguments).\n\n### What Are Experimental Features?\n\nSince GRDB 1.0, all backwards compatibility guarantees of [semantic versioning](http://semver.org) apply: no breaking change will happen until the next major version of the library.\n\nThere is an exception, though: *experimental features*, marked with the \"**:fire: EXPERIMENTAL**\" badge. Those are advanced features that are too young, or lack user feedback. They are not stabilized yet.\n\nThose experimental features are not protected by semantic versioning, and may break between two minor releases of the library. To help them becoming stable, [your feedback](https://github.com/groue/GRDB.swift/issues) is greatly appreciated.\n\n### Does GRDB support library evolution and ABI stability?\n\nNo, GRDB does not support library evolution and ABI stability. The only promise is API stability according to [semantic versioning](http://semver.org), with an exception for [experimental features](#what-are-experimental-features).\n\nYet, GRDB can be built with the \"Build Libraries for Distribution\" Xcode option (`BUILD_LIBRARY_FOR_DISTRIBUTION`), so that you can build binary frameworks at your convenience.\n\n## FAQ: Associations\n\n- :arrow_up: [FAQ]\n- [How do I filter records and only keep those that are associated to another record?](#how-do-i-filter-records-and-only-keep-those-that-are-associated-to-another-record)\n- [How do I filter records and only keep those that are NOT associated to another record?](#how-do-i-filter-records-and-only-keep-those-that-are-not-associated-to-another-record)\n- [How do I select only one column of an associated record?](#how-do-i-select-only-one-column-of-an-associated-record)\n\n### How do I filter records and only keep those that are associated to another record?\n\nLet's say you have two record types, `Book` and `Author`, and you want to only fetch books that have an author, and discard anonymous books.\n\nWe start by defining the association between books and authors:\n\n```swift\nstruct Book: TableRecord {\n    ...\n    static let author = belongsTo(Author.self)\n}\n\nstruct Author: TableRecord {\n    ...\n}\n```\n\nAnd then we can write our request and only fetch books that have an author, discarding anonymous ones:\n\n```swift\nlet books: [Book] = try dbQueue.read { db in\n    // SELECT book.* FROM book \n    // JOIN author ON author.id = book.authorID\n    let request = Book.joining(required: Book.author)\n    return try request.fetchAll(db)\n}\n```\n\nNote how this request does not use the `filter` method. Indeed, we don't have any condition to express on any column. Instead, we just need to \"require that a book can be joined to its author\".\n\nSee [How do I filter records and only keep those that are NOT associated to another record?](#how-do-i-filter-records-and-only-keep-those-that-are-not-associated-to-another-record) below for the opposite question.\n\n\n### How do I filter records and only keep those that are NOT associated to another record?\n\nLet's say you have two record types, `Book` and `Author`, and you want to only fetch anonymous books that do not have any author.\n\nWe start by defining the association between books and authors:\n\n```swift\nstruct Book: TableRecord {\n    ...\n    static let author = belongsTo(Author.self)\n}\n\nstruct Author: TableRecord {\n    ...\n}\n```\n\nAnd then we can write our request and only fetch anonymous books that don't have any author:\n\n```swift\nlet books: [Book] = try dbQueue.read { db in\n    // SELECT book.* FROM book\n    // LEFT JOIN author ON author.id = book.authorID\n    // WHERE author.id IS NULL\n    let authorAlias = TableAlias<Author>()\n    let request = Book\n        .joining(optional: Book.author.aliased(authorAlias))\n        .filter(!authorAlias.exists)\n    return try request.fetchAll(db)\n}\n```\n\nThis request uses a TableAlias in order to be able to filter on the eventual associated author. We make sure that the `Author.primaryKey` is nil, which is another way to say it does not exist: the book has no author.\n\nSee [How do I filter records and only keep those that are associated to another record?](#how-do-i-filter-records-and-only-keep-those-that-are-associated-to-another-record) above for the opposite question.\n\n\n### How do I select only one column of an associated record?\n\nLet's say you have two record types, `Book` and `Author`, and you want to fetch all books with their author name, but not the full associated author records.\n\nWe start by defining the association between books and authors:\n\n```swift\nstruct Book: Decodable, TableRecord {\n    ...\n    static let author = belongsTo(Author.self)\n}\n\nstruct Author: Decodable, TableRecord {\n    ...\n    enum Columns {\n        static let name = Column(CodingKeys.name)\n    }\n}\n```\n\nAnd then we can write our request and the ad-hoc record that decodes it:\n\n```swift\nstruct BookInfo: Decodable, FetchableRecord {\n    var book: Book\n    var authorName: String? // nil when the book is anonymous\n    \n    static func all() -> QueryInterfaceRequest<BookInfo> {\n        // SELECT book.*, author.name AS authorName\n        // FROM book\n        // LEFT JOIN author ON author.id = book.authorID\n        return Book\n            .annotated(withOptional: Book.author.select { \n                $0.name.forKey(CodingKeys.authorName)\n            })\n            .asRequest(of: BookInfo.self)\n    }\n}\n\nlet bookInfos: [BookInfo] = try dbQueue.read { db in\n    BookInfo.all().fetchAll(db)\n}\n```\n\nBy defining the request as a static method of BookInfo, you have access to the private `CodingKeys.authorName`, and a compiler-checked SQL column name.\n\nBy using the `annotated(withOptional:)` method, you append the author name to the top-level selection that can be decoded by the ad-hoc record.\n\nBy using `asRequest(of:)`, you enhance the type-safety of your request.\n\n\n## FAQ: ValueObservation\n\n- :arrow_up: [FAQ]\n- [Why is ValueObservation not publishing value changes?](#why-is-valueobservation-not-publishing-value-changes)\n\n### Why is ValueObservation not publishing value changes?\n\nSometimes it looks that a [ValueObservation] does not notify the changes you expect.\n\nThere may be four possible reasons for this:\n\n1. The expected changes were not committed into the database.\n2. The expected changes were committed into the database, but were quickly overwritten.\n3. The observation was stopped.\n4. The observation does not track the expected database region.\n\nTo answer the first two questions, look at SQL statements executed by the database. This is done when you open the database connection:\n\n```swift\n// Prints all SQL statements\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    db.trace { print(\"SQL: \\($0)\") }\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\nIf, after that, you are convinced that the expected changes were committed into the database, and not overwritten soon after, trace observation events:\n\n```swift\nlet observation = ValueObservation\n    .tracking { db in ... }\n    .print() // <- trace observation events\nlet cancellable = observation.start(...)\n```\n\nLook at the observation logs which start with `cancel` or `failure`: maybe the observation was cancelled by your app, or did fail with an error.\n\nLook at the observation logs which start with `value`: make sure, again, that the expected value was not actually notified, then overwritten.\n\nFinally, look at the observation logs which start with `tracked region`. Does the printed database region cover the expected changes?\n\nFor example:\n\n- `empty`: The empty region, which tracks nothing and never triggers the observation.\n- `player(*)`: The full `player` table\n- `player(id,name)`: The `id` and `name` columns of the `player` table\n- `player(id,name)[1]`: The `id` and `name` columns of the row with id 1 in the `player` table\n- `player(*),team(*)`: Both the full `player` and `team` tables\n\nIf you happen to use the `ValueObservation.trackingConstantRegion(_:)` method and see a mismatch between the tracked region and your expectation, then change the definition of your observation by using `tracking(_:)`. You should witness that the logs which start with `tracked region` now evolve in order to include the expected changes, and that you get the expected notifications.\n\nIf after all those steps (thanks you!), your observation is still failing you, please [open an issue](https://github.com/groue/GRDB.swift/issues/new) and provide a [minimal reproducible example](https://stackoverflow.com/help/minimal-reproducible-example)!\n\n\n## FAQ: Errors\n\n- :arrow_up: [FAQ]\n- [Generic parameter 'T' could not be inferred](#generic-parameter-t-could-not-be-inferred)\n- [Mutation of captured var in concurrently-executing code](#mutation-of-captured-var-in-concurrently-executing-code)\n- [SQLite error 1 \"no such column\"](#sqlite-error-1-no-such-column)\n- [SQLite error 10 \"disk I/O error\", SQLite error 23 \"not authorized\"](#sqlite-error-10-disk-io-error-sqlite-error-23-not-authorized)\n- [SQLite error 21 \"wrong number of statement arguments\" with LIKE queries](#sqlite-error-21-wrong-number-of-statement-arguments-with-like-queries)\n\n### Generic parameter 'T' could not be inferred\n    \nYou may get this error when using the `read` and `write` methods of database queues and pools:\n\n```swift\n// Generic parameter 'T' could not be inferred\nlet string = try dbQueue.read { db in\n    let result = try String.fetchOne(db, ...)\n    return result\n}\n```\n\nThis is a limitation of the Swift compiler.\n\nThe general workaround is to explicitly declare the type of the closure result:\n\n```swift\n// General Workaround\nlet string = try dbQueue.read { db -> String? in\n    let result = try String.fetchOne(db, ...)\n    return result\n}\n```\n\nYou can also, when possible, write a single-line closure:\n\n```swift\n// Single-line closure workaround:\nlet string = try dbQueue.read { db in\n    try String.fetchOne(db, ...)\n}\n```\n\n\n### Mutation of captured var in concurrently-executing code\n\nThe `insert` and `save` [persistence methods](#persistablerecord-protocol) can trigger a compiler error in async contexts:\n\n```swift\nvar player = Player(id: nil, name: \"Arthur\")\ntry await dbWriter.write { db in\n    // Error: Mutation of captured var 'player' in concurrently-executing code\n    try player.insert(db)\n}\nprint(player.id) // A non-nil id\n```\n\nWhen this happens, prefer the `inserted` and `saved` methods instead:\n\n```swift\n// OK\nvar player = Player(id: nil, name: \"Arthur\")\nplayer = try await dbWriter.write { [player] db in\n    return try player.inserted(db)\n}\nprint(player.id) // A non-nil id\n```\n\n\n### SQLite error 1 \"no such column\"\n\nThis error message is self-explanatory: do check for misspelled or non-existing column names.\n\nHowever, sometimes this error only happens when an app runs on a recent operating system (iOS 14+, Big Sur+, etc.) The error does not happen with previous ones.\n\nWhen this is the case, there are two possible explanations:\n\n1. Maybe a column name is *really* misspelled or missing from the database schema.\n    \n    To find it, check the SQL statement that comes with the [DatabaseError](#databaseerror).\n\n2. Maybe the application is using the character `\"` instead of the single quote `'` as the delimiter for string literals in raw SQL queries. Recent versions of SQLite have learned to tell about this deviation from the SQL standard, and this is why you are seeing this error. \n    \n    For example: this is not standard SQL: `UPDATE player SET name = \"Arthur\"`.\n    \n    The standard version is: `UPDATE player SET name = 'Arthur'`.\n    \n    It just happens that old versions of SQLite used to accept the former, non-standard version. Newer versions are able to reject it with an error.\n    \n    The fix is to change the SQL statements run by the application: replace `\"` with `'` in your string literals.\n    \n    It may also be time to learn about statement arguments and [SQL injection](#avoiding-sql-injection):\n    \n    ```swift\n    let name: String = ...\n    \n    // NOT STANDARD (double quote)\n    try db.execute(sql: \"\"\"\n        UPDATE player SET name = \"\\(name)\"\n        \"\"\")\n    \n    // STANDARD, BUT STILL NOT RECOMMENDED (single quote)\n    try db.execute(sql: \"UPDATE player SET name = '\\(name)'\")\n    \n    // STANDARD, AND RECOMMENDED (statement arguments)\n    try db.execute(sql: \"UPDATE player SET name = ?\", arguments: [name])\n    \n    // STANDARD, AND RECOMMENDED (SQL interpolation)\n    try db.execute(literal: \"UPDATE player SET name = \\(name)\")\n    ```\n    \nFor more information, see [Double-quoted String Literals Are Accepted](https://sqlite.org/quirks.html#double_quoted_string_literals_are_accepted), and [Configuration.acceptsDoubleQuotedStringLiterals](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/configuration/acceptsdoublequotedstringliterals).\n    \n\n\n### SQLite error 10 \"disk I/O error\", SQLite error 23 \"not authorized\"\n\nThose errors may be the sign that SQLite can't access the database due to [data protection](https://developer.apple.com/documentation/uikit/protecting_the_user_s_privacy/encrypting_your_app_s_files).\n\nWhen your application should be able to run in the background on a locked device, it has to catch this error, and, for example, wait for [UIApplicationDelegate.applicationProtectedDataDidBecomeAvailable(_:)](https://developer.apple.com/reference/uikit/uiapplicationdelegate/1623044-applicationprotecteddatadidbecom) or [UIApplicationProtectedDataDidBecomeAvailable](https://developer.apple.com/reference/uikit/uiapplicationprotecteddatadidbecomeavailable) notification and retry the failed database operation.\n\n```swift\ndo {\n    try ...\n} catch DatabaseError.SQLITE_IOERR, DatabaseError.SQLITE_AUTH {\n    // Handle possible data protection error\n}\n```\n\nThis error can also be prevented altogether by using a more relaxed [file protection](https://developer.apple.com/reference/foundation/filemanager/1653059-file_protection_values).\n\n\n### SQLite error 21 \"wrong number of statement arguments\" with LIKE queries\n\nYou may get the error \"wrong number of statement arguments\" when executing a LIKE query similar to:\n\n```swift\nlet name = textField.text\nlet players = try dbQueue.read { db in\n    try Player.fetchAll(db, sql: \"SELECT * FROM player WHERE name LIKE '%?%'\", arguments: [name])\n}\n```\n\nThe problem lies in the `'%?%'` pattern.\n\nSQLite only interprets `?` as a parameter when it is a placeholder for a whole value (int, double, string, blob, null). In this incorrect query, `?` is just a character in the `'%?%'` string: it is not a query parameter, and is not processed in any way. See [https://www.sqlite.org/lang_expr.html#varparam](https://www.sqlite.org/lang_expr.html#varparam) for more information about SQLite parameters.\n\nTo fix the error, you can feed the request with the pattern itself, instead of the name:\n\n```swift\nlet name = textField.text\nlet players: [Player] = try dbQueue.read { db in\n    let pattern = \"%\\(name)%\"\n    return try Player.fetchAll(db, sql: \"SELECT * FROM player WHERE name LIKE ?\", arguments: [pattern])\n}\n```\n\n\nSample Code\n===========\n\n- The [Documentation](#documentation) is full of GRDB snippets.\n- [Demo Applications]\n- Open `GRDB.xcworkspace`: it contains GRDB-enabled playgrounds to play with.\n- [groue/SortedDifference](https://github.com/groue/SortedDifference): How to synchronize a database table with a JSON payload\n\n\n---\n\n**Thanks**\n\n- [Pierlis](http://pierlis.com), where we write great software.\n- [@alextrob](https://github.com/alextrob), [@alexwlchan](https://github.com/alexwlchan), [@bellebethcooper](https://github.com/bellebethcooper), [@bfad](https://github.com/bfad), [@cfilipov](https://github.com/cfilipov), [@charlesmchen-signal](https://github.com/charlesmchen-signal), [@Chiliec](https://github.com/Chiliec), [@chrisballinger](https://github.com/chrisballinger), [@darrenclark](https://github.com/darrenclark), [@davidkraus](https://github.com/davidkraus), [@eburns-vmware](https://github.com/eburns-vmware), [@felixscheinost](https://github.com/felixscheinost), [@fpillet](https://github.com/fpillet), [@gcox](https://github.com/gcox), [@GetToSet](https://github.com/GetToSet), [@gjeck](https://github.com/gjeck), [@guidedways](https://github.com/guidedways), [@gusrota](https://github.com/gusrota), [@haikusw](https://github.com/haikusw), [@hartbit](https://github.com/hartbit), [@holsety](https://github.com/holsety), [@jroselightricks](https://github.com/jroselightricks), [@kdubb](https://github.com/kdubb), [@kluufger](https://github.com/kluufger), [@KyleLeneau](https://github.com/KyleLeneau), [@layoutSubviews](https://github.com/layoutSubviews), [@mallman](https://github.com/mallman), [@MartinP7r](https://github.com/MartinP7r), [@Marus](https://github.com/Marus), [@mattgallagher](https://github.com/mattgallagher), [@MaxDesiatov](https://github.com/MaxDesiatov), [@michaelkirk-signal](https://github.com/michaelkirk-signal), [@mtancock](https://github.com/mtancock), [@pakko972](https://github.com/pakko972), [@peter-ss](https://github.com/peter-ss), [@pierlo](https://github.com/pierlo), [@pocketpixels](https://github.com/pocketpixels), [@pp5x](https://github.com/pp5x), [@professordeng](https://github.com/professordeng), [@robcas3](https://github.com/robcas3), [@runhum](https://github.com/runhum), [@sberrevoets](https://github.com/sberrevoets), [@schveiguy](https://github.com/schveiguy), [@SD10](https://github.com/SD10), [@sobri909](https://github.com/sobri909), [@sroddy](https://github.com/sroddy), [@steipete](https://github.com/steipete), [@swiftlyfalling](https://github.com/swiftlyfalling), [@Timac](https://github.com/Timac), [@tternes](https://github.com/tternes), [@valexa](https://github.com/valexa), [@wuyuehyang](https://github.com/wuyuehyang), [@ZevEisenberg](https://github.com/ZevEisenberg), and [@zmeyc](https://github.com/zmeyc) for their contributions, help, and feedback on GRDB.\n- [@aymerick](https://github.com/aymerick) and [@kali](https://github.com/kali) because SQL.\n- [ccgus/fmdb](https://github.com/ccgus/fmdb) for its excellency.\n\n---\n\n[URIs don't change: people change them.](https://www.w3.org/Provider/Style/URI)\n\n#### Adding support for missing SQL functions or operators\n\nThis chapter was renamed to [Embedding SQL in Query Interface Requests].\n\n#### Advanced DatabasePool\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### After Commit Hook\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/database/afternexttransaction(oncommit:onrollback:)).\n\n#### Asynchronous APIs\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### Changes Tracking\n\nThis chapter has been renamed [Record Comparison].\n\n#### Concurrency\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### Custom Value Types\n\nCustom Value Types conform to the [`DatabaseValueConvertible`] protocol.\n\n#### Customized Decoding of Database Rows\n\nThis chapter has been renamed [Beyond FetchableRecord].\n\n#### Customizing the Persistence Methods\n\nThis chapter was replaced with [Persistence Callbacks].\n\n#### Database Changes Observation\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseobservation).\n\n#### Database Configuration\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/configuration).\n\n#### Database Queues\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasequeue).\n\n#### Database Pools\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasepool).\n\n#### Database Snapshots\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### DatabaseWriter and DatabaseReader Protocols\n\nThis chapter was removed. See the references of [DatabaseReader](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasereader) and [DatabaseWriter](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasewriter).\n\n#### Date and UUID Coding Strategies\n\nThis chapter has been renamed [Data, Date, and UUID Coding Strategies].\n\n#### Dealing with External Connections\n\nThis chapter has been superseded by the [Sharing a Database] guide.\n\n#### Differences between Database Queues and Pools\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### Enabling FTS5 Support\n\nFTS5 is enabled by default since GRDB 6.7.0.\n\n#### FetchedRecordsController\n\nFetchedRecordsController has been removed in GRDB 5.\n\nThe [Database Observation] chapter describes the other ways to observe the database.\n\n#### Full-Text Search\n\nThis chapter has [moved](Documentation/FullTextSearch.md).\n\n#### Guarantees and Rules\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### Joined Queries Support\n\nThis chapter was replaced with the documentation of [splittingRowAdapters(columnCounts:)](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/splittingrowadapters(columncounts:)).\n\n#### List of Record Methods\n\nSee [Records and the Query Interface](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterface).\n\n#### Migrations\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/migrations).\n\n#### NSNumber and NSDecimalNumber\n\nThis chapter has [moved](#nsnumber-nsdecimalnumber-and-decimal).\n\n#### Persistable Protocol\n\nThis protocol has been renamed [PersistableRecord] in GRDB 3.0.\n\n#### PersistenceError\n\nThis error was renamed to [RecordError].\n\n#### Prepared Statements\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statement).\n\n#### Record Class\n\nThe [`Record`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/record) class is a legacy GRDB type. Since GRDB 7, it is not recommended to define record types by subclassing the `Record` class.\n\n#### Row Adapters\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/rowadapter).\n\n#### RowConvertible Protocol\n\nThis protocol has been renamed [FetchableRecord] in GRDB 3.0.\n\n#### TableMapping Protocol\n\nThis protocol has been renamed [TableRecord] in GRDB 3.0.\n\n#### Transactions and Savepoints\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/transactions).\n\n#### Transaction Hook\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/database/afternexttransaction(oncommit:onrollback:)).\n\n#### TransactionObserver Protocol\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/transactionobserver).\n\n#### Unsafe Concurrency APIs\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### ValueObservation\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/valueobservation).\n\n#### ValueObservation and DatabaseRegionObservation\n\nThis chapter has been superseded by [ValueObservation] and [DatabaseRegionObservation].\n\n[Associations]: Documentation/AssociationsBasics.md\n[Beyond FetchableRecord]: #beyond-fetchablerecord\n[Identifiable Records]: #identifiable-records\n[Codable Records]: #codable-records\n[Columns Selected by a Request]: #columns-selected-by-a-request\n[common table expression]: Documentation/CommonTableExpressions.md\n[Common Table Expressions]: Documentation/CommonTableExpressions.md\n[Conflict Resolution]: #conflict-resolution\n[Column Names Coding Strategies]: #column-names-coding-strategies\n[Data, Date, and UUID Coding Strategies]: #data-date-and-uuid-coding-strategies\n[Fetching from Requests]: #fetching-from-requests\n[Embedding SQL in Query Interface Requests]: #embedding-sql-in-query-interface-requests\n[Full-Text Search]: Documentation/FullTextSearch.md\n[Migrations]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/migrations\n[The userInfo Dictionary]: #the-userinfo-dictionary\n[JSON Columns]: #json-columns\n[FetchableRecord]: #fetchablerecord-protocol\n[EncodableRecord]: #persistablerecord-protocol\n[PersistableRecord]: #persistablerecord-protocol\n[Record Comparison]: #record-comparison\n[Record Customization Options]: #record-customization-options\n[Persistence Callbacks]: #persistence-callbacks\n[persistence callbacks]: #persistence-callbacks\n[Record Timestamps and Transaction Date]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/recordtimestamps\n[TableRecord]: #tablerecord-protocol\n[ValueObservation]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/valueobservation\n[DatabaseRegionObservation]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseregionobservation\n[RxGRDB]: https://github.com/RxSwiftCommunity/RxGRDB\n[DatabaseRegion]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseregion\n[SQL Interpolation]: Documentation/SQLInterpolation.md\n[custom SQLite build]: Documentation/CustomSQLiteBuilds.md\n[Combine]: https://developer.apple.com/documentation/combine\n[Combine Support]: Documentation/Combine.md\n[Concurrency]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency\n[Demo Applications]: Documentation/DemoApps\n[Sharing a Database]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasesharing\n[FAQ]: #faq\n[Database Observation]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseobservation\n[SQLRequest]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/sqlrequest\n[SQL literal]: Documentation/SQLInterpolation.md#sql-literal\n[Identifiable]: https://developer.apple.com/documentation/swift/identifiable\n[Query Interface Organization]: Documentation/QueryInterfaceOrganization.md\n[Database Configuration]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/configuration\n[Persistence Methods]: #persistence-methods\n[persistence methods]: #persistence-methods\n[Persistence Methods and the `RETURNING` clause]: #persistence-methods-and-the-returning-clause\n[RecordError]: #recorderror\n[RowDecodingError]: #rowdecodingerror\n[Transactions and Savepoints]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/transactions\n[`DatabaseQueue`]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasequeue\n[Database queues]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasequeue\n[`DatabasePool`]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasepool\n[database pools]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasepool\n[`DatabaseValueConvertible`]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasevalueconvertible\n[`StatementArguments`]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statementarguments\n[Prepared Statements]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statement\n[prepared statements]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statement\n[`Statement`]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statement\n[Database Connections]: #database-connections\n[Database connections]: #database-connections\n[database connection]: #database-connections\n",
      "stars_today": 3
    },
    {
      "id": 178613040,
      "name": "magic_enum",
      "full_name": "Neargye/magic_enum",
      "description": "Static reflection for enums (to string, from string, iteration) for modern C++, work with any enum type without any macro or boilerplate code",
      "html_url": "https://github.com/Neargye/magic_enum",
      "stars": 5903,
      "forks": 530,
      "language": "C++",
      "topics": [
        "c-plus-plus",
        "c-plus-plus-17",
        "cplusplus",
        "cplusplus-17",
        "cpp",
        "cpp17",
        "enum",
        "enum-to-string",
        "header-only",
        "metaprogramming",
        "no-dependencies",
        "reflection",
        "serialization",
        "single-file",
        "string-to-enum"
      ],
      "created_at": "2019-03-30T21:25:29Z",
      "updated_at": "2026-01-14T20:39:24Z",
      "pushed_at": "2026-01-01T06:03:40Z",
      "open_issues": 22,
      "owner": {
        "login": "Neargye",
        "avatar_url": "https://avatars.githubusercontent.com/u/7997966?v=4"
      },
      "readme": "[![Github releases](https://img.shields.io/github/release/Neargye/magic_enum.svg)](https://github.com/Neargye/magic_enum/releases)\n[![Conan package](https://img.shields.io/badge/Conan-package-blueviolet)](https://conan.io/center/recipes/magic_enum)\n[![Vcpkg package](https://img.shields.io/badge/Vcpkg-package-blueviolet)](https://github.com/microsoft/vcpkg/tree/master/ports/magic-enum)\n[![Build2 package](https://img.shields.io/badge/Build2-package-blueviolet)](https://www.cppget.org/magic_enum?q=magic_enum)\n[![Meson wrap](https://img.shields.io/badge/Meson-wrap-blueviolet)](https://github.com/mesonbuild/wrapdb/blob/master/subprojects/magic_enum.wrap)\n[![License](https://img.shields.io/github/license/Neargye/magic_enum.svg)](LICENSE)\n[![Compiler explorer](https://img.shields.io/badge/compiler_explorer-online-blue.svg)](https://godbolt.org/z/feqcPa5G6)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/Neargye/magic_enum/badge)](https://securityscorecards.dev/viewer/?uri=github.com/Neargye/magic_enum)\n[![Stand With Ukraine](https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/badges/StandWithUkraine.svg)](https://stand-with-ukraine.pp.ua)\n\n# Magic Enum C++\n\nHeader-only C++17 library provides static reflection for enums, work with any enum type without any macro or boilerplate code.\n\nIf you like this project, please consider donating to one of the funds that help victims of the war in Ukraine: https://u24.gov.ua.\n\n## Documentation\n\n* [Reference](doc/reference.md)\n* [Limitations](doc/limitations.md)\n* [Integration](#Integration)\n\n## [Features & Examples](example/)\n\n* Basic\n\n  ```cpp\n  #include <magic_enum/magic_enum.hpp>\n  #include <iostream>\n\n  enum class Color { RED = -10, BLUE = 0, GREEN = 10 };\n\n  int main() {\n    Color c1 = Color::RED;\n    std::cout << magic_enum::enum_name(c1) << std::endl; // RED\n    return 0;\n  }\n  ```\n\n* Enum value to string\n\n  ```cpp\n  Color color = Color::RED;\n  auto color_name = magic_enum::enum_name(color);\n  // color_name -> \"RED\"\n  ```\n\n* String to enum value\n\n  ```cpp\n  std::string color_name{\"GREEN\"};\n  auto color = magic_enum::enum_cast<Color>(color_name);\n  if (color.has_value()) {\n    // color.value() -> Color::GREEN\n  }\n\n  // case insensitive enum_cast\n  auto color = magic_enum::enum_cast<Color>(value, magic_enum::case_insensitive);\n\n  // enum_cast with BinaryPredicate\n  auto color = magic_enum::enum_cast<Color>(value, [](char lhs, char rhs) { return std::tolower(lhs) == std::tolower(rhs); }\n\n  // enum_cast with default\n  auto color_or_default = magic_enum::enum_cast<Color>(value).value_or(Color::NONE);\n  ```\n\n* Integer to enum value\n\n  ```cpp\n  int color_integer = 2;\n  auto color = magic_enum::enum_cast<Color>(color_integer);\n  if (color.has_value()) {\n    // color.value() -> Color::BLUE\n  }\n\n  auto color_or_default = magic_enum::enum_cast<Color>(value).value_or(Color::NONE);\n  ```\n\n* Indexed access to enum value\n\n  ```cpp\n  std::size_t i = 0;\n  Color color = magic_enum::enum_value<Color>(i);\n  // color -> Color::RED\n  ```\n\n* Enum value sequence\n\n  ```cpp\n  constexpr auto colors = magic_enum::enum_values<Color>();\n  // colors -> {Color::RED, Color::BLUE, Color::GREEN}\n  // colors[0] -> Color::RED\n  ```\n\n* Number of enum elements\n\n  ```cpp\n  constexpr std::size_t color_count = magic_enum::enum_count<Color>();\n  // color_count -> 3\n  ```\n\n* Enum value to integer\n\n  ```cpp\n  Color color = Color::RED;\n  auto color_integer = magic_enum::enum_integer(color); // or magic_enum::enum_underlying(color);\n  // color_integer -> 1\n  ```\n\n* Enum names sequence\n\n  ```cpp\n  constexpr auto color_names = magic_enum::enum_names<Color>();\n  // color_names -> {\"RED\", \"BLUE\", \"GREEN\"}\n  // color_names[0] -> \"RED\"\n  ```\n\n* Enum entries sequence\n\n  ```cpp\n  constexpr auto color_entries = magic_enum::enum_entries<Color>();\n  // color_entries -> {{Color::RED, \"RED\"}, {Color::BLUE, \"BLUE\"}, {Color::GREEN, \"GREEN\"}}\n  // color_entries[0].first -> Color::RED\n  // color_entries[0].second -> \"RED\"\n  ```\n\n* Enum fusion for multi-level switch/case statements\n\n  ```cpp\n  switch (magic_enum::enum_fuse(color, direction).value()) {\n    case magic_enum::enum_fuse(Color::RED, Directions::Up).value(): // ...\n    case magic_enum::enum_fuse(Color::BLUE, Directions::Down).value(): // ...\n  // ...\n  }\n  ```\n\n* Enum switch runtime value as constexpr constant\n  ```cpp\n  Color color = Color::RED;\n  magic_enum::enum_switch([] (auto val) {\n    constexpr Color c_color = val;\n    // ...\n  }, color);\n  ```\n\n* Enum iterate for each enum as constexpr constant\n  ```cpp\n  magic_enum::enum_for_each<Color>([] (auto val) {\n    constexpr Color c_color = val;\n    // ...\n  });\n  ```\n\n* Check if enum contains\n\n  ```cpp\n  magic_enum::enum_contains(Color::GREEN); // -> true\n  magic_enum::enum_contains<Color>(2); // -> true\n  magic_enum::enum_contains<Color>(123); // -> false\n  magic_enum::enum_contains<Color>(\"GREEN\"); // -> true\n  magic_enum::enum_contains<Color>(\"fda\"); // -> false\n  ```\n\n* Enum index in sequence\n\n  ```cpp\n  constexpr auto color_index = magic_enum::enum_index(Color::BLUE);\n  // color_index.value() -> 1\n  // color_index.has_value() -> true\n  ```\n\n* Functions for flags\n\n  ```cpp\n  enum Directions : std::uint64_t {\n    Left = 1,\n    Down = 2,\n    Up = 4,\n    Right = 8,\n  };\n  template <>\n  struct magic_enum::customize::enum_range<Directions> {\n    static constexpr bool is_flags = true;\n  };\n\n  magic_enum::enum_flags_name(Directions::Up | Directions::Right); // -> \"Directions::Up|Directions::Right\"\n  magic_enum::enum_flags_contains(Directions::Up | Directions::Right); // -> true\n  magic_enum::enum_flags_cast(3); // -> \"Directions::Left|Directions::Down\"\n  ```\n\n* Enum type name\n\n  ```cpp\n  Color color = Color::RED;\n  auto type_name = magic_enum::enum_type_name<decltype(color)>();\n  // type_name -> \"Color\"\n  ```\n\n* IOstream operator for enum\n\n  ```cpp\n  using magic_enum::iostream_operators::operator<<; // out-of-the-box ostream operators for enums.\n  Color color = Color::BLUE;\n  std::cout << color << std::endl; // \"BLUE\"\n  ```\n\n  ```cpp\n  using magic_enum::iostream_operators::operator>>; // out-of-the-box istream operators for enums.\n  Color color;\n  std::cin >> color;\n  ```\n\n* Bitwise operator for enum\n\n  ```cpp\n  enum class Flags { A = 1 << 0, B = 1 << 1, C = 1 << 2, D = 1 << 3 };\n  using namespace magic_enum::bitwise_operators; // out-of-the-box bitwise operators for enums.\n  // Support operators: ~, |, &, ^, |=, &=, ^=.\n  Flags flags = Flags::A | Flags::B & ~Flags::C;\n  ```\n\n* Checks whether type is an [Unscoped enumeration](https://en.cppreference.com/w/cpp/language/enum#Unscoped_enumeration).\n\n  ```cpp\n  enum color { red, green, blue };\n  enum class direction { left, right };\n\n  magic_enum::is_unscoped_enum<color>::value -> true\n  magic_enum::is_unscoped_enum<direction>::value -> false\n  magic_enum::is_unscoped_enum<int>::value -> false\n\n  // Helper variable template.\n  magic_enum::is_unscoped_enum_v<color> -> true\n  ```\n\n* Checks whether type is an [Scoped enumeration](https://en.cppreference.com/w/cpp/language/enum#Scoped_enumerations).\n\n  ```cpp\n  enum color { red, green, blue };\n  enum class direction { left, right };\n\n  magic_enum::is_scoped_enum<color>::value -> false\n  magic_enum::is_scoped_enum<direction>::value -> true\n  magic_enum::is_scoped_enum<int>::value -> false\n\n  // Helper variable template.\n  magic_enum::is_scoped_enum_v<direction> -> true\n  ```\n\n* Static storage enum variable to string\n  This version is much lighter on the compile times and is not restricted to the enum_range [limitation](doc/limitations.md).\n\n  ```cpp\n  constexpr Color color = Color::BLUE;\n  constexpr auto color_name = magic_enum::enum_name<color>();\n  // color_name -> \"BLUE\"\n  ```\n\n* `containers::array` array container for enums.\n\n  ```cpp\n  magic_enum::containers::array<Color, RGB> color_rgb_array {};\n  color_rgb_array[Color::RED] = {255, 0, 0};\n  color_rgb_array[Color::GREEN] = {0, 255, 0};\n  color_rgb_array[Color::BLUE] = {0, 0, 255};\n  magic_enum::containers::get<Color::BLUE>(color_rgb_array) // -> RGB{0, 0, 255}\n  ```\n\n* `containers::bitset` bitset container for enums.\n\n  ```cpp\n  constexpr magic_enum::containers::bitset<Color> color_bitset_red_green {Color::RED|Color::GREEN};\n  bool all = color_bitset_red_green.all();\n  // all -> false\n  // Color::BLUE is missing\n  bool test = color_bitset_red_green.test(Color::RED);\n  // test -> true\n  ```\n\n* `containers::set` set container for enums.\n\n  ```cpp\n  auto color_set = magic_enum::containers::set<Color>();\n  bool empty = color_set.empty();\n  // empty -> true\n  color_set.insert(Color::GREEN);\n  color_set.insert(Color::BLUE);\n  color_set.insert(Color::RED);\n  std::size_t size = color_set.size();\n  // size -> 3\n  ```\n\n* Improved UB-free \"SFINAE-friendly\" [underlying_type](https://en.cppreference.com/w/cpp/types/underlying_type).\n\n  ```cpp\n  magic_enum::underlying_type<color>::type -> int\n\n  // Helper types.\n  magic_enum::underlying_type_t<Direction> -> int\n  ```\n## Remarks\n\n* `magic_enum` does not pretend to be a silver bullet for reflection for enums, it was originally designed for small enum.\n\n* Before use, read the [limitations](doc/limitations.md) of functionality.\n\n## Integration\n\n* You should add the required file [magic_enum.hpp](include/magic_enum/magic_enum.hpp), and optionally other headers from [include dir](include/) or [release archive](https://github.com/Neargye/magic_enum/releases/latest). Alternatively, you can build the library with CMake.\n\n* If you are using [vcpkg](https://github.com/Microsoft/vcpkg/) on your project for external dependencies, then you can use the [magic-enum package](https://github.com/microsoft/vcpkg/tree/master/ports/magic-enum).\n\n* If you are using [Conan](https://www.conan.io/) to manage your dependencies, merely add `magic_enum/x.y.z` to your conan's requires, where `x.y.z` is the release version you want to use.\n\n* If you are using [Build2](https://build2.org/) to build and manage your dependencies, add `depends: magic_enum ^x.y.z` to the manifest file where `x.y.z` is the release version you want to use. You can then import the target using `magic_enum%lib{magic_enum}`.\n\n* Alternatively, you can use something like [CPM](https://github.com/TheLartians/CPM) which is based on CMake's `Fetch_Content` module.\n\n  ```cmake\n  CPMAddPackage(\n      NAME magic_enum\n      GITHUB_REPOSITORY Neargye/magic_enum\n      GIT_TAG vx.y.z # Where `x.y.z` is the release version you want to use.\n  )\n  ```\n\n* Bazel is also supported, simply add to your WORKSPACE file:\n\n  ```\n  http_archive(\n      name = \"magic_enum\",\n      strip_prefix = \"magic_enum-<commit>\",\n      urls = [\"https://github.com/Neargye/magic_enum/archive/<commit>.zip\"],\n  )\n  ```\n\n  To use bazel inside the repository it's possible to do:\n\n  ```\n  bazel build //...\n  bazel test //...\n  bazel run //example\n  ```\n\n  (Note that you must use a supported compiler or specify it with `export CC= <compiler>`.)\n\n* If you are using [Ros](https://www.ros.org/), you can include this package by adding `<depend>magic_enum</depend>` to your package.xml and include this package in your workspace. In your CMakeLists.txt add the following:\n  ```cmake\n  find_package(magic_enum CONFIG REQUIRED)\n  ...\n  target_link_libraries(your_executable magic_enum::magic_enum)\n  ```\n\n## Compiler compatibility\n\n* Clang/LLVM >= 5\n* MSVC++ >= 14.11 / Visual Studio >= 2017\n* Xcode >= 10\n* GCC >= 9\n* MinGW >= 9\n\n## Licensed under the [MIT License](LICENSE)\n",
      "stars_today": 3
    },
    {
      "id": 11512043,
      "name": "tasks",
      "full_name": "tasks/tasks",
      "description": "Bringing Astrid Tasks back from the dead",
      "html_url": "https://github.com/tasks/tasks",
      "stars": 4767,
      "forks": 588,
      "language": "Kotlin",
      "topics": [
        "android",
        "productivity"
      ],
      "created_at": "2013-07-18T19:44:38Z",
      "updated_at": "2026-01-14T01:30:24Z",
      "pushed_at": "2026-01-13T05:46:55Z",
      "open_issues": 1065,
      "owner": {
        "login": "tasks",
        "avatar_url": "https://avatars.githubusercontent.com/u/9398829?v=4"
      },
      "readme": "Astrid was a popular cross-platform productivity service that was [acquired](https://web.archive.org/web/20130811052500/http://blog.astrid.com/blog/2013/05/01/yahoo-acquires-astrid/) and [discontinued](https://techcrunch.com/2013/07/06/astrid-goes-dark-august-5-goodnight-sweet-squid/) in 2013. The source code from Astrid's open source Android app serves as the basis of Tasks.\n\n[<img src=\"https://play.google.com/intl/en_us/badges/images/generic/en_badge_web_generic.png\"\n    alt=\"Get it on Google Play\"\n    height=\"80\">](https://play.google.com/store/apps/details?id=org.tasks)\n[<img src=\"https://fdroid.gitlab.io/artwork/badge/get-it-on.png\"\n    alt=\"Get it on F-Droid\"\n    height=\"80\">](https://f-droid.org/packages/org.tasks)\n\nPlease visit [tasks.org](https://tasks.org) for end user documentation and support\n\n---\n\n[![Donate with Bitcoin](https://img.shields.io/badge/bitcoin-donate-yellow.svg?logo=bitcoin)](https://tasks.org/docs/donate)\n[![PayPal donate button](https://img.shields.io/badge/paypal-donate-yellow.svg?logo=paypal)](https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=alex@tasks.org)\n[![Liberapay donate button](https://img.shields.io/liberapay/receives/tasks.svg?logo=liberapay)](https://liberapay.com/tasks/donate)\n\n[![build](https://github.com/tasks/tasks/actions/workflows/bundle.yml/badge.svg)](https://github.com/tasks/tasks/actions/workflows/bundle.yml) [![weblate](https://hosted.weblate.org/widgets/tasks/-/android/svg-badge.svg)](https://hosted.weblate.org/engage/tasks/?utm_source=widget) \n\n### Contributing\n\nContributions are always welcome! Whether translations, code changes, bug reports, feature requests, or otherwise, your help is appreciated. To get started, take a look at [CONTRIBUTING.md](CONTRIBUTING.md).\n\n### Communication\n\nYou can submit questions to [GitHub Discussions](https://github.com/tasks/tasks/discussions).\n\nIf you have a suggestion or want to report a bug, please see [CONTRIBUTING.md](CONTRIBUTING.md).\n",
      "stars_today": 3
    },
    {
      "id": 23097173,
      "name": "opus",
      "full_name": "xiph/opus",
      "description": "Modern audio compression for the internet.",
      "html_url": "https://github.com/xiph/opus",
      "stars": 2933,
      "forks": 732,
      "language": "C",
      "topics": [
        "audio",
        "c",
        "codec",
        "compression"
      ],
      "created_at": "2014-08-19T04:45:15Z",
      "updated_at": "2026-01-14T18:53:59Z",
      "pushed_at": "2026-01-13T20:16:59Z",
      "open_issues": 155,
      "owner": {
        "login": "xiph",
        "avatar_url": "https://avatars.githubusercontent.com/u/8365509?v=4"
      },
      "readme": "== Opus audio codec ==\n\nOpus is a codec for interactive speech and audio transmission over the Internet.\n\n  Opus can handle a wide range of interactive audio applications, including\nVoice over IP, videoconferencing, in-game  chat, and even remote live music\nperformances. It can scale from low bit-rate narrowband speech to very high\nquality stereo music.\n\n  Opus, when coupled with an appropriate container format, is also suitable\nfor non-realtime  stored-file applications such as music distribution, game\nsoundtracks, portable music players, jukeboxes, and other applications that\nhave historically used high latency formats such as MP3, AAC, or Vorbis.\n\n                    Opus is specified by IETF RFC 6716:\n                    https://tools.ietf.org/html/rfc6716\n\n  The Opus format and this implementation of it are subject to the royalty-\nfree patent and copyright licenses specified in the file COPYING.\n\nThis package implements a shared library for encoding and decoding raw Opus\nbitstreams. Raw Opus bitstreams should be used over RTP according to\n https://tools.ietf.org/html/rfc7587\n\nThe package also includes a number of test tools used for testing the\ncorrect operation of the library. The bitstreams read/written by these\ntools should not be used for Opus file distribution: They include\nadditional debugging data and cannot support seeking.\n\nOpus stored in files should use the Ogg encapsulation for Opus which is\ndescribed at:\n https://tools.ietf.org/html/rfc7845\n\nAn opus-tools package is available which provides encoding and decoding of\nOgg encapsulated Opus files and includes a number of useful features.\n\nOpus-tools can be found at:\n https://gitlab.xiph.org/xiph/opus-tools.git\nor on the main Opus website:\n https://opus-codec.org/\n\n== Deep Learning and Opus ==\n\nLossy networks continue to be a challenge for real-time communications.\nWhile the original implementation of Opus provides an excellent packet loss\nconcealment mechanism, the team has continued to advance the methodology used\nto improve audio quality in challenge network environments.\n\nIn Opus 1.5, we added a deep learning based redundancy encoder that enhances\naudio in lossy networks by embedding one second of recovery data in the padding\ndata of each packet. The underlying algorithm behind encoding and decoding the\nrecovery data is called the deep redundancy (DRED) algorithm. By leveraging\nthe padding data within the packet, Opus 1.5 is fully backward compatible with\nprior revisions of Opus. Please see the README under the \"dnn\" subdirectory to\nunderstand DRED.\n\nDRED was developed by a team that Amazon Web Services initially sponsored,\nwho open-sourced the implementation as well as began the\nstandardization process at the IETF:\n  https://datatracker.ietf.org/doc/draft-ietf-mlcodec-opus-extension/\nThe license behind Opus or the intellectual property position of Opus does\nnot change with Opus 1.5.\n\n== Compiling libopus ==\n\nTo build from a distribution tarball, you only need to do the following:\n\n    % ./configure\n    % make\n\nTo build from the git repository, the following steps are necessary:\n\n0) Set up a development environment:\n\nOn an Ubuntu or Debian family Linux distribution:\n\n    % sudo apt-get install git autoconf automake libtool gcc make\n\nOn a Fedora/Redhat based Linux:\n\n    % sudo dnf install git autoconf automake libtool gcc make\n\nOr for older Redhat/Centos Linux releases:\n\n    % sudo yum install git autoconf automake libtool gcc make\n\nOn Apple macOS, install Xcode and brew.sh, then in the Terminal enter:\n\n    % brew install autoconf automake libtool\n\n1) Clone the repository:\n\n    % git clone https://gitlab.xiph.org/xiph/opus.git\n    % cd opus\n\n2) Compiling the source\n\n    % ./autogen.sh\n    % ./configure\n    % make\n\nOn x86, it's a good idea to use a -march= option that allows the use of AVX2.\n\n3) Install the codec libraries (optional)\n\n    % sudo make install\n\nOnce you have compiled the codec, there will be a opus_demo executable\nin the top directory.\n\nUsage: opus_demo [-e] <application> <sampling rate (Hz)> <channels (1/2)>\n         <bits per second> [options] <input> <output>\n       opus_demo -d <sampling rate (Hz)> <channels (1/2)> [options]\n         <input> <output>\n\nmode: voip | audio | restricted-lowdelay\noptions:\n  -e                : only runs the encoder (output the bit-stream)\n  -d                : only runs the decoder (reads the bit-stream as input)\n  -cbr              : enable constant bitrate; default: variable bitrate\n  -cvbr             : enable constrained variable bitrate; default:\n                      unconstrained\n  -bandwidth <NB|MB|WB|SWB|FB>\n                    : audio bandwidth (from narrowband to fullband);\n                      default: sampling rate\n  -framesize <2.5|5|10|20|40|60>\n                    : frame size in ms; default: 20\n  -max_payload <bytes>\n                    : maximum payload size in bytes, default: 1024\n  -complexity <comp>\n                    : complexity, 0 (lowest) ... 10 (highest); default: 10\n  -inbandfec        : enable SILK inband FEC\n  -forcemono        : force mono encoding, even for stereo input\n  -dtx              : enable SILK DTX\n  -loss <perc>      : simulate packet loss, in percent (0-100); default: 0\n\ninput and output are little-endian signed 16-bit PCM files or opus\nbitstreams with simple opus_demo proprietary framing.\n\n== Testing ==\n\nThis package includes a collection of automated unit and system tests\nwhich SHOULD be run after compiling the package especially the first\ntime it is run on a new platform.\n\nTo run the integrated tests:\n\n    % make check\n\nThere is also collection of standard test vectors which are not\nincluded in this package for size reasons but can be obtained from:\nhttps://opus-codec.org/docs/opus_testvectors-rfc8251.tar.gz\n\nTo run compare the code to these test vectors:\n\n    % curl -OL https://opus-codec.org/docs/opus_testvectors-rfc8251.tar.gz\n    % tar -zxf opus_testvectors-rfc8251.tar.gz\n    % ./tests/run_vectors.sh ./ opus_newvectors 48000\n\n== Compiling libopus for Windows and alternative build systems ==\n\nSee cmake/README.md or meson/README.md.\n\n== Portability notes ==\n\nThis implementation uses floating-point by default but can be compiled to\nuse only fixed-point arithmetic by setting --enable-fixed-point (if using\nautoconf) or by defining the FIXED_POINT macro (if building manually).\nThe fixed point implementation has somewhat lower audio quality and is\nslower on platforms with fast FPUs, it is normally only used in embedded\nenvironments.\n\nThe implementation can be compiled with either a C89 or a C99 compiler.\nWhile it does not rely on any _undefined behavior_ as defined by C89 or\nC99, it relies on common _implementation-defined behavior_ for two's\ncomplement architectures:\n\no Right shifts of negative values are consistent with two's\n  complement arithmetic, so that a>>b is equivalent to\n  floor(a/(2^b)),\n\no For conversion to a signed integer of N bits, the value is reduced\n  modulo 2^N to be within range of the type,\n\no The result of integer division of a negative value is truncated\n  towards zero, and\n\no The compiler provides a 64-bit integer type (a C99 requirement\n  which is supported by most C89 compilers).\n",
      "stars_today": 3
    },
    {
      "id": 2319498,
      "name": "subsurface",
      "full_name": "subsurface/subsurface",
      "description": "This is the official upstream of the Subsurface divelog program",
      "html_url": "https://github.com/subsurface/subsurface",
      "stars": 2990,
      "forks": 566,
      "language": "C++",
      "topics": [],
      "created_at": "2011-09-03T15:27:51Z",
      "updated_at": "2026-01-14T21:44:41Z",
      "pushed_at": "2026-01-07T11:15:33Z",
      "open_issues": 325,
      "owner": {
        "login": "subsurface",
        "avatar_url": "https://avatars.githubusercontent.com/u/3876063?v=4"
      },
      "readme": "# Subsurface\n\n[![Windows](https://github.com/subsurface/subsurface/actions/workflows/windows.yml/badge.svg)](https://github.com/subsurface/subsurface/actions/workflows/windows.yml)\n[![Mac](https://github.com/subsurface/subsurface/actions/workflows/mac.yml/badge.svg)](https://github.com/subsurface/subsurface/actions/workflows/mac.yml)\n[![iOS](https://github.com/subsurface/subsurface/actions/workflows/ios.yml/badge.svg)](https://github.com/subsurface/subsurface/actions/workflows/ios.yml)\n[![Android](https://github.com/subsurface/subsurface/actions/workflows/android.yml/badge.svg)](https://github.com/subsurface/subsurface/actions/workflows/android.yml)\n\n[![Snap](https://github.com/subsurface/subsurface/actions/workflows/linux-snap.yml/badge.svg)](https://github.com/subsurface/subsurface/actions/workflows/linux-snap.yml)\n[![Ubuntu 20.04 / Qt 5.15-- for AppImage](https://github.com/subsurface/subsurface/actions/workflows/linux-ubuntu-20.04-qt5-appimage.yml/badge.svg)](https://github.com/subsurface/subsurface/actions/workflows/linux-ubuntu-20.04-qt5-appimage.yml)\n[![Ubuntu 24.04 / Qt 5.15--](https://github.com/subsurface/subsurface/actions/workflows/linux-ubuntu-24.04-qt5.yml/badge.svg)](https://github.com/subsurface/subsurface/actions/workflows/linux-ubuntu-24.04-qt5.yml)\n[![Fedora 35 / Qt 6--](https://github.com/subsurface/subsurface/actions/workflows/linux-fedora-35-qt6.yml/badge.svg)](https://github.com/subsurface/subsurface/actions/workflows/linux-fedora-35-qt6.yml)\n[![Debian Bookworm / Qt 5.15--](https://github.com/subsurface/subsurface/actions/workflows/linux-debian-bookworm-5.15.yml/badge.svg)](https://github.com/subsurface/subsurface/actions/workflows/linux-debian-bookworm-5.15.yml)\n\nSubsurface can be found at http://subsurface-divelog.org/\n\nOur user forum is at http://subsurface-divelog.org/user-forum/\n\nReport bugs and issues at https://github.com/Subsurface/subsurface/issues\n\nLicense: GPLv2\n\nWe are releasing 'nightly' builds of Subsurface that are built from the latest version of the code. Versions of this build for Windows, macOS, Android (requiring sideloading), and a Linux AppImage can be downloaded from the [Latest Dev Release](https://www.subsurface-divelog.org/latest-release/) page on [our website](https://www.subsurface-divelog.org/). Alternatively, they can be downloaded [directly from GitHub](https://github.com/subsurface/nightly-builds/releases). Additionally, those same versions are\nposted to the Subsurface-daily repos on Ubuntu Launchpad, Fedora COPR, and\nOpenSUSE OBS, and released to [Snapcraft](https://snapcraft.io/subsurface) into the 'edge' channel of subsurface.\n\nYou can get the sources to the latest development version from the git\nrepository:\n\n```\ngit clone https://github.com/Subsurface/subsurface.git\n```\n\nYou can also fork the repository and browse the sources at the same site,\nsimply using https://github.com/Subsurface/subsurface\n\nAdditionally, artifacts for Windows, macOS, Android, Linux AppImage, and iOS (simulator build) are generated for all open pull requests and linked in pull request comments. Use these if you want to test the changes in a specific pull request and provide feedback before it has been merged.\n\nIf you want a more stable version that is a little bit more tested you can get this from the [Curent Release](https://www.subsurface-divelog.org/current-release/) page on [our website](https://www.subsurface-divelog.org/).\n\nDetailed build instructions can be found in the [INSTALL.md](/INSTALL.md) file.\n\n## System Requirements\n\nOn desktop, the integrated Googlemaps feature of Subsurface requires a GPU\ndriver that has support for at least OpenGL 2.1. If your driver does not\nsupport that, you may have to run Subsurface in software renderer mode.\n\nSubsurface will automatically attempt to detect this scenario, but in case\nit doesn't you may have to enable the software renderer manually with\nthe following:\n1) Learn how to set persistent environment variables on your OS\n2) Set the environment variable 'QT_QUICK_BACKEND' with the value of 'software'\n\n## Basic Usage\n\nInstall and start from the desktop, or you can run it locally from the\nbuild directory:\n\nOn Linux:\n\n```\n$ ./subsurface\n```\n\nOn Mac:\n\n```\n$ open Subsurface.app\n```\n\nNative builds on Windows are not really supported (the official Windows\ninstallers are cross-built on Linux).\n\nYou can give a data file as command line argument, or (once you have\nset this up in the Preferences) Subsurface picks a default file for\nyou when started from the desktop or without an argument.\n\nIf you have a dive computer supported by libdivecomputer, you can just\nselect \"Import from Divecomputer\" from the \"Import\" menu, select which\ndive computer you have (and where it is connected if you need to - note\nthat there's a special selection for Bluetooth dive computers), and click\non \"Download\".\n\nThe latest list of supported dive computers can be found in the file\nSupportedDivecomputers.txt.\n\nMuch more detailed end user instructions can be found from inside\nSubsurface by selecting Help (typically F1). When building from source\nthis is also available as Documentation/user-manual.txt. The\ndocumentation for the latest release is also available on-line\nhttp://subsurface-divelog.org/documentation/\n\n## Contributing\n\nThere is a user forum for questions, bug reports, and feature requests:\nhttps://groups.google.com/g/subsurface-divelog\n\nIf you want to contribute code, please open a pull request with signed-off\ncommits at https://github.com/Subsurface/subsurface/pulls\n(alternatively, you can also send your patches as emails to the developer\nmailing list).\n\nEither way, if you don't sign off your patches, we will not accept them.\nThis means adding a line that says \"Signed-off-by: Name <email>\" at the\nend of each commit, indicating that you wrote the code and have the right\nto pass it on as an open source patch under the GPLv2 license.\n\nSee: http://developercertificate.org/\n\nAlso, please write good git commit messages.  A good commit message\nlooks like this:\n\n```\nHeader line: explain the commit in one line (use the imperative)\n\nBody of commit message is a few lines of text, explaining things\nin more detail, possibly giving some background about the issue\nbeing fixed, etc etc.\n\nThe body of the commit message can be several paragraphs, and\nplease do proper word-wrap and keep columns shorter than about\n74 characters or so. That way \"git log\" will show things\nnicely even when it's indented.\n\nMake sure you explain your solution and why you're doing what you're\ndoing, as opposed to describing what you're doing. Reviewers and your\nfuture self can read the patch, but might not understand why a\nparticular solution was implemented.\n\nReported-by: whoever-reported-it\nSigned-off-by: Your Name <you@example.com>\n```\n\nwhere that header line really should be meaningful, and really should be\njust one line.  That header line is what is shown by tools like gitk and\nshortlog, and should summarize the change in one readable line of text,\nindependently of the longer explanation. Please use verbs in the\nimperative in the commit message, as in \"Fix bug that...\", \"Add\nfile/feature ...\", or \"Make Subsurface...\"\n\n## A bit of Subsurface history\n\nIn fall of 2011, when a forced lull in kernel development gave him an\nopportunity to start on a new endeavor, Linus Torvalds decided to tackle\nhis frustration with the lack of decent divelog software on Linux.\n\nSubsurface is the result of the work of him and a team of developers since\nthen. It now supports Linux, Windows and macOS and allows data import from\na large number of dive computers and several existing divelog programs. It\nprovides advanced visualization of the key information provided by a\nmodern dive computer and allows the user to track a wide variety of data\nabout their diving.\n\nIn fall of 2012 Dirk Hohndel took over as maintainer of Subsurface.\n",
      "stars_today": 3
    },
    {
      "id": 242202166,
      "name": "opentelemetry-go-contrib",
      "full_name": "open-telemetry/opentelemetry-go-contrib",
      "description": "Collection of extensions for OpenTelemetry-Go.",
      "html_url": "https://github.com/open-telemetry/opentelemetry-go-contrib",
      "stars": 1578,
      "forks": 727,
      "language": "Go",
      "topics": [],
      "created_at": "2020-02-21T18:12:36Z",
      "updated_at": "2026-01-14T10:40:04Z",
      "pushed_at": "2026-01-14T20:30:49Z",
      "open_issues": 177,
      "owner": {
        "login": "open-telemetry",
        "avatar_url": "https://avatars.githubusercontent.com/u/49998002?v=4"
      },
      "readme": "# OpenTelemetry-Go Contrib\n\n[![build_and_test](https://github.com/open-telemetry/opentelemetry-go-contrib/workflows/build_and_test/badge.svg)](https://github.com/open-telemetry/opentelemetry-go-contrib/actions?query=workflow%3Abuild_and_test+branch%3Amain)\n[![codecov.io](https://codecov.io/gh/open-telemetry/opentelemetry-go-contrib/coverage.svg?branch=main)](https://app.codecov.io/gh/open-telemetry/opentelemetry-go-contrib?branch=main)\n[![Docs](https://godoc.org/go.opentelemetry.io/contrib?status.svg)](https://pkg.go.dev/go.opentelemetry.io/contrib)\n[![Go Report Card](https://goreportcard.com/badge/go.opentelemetry.io/contrib)](https://goreportcard.com/report/go.opentelemetry.io/contrib)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/opentelemetry-go-contrib.svg)](https://issues.oss-fuzz.com/issues?q=project:opentelemetry-go-contrib)\n[![Slack](https://img.shields.io/badge/slack-@cncf/otel--go-brightgreen.svg?logo=slack)](https://cloud-native.slack.com/archives/C01NPAXACKT)\n\nCollection of 3rd-party packages for [OpenTelemetry-Go](https://github.com/open-telemetry/opentelemetry-go).\n\n## Contents\n\n- [Examples](./examples/): Examples of OpenTelemetry libraries usage.\n- [Instrumentation](./instrumentation/): Packages providing OpenTelemetry instrumentation for 3rd-party libraries.\n- [Propagators](./propagators/): Packages providing OpenTelemetry context propagators for 3rd-party propagation formats.\n- [Detectors](./detectors/): Packages providing OpenTelemetry resource detectors for 3rd-party cloud computing environments.\n- [Exporters](./exporters/): Packages providing OpenTelemetry exporters for 3rd-party export formats.\n- [Samplers](./samplers/): Packages providing additional implementations of OpenTelemetry samplers.\n- [Bridges](./bridges/): Packages providing adapters for 3rd-party instrumentation frameworks.\n- [Processors](./processors/): Packages providing additional implementations of OpenTelemetry processors.\n\n## Project Status\n\nThis project contains both stable and unstable modules.\nRefer to the module for its version or our [versioning manifest](./versions.yaml).\n\nProject versioning information and stability guarantees can be found in the [versioning documentation](https://github.com/open-telemetry/opentelemetry-go/blob/a724cf884287e04785eaa91513d26a6ef9699288/VERSIONING.md).\n\nProgress and status specific to this repository is tracked in our local [project boards](https://github.com/open-telemetry/opentelemetry-go-contrib/projects?query=is%3Aopen) and [milestones](https://github.com/open-telemetry/opentelemetry-go-contrib/milestones).\n\n### Compatibility\n\nOpenTelemetry-Go Contrib ensures compatibility with the current supported\nversions of\nthe [Go language](https://golang.org/doc/devel/release#policy):\n\n> Each major Go release is supported until there are two newer major releases.\n> For example, Go 1.5 was supported until the Go 1.7 release, and Go 1.6 was supported until the Go 1.8 release.\n\nFor versions of Go that are no longer supported upstream, opentelemetry-go-contrib will\nstop ensuring compatibility with these versions in the following manner:\n\n- A minor release of opentelemetry-go-contrib will be made to add support for the new\n  supported release of Go.\n- The following minor release of opentelemetry-go-contrib will remove compatibility\n  testing for the oldest (now archived upstream) version of Go. This, and\n  future, releases of opentelemetry-go-contrib may include features only supported by\n  the currently supported versions of Go.\n\nThis project is tested on the following systems.\n\n| OS       | Go Version | Architecture |\n| -------- | ---------- | ------------ |\n| Ubuntu   | 1.25       | amd64        |\n| Ubuntu   | 1.24       | amd64        |\n| Ubuntu   | 1.25       | 386          |\n| Ubuntu   | 1.24       | 386          |\n| macOS    | 1.25       | amd64        |\n| macOS    | 1.24       | amd64        |\n| macOS    | 1.25       | arm64        |\n| macOS    | 1.24       | arm64        |\n| Windows  | 1.25       | amd64        |\n| Windows  | 1.24       | amd64        |\n| Windows  | 1.25       | 386          |\n| Windows  | 1.24       | 386          |\n\nWhile this project should work for other systems, no compatibility guarantees\nare made for those systems currently.\n\n## Contributing\n\nFor information on how to contribute, consult [the contributing guidelines](./CONTRIBUTING.md)\n",
      "stars_today": 3
    },
    {
      "id": 429651616,
      "name": "autoware_universe",
      "full_name": "autowarefoundation/autoware_universe",
      "description": null,
      "html_url": "https://github.com/autowarefoundation/autoware_universe",
      "stars": 1451,
      "forks": 832,
      "language": "C++",
      "topics": [
        "3d-map",
        "autonomous-driving",
        "autonomous-vehicles",
        "autoware",
        "calibration",
        "planner",
        "ros",
        "ros2",
        "self-driving-car"
      ],
      "created_at": "2021-11-19T02:59:37Z",
      "updated_at": "2026-01-15T00:25:09Z",
      "pushed_at": "2026-01-15T00:25:05Z",
      "open_issues": 391,
      "owner": {
        "login": "autowarefoundation",
        "avatar_url": "https://avatars.githubusercontent.com/u/48420599?v=4"
      },
      "readme": "# Autoware Universe\n\n## Welcome to Autoware Universe\n\nAutoware Universe serves as a foundational pillar within the Autoware ecosystem, playing a critical role in enhancing the core functionalities of autonomous driving technologies.\nThis repository is a pivotal element of the Autoware Core/Universe concept, managing a wide array of packages that significantly extend the capabilities of autonomous vehicles.\n\n![autoware_universe_front](docs/assets/images/autoware_universe_front.png)\n\n## Getting Started\n\nTo dive into the vast world of Autoware and understand how Autoware Universe fits into the bigger picture, we recommend starting with the [Autoware Documentation](https://autowarefoundation.github.io/autoware-documentation/). This resource provides a thorough overview of the Autoware ecosystem, guiding you through its components, functionalities, and how to get started with development.\n\n### Explore Autoware Universe documentation\n\nFor those looking to explore the specifics of Autoware Universe components, the [Autoware Universe Documentation](https://autowarefoundation.github.io/autoware_universe/), deployed with MKDocs, offers detailed insights.\n\n## Code Coverage Metrics\n\nBelow table shows the coverage rate of entire Autoware Universe and sub-components respectively.\n\n### Entire Project Coverage\n\n[![codecov](https://codecov.io/github/autowarefoundation/autoware_universe/graph/badge.svg?token=KQP68YQ65D)](https://codecov.io/github/autowarefoundation/autoware_universe)\n\n### Component-wise Coverage\n\nYou can check more details by clicking the badge and navigating the codecov website.\n\n| Component    | Coverage                                                                                                                                                                                                                                                                                                        |\n| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Common       | [![codecov](https://img.shields.io/badge/dynamic/json?url=https://codecov.io/api/v2/github/autowarefoundation/repos/autoware_universe/components&label=Common%20Packages&query=$.[0].coverage)](https://app.codecov.io/gh/autowarefoundation/autoware_universe?components%5B0%5D=Common%20Packages)             |\n| Control      | [![codecov](https://img.shields.io/badge/dynamic/json?url=https://codecov.io/api/v2/github/autowarefoundation/repos/autoware_universe/components&label=Control%20Packages&query=$.[1].coverage)](https://app.codecov.io/gh/autowarefoundation/autoware_universe?components%5B0%5D=Control%20Packages)           |\n| Evaluator    | [![codecov](https://img.shields.io/badge/dynamic/json?url=https://codecov.io/api/v2/github/autowarefoundation/repos/autoware_universe/components&label=Evaluator%20Packages&query=$.[2].coverage)](https://app.codecov.io/gh/autowarefoundation/autoware_universe?components%5B0%5D=Evaluator%20Packages)       |\n| Launch       | TBD                                                                                                                                                                                                                                                                                                             |\n| Localization | [![codecov](https://img.shields.io/badge/dynamic/json?url=https://codecov.io/api/v2/github/autowarefoundation/repos/autoware_universe/components&label=Localization%20Packages&query=$.[4].coverage)](https://app.codecov.io/gh/autowarefoundation/autoware_universe?components%5B0%5D=Localization%20Packages) |\n| Map          | [![codecov](https://img.shields.io/badge/dynamic/json?url=https://codecov.io/api/v2/github/autowarefoundation/repos/autoware_universe/components&label=Map%20Packages&query=$.[5].coverage)](https://app.codecov.io/gh/autowarefoundation/autoware_universe?components%5B0%5D=Map%20Packages)                   |\n| Perception   | [![codecov](https://img.shields.io/badge/dynamic/json?url=https://codecov.io/api/v2/github/autowarefoundation/repos/autoware_universe/components&label=Perception%20Packages&query=$.[6].coverage)](https://app.codecov.io/gh/autowarefoundation/autoware_universe?components%5B0%5D=Perception%20Packages)     |\n| Planning     | [![codecov](https://img.shields.io/badge/dynamic/json?url=https://codecov.io/api/v2/github/autowarefoundation/repos/autoware_universe/components&label=Planning%20Packages&query=$.[7].coverage)](https://app.codecov.io/gh/autowarefoundation/autoware_universe?components%5B0%5D=Planning%20Packages)         |\n| Sensing      | [![codecov](https://img.shields.io/badge/dynamic/json?url=https://codecov.io/api/v2/github/autowarefoundation/repos/autoware_universe/components&label=Sensing%20Packages&query=$.[8].coverage)](https://app.codecov.io/gh/autowarefoundation/autoware_universe?components%5B0%5D=Sensing%20Packages)           |\n| Simulator    | [![codecov](https://img.shields.io/badge/dynamic/json?url=https://codecov.io/api/v2/github/autowarefoundation/repos/autoware_universe/components&label=Simulator%20Packages&query=$.[9].coverage)](https://app.codecov.io/gh/autowarefoundation/autoware_universe?components%5B0%5D=Simulator%20Packages)       |\n| System       | [![codecov](https://img.shields.io/badge/dynamic/json?url=https://codecov.io/api/v2/github/autowarefoundation/repos/autoware_universe/components&label=System%20Packages&query=$.[10].coverage)](https://app.codecov.io/gh/autowarefoundation/autoware_universe?components%5B0%5D=System%20Packages)            |\n| Vehicle      | [![codecov](https://img.shields.io/badge/dynamic/json?url=https://codecov.io/api/v2/github/autowarefoundation/repos/autoware_universe/components&label=Vehicle%20Packages&query=$.[11].coverage)](https://app.codecov.io/gh/autowarefoundation/autoware_universe?components%5B0%5D=Vehicle%20Packages)          |\n\n<!-- NOTE: `query` fields to shields.io should be converted to slug form   -->\n<!--\nTODO(soblin):\n- dynamic `label` name(maybe not supported by shields.io)\n- use https://github.com/marketplace/actions/dynamic-badges\n-->\n",
      "stars_today": 3
    },
    {
      "id": 597829630,
      "name": "pixel-volte-patch",
      "full_name": "kyujin-cho/pixel-volte-patch",
      "description": "Pixel IMS: Rootless replacement for Tensor Pixel VoLTE patch",
      "html_url": "https://github.com/kyujin-cho/pixel-volte-patch",
      "stars": 2485,
      "forks": 177,
      "language": "Kotlin",
      "topics": [],
      "created_at": "2023-02-05T19:02:28Z",
      "updated_at": "2026-01-14T14:19:52Z",
      "pushed_at": "2025-12-08T09:41:19Z",
      "open_issues": 74,
      "owner": {
        "login": "kyujin-cho",
        "avatar_url": "https://avatars.githubusercontent.com/u/2904494?v=4"
      },
      "readme": "# Pixel IMS: Tensor Pixel VoLTE í™œì„±í™”\n\nEnglish version available [here](https://github.com/kyujin-cho/pixel-volte-patch/blob/main/README.en.md).\n\n## ì£¼ì˜: Android 16 QPR2 Beta 3 ì´ìƒì˜ ë²„ì „ì„ ì‚¬ìš© ì¤‘ì¸ ê²½ìš°\nêµ¬ê¸€ ë³´ì•ˆ íŒ¨ì¹˜ì˜ ë„ì…ìœ¼ë¡œ ì¸í•´ Android 16 QPR2 Beta 3 ì´ìƒì˜ ë²„ì „ì—ì„œëŠ” í•´ë‹¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì´ìš©í•œ íŒ¨ì¹˜ê°€ ì¬ë¶€íŒ… ì‹œ ì´ˆê¸°í™” ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ì‚¬í•­ì€ ì•„ë˜ì˜ \"ìì£¼ ë¬»ëŠ” ì§ˆë¬¸\" ì„ ì°¸ê³ í•˜ì„¸ìš”.\n\n## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…\n\n[ì´ê³³](https://github.com/kyujin-cho/pixel-volte-patch/blob/main/docs/troubleshooting.md)ì„ ì°¸ì¡°í•˜ì„¸ìš”.\n\n## ê°œìš”\n\nì´ ë¬¸ì„œì—ì„œëŠ” Android ë‚´ë¶€ API ì¤‘ `telephony.ICarrierConfigLoader.overrideConfig()` APIë¥¼ ì´ìš©í•˜ì—¬ ë£¨íŒ… í˜¹ì€ ë¶€íŠ¸ë¡œë” ë³€ì¡° ì—†ì´ VoLTE (IMS) ê¸°ëŠ¥ì„ í™œì„±í™”í•˜ëŠ” ë²•ì— ê´€í•´ ì„¤ëª…í•©ë‹ˆë‹¤.\n\n## ì§€ì› í†µì‹ ì‚¬\n\n### 1ì°¨ ì§€ì›\n\nì•„ë˜ ëª©ë¡ì€ ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ê°€ ê°€ëŠ¥í•˜ì—¬ ì‘ë™ ê°€ëŠ¥ ì—¬ë¶€ë¥¼ ë°”ë¡œ í™•ì¸í•  ìˆ˜ ìˆëŠ” í†µì‹ ì‚¬ì…ë‹ˆë‹¤.\n\n- LG U+ (ëŒ€í•œë¯¼êµ­)\n\n### 2ì°¨ ì§€ì›\n\ní…ŒìŠ¤íŠ¸ê°€ ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ ì»¤ë®¤ë‹ˆí‹°ì— ì˜í•´ ê°€ëŠ¥í•¨ì´ í™•ì¸ëœ í†µì‹ ì‚¬ì…ë‹ˆë‹¤. ëª©ë¡ì€ [ë§í¬](https://github.com/kyujin-cho/pixel-volte-patch/blob/main/docs/compatibility-chart.md) ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n\n## ì ìš© ë°©ë²•\n\n### ì¤€ë¹„ë¬¼\n\n- Google Tensor Chipsetì´ ì ìš©ë˜ì—ˆìœ¼ë©° Android 11 ì´ìƒì´ ì„¤ì¹˜ëœ Pixel ë‹¨ë§ê¸°\n  - Google Pixel 6\n  - Google Pixel 6a\n  - Google Pixel 6 Pro\n  - Google Pixel 7\n  - Google Pixel 7a\n  - Google Pixel 7 Pro\n  - Google Pixel 8\n  - Google Pixel 8 Pro\n  - Google Pixel Fold\n- [Android Platform Tools](https://developer.android.com/studio/command-line/adb) ì´ ì„¤ì¹˜ëœ Windows, macOS í˜¹ì€ Linux ì»´í“¨í„°\n- ë°ì´í„° í†µì‹ ì´ ê°€ëŠ¥í•œ USB-A to USB-C í˜¹ì€ USB-C to USB-C ì¼€ì´ë¸”\n\n### Shizuku ì„¤ì¹˜\n\n[Shizuku](https://shizuku.rikka.app/) ëŠ” ADB í˜¹ì€ ë£¨íŠ¸ ê¶Œí•œìœ¼ë¡œ ë™ì‘í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ í†µí•˜ì—¬ ì¼ë°˜ì ì¸ ê²½ë¡œë¡œëŠ” ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” ì‹œìŠ¤í…œ APIë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì‹œìŠ¤í…œ APIì˜ í˜¸ì¶œì´ í•„ìš”í•©ë‹ˆë‹¤.\n\n1. VoLTE íŒ¨ì¹˜ë¥¼ ì ìš©í•  Pixel ë‹¨ë§ê¸°ì˜ Google Play Store ë¥¼ ì‹¤í–‰í•œ í›„ [Shizuku](https://play.google.com/store/apps/details?id=moe.shizuku.privileged.api) ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n   ![image-1](https://github.com/kyujin-cho/pixel-volte-patch/raw/main/assets/Screenshot_20230206-035249.png)\n2. ì„¤ì¹˜í•œ Shizuku ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n   ![image-2](https://github.com/kyujin-cho/pixel-volte-patch/raw/main/assets/Screenshot_20230206-035312.png)\n3. Pixel ë‹¨ë§ê¸°ì™€ ì»´í“¨í„° ê°„ ADB í†µì‹ ì´ ê°€ëŠ¥í•œ ìƒíƒœë¡œ ì¤€ë¹„ í›„ Pixel ë‹¨ë§ê¸°ì™€ ì»´í“¨í„°ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤. ADB í†µì‹ ì´ ê°€ëŠ¥í•œ ìƒíƒœë¡œ ì¤€ë¹„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œëŠ” [Shizuku ë¬¸ì„œ (ì˜ë¬¸)](https://shizuku.rikka.app/guide/setup/#start-by-connecting-to-a-computer) ì„ ì°¸ê³ í•˜ì„¸ìš”.\n4. ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì—¬ Shizuku ì„œë¹„ìŠ¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n   `adb shell sh /sdcard/Android/data/moe.shizuku.privileged.api/start.sh`\n   ![image-3](https://github.com/kyujin-cho/pixel-volte-patch/raw/main/assets/Screenshot%202023-02-06%20at%203.54.00%20AM.png)\n5. Shizuku ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í™”ë©´ì— ë‹¤ìŒê³¼ ê°™ì€ ë¬¸êµ¬ê°€ í‘œì‹œë˜ëŠ” ê²ƒì„ í™•ì¸í•©ë‹ˆë‹¤.\n   ```\n   Shizuku is running\n   Version <ì„ì˜ì˜ ë²„ì „ ë²ˆí˜¸>, adb\n   ```\n   ![image-4](https://github.com/kyujin-cho/pixel-volte-patch/raw/main/assets/Screenshot_20230206-035351.png)\n6. ì´ì œ ì¼€ì´ë¸”ì„ ì—°ê²°í•œ ì±„ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™í•©ë‹ˆë‹¤.\n\n### Pixel IMS ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì¹˜\n\n1. í˜„ì¬ ë‹¤ìŒ ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ Pixel IMS ì•±ì„ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n   - [Github Releases](https://github.com/kyujin-cho/pixel-volte-patch/releases/download/1.3.1/dev.bluehouse.enablevolte.apk) ì—ì„œ APK ë‹¤ìš´ë¡œë“œ í›„ ì„¤ì¹˜\n   - [Play Store](https://play.google.com/store/apps/details?id=dev.bluehouse.enablevolte) ì—ì„œ ë‹¤ìš´ë¡œë“œ\n2. ì„¤ì¹˜í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n3. ë‹¤ìŒê³¼ ê°™ì´ Shizuku ê¶Œí•œì„ ë¬»ëŠ” íŒì—… ì°½ì´ ëœ° ê²½ìš° \"ëª¨ë“  ê²½ìš°ì— í—ˆìš©\" ì„ ì„ íƒí•©ë‹ˆë‹¤.\n   ![image-5](https://github.com/kyujin-cho/pixel-volte-patch/raw/main/assets/Screenshot_20230208-235239.png)\n4. VoLTEë¥¼ í™œì„±í™” í•  SIMì˜ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤. \"Enable VoLTE\" í† ê¸€ì„ í™œì„±í™”í•©ë‹ˆë‹¤.\n   ![image-6](https://github.com/kyujin-cho/pixel-volte-patch/raw/main/assets/Screenshot_20230208-234343.png)\n6. VoLTEê°€ ì‘ë™í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ë•Œ ê¹Œì§€ 5ë¶„ ê°„ê²©ìœ¼ë¡œ 2-3íšŒ Pixel ê¸°ê¸°ë¥¼ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.\n\n### APK ì§ì ‘ ë¹Œë“œ\n[íŒ¨ì¹˜ëœ android.jar](https://github.com/Reginer/aosp-android-jar/raw/main/android-34/android.jar) íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í›„ì— `$ANDROID_PATH/sdk/platforms/android-34` ê²½ë¡œ ì•„ë˜ì— ë¶™ì—¬ ë„£ìŠµë‹ˆë‹¤. ì´í›„ì— ì•±ì„ ë¹Œë“œ ë° ì‹œì‘í•©ë‹ˆë‹¤.\n\n## ìì£¼ ë¬»ëŠ” ì§ˆë¬¸\n\n### ì¶”ê°€ì ì¸ ì§ˆë¬¸, ê±´ì˜ ì‚¬í•­, ë²„ê·¸ ì œë³´ ë“±ì´ ìˆìŠµë‹ˆë‹¤.\n\nì´ íŒ¨ì¹˜ì— ëŒ€í•´ ë¬¸ì˜í•  ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ë‹¤ìŒ ê¸°ëŠ¥ì„ í™œìš©í•´ ì£¼ì„¸ìš”. ëª©ì ì„ êµ¬ë¶„í•˜ì§€ ì•Šì€ ê²Œì‹œê¸€ ì‘ì„±ì˜ ê²½ìš° ì‚­ì œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n- ë²„ê·¸ ì œë³´, ê¸°ëŠ¥ ì¶”ê°€ ìš”ì²­: [Issues](https://github.com/kyujin-cho/pixel-volte-patch)\n- ê·¸ ì™¸ì˜ ëª¨ë“  ê²ƒ: [Discussions](https://github.com/kyujin-cho/pixel-volte-patch/discussions)\n\n### U+ ì´ì™¸ì˜ ë‹¤ë¥¸ í†µì‹ ì‚¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ë„ VoLTE íŒ¨ì¹˜ê°€ ê°€ëŠ¥í•œê°€ìš”?\n\nì•„ë‹ˆì˜¤. ì§€ì› ëŒ€ìƒì€ LG U+ ë° U+ í†µì‹ ë§ì„ ì‚¬ìš©í•˜ëŠ” MVNO (ì•Œëœ°í°)ìœ¼ë¡œ í•œì •ë©ë‹ˆë‹¤.\n\n### VoLTEê°€ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸ ê°€ëŠ¥í•œ ë°©ë²•ì´ ìˆë‚˜ìš”?\n\nì• í”Œë¦¬ì¼€ì´ì…˜ì˜ Home í˜ì´ì§€ì—ì„œ `IMS Status` í•­ëª©ì´ `Registered`ì´ë©´ VoLTEê°€ ì„±ê³µì ìœ¼ë¡œ í™œì„±í™”ëœ ê²ƒì…ë‹ˆë‹¤.\n![image-7](https://github.com/kyujin-cho/pixel-volte-patch/raw/main/assets/Screenshot_20230208-234340.png)\n\në”ìš± ìƒì„¸í•œ ì •ë³´ê°€ í•„ìš”í•  ê²½ìš°, Pixel ë‹¨ë§ê¸°ì— ë‚´ì¥ ì œê³µë˜ëŠ” í†µì‹  ì •ë³´ í™•ì¸ìš© ë‚´ë¶€ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n1. Pixel ë‹¨ë§ê¸°ì˜ ê¸°ë³¸ ì „í™” ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\n   ![image-8](https://github.com/kyujin-cho/pixel-volte-patch/raw/main/assets/Screenshot_20230206-035705.png)\n2. í‚¤íŒ¨ë“œì—ì„œ `*#*#4636#*#*` í‚¤ë¥¼ ì°¨ë¡€ëŒ€ë¡œ ì…ë ¥í•©ë‹ˆë‹¤.\n   ![image-9](https://github.com/kyujin-cho/pixel-volte-patch/raw/main/assets/Screenshot_20230206-035701.png)\n3. \"Phone information\" í•­ëª©ì„ í„°ì¹˜í•©ë‹ˆë‹¤.\n   ![image-10](https://github.com/kyujin-cho/pixel-volte-patch/raw/main/assets/Screenshot_20230206-035650.png)\n4. ìš°ì¸¡ ìƒë‹¨ì˜ ì‚¼ì  ë©”ë‰´ë¥¼ í„°ì¹˜ í›„ \"IMS Service Status\" í•­ëª©ì„ í„°ì¹˜í•©ë‹ˆë‹¤.\n   ![image-11](https://github.com/kyujin-cho/pixel-volte-patch/raw/main/assets/Screenshot_20230206-030524.png)\n5. ë‹¤ìŒê³¼ ê°™ì€ ë¬¸êµ¬ê°€ í‘œì‹œëœë‹¤ë©´ VoLTEê°€ í™œì„±í™” ëœ ê²ƒì…ë‹ˆë‹¤.  \n   `IMS Registration: Registered`\n   ![image-12](https://github.com/kyujin-cho/pixel-volte-patch/raw/main/assets/Screenshot_20230206-035645.png)\n\n### í•´ë‹¹ íŒ¨ì¹˜ëŠ” ì¬ë¶€íŒ… ì‹œë§ˆë‹¤ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ì•¼ í•˜ë‚˜ìš”?\n\n- ì¥ì¹˜ê°€ Android 16 QPR2 Beta 3 ì´ìƒì˜ ì†Œí”„íŠ¸ì›¨ì–´ ë²„ì „ì—ì„œ ì‘ë™í•  ê²½ìš°: [ë„¤](https://github.com/kyujin-cho/pixel-volte-patch/issues/398).\n- ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš°: ì•„ë‹ˆì˜¤.\n\n### í•´ë‹¹ íŒ¨ì¹˜ëŠ” ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì‹œë§ˆë‹¤ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ì•¼ í•˜ë‚˜ìš”?\n\në„¤.\n\n### í•´ë‹¹ íŒ¨ì¹˜ì˜ ì‘ë™ ì›ë¦¬ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\n\nAndroidì—ì„œ VoLTE (IMS) ê°€ í™œì„±í™”ë˜ê¸° ìœ„í•´ì„œëŠ” `ImsManager.isVolteEnabledByPlatform(Context)` ë©”ì„œë“œê°€ trueë¥¼ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤. í•´ë‹¹ ë©”ì„œë“œì˜ êµ¬í˜„ì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ (ref: [googlesource.com](https://android.googlesource.com/platform/frameworks/opt/net/ims/+/002b204/src/java/com/android/ims/ImsManager.java)).\n\n1. `persist.dbg.volte_avail_ovr` System Propertyê°€ trueì¸ì§€ í™•ì¸ (ê¸°ì¡´ì˜ setpropì„ ì´ìš©í•œ VoLTE íŒ¨ì¹˜ ë°©ì‹)\n   - ê·¸ëŸ´ ê²½ìš° true ë°˜í™˜\n   - ì•„ë‹ ê²½ìš° ê³„ì†\n2. ê¸°ê¸° ìì²´ì—ì„œ VoLTE ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸\n   - ì•„ë‹ ê²½ìš° false ë°˜í™˜\n   - ê·¸ëŸ´ ê²½ìš° ê³„ì†\n3. í†µì‹ ì‚¬ì—ì„œ VoLTE ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸\n   - ì•„ë‹ ê²½ìš° false ë°˜í™˜\n   - ê·¸ëŸ´ ê²½ìš° ê³„ì†\n4. í†µì‹ ì‚¬ì—ì„œ IMS í™œì„±í™”ë¥¼ ìœ„í•´ GBA capable SIMì„ ìš”êµ¬í•˜ëŠ”ì§€ í™•ì¸\n   - ì•„ë‹ ê²½ìš° true ë°˜í™˜\n   - ê·¸ëŸ´ ê²½ìš° ê³„ì†\n5. EF ISTì— GBA bitì´ í™œì„±í™” ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n   - ê·¸ëŸ´ ê²½ìš° true ë°˜í™˜\n   - ì•„ë‹ ê²½ìš° false ë°˜í™˜\n\nëŒ€í•œë¯¼êµ­ì—ì„œ Tensor Chipì„ íƒ‘ì¬í•œ Pixelë¡œ LG U+ë¥¼ ì‚¬ìš©í•˜ë ¤ëŠ” ê²½ìš°, ê¸°ê¸°ì—ì„œëŠ” VoLTEë¥¼ ì§€ì›í•˜ì§€ë§Œ í†µì‹ ì‚¬ì—ì„œ ìì²´ ì„¤ì •ì„ í”„ë¡œë¹„ì „í•˜ì§€ ì•Šì•„ 3ë²ˆ \"í†µì‹ ì‚¬ì—ì„œ VoLTE ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸\"ì´ falseë¡œ ì²˜ë¦¬ë˜ì–´ ê¸°ê¸°ì—ì„œ IMSê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤. LG U+ì˜ ê²½ìš°ì—ëŠ” Pixelì— ë‚´ì¥ëœ VoLTE ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ í†µì‹ ì‚¬ì˜ ì¶”ê°€ì ì¸ ì„¤ì •ì´ ì—†ì–´ VoLTEê°€ ë¹„í™œì„±í™” ë˜ëŠ” ê²ƒì´ë¯€ë¡œ, ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ìœ„ì—ì„œ ì–¸ê¸‰í•œ Shizukuì™€ `CarrierConfigLoader`ì˜ ì„¤ì • ê°•ì œ í™œì„±í™” APIë¥¼ ì¡°í•©í•˜ì—¬ í•´ë‹¹ ì„¤ì •ì„ ê°•ì œë¡œ trueë¡œ ë³€ê²½í•˜ì—¬ ì‹œìŠ¤í…œì—ì„œ VoLTE í™œì„±í™”ë¥¼ ì‹œë„í•  ìˆ˜ ìˆë„ë¡ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
      "stars_today": 3
    },
    {
      "id": 883876139,
      "name": "metro",
      "full_name": "ZacSweers/metro",
      "description": "A multiplatform, compile-time dependency injection framework for Kotlin",
      "html_url": "https://github.com/ZacSweers/metro",
      "stars": 953,
      "forks": 70,
      "language": "Kotlin",
      "topics": [
        "compiler-plugin",
        "dependency-injection",
        "di",
        "jakarta-inject",
        "javax-inject",
        "jsr-330",
        "kotlin"
      ],
      "created_at": "2024-11-05T18:19:16Z",
      "updated_at": "2026-01-14T23:53:33Z",
      "pushed_at": "2026-01-14T05:34:58Z",
      "open_issues": 41,
      "owner": {
        "login": "ZacSweers",
        "avatar_url": "https://avatars.githubusercontent.com/u/1361086?v=4"
      },
      "readme": "# ğŸš‡ Metro\n\nA compile-time dependency injection framework for Kotlin Multiplatform, powered by a Kotlin compiler plugin.\n\n[![Maven Central](https://img.shields.io/maven-central/v/dev.zacsweers.metro/runtime.svg)](https://github.com/ZacSweers/metro/releases)\n[![Kotlin](https://img.shields.io/badge/Kotlin-2.2.20%20--%202.3.20--dev--5437-blue.svg?logo=kotlin)](docs/compatibility.md)\n[![Build Status](https://github.com/ZacSweers/metro/actions/workflows/ci.yml/badge.svg)](https://github.com/ZacSweers/metro/actions/workflows/ci.yml)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0)\n\n---\n\n## What is Metro?\n\nMetro is a compile-time dependency injection framework that combines the best of [Dagger](https://github.com/google/dagger), [Anvil](https://github.com/square/anvil), and [kotlin-inject](https://github.com/evant/kotlin-inject) into one cohesive solution.\n\n**Key Features:**\n\n- âœ… **Compile-time validation** â€“ Catch dependency graph errors during compilation, not at runtime\n- ğŸ§© **FIR/IR code generation** â€“ No KAPT or KSP required, just a Kotlin compiler plugin\n- ğŸ¯ **Kotlin-first API** â€“ Inspired by kotlin-inject with top-level function injection and optional dependencies\n- ğŸ—¡ï¸ **Dagger-esque runtime** â€“ Lean generated code with familiar patterns\n- âš’ï¸ **Anvil-style aggregation** â€“ `@ContributesTo`, `@ContributesBinding`, and more\n- ğŸŒ **Multiplatform** â€“ Supports JVM, JS, WASM, and Native targets\n- ğŸ’¡ **Helpful diagnostics** â€“ Detailed error messages with actionable suggestions\n- ğŸ”— **Advanced interop** â€“ Migrate incrementally from Dagger, kotlin-inject, or Guice\n- âš¡ï¸ **Fast** - At build time, at runtime\n\n---\n\n## Quick Start\n\n**1. Apply the Gradle plugin:**\n\n```kotlin\nplugins {\n  id(\"dev.zacsweers.metro\") version \"<version>\"\n}\n```\n\n**2. Define a dependency graph:**\n\n```kotlin\n@DependencyGraph\ninterface AppGraph {\n  val repository: UserRepository\n\n  @Provides\n  fun provideApi(): Api = ApiImpl()\n}\n\n@Inject\nclass UserRepository(private val api: Api)\n```\n\n**3. Create and use the graph:**\n\n```kotlin\nval graph = createGraph<AppGraph>()\nval repository = graph.repository\n```\n\n---\n\n## Documentation\n\nğŸ“š **[zacsweers.github.io/metro](https://zacsweers.github.io/metro/latest/)**\n\n| Topic                                                                            |                                                |\n|----------------------------------------------------------------------------------|------------------------------------------------|\n| [Installation](https://zacsweers.github.io/metro/latest/installation/)           | Setup and configuration                        |\n| [Dependency Graphs](https://zacsweers.github.io/metro/latest/dependency-graphs/) | Define and create graphs                       |\n| [Provides](https://zacsweers.github.io/metro/latest/provides/)                   | Provider functions and properties              |\n| [Injection Types](https://zacsweers.github.io/metro/latest/injection-types/)     | Constructor, assisted, and member injection    |\n| [Scopes](https://zacsweers.github.io/metro/latest/scopes/)                       | Scoping and lifecycle management               |\n| [Aggregation](https://zacsweers.github.io/metro/latest/aggregation/)             | Anvil-style contributions across modules       |\n| [Interop](https://zacsweers.github.io/metro/latest/interop/)                     | Dagger, kotlin-inject, and Guice compatibility |\n| [Performance](https://zacsweers.github.io/metro/latest/performance/)             | Build and runtime performance                  |\n| [Compatibility](https://zacsweers.github.io/metro/latest/compatibility/)         | Supported Kotlin versions                      |\n| [FAQ](https://zacsweers.github.io/metro/latest/faq/)                             | Frequently asked questions                     |\n| [API Docs](https://zacsweers.github.io/metro/latest/api/)                        | Generated KDocs                                |\n\n---\n\n## Supported Platforms\n\nMetro supports JVM, Android, JS, WASM, and Native targets. The compiler plugin works with all Kotlin Multiplatform project types.\n\nSee the [multiplatform docs](https://zacsweers.github.io/metro/latest/multiplatform/) for full details.\n\n---\n\nLicense\n-------\n\n    Copyright (C) 2025 Zac Sweers\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       https://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n",
      "stars_today": 3
    },
    {
      "id": 248836530,
      "name": "scTenifoldKnk",
      "full_name": "cailab-tamu/scTenifoldKnk",
      "description": " R/MATLAB package to perform virtual knockout experiments on single-cell gene regulatory networks.",
      "html_url": "https://github.com/cailab-tamu/scTenifoldKnk",
      "stars": 125,
      "forks": 13,
      "language": "R",
      "topics": [
        "functional-genomics",
        "gene-function",
        "gene-knockout",
        "gene-regulatory-network",
        "virtual-knockout-experiments"
      ],
      "created_at": "2020-03-20T19:31:03Z",
      "updated_at": "2026-01-14T15:28:02Z",
      "pushed_at": "2025-09-16T19:17:04Z",
      "open_issues": 23,
      "owner": {
        "login": "cailab-tamu",
        "avatar_url": "https://avatars.githubusercontent.com/u/31963060?v=4"
      },
      "readme": "scTenifoldKnk\n=============\n\nA R/MATLAB/Python package to perform virtual knockout experiments on single-cell gene regulatory networks. **scTenifoldKnk** is a machine learning workflow that performs virtual knockout experiments using single-cell RNA sequencing (scRNAseq) data from wild-type (WT) control samples as input. Constructs a single-cell gene regulatory network (scGRN) and knocks out a target gene from the adjacency matrix of the WT scGRN by setting the geneâ€™s outdegree edges to zero. **scTenifoldKnk** then compares the knocked out scGRN with the WT scGRN to identify differentially regulated genes, called virtual-knockout perturbed genes, which are used to assess the impact of the gene knockout and reveal the geneâ€™s function in the analyzed cells.\n\nPython version of scTenifoldKnk is available at: https://github.com/qwerty239qwe/scTenifoldpy\n\nMATLAB version is available at: https://github.com/jamesjcai/scGEAToolbox\n\nInstall:\n-------\nYou can install **scTenifoldKnk/R** using the following command:\n\n```{R}\nlibrary(remotes)\ninstall_github('cailab-tamu/scTenifoldKnk')\nlibrary(scTenifoldKnk)\n```\n\nAvailable functions:\n--------------------\n\n|Code| Function |\n|:-|:-|\n|scTenifoldKnk|Perform virtual knockout experiments on single-cell gene regulatory networks|\n\nInput:\n--------\nThe required input for **scTenifoldKnk** is an expression matrix with genes in the rows and cells (barcodes) in the columns. Data is expected to be previously normalized or _not normalized_ if `QC = TRUE`.\n\nRunning time:\n--------\nThe running time of scTenifoldKnk is largely dependent on how long it takes to construct scGRNs from subsampled expression matrices. Time increases proportional to the number of cells and genes in the dataset used as input. Below is a table of running times under different scenarios:\n\n| Number of Cells | Number of Genes | Running Time |\n|-----------------|-----------------|--------------|\n| 300             | 1000            | 3.45 min     |\n| 1000            | 1000            | 4.25 min     |\n| 1000            | 5000            | 171.88 min (2 h 51.6 min) |\n| 2500            | 5000            | 175.29 min (2 h 55.3 min) |\n| 5000            | 5000            | 188.88 min (3 h 8.9 min) |\n| 5000            | 7500            | 189.51 min (3 h 9.5 min)  |\n| 7500            | 5000            | 615.45 min (10 h 15.5 min) |\n| 7500            | 7500            | 616.12 min (10 h 16.1 min)  |\n\n\nOutput:\n--------\nThe output of **scTenifoldKnk** is a list with 3 slots as follows: \n  * **tensorNetworks**: The computed weight-averaged denoised gene regulatory networks after CANDECOMP/PARAFAC (CP) tensor decomposition. It includes two slots with:\n    * **X**: The constructed network for the _X_ sample.\n    * **Y**: The constructed network for the _Y_ sample.\n  * **manifoldAlignment**: The generated low-dimensional features result of the non-linear manifold alignment. It is a data frame with _2 times the number of genes_ in the rows and _d_ (default= 2) dimensions in the columns\n  * **diffRegulation**: The results of the differential regulation analysis. It is a data frame with 6 columns as follows:\n    * **gene**: A character vector with the gene id identified from the manifoldAlignment output.\n    * **distance**: A numeric vector of the Euclidean distance computed between the coordinates of the same gene in both conditions.\n    * **Z**: A numeric vector of the Z-scores computed after Box-Cox power transformation.\n    * **FC**: A numeric vector of the FC computed with respect to the expectation.\n    * **p.value**: A numeric vector of the p-values associated to the fold-changes, probabilities are asigned as P[X > x] using the Chi-square distribution with one degree of freedom.\n    * **p.adj**: A numeric vector of adjusted p-values using Benjamini & Hochberg (1995) FDR correction.\n\n---\nThe function to plot the egocentric KO, the code is available at [https://github.com/dosorio/utilities/blob/master/singleCell/plotKO.R](https://github.com/dosorio/utilities/blob/master/singleCell/plotKO.R), it requires: The object out of Knk (as X), the gene to knockout (gKO).\n\n\nÂ©ï¸ The Texas A & M University System. All rights reserved.\n",
      "stars_today": 3
    },
    {
      "id": 63537249,
      "name": "create-react-app",
      "full_name": "facebook/create-react-app",
      "description": "Set up a modern web app by running one command.",
      "html_url": "https://github.com/facebook/create-react-app",
      "stars": 103950,
      "forks": 27152,
      "language": "JavaScript",
      "topics": [
        "build-tools",
        "react",
        "zero-configuration"
      ],
      "created_at": "2016-07-17T14:55:11Z",
      "updated_at": "2026-01-14T19:36:45Z",
      "pushed_at": "2025-02-15T01:32:11Z",
      "open_issues": 2361,
      "owner": {
        "login": "facebook",
        "avatar_url": "https://avatars.githubusercontent.com/u/69631?v=4"
      },
      "readme": "## Create React App [![Build & Test](https://github.com/facebook/create-react-app/actions/workflows/build-and-test.yml/badge.svg?branch=main)](https://github.com/facebook/create-react-app/actions/workflows/build-and-test.yml) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-green.svg)](https://github.com/facebook/create-react-app/blob/main/CONTRIBUTING.md)\n\n> [!CAUTION]\n>\n> ## Deprecated\n>\n> Create React App was one of the key tools for getting a React project up-and-running in 2017-2021, it is now in long-term stasis and we recommend that you migrate to one of React frameworks documented on [Start a New React Project](https://react.dev/learn/start-a-new-react-project).\n>\n> If you are following a tutorial to learn React, there is still value in continuing your tutorial, but we do not recommend starting production apps based on Create React App.\n\n<img alt=\"Logo\" align=\"right\" src=\"https://create-react-app.dev/img/logo.svg\" width=\"20%\" />\n\nCreate React apps with no build configuration.\n\n- [Creating an App](#creating-an-app) â€“ How to create a new app.\n- [User Guide](https://facebook.github.io/create-react-app/) â€“ How to develop apps bootstrapped with Create React App.\n\nCreate React App works on macOS, Windows, and Linux.<br>\nIf something doesnâ€™t work, please [file an issue](https://github.com/facebook/create-react-app/issues/new).<br>\nIf you have questions or need help, please ask in [GitHub Discussions](https://github.com/facebook/create-react-app/discussions).\n\n## Quick Overview\n\n```sh\nnpx create-react-app my-app\ncd my-app\nnpm start\n```\n\nIf you've previously installed `create-react-app` globally via `npm install -g create-react-app`, we recommend you uninstall the package using `npm uninstall -g create-react-app` or `yarn global remove create-react-app` to ensure that npx always uses the latest version.\n\n_([npx](https://medium.com/@maybekatz/introducing-npx-an-npm-package-runner-55f7d4bd282b) comes with npm 5.2+ and higher, see [instructions for older npm versions](https://gist.github.com/gaearon/4064d3c23a77c74a3614c498a8bb1c5f))_\n\nThen open [http://localhost:3000/](http://localhost:3000/) to see your app.<br>\nWhen youâ€™re ready to deploy to production, create a minified bundle with `npm run build`.\n\n<p align='center'>\n<img src='https://cdn.jsdelivr.net/gh/facebook/create-react-app@27b42ac7efa018f2541153ab30d63180f5fa39e0/screencast.svg' width='600' alt='npm start'>\n</p>\n\n### Get Started Immediately\n\nYou **donâ€™t** need to install or configure tools like webpack or Babel.<br>\nThey are preconfigured and hidden so that you can focus on the code.\n\nCreate a project, and youâ€™re good to go.\n\n## Creating an App\n\n**Youâ€™ll need to have Node 14.0.0 or later version on your local development machine** (but itâ€™s not required on the server). We recommend using the latest LTS version. You can use [nvm](https://github.com/creationix/nvm#installation) (macOS/Linux) or [nvm-windows](https://github.com/coreybutler/nvm-windows#node-version-manager-nvm-for-windows) to switch Node versions between different projects.\n\nTo create a new app, you may choose one of the following methods:\n\n### npx\n\n```sh\nnpx create-react-app my-app\n```\n\n_([npx](https://medium.com/@maybekatz/introducing-npx-an-npm-package-runner-55f7d4bd282b) is a package runner tool that comes with npm 5.2+ and higher, see [instructions for older npm versions](https://gist.github.com/gaearon/4064d3c23a77c74a3614c498a8bb1c5f))_\n\n### npm\n\n```sh\nnpm init react-app my-app\n```\n\n_`npm init <initializer>` is available in npm 6+_\n\n### Yarn\n\n```sh\nyarn create react-app my-app\n```\n\n_[`yarn create <starter-kit-package>`](https://yarnpkg.com/lang/en/docs/cli/create/) is available in Yarn 0.25+_\n\nIt will create a directory called `my-app` inside the current folder.<br>\nInside that directory, it will generate the initial project structure and install the transitive dependencies:\n\n```\nmy-app\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ node_modules\nâ”œâ”€â”€ package.json\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ public\nâ”‚   â”œâ”€â”€ favicon.ico\nâ”‚   â”œâ”€â”€ index.html\nâ”‚   â””â”€â”€ manifest.json\nâ””â”€â”€ src\n    â”œâ”€â”€ App.css\n    â”œâ”€â”€ App.js\n    â”œâ”€â”€ App.test.js\n    â”œâ”€â”€ index.css\n    â”œâ”€â”€ index.js\n    â”œâ”€â”€ logo.svg\n    â””â”€â”€ serviceWorker.js\n    â””â”€â”€ setupTests.js\n```\n\nNo configuration or complicated folder structures, only the files you need to build your app.<br>\nOnce the installation is done, you can open your project folder:\n\n```sh\ncd my-app\n```\n\nInside the newly created project, you can run some built-in commands:\n\n### `npm start` or `yarn start`\n\nRuns the app in development mode.<br>\nOpen [http://localhost:3000](http://localhost:3000) to view it in the browser.\n\nThe page will automatically reload if you make changes to the code.<br>\nYou will see the build errors and lint warnings in the console.\n\n<p align='center'>\n<img src='https://cdn.jsdelivr.net/gh/marionebl/create-react-app@9f6282671c54f0874afd37a72f6689727b562498/screencast-error.svg' width='600' alt='Build errors'>\n</p>\n\n### `npm test` or `yarn test`\n\nRuns the test watcher in an interactive mode.<br>\nBy default, runs tests related to files changed since the last commit.\n\n[Read more about testing.](https://facebook.github.io/create-react-app/docs/running-tests)\n\n### `npm run build` or `yarn build`\n\nBuilds the app for production to the `build` folder.<br>\nIt correctly bundles React in production mode and optimizes the build for the best performance.\n\nThe build is minified and the filenames include the hashes.<br>\n\nYour app is ready to be deployed.\n\n## User Guide\n\nYou can find detailed instructions on using Create React App and many tips in [its documentation](https://facebook.github.io/create-react-app/).\n\n## How to Update to New Versions?\n\nPlease refer to the [User Guide](https://facebook.github.io/create-react-app/docs/updating-to-new-releases) for this and other information.\n\n## Philosophy\n\n- **One Dependency:** There is only one build dependency. It uses webpack, Babel, ESLint, and other amazing projects, but provides a cohesive curated experience on top of them.\n\n- **No Configuration Required:** You don't need to configure anything. A reasonably good configuration of both development and production builds is handled for you so you can focus on writing code.\n\n- **No Lock-In:** You can â€œejectâ€ to a custom setup at any time. Run a single command, and all the configuration and build dependencies will be moved directly into your project, so you can pick up right where you left off.\n\n## Whatâ€™s Included?\n\nYour environment will have everything you need to build a modern single-page React app:\n\n- React, JSX, ES6, TypeScript and Flow syntax support.\n- Language extras beyond ES6 like the object spread operator.\n- Autoprefixed CSS, so you donâ€™t need `-webkit-` or other prefixes.\n- A fast interactive unit test runner with built-in support for coverage reporting.\n- A live development server that warns about common mistakes.\n- A build script to bundle JS, CSS, and images for production, with hashes and sourcemaps.\n- An offline-first [service worker](https://developers.google.com/web/fundamentals/getting-started/primers/service-workers) and a [web app manifest](https://developers.google.com/web/fundamentals/engage-and-retain/web-app-manifest/), meeting all the [Progressive Web App](https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app) criteria. (_Note: Using the service worker is opt-in as of `react-scripts@2.0.0` and higher_)\n- Hassle-free updates for the above tools with a single dependency.\n\nCheck out [this guide](https://github.com/nitishdayal/cra_closer_look) for an overview of how these tools fit together.\n\nThe tradeoff is that **these tools are preconfigured to work in a specific way**. If your project needs more customization, you can [\"eject\"](https://facebook.github.io/create-react-app/docs/available-scripts#npm-run-eject) and customize it, but then you will need to maintain this configuration.\n\n## Popular Alternatives\n\nCreate React App is a great fit for:\n\n- **Learning React** in a comfortable and feature-rich development environment.\n- **Starting new single-page React applications.**\n- **Creating examples** with React for your libraries and components.\n\nHere are a few common cases where you might want to try something else:\n\n- If you want to **try React** without hundreds of transitive build tool dependencies, consider [using a single HTML file or an online sandbox instead](https://reactjs.org/docs/getting-started.html#try-react).\n\n- If you need to **integrate React code with a server-side template framework** like Rails, Django or Symfony, or if youâ€™re **not building a single-page app**, consider using [nwb](https://github.com/insin/nwb), or [Neutrino](https://neutrino.js.org/) which are more flexible. For Rails specifically, you can use [Rails Webpacker](https://github.com/rails/webpacker). For Symfony, try [Symfony's webpack Encore](https://symfony.com/doc/current/frontend/encore/reactjs.html).\n\n- If you need to **publish a React component**, [nwb](https://github.com/insin/nwb) can [also do this](https://github.com/insin/nwb#react-components-and-libraries), as well as [Neutrino's react-components preset](https://neutrino.js.org/packages/react-components/).\n\n- If you want to do **server rendering** with React and Node.js, check out [Next.js](https://nextjs.org/) or [Razzle](https://github.com/jaredpalmer/razzle). Create React App is agnostic of the backend, and only produces static HTML/JS/CSS bundles.\n\n- If your website is **mostly static** (for example, a portfolio or a blog), consider using [Gatsby](https://www.gatsbyjs.org/) or [Next.js](https://nextjs.org/). Unlike Create React App, Gatsby pre-renders the website into HTML at build time. Next.js supports both server rendering and pre-rendering.\n\n- Finally, if you need **more customization**, check out [Neutrino](https://neutrino.js.org/) and its [React preset](https://neutrino.js.org/packages/react/).\n\nAll of the above tools can work with little to no configuration.\n\nIf you prefer configuring the build yourself, [follow this guide](https://reactjs.org/docs/add-react-to-a-website.html).\n\n## React Native\n\nLooking for something similar, but for React Native?<br>\nCheck out [Expo CLI](https://github.com/expo/expo-cli).\n\n## Contributing\n\nWe'd love to have your helping hand on `create-react-app`! See [CONTRIBUTING.md](CONTRIBUTING.md) for more information on what we're looking for and how to get started.\n\n## Supporting Create React App\n\nCreate React App is a community maintained project and all contributors are volunteers. If you'd like to support the future development of Create React App then please consider donating to our [Open Collective](https://opencollective.com/create-react-app).\n\n## Credits\n\nThis project exists thanks to all the people who [contribute](CONTRIBUTING.md).<br>\n<a href=\"https://github.com/facebook/create-react-app/graphs/contributors\"><img src=\"https://opencollective.com/create-react-app/contributors.svg?width=890&button=false\" /></a>\n\nThanks to [Netlify](https://www.netlify.com/) for hosting our documentation.\n\n## Acknowledgements\n\nWe are grateful to the authors of existing related projects for their ideas and collaboration:\n\n- [@eanplatter](https://github.com/eanplatter)\n- [@insin](https://github.com/insin)\n- [@mxstbr](https://github.com/mxstbr)\n\n## License\n\nCreate React App is open source software [licensed as MIT](https://github.com/facebook/create-react-app/blob/main/LICENSE). The Create React App logo is licensed under a [Creative Commons Attribution 4.0 International license](https://creativecommons.org/licenses/by/4.0/).\n",
      "stars_today": 2
    },
    {
      "id": 22458259,
      "name": "Alamofire",
      "full_name": "Alamofire/Alamofire",
      "description": "Elegant HTTP Networking in Swift",
      "html_url": "https://github.com/Alamofire/Alamofire",
      "stars": 42298,
      "forks": 7669,
      "language": "Swift",
      "topics": [
        "alamofire",
        "carthage",
        "certificate-pinning",
        "cocoapods",
        "httpurlresponse",
        "networking",
        "parameter-encoding",
        "public-key-pinning",
        "request",
        "response",
        "swift",
        "swift-package-manager",
        "urlrequest",
        "urlsession",
        "xcode"
      ],
      "created_at": "2014-07-31T05:56:19Z",
      "updated_at": "2026-01-14T19:43:29Z",
      "pushed_at": "2025-12-20T07:34:46Z",
      "open_issues": 39,
      "owner": {
        "login": "Alamofire",
        "avatar_url": "https://avatars.githubusercontent.com/u/7774181?v=4"
      },
      "readme": "![Alamofire: Elegant Networking in Swift](https://raw.githubusercontent.com/Alamofire/Alamofire/master/Resources/AlamofireLogo.png)\n\n[![Swift](https://img.shields.io/badge/Swift-6.0_6.1_6.2-orange?style=flat-square)](https://img.shields.io/badge/Swift-6.0_6.1_6.2-Orange?style=flat-square)\n[![Platforms](https://img.shields.io/badge/Platforms-macOS_iOS_tvOS_watchOS_visionOS_Linux_Windows_Android-yellowgreen?style=flat-square)](https://img.shields.io/badge/Platforms-macOS_iOS_tvOS_watchOS_vision_OS_Linux_Windows_Android-Green?style=flat-square)\n[![CocoaPods Compatible](https://img.shields.io/cocoapods/v/Alamofire.svg?style=flat-square)](https://img.shields.io/cocoapods/v/Alamofire.svg)\n[![Carthage Compatible](https://img.shields.io/badge/Carthage-compatible-4BC51D.svg?style=flat-square)](https://github.com/Carthage/Carthage)\n[![Swift Package Manager](https://img.shields.io/badge/Swift_Package_Manager-compatible-orange?style=flat-square)](https://img.shields.io/badge/Swift_Package_Manager-compatible-orange?style=flat-square)\n[![Swift Forums](https://img.shields.io/badge/Swift_Forums-Alamofire-orange?style=flat-square)](https://forums.swift.org/c/related-projects/alamofire/37)\n\nAlamofire is an HTTP networking library written in Swift.\n\n- [Features](#features)\n- [Component Libraries](#component-libraries)\n- [Requirements](#requirements)\n- [Migration Guides](#migration-guides)\n- [Communication](#communication)\n- [Installation](#installation)\n- [Contributing](#contributing)\n- [Usage](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#using-alamofire)\n  - [**Introduction -**](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#introduction) [Making Requests](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#making-requests), [Response Handling](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#response-handling), [Response Validation](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#response-validation), [Response Caching](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#response-caching)\n  - **HTTP -** [HTTP Methods](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#http-methods), [Parameters and Parameter Encoder](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md##request-parameters-and-parameter-encoders), [HTTP Headers](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#http-headers), [Authentication](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#authentication)\n  - **Large Data -** [Downloading Data to a File](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#downloading-data-to-a-file), [Uploading Data to a Server](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#uploading-data-to-a-server)\n  - **Tools -** [Statistical Metrics](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#statistical-metrics), [cURL Command Output](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Usage.md#curl-command-output)\n- [Advanced Usage](https://github.com/Alamofire/Alamofire/blob/master/Documentation/AdvancedUsage.md)\n  - **URL Session -** [Session Manager](https://github.com/Alamofire/Alamofire/blob/master/Documentation/AdvancedUsage.md#session), [Session Delegate](https://github.com/Alamofire/Alamofire/blob/master/Documentation/AdvancedUsage.md#sessiondelegate), [Request](https://github.com/Alamofire/Alamofire/blob/master/Documentation/AdvancedUsage.md#request)\n  - **Routing -** [Routing Requests](https://github.com/Alamofire/Alamofire/blob/master/Documentation/AdvancedUsage.md#routing-requests), [Adapting and Retrying Requests](https://github.com/Alamofire/Alamofire/blob/master/Documentation/AdvancedUsage.md#adapting-and-retrying-requests-with-requestinterceptor)\n  - **Model Objects -** [Custom Response Handlers](https://github.com/Alamofire/Alamofire/blob/master/Documentation/AdvancedUsage.md#customizing-response-handlers)\n  - **Advanced Concurrency -** [Swift Concurrency](https://github.com/Alamofire/Alamofire/blob/master/Documentation/AdvancedUsage.md#using-alamofire-with-swift-concurrency) and [Combine](https://github.com/Alamofire/Alamofire/blob/master/Documentation/AdvancedUsage.md#using-alamofire-with-combine)\n  - **Connection -** [Security](https://github.com/Alamofire/Alamofire/blob/master/Documentation/AdvancedUsage.md#security), [Network Reachability](https://github.com/Alamofire/Alamofire/blob/master/Documentation/AdvancedUsage.md#network-reachability)\n- [Open Radars](#open-radars)\n- [FAQ](#faq)\n- [Credits](#credits)\n- [Donations](#donations)\n- [License](#license)\n\n## Features\n\n- [x] Chainable Request / Response Methods\n- [x] Swift Concurrency Support Back to iOS 13, macOS 10.15, tvOS 13, and watchOS 6.\n- [x] Combine Support\n- [x] URL / JSON Parameter Encoding\n- [x] Upload File / Data / Stream / MultipartFormData\n- [x] Download File using Request or Resume Data\n- [x] Authentication with `URLCredential`\n- [x] HTTP Response Validation\n- [x] Upload and Download Progress Closures with Progress\n- [x] cURL Command Output\n- [x] Dynamically Adapt and Retry Requests\n- [x] TLS Certificate and Public Key Pinning\n- [x] Network Reachability\n- [x] Comprehensive Unit and Integration Test Coverage\n- [x] [Complete Documentation](https://alamofire.github.io/Alamofire)\n\n## Write Requests Fast!\n\nAlamofire's compact syntax and extensive feature set allow requests with powerful features like automatic retry to be written in just a few lines of code.\n\n```swift\n// Automatic String to URL conversion, Swift concurrency support, and automatic retry.\nlet response = await AF.request(\"https://httpbin.org/get\", interceptor: .retryPolicy)\n                       // Automatic HTTP Basic Auth.\n                       .authenticate(username: \"user\", password: \"pass\")\n                       // Caching customization.\n                       .cacheResponse(using: .cache)\n                       // Redirect customization.\n                       .redirect(using: .follow)\n                       // Validate response code and Content-Type.\n                       .validate()\n                       // Produce a cURL command for the request.\n                       .cURLDescription { description in\n                         print(description)\n                       }\n                       // Automatic Decodable support with background parsing.\n                       .serializingDecodable(DecodableType.self)\n                       // Await the full response with metrics and a parsed body.\n                       .response\n// Detailed response description for easy debugging.\ndebugPrint(response)\n```\n\n## Component Libraries\n\nIn order to keep Alamofire focused specifically on core networking implementations, additional component libraries have been created by the [Alamofire Software Foundation](https://github.com/Alamofire/Foundation) to bring additional functionality to the Alamofire ecosystem.\n\n- [AlamofireImage](https://github.com/Alamofire/AlamofireImage) - An image library including image response serializers, `UIImage` and `UIImageView` extensions, custom image filters, an auto-purging in-memory cache, and a priority-based image downloading system.\n- [AlamofireNetworkActivityIndicator](https://github.com/Alamofire/AlamofireNetworkActivityIndicator) - Controls the visibility of the network activity indicator on iOS using Alamofire. It contains configurable delay timers to help mitigate flicker and can support `URLSession` instances not managed by Alamofire.\n\n## Requirements\n\n| Platform                                             | Minimum Swift Version | Installation                                                                                                         | Status                   |\n| ---------------------------------------------------- | --------------------- | -------------------------------------------------------------------------------------------------------------------- | ------------------------ |\n| iOS 10.0+ / macOS 10.12+ / tvOS 10.0+ / watchOS 3.0+ | 6.0 / Xcode 16.0      | [CocoaPods](#cocoapods), [Carthage](#carthage), [Swift Package Manager](#swift-package-manager), [Manual](#manually) | Fully Tested             |\n| Linux                                                | Latest Only           | [Swift Package Manager](#swift-package-manager)                                                                      | Building But Unsupported |\n| Windows                                              | Latest Only           | [Swift Package Manager](#swift-package-manager)                                                                      | Building But Unsupported |\n| Android                                              | Latest Only           | [Swift Package Manager](#swift-package-manager)                                                                      | Building But Unsupported |\n\n#### Known Issues on Linux and Windows\n\nAlamofire builds on Linux, Windows, and Android but there are missing features and many issues in the underlying `swift-corelibs-foundation` that prevent full functionality and may cause crashes. These include:\n\n- `ServerTrustManager` and associated certificate functionality is unavailable, so there is no certificate pinning and no client certificate support.\n- Various methods of HTTP authentication may crash, including HTTP Basic and HTTP Digest. Crashes may occur if responses contain server challenges.\n- Cache control through `CachedResponseHandler` and associated APIs is unavailable, as the underlying delegate methods aren't called.\n- `URLSessionTaskMetrics` are never gathered.\n- `WebSocketRequest` is not available.\n\nDue to these issues, Alamofire is unsupported on Linux, Windows, and Android. Please report any crashes to the [Swift bug reporter](https://bugs.swift.org).\n\n## Migration Guides\n\n- [Alamofire 5.0 Migration Guide](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Alamofire%205.0%20Migration%20Guide.md)\n- [Alamofire 4.0 Migration Guide](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Alamofire%204.0%20Migration%20Guide.md)\n- [Alamofire 3.0 Migration Guide](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Alamofire%203.0%20Migration%20Guide.md)\n- [Alamofire 2.0 Migration Guide](https://github.com/Alamofire/Alamofire/blob/master/Documentation/Alamofire%202.0%20Migration%20Guide.md)\n\n## Communication\n\n- If you **need help with making network requests** using Alamofire, use [Stack Overflow](https://stackoverflow.com/questions/tagged/alamofire) and tag `alamofire`.\n- If you need to **find or understand an API**, check [our documentation](http://alamofire.github.io/Alamofire/) or [Apple's documentation for `URLSession`](https://developer.apple.com/documentation/foundation/url_loading_system), on top of which Alamofire is built.\n- If you need **help with an Alamofire feature**, use [our forum on swift.org](https://forums.swift.org/c/related-projects/alamofire).\n- If you'd like to **discuss Alamofire best practices**, use [our forum on swift.org](https://forums.swift.org/c/related-projects/alamofire).\n- If you'd like to **discuss a feature request**, use [our forum on swift.org](https://forums.swift.org/c/related-projects/alamofire).\n- If you **found a bug**, open an issue here on GitHub and follow the guide. The more detail the better!\n\n## Installation\n\n### Swift Package Manager\n\nThe [Swift Package Manager](https://swift.org/package-manager/) is a tool for automating the distribution of Swift code and is integrated into the `swift` compiler.\n\nOnce you have your Swift package set up, adding Alamofire as a dependency is as easy as adding it to the `dependencies` value of your `Package.swift` or the Package list in Xcode.\n\n```swift\ndependencies: [\n    .package(url: \"https://github.com/Alamofire/Alamofire.git\", .upToNextMajor(from: \"5.11.0\"))\n]\n```\n\nNormally you'll want to depend on the `Alamofire` target:\n\n```swift\n.product(name: \"Alamofire\", package: \"Alamofire\")\n```\n\nBut if you want to force Alamofire to be dynamically linked (do not do this unless you're sure you need it), you can depend on the `AlamofireDynamic` target:\n\n```swift\n.product(name: \"AlamofireDynamic\", package: \"Alamofire\")\n```\n\n### CocoaPods\n\n[CocoaPods](https://cocoapods.org) is a dependency manager for Cocoa projects. For usage and installation instructions, visit their website. To integrate Alamofire into your Xcode project using CocoaPods, specify it in your `Podfile`:\n\n```ruby\npod 'Alamofire'\n```\n\n### Carthage\n\n[Carthage](https://github.com/Carthage/Carthage) is a decentralized dependency manager that builds your dependencies and provides you with binary frameworks. To integrate Alamofire into your Xcode project using Carthage, specify it in your `Cartfile`:\n\n```ogdl\ngithub \"Alamofire/Alamofire\"\n```\n\n### Manually\n\nIf you prefer not to use any of the aforementioned dependency managers, you can integrate Alamofire into your project manually.\n\n#### Embedded Framework\n\n- Open up Terminal, `cd` into your top-level project directory, and run the following command \"if\" your project is not initialized as a git repository:\n\n  ```bash\n  $ git init\n  ```\n\n- Add Alamofire as a git [submodule](https://git-scm.com/docs/git-submodule) by running the following command:\n\n  ```bash\n  $ git submodule add https://github.com/Alamofire/Alamofire.git\n  ```\n\n- Open the new `Alamofire` folder, and drag the `Alamofire.xcodeproj` into the Project Navigator of your application's Xcode project.\n\n  > It should appear nested underneath your application's blue project icon. Whether it is above or below all the other Xcode groups does not matter.\n\n- Select the `Alamofire.xcodeproj` in the Project Navigator and verify the deployment target matches that of your application target.\n- Next, select your application project in the Project Navigator (blue project icon) to navigate to the target configuration window and select the application target under the \"Targets\" heading in the sidebar.\n- In the tab bar at the top of that window, open the \"General\" panel.\n- Click on the `+` button under the \"Embedded Binaries\" section.\n- You will see two different `Alamofire.xcodeproj` folders each with two different versions of the `Alamofire.framework` nested inside a `Products` folder.\n\n  > It does not matter which `Products` folder you choose from, but it does matter whether you choose the top or bottom `Alamofire.framework`.\n\n- Select the top `Alamofire.framework` for iOS and the bottom one for macOS.\n\n  > You can verify which one you selected by inspecting the build log for your project. The build target for `Alamofire` will be listed as `Alamofire iOS`, `Alamofire macOS`, `Alamofire tvOS`, or `Alamofire watchOS`.\n\n- And that's it!\n\n  > The `Alamofire.framework` is automagically added as a target dependency, linked framework and embedded framework in a copy files build phase which is all you need to build on the simulator and a device.\n\n## Contributing\n\nBefore contributing to Alamofire, please read the instructions detailed in our [contribution guide](https://github.com/Alamofire/Alamofire/blob/master/CONTRIBUTING.md).\n\n## Open Radars\n\nThe following radars have some effect on the current implementation of Alamofire.\n\n- [`rdar://21349340`](http://www.openradar.me/radar?id=5517037090635776) - Compiler throwing warning due to toll-free bridging issue in the test case\n- `rdar://26870455` - Background URL Session Configurations do not work in the simulator\n- `rdar://26849668` - Some URLProtocol APIs do not properly handle `URLRequest`\n\n## Resolved Radars\n\nThe following radars have been resolved over time after being filed against the Alamofire project.\n\n- [`rdar://26761490`](http://www.openradar.me/radar?id=5010235949318144) - Swift string interpolation causing memory leak with common usage.\n  - (Resolved): 9/1/17 in Xcode 9 beta 6.\n- [`rdar://36082113`](http://openradar.appspot.com/radar?id=4942308441063424) - `URLSessionTaskMetrics` failing to link on watchOS 3.0+\n  - (Resolved): Just add `CFNetwork` to your linked frameworks.\n- `FB7624529` - `urlSession(_:task:didFinishCollecting:)` never called on watchOS\n  - (Resolved): Metrics now collected on watchOS 7+.\n\n## FAQ\n\n### What's the origin of the name Alamofire?\n\nAlamofire is named after the [Alamo Fire flower](https://aggie-horticulture.tamu.edu/wildseed/alamofire.html), a hybrid variant of the Bluebonnet, the official state flower of Texas.\n\n## Credits\n\nAlamofire is owned and maintained by the [Alamofire Software Foundation](http://alamofire.org). You can follow them on Twitter at [@AlamofireSF](https://twitter.com/AlamofireSF) for project updates and releases.\n\n### Security Disclosure\n\nIf you believe you have identified a security vulnerability with Alamofire, you should report it as soon as possible via email to security@alamofire.org. Please do not post it to a public issue tracker.\n\n## Sponsorship\n\nThe [ASF](https://github.com/Alamofire/Foundation#members) is looking to raise money to officially stay registered as a federal non-profit organization.\nRegistering will allow Foundation members to gain some legal protections and also allow us to put donations to use, tax-free.\nSponsoring the ASF will enable us to:\n\n- Pay our yearly legal fees to keep the non-profit in good status\n- Pay for our mail servers to help us stay on top of all questions and security issues\n- Potentially fund test servers to make it easier for us to test the edge cases\n- Potentially fund developers to work on one of our projects full-time\n\nThe community adoption of the ASF libraries has been amazing.\nWe are greatly humbled by your enthusiasm around the projects and want to continue to do everything we can to move the needle forward.\nWith your continued support, the ASF will be able to improve its reach and also provide better legal safety for the core members.\nIf you use any of our libraries for work, see if your employers would be interested in donating.\nAny amount you can donate, whether once or monthly, to help us reach our goal would be greatly appreciated.\n\n[Sponsor Alamofire](https://github.com/sponsors/Alamofire)\n\n## Supporters\n\n[MacStadium](https://macstadium.com) provides Alamofire with a free, hosted Mac mini.\n\n![Powered by MacStadium](https://raw.githubusercontent.com/Alamofire/Alamofire/master/Resources/MacStadiumLogo.png)\n\n## License\n\nAlamofire is released under the MIT license. [See LICENSE](https://github.com/Alamofire/Alamofire/blob/master/LICENSE) for details.\n",
      "stars_today": 2
    },
    {
      "id": 7587038,
      "name": "canal",
      "full_name": "alibaba/canal",
      "description": "é˜¿é‡Œå·´å·´ MySQL binlog å¢é‡è®¢é˜…&æ¶ˆè´¹ç»„ä»¶ ",
      "html_url": "https://github.com/alibaba/canal",
      "stars": 29573,
      "forks": 7676,
      "language": "Java",
      "topics": [],
      "created_at": "2013-01-13T10:59:52Z",
      "updated_at": "2026-01-14T22:10:35Z",
      "pushed_at": "2025-12-25T03:54:28Z",
      "open_issues": 1270,
      "owner": {
        "login": "alibaba",
        "avatar_url": "https://avatars.githubusercontent.com/u/1961952?v=4"
      },
      "readme": "[![build status](https://travis-ci.com/alibaba/canal.svg?branch=master)](https://travis-ci.com/alibaba/canal)\n[![codecov](https://codecov.io/gh/alibaba/canal/branch/master/graph/badge.svg)](https://codecov.io/gh/alibaba/canal)\n![maven](https://img.shields.io/maven-central/v/com.alibaba.otter/canal.svg)\n![license](https://img.shields.io/github/license/alibaba/canal.svg)\n[![average time to resolve an issue](http://isitmaintained.com/badge/resolution/alibaba/canal.svg)](http://isitmaintained.com/project/alibaba/canal \"average time to resolve an issue\")\n[![percentage of issues still open](http://isitmaintained.com/badge/open/alibaba/canal.svg)](http://isitmaintained.com/project/alibaba/canal \"percentage of issues still open\")\n[![Leaderboard](https://img.shields.io/badge/Canal-%E6%9F%A5%E7%9C%8B%E8%B4%A1%E7%8C%AE%E6%8E%92%E8%A1%8C%E6%A6%9C-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=canal)\n\n\n## ç®€ä»‹\n\n![](https://img-blog.csdnimg.cn/20191104101735947.png)\n\n**canal [kÉ™'nÃ¦l]**ï¼Œè¯‘æ„ä¸ºæ°´é“/ç®¡é“/æ²Ÿæ¸ ï¼Œä¸»è¦ç”¨é€”æ˜¯åŸºäº MySQL æ•°æ®åº“å¢é‡æ—¥å¿—è§£æï¼Œæä¾›å¢é‡æ•°æ®è®¢é˜…å’Œæ¶ˆè´¹\n\næ—©æœŸé˜¿é‡Œå·´å·´å› ä¸ºæ­å·å’Œç¾å›½åŒæœºæˆ¿éƒ¨ç½²ï¼Œå­˜åœ¨è·¨æœºæˆ¿åŒæ­¥çš„ä¸šåŠ¡éœ€æ±‚ï¼Œå®ç°æ–¹å¼ä¸»è¦æ˜¯åŸºäºä¸šåŠ¡ trigger è·å–å¢é‡å˜æ›´ã€‚ä» 2010 å¹´å¼€å§‹ï¼Œä¸šåŠ¡é€æ­¥å°è¯•æ•°æ®åº“æ—¥å¿—è§£æè·å–å¢é‡å˜æ›´è¿›è¡ŒåŒæ­¥ï¼Œç”±æ­¤è¡ç”Ÿå‡ºäº†å¤§é‡çš„æ•°æ®åº“å¢é‡è®¢é˜…å’Œæ¶ˆè´¹ä¸šåŠ¡ã€‚\n\nåŸºäºæ—¥å¿—å¢é‡è®¢é˜…å’Œæ¶ˆè´¹çš„ä¸šåŠ¡åŒ…æ‹¬\n- æ•°æ®åº“é•œåƒ\n- æ•°æ®åº“å®æ—¶å¤‡ä»½\n- ç´¢å¼•æ„å»ºå’Œå®æ—¶ç»´æŠ¤(æ‹†åˆ†å¼‚æ„ç´¢å¼•ã€å€’æ’ç´¢å¼•ç­‰)\n- ä¸šåŠ¡ cache åˆ·æ–°\n- å¸¦ä¸šåŠ¡é€»è¾‘çš„å¢é‡æ•°æ®å¤„ç†\n\nå½“å‰çš„ canal æ”¯æŒæºç«¯ MySQL ç‰ˆæœ¬åŒ…æ‹¬ 5.1.x , 5.5.x , 5.6.x , 5.7.x , 8.0.x\n\n## å·¥ä½œåŸç†\n\n#### MySQLä¸»å¤‡å¤åˆ¶åŸç†\n![](http://dl.iteye.com/upload/attachment/0080/3086/468c1a14-e7ad-3290-9d3d-44ac501a7227.jpg)\n\n- MySQL master å°†æ•°æ®å˜æ›´å†™å…¥äºŒè¿›åˆ¶æ—¥å¿—( binary log, å…¶ä¸­è®°å½•å«åšäºŒè¿›åˆ¶æ—¥å¿—äº‹ä»¶binary log eventsï¼Œå¯ä»¥é€šè¿‡ show binlog events è¿›è¡ŒæŸ¥çœ‹)\n- MySQL slave å°† master çš„ binary log events æ‹·è´åˆ°å®ƒçš„ä¸­ç»§æ—¥å¿—(relay log)\n- MySQL slave é‡æ”¾ relay log ä¸­äº‹ä»¶ï¼Œå°†æ•°æ®å˜æ›´åæ˜ å®ƒè‡ªå·±çš„æ•°æ®\n\n#### canal å·¥ä½œåŸç†\n\n- canal æ¨¡æ‹Ÿ MySQL slave çš„äº¤äº’åè®®ï¼Œä¼ªè£…è‡ªå·±ä¸º MySQL slave ï¼Œå‘ MySQL master å‘é€dump åè®®\n- MySQL master æ”¶åˆ° dump è¯·æ±‚ï¼Œå¼€å§‹æ¨é€ binary log ç»™ slave (å³ canal )\n- canal è§£æ binary log å¯¹è±¡(åŸå§‹ä¸º byte æµ)\n\n## é‡è¦ç‰ˆæœ¬æ›´æ–°è¯´æ˜\n\n1. canal 1.1.x ç‰ˆæœ¬ï¼ˆ[release_note](https://github.com/alibaba/canal/releases)ï¼‰,æ€§èƒ½ä¸åŠŸèƒ½å±‚é¢æœ‰è¾ƒå¤§çš„çªç ´,é‡è¦æå‡åŒ…æ‹¬:\n\n- æ•´ä½“æ€§èƒ½æµ‹è¯•&ä¼˜åŒ–,æå‡äº†150%. #726 å‚è€ƒ: [Performance](https://github.com/alibaba/canal/wiki/Performance)\n- åŸç”Ÿæ”¯æŒprometheusç›‘æ§ #765 [Prometheus QuickStart](https://github.com/alibaba/canal/wiki/Prometheus-QuickStart)\n- åŸç”Ÿæ”¯æŒkafkaæ¶ˆæ¯æŠ•é€’ #695 [Canal Kafka/RocketMQ QuickStart](https://github.com/alibaba/canal/wiki/Canal-Kafka-RocketMQ-QuickStart)\n- åŸç”Ÿæ”¯æŒaliyun rdsçš„binlogè®¢é˜… (è§£å†³è‡ªåŠ¨ä¸»å¤‡åˆ‡æ¢/oss binlogç¦»çº¿è§£æ) å‚è€ƒ: [Aliyun RDS QuickStart](https://github.com/alibaba/canal/wiki/aliyun-RDS-QuickStart)\n- åŸç”Ÿæ”¯æŒdockeré•œåƒ #801 å‚è€ƒ: [Docker QuickStart](https://github.com/alibaba/canal/wiki/Docker-QuickStart)\n\n2.  canal 1.1.4ç‰ˆæœ¬ï¼Œè¿æ¥æœ€é‡è¦çš„WebUIèƒ½åŠ›ï¼Œå¼•å…¥canal-adminå·¥ç¨‹ï¼Œæ”¯æŒé¢å‘WebUIçš„canalåŠ¨æ€ç®¡ç†èƒ½åŠ›ï¼Œæ”¯æŒé…ç½®ã€ä»»åŠ¡ã€æ—¥å¿—ç­‰åœ¨çº¿ç™½å±è¿ç»´èƒ½åŠ›ï¼Œå…·ä½“æ–‡æ¡£ï¼š[Canal Admin Guide](https://github.com/alibaba/canal/wiki/Canal-Admin-Guide)\n\n## æ–‡æ¡£\n\n- [Home](https://github.com/alibaba/canal/wiki/Home)\n- [Introduction](https://github.com/alibaba/canal/wiki/Introduction)\n- [QuickStart](https://github.com/alibaba/canal/wiki/QuickStart)\n  - [Docker QuickStart](https://github.com/alibaba/canal/wiki/Docker-QuickStart)\n  - [Canal Kafka/RocketMQ QuickStart](https://github.com/alibaba/canal/wiki/Canal-Kafka-RocketMQ-QuickStart\")\n  - [Aliyun RDS for MySQL QuickStart](https://github.com/alibaba/canal/wiki/aliyun-RDS-QuickStart)\n  - [Prometheus QuickStart](https://github.com/alibaba/canal/wiki/Prometheus-QuickStart)\n- Canal Admin\n  - [Canal Admin QuickStart](https://github.com/alibaba/canal/wiki/Canal-Admin-QuickStart)\n  - [Canal Admin Guide](https://github.com/alibaba/canal/wiki/Canal-Admin-Guide)\n  - [Canal Admin ServerGuide](https://github.com/alibaba/canal/wiki/Canal-Admin-ServerGuide)\n  - [Canal Admin Docker](https://github.com/alibaba/canal/wiki/Canal-Admin-Docker)\n- [AdminGuide](https://github.com/alibaba/canal/wiki/AdminGuide)\n- [ClientExample](https://github.com/alibaba/canal/wiki/ClientExample)\n- [ClientAPI](https://github.com/alibaba/canal/wiki/ClientAPI)\n- [Performance](https://github.com/alibaba/canal/wiki/Performance)\n- [DevGuide](https://github.com/alibaba/canal/wiki/DevGuide)\n- [BinlogChange(MySQL 5.6)](https://github.com/alibaba/canal/wiki/BinlogChange%28mysql5.6%29)\n- [BinlogChange(MariaDB)](https://github.com/alibaba/canal/wiki/BinlogChange%28MariaDB%29)\n- [TableMetaTSDB](https://github.com/alibaba/canal/wiki/TableMetaTSDB)\n- [ReleaseNotes](http://alibaba.github.com/canal/release.html)\n- [Download](https://github.com/alibaba/canal/releases)\n- [FAQ](https://github.com/alibaba/canal/wiki/FAQ)\n\n## å¤šè¯­è¨€\n\ncanal ç‰¹åˆ«è®¾è®¡äº† client-server æ¨¡å¼ï¼Œäº¤äº’åè®®ä½¿ç”¨ protobuf 3.0 , client ç«¯å¯é‡‡ç”¨ä¸åŒè¯­è¨€å®ç°ä¸åŒçš„æ¶ˆè´¹é€»è¾‘ï¼Œæ¬¢è¿å¤§å®¶æäº¤ pull request \n  \n- canal java å®¢æˆ·ç«¯: [https://github.com/alibaba/canal/wiki/ClientExample](https://github.com/alibaba/canal/wiki/ClientExample)\n- canal c# å®¢æˆ·ç«¯: [https://github.com/dotnetcore/CanalSharp](https://github.com/dotnetcore/CanalSharp)\n- canal goå®¢æˆ·ç«¯: [https://github.com/CanalClient/canal-go](https://github.com/CanalClient/canal-go)\n- canal phpå®¢æˆ·ç«¯: [https://github.com/xingwenge/canal-php](https://github.com/xingwenge/canal-php)\n- canal Pythonå®¢æˆ·ç«¯ï¼š[https://github.com/haozi3156666/canal-python](https://github.com/haozi3156666/canal-python)\n- canal Rustå®¢æˆ·ç«¯ï¼š[https://github.com/laohanlinux/canal-rs](https://github.com/laohanlinux/canal-rs)\n- canal Nodejså®¢æˆ·ç«¯ï¼š[https://github.com/marmot-z/canal-nodejs](https://github.com/marmot-z/canal-nodejs)\n\ncanal ä½œä¸º MySQL binlog å¢é‡è·å–å’Œè§£æå·¥å…·ï¼Œå¯å°†å˜æ›´è®°å½•æŠ•é€’åˆ° MQ ç³»ç»Ÿä¸­ï¼Œæ¯”å¦‚ Kafka/RocketMQï¼Œå¯ä»¥å€ŸåŠ©äº MQ çš„å¤šè¯­è¨€èƒ½åŠ› \n\n- å‚è€ƒæ–‡æ¡£: [Canal Kafka/RocketMQ QuickStart](https://github.com/alibaba/canal/wiki/Canal-Kafka-RocketMQ-QuickStart)\n\n## åŸºäºcanalå¼€å‘çš„å·¥å…·\n\n-  canal2sql(åŸºäºbinlogç”ŸæˆSQL) : [https://github.com/zhuchao941/canal2sql]\n\n## ç›¸å…³å¼€æº&äº§å“\n\n- [canal æ¶ˆè´¹ç«¯å¼€æºé¡¹ç›®: Otter](http://github.com/alibaba/otter)\n- [é˜¿é‡Œå·´å·´å» Oracle æ•°æ®è¿ç§»åŒæ­¥å·¥å…·: yugong](http://github.com/alibaba/yugong)\n- [é˜¿é‡Œå·´å·´ç¦»çº¿åŒæ­¥å¼€æºé¡¹ç›® DataX](https://github.com/alibaba/datax)\n- [é˜¿é‡Œå·´å·´æ•°æ®åº“è¿æ¥æ± å¼€æºé¡¹ç›® Druid](https://github.com/alibaba/druid)\n- [é˜¿é‡Œå·´å·´å®æ—¶æ•°æ®åŒæ­¥å·¥å…· DTS](https://www.aliyun.com/product/dts)\n\n## é—®é¢˜åé¦ˆ\n- æŠ¥å‘Š issue: [github issues](https://github.com/alibaba/canal/issues)\n\næœ¬é¡¹ç›®çš„Issuesä¼šè¢«åŒæ­¥æ²‰æ·€è‡³[é˜¿é‡Œäº‘å¼€å‘è€…ç¤¾åŒº](https://developer.aliyun.com/ask)\n",
      "stars_today": 2
    },
    {
      "id": 23418517,
      "name": "hadoop",
      "full_name": "apache/hadoop",
      "description": "Apache Hadoop",
      "html_url": "https://github.com/apache/hadoop",
      "stars": 15449,
      "forks": 9183,
      "language": "Java",
      "topics": [
        "hadoop"
      ],
      "created_at": "2014-08-28T07:00:08Z",
      "updated_at": "2026-01-15T00:53:21Z",
      "pushed_at": "2026-01-13T19:43:20Z",
      "open_issues": 91,
      "owner": {
        "login": "apache",
        "avatar_url": "https://avatars.githubusercontent.com/u/47359?v=4"
      },
      "readme": "For the latest information about Hadoop, please visit our website at:\n\n   http://hadoop.apache.org/\n\nand our wiki, at:\n\n   https://cwiki.apache.org/confluence/display/HADOOP/\n",
      "stars_today": 2
    },
    {
      "id": 1335132,
      "name": "ninja",
      "full_name": "ninja-build/ninja",
      "description": "a small build system with a focus on speed",
      "html_url": "https://github.com/ninja-build/ninja",
      "stars": 12609,
      "forks": 1763,
      "language": "C++",
      "topics": [],
      "created_at": "2011-02-06T19:07:12Z",
      "updated_at": "2026-01-14T23:25:01Z",
      "pushed_at": "2026-01-14T00:16:20Z",
      "open_issues": 392,
      "owner": {
        "login": "ninja-build",
        "avatar_url": "https://avatars.githubusercontent.com/u/11653218?v=4"
      },
      "readme": "# Ninja\n\nNinja is a small build system with a focus on speed.\nhttps://ninja-build.org/\n\nSee [the manual](https://ninja-build.org/manual.html) or\n`doc/manual.asciidoc` included in the distribution for background\nand more details.\n\nBinaries for Linux, Mac and Windows are available on\n  [GitHub](https://github.com/ninja-build/ninja/releases).\nRun `./ninja -h` for Ninja help.\n\nInstallation is not necessary because the only required file is the\nresulting ninja binary. However, to enable features like Bash\ncompletion and Emacs and Vim editing modes, some files in misc/ must be\ncopied to appropriate locations.\n\nIf you're interested in making changes to Ninja, read\n[CONTRIBUTING.md](CONTRIBUTING.md) first.\n\n## Building Ninja itself\n\nYou can either build Ninja via the custom generator script written in Python or\nvia CMake. For more details see\n[the wiki](https://github.com/ninja-build/ninja/wiki).\n\n### Python\n\n```\n./configure.py --bootstrap\n```\n\nThis will generate the `ninja` binary and a `build.ninja` file you can now use\nto build Ninja with itself.\n\nIf you have a GoogleTest source directory, you can build the tests\nby passing its path with `--gtest-source-dir=PATH` option, or the\n`GTEST_SOURCE_DIR` environment variable, e.g.:\n\n```\n./configure.py --bootstrap --gtest-source-dir=/path/to/googletest\n./ninja all     # build ninja_test and other auxiliary binaries\n./ninja_test`   # run the unit-test suite.\n```\n\nUse the CMake build below if you want to use a preinstalled binary\nversion of the library.\n\n### CMake\n\nTo build the ninja binary without building the unit tests, disable test building by setting `BUILD_TESTING` to `OFF`:\n\n```\ncmake -Bbuild-cmake -DBUILD_TESTING=OFF\ncmake --build build-cmake\n```\n\nThe `ninja` binary will now be inside the `build-cmake` directory (you can\nchoose any other name you like).\n\nTo run the unit tests, omit the `-DBUILD_TESTING=OFF` option, and after building, run:\n\n```\n./build-cmake/ninja_test\n```\n\n## Generating documentation\n\n### Ninja Manual\n\nYou must have `asciidoc` and `xsltproc` in your PATH, then do:\n\n```\n./configure.py\nninja manual doc/manual.html\n```\n\nWhich will generate `doc/manual.html`.\n\nTo generate the PDF version of the manual, you must have `dblatext` in your PATH then do:\n\n```\n./configure.py    # only if you didn't do it previously.\nninja doc/manual.pdf\n```\n\nWhich will generate `doc/manual.pdf`.\n\n### Doxygen documentation\n\nIf you have `doxygen` installed, you can build documentation extracted from C++\ndeclarations and comments to help you navigate the code. Note that Ninja is a standalone\nexecutable, not a library, so there is no public API, all details exposed here are\ninternal.\n\n```\n./configure.py   # if needed\nninja doxygen\n```\n\nThen open `doc/doxygen/html/index.html` in a browser to look at it.\n",
      "stars_today": 2
    },
    {
      "id": 80134675,
      "name": "buildah",
      "full_name": "containers/buildah",
      "description": "A tool that facilitates building OCI images.",
      "html_url": "https://github.com/containers/buildah",
      "stars": 8539,
      "forks": 869,
      "language": "Go",
      "topics": [
        "container",
        "container-image",
        "containers",
        "docker",
        "oci",
        "oci-image",
        "podman"
      ],
      "created_at": "2017-01-26T16:59:13Z",
      "updated_at": "2026-01-15T00:39:50Z",
      "pushed_at": "2026-01-13T20:05:38Z",
      "open_issues": 252,
      "owner": {
        "login": "containers",
        "avatar_url": "https://avatars.githubusercontent.com/u/5874934?v=4"
      },
      "readme": "![buildah logo (light)](logos/buildah-logo_large.png#gh-light-mode-only)\n![buildah logo (dark)](logos/buildah-logo_reverse_large.png#gh-dark-mode-only)\n\n# [Buildah](https://www.youtube.com/embed/YVk5NgSiUw8) - a tool that facilitates building [Open Container Initiative (OCI)](https://www.opencontainers.org/) container images\n\n[![Go Report Card](https://goreportcard.com/badge/github.com/containers/buildah)](https://goreportcard.com/report/github.com/containers/buildah)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/10579/badge)](https://www.bestpractices.dev/projects/10579)\n\n\nThe Buildah package provides a command line tool that can be used to\n* create a working container, either from scratch or using an image as a starting point\n* create an image, either from a working container or via the instructions in a Dockerfile\n* images can be built in either the OCI image format or the traditional upstream docker image format\n* mount a working container's root filesystem for manipulation\n* unmount a working container's root filesystem\n* use the updated contents of a container's root filesystem as a filesystem layer to create a new image\n* delete a working container or an image\n* rename a local container\n\n## Buildah Information for Developers\n\nFor blogs, release announcements and more, please checkout the [buildah.io](https://buildah.io) website!\n\n**[Buildah Container Images](https://github.com/containers/image_build/blob/main/buildah/README.md)**\n\n**[Buildah Demos](demos)**\n\n**[Changelog](CHANGELOG.md)**\n\n**[Contributing](CONTRIBUTING.md)**\n\n**[Development Plan](developmentplan.md)**\n\n**[Installation notes](install.md)**\n\n**[Troubleshooting Guide](troubleshooting.md)**\n\n**[Tutorials](docs/tutorials)**\n\n## Buildah and Podman relationship\n\nBuildah and Podman are two complementary open-source projects that are\navailable on most Linux platforms and both projects reside at\n[GitHub.com](https://github.com) with Buildah\n[here](https://github.com/containers/buildah) and Podman\n[here](https://github.com/containers/podman).  Both, Buildah and Podman are\ncommand line tools that work on Open Container Initiative (OCI) images and\ncontainers.  The two projects differentiate in their specialization.\n\nBuildah specializes in building OCI images.  Buildah's commands replicate all\nof the commands that are found in a Dockerfile.  This allows building images\nwith and without Dockerfiles while not requiring any root privileges.\nBuildahâ€™s ultimate goal is to provide a lower-level coreutils interface to\nbuild images.  The flexibility of building images without Dockerfiles allows\nfor the integration of other scripting languages into the build process.\nBuildah follows a simple fork-exec model and does not run as a daemon\nbut it is based on a comprehensive API in golang, which can be vendored\ninto other tools.\n\nPodman specializes in all of the commands and functions that help you to maintain and modify\nOCI images, such as pulling and tagging.  It also allows you to create, run, and maintain those containers\ncreated from those images.  For building container images via Dockerfiles, Podman uses Buildah's\ngolang API and can be installed independently from Buildah.\n\nA major difference between Podman and Buildah is their concept of a container.  Podman\nallows users to create \"traditional containers\" where the intent of these containers is\nto be long lived.  While Buildah containers are really just created to allow content\nto be added back to the container image.  An easy way to think of it is the\n`buildah run` command emulates the RUN command in a Dockerfile while the `podman run`\ncommand emulates the `docker run` command in functionality.  Because of this and their underlying\nstorage differences, you can not see Podman containers from within Buildah or vice versa.\n\nIn short, Buildah is an efficient way to create OCI images while Podman allows\nyou to manage and maintain those images and containers in a production environment using\nfamiliar container cli commands.  For more details, see the\n[Container Tools Guide](https://github.com/containers/buildah/tree/main/docs/containertools).\n\n## Example\n\nFrom [`./examples/lighttpd.sh`](examples/lighttpd.sh):\n\n```bash\n$ cat > lighttpd.sh <<\"EOF\"\n#!/usr/bin/env bash\n\nset -x\n\nctr1=$(buildah from \"${1:-fedora}\")\n\n## Get all updates and install our minimal httpd server\nbuildah run \"$ctr1\" -- dnf update -y\nbuildah run \"$ctr1\" -- dnf install -y lighttpd\n\n## Include some buildtime annotations\nbuildah config --annotation \"com.example.build.host=$(uname -n)\" \"$ctr1\"\n\n## Run our server and expose the port\nbuildah config --cmd \"/usr/sbin/lighttpd -D -f /etc/lighttpd/lighttpd.conf\" \"$ctr1\"\nbuildah config --port 80 \"$ctr1\"\n\n## Commit this container to an image name\nbuildah commit \"$ctr1\" \"${2:-$USER/lighttpd}\"\nEOF\n\n$ chmod +x lighttpd.sh\n$ ./lighttpd.sh\n```\n\n## Commands\n| Command                                              | Description                                                                                          |\n| ---------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |\n| [buildah-add(1)](/docs/buildah-add.1.md)               | Add the contents of a file, URL, or a directory to the container.                                    |\n| [buildah-build(1)](/docs/buildah-build.1.md)           | Build an image using instructions from Containerfiles or Dockerfiles.                                |\n| [buildah-commit(1)](/docs/buildah-commit.1.md)         | Create an image from a working container.                                                            |\n| [buildah-config(1)](/docs/buildah-config.1.md)         | Update image configuration settings.                                                                 |\n| [buildah-containers(1)](/docs/buildah-containers.1.md) | List the working containers and their base images.                                                   |\n| [buildah-copy(1)](/docs/buildah-copy.1.md)             | Copies the contents of a file, URL, or directory into a container's working directory.               |\n| [buildah-from(1)](/docs/buildah-from.1.md)             | Creates a new working container, either from scratch or using a specified image as a starting point. |\n| [buildah-images(1)](/docs/buildah-images.1.md)         | List images in local storage.                                                                        |\n| [buildah-info(1)](/docs/buildah-info.1.md)             | Display Buildah system information.                                                                  |\n| [buildah-inspect(1)](/docs/buildah-inspect.1.md)       | Inspects the configuration of a container or image.                                                  |\n| [buildah-mount(1)](/docs/buildah-mount.1.md)           | Mount the working container's root filesystem.                                                       |\n| [buildah-pull(1)](/docs/buildah-pull.1.md)             | Pull an image from the specified location.                                                           |\n| [buildah-push(1)](/docs/buildah-push.1.md)             | Push an image from local storage to elsewhere.                                                       |\n| [buildah-rename(1)](/docs/buildah-rename.1.md)         | Rename a local container.                                                                            |\n| [buildah-rm(1)](/docs/buildah-rm.1.md)                 | Removes one or more working containers.                                                              |\n| [buildah-rmi(1)](/docs/buildah-rmi.1.md)               | Removes one or more images.                                                                          |\n| [buildah-run(1)](/docs/buildah-run.1.md)               | Run a command inside of the container.                                                               |\n| [buildah-tag(1)](/docs/buildah-tag.1.md)               | Add an additional name to a local image.                                                             |\n| [buildah-umount(1)](/docs/buildah-umount.1.md)         | Unmount a working container's root file system.                                                      |\n| [buildah-unshare(1)](/docs/buildah-unshare.1.md)       | Launch a command in a user namespace with modified ID mappings.                                      |\n| [buildah-version(1)](/docs/buildah-version.1.md)       | Display the Buildah Version Information                                                              |\n\n**Future goals include:**\n* more CI tests\n* additional CLI commands (?)\n",
      "stars_today": 2
    },
    {
      "id": 3354902,
      "name": "iodine",
      "full_name": "yarrick/iodine",
      "description": "Official git repo for iodine dns tunnel",
      "html_url": "https://github.com/yarrick/iodine",
      "stars": 7564,
      "forks": 572,
      "language": "C",
      "topics": [
        "dns",
        "dns-tunnel",
        "hacktoberfest",
        "iodine",
        "tunnel",
        "vpn"
      ],
      "created_at": "2012-02-04T19:51:39Z",
      "updated_at": "2026-01-14T21:55:53Z",
      "pushed_at": "2025-09-04T19:18:00Z",
      "open_issues": 18,
      "owner": {
        "login": "yarrick",
        "avatar_url": "https://avatars.githubusercontent.com/u/1261015?v=4"
      },
      "readme": "iodine - <https://code.kryo.se/iodine>\n=====================================\n\n\nThis is a piece of software that lets you tunnel IPv4 data through a DNS\nserver. This can be usable in different situations where internet access is\nfirewalled, but DNS queries are allowed.\n\n\nCOMPILING\n---------\n\nIodine has no configure script. There are two optional features for Linux\n(SELinux and systemd support) that will be enabled automatically if the\nrelevant header files are found in `/usr/include`.\n(See script at `./src/osflags`)\n\nRun `make` to compile the server and client binaries.\nRun `make install` to copy binaries and manpage to the destination directory.\nRun `make test` to compile and run the unit tests. (Requires the `check` library)\n\n\nQUICKSTART\n----------\n\nTry it out within your own LAN! Follow these simple steps:\n- On your server, run: `./iodined -f 10.0.0.1 test.com`.\n  If you already use the `10.0.0.0` network, use another internal net like\n  `172.16.0.0`.\n- Enter a password.\n- On the client, run: `./iodine -f -r 192.168.0.1 test.com`.\n  Replace `192.168.0.1` with your server's ip address.\n- Enter the same password.\n- Now the client has the tunnel ip `10.0.0.2` and the server has `10.0.0.1`.\n- Try pinging each other through the tunnel.\n- Done! :)\n\nTo actually use it through a relaying nameserver, see below.\n\n\nHOW TO USE\n----------\n\nNote: server and client are required to speak the exact same protocol. In most\ncases, this means running the same iodine version. Unfortunately, implementing\nbackward and forward protocol compatibility is usually not feasible.\n\n### Server side\nTo use this tunnel, you need control over a real domain (like `mydomain.com`),\nand a server with a public IP address to run `iodined` on. If this server\nalready runs a DNS program, change its listening port and then use `iodined`'s\n`-b` option to let `iodined` forward the DNS requests. (Note that this procedure\nis not advised in production environments, because `iodined`'s DNS forwarding\nis not completely transparent, for example zone transfers will not work.)\nAlternatively you can forward the subdomain from your DNS server to `iodined`\nwhich must then run on a different port (`-p`).\n\nThen, delegate a subdomain (say, `t1.mydomain.com`) to the iodined server.\nIf you use BIND for your domain, add two lines like these to the zone file:\n\n\tt1\t\tIN\tNS\tt1ns.mydomain.com.\t\t; note the dot!\n\tt1ns\t\tIN\tA\t10.15.213.99\n\nThe `NS` line is all that's needed to route queries for the `t1` subdomain\nto the `t1ns` server. We use a short name for the subdomain, to keep as much\nspace as possible available for the data traffic. At the end of the `NS` line\nis the name of your `iodined` server. This can be any name, pointing anywhere,\nbut in this case it's easily kept in the same zone file. It must be a name\n(not an IP address), and that name itself must have an `A` record\n(not a `CNAME`).\n\nIf your `iodined` server has a dynamic IP, use a dynamic DNS provider. Simply\npoint the `NS` line to it, and leave the `A` line out:\n\n\tt1\t\tIN\tNS\tmyname.mydyndnsprovider.com.\t; note the dot!\n\nThen reload or restart your nameserver program. Now any DNS queries for\ndomains ending in `t1.mydomain.com` will be sent to your `iodined` server.\n\nFinally start `iodined` on your server. The first argument is the IP address\ninside the tunnel, which can be from any range that you don't use yet (for\nexample `192.168.99.1`), and the second argument is the assigned domain (in this\ncase `t1.mydomain.com`). Using the `-f` option will keep iodined running in the\nforeground, which helps when testing. iodined will open a virtual interface\n(\"tun device\"), and will also start listening for DNS queries on UDP port 53.\nEither enter a password on the commandline (`-P pass`) or after the server has\nstarted. Now everything is ready for the client.\n\nIf there is a chance you'll be using an iodine tunnel from unexpected\nenvironments, start `iodined` with a `-c` option.\nResulting commandline in this example situation:\n\n\t./iodined -f -c -P secretpassword 192.168.99.1 t1.mydomain.com\n\n### Client side\nAll the setup is done, just start `iodine`. It takes one or two arguments, the\nfirst is the local relaying DNS server (optional) and the second is the domain\nyou used (`t1.mydomain.com`). If you don't specify the first argument, the\nsystem's current DNS setting will be consulted.\n\nIf DNS queries are allowed to any computer, you can directly give the `iodined`\nserver's address as first argument (in the example: `t1ns.mydomain.com` or\n`10.15.213.99`). In that case, it may also happen that _any_ traffic is allowed\nto the DNS port (53 UDP) of any computer. Iodine will detect this, and switch\nto raw UDP tunneling if possible. To force DNS tunneling in any case, use the\n`-r` option (especially useful when testing within your own network).\n\nThe client's tunnel interface will get an IP close to the server's (in this\ncase `192.168.99.2` or `.3` etc.) and a suitable MTU. Enter the same password as\non the server either as commandline option or after the client has started.\nUsing the `-f` option will keep the iodine client running in the foreground.\n\nResulting commandline in this example situation, adding -r forces DNS tunneling\neven if raw UDP tunneling would be possible:\n\n\t./iodine -f -P secretpassword t1.mydomain.com\n\nFrom either side, you should now be able to ping the IP address on the other\nend of the tunnel. In this case, `ping 192.168.99.1` from the iodine client, and\n`192.168.99.2` from the iodine server.\n\n\n### MISC. INFO\n\n#### IPv6\nThe data inside the tunnel is IPv4 only.\n\nThe server listens to both IPv4 and IPv6 for incoming requests by default.\nUse options `-4` or `-6` to only listen on one protocol. Raw mode will be\nattempted on the same protocol as used for the login.\n\nThe client can use IPv4 or IPv6 nameservers to connect to iodined. The relay\nnameservers will translate between protocols automatically if needed. Use\noptions `-4` or `-6` to force the client to use a specific IP version for its DNS\nqueries.\n\nIf your server is listening on IPv6 and is reachable, add an AAAA record for it\nto your DNS setup. Extending the example above would look like this:\n\n\tt1\t\tIN\tNS\tt1ns.mydomain.com.\t\t; note the dot!\n\tt1ns\t\tIN\tA\t10.15.213.99\n\tt1ns\t\tIN\tAAAA\t2001:db8::1001:99\n\n#### Routing\nIt is possible to route all traffic through the DNS tunnel. To do this, first\nadd a host route to the nameserver used by iodine over the wired/wireless\ninterface with the default gateway as gateway. Then replace the default\ngateway with the iodined server's IP address inside the DNS tunnel, and\nconfigure the server to do NAT.\n\nHowever, note that the tunneled data traffic is not encrypted at all, and can\nbe read and changed by external parties relatively easily. For maximum\nsecurity, run a VPN through the DNS tunnel (=double tunneling), or use secure\nshell (SSH) access, possibly with port forwarding. The latter can also be used\nfor web browsing, when you run a web proxy (for example Privoxy) on your\nserver.\n\n#### Testing\nThe `iodined` server replies to `NS` requests sent for subdomains of the tunnel\ndomain. If your iodined subdomain is `t1.mydomain.com`, send a `NS` request for\n`foo123.t1.mydomain.com` to see if the delegation works.\n`dig` is a good tool for this:\n\n\t% dig -t NS foo123.t1.mydomain.com\n\tns.io.citronna.de.\n\nAlso, the iodined server will answer requests starting with 'z' for any of the\nsupported request types, for example:\n\n\tdig -t TXT z456.t1.mydomain.com\n\tdig -t SRV z456.t1.mydomain.com\n\tdig -t CNAME z456.t1.mydomain.com\n\nThe reply should look like garbled text in all these cases.\n\n#### Mac OS X\nOn Mac OS X 10.6 and later, iodine supports the native utun devices built into\nthe OS - use `-d utunX`.\n\n\nOperational info\n----------------\n\nThe DNS-response fragment size is normally autoprobed to get maximum bandwidth.\nTo force a specific value (and speed things up), use the `-m` option.\n\nThe DNS hostnames are normally used up to their maximum length, 255 characters.\nSome DNS relays have been found that answer full-length queries rather\nunreliably, giving widely varying (and mostly very bad) results of the\nfragment size autoprobe on repeated tries. In these cases, use the `-M` switch\nto reduce the DNS hostname length to, for example 200 characters, which makes\nthese DNS relays much more stable. This is also useful on some â€œde-optimizingâ€\nDNS relays that stuff the response with two full copies of the query, leaving\nvery little space for downstream data (also not capable of EDNS0). The `-M`\nswitch can trade some upstream bandwidth for downstream bandwidth. Note that\nthe minimum `-M` value is about 100, since the protocol can split packets (1200\nbytes max) in only 16 fragments, requiring at least 75 real data bytes per\nfragment.\n\nThe upstream data is sent gzipped encoded with Base32; or Base64 if the relay\nserver supports mixed case and `+` in domain names; or Base64u if `_` is\nsupported instead; or Base128 if high-byte-value characters are supported.\nThis upstream encoding is autodetected. The DNS protocol allows one query per\npacket, and one query can be max 256 chars. Each domain name part can be max\n63 chars. So your domain name and subdomain should be as short as possible to\nallow maximum upstream throughput.\n\nSeveral DNS request types are supported, with the `NULL` and `PRIVATE` types\nexpected to provide the largest downstream bandwidth. The `PRIVATE` type uses\nvalue 65399 in the private-use range. Other available types are `TXT`, `SRV`,\n`MX`, `CNAME` and `A` (returning `CNAME`), in decreasing bandwidth order.\nNormally the â€œbestâ€ request type is autodetected and used. However, DNS relays\nmay impose limits on for example NULL and TXT, making SRV or MX actually the best\nchoice. This is not autodetected, but can be forced using the `-T` option.\nIt is advisable to try various alternatives especially when the autodetected\nrequest type provides a downstream fragment size of less than 200 bytes.\n\nNote that `SRV`, `MX` and `A` (returning `CNAME`) queries may/will cause\nadditional lookups by \"smart\" caching nameservers to get an actual IP address,\nwhich may either slow down or fail completely.\n\nDNS responses for non-`NULL/PRIVATE` queries can be encoded with the same set of\ncodecs as upstream data. This is normally also autodetected, but no fully\nexhaustive tests are done, so some problems may not be noticed when selecting\nmore advanced codecs. In that case, you'll see failures/corruption in the\nfragment size autoprobe. In particular, several DNS relays have been found that\nchange replies returning hostnames (`SRV`, `MX`, `CNAME`, `A`) to lowercase only\nwhen that hostname exceeds ca. 180 characters. In these and similar cases, use\nthe `-O` option to try other downstream codecs; Base32 should always work.\n\nNormal operation now is for the server to _not_ answer a DNS request until\nthe next DNS request has come in, a.k.a. being â€œlazyâ€. This way, the server\nwill always have a DNS request handy when new downstream data has to be sent.\nThis greatly improves (interactive) performance and latency, and allows to\nslow down the quiescent ping requests to 4 second intervals by default, and\npossibly much slower. In fact, the main purpose of the pings now is to force\na reply to the previous ping, and prevent DNS server timeouts (usually at\nleast 5-10 seconds per RFC1035). Some DNS servers are more impatient and will\ngive SERVFAIL errors (timeouts) in periods without tunneled data traffic. All\ndata should still get through in these cases, but `iodine` will reduce the ping\ninterval to 1 second anyway (-I1) to reduce the number of error messages. This\nmay not help for very impatient DNS relays like `dnsadvantage.com` (ultradns),\nwhich time out in 1 second or even less. Yet data will still get trough, and\nyou can ignore the `SERVFAIL` errors.\n\nIf you are running on a local network without any DNS server in-between, try\n`-I 50` (iodine and iodined close the connection after 60 seconds of silence).\nThe only time you'll notice a slowdown, is when DNS reply packets go missing;\nthe `iodined` server then has to wait for a new ping to re-send the data. You can\nspeed this up by generating some upstream traffic (keypress, ping). If this\nhappens often, check your network for bottlenecks and/or run with `-I1`.\n\nThe delayed answering in lazy mode will cause some â€œcarrier gradeâ€ commercial\nDNS relays to repeatedly re-send the same DNS query to the iodined server.\nIf the DNS relay is actually implemented as a pool of parallel servers,\nduplicate requests may even arrive from multiple sources. This effect will\nonly be visible in the network traffic at the `iodined` server, and will not\naffect the client's connection. Iodined will notice these duplicates, and send\nthe same answer (when its time has come) to both the original query and the\nlatest duplicate. After that, the full answer is cached for a short while.\nDelayed duplicates that arrive at the server even later, get a reply that the\niodine client will ignore (if it ever arrives there).\n\nIf you have problems, try inspecting the traffic with network monitoring tools\nlike tcpdump or ethereal/wireshark, and make sure that the relaying DNS server\nhas not cached the response. A cached error message could mean that you\nstarted the client before the server. The `-D` (and `-DD`) option on the server\ncan also show received and sent queries.\n\n\nTIPS & TRICKS\n-------------\n\nIf your port 53 is taken on a specific interface by an application that does\nnot use it, use `-p` on iodined to specify an alternate port (like `-p 5353`)\nand use for instance iptables (on Linux) to forward the traffic:\n\n\tiptables -t nat -A PREROUTING -i eth0 -p udp --dport 53 -j DNAT --to :5353\n\n(Sent in by Tom Schouten)\n\nIodined will reject data from clients that have not been active (data/pings)\nfor more than 60 seconds. Similarly, iodine will exit when no downstream\ndata has been received for 60 seconds. In case of a long network outage or\nsimilar, just restart iodine (re-login), possibly multiple times until you get\nyour old IP address back. Once that's done, just wait a while, and you'll\neventually see the tunneled TCP traffic continue to flow from where it left\noff before the outage.\n\nWith the introduction of the downstream packet queue in the server, its memory\nusage has increased with several megabytes in the default configuration.\nFor use in low-memory environments (e.g. running on your DSL router), you can\ndecrease USERS and undefine OUTPACKETQ_LEN in user.h without any ill conse-\nquence, assuming at most one client will be connected at any time. A small\nDNSCACHE_LEN is still advised, preferably 2 or higher, however you can also\nundefine it to save a few more kilobytes.\n\nOne iodine server can handle multiple domains. Set up different NS records\non the same domain all pointing to the same host, and use a wildcard\nat the start of the topdomain argument (example `*.mydomain.com`). iodine\nwill accept tunnel traffic for all domains matching that pattern. The wildcard\nhas to be at the start of the topdomain argument and be followed by a dot.\n\n\nPERFORMANCE\n-----------\n\nThis section tabulates some performance measurements. To view properly, use\na fixed-width font like Courier.\n\nMeasurements were done in protocol 00000502 in lazy mode; upstream encoding\nalways Base128; `iodine -M255`; `iodined -m1130`. Network conditions were not\nextremely favorable; results are not benchmarks but a realistic indication of\nreal-world performance that can be expected in similar situations.\n\nUpstream/downstream throughput was measured by `scp`'ing a file previously\nread from `/dev/urandom` (i.e. incompressible), and measuring size with\n`ls -l ; sleep 30 ; ls -l` on a separate non-tunneled connection. Given the\nlarge `scp` block size of 16 kB, this gives a resolution of 4.3 kbit/s, which\nexplains why some values are exactly equal.\nPing round-trip times measured with `ping -c100`, presented are average rtt\nand mean deviation (indicating spread around the average), in milliseconds.\n\n\n### Situation 1: `Laptop  ->   Wifi AP   ->  Home server  ->  DSL provider  ->  Datacenter`\n\n\t iodine    DNS \"relay\"        bind9           DNS cache        iodined\n\n\t                        downstr.  upstream downstr.  ping-up       ping-down\n\t                        fragsize   kbit/s   kbit/s  avg +/-mdev   avg +/-mdev\n\t-----------------------------------------------------------------------------\n\n\tiodine -> Wifi AP :53\n\t  -Tnull (= -Oraw)           982    43.6    131.0   28.0    4.6   26.8    3.4\n\n\tiodine -> Home server :53\n\t  -Tnull (= -Oraw)          1174    48.0    305.8   26.6    5.0   26.9    8.4\n\n\tiodine -> DSL provider :53\n\t  -Tnull (= -Oraw)          1174    56.7    367.0   20.6    3.1   21.2    4.4\n\t  -Ttxt -Obase32             730    56.7    174.7*\n\t  -Ttxt -Obase64             874    56.7    174.7\n\t  -Ttxt -Obase128           1018    56.7    174.7\n\t  -Ttxt -Oraw               1162    56.7    358.2\n\t  -Tsrv -Obase128            910    56.7    174.7\n\t  -Tcname -Obase32           151    56.7     43.6\n\t  -Tcname -Obase128          212    56.7     52.4\n\n\tiodine -> DSL provider :53\n\t  wired (no Wifi) -Tnull    1174    74.2    585.4   20.2    5.6   19.6    3.4\n\n\t [174.7* : these all have 2frag/packet]\n\n\n### Situation 2: `Laptop  ->  Wifi+vpn / wired  ->  Home server`\n\n\t iodine                            iodined\n\n\t                        downstr.  upstream downstr.  ping-up       ping-down\n\t                        fragsize   kbit/s   kbit/s  avg +/-mdev   avg +/-mdev\n\t-----------------------------------------------------------------------------\n\n\twifi + openvpn  -Tnull      1186   166.0   1022.3    6.3    1.3    6.6    1.6\n\n\twired  -Tnull               1186   677.2   2464.1    1.3    0.2    1.3    0.1\n\n\n### Notes\n\nPerformance is strongly coupled to low ping times, as iodine requires\nconfirmation for every data fragment before moving on to the next. Allowing\nmultiple fragments in-flight like TCP could possibly increase performance,\nbut it would likely cause serious overload for the intermediary DNS servers.\nThe current protocol scales performance with DNS responsivity, since the\nDNS servers are on average handling at most one DNS request per client.\n\n\nPORTABILITY\n-----------\n\niodine has been tested on Linux (arm, ia64, x86, AMD64 and SPARC64), FreeBSD\n(ia64, x86), OpenBSD (x86), NetBSD (x86), MacOS X (ppc and x86, with\n<http://tuntaposx.sourceforge.net/>). and Windows (with OpenVPN TAP32 driver, see\nwin32 readme file).  It should be easy to port to other unix-like systems that\nhave TUN/TAP tunneling support. Let us know if you get it to run on other\nplatforms.\n\n\nTHE NAME\n--------\n\nThe name iodine was chosen since it starts with IOD (IP Over DNS) and since\niodine has atomic number 53, which happens to be the DNS port number.\n\n\nTHANKS\n------\n\n- To kuxien for FreeBSD and OS X testing\n- To poplix for code audit\n\n\nAUTHORS & LICENSE\n-----------------\n\nCopyright (c) 2006-2014 Erik Ekman <yarrick@kryo.se>, 2006-2009 Bjorn\nAndersson <flex@kryo.se>. Also major contributions by Anne Bezemer.\n\nPermission to use, copy, modify, and/or distribute this software for any purpose\nwith or without fee is hereby granted, provided that the above copyright notice\nand this permission notice appear in all copies.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH\nREGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND\nFITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT,\nINDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM\nLOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR\nOTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR\nPERFORMANCE OF THIS SOFTWARE.\n\n\nMD5 implementation by L. Peter Deutsch (license and source in `src/md5.[ch]`)\nCopyright (C) 1999, 2000, 2002 Aladdin Enterprises.  All rights reserved.\n",
      "stars_today": 2
    },
    {
      "id": 48418599,
      "name": "fineract",
      "full_name": "apache/fineract",
      "description": "Apache Fineract",
      "html_url": "https://github.com/apache/fineract",
      "stars": 1967,
      "forks": 2229,
      "language": "Java",
      "topics": [
        "apache",
        "banking",
        "finance",
        "fintech",
        "group-lending",
        "group-savings",
        "java",
        "lending",
        "loans",
        "microfinance",
        "savings",
        "social-impact",
        "tech4good"
      ],
      "created_at": "2015-12-22T08:00:06Z",
      "updated_at": "2026-01-14T14:26:41Z",
      "pushed_at": "2026-01-13T17:06:12Z",
      "open_issues": 46,
      "owner": {
        "login": "apache",
        "avatar_url": "https://avatars.githubusercontent.com/u/47359?v=4"
      },
      "readme": "# Apache Fineract\n<!-- TODO Reactivate when there is a working CI-CD instance: [![Swagger Validation](https://validator.swagger.io/validator?url=https://sandbox.mifos.community/fineract-provider/swagger-ui/fineract.yaml)](https://validator.swagger.io/validator/debug?url=https://sandbox.mifos.community/fineract-provider/swagger-ui/fineract.yaml) -->\n[![Build](https://github.com/apache/fineract/actions/workflows/build-mariadb.yml/badge.svg?branch=develop)](https://github.com/apache/fineract/actions/workflows/build-mariadb.yml)\n[![Docker Hub](https://img.shields.io/docker/pulls/apache/fineract.svg?logo=Docker)](https://hub.docker.com/r/apache/fineract)\n[![Docker Build](https://github.com/apache/fineract/actions/workflows/publish-dockerhub.yml/badge.svg)](https://github.com/apache/fineract/actions/workflows/publish-dockerhub.yml)\n[![Technical Debt](https://sonarcloud.io/api/project_badges/measure?project=apache_fineract&metric=sqale_index)](https://sonarcloud.io/summary/new_code?id=apache_fineract)\n\nApache Fineract is an open-source core banking platform providing a\nflexible, extensible foundation for a wide range of financial services. By\nmaking robust banking technology openly available, it lowers barriers for\ninstitutions and innovators to reach underserved and unbanked populations.\n\nHave a look at the [documentation](https://fineract.apache.org/docs/current), the [wiki](https://cwiki.apache.org/confluence/display/FINERACT) or at the [FAQ](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=91554327), if this README does not answer what you are looking for.\n\n\nCOMMUNITY\n=========\n\nIf you are interested in contributing to this project, but perhaps don't quite know how and where to get started, please [join our developer mailing list](http://fineract.apache.org/#contribute), listen into our conversations, chime into threads, or just send us a \"Hello!\" introduction email; we're a friendly bunch, and look forward to hearing from you. A more informal alternative is the [Fineract Slack channel](https://app.slack.com/client/T0F5GHE8Y/C028634A61L) (thank you, Mifos, for supporting the Slack channel!).\n\nFor the developer wiki, see [Contributor's Zone](https://cwiki.apache.org/confluence/display/FINERACT/Contributor%27s+Zone). Maybe [these how-to articles](https://cwiki.apache.org/confluence/display/FINERACT/How-to+articles) help you to get started.\n\nIn any case visit [our JIRA Dashboard](https://issues.apache.org/jira/secure/Dashboard.jspa?selectPageId=12335824) to find issues to work on, see what others are doing, or open new issues.\n\nIn the moment you get started writing code, please consult our [CONTRIBUTING](CONTRIBUTING.md) guidelines, where you will find more information on subjects like coding style, testing and pull requests.\n\n\nREQUIREMENTS\n============\n* min. 16GB RAM and 8 core CPU\n* `MariaDB >= 11.5.2` or `PostgreSQL >= 17.0`\n* `Java >= 21` (Azul Zulu JVM is tested by our CI on GitHub Actions)\n\nTomcat (min. v10) is only required, if you wish to deploy the Fineract WAR to a separate external servlet container.  You do not need to install Tomcat to run Fineract. We recommend the use of the self-contained JAR, which transparently embeds a servlet container using Spring Boot.\n\n\nSECURITY\n============\nFor a list of known vulnerabilities, see [Apache Fineract Security Reports](https://fineract.apache.org/security.html).\n\nIf you believe you have found a new vulnerability, [let us know privately](https://fineract.apache.org/#contribute).\n\nFor details about security during development and deployment, see the documentation [here](https://fineract.apache.org/docs/current/#_security).\n\n\nINSTRUCTIONS\n============\n\nThe following how-to's assume you have Java installed, you cloned the repository (or downloaded and extracted a [specific version](https://github.com/apache/fineract/releases)) and you have a [database server](#database-and-tables) (MariaDB or PostgreSQL) running.\n\nHow to run for local development\n---\n\nRun the following commands in this order:\n```bash\n./gradlew createDB -PdbName=fineract_tenants\n./gradlew createDB -PdbName=fineract_default\n./gradlew devRun\n```\n\nThis creates two databases and builds and runs Fineract, which will be listening for API requests on port 8443 (by default) now.\n\nConfirm Fineract is ready with, for example:\n\n```bash\ncurl --insecure https://localhost:8443/fineract-provider/actuator/health\n```\n\nTo test authenticated endpoints, include credentials in your request:\n\n```bash\ncurl --location \\\n  https://localhost:8443/fineract-provider/api/v1/clients \\\n  --header 'Content-Type: application/json' \\\n  --header 'Fineract-Platform-TenantId: default' \\\n  --header 'Authorization: Basic bWlmb3M6cGFzc3dvcmQ='\n```\n\nHow to run for production\n---\nRunning Fineract to try it out is relatively easy. If you intend to use it in a production environment, be aware that a proper deployment can be complex, costly, and time-consuming. Considerations include: Security, privacy, compliance, performance, service availability, backups, and more. The Fineract project does not provide a comprehensive guide for deploying Fineract in production. You might need skills in enterprise Java applications and more. Alternatively, you could pay a vendor for Fineract deployment and maintenance. You will find tips and tricks for deploying and securing Fineract in our official documentation and in the community-maintained wiki.\n\n\nHow to build the JAR file\n---\nBuild a modern, cloud native, fully self contained JAR file:\n```bash\n./gradlew clean bootJar\n```\nThe JAR will be created in the `fineract-provider/build/libs` directory.\nAs we are not allowed to include a JDBC driver in the built JAR, download a JDBC driver of your choice. For example:\n```bash\nwget https://dlm.mariadb.com/4174416/Connectors/java/connector-java-3.5.2/mariadb-java-client-3.5.2.jar\n```\nStart the JAR and specify the directory containing the JDBC driver using the loader.path option, for example:\n```bash\njava -Dloader.path=. -jar fineract-provider/build/libs/fineract-provider.jar\n```\nThis does not require an external Tomcat.\n\nThe tenants database connection details are configured [via environment variables (as with Docker container)](#instructions-to-run-using-docker-or-podman), e.g. like this:\n```bash\nexport FINERACT_HIKARI_PASSWORD=verysecret\n...\njava -jar fineract-provider.jar\n```\n\nHow to build the WAR file\n---\nBuild a traditional WAR file:\n```bash\n./gradlew :fineract-war:clean :fineract-war:war\n```\nThe WAR will be created in the `fineract-war/build/libs` directory. Afterwards deploy the WAR to your Tomcat Servlet Container.\n\nWe recommend using the JAR instead of the WAR file deployment, because it's much easier.\n\n\nHow to run using Docker or Podman\n---\n\nIt is possible to do a 'one-touch' installation of Fineract using containers (AKA \"Docker\").\nThis includes the database running in the container.\n\nAs prerequisites, you must have `docker` and `docker-compose` installed on your machine; see\n[Docker Install](https://docs.docker.com/install/) and [Docker Compose Install](https://docs.docker.com/compose/install/).\n\nAlternatively, you can also use [Podman](https://github.com/containers/libpod)\n(e.g. via `dnf install podman-docker`), and [Podman Compose](https://github.com/containers/podman-compose/)\n(e.g. via `pip3 install podman-compose`) instead of Docker.\n\nTo run a new Fineract instance on Linux you can simply:\n```bash\ngit clone https://github.com/apache/fineract.git\ncd fineract\n./gradlew :fineract-provider:jibDockerBuild -x test\n```\nOn Windows, do this instead:\n```cmd\ngit clone https://github.com/apache/fineract.git --config core.autocrlf=input\ncd fineract\ngradlew :fineract-provider:jibDockerBuild -x test\n```\nInstall the Loki log driver and start:\n```bash\ndocker plugin install grafana/loki-docker-driver:latest \\\n  --alias loki --grant-all-permissions\ndocker compose -f docker-compose-development.yml up -d\n```\nThe Fineract (back-end) should be running at https://localhost:8443/fineract-provider/ now.\nWait for https://localhost:8443/fineract-provider/actuator/health to return `{\"status\":\"UP\"}`.\nYou must go to https://localhost:8443 and remember to accept the self-signed SSL certificate of the API once in your browser.\n\n[Docker Hub](https://hub.docker.com/r/apache/fineract) has a pre-built container image of this project, built continuously.\n\nYou must specify the MySQL tenants database JDBC URL by passing it to the `fineract` container via environment\nvariables; please consult the [`docker-compose.yml`](docker-compose.yml) for exact details how to specify those.\n\nThe logfiles and the Java Flight Recorder output are available in `PROJECT_ROOT/build/fineract/logs`. If you use IntelliJ then you can double-click on the `.jfr` file and open it with the IDE. You can also download [Azul Mission Control](https://www.azul.com/products/components/azul-mission-control/) to analyze the Java Flight Recorder file.\n\nNOTE: If you have issues with the file permissions and Docker Compose then you might need to change the variable values for `FINERACT_USER` and `FINERACT_GROUP` in `PROJECT_ROOT/config/docker/env/fineract-common.env`. You can find out what values you need to put there with the following commands:\n\n```bash\nid -u ${USER}\nid -g ${GROUP}\n```\n\nPlease make sure that you are not checking in your changed values. The defaults should work for most people.\n\n\nHow to run on Kubernetes\n---\n\n### General Clusters\n\nYou can also run Fineract using containers on a Kubernetes cluster.\nMake sure you set up and connect to your Kubernetes cluster.\nYou can follow [this](https://cwiki.apache.org/confluence/display/FINERACT/Install+and+configure+kubectl+and+Google+Cloud+SDK+on+ubuntu+16.04) guide to set up a Kubernetes cluster on GKE. Make sure to replace `apache-fineract-cn` with `apache-fineract`\n\nNow e.g. from your Google Cloud shell, run the following commands:\n\n```bash\ngit clone https://github.com/apache/fineract.git\ncd fineract/kubernetes\n./kubectl-startup.sh\n```\n\nTo shutdown and reset your Cluster, run:\n```bash\n./kubectl-shutdown.sh\n```\n\n### Using Minikube\n\nAlternatively, you can run fineract on a local kubernetes cluster using [minikube](https://minikube.sigs.k8s.io/docs/).\nAs prerequisite you must have `minikube` and `kubectl` installed on your machine; see\n[Minikube & Kubectl install](https://kubernetes.io/docs/tasks/tools/install-minikube/).\n\nTo run a new Fineract instance on Minikube you can simply:\n\n```bash\ngit clone https://github.com/apache/fineract.git\ncd fineract/kubernetes\nminikube start\n./kubectl-startup.sh\n```\n\nWait for all pods to be ready:\n```bash\nkubectl get pods -w\n```\n\nOnce all pods are running, access the Mifos web application:\n```bash\nminikube service mifos-community\n```\n\nThis opens the Mifos X web application in your browser. The nginx reverse proxy in the mifos-community pod forwards API requests to the fineract-server backend.\n\n**Default credentials:**\n- Username: `mifos`\n- Password: `password`\n\nYou can also access the Fineract API directly:\n```bash\nminikube service fineract-server --url --https\n```\n\nFineract is now running at the printed URL, which you can check e.g. using:\n```bash\nhttp --verify=no --timeout 240 --check-status get $(minikube service fineract-server --url --https)/fineract-provider/actuator/health\n```\n\nTo check the status of your containers on your local minikube Kubernetes cluster, run:\n```bash\nminikube dashboard\n```\n\nYou can check Fineract logs using:\n```bash\nkubectl logs deployment/fineract-server\n```\n\nYou can check Mifos web app logs using:\n```bash\nkubectl logs deployment/mifos-community\n```\n\nTo shutdown and reset your cluster, run:\n```bash\n./kubectl-shutdown.sh\n```\n\n\nHow to enable External Message Broker (ActiveMQ or Apache Kafka)\n---\n\nThere are two use-cases where external message broker is needed:\n - External Business Events / Reliable Event Framework\n - Executing Partitioned Spring Batch Jobs\n\nExternal Events are business events, e.g.: `ClientCreated`, which might be important for third party systems. Apache Fineract supports ActiveMQ (or other JMS compliant brokers) and Apache Kafka endpoints for sending out Business Events. By default, they are not emitted.\n\nIn case of a large deployment with millions of accounts, the Close of Business Day Spring Batch job may run several hours. In order to speed up this task, remote partitioning of the job is supported. The Manager node partitions breaks up the COB job into smaller pieces (sub tasks), which then can be executed on multiple Worker nodes in parallel. The worker nodes are notified either by ActiveMQ or Kafka regarding their new sub tasks.\n\n### ActiveMQ\n\nJMS based messaging is disabled by default. In `docker-compose-postgresql-activemq.yml` an example is shown, where ActiveMQ is enabled. In that configuration one Spring Batch Manager instance and two Spring Batch Worker instances are created.\nSpring based events should be disabled and jms based event handling should be enabled. Furthermore, proper broker JMS URL should be configured.\n\n```\n      FINERACT_REMOTE_JOB_MESSAGE_HANDLER_JMS_ENABLED=true\n      FINERACT_REMOTE_JOB_MESSAGE_HANDLER_SPRING_EVENTS_ENABLED=false\n      FINERACT_REMOTE_JOB_MESSAGE_HANDLER_JMS_BROKER_URL=tcp://activemq:61616\n```\n\nFor additional ActiveMQ related configuration please take a look to the `application.properties` where the supported configuration parameters are listed with their default values.\n\n### Kafka\n\nKafka support is also disabled by default. In `docker-compose-postgresql-kafka.yml` an example is shown, where self-hosted Kafka is enabled for both External Events and Spring Batch Remote Job execution.\n\nDuring the development Fineract was tested with PLAINTEXT Kafka brokers without authentication and with AWS MSK using IAM authentication. The extra [JAR file](https://github.com/aws/aws-msk-iam-auth/releases) required for IAM authentication is already added to the classpath.\nAn example MSK setup can be found in `docker-compose-postgresql-kafka-msk.yml`.\n\nThe full list of supported Kafka related properties is documented in the [Fineract Platform documentation](https://fineract.apache.org/docs/current/).\n\n\nDATABASE AND TABLES\n===================\n\nYou can run the required version of the database server in a container, instead of having to install it, like this:\n\n    docker run --name mariadb-11.5 -p 3306:3306 -e MARIADB_ROOT_PASSWORD=mysql -d mariadb:11.5.2\n\nand stop and destroy it like this:\n\n    docker rm -f mariadb-11.5\n\nBeware that this container database keeps its state inside the container and not on the host filesystem.  It is lost when you destroy (rm) this container.  This is typically fine for development.  See [Caveats: Where to Store Data on the database container documentation](https://hub.docker.com/_/mariadb) regarding how to make it persistent instead of ephemeral.\n\n\nMySQL/MariaDB and UTC timezone\n---\nWith release `1.8.0` we introduced improved date time handling in Fineract. Date time is stored in UTC, and UTC timezone enforced even on the JDBC driver, e. g. for MySQL:\n\n```\nserverTimezone=UTC&useLegacyDatetimeCode=false&sessionVariables=time_zone='-00:00'\n```\n\nIf you use MySQL as Fineract database, the following configuration is highly recommended:\n\n* Run the application in UTC (the default command line in our Docker image has the necessary parameters already set)\n* Run the MySQL database server in UTC (if you use managed services like AWS RDS, then this should be the default anyway, but it would be good to double-check)\n\nIn case Fineract and MySQL do not run in UTC, MySQL might save date time values differently from PostgreSQL\n\nExample scenario: If the Fineract instance runs in timezone: GMT+2, and the local date time is 2022-08-11 17:15 ...\n* ... then PostgreSQL saves the LocalDateTime as is: 2022-08-11 17:15\n* ... and MySQL saves the LocalDateTime in UTC: 2022-08-11 15:15\n* ... but when we read the date time from PostgreSQL or from MySQL, both systems give us the same value: 2022-08-11 17:15 GMT+2\n\nIf a previously used Fineract instance didn't run in UTC (backward compatibility), all prior dates will be read wrongly by MySQL. This can cause issues, when you run the database migration scripts.\n\nRecommendation: Shift all dates in your database by the timezone offset that your Fineract instance used.\n\n\nCONNECTION POOL CONFIGURATION\n=======\n\nPlease check `application.properties` to see which connection pool settings can be tweaked. The associated environment variables are prefixed with `FINERACT_HIKARI_*`. You can find more information about specific connection pool settings at the [HikariCP Github repository](https://github.com/brettwooldridge/HikariCP?tab=readme-ov-file#gear-configuration-knobs-baby).\n\nNOTE: We keep backwards compatibility until one of the next releases to ensure that things are working as expected. Environment variables prefixed `fineract_tenants_*` can still be used to configure the database connection, but we strongly encourage using `FINERACT_HIKARI_*` with more options.\n\n\nVERSIONS\n============\n\nA release version is derived from source control. The version will include `-SNAPSHOT` unless the current branch looks like a release or release maintenance branch. See `gitVersioning` settings in `build.gradle` for details.\n\nThe latest stable release can be viewed on the develop branch: [Latest Release on Develop](https://github.com/apache/fineract/tree/develop \"Latest Release\").\n\nThe progress of this project can be viewed in the left hand navigation under [this page of the wiki](https://cwiki.apache.org/confluence/display/FINERACT/Fineract+Releases)\n\n\nLICENSE\n============\n\nThis project is licensed under [Apache License Version 2.0](https://github.com/apache/fineract/blob/develop/APACHE_LICENSETEXT.md).\n\nThe Connector/J JDBC Driver client library from [MariaDB](https://www.mariadb.org) is licensed under the LGPL.\nThe library is often used in development when running integration tests that use the Liquibase library. That JDBC\ndriver is however not distributed with the Fineract product and is not required to use the product.\nIf you are a developer and object to using the LGPL licensed Connector/J JDBC driver,\nsimply do not run the integration tests that use the Liquibase library and use another JDBC driver.\nAs discussed in [LEGAL-462](https://issues.apache.org/jira/browse/LEGAL-462), this project therefore\ncomplies with the [Apache Software Foundation third-party license policy](https://www.apache.org/legal/resolved.html).\n\n\nPLATFORM API\n============\n\nFineract does not provide a UI, but provides an API. Running Fineract locally, the Swagger documentation can be accessed under `https://localhost:8443/fineract-provider/swagger-ui/index.html`. A live version can be accessed via [this Sandbox](https://sandbox.mifos.community/fineract-provider/swagger-ui/index.html) (not hosted by us).\n\nApache Fineract supports client code generation using [Swagger Codegen](https://github.com/swagger-api/swagger-codegen) based on the [OpenAPI Specification](https://swagger.io/specification/). For more instructions on how to generate client code, check [this section](https://fineract.apache.org/docs/current/#_generate_api_client) of the Fineract documentation. [This video](https://www.youtube.com/watch?v=FlVd-0YAo6c) documents the use of the Swagger-UI.\n",
      "stars_today": 2
    },
    {
      "id": 713058315,
      "name": "quickjs",
      "full_name": "quickjs-ng/quickjs",
      "description": "QuickJS, the Next Generation: a mighty JavaScript engine",
      "html_url": "https://github.com/quickjs-ng/quickjs",
      "stars": 2581,
      "forks": 240,
      "language": "C",
      "topics": [],
      "created_at": "2023-11-01T18:49:43Z",
      "updated_at": "2026-01-14T20:34:22Z",
      "pushed_at": "2026-01-13T14:38:46Z",
      "open_issues": 95,
      "owner": {
        "login": "quickjs-ng",
        "avatar_url": "https://avatars.githubusercontent.com/u/149480552?v=4"
      },
      "readme": "# âš¡ï¸ QuickJS - A mighty JavaScript engine\n\n## Overview\n\nQuickJS is a small and embeddable JavaScript engine. It aims to support the latest\n[ECMAScript] specification.\n\nThis project is a _fork_ of the [original QuickJS project] by Fabrice Bellard and Charlie Gordon, after it went dormant, with the intent of reigniting its development.\n\n## Getting started\n\nHead over to the [project website] for instructions on how to get started and more\ndocumentation.\n\n## Authors\n\n[@bnoordhuis], [@saghul], and many more [contributors].\n\n[ECMAScript]: https://tc39.es/ecma262/\n[original QuickJS project]: https://bellard.org/quickjs\n[@bnoordhuis]: https://github.com/bnoordhuis\n[@saghul]: https://github.com/saghul\n[contributors]: https://github.com/quickjs-ng/quickjs/graphs/contributors\n[project website]: https://quickjs-ng.github.io/quickjs/\n",
      "stars_today": 2
    },
    {
      "id": 71057302,
      "name": "openems",
      "full_name": "OpenEMS/openems",
      "description": "OpenEMS - Open Source Energy Management System",
      "html_url": "https://github.com/OpenEMS/openems",
      "stars": 1243,
      "forks": 541,
      "language": "Java",
      "topics": [
        "climatechange",
        "electric-vehicle-charging-station",
        "energy-management",
        "energy-storage",
        "hacktoberfest",
        "heatpump",
        "photovoltaics",
        "time-of-use-tariff"
      ],
      "created_at": "2016-10-16T14:57:33Z",
      "updated_at": "2026-01-14T21:39:24Z",
      "pushed_at": "2026-01-14T21:39:19Z",
      "open_issues": 61,
      "owner": {
        "login": "OpenEMS",
        "avatar_url": "https://avatars.githubusercontent.com/u/20765902?v=4"
      },
      "readme": "[![Build Status](https://github.com/OpenEMS/openems/actions/workflows/build.yml/badge.svg)](https://github.com/OpenEMS/openems/actions/workflows/build.yml)\n[![Gitpod live-demo](https://img.shields.io/badge/Gitpod-live--demo-blue?logo=gitpod)](https://gitpod.io/#https://github.com/OpenEMS/openems/tree/main)\n[![Cite via Zenodo](https://zenodo.org/badge/DOI/10.5281/zenodo.4440884.svg)](https://doi.org/10.5281/zenodo.4440883)\n[![codecov](https://codecov.io/gh/openems/openems/graph/badge.svg?token=xliIughqt1)](https://codecov.io/gh/openems/openems)\n\n<h1 align=\"center\">\n  <img src=\"./doc/modules/ROOT/assets/images/OpenEMS-Logo.png\" alt=\"the Feneco - OpenEMS Logo\" width=\"200\"></a>\n  <br/>Open Source Energy Management System\n</h1>\n\nOpenEMS - the Open Source Energy Management System - is a modular platform for energy management applications. It was developed around the requirements of monitoring, controlling, and integrating energy storage together with renewable energy sources and complementary devices and services like electric vehicle charging stations, heat-pumps, electrolysers, time-of-use electricity tariffs and more.\n\nIf you plan to use OpenEMS for your own projects, please consider joining the [OpenEMS Association e.V.](https://openems.io/association), a network of universities, hardware manufacturers, software companies as well as commercial and private owners, and get in touch in the [OpenEMS Community forum](https://community.openems.io). \n\n### OpenEMS in Â»Local Energy ManagementÂ«\n\n![Local Energy Management](./doc/modules/ROOT/assets/images/local-energy-management.png \"Local Energy Management\")\n\n### OpenEMS in Â»Areal Energy ManagementÂ«\n\n![Areal Energy Management](./doc/modules/ROOT/assets/images/areal-energy-management.png \"Areal Energy Management\")\n\n## OpenEMS IoT stack\n\nThe OpenEMS 'Internet of Things' stack contains three main components:\n\n * **OpenEMS Edge** runs on site, communicates with devices and services, collects data and executes control algorithms\n * **OpenEMS UI** is the real-time user interface for web browsers and smartphones\n * **OpenEMS Backend** runs on a (cloud) server, connects the decentralized Edge systems and provides aggregation, monitoring and control via internet\n\n## Features\n\nThe OpenEMS software architecture was designed to leverage some features that are required by a modern and flexible Energy Management System:\n\n * Fast, PLC-like control of devices\n * Easily extendable due to the use of modern programming languages and modular architecture\n * Reusable, device independent control algorithms due to clear device abstraction\n * Wide range of supported devices and protocols\n\n## OpenEMS UI Screenshots\n\n![OpenEMS UI Live View](./doc/modules/ROOT/assets/images/ui-live.png \"OpenEMS UI Live View\")\n![OpenEMS UI History View](./doc/modules/ROOT/assets/images/ui-history.png \"OpenEMS UI History View\")\n\n## System architecture\n\nOpenEMS is generally used in combination with external hardware and software components\n(the exception is a simulated development environment - see [Getting Started](https://openems.github.io/openems.io/openems/latest/gettingstarted.html)). As a brief overview, this is how OpenEMS is used in production setups:\n![OpenEMS System Architecture](./doc/modules/ROOT/assets/images/system-architecture.png \"OpenEMS System Architecture\")\n\n## Getting Started\n\n* Open up a [Live-Demo on Gitpod](https://gitpod.io/#https://github.com/OpenEMS/openems)\n* Follow the [Getting Started](https://openems.github.io/openems.io/openems/latest/gettingstarted.html) guide to setup OpenEMS on your own computer\n* Please checkout our [contribution guidelines](/.github/CONTRIBUTING.md) before submitting code\n\n## Documentation\n\n* [Latest version of documentation](https://openems.github.io/openems.io/openems/latest/introduction.html)\n* [Javadoc](https://openems.github.io/openems.io/javadoc/)\n\n## Open Source philosophy\n\nThe OpenEMS project is driven by the [OpenEMS Association e.V.](https://openems.io/association), a network of users, vendors and scientific institutions from all kinds of areas like hardware manufacturers, software companies, grid operators and more. They share the common target of developing a free and open-source platform for energy management, that supports the 100 % energy transition.\n\nWe are inviting third parties to use OpenEMS for their own projects and are glad to support them with their first steps. In any case if you are interested in OpenEMS we would be glad to hear from you in the [OpenEMS Community forum](https://community.openems.io).\n\nOpenEMS development was started by [FENECON GmbH](https://www.fenecon.de), a German company specialized in manufacturing and project development of energy storage systems. It is the software stack behind [FEMS - FENECON Energy Management System](https://fenecon.de/page/fems) and widely used in private, commercial and industrial applications.\n\nOpenEMS is funded by several federal and EU funding projects. If you are a developer and you would like to get hired by one of the partner companies or universities for working on OpenEMS, please send your motivation letter to info@openems.io.\n\n## Scientific Research\n\nIf you use OpenEMS in your scientific research, please use our Zenodo Digital Object Identifier (DOI) as reference:\n\n[![Cite via Zenodo](https://zenodo.org/badge/DOI/10.5281/zenodo.4440884.svg)](https://doi.org/10.5281/zenodo.4440883)\n\n## License\n\n* OpenEMS Edge \n* OpenEMS Backend\n\nCopyright (C) 2016-2025 OpenEMS Association e.V.\n\nThis product includes software developed at FENECON GmbH: you can\nredistribute it and/or modify it under the terms of the [Eclipse Public License version 2.0](LICENSE-EPL-2.0). \n\n * OpenEMS UI\n\nCopyright (C) 2016-2025 OpenEMS Association e.V.\n\nThis product includes software developed at FENECON GmbH: you can\nredistribute it and/or modify it under the terms of the [GNU Affero General Public License version 3](LICENSE-AGPL-3.0).\n",
      "stars_today": 2
    },
    {
      "id": 573301536,
      "name": "sing-box-for-android",
      "full_name": "SagerNet/sing-box-for-android",
      "description": "Experimental Android client for sing-box",
      "html_url": "https://github.com/SagerNet/sing-box-for-android",
      "stars": 849,
      "forks": 325,
      "language": "Kotlin",
      "topics": [],
      "created_at": "2022-12-02T06:15:42Z",
      "updated_at": "2026-01-15T00:23:39Z",
      "pushed_at": "2026-01-15T00:23:35Z",
      "open_issues": 7,
      "owner": {
        "login": "SagerNet",
        "avatar_url": "https://avatars.githubusercontent.com/u/83217677?v=4"
      },
      "readme": "# SFA\n\nExperimental Android client for sing-box, the universal proxy platform.\n\n## Documentation\n\nhttps://sing-box.sagernet.org/installation/clients/sfa/\n\n## License\n\n```\nCopyright (C) 2022 by nekohasekai <contact-sagernet@sekai.icu>\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see <http://www.gnu.org/licenses/>.\n\nIn addition, no derivative work may use the name or imply association\nwith this application without prior consent.\n```\n\nUnder the license, that forks of the app are not allowed to be listed on F-Droid or other app stores\nunder the original name.\n",
      "stars_today": 2
    },
    {
      "id": 70198664,
      "name": "lottie-ios",
      "full_name": "airbnb/lottie-ios",
      "description": "An iOS library to natively render After Effects vector animations",
      "html_url": "https://github.com/airbnb/lottie-ios",
      "stars": 26614,
      "forks": 3830,
      "language": "Swift",
      "topics": [
        "animation",
        "bodymovin",
        "custom-transitions",
        "ios",
        "ios-animation",
        "ios-transition",
        "keyframes",
        "swift",
        "transition-animation"
      ],
      "created_at": "2016-10-06T22:38:38Z",
      "updated_at": "2026-01-14T21:39:56Z",
      "pushed_at": "2026-01-13T14:44:25Z",
      "open_issues": 44,
      "owner": {
        "login": "airbnb",
        "avatar_url": "https://avatars.githubusercontent.com/u/698437?v=4"
      },
      "readme": "# Lottie for iOS\n [![Version](https://img.shields.io/cocoapods/v/lottie-ios.svg?style=flat)](https://cocoapods.org/pods/lottie-ios) [![Carthage Compatible](https://img.shields.io/badge/Carthage-compatible-4BC51D.svg?style=flat)](https://github.com/Carthage/Carthage) [![SwiftPM](https://img.shields.io/badge/SPM-supported-DE5C43.svg?style=flat)](https://swift.org/package-manager/) [![License](https://img.shields.io/cocoapods/l/lottie-ios.svg?style=flat)](https://cocoapods.org/pods/lottie-ios) [![Platform](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fairbnb%2Flottie-ios%2Fbadge%3Ftype%3Dplatforms)](https://swiftpackageindex.com/airbnb/lottie-ios) [![Swift Versions](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fairbnb%2Flottie-ios%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/airbnb/lottie-ios)\n\n**View documentation, FAQ, help, examples, and more at [airbnb.io/lottie](https://airbnb.io/lottie/)**\n\nLottie is a cross-platform library for iOS, macOS, tvOS, visionOS, [Android](https://github.com/airbnb/lottie-android), and [Web](https://github.com/airbnb/lottie-web) that natively renders vector-based animations and art in realtime with minimal code.\n\nLottie loads and renders animations and vectors exported in the bodymovin JSON format. Bodymovin JSON can be created and exported from After Effects with [bodymovin](https://github.com/bodymovin/bodymovin), Sketch with [Lottie Sketch Export](https://github.com/buba447/Lottie-Sketch-Export), and from [Haiku](https://www.haikuanimator.com).\n\nDesigners can create **and ship** beautiful animations without an engineer painstakingly recreating them by hand.\nSince the animations are backed by JSON, they are extremely small in size but can be large in complexity!\nAnimations can be played, resized, looped, sped up, slowed down, reversed, and even interactively scrubbed.\nLottie can play or loop just a portion of the animation as well, the possibilities are endless!\nAnimations can even be ***changed at runtime*** in various ways! Change the color, position, or any keyframable value!\n\nHere is just a small sampling of the power of Lottie\n\n![Example1](_Gifs/Examples1.gif)\n![Example2](_Gifs/Examples2.gif)\n\n<img src=\"_Gifs/Community 2_3.gif\" />\n\n![Example3](_Gifs/Examples3.gif)\n\n![Abcs](_Gifs/Examples4.gif)\n\n## Installing Lottie\nLottie supports [Swift Package Manager](https://www.swift.org/package-manager/), [CocoaPods](https://cocoapods.org/), and [Carthage](https://github.com/Carthage/Carthage) (Both dynamic and static).\n\n### Github Repo\n\nYou can pull the [Lottie Github Repo](https://github.com/airbnb/lottie-ios/) and include the `Lottie.xcodeproj` to build a dynamic or static library.\n\n### Swift Package Manager\n\nTo install Lottie using [Swift Package Manager](https://github.com/swiftlang/swift-package-manager) you can follow the [tutorial published by Apple](https://developer.apple.com/documentation/xcode/adding_package_dependencies_to_your_app) using the URL for the Lottie repo with the current version:\n\n1. In Xcode, select â€œFileâ€ â†’ â€œAdd Packages...â€\n1. Enter https://github.com/airbnb/lottie-spm.git\n\nor you can add the following dependency to your `Package.swift`:\n\n```swift\n.package(url: \"https://github.com/airbnb/lottie-spm.git\", from: \"4.5.2\")\n```\n\nWhen using Swift Package Manager we recommend using the [lottie-spm](https://github.com/airbnb/lottie-spm) repo instead of the main lottie-ios repo.  The main git repository for [lottie-ios](https://github.com/airbnb/lottie-ios) is somewhat large (300+ MB), and Swift Package Manager always downloads the full repository with all git history. The [lottie-spm](https://github.com/airbnb/lottie-spm) repo is much smaller (less than 500kb), so can be downloaded much more quickly. \n\nInstead of downloading the full git history of Lottie and building it from source, the lottie-spm repo just contains a pointer to the precompiled XCFramework included in the [latest lottie-ios release](https://github.com/airbnb/lottie-ios/releases/latest) (typically ~8MB). If you prefer to include Lottie source directly your project, you can directly depend on the main lottie-ios repo by referencing `https://github.com/airbnb/lottie-ios.git` instead.\n\n### CocoaPods\nAdd the pod to your Podfile:\n```ruby\npod 'lottie-ios'\n```\n\nAnd then run:\n```ruby\npod install\n```\nAfter installing the cocoapod into your project import Lottie with\n```swift\nimport Lottie\n```\n\n### Carthage\nAdd Lottie to your Cartfile:\n```\ngithub \"airbnb/lottie-ios\" \"master\"\n```\n\nAnd then run:\n```\ncarthage update\n```\nIn your application targets â€œGeneralâ€ tab under the â€œLinked Frameworks and Librariesâ€ section, drag and drop lottie-ios.framework from the Carthage/Build/iOS directory that `carthage update` produced.\n\n## Swift Version Support\n\nLottie supports Swift / Xcode versions back to the minimum version that is permitted by Apple for submissions to the App Store. You can see the most up-to-date information for which Swift versions Lottie supports on [Swift Package Index](https://swiftpackageindex.com/airbnb/lottie-ios):\n\n[![Swift Versions](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fairbnb%2Flottie-ios%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/airbnb/lottie-ios)\n\n## Privacy\n\nLottie does not collect any data. We provide this notice to help you fill out [App Privacy Details](https://developer.apple.com/app-store/app-privacy-details/). We additionally provide a [privacy manifest](https://github.com/airbnb/lottie-ios/blob/master/Sources/PrivacyInfo.xcprivacy) which can be included in your app.\n\n## Security\n\nWe distribute XCFramework bundles for each release on [GitHub](https://github.com/airbnb/lottie-ios/releases/latest). In Lottie 4.4.0 and later, these XCFramework bundles include a [code signature](https://developer.apple.com/documentation/xcode/verifying-the-origin-of-your-xcframeworks). These bundles are self-signed under the name \"Lottie iOS\" and have the following fingerprint:\n\n```\n89 2F 1B 43 04 7B 50 53 8F 2F 46 EA D9 29 00 DD 3D 48 11 F358 21 78 C0 61 A5 FB 20 F1 11 CB 26\n```\n\nIn Xcode you can verify this by selecting `Lottie.xcframework` and confirming that it shows the following information:\n\n![Code Signature in Xcode](_Gifs/code_signature.png)\n\n## Contributing\n\nWe always appreciate contributions from the community. To make changes to the project, you can clone the repo and open `Lottie.xcworkspace`. This workspace includes:\n - the Lottie framework (for iOS, macOS, and tvOS)\n - unit tests and snapshot tests (for iOS, must be run on an iPhone 8 simulator)\n - an Example iOS app that lets you browse and test over 100 sample animations included in the repo\n\nAll pull requests with new features or bug fixes that affect how animations render should include snapshot test cases that validate the included changes. \n  - To add a new sample animation to the snapshot testing suite, you can add the `.json` file to `Tests/Samples`. Re-run the snapshot tests to generate the new snapshot image files.\n  - To update existing snapshots after making changes, you can set `isRecording = true` in `SnapshotTests.swift` `setUp()` method and then re-run the snapshot tests.\n\nThe project also includes several helpful commands defined in our [Rakefile](https://github.com/airbnb/lottie-ios/blob/master/Rakefile). To use these, you need to install [Bundler](https://bundler.io/):\n\n```bash\n$ sudo gem install bundle\n$ bundle install\n```\n\nFor example, all Swift code should be formatted according to the [Airbnb Swift Style Guide](https://github.com/airbnb/swift). After making changes, you can reformat the code automatically using [SwiftFormat](https://github.com/nicklockwood/SwiftFormat) and [SwiftLint](https://github.com/realm/SwiftLint) by running `bundle exec rake format:swift`. Other helpful commands include:\n\n```bash\n$ bundle exec rake build:all # builds all targets for all platforms\n$ bundle exec rake build:package:iOS # builds the Lottie package for iOS\n$ bundle exec rake test:package # tests the Lottie package\n$ bundle exec rake format:swift # reformat Swift code based on the Airbnb Swift Style Guide\n```\n",
      "stars_today": 1
    },
    {
      "id": 27729907,
      "name": "grpc-go",
      "full_name": "grpc/grpc-go",
      "description": "The Go language implementation of gRPC. HTTP/2 based RPC",
      "html_url": "https://github.com/grpc/grpc-go",
      "stars": 22710,
      "forks": 4634,
      "language": "Go",
      "topics": [
        "dogs-over-cats",
        "giant-robots",
        "go",
        "golang",
        "grpc",
        "hacktoberfest",
        "microservices",
        "not-nanoservices",
        "proto",
        "rpc"
      ],
      "created_at": "2014-12-08T18:59:34Z",
      "updated_at": "2026-01-15T01:02:31Z",
      "pushed_at": "2026-01-14T06:02:59Z",
      "open_issues": 149,
      "owner": {
        "login": "grpc",
        "avatar_url": "https://avatars.githubusercontent.com/u/7802525?v=4"
      },
      "readme": "# gRPC-Go\n\n[![GoDoc](https://pkg.go.dev/badge/google.golang.org/grpc)][API]\n[![GoReportCard](https://goreportcard.com/badge/grpc/grpc-go)](https://goreportcard.com/report/github.com/grpc/grpc-go)\n[![codecov](https://codecov.io/gh/grpc/grpc-go/graph/badge.svg)](https://codecov.io/gh/grpc/grpc-go)\n\nThe [Go][] implementation of [gRPC][]: A high performance, open source, general\nRPC framework that puts mobile and HTTP/2 first. For more information see the\n[Go gRPC docs][], or jump directly into the [quick start][].\n\n## Prerequisites\n\n- **[Go][]**: any one of the **two latest major** [releases][go-releases].\n\n## Installation\n\nSimply add the following import to your code, and then `go [build|run|test]`\nwill automatically fetch the necessary dependencies:\n\n\n```go\nimport \"google.golang.org/grpc\"\n```\n\n> **Note:** If you are trying to access `grpc-go` from **China**, see the\n> [FAQ](#FAQ) below.\n\n## Learn more\n\n- [Go gRPC docs][], which include a [quick start][] and [API\n  reference][API] among other resources\n- [Low-level technical docs](Documentation) from this repository\n- [Performance benchmark][]\n- [Examples](examples)\n- [Contribution guidelines](CONTRIBUTING.md)\n\n## FAQ\n\n### I/O Timeout Errors\n\nThe `golang.org` domain may be blocked from some countries. `go get` usually\nproduces an error like the following when this happens:\n\n```console\n$ go get -u google.golang.org/grpc\npackage google.golang.org/grpc: unrecognized import path \"google.golang.org/grpc\" (https fetch: Get https://google.golang.org/grpc?go-get=1: dial tcp 216.239.37.1:443: i/o timeout)\n```\n\nTo build Go code, there are several options:\n\n- Set up a VPN and access google.golang.org through that.\n\n- With Go module support: it is possible to use the `replace` feature of `go\n  mod` to create aliases for golang.org packages.  In your project's directory:\n\n  ```sh\n  go mod edit -replace=google.golang.org/grpc=github.com/grpc/grpc-go@latest\n  go mod tidy\n  go mod vendor\n  go build -mod=vendor\n  ```\n\n  Again, this will need to be done for all transitive dependencies hosted on\n  golang.org as well. For details, refer to [golang/go issue\n  #28652](https://github.com/golang/go/issues/28652).\n\n### Compiling error, undefined: grpc.SupportPackageIsVersion\n\nPlease update to the latest version of gRPC-Go using\n`go get google.golang.org/grpc`.\n\n### How to turn on logging\n\nThe default logger is controlled by environment variables. Turn everything on\nlike this:\n\n```console\n$ export GRPC_GO_LOG_VERBOSITY_LEVEL=99\n$ export GRPC_GO_LOG_SEVERITY_LEVEL=info\n```\n\n### The RPC failed with error `\"code = Unavailable desc = transport is closing\"`\n\nThis error means the connection the RPC is using was closed, and there are many\npossible reasons, including:\n 1. mis-configured transport credentials, connection failed on handshaking\n 1. bytes disrupted, possibly by a proxy in between\n 1. server shutdown\n 1. Keepalive parameters caused connection shutdown, for example if you have\n    configured your server to terminate connections regularly to [trigger DNS\n    lookups](https://github.com/grpc/grpc-go/issues/3170#issuecomment-552517779).\n    If this is the case, you may want to increase your\n    [MaxConnectionAgeGrace](https://pkg.go.dev/google.golang.org/grpc/keepalive?tab=doc#ServerParameters),\n    to allow longer RPC calls to finish.\n\nIt can be tricky to debug this because the error happens on the client side but\nthe root cause of the connection being closed is on the server side. Turn on\nlogging on __both client and server__, and see if there are any transport\nerrors.\n\n[API]: https://pkg.go.dev/google.golang.org/grpc\n[Go]: https://golang.org\n[Go module]: https://github.com/golang/go/wiki/Modules\n[gRPC]: https://grpc.io\n[Go gRPC docs]: https://grpc.io/docs/languages/go\n[Performance benchmark]: https://performance-dot-grpc-testing.appspot.com/explore?dashboard=5180705743044608\n[quick start]: https://grpc.io/docs/languages/go/quickstart\n[go-releases]: https://golang.org/doc/devel/release.html\n",
      "stars_today": 1
    },
    {
      "id": 1613176,
      "name": "edk2",
      "full_name": "tianocore/edk2",
      "description": "EDK II",
      "html_url": "https://github.com/tianocore/edk2",
      "stars": 5719,
      "forks": 2999,
      "language": "C",
      "topics": [
        "c",
        "firmware",
        "python",
        "uefi"
      ],
      "created_at": "2011-04-14T07:35:08Z",
      "updated_at": "2026-01-14T19:15:19Z",
      "pushed_at": "2026-01-14T11:51:50Z",
      "open_issues": 1235,
      "owner": {
        "login": "tianocore",
        "avatar_url": "https://avatars.githubusercontent.com/u/352162?v=4"
      },
      "readme": "==============\r\nEDK II Project\r\n==============\r\n\r\nA modern, feature-rich, cross-platform firmware development\r\nenvironment for the UEFI and PI specifications from www.uefi.org.\r\n\r\n.. image:: https://img.shields.io/badge/dynamic/toml?url=https%3A%2F%2Fraw.githubusercontent.com%2Ftianocore%2Fedk2-pytool-extensions%2Frefs%2Fheads%2Fmaster%2Fpyproject.toml&query=%24.%5B'requires-python'%5D&style=for-the-badge&logo=python&logoColor=ffd343&label=Minimum%20Python%20Version%20for%20CI&color=3776ab&link=https%3A%2F%2Fwww.python.org%2Fdownloads%2F\r\n   :alt: CI Minimum Python Version\r\n\r\nIt is recommended to install this Python version to run the full set of scripts that enable CI in the project.\r\n\r\nOther Python requirements for build can be found in the `EDK II Build Instructions <https://github.com/tianocore/tianocore.github.io/wiki/Build-Instructions/>`__.\r\n\r\nCore CI Build Status\r\n--------------------\r\n\r\n============================= ================= =============== ===================\r\n Host Type & Toolchain        Build Status      Test Status     Code Coverage\r\n============================= ================= =============== ===================\r\nWindows_VS_                   |WindowsCiBuild|  |WindowsCiTest| |WindowsCiCoverage|\r\nUbuntu_GCC_                   |UbuntuCiBuild|   |UbuntuCiTest|  |UbuntuCiCoverage|\r\n============================= ================= =============== ===================\r\n\r\n`More CI Build information <.pytool/Readme.md>`__\r\n\r\nPlatform CI Build Status\r\n------------------------\r\n\r\nMicrosoft Windows Visual Studio (VS)\r\n````````````````````````\r\n\r\n============================= ================= ============= ============= ==============\r\n Toolchain                    CONFIG            DEBUG         RELEASE       NOOPT\r\n============================= ================= ============= ============= ==============\r\nEmulatorPkg_Win_VS_           | IA32            |em32d|       |em32r|       |em32n|\r\n|                             | X64             |em64d|       |em64r|       |em64n|\r\n|                             | IA32 FULL       |em32fd|      |em32fr|      |em32fn|\r\n|                             | X64 FULL        |em64fd|      |em64fr|      |em64fn|\r\nOvmfPkg_Win_VS_               | IA32            |op32d|       |op32r|       |op32n|\r\n|                             | X64             |op64d|       |op64r|       |op64n|\r\n============================= ================= ============= ============= ==============\r\n\r\nUbuntu 24.04\r\n`````````````````\r\n\r\n============================= ================= ============= ============= ==============\r\n Toolchain                    CONFIG            DEBUG         RELEASE       NOOPT\r\n============================= ================= ============= ============= ==============\r\nArmVirtPkg_Ubuntu_GCC_        | AARCH64         |avAArch64du| |avAArch64ru| |avAArch64nu|\r\nEmulatorPkg_Ubuntu_GCC_       | IA32            **N/A**       **N/A**       **N/A**\r\n|                             | X64             |em64du|      |em64ru|      |em64nu|\r\n|                             | IA32 FULL       **N/A**       **N/A**       **N/A**\r\n|                             | X64 FULL        |em64fdu|     |em64fru|     |em64fnu|\r\nOvmfPkg_Ubuntu_GCC_           | IA32            |op32du|      |op32ru|      |op32nu|\r\n|                             | X64             |op64du|      |op64ru|      |op64nu|\r\n============================= ================= ============= ============= ==============\r\n\r\n|TCBZ_2639|_ - EmulatorPkg Ubuntu GCC5 Segfaults during execution.\r\n\r\n`More ArmVirtPkg CI Build Information <ArmVirtPkg/PlatformCI/ReadMe.md>`__\r\n\r\n`More EmulatorPkg CI Build Information <EmulatorPkg/PlatformCI/ReadMe.md>`__\r\n\r\n`More OvmfPkg CI Build Information <OvmfPkg/PlatformCI/ReadMe.md>`__\r\n\r\n\r\nLicense Details\r\n---------------\r\n\r\nThe majority of the content in the EDK II open source project uses a\r\n`BSD-2-Clause Plus Patent License <License.txt>`__. The EDK II open\r\nsource project contains the following components that are covered by additional\r\nlicenses:\r\n\r\n-  `BaseTools/Plugin/CodeQL/analyze <https://www.apache.org/licenses/LICENSE-2.0>`__\r\n-  `BaseTools/Source/C/LzmaCompress <BaseTools/Source/C/LzmaCompress/LZMA-SDK-README.txt>`__\r\n-  `BaseTools/Source/C/VfrCompile/Pccts <BaseTools/Source/C/VfrCompile/Pccts/RIGHTS>`__\r\n-  `CryptoPkg\\Library\\BaseCryptLib\\SysCall\\inet_pton.c <CryptoPkg\\Library\\BaseCryptLib\\SysCall\\inet_pton.c>`__\r\n-  `CryptoPkg\\Library\\Include\\crypto\\dso_conf.h <https://github.com/openssl/openssl/blob/e2e09d9fba1187f8d6aafaa34d4172f56f1ffb72/LICENSE>`__\r\n-  `CryptoPkg\\Library\\Include\\openssl\\opensslconf.h <https://github.com/openssl/openssl/blob/e2e09d9fba1187f8d6aafaa34d4172f56f1ffb72/LICENSE>`__\r\n-  `MdeModulePkg/Library/LzmaCustomDecompressLib <MdeModulePkg/Library/LzmaCustomDecompressLib/LZMA-SDK-README.txt>`__\r\n-  `OvmfPkg <OvmfPkg/License.txt>`__\r\n\r\nThe EDK II open source project uses content from upstream projects as git submodules\r\nthat are covered by additional licenses.\r\n\r\n-  `BaseTools/Source/C/BrotliCompress/brotli <https://github.com/google/brotli/blob/666c3280cc11dc433c303d79a83d4ffbdd12cc8d/LICENSE>`__\r\n-  `CryptoPkg/Library/OpensslLib/openssl <https://github.com/openssl/openssl/blob/e2e09d9fba1187f8d6aafaa34d4172f56f1ffb72/LICENSE>`__\r\n-  `CryptoPkg/Library/MbedTlsLib/mbedtls <https://github.com/Mbed-TLS/mbedtls/blob/8c89224991adff88d53cd380f42a2baa36f91454/LICENSE>`__\r\n-  `MdeModulePkg/Library/BrotliCustomDecompressLib/brotli <https://github.com/google/brotli/blob/666c3280cc11dc433c303d79a83d4ffbdd12cc8d/LICENSE>`__\r\n-  `MdeModulePkg/Universal/RegularExpressionDxe/oniguruma <https://github.com/kkos/oniguruma/blob/abfc8ff81df4067f309032467785e06975678f0d/COPYING>`__\r\n-  `UnitTestFrameworkPkg/Library/CmockaLib/cmocka <https://github.com/tianocore/edk2-cmocka/blob/f5e2cd77c88d9f792562888d2b70c5a396bfbf7a/COPYING>`__\r\n-  `UnitTestFrameworkPkg/Library/GoogleTestLib/googletest <https://github.com/google/googletest/blob/86add13493e5c881d7e4ba77fb91c1f57752b3a4/LICENSE>`__\r\n-  `UnitTestFrameworkPkg/Library/SubhookLib/subhook <https://github.com/tianocore/edk2-subhook/blob/83d4e1ebef3588fae48b69a7352cc21801cb70bc/LICENSE.txt>`__\r\n-  `RedfishPkg/Library/JsonLib/jansson <https://github.com/akheron/jansson/blob/2882ead5bb90cf12a01b07b2c2361e24960fae02/LICENSE>`__\r\n-  `MdePkg/Library/BaseFdtLib/libfdt <https://github.com/devicetree-org/pylibfdt/blob/f39368a217496d32c4091a2dba4045b60649e3a5/BSD-2-Clause>`__\r\n-  `MdePkg/Library/MipiSysTLib/mipisyst <https://github.com/MIPI-Alliance/public-mipi-sys-t/blob/aae857d0d05ac65152ed24992a4acd834a0a107c/LICENSE>`__\r\n-  `SecurityPkg/DeviceSecurity/SpdmLib/libspdm <https://github.com/DMTF/libspdm/blob/main/LICENSE.md>`__\r\n\r\nThe EDK II Project is composed of packages. The maintainers for each package\r\nare listed in `Maintainers.txt <Maintainers.txt>`__.\r\n\r\nResources\r\n---------\r\n\r\n-  `TianoCore <http://www.tianocore.org>`__\r\n-  `EDK\r\n   II <https://github.com/tianocore/tianocore.github.io/wiki/EDK-II>`__\r\n-  `Getting Started with EDK\r\n   II <https://github.com/tianocore/tianocore.github.io/wiki/Getting-Started-with-EDK-II>`__\r\n-  `Mailing\r\n   Lists <https://github.com/tianocore/tianocore.github.io/wiki/Mailing-Lists>`__\r\n-  `How To\r\n   Contribute <https://github.com/tianocore/tianocore.github.io/wiki/How-To-Contribute>`__\r\n-  `Release\r\n   Planning <https://github.com/tianocore/tianocore.github.io/wiki/EDK-II-Release-Planning>`__\r\n\r\nCode Contributions\r\n------------------\r\n\r\nTo make a contribution to a TianoCore project, follow these steps.\r\n\r\n#. Create a change description in the format specified below to\r\n    use in the source control commit log.\r\n#. Your commit message must include your ``Signed-off-by`` signature\r\n#. Submit your code to the TianoCore project using the process\r\n    that the project documents on its web page. If the process is\r\n    not documented, then submit the code on development email list\r\n    for the project.\r\n#. It is preferred that contributions are submitted using the same\r\n    copyright license as the base project. When that is not possible,\r\n    then contributions using the following licenses can be accepted:\r\n\r\n-  Apache License, Version 2.0: https://opensource.org/license/apache-2-0/\r\n-  BSD (2-clause): https://opensource.org/license/BSD-2-Clause\r\n-  BSD (3-clause): https://opensource.org/license/BSD-3-Clause\r\n-  MIT: https://opensource.org/license/MIT\r\n-  Python-2.0: https://opensource.org/license/Python-2.0\r\n-  Zlib: https://opensource.org/license/Zlib\r\n\r\nFor documentation:\r\n\r\n-  FreeBSD Documentation License\r\n    https://www.freebsd.org/copyright/freebsd-doc-license.html\r\n\r\nContributions of code put into the public domain can also be accepted.\r\n\r\nContributions using other licenses might be accepted, but further\r\nreview will be required.\r\n\r\nDeveloper Certificate of Origin\r\n-------------------------------\r\n\r\nYour change description should use the standard format for a\r\ncommit message, and must include your ``Signed-off-by`` signature.\r\n\r\nIn order to keep track of who did what, all patches contributed must\r\ninclude a statement that to the best of the contributor's knowledge\r\nthey have the right to contribute it under the specified license.\r\n\r\nThe test for this is as specified in the `Developer's Certificate of\r\nOrigin (DCO) 1.1 <https://developercertificate.org/>`__. The contributor\r\ncertifies compliance by adding a line saying\r\n\r\nSigned-off-by: Developer Name developer@example.org\r\n\r\nwhere ``Developer Name`` is the contributor's real name, and the email\r\naddress is one the developer is reachable through at the time of\r\ncontributing.\r\n\r\n::\r\n\r\n    Developer's Certificate of Origin 1.1\r\n\r\n    By making a contribution to this project, I certify that:\r\n\r\n    (a) The contribution was created in whole or in part by me and I\r\n        have the right to submit it under the open source license\r\n        indicated in the file; or\r\n\r\n    (b) The contribution is based upon previous work that, to the best\r\n        of my knowledge, is covered under an appropriate open source\r\n        license and I have the right under that license to submit that\r\n        work with modifications, whether created in whole or in part\r\n        by me, under the same open source license (unless I am\r\n        permitted to submit under a different license), as indicated\r\n        in the file; or\r\n\r\n    (c) The contribution was provided directly to me by some other\r\n        person who certified (a), (b) or (c) and I have not modified\r\n        it.\r\n\r\n    (d) I understand and agree that this project and the contribution\r\n        are public and that a record of the contribution (including all\r\n        personal information I submit with it, including my sign-off) is\r\n        maintained indefinitely and may be redistributed consistent with\r\n        this project or the open source license(s) involved.\r\n\r\nSample Change Description / Commit Message\r\n------------------------------------------\r\n\r\n::\r\n\r\n    From: Contributor Name <contributor@example.com>\r\n    Subject: [Repository/Branch PATCH] Pkg-Module: Brief-single-line-summary\r\n\r\n    Full-commit-message\r\n\r\n    Signed-off-by: Contributor Name <contributor@example.com>\r\n\r\nNotes for sample patch email\r\n````````````````````````````\r\n\r\n-  The first line of commit message is taken from the email's subject\r\n   line following ``[Repository/Branch PATCH]``. The remaining portion\r\n   of the commit message is the email's content.\r\n-  ``git format-patch`` is one way to create this format\r\n\r\nDefinitions for sample patch email\r\n``````````````````````````````````\r\n\r\n-  ``Repository`` is the identifier of the repository the patch applies.\r\n    This identifier should only be provided for repositories other than\r\n    ``edk2``. For example ``edk2-BuildSpecification`` or ``staging``.\r\n-  ``Branch`` is the identifier of the branch the patch applies. This\r\n    identifier should only be provided for branches other than\r\n   ``edk2/master``.\r\n    For example ``edk2/UDK2015``,\r\n   ``edk2-BuildSpecification/release/1.27``, or\r\n    ``staging/edk2-test``.\r\n-  ``Module`` is a short identifier for the affected code or\r\n   documentation. For example ``MdePkg``, ``MdeModulePkg/UsbBusDxe``, ``Introduction``, or\r\n    ``EDK II INF File Format``.\r\n-  ``Brief-single-line-summary`` is a short summary of the change.\r\n-  The entire first line should be less than ~70 characters.\r\n-  ``Full-commit-message`` a verbose multiple line comment describing\r\n    the change. Each line should be less than ~70 characters.\r\n-  ``Signed-off-by`` is the contributor's signature identifying them\r\n    by their real/legal name and their email address.\r\n\r\nSubmodules\r\n----------\r\n\r\nThe current submodules used in EDK II are in `.gitmodules <.gitmodules>`__.\r\n\r\nTo get a full, buildable EDK II repository, use following steps of git\r\ncommand\r\n\r\n.. code-block:: bash\r\n\r\n  git clone https://github.com/tianocore/edk2.git\r\n  cd edk2\r\n  git submodule update --init\r\n  cd ..\r\n\r\nIf there's update for submodules, use following git commands to get\r\nthe latest submodules code.\r\n\r\n.. code-block:: bash\r\n\r\n  cd edk2\r\n  git pull\r\n  git submodule update\r\n\r\nNote: When cloning submodule repos, '--recursive' option is not\r\nrecommended. EDK II itself will not use any code/feature from\r\nsubmodules in above submodules. So using '--recursive' adds a\r\ndependency on being able to reach servers we do not actually want\r\nany code from, as well as needlessly downloading code we will not\r\nuse.\r\n\r\n.. ===================================================================\r\n.. This is a bunch of directives to make the README file more readable\r\n.. ===================================================================\r\n\r\n.. CoreCI\r\n\r\n.. _Windows_VS: https://dev.azure.com/tianocore/edk2-ci/_build/latest?definitionId=74&branchName=master\r\n.. |WindowsCiBuild| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FWindows%20VS%20-%20CI?branchName=master\r\n.. |WindowsCiTest| image:: https://img.shields.io/azure-devops/tests/tianocore/edk2-ci/74.svg\r\n.. |WindowsCiCoverage| image:: https://img.shields.io/badge/coverage-coming_soon-blue\r\n\r\n.. _Ubuntu_GCC: https://dev.azure.com/tianocore/edk2-ci/_build/latest?definitionId=76&branchName=master\r\n.. |UbuntuCiBuild| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FUbuntu%20GCC%20-%20CI?branchName=master\r\n.. |UbuntuCiTest| image:: https://img.shields.io/azure-devops/tests/tianocore/edk2-ci/76.svg\r\n.. |UbuntuCiCoverage| image:: https://img.shields.io/badge/coverage-coming_soon-blue\r\n\r\n.. ArmVirtPkg\r\n\r\n.. _ArmVirtPkg_Ubuntu_GCC: https://dev.azure.com/tianocore/edk2-ci/_build/latest?definitionId=79&branchName=master\r\n.. |avAArch64du| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FArmVirtPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20QEMU_AARCH64_DEBUG\r\n.. |avAArch64ru| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FArmVirtPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20QEMU_AARCH64_RELEASE\r\n.. |avAArch64nu| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FArmVirtPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20QEMU_AARCH64_NOOPT\r\n\r\n.. |avArmdu| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FArmVirtPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20QEMU_ARM_DEBUG\r\n.. |avArmru| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FArmVirtPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20QEMU_ARM_RELEASE\r\n.. |avArmnu| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FArmVirtPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20QEMU_ARM_NOOPT\r\n\r\n.. EmulatorPkg\r\n\r\n.. |TCBZ_2639| image:: https://img.shields.io/github/issues/tianocore/edk2?baseUrl=https%3A%2F%2Fgithub.com\r\n.. _TCBZ_2639: https://github.com/tianocore/edk2/issues/9905\r\n\r\n.. _EmulatorPkg_Win_VS:  https://dev.azure.com/tianocore/edk2-ci/_build/latest?definitionId=73&branchName=master\r\n.. _EmulatorPkg_Ubuntu_GCC: https://dev.azure.com/tianocore/edk2-ci/_build/latest?definitionId=78&branchName=master\r\n\r\n.. |em32d| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_IA32_DEBUG\r\n.. |em32r| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_IA32_RELEASE\r\n.. |em32n| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_IA32_NOOPT\r\n\r\n.. |em32fd| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_IA32_FULL_DEBUG\r\n.. |em32fr| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_IA32_FULL_RELEASE\r\n.. |em32fn| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_IA32_FULL_NOOPT\r\n\r\n.. |em64d| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_X64_DEBUG\r\n.. |em64du| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_X64_DEBUG\r\n.. |em64r| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_X64_RELEASE\r\n.. |em64ru| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_X64_RELEASE\r\n.. |em64n| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_X64_NOOPT\r\n.. |em64nu| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_X64_NOOPT\r\n\r\n.. |em64fd| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_X64_FULL_DEBUG\r\n.. |em64fdu| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_X64_FULL_DEBUG\r\n.. |em64fr| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_X64_FULL_RELEASE\r\n.. |em64fru| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_X64_FULL_RELEASE\r\n.. |em64fn| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_X64_FULL_NOOPT\r\n.. |em64fnu| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FEmulatorPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20EmulatorPkg_X64_FULL_NOOPT\r\n\r\n.. OvmfPkg\r\n\r\n.. _OvmfPkg_Win_VS:  https://dev.azure.com/tianocore/edk2-ci/_build/latest?definitionId=72&branchName=master\r\n.. _OvmfPkg_Ubuntu_GCC: https://dev.azure.com/tianocore/edk2-ci/_build/latest?definitionId=77&branchName=master\r\n\r\n.. |op32d| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FOvmfPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20OVMF_IA32_DEBUG\r\n.. |op32du| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FOvmfPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20OVMF_IA32_DEBUG\r\n.. |op32r| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FOvmfPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20OVMF_IA32_RELEASE\r\n.. |op32ru| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FOvmfPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20OVMF_IA32_RELEASE\r\n.. |op32n| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FOvmfPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20OVMF_IA32_DEBUG\r\n.. |op32nu| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FOvmfPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20OVMF_IA32_NOOPT\r\n\r\n.. |op64d| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FOvmfPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20OVMF_X64_DEBUG\r\n.. |op64du| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FOvmfPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20OVMF_X64_DEBUG\r\n.. |op64r| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FOvmfPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20OVMF_X64_RELEASE\r\n.. |op64ru| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FOvmfPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20OVMF_X64_RELEASE\r\n.. |op64n| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FOvmfPkg%20-%20Windows%20VS%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20OVMF_X64_NOOPT\r\n.. |op64nu| image:: https://dev.azure.com/tianocore/edk2-ci/_apis/build/status%2FCI%2FOvmfPkg%20-%20Ubuntu%20GCC%20-%20CI?branchName=master&jobName=Platform_CI&configuration=Platform_CI%20OVMF_X64_NOOPT\r\n",
      "stars_today": 1
    },
    {
      "id": 33100064,
      "name": "yaml-cpp",
      "full_name": "jbeder/yaml-cpp",
      "description": "A YAML parser and emitter in C++",
      "html_url": "https://github.com/jbeder/yaml-cpp",
      "stars": 5788,
      "forks": 2064,
      "language": "C++",
      "topics": [],
      "created_at": "2015-03-30T02:52:32Z",
      "updated_at": "2026-01-14T06:46:11Z",
      "pushed_at": "2026-01-01T16:15:24Z",
      "open_issues": 365,
      "owner": {
        "login": "jbeder",
        "avatar_url": "https://avatars.githubusercontent.com/u/1059334?v=4"
      },
      "readme": "# yaml-cpp ![Build Status](https://github.com/jbeder/yaml-cpp/actions/workflows/build.yml/badge.svg) [![Documentation](https://codedocs.xyz/jbeder/yaml-cpp.svg)](https://codedocs.xyz/jbeder/yaml-cpp/)\n\n`yaml-cpp` is a [YAML](http://www.yaml.org/) parser and emitter in C++ matching the [YAML 1.2 spec](http://www.yaml.org/spec/1.2/spec.html).\n\n## Usage\n\nSee [Tutorial](https://github.com/jbeder/yaml-cpp/wiki/Tutorial) and [How to Emit YAML](https://github.com/jbeder/yaml-cpp/wiki/How-To-Emit-YAML) for reference. For the old API (until 0.5.0), see [How To Parse A Document](https://github.com/jbeder/yaml-cpp/wiki/How-To-Parse-A-Document-(Old-API)).\n\n## Any Problems?\n\nIf you find a bug, post an [issue](https://github.com/jbeder/yaml-cpp/issues)! If you have questions about how to use yaml-cpp, please post it on http://stackoverflow.com and tag it [`yaml-cpp`](http://stackoverflow.com/questions/tagged/yaml-cpp).\n\n## How to Build\n\n`yaml-cpp` uses [CMake](http://www.cmake.org) to support cross-platform building. Install [CMake](http://www.cmake.org) _(Resources -> Download)_ before proceeding. The basic steps to build are:\n\n**Note:** If you don't use the provided installer for your platform, make sure that you add `CMake`'s bin folder to your path.\n\n#### 1. Navigate into the source directory, create build folder and run `CMake`:\n\n```sh\nmkdir build\ncd build\ncmake [-G generator] [-DYAML_BUILD_SHARED_LIBS=on|OFF] ..\n```\n\n  * The `generator` option is the build system you'd like to use. Run `cmake` without arguments to see a full list of available generators.\n    * On Windows, you might use \"Visual Studio 12 2013\" (VS 2013 32-bits), or \"Visual Studio 14 2015 Win64\" (VS 2015 64-bits).\n    * On OS X, you might use \"Xcode\".\n    * On a UNIX-like system, omit the option (for a Makefile).\n\n  * `yaml-cpp` builds a static library by default, you may want to build a shared library by specifying `-DYAML_BUILD_SHARED_LIBS=ON`.\n\n  * [Debug mode of the GNU standard C++\n    library](https://gcc.gnu.org/onlinedocs/libstdc++/manual/debug_mode.html)\n    can be used when both `yaml-cpp` and client code is compiled with the\n    `_GLIBCXX_DEBUG` flag (e.g. by calling CMake with `-D\n    CMAKE_CXX_FLAGS_DEBUG='-g -D_GLIBCXX_DEBUG'` option).\n\n    Note that for `yaml-cpp` unit tests to run successfully, the _GoogleTest_\n    library also must be built with this flag, i.e. the system one cannot be\n    used (the _YAML_USE_SYSTEM_GTEST_ CMake option must be _OFF_, which is the\n    default).\n\n  * For more options on customizing the build, see the [CMakeLists.txt](https://github.com/jbeder/yaml-cpp/blob/master/CMakeLists.txt) file.\n\n#### 2. Build it!\n  * The command you'll need to run depends on the generator you chose earlier.\n\n**Note:** To clean up, just remove the `build` directory.\n\n## How to Integrate it within your project using CMake\n\nYou can use for example FetchContent :\n\n```cmake\ninclude(FetchContent)\n\nFetchContent_Declare(\n  yaml-cpp\n  GIT_REPOSITORY https://github.com/jbeder/yaml-cpp.git\n  GIT_TAG <tag_name> # Can be a tag (yaml-cpp-x.x.x), a commit hash, or a branch name (master)\n)\nFetchContent_MakeAvailable(yaml-cpp)\n\ntarget_link_libraries(YOUR_LIBRARY PUBLIC yaml-cpp::yaml-cpp) # The library or executable that require yaml-cpp library\n```\n\n## Recent Releases\n\n[yaml-cpp 0.8.0](https://github.com/jbeder/yaml-cpp/releases/tag/0.8.0) released!\n\n[yaml-cpp 0.3.0](https://github.com/jbeder/yaml-cpp/releases/tag/release-0.3.0) is still available if you want the old API.\n\n**The old API will stop receiving bugfixes in 2026.** The 0.3.x versions provide the old API, and 0.5.x and above all provide the new API.\n\n# API Documentation \n\nThe autogenerated API reference is hosted on [CodeDocs](https://codedocs.xyz/jbeder/yaml-cpp/index.html)\n\n# Third Party Integrations\n\nThe following projects are not officially supported:\n\n- [Qt wrapper](https://gist.github.com/brcha/d392b2fe5f1e427cc8a6)\n- [UnrealEngine Wrapper](https://github.com/jwindgassen/UnrealYAML)\n",
      "stars_today": 1
    },
    {
      "id": 4729944,
      "name": "shiny",
      "full_name": "rstudio/shiny",
      "description": "Easy interactive web applications with R",
      "html_url": "https://github.com/rstudio/shiny",
      "stars": 5581,
      "forks": 1880,
      "language": "R",
      "topics": [
        "r",
        "reactive",
        "rstudio",
        "shiny",
        "web-app",
        "web-development"
      ],
      "created_at": "2012-06-20T18:45:11Z",
      "updated_at": "2026-01-14T18:25:37Z",
      "pushed_at": "2026-01-12T16:26:11Z",
      "open_issues": 868,
      "owner": {
        "login": "rstudio",
        "avatar_url": "https://avatars.githubusercontent.com/u/513560?v=4"
      },
      "readme": "# shiny <img src=\"man/figures/logo.png\" align=\"right\" width=120 height=139 alt=\"\" />\n\n<!-- badges: start -->\n[![CRAN](https://www.r-pkg.org/badges/version/shiny)](https://CRAN.R-project.org/package=shiny)\n[![R build status](https://github.com/rstudio/shiny/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rstudio/shiny/actions)\n[![RStudio community](https://img.shields.io/badge/community-shiny-blue?style=social&logo=rstudio&logoColor=75AADB)](https://forum.posit.co/new-topic?category=shiny&tags=shiny)\n\n<!-- badges: end -->\n\nEasily build rich and productive interactive web apps in R &mdash; no HTML/CSS/JavaScript required.\n\n## Features\n\n* An intuitive and extensible [reactive programming](https://en.wikipedia.org/wiki/Reactive_programming) model which makes it easy to transform existing R code into a \"live app\" where outputs automatically react to new user input.\n  * Compared to event-based programming, reactivity allows Shiny to do the minimum amount of work when input(s) change, and allows humans to more easily reason about complex [MVC logic](https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller).\n* A prebuilt set of highly sophisticated, customizable, and easy-to-use widgets (e.g., plots, tables, sliders, dropdowns, date pickers, and more).\n* An attractive default look based on [Bootstrap](https://getbootstrap.com/) which can also be easily customized with the [bslib](https://github.com/rstudio/bslib) package or avoided entirely with more direct R bindings to HTML/CSS/JavaScript.\n* Seamless integration with [R Markdown](https://shiny.rstudio.com/articles/interactive-docs.html), making it easy to embed numerous applications natively within a larger dynamic document.\n* Tools for improving and monitoring performance, including native support for [async programming](https://posit.co/blog/shiny-1-1-0/), [caching](https://talks.cpsievert.me/20201117), [load testing](https://rstudio.github.io/shinyloadtest/), and more.\n* [Modules](https://shiny.rstudio.com/articles/modules.html): a framework for reducing code duplication and complexity.\n* An ability to [bookmark application state](https://shiny.rstudio.com/articles/bookmarking-state.html) and/or [generate code to reproduce output(s)](https://github.com/rstudio/shinymeta).\n* A rich ecosystem of extension packages for more [custom widgets](http://www.htmlwidgets.org/), [input validation](https://github.com/rstudio/shinyvalidate), [unit testing](https://github.com/rstudio/shinytest), and more.\n\n## Installation\n\nTo install the stable version from CRAN:\n\n```r\ninstall.packages(\"shiny\")\n```\n\n## Getting Started\n\nOnce installed, load the library and run an example:\n\n```r\nlibrary(shiny)\n# Launches an app, with the app's source code included\nrunExample(\"06_tabsets\")\n# Lists more prepackaged examples\nrunExample()\n```\n\nFor more examples and inspiration, check out the [Shiny User Gallery](https://shiny.rstudio.com/gallery/).\n\nFor help with learning fundamental Shiny programming concepts, check out the [Mastering Shiny](https://mastering-shiny.org/) book and the [Shiny Tutorial](https://shiny.rstudio.com/tutorial/). The former is currently more up-to-date with modern Shiny features, whereas the latter takes a deeper, more visual, dive into fundamental concepts.\n\n## Join the conversation\n\nIf you want to chat about Shiny, meet other developers, or help us decide what to work on next, [join us on Discord](https://discord.com/invite/yMGCamUMnS).\n\n## Getting Help\n\nTo ask a question about Shiny, please use the [RStudio Community website](https://forum.posit.co/new-topic?category=shiny&tags=shiny).\n\nFor bug reports, please use the [issue tracker](https://github.com/rstudio/shiny/issues) and also keep in mind that by [writing a good bug report](https://github.com/rstudio/shiny/wiki/Writing-Good-Bug-Reports), you're more likely to get help with your problem.\n\n## Contributing\n\nWe welcome contributions to the **shiny** package. Please see our [CONTRIBUTING.md](https://github.com/rstudio/shiny/blob/main/.github/CONTRIBUTING.md) file for detailed guidelines of how to contribute.\n\n## License\n\nThe shiny package as a whole is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.\n\n## R version support\n\nShiny is supported on the latest release version of R, as well as the previous four minor release versions of R. For example, if the latest release R version is 4.3, then that version is supported, as well as 4.2, 4.1, 4.0, 3.6.\n",
      "stars_today": 1
    },
    {
      "id": 58194180,
      "name": "strimzi-kafka-operator",
      "full_name": "strimzi/strimzi-kafka-operator",
      "description": "Apache KafkaÂ® running on Kubernetes",
      "html_url": "https://github.com/strimzi/strimzi-kafka-operator",
      "stars": 5644,
      "forks": 1439,
      "language": "Java",
      "topics": [
        "data-stream",
        "data-streaming",
        "data-streams",
        "hacktoberfest",
        "kafka",
        "kafka-connect",
        "kafka-streams",
        "kubernetes",
        "kubernetes-controller",
        "kubernetes-operator",
        "messaging",
        "openshift"
      ],
      "created_at": "2016-05-06T08:52:33Z",
      "updated_at": "2026-01-15T01:02:39Z",
      "pushed_at": "2026-01-15T00:10:24Z",
      "open_issues": 133,
      "owner": {
        "login": "strimzi",
        "avatar_url": "https://avatars.githubusercontent.com/u/34767428?v=4"
      },
      "readme": "[![Strimzi](./documentation/logo/strimzi.png)](https://strimzi.io/)\n\n# Run Apache Kafka on Kubernetes and OpenShift\n\n[![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/strimzi/strimzi-kafka-operator/badge)](https://scorecard.dev/viewer/?uri=github.com/strimzi/strimzi-kafka-operator)\n[![Build Status](https://github.com/strimzi/strimzi-kafka-operator/actions/workflows/build.yml/badge.svg?branch=main)](https://github.com/strimzi/strimzi-kafka-operator/actions/workflows/build.yml?query=branch%3Amain)\n[![GitHub release](https://img.shields.io/github/release/strimzi/strimzi-kafka-operator.svg)](https://github.com/strimzi/strimzi-kafka-operator/releases/latest)\n[![License](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0)\n[![Twitter Follow](https://img.shields.io/twitter/follow/strimziio?style=social)](https://twitter.com/strimziio)\n[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/strimzi-kafka-operator)](https://artifacthub.io/packages/search?repo=strimzi-kafka-operator)\n\nStrimzi provides a way to run an [Apache KafkaÂ®][kafka] cluster on \n[Kubernetes][k8s] or [OpenShift][os] in various deployment configurations.\nSee our [website][strimzi] for more details about the project.\n\n## Quick Starts\n\nTo get up and running quickly, check our [Quick Start for Minikube, OKD (OpenShift Origin) and Kubernetes Kind](https://strimzi.io/quickstarts/). \n\n## Documentation\n\nDocumentation for the current _main_ branch as well as all releases can be found on our [website][strimzi].\n\n## Roadmap\n\nThe roadmap of the Strimzi Operator project is maintained as [GitHub Project](https://github.com/orgs/strimzi/projects/4).\n\n## Getting help\n\nIf you encounter any issues while using Strimzi, you can get help using:\n\n- [#strimzi channel on CNCF Slack](https://slack.cncf.io/)\n- [Strimzi Users mailing list](https://lists.cncf.io/g/cncf-strimzi-users/topics)\n- [GitHub Discussions](https://github.com/strimzi/strimzi-kafka-operator/discussions)\n\n## Strimzi Community Meetings\n\nYou can join our regular community meetings:\n* Thursday 9:00 AM UTC (every 4 weeks) - [convert to your timezone](https://www.thetimezoneconverter.com/?t=8%3A00&tz=UTC)\n* Thursday 4:00 PM UTC (every 4 weeks, offset by 2 weeks from above meeting) - [convert to your timezone](https://www.thetimezoneconverter.com/?t=16%3A00&tz=UTC)\n\nResources:\n* [Meeting minutes, agenda and Zoom link](https://docs.google.com/document/d/1V1lMeMwn6d2x1LKxyydhjo2c_IFANveelLD880A6bYc/edit#heading=h.vgkvn1hr5uor)\n* [Recordings](https://youtube.com/playlist?list=PLpI4X8PMthYfONZopcRd4X_stq1C14Rtn)\n* [Calendar](https://calendar.google.com/calendar/embed?src=c_m9pusj5ce1b4hr8c92hsq50i00%40group.calendar.google.com) ([Subscribe to the calendar](https://calendar.google.com/calendar/u/0?cid=Y19tOXB1c2o1Y2UxYjRocjhjOTJoc3E1MGkwMEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t))\n\n## Contributing\n\nYou can contribute by:\n- Raising issues you find while using Strimzi\n- Fixing issues by opening Pull Requests\n- Improving Strimzi documentation\n- Talking about Strimzi\n\nAll bugs, tasks or enhancements are tracked as [GitHub issues](https://github.com/strimzi/strimzi-kafka-operator/issues). Issues which \nmight be a good start for new contributors are marked with [\"good-start\"](https://github.com/strimzi/strimzi-kafka-operator/labels/good-start)\nlabel.\n\nThe [Dev guide](https://github.com/strimzi/strimzi-kafka-operator/blob/main/development-docs/DEV_GUIDE.md) describes how to build Strimzi.\nBefore submitting a patch, please make sure to understand, how to test your changes before opening a PR [Test guide](https://github.com/strimzi/strimzi-kafka-operator/blob/main/development-docs/TESTING.md).\n\nThe [Documentation Contributor Guide](https://strimzi.io/contributing/guide/) describes how to contribute to Strimzi documentation.\n\nIf you want to get in touch with us first before contributing, you can use:\n\n- [#strimzi channel on CNCF Slack](https://slack.cncf.io/)\n- [Strimzi Dev mailing list](https://lists.cncf.io/g/cncf-strimzi-dev/topics)\n\n## License\nStrimzi is licensed under the [Apache License](./LICENSE), Version 2.0\n\n## Community Testing\n\n### Linux on IBM Z (s390x)\n\n[![Jenkins](https://ibmz-ci.osuosl.org/job/Strimzi_Kafka_Operator_IBMZ_CI/badge/icon)](https://ibmz-ci.osuosl.org/job/Strimzi_Kafka_Operator_IBMZ_CI/)\n\n_Note: This badge represents a community-led initiative and is not officially endorsed by the Strimzi project maintainers._\n\n## Container signatures\n\nFrom the 0.38.0 release, Strimzi containers are signed using the [`cosign` tool](https://github.com/sigstore/cosign).\nStrimzi uses keyless signing since 0.49.0 release. \nTo verify the container, you can run the following command:\n\n```shell\ncosign verify --certificate-identity-regexp='https://github.com/strimzi/.*' \\\n    --certificate-oidc-issuer='https://token.actions.githubusercontent.com' \\\n    quay.io/strimzi/operator:latest\n```\n\nIn case you want to verify containers of older version of Strimzi than 0.49.0, then use our public key:\n\n```\n-----BEGIN PUBLIC KEY-----\nMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAET3OleLR7h0JqatY2KkECXhA9ZAkC\nTRnbE23Wb5AzJPnpevvQ1QUEQQ5h/I4GobB7/jkGfqYkt6Ct5WOU2cc6HQ==\n-----END PUBLIC KEY-----\n```\n\nAnd use it to verify the signature:\n\n```shell\ncosign verify --key strimzi.pub quay.io/strimzi/operator:latest --insecure-ignore-tlog=true\n```\n\n## Software Bill of Materials (SBOM)\n\nFrom the 0.38.0 release, Strimzi publishes the software bill of materials (SBOM) of our containers.\nThe SBOMs are published as an archive with `SPDX-JSON` and `Syft-Table` formats signed using cosign.\nFor releases, they are also pushed into the container registry.\n\nStrimzi uses keyless signing since 0.49.0 release.\nTo verify the SBOM signatures, you can run the following command:\n\n```shell\ncosign verify-blob --bundle <SBOM-file>.bundle \\\n    --certificate-identity-regexp='https://github.com/strimzi/.*' \\\n    --certificate-oidc-issuer='https://token.actions.githubusercontent.com' \\\n    <SBOM-file>\n```\n\nIn case you want to verify SBOM signatures of older version of Strimzi than 0.49.0, then use our public key:\n\n```\n-----BEGIN PUBLIC KEY-----\nMFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAET3OleLR7h0JqatY2KkECXhA9ZAkC\nTRnbE23Wb5AzJPnpevvQ1QUEQQ5h/I4GobB7/jkGfqYkt6Ct5WOU2cc6HQ==\n-----END PUBLIC KEY-----\n```\n\nYou can use it to verify the signature of the SBOM files with the following command:\n\n```shell\ncosign verify-blob --key cosign.pub --bundle <SBOM-file>.bundle --insecure-ignore-tlog=true <SBOM-file>\n```\n\n---\n\nStrimzi is a <a href=\"http://cncf.io\">Cloud Native Computing Foundation</a> incubating project.\n\n![CNCF ><](./documentation/logo/cncf-color.png)\n\n[strimzi]: https://strimzi.io \"Strimzi\"\n[kafka]: https://kafka.apache.org \"Apache Kafka\"\n[k8s]: https://kubernetes.io/ \"Kubernetes\"\n[os]: https://www.openshift.com/ \"OpenShift\"\n",
      "stars_today": 1
    },
    {
      "id": 54298946,
      "name": "camunda",
      "full_name": "camunda/camunda",
      "description": "Process Orchestration Framework",
      "html_url": "https://github.com/camunda/camunda",
      "stars": 3974,
      "forks": 734,
      "language": "Java",
      "topics": [
        "bpmn",
        "grpc",
        "java",
        "microservices",
        "orchestration-framework",
        "workflow",
        "workflow-engine"
      ],
      "created_at": "2016-03-20T03:38:04Z",
      "updated_at": "2026-01-15T00:53:55Z",
      "pushed_at": "2026-01-15T00:49:42Z",
      "open_issues": 2610,
      "owner": {
        "login": "camunda",
        "avatar_url": "https://avatars.githubusercontent.com/u/2443838?v=4"
      },
      "readme": "# Camunda 8 orchestrates complex business processes that span people, systems, and devices\n\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/io.camunda/camunda-zeebe/badge.svg)](https://maven-badges.herokuapp.com/maven-central/io.camunda/camunda-zeebe)\n\nCamunda 8 delivers scalable, on-demand process automation as a service. Camunda 8 is combined with powerful execution engines for BPMN processes and DMN decisions, and paired with tools for collaborative modeling, operations, and analytics.\n\nThis repository contains the [Orchestration cluster](https://docs.camunda.io/docs/next/components/orchestration-cluster/) components of Camunda 8 and Optimize:\n\n* [Zeebe](https://docs.camunda.io/docs/components/zeebe/zeebe-overview/) - The cloud-native process engine of Camunda 8.\n* [Tasklist](https://docs.camunda.io/docs/components/tasklist/introduction-to-tasklist/) - Complete tasks that require human input.\n* [Operate](https://docs.camunda.io/docs/components/operate/operate-introduction/) - Manage, monitor, and troubleshoot your processes.\n* [Identity](https://docs.camunda.io/docs/next/components/identity/identity-introduction/) - Manage integrated authentication and authorization.\n* [Optimize](https://docs.camunda.io/optimize/components/what-is-optimize/) - Improve your processes by identifying constraints in your system.\n\nIn addition, the Camunda 8 stack also includes:\n* [Console](https://docs.camunda.io/docs/components/console/introduction-to-console/) - Configure and deploy clusters with Console.\n* [Web Modeler](https://docs.camunda.io/docs/components/modeler/about-modeler/) - Web Application to model BPMN, DMN, & Forms and deploy or start new instances.\n* [Desktop Modeler](https://docs.camunda.io/docs/components/modeler/desktop-modeler/) - Use Desktop Modeler as a desktop application for modeling BPMN, DMN, and Forms with your local process application project.\n* [Connectors](https://docs.camunda.io/docs/components/connectors/introduction-to-connectors/) - Integrate with an external system by using a Connector.\n* [Management Identity](https://docs.camunda.io/docs/self-managed/identity/what-is-identity/) - Manage authentication, access, and authorization for components outside the Orchestration cluster (Console, Web Modeler, and Optimize)\n\nUsing Camunda 8, you can:\n\n* Define processes visually in [BPMN 2.0](https://www.omg.org/spec/BPMN/2.0.2/)\n* Choose your programming language\n* Deploy with [Docker](https://www.docker.com/) and [Kubernetes](https://kubernetes.io/)\n* Build processes that react to messages from [Kafka](https://kafka.apache.org/) and other message queues\n* Scale horizontally to handle very high throughput\n* Fault tolerance (no relational database required)\n* Export process data for monitoring and analysis\n* Engage with an active community\n\n[Learn more at camunda.com](https://camunda.com/platform/).\n\n## Status\n\nTo learn more about what we're currently working on, check the [GitHub issues](https://github.com/camunda/camunda/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc) and the [latest commits](https://github.com/camunda/camunda/commits/main).\n\n## Helpful Links\n\n* [Releases](https://github.com/camunda/camunda/releases)\n* [Pre-built Docker images](https://hub.docker.com/r/camunda/camunda/tags?page=1&ordering=last_updated)\n* [Building Docker images for other platforms](docs/zeebe/building_docker_images.md)\n* [Blog](https://camunda.com/blog/category/process-automation-as-a-service/)\n* [Documentation Home](https://docs.camunda.io)\n* [Issue Tracker](https://github.com/camunda/camunda/issues)\n* [User Forum](https://forum.camunda.io)\n* [Contribution Guidelines](/CONTRIBUTING.md)\n\n## Documentation\n\nThis repository includes comprehensive documentation in the [`docs/`](docs/) directory. The content is served via a Docusaurus site located in [`monorepo-docs-site/`](monorepo-docs-site/).\n\n### Running the Documentation Site\n\n```bash\ncd monorepo-docs-site\nnpm install\nnpm start\n```\n\nThe documentation site will be available at `http://localhost:3000/camunda/`.\n\n### Adding Documentation\n\n1. Add new markdown files to the [`docs/`](docs/) directory\n2. Update the sidebar configuration in [`monorepo-docs-site/sidebars.js`](monorepo-docs-site/sidebars.js) to include your new documentation file in the appropriate array position.\n   For example:\n\n```js\n// in monorepo-docs-site/sidebars.js\nmodule.exports = {\n  docs: [\n    'introduction',\n    'getting-started',\n    // add your new doc ID here\n    'your-doc-id',\n  ],\n};\n```\n\nFor more details, see the [documentation site README](monorepo-docs-site/README.md).\n\n## Recommended Docs Entries for New Users\n\n* [What is Camunda Platform 8?](https://docs.camunda.io/docs/components/)\n* [Getting Started Tutorial](https://docs.camunda.io/docs/guides/)\n* [Technical Concepts](https://docs.camunda.io/docs/components/concepts/concepts-overview/)\n* [BPMN Processes](https://docs.camunda.io/docs/components/modeler/bpmn/bpmn-primer/)\n* [Installation and Configuration](https://docs.camunda.io/docs/self-managed/setup/overview/)\n* [Java Client](https://docs.camunda.io/docs/apis-tools/java-client/)\n* [Camunda Spring Boot Starter](https://docs.camunda.io/docs/apis-tools/spring-zeebe-sdk/getting-started/)\n\n## Contributing\n\nRead the [Contributions Guide](/CONTRIBUTING.md).\n\n## Code of Conduct\n\nThis project adheres to the [Camunda Code of Conduct](https://camunda.com/events/code-conduct/).\nBy participating, you are expected to uphold this code. Please [report](https://camunda.com/events/code-conduct/reporting-violations/)\nunacceptable behavior as soon as possible.\n\n## Release Lifecycle\n\nPlease refer to our [Release Policy](https://camunda.com/release-policy/) to learn about our release cadence, maintenance periods, etc.\n\n## License\n\nZeebe, Operate, and Tasklist source files are made available under the\n[Camunda License Version 1.0](/licenses/CAMUNDA-LICENSE-1.0.txt) except for the parts listed\nbelow, which are made available under the [Apache License, Version\n2.0](/licenses/APACHE-2.0.txt).  See individual source files for details.\n\nAvailable under the [Apache License, Version 2.0](/licenses/APACHE-2.0.txt):\n- Java Client ([clients/java](/clients/java))\n- Camunda Spring Boot Starter ([camunda-spring-boot-starter](/clients/camunda-spring-boot-starter))\n- Exporter API ([exporter-api](/zeebe/exporter-api))\n- Protocol ([protocol](/zeebe/protocol))\n- Gateway Protocol Implementation ([gateway-protocol-impl](/zeebe/gateway-protocol-impl))\n- BPMN Model API ([bpmn-model](/zeebe/bpmn-model))\n\n### Clarification on gRPC Code Generation\n\nThe Zeebe Gateway Protocol (API) as published in the\n[gateway-protocol](/zeebe/gateway-protocol/src/main/proto/gateway.proto) is licensed\nunder the [Camunda License 1.0](/licenses/CAMUNDA-LICENSE-1.0.txt). Using gRPC tooling to generate stubs for\nthe protocol does not constitute creating a derivative work under the Camunda License 1.0 and no licensing restrictions are imposed on the\nresulting stub code by the Camunda License 1.0.\n",
      "stars_today": 1
    },
    {
      "id": 16146440,
      "name": "rmarkdown",
      "full_name": "rstudio/rmarkdown",
      "description": "Dynamic Documents for R",
      "html_url": "https://github.com/rstudio/rmarkdown",
      "stars": 3006,
      "forks": 996,
      "language": "R",
      "topics": [
        "literate-programming",
        "markdown",
        "pandoc",
        "r",
        "r-package",
        "rmarkdown"
      ],
      "created_at": "2014-01-22T17:25:19Z",
      "updated_at": "2026-01-14T05:40:32Z",
      "pushed_at": "2025-11-26T19:36:51Z",
      "open_issues": 264,
      "owner": {
        "login": "rstudio",
        "avatar_url": "https://avatars.githubusercontent.com/u/513560?v=4"
      },
      "readme": "# rmarkdown <a href=\"https://pkgs.rstudio.com/rmarkdown/\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" /></a>\n\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/rstudio/rmarkdown/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rstudio/rmarkdown/actions/workflows/R-CMD-check.yaml)\n[![CRAN release](https://www.r-pkg.org/badges/version/rmarkdown)](https://cran.r-project.org/package=rmarkdown)\n[![Codecov test coverage](https://codecov.io/gh/rstudio/rmarkdown/branch/main/graph/badge.svg)](https://app.codecov.io/gh/rstudio/rmarkdown?branch=main)\n<!-- badges: end -->\n\n\nThe **rmarkdown** package helps you create dynamic analysis documents that combine code, rendered output (such as figures), and prose. You bring your data, code, and ideas, and R Markdown renders your content into a polished document that can be used to:\n\n- Do data science interactively within the RStudio IDE,\n\n- Reproduce your analyses,\n\n- Collaborate and share code with others, and\n\n- Communicate your results with others.\n\nR Markdown documents can be rendered to many output formats including HTML documents, PDFs, Word files, slideshows, and more, allowing you to focus on the content while R Markdown takes care of your presentation. \n\n## Books\n\n<a href=\"https://bookdown.org/yihui/rmarkdown/\"><img class=\"book\" src=\"https://bookdown.org/yihui/rmarkdown/images/cover.png\" alt=\"R Markdown: The Definitive Guide\" height=\"400\"></a>\n<a href=\"https://bookdown.org/yihui/rmarkdown-cookbook/\"><img class=\"book\" src=\"https://bookdown.org/yihui/rmarkdown-cookbook/images/cover.png\" alt=\"R Markdown Cookbook\" height=\"400\"></a>\n\nSee more about them in [Get Started](https://pkgs.rstudio.com/rmarkdown/articles/rmarkdown.html).\n\n## Installation\n\nThe easiest way to install the **rmarkdown** package is from within the [RStudio IDE](https://posit.co/download/rstudio-desktop/), but you don't need to explicitly install it or load it, as RStudio automatically does both when needed. A recent version of Pandoc (>= 1.12.3) is also required; RStudio also automatically includes this too so you do not need to download Pandoc if you plan to use rmarkdown from the RStudio IDE.\n\nIf you want to use the rmarkdown package outside of RStudio, you can install the package from CRAN as follows:\n\n```r\ninstall.packages(\"rmarkdown\")\n```\n\nIf you want to use the development version of the rmarkdown package (either with or without RStudio), you can install the package from GitHub via the [**pak** package](https://pak.r-lib.org):\n\n```r\n# install.packages(\"pak\")\npak::pak('rstudio/rmarkdown')\n```\n\nIf not using the RStudio IDE, you'll need to install a recent version of Pandoc (>= 1.12.3); see the [Pandoc installation instructions](https://pandoc.org/installing.html) for help.\n\n## Usage\n\nThe easiest way to make a new R Markdown document is from within RStudio. Go to _File > New File > R Markdown_. From the new file wizard, you may:\n\n+ Provide a document title (_optional but recommended_),\n+ Provide an author name (_optional but recommended_),\n+ Select a default output format- HTML is the recommended format for authoring, and you can switch the output format anytime (_required_), \n+ Click **OK** (_required_).\n\nOnce inside your new `.Rmd` file, you should see some boilerplate text that includes code chunks. Use the \"Knit\" button in the RStudio IDE to render the file and preview the output with a single click or use the keyboard shortcut Cmd/Ctrl + Shift + K. \n\nYou can also delete all the text below the YAML frontmatter and fill in your own `.Rmd` by:\n\n+ Adding code chunks (keyboard shortcut: `Ctrl + Alt + I`; OS X: `Cmd + Option + I`),\n+ Writing prose with [Markdown formatting](https://www.markdowntutorial.com/), and\n+ Running each code chunk interactively by clicking the ![The run button](https://rmarkdown.rstudio.com/images/notebook-run-chunk.png) icon within RStudio. \n\nYou can also click \"Knit to HTML\" again to render the full document with all code chunks. For more help getting started in R Markdown, please see the [R Markdown website](https://rmarkdown.rstudio.com/lesson-1.html) or use the **\"Get Started\"** links at the top of this page.\n\n## Getting help\n\nThere are two main places to get help:\n\n1. The [Posit community](https://forum.posit.co/c/quarto-r-markdown/10) is a friendly place to ask any questions about rmarkdown and the R Markdown family of packages.\n\n1. [Stack Overflow](https://stackoverflow.com/questions/tagged/r-markdown) is a great source of answers to common rmarkdown questions. It is also a great place to get help, once you have created a reproducible example that illustrates your problem.\n\n## Code of Conduct\n\nPlease note that the **rmarkdown** project is released with a [Contributor Code of Conduct](https://pkgs.rstudio.com/rmarkdown/CODE_OF_CONDUCT.html). By contributing to this project, you agree to abide by its terms.\n",
      "stars_today": 1
    },
    {
      "id": 80064252,
      "name": "CLI11",
      "full_name": "CLIUtils/CLI11",
      "description": "CLI11 is a command line parser for C++11 and beyond that provides a rich feature set with a simple and intuitive interface.",
      "html_url": "https://github.com/CLIUtils/CLI11",
      "stars": 4083,
      "forks": 421,
      "language": "C++",
      "topics": [
        "cli",
        "cli-parser",
        "cpp11",
        "no-dependencies"
      ],
      "created_at": "2017-01-25T22:29:34Z",
      "updated_at": "2026-01-14T16:13:35Z",
      "pushed_at": "2026-01-02T18:07:31Z",
      "open_issues": 74,
      "owner": {
        "login": "CLIUtils",
        "avatar_url": "https://avatars.githubusercontent.com/u/26603456?v=4"
      },
      "readme": "# CLI11: Command line parser for C++11\n\n![CLI11 Logo](./docs/CLI11_300.png)\n\n[![Build Status Azure][azure-badge]][azure]\n[![Actions Status][actions-badge]][actions-link]\n[![Code Coverage][codecov-badge]][codecov]\n[![Codacy Badge][codacy-badge]][codacy-link]\n[![License: BSD][license-badge]](./LICENSE) [![DOI][doi-badge]][doi-link]\n\n[![Gitter chat][gitter-badge]][gitter]\n[![Latest GHA release][releases-badge]][github releases]\n[![Latest release][repology-badge]][repology]\n[![Conan.io][conan-badge]][conan-link]\n[![Conda Version][conda-badge]][conda-link]\n[![Try CLI11 2.4 online][wandbox-badge]][wandbox-link]\n\n[What's new](./CHANGELOG.md) â€¢ [Documentation][gitbook] â€¢ [API\nReference][api-docs]\n\nCLI11 is a command line parser for C++11 and beyond that provides a rich feature\nset with a simple and intuitive interface.\n\n## Table of Contents\n\n- [CLI11: Command line parser for C++11](#cli11-command-line-parser-for-c11)\n  - [Table of Contents](#table-of-contents)\n  - [Background](#background)\n    - [Introduction](#introduction)\n    - [Why write another CLI parser?](#why-write-another-cli-parser)\n    - [Other parsers](#other-parsers)\n    - [Features not supported by this library](#features-not-supported-by-this-library)\n  - [Install](#install)\n  - [Usage](#usage)\n    - [Adding options](#adding-options)\n      - [Option types](#option-types)\n      - [Example](#example)\n      - [Option options](#option-options)\n      - [Validators](#validators)\n      - [Default Validators](#default-validators)\n      - [Validators that may be disabled ğŸ†•](#validators-that-may-be-disabled-)\n      - [Extra Validators ğŸ†•](#extra-validators-)\n      - [Validator Usage](#validator-usage)\n        - [Transforming Validators](#transforming-validators)\n        - [Validator operations](#validator-operations)\n        - [Custom Validators](#custom-validators)\n        - [Querying Validators](#querying-validators)\n      - [Getting results](#getting-results)\n    - [Subcommands](#subcommands)\n      - [Subcommand options](#subcommand-options)\n      - [Callbacks](#callbacks)\n      - [Option groups](#option-groups)\n    - [Configuration file](#configuration-file)\n    - [Inheriting defaults](#inheriting-defaults)\n    - [Formatting](#formatting)\n    - [Subclassing](#subclassing)\n    - [How it works](#how-it-works)\n    - [Unicode support](#unicode-support)\n      - [Note on using Unicode paths](#note-on-using-unicode-paths)\n    - [Utilities](#utilities)\n    - [Other libraries](#other-libraries)\n  - [API](#api)\n  - [Examples](#examples)\n  - [Contribute](#contribute)\n  - [License](#license)\n\nFeatures that were added in the last released minor version are marked with\n\"ğŸ†•\". Features only available in main are marked with \"ğŸš§\".\n\n## Background\n\n### Introduction\n\nCLI11 provides all the features you expect in a powerful command line parser,\nwith a beautiful, minimal syntax and no dependencies beyond C++11. It is header\nonly, and comes in a single file form for easy inclusion in projects. It is easy\nto use for small projects, but powerful enough for complex command line\nprojects, and can be customized for frameworks. It is tested on [Azure][] and\n[GitHub Actions][actions-link], and was originally used by the [GooFit GPU\nfitting framework][goofit]. It was inspired by [`plumbum.cli`][plumbum] for\nPython. CLI11 has a user friendly introduction in this README, a more in-depth\ntutorial [GitBook][], as well as [API documentation][api-docs] generated by\nTravis. See the [changelog](./CHANGELOG.md) or [GitHub Releases][] for details\nfor current and past releases. Also see the [Version 1.0 post][], [Version 1.3\npost][], [Version 1.6 post][], or [Version 2.0 post][] for more information.\n\nYou can be notified when new releases are made by subscribing to\n<https://github.com/CLIUtils/CLI11/releases.atom> on an RSS reader, like Feedly,\nor use the releases mode of the GitHub watching tool.\n\n### Why write another CLI parser?\n\nAn acceptable CLI parser library should be all of the following:\n\n- Easy to include (i.e., header only, one file if possible, **no external\n  requirements**).\n- Short, simple syntax: This is one of the main reasons to use a CLI parser, it\n  should make variables from the command line nearly as easy to define as any\n  other variables. If most of your program is hidden in CLI parsing, this is a\n  problem for readability.\n- C++11 or better: Should work with GCC 4.8+ (default on CentOS/RHEL 7), Clang\n  3.4+, AppleClang 7+, NVCC 7.0+, or MSVC 2015+.\n- Work on Linux, macOS, and Windows.\n- Well tested on all common platforms and compilers. \"Well\" is defined as having\n  good coverage measured by [CodeCov][].\n- Clear help printing.\n- Nice error messages.\n- Standard shell idioms supported naturally, like grouping flags, a positional\n  separator, etc.\n- Easy to execute, with help, parse errors, etc. providing correct exit and\n  details.\n- Easy to extend as part of a framework that provides \"applications\" to users.\n- Usable subcommand syntax, with support for multiple subcommands, nested\n  subcommands, option groups, and optional fallthrough (explained later).\n- Ability to add a configuration file (`TOML`, `INI`, or custom format), and\n  produce it as well.\n- Produce real values that can be used directly in code, not something you have\n  pay compute time to look up, for HPC applications.\n- Work with common types, simple custom types, and extensible to exotic types.\n- Permissively licensed.\n\n### Other parsers\n\n<details><summary>The major CLI parsers for C++ include, with my biased opinions: (click to expand)</summary><p>\n\n| Library                             | My biased opinion                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| ----------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [Boost Program Options][]           | A great library if you already depend on Boost, but its pre-C++11 syntax is really odd and setting up the correct call in the main function is poorly documented (and is nearly a page of code). A simple wrapper for the Boost library was originally developed, but was discarded as CLI11 became more powerful. The idea of capturing a value and setting it originated with Boost PO. [See this comparison.][cli11-po-compare] |\n| [The Lean Mean C++ Option Parser][] | One header file is great, but the syntax is atrocious, in my opinion. It was quite impractical to wrap the syntax or to use in a complex project. It seems to handle standard parsing quite well.                                                                                                                                                                                                                                  |\n| [TCLAP][]                           | The not-quite-standard command line parsing causes common shortcuts to fail. It also seems to be poorly supported, with only minimal bugfixes accepted. Header only, but in quite a few files. Has not managed to get enough support to move to GitHub yet. No subcommands. Produces wrapped values.                                                                                                                               |\n| [Cxxopts][]                         | C++11, single file, and nice CMake support, but requires regex, therefore GCC 4.8 (CentOS 7 default) does not work. Syntax closely based on Boost PO, so not ideal but familiar.                                                                                                                                                                                                                                                   |\n| [DocOpt][]                          | Completely different approach to program options in C++11, you write the docs and the interface is generated. Too fragile and specialized.                                                                                                                                                                                                                                                                                         |\n\nAfter I wrote this, I also found the following libraries:\n\n| Library                 | My biased opinion                                                                                                                                                                    |\n| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [GFlags][]              | The Google Commandline Flags library. Uses macros heavily, and is limited in scope, missing things like subcommands. It provides a simple syntax and supports config files/env vars. |\n| [GetOpt][]              | Very limited C solution with long, convoluted syntax. Does not support much of anything, like help generation. Always available on UNIX, though (but in different flavors).          |\n| [ProgramOptions.hxx][]  | Interesting library, less powerful and no subcommands. Nice callback system.                                                                                                         |\n| [Args][]                | Also interesting, and supports subcommands. I like the optional-like design, but CLI11 is cleaner and provides direct value access, and is less verbose.                             |\n| [Argument Aggregator][] | I'm a big fan of the [fmt][] library, and the try-catch statement looks familiar. :thumbsup: Doesn't seem to support subcommands.                                                    |\n| [Clara][]               | Simple library built for the excellent [Catch][] testing framework. Unique syntax, limited scope.                                                                                    |\n| [Argh!][]               | Very minimalistic C++11 parser, single header. Don't have many features. No help generation?!?! At least it's exception-free.                                                        |\n| [CLI][]                 | Custom language and parser. Huge build-system overkill for very little benefit. Last release in 2009, but still occasionally active.                                                 |\n| [argparse][]            | C++17 single file argument parser. Design seems similar to CLI11 in some ways. The author has several other interesting projects.                                                    |\n| [lyra][]                | a simple header only parser with composable options. Might work well for simple standardized parsing                                                                                 |\n\nSee [Awesome C++][] for a less-biased list of parsers. You can also find other\nsingle file libraries at [Single file libs][].\n\n</p></details>\n<br/>\n\nNone of these libraries fulfill all the above requirements, or really even come\nclose. As you probably have already guessed, CLI11 does. So, this library was\ndesigned to provide a great syntax, good compiler compatibility, and minimal\ninstallation fuss.\n\n### Features not supported by this library\n\nThere are some other possible \"features\" that are intentionally not supported by\nthis library:\n\n- Completion of partial options, such as Python's `argparse` supplies for\n  incomplete arguments. It's better not to guess. Most third party command line\n  parsers for python actually reimplement command line parsing rather than using\n  argparse because of this perceived design flaw (recent versions do have an\n  option to disable it). Recent releases of CLI11 do include partial option\n  matching for option prefixes ğŸ†•. This is enabled by\n  `.allow_subcommand_prefix_matching()`, along with an example that generates\n  suggested close matches.\n- Autocomplete: This might eventually be added to both Plumbum and CLI11, but it\n  is not supported yet.\n- While not recommended, CLI11 does now support non standard option names such\n  as `-option`. This is enabled through `allow_non_standard_option_names()`\n  modifier on an app.\n\n## Install\n\nTo use, the most common methods are described here additional methods and\ndetails are available at [installation][]:\n\n- All-in-one local header: Copy `CLI11.hpp` from the [most recent\n  release][github releases] into your include directory, and you are set. This\n  is combined from the source files for every release. This includes the entire\n  command parser library, but does not include separate utilities (like `Timer`,\n  `AutoTimer`). The utilities are completely self contained and can be copied\n  separately.\n- All-in-one global header: Like above, but copying the file to a shared folder\n  location like `/opt/CLI11`. Then, the C++ include path has to be extended to\n  point at this folder. With CMake 3.10+, use `include_directories(/opt/CLI11)`\n- For other methods including using CMake, conan or vcpkg and some specific\n  instructions for GCC 8 or WASI see [installation][].\n\n## Usage\n\n### Adding options\n\nTo set up, add options, and run, your main function will look something like\nthis:\n\n```cpp\nint main(int argc, char** argv) {\n    CLI::App app{\"App description\"};\n    argv = app.ensure_utf8(argv);\n\n    std::string filename = \"default\";\n    app.add_option(\"-f,--file\", filename, \"A help string\");\n\n    CLI11_PARSE(app, argc, argv);\n    return 0;\n}\n```\n\nWhen adding options the names should not conflict with each other, if an option\nis added, or a modifier changed that would cause naming conflicts a run time\nerror will be thrown in the add_option method. This includes default options for\nhelp `-h, --help`. For more information about `ensure_utf8` the section on\n[Unicode support](#unicode-support) below.\n\n<details><summary>Note: If you don't like macros, this is what that macro expands to: (click to expand)</summary><p>\n\n```cpp\ntry {\n    app.parse(argc, argv);\n} catch (const CLI::ParseError &e) {\n    return app.exit(e);\n}\n```\n\nThe try/catch block ensures that `-h,--help` or a parse error will exit with the\ncorrect return code (selected from `CLI::ExitCodes`). (The return here should be\ninside `main`). You should not assume that the option values have been set\ninside the catch block; for example, help flags intentionally short-circuit all\nother processing for speed and to ensure required options and the like do not\ninterfere.\n\n</p></details>\n</br>\n\nThe initialization is just one line, adding options is just two each. The parse\nmacro is just one line (or 5 for the contents of the macro). After the app runs,\nthe filename will be set to the correct value if it was passed, otherwise it\nwill be set to the default. You can check to see if this was passed on the\ncommand line with `app.count(\"--file\")`.\n\n#### Option types\n\nWhile all options internally are the same type, there are several ways to add an\noption depending on what you need. The supported values are:\n\n```cpp\n// Add options\napp.add_option(option_name, help_str=\"\")\n\napp.add_option(option_name,\n               variable_to_bind_to, // bool, char(see note), int, float, vector, enum, std::atomic, or string-like, or anything with a defined conversion from a string or that takes an int, double, or string in a constructor. Also allowed are tuples, std::array or std::pair. Also supported are complex numbers, wrapper types, and containers besides vectors of any other supported type.\n               help_string=\"\")\n\napp.add_option_function<type>(option_name,\n               function <void(const type &value)>, // type can be any type supported by add_option\n               help_string=\"\")\n\n// char as an option type is supported before 2.0 but in 2.0 it defaulted to allowing single non numerical characters in addition to the numeric values.\n\n// There is a template overload which takes two template parameters the first is the type of object to assign the value to, the second is the conversion type.  The conversion type should have a known way to convert from a string, such as any of the types that work in the non-template version.  If XC is a std::pair and T is some non pair type.  Then a two argument constructor for T is called to assign the value.  For tuples or other multi element types, XC must be a single type or a tuple like object of the same size as the assignment type\napp.add_option<typename T, typename XC>(option_name,\n               T &output, // output must be assignable or constructible from a value of type XC\n               help_string=\"\")\n\n// Add flags\napp.add_flag(option_name,\n             help_string=\"\")\n\napp.add_flag(option_name,\n             variable_to_bind_to, // bool, int, float, complex, containers, enum, std::atomic, or string-like, or any singular object with a defined conversion from a string like add_option\n             help_string=\"\")\n\napp.add_flag_function(option_name,\n             function <void(std::int64_t count)>,\n             help_string=\"\")\n\napp.add_flag_callback(option_name,function<void(void)>,help_string=\"\")\n\n// Add subcommands\nApp* subcom = app.add_subcommand(name, description);\n\nOption_group *app.add_option_group(name,description);\n```\n\nAn option name may start with any character except ('-', ' ', '\\n', and '!').\nFor long options, after the first character all characters are allowed except\n('=',':','{',' ', '\\n'). For the `add_flag*` functions '{' and '!' have special\nmeaning which is why they are not allowed. Names are given as a comma separated\nstring, with the dash or dashes. An option or flag can have as many names as you\nwant, and afterward, using `count`, you can use any of the names, with dashes as\nneeded, to count the options. One of the names is allowed to be given without\nproceeding dash(es); if present the option is a positional option, and that name\nwill be used on the help line for its positional form. The string `++` is also\nnot allowed as option name due to its use as an array separator and marker on\nconfig files.\n\nThe `add_option_function<type>(...` function will typically require the template\nparameter be given unless a `std::function` object with an exact match is\npassed. The type can be any type supported by the `add_option` function. The\nfunction should throw an error (`CLI::ConversionError` or `CLI::ValidationError`\npossibly) if the value is not valid.\n\nThe two parameter template overload can be used in cases where you want to\nrestrict the input such as\n\n```cpp\ndouble val\napp.add_option<double,unsigned int>(\"-v\",val);\n```\n\nwhich would first verify the input is convertible to an `unsigned int` before\nassigning it. Or using some variant type\n\n```cpp\nusing vtype=std::variant<int, double, std::string>;\n vtype v1;\napp.add_option<vtype,std::string>(\"--vs\",v1);\napp.add_option<vtype,int>(\"--vi\",v1);\napp.add_option<vtype,double>(\"--vf\",v1);\n```\n\notherwise the output would default to a string. The `add_option` can be used\nwith any integral or floating point types, enumerations, or strings. Or any type\nthat takes an int, double, or std\\::string in an assignment operator or\nconstructor. If an object can take multiple varieties of those, std::string\ntakes precedence, then double then int. To better control which one is used or\nto use another type for the underlying conversions use the two parameter\ntemplate to directly specify the conversion type.\n\nTypes such as (std or boost) `optional<int>`, `optional<double>`, and\n`optional<string>` and any other wrapper types are supported directly. For\npurposes of CLI11 wrapper types are those which `value_type` definition. See\n[CLI11 Advanced Topics/Custom Converters][] for information on how you can add\nyour own converters for additional types.\n\nVector types can also be used in the two parameter template overload\n\n```cpp\nstd::vector<double> v1;\napp.add_option<std::vector<double>,int>(\"--vs\",v1);\n```\n\nwould load a vector of doubles but ensure all values can be represented as\nintegers.\n\nAutomatic direct capture of the default string is disabled when using the two\nparameter template. Use `set_default_str(...)` or\n`->default_function(std::string())` to set the default string or capture\nfunction directly for these cases.\n\nFlag options specified through the `add_flag*` functions allow a syntax for the\noption names to default particular options to a false value or any other value\nif some flags are passed. For example:\n\n```cpp\napp.add_flag(\"--flag,!--no-flag\",result,\"help for flag\");\n```\n\nspecifies that if `--flag` is passed on the command line result will be true or\ncontain a value of 1. If `--no-flag` is passed `result` will contain false or -1\nif `result` is a signed integer type, or 0 if it is an unsigned type. An\nalternative form of the syntax is more explicit: `\"--flag,--no-flag{false}\"`;\nthis is equivalent to the previous example. This also works for short form\noptions `\"-f,!-n\"` or `\"-f,-n{false}\"`. If `variable_to_bind_to` is anything but\nan integer value the default behavior is to take the last value given, while if\n`variable_to_bind_to` is an integer type the behavior will be to sum all the\ngiven arguments and return the result. This can be modified if needed by\nchanging the `multi_option_policy` on each flag (this is not inherited). The\ndefault value can be any value. For example if you wished to define a numerical\nflag:\n\n```cpp\napp.add_flag(\"-1{1},-2{2},-3{3}\",result,\"numerical flag\")\n```\n\nUsing any of those flags on the command line will result in the specified number\nin the output. Similar things can be done for string values, and enumerations,\nas long as the default value can be converted to the given type.\n\nOn a `C++14` compiler, you can pass a callback function directly to `.add_flag`,\nwhile in C++11 mode you'll need to use `.add_flag_function` if you want a\ncallback function. The function will be given the number of times the flag was\npassed. You can throw a relevant `CLI::ParseError` to signal a failure.\n\n#### Example\n\n- `\"one,-o,--one\"`: Valid as long as not a flag, would create an option that can\n  be specified positionally, or with `-o` or `--one`\n- `\"this\"` Can only be passed positionally\n- `\"-a,-b,-c\"` No limit to the number of non-positional option names\n\nThe add commands return a pointer to an internally stored `Option`. This option\ncan be used directly to check for the count (`->count()`) after parsing to avoid\na string based lookup.\n\n#### Option options\n\nBefore parsing, you can set the following options:\n\n- `->required()`: The program will quit if this option is not present. This is\n  `mandatory` in Plumbum, but required options seems to be a more standard term.\n  For compatibility, `->mandatory()` also works.\n- `->expected(N)`: Take `N` values instead of as many as possible, only for\n  vector args. If negative, require at least `-N`; end with `--` or another\n  recognized option or subcommand.\n- `->expected(MIN,MAX)`: Set a range of expected values to accompany an option.\n  `expected(0,1)` is the equivalent of making a flag.\n- `->type_name(typename)`: Set the name of an Option's type (`type_name_fn`\n  allows a function instead)\n- `->type_size(N)`: Set the intrinsic size of an option value. The parser will\n  require multiples of this number if negative. Most of the time this is\n  detected automatically though can be modified for specific use cases.\n- `->type_size(MIN,MAX)`: Set the intrinsic size of an option to a range.\n- `->needs(opt)`: This option requires another option to also be present, opt is\n  an `Option` pointer. Options can be removed from the `needs` with\n  `remove_needs(opt)`. The option can also be specified with a string containing\n  the name of the option\n- `->excludes(opt)`: This option cannot be given with `opt` present, opt is an\n  `Option` pointer. Can also be given as a string containing the name of the\n  option. Options can be removed from the excludes list with\n  `->remove_excludes(opt)`\n- `->envname(name)`: Gets the value from the environment if present and not\n  passed on the command line. The value must also pass any validators to be\n  used.\n- `->group(name)`: The help group to put the option in. No effect for positional\n  options. Defaults to `\"Options\"`. Options given an empty string will not show\n  up in the help print (hidden).\n- `->ignore_case()`: Ignore the case on the command line (also works on\n  subcommands, does not affect arguments).\n- `->ignore_underscore()`: Ignore any underscores in the options names (also\n  works on subcommands, does not affect arguments). For example \"option_one\"\n  will match with \"optionone\". This does not apply to short form options since\n  they only have one character\n- `->disable_flag_override()`: From the command line long form flag options can\n  be assigned a value on the command line using the `=` notation `--flag=value`.\n  If this behavior is not desired, the `disable_flag_override()` disables it and\n  will generate an exception if it is done on the command line. The `=` does not\n  work with short form flag options.\n- `->allow_extra_args(true/false)`: If set to true the option will take an\n  unlimited number of arguments like a vector, if false it will limit the number\n  of arguments to the size of the type used in the option. Default value depends\n  on the nature of the type use, containers default to true, others default to\n  false.\n- `->delimiter(char)`: Allows specification of a custom delimiter for separating\n  single arguments into vector arguments, for example specifying\n  `->delimiter(',')` on an option would result in `--opt=1,2,3` producing 3\n  elements of a vector and the equivalent of --opt 1 2 3 assuming opt is a\n  vector value.\n- `->description(str)`: Set/change the description.\n- `->multi_option_policy(CLI::MultiOptionPolicy::Throw)`: Set the multi-option\n  policy. Shortcuts available: `->take_last()`, `->take_first()`,`->take_all()`,\n  and `->join()`. This will only affect options expecting 1 argument or bool\n  flags (which do not inherit their default but always start with a specific\n  policy). `->join(delim)` can also be used to join with a specific delimiter.\n  This equivalent to calling `->delimiter(delim)` and `->join()`. Valid values\n  are `CLI::MultiOptionPolicy::Throw`, `CLI::MultiOptionPolicy::Throw`,\n  `CLI::MultiOptionPolicy::TakeLast`, `CLI::MultiOptionPolicy::TakeFirst`,\n  `CLI::MultiOptionPolicy::Join`, `CLI::MultiOptionPolicy::TakeAll`,\n  `CLI::MultiOptionPolicy::Sum`, and `CLI::MultiOptionPolicy::Reverse`.\n- `->check(std::string(const std::string &), validator_name=\"\",validator_description=\"\")`:\n  Define a check function. The function should return a non empty string with\n  the error message if the check fails\n- `->check(Validator)`: Use a Validator object to do the check see\n  [Validators](#validators) for a description of available Validators and how to\n  create new ones.\n- `->transform(std::string(std::string &), validator_name=\"\",validator_description=\")`:\n  Converts the input string into the output string, in-place in the parsed\n  options.\n- `->transform(Validator)`: Uses a Validator object to do the transformation see\n  [Validators](#validators) for a description of available Validators and how to\n  create new ones.\n- `->each(void(const std::string &)>`: Run this function on each value received,\n  as it is received. It should throw a `ValidationError` if an error is\n  encountered.\n- `->configurable(false)`: Disable this option from being in a configuration\n  file.\n- `->capture_default_str()`: Store the current value attached and display it in\n  the help string. This should work with almost any type that `add_option` can\n  accept.\n- `->default_function(std::string())`: Advanced: Change the function that\n  `capture_default_str()` uses.\n- `->always_capture_default()`: Always run `capture_default_str()` when creating\n  new options. Only useful on an App's `option_defaults`.\n- `->default_str(string)`: Set the default string directly (NO VALIDATION OR\n  CALLBACKS). This string will also be used as a default value if no arguments\n  are passed and the value is requested.\n- `->default_val(value)`: Generate the default string from a value and validate\n  that the value is also valid. For options that assign directly to a value type\n  the value in that type is also updated. Value must be convertible to a\n  string(one of known types or have a stream operator). The callback may be\n  triggered if the `run_callback_for_default` is set.\n- `->run_callback_for_default()`: This will force the option callback to be\n  executed or the variable set when the `default_val` is set.\n- `->option_text(string)`: Sets the text between the option name and\n  description.\n- `->force_callback()`: Causes the option callback or value set to be triggered\n  even if the option was not present in parsing.\n- `->trigger_on_parse()`: If set, causes the callback and all associated\n  validation checks for the option to be executed when the option value is\n  parsed vs. at the end of all parsing. This could cause the callback to be\n  executed multiple times. Also works with positional options.\n- `->callback_priority(CallbackPriority priority)`: ğŸ†• changes the order in\n  which the option callback is executed. Four principal callback call-points are\n  available. `CallbackPriority::First` executes at the very beginning of\n  processing, before configuration files are read and environment variables are\n  interpreted. `CallbackPriority::PreRequirementsCheck` executes after\n  configuration and environment processing but before requirements checking.\n  `CallbackPriority::Normal` executes after the requirements check but before\n  any previously potentially raised exceptions are re-thrown.\n  `CallbackPriority::Last` executes after exception handling is completed. For\n  each position, both ordinary option callbacks and help callbacks are invoked.\n  The relative order between them can be controlled using the corresponding\n  `PreHelp` variants. `CallbackPriority::FirstPreHelp` executes ordinary option\n  callbacks before help callbacks at the very beginning of processing.\n  `CallbackPriority::PreRequirementsCheckPreHelp` executes ordinary option\n  callbacks before help callbacks after configuration and environment processing\n  but before requirements checking. `CallbackPriority::NormalPreHelp` executes\n  ordinary option callbacks before help callbacks after the requirements check\n  but before exception re-throwing. `CallbackPriority::LastPreHelp` executes\n  ordinary option callbacks before help callbacks after exception handling has\n  completed. When using the standard priorities (`CallbackPriority::First`,\n  `CallbackPriority::PreRequirementsCheck`, `CallbackPriority::Normal`,\n  `CallbackPriority::Last`), help callbacks are executed before ordinary option\n  callbacks. By default, help callbacks use `CallbackPriority::First`, and\n  ordinary option callbacks use `CallbackPriority::Normal`. This mechanism\n  provides fine-grained control over when option values are set and when help or\n  requirement checks occur, enabling precise customization of the processing\n  sequence.\n\nThese options return the `Option` pointer, so you can chain them together, and\neven skip storing the pointer entirely. The `each` function takes any function\nthat has the signature `void(const std::string&)`; it should throw a\n`ValidationError` when validation fails. The help message will have the name of\nthe parent option prepended. Since `each`, `check` and `transform` use the same\nunderlying mechanism, you can chain as many as you want, and they will be\nexecuted in order. Operations added through `transform` are executed first in\nreverse order of addition, and `check` and `each` are run following the\ntransform functions in order of addition. If you just want to see the\nunconverted values, use `.results()` to get the `std::vector<std::string>` of\nresults.\n\nOn the command line, options can be given as:\n\n- `-a` (flag)\n- `-abc` (flags can be combined)\n- `-f filename` (option)\n- `-ffilename` (no space required)\n- `-abcf filename` (flags and option can be combined)\n- `--long` (long flag)\n- `--long_flag=true` (long flag with equals -- to override default value)\n- `--file filename` (space)\n- `--file=filename` (equals)\n\nIf `allow_windows_style_options()` is specified in the application or subcommand\noptions can also be given as:\n\n- `/a` (flag)\n- `/f filename` (option)\n- `/long` (long flag)\n- `/file filename` (space)\n- `/file:filename` (colon)\n- `/long_flag:false` (long flag with : to override the default value)\n  - Windows style options do not allow combining short options or values not\n    separated from the short option like with `-` options\n\nLong flag options may be given with an `=<value>` to allow specifying a false\nvalue, or some other value to the flag. See [config files](#configuration-file)\nfor details on the values supported. NOTE: only the `=` or `:` for windows-style\noptions may be used for this, using a space will result in the argument being\ninterpreted as a positional argument. This syntax can override the default\nvalues, and can be disabled by using `disable_flag_override()`.\n\nExtra positional arguments will cause the program to exit, so at least one\npositional option with a vector is recommended if you want to allow extraneous\narguments. If you set `.allow_extras()` on the main `App`, you will not get an\nerror. You can access the missing options using `remaining` (if you have\nsubcommands, `app.remaining(true)` will get all remaining options, subcommands\nincluded). If the remaining arguments are to processed by another `App` then the\nfunction `remaining_for_passthrough()` can be used to get the remaining\narguments in reverse order such that `app.parse(vector)` works directly and\ncould even be used inside a subcommand callback.\n\nYou can access a vector of pointers to the parsed options in the original order\nusing `parse_order()`. If `--` is present in the command line that does not end\nan unlimited option, then everything after that is positional only.\n\n#### Validators\n\nValidators are structures to check or modify inputs, they can be used to verify\nthat an input meets certain criteria or transform it into another value. They\nare added through the `check` or `transform` functions. The differences between\nthe two function are that checks do not modify the input whereas transforms can\nand are executed before any Validators added through `check`.\n\nCLI11 has several Validators included that perform some common checks. By\ndefault the most commonly used ones are available. ğŸ†• If some are not needed\nthey can be disabled by using\n\n```c++\n#define CLI11_DISABLE_EXTRA_VALIDATORS 1\n```\n\n#### Default Validators\n\nThese validators are always available regardless of definitions. These are used\ninternally or are very commonly used, so will always remain available regardless\nof flags.\n\n- `CLI::ExistingFile`: Requires that the file exists if given.\n- `CLI::ExistingDirectory`: Requires that the directory exists.\n- `CLI::ExistingPath`: Requires that the path (file or directory) exists.\n- `CLI::NonexistentPath`: Requires that the path does not exist.\n- `CLI::FileOnDefaultPath`: Best used as a transform, Will check that a file\n  exists either directly or in a default path and update the path appropriately.\n  See [Transforming Validators](#transforming-validators) for more details\n- `CLI::Range(min,max)`: Requires that the option be between min and max (make\n  sure to use floating point if needed). Min defaults to 0.\n- `CLI::PositiveNumber`: Requires the number be greater than 0\n- `CLI::NonNegativeNumber`: Requires the number be greater or equal to 0\n- `CLI::Number`: Requires the input be a number.\n\n#### Validators that may be disabled ğŸ†•\n\nValidators that may be disabled by setting `CLI11_DISABLE_EXTRA_VALIDATORS` to 1\nor enabled by setting `CLI11_ENABLE_EXTRA_VALIDATORS` to 1. By default they are\nenabled. In version 3.0 these will likely move to be disabled by default and be\ncontrolled solely by the `CLI11_ENABLE_EXTRA_VALIDATORS` option. These\nvalidators are less commonly used or are template heavy and require additional\ncomputation time that may not be valuable for some use cases.\n\n- `CLI::IsMember(...)`: Require an option be a member of a given set. See\n  [Transforming Validators](#transforming-validators) for more details.\n- `CLI::Transformer(...)`: Modify the input using a map. See\n  [Transforming Validators](#transforming-validators) for more details.\n- `CLI::CheckedTransformer(...)`: Modify the input using a map, and require that\n  the input is either in the set or already one of the outputs of the set. See\n  [Transforming Validators](#transforming-validators) for more details.\n- `CLI::AsNumberWithUnit(...)`: Modify the `<NUMBER> <UNIT>` pair by matching\n  the unit and multiplying the number by the corresponding factor. It can be\n  used as a base for transformers, that accept things like size values (`1 KB`)\n  or durations (`0.33 ms`).\n- `CLI::AsSizeValue(...)`: Convert inputs like `100b`, `42 KB`, `101 Mb`,\n  `11 Mib` to absolute values. `KB` can be configured to be interpreted as 10^3\n  or 2^10.\n\n- `CLI::Bounded(min,max)`: Modify the input such that it is always between min\n  and max (make sure to use floating point if needed). Min defaults to 0. Will\n  produce an error if conversion is not possible.\n\n- `CLI::ValidIPV4`: Requires that the option be a valid IPv4 string e.g.\n  `'255.255.255.255'`, `'10.1.1.7'`.\n- `CLI::TypeValidator<TYPE>`:Requires that the option be convertible to the\n  specified type e.g. `CLI::TypeValidator<unsigned int>()` would require that\n  the input be convertible to an `unsigned int` regardless of the end\n  conversion.\n\n#### Extra Validators ğŸ†•\n\nNew validators will go into code sections that must be explicitly enabled by\nsetting `CLI11_ENABLE_EXTRA_VALIDATORS` to 1\n\n- `CLI::ReadPermission`: Requires that the file or folder given exist and have\n  read permission. Requires C++17.\n- `CLI::WritePermission`: Requires that the file or folder given exist and have\n  write permission. Requires C++17.\n- `CLI::ExecPermission`: Requires that the file given exist and have execution\n  permission. Requires C++17.\n\n#### Validator Usage\n\nThese Validators once enabled can be used by simply passing the name into the\n`check` or `transform` methods on an option\n\n```cpp\n->check(CLI::ExistingFile);\n->check(CLI::Range(0,10));\n```\n\nValidators can be merged using `&` and `|` and inverted using `!`. For example:\n\n```cpp\n->check(CLI::Range(0,10)|CLI::Range(20,30));\n```\n\nwill produce a check to ensure a value is between 0 and 10 or 20 and 30.\n\n```cpp\n->check(!CLI::PositiveNumber);\n```\n\nwill produce a check for a number less than or equal to 0.\n\n##### Transforming Validators\n\nThere are a few built in Validators that let you transform values if used with\nthe `transform` function. If they also do some checks then they can be used\n`check` but some may do nothing in that case.\n\n- `CLI::Bounded(min,max)` will bound values between min and max and values\n  outside of that range are limited to min or max, it will fail if the value\n  cannot be converted and produce a `ValidationError`\n- The `IsMember` Validator lets you specify a set of predefined options. You can\n  pass any container or copyable pointer (including `std::shared_ptr`) to a\n  container to this Validator; the container just needs to be iterable and have\n  a `::value_type`. The key type should be convertible from a string, You can\n  use an initializer list directly if you like. If you need to modify the set\n  later, the pointer form lets you do that; the type message and check will\n  correctly refer to the current version of the set. The container passed in can\n  be a set, vector, or a map like structure. If used in the `transform` method\n  the output value will be the matching key as it could be modified by filters.\n\nAfter specifying a set of options, you can also specify \"filter\" functions of\nthe form `T(T)`, where `T` is the type of the values. The most common choices\nprobably will be `CLI::ignore_case` an `CLI::ignore_underscore`, and\n`CLI::ignore_space`. These all work on strings but it is possible to define\nfunctions that work on other types. Here are some examples of `IsMember`:\n\n- `CLI::IsMember({\"choice1\", \"choice2\"})`: Select from exact match to choices.\n- `CLI::IsMember({\"choice1\", \"choice2\"}, CLI::ignore_case, CLI::ignore_underscore)`:\n  Match things like `Choice_1`, too.\n- `CLI::IsMember(std::set<int>({2,3,4}))`: Most containers and types work; you\n  just need `std::begin`, `std::end`, and `::value_type`.\n- `CLI::IsMember(std::map<std::string, TYPE>({{\"one\", 1}, {\"two\", 2}}))`: You\n  can use maps; in `->transform()` these replace the matched value with the\n  matched key. The value member of the map is not used in `IsMember`, so it can\n  be any type.\n- `auto p = std::make_shared<std::vector<std::string>>(std::initializer_list<std::string>(\"one\", \"two\")); CLI::IsMember(p)`:\n  You can modify `p` later.\n- The `Transformer` and `CheckedTransformer` Validators transform one value into\n  another. Any container or copyable pointer (including `std::shared_ptr`) to a\n  container that generates pairs of values can be passed to these `Validator's`;\n  the container just needs to be iterable and have a `::value_type` that\n  consists of pairs. The key type should be convertible from a string, and the\n  value type should be convertible to a string You can use an initializer list\n  directly if you like. If you need to modify the map later, the pointer form\n  lets you do that; the description message will correctly refer to the current\n  version of the map. `Transformer` does not do any checking so values not in\n  the map are ignored. `CheckedTransformer` takes an extra step of verifying\n  that the value is either one of the map key values, in which case it is\n  transformed, or one of the expected output values, and if not will generate a\n  `ValidationError`. A Transformer placed using `check` will not do anything.\n\nAfter specifying a map of options, you can also specify \"filter\" just like in\n`CLI::IsMember`. Here are some examples (`Transformer` and `CheckedTransformer`\nare interchangeable in the examples) of `Transformer`:\n\n- `CLI::Transformer({{\"key1\", \"map1\"},{\"key2\",\"map2\"}})`: Select from key values\n  and produce map values.\n- `CLI::Transformer(std::map<std::string,int>({\"two\",2},{\"three\",3},{\"four\",4}}))`:\n  most maplike containers work, the `::value_type` needs to produce a pair of\n  some kind.\n- `CLI::CheckedTransformer(std::map<std::string, int>({{\"one\", 1}, {\"two\", 2}}))`:\n  You can use maps; in `->transform()` these replace the matched key with the\n  value. `CheckedTransformer` also requires that the value either match one of\n  the keys or match one of known outputs.\n- `auto p = std::make_shared<CLI::TransformPairs<std::string>>(std::initializer_list<std::pair<std::string,std::string>>({\"key1\", \"map1\"},{\"key2\",\"map2\"})); CLI::Transformer(p)`:\n  You can modify `p` later. `TransformPairs<T>` is an alias for\n  `std::vector<std::pair<<std::string,T>>`\n\nNOTES: If the container used in `IsMember`, `Transformer`, or\n`CheckedTransformer` has a `find` function like `std::unordered_map` or\n`std::map` then that function is used to do the searching. If it does not have a\n`find` function a linear search is performed. If there are filters present, the\nfast search is performed first, and if that fails a linear search with the\nfilters on the key values is performed.\n\n- `CLI::FileOnDefaultPath(default_path)`: can be used to check for files in a\n  default path. If used as a transform it will first check that a file exists,\n  if it does nothing further is done, if it does not it tries to add a default\n  Path to the file and search there again. If the file does not exist an error\n  is returned normally but this can be disabled using\n  `CLI::FileOnDefaultPath(default_path, false)`. This allows multiple paths to\n  be chained using multiple transform calls.\n\n- `CLI::EscapedString`: can be used to process an escaped string. The processing\n  is equivalent to that used for TOML config files, see\n  [TOML strings](https://toml.io/en/v1.0.0#string). With 2 notable exceptions.\n  \\` can also be used as a literal string notation, and it also allows binary\n  string notation see\n  [binary strings](https://cliutils.github.io/CLI11/book/chapters/config.html).\n  The escaped string processing will remove outer quotes if present, `\"` will\n  indicate a string with potential escape sequences, `'` and \\` will indicate a\n  literal string and the quotes removed but no escape sequences will be\n  processed. This is the same escape processing as used in config files.\n\n##### Validator operations\n\nValidators are copyable and have a few operations that can be performed on them\nto alter settings. Most of the built in Validators have a default description\nthat is displayed in the help. This can be altered via\n`.description(validator_description)`. The name of a Validator, which is useful\nfor later reference from the `get_validator(name)` method of an `Option` can be\nset via `.name(validator_name)` The operation function of a Validator can be set\nvia `.operation(std::function<std::string(std::string &>)`. The `.active()`\nfunction can activate or deactivate a Validator from the operation. A validator\ncan be set to apply only to a specific element of the output. For example in a\npair option `std::pair<int, std::string>` the first element may need to be a\npositive integer while the second may need to be a valid file. The\n`.application_index(int)` function can specify this. It is zero based and\nnegative indices apply to all values.\n\n```cpp\nopt->check(CLI::Validator(CLI::PositiveNumber).application_index(0));\nopt->check(CLI::Validator(CLI::ExistingFile).application_index(1));\n```\n\nAll the validator operation functions return a Validator reference allowing them\nto be chained. For example\n\n```cpp\nopt->check(CLI::Range(10,20).description(\"range is limited to sensible values\").active(false).name(\"range\"));\n```\n\nwill specify a check on an option with a name \"range\", but deactivate it for the\ntime being. The check can later be activated through\n\n```cpp\nopt->get_validator(\"range\")->active();\n```\n\n##### Custom Validators\n\nA validator object with a custom function can be created via\n\n```cpp\nCLI::Validator(std::function<std::string(std::string &)>,validator_description,validator_name=\"\");\n```\n\nor if the operation function is set later they can be created with\n\n```cpp\nCLI::Validator(validator_description);\n```\n\nIt is also possible to create a subclass of `CLI::Validator`, in which case it\ncan also set a custom description function, and operation function. One example\nof this is in the\n[custom validator example](https://github.com/CLIUtils/CLI11/blob/main/examples/custom_validator.cpp).\nexample. The `check` and `transform` operations can also take a shared_ptr ğŸ†• to\na validator if you wish to reuse the validator in multiple locations or it is\nmutating and the check is dependent on other operations or is variable. Note\nthat in this case it is not recommended to use the same object for both check\nand transform operations, the check will modify some internal flags on the\nobject so it will not be usable for transform operations.\n\n##### Querying Validators\n\nOnce loaded into an Option, a pointer to a named Validator can be retrieved via\n\n```cpp\nauto *validator = opt->get_validator(name);\n```\n\nThis will retrieve a Validator with the given name or throw a\n`CLI::OptionNotFound` error. If no name is given or name is empty the first\nunnamed Validator will be returned or the first Validator if there is only one.\n\nor\n\n```cpp\nauto *validator = opt->get_validator(index);\n```\n\nWhich will return a validator in the index it is applied which isn't necessarily\nthe order in which was defined. The pointer can be `nullptr` if an invalid index\nis given. Validators have a few functions to query the current values:\n\n- `get_description()`: Will return a description string\n- `get_name()`: Will return the Validator name\n- `get_active()`: Will return the current active state, true if the Validator is\n  active.\n- `get_application_index()`: Will return the current application index.\n- `get_modifying()`: Will return true if the Validator is allowed to modify the\n  input, this can be controlled via the `non_modifying()` method, though it is\n  recommended to let `check` and `transform` option methods manipulate it if\n  needed.\n\n#### Getting results\n\nIn most cases, the fastest and easiest way is to return the results through a\ncallback or variable specified in one of the `add_*` functions. But there are\nsituations where this is not possible or desired. For these cases the results\nmay be obtained through one of the following functions. Please note that these\nfunctions will do any type conversions and processing during the call so should\nnot used in performance critical code:\n\n- `->results()`: Retrieves a vector of strings with all the results in the order\n  they were given.\n- `->results(variable_to_bind_to)`: Gets the results according to the\n  MultiOptionPolicy and converts them just like the `add_option_function` with a\n  variable.\n- `Value=opt->as<type>()`: Returns the result or default value directly as the\n  specified type if possible, can be vector to return all results, and a\n  non-vector to get the result according to the MultiOptionPolicy in place. If\n  it is expected that the results will be needed as a vector, it is suggested\n  that `->expected(CLI::details::expected_max_vector_size)` or\n  `allow_extra_args()` be used on the option to inform CLI11 that vector args\n  are expected and allowed.\n\n### Subcommands\n\nSubcommands are keywords that invoke a new set of options and features. For\nexample, the `git` command has a long series of subcommands, like `add` and\n`commit`. Each can have its own options and implementations. Subcommands are\nsupported in CLI11, and can be nested infinitely. To add a subcommand, call the\n`add_subcommand` method with a name and an optional description. This gives a\npointer to an `App` that behaves just like the main app, and can take options or\nfurther subcommands. Add `->ignore_case()` to a subcommand to allow any\nvariation of caps to also be accepted. `->ignore_underscore()` is similar, but\nfor underscores. Children inherit the current setting from the parent. You\ncannot add multiple matching subcommand names at the same level (including\n`ignore_case` and `ignore_underscore`).\n\nIf you want to require that at least one subcommand is given, use\n`.require_subcommand()` on the parent app. You can optionally give an exact\nnumber of subcommands to require, as well. If you give two arguments, that sets\nthe min and max number allowed. 0 for the max number allowed will allow an\nunlimited number of subcommands. As a handy shortcut, a single negative value N\nwill set \"up to N\" values. Limiting the maximum number allows you to keep\narguments that match a previous subcommand name from matching.\n\nIf an `App` (main or subcommand) has been parsed on the command line, `->parsed`\nwill be true (or convert directly to bool). All `App`s have a\n`get_subcommands()` method, which returns a list of pointers to the subcommands\npassed on the command line. A `got_subcommand(App_or_name)` method is also\nprovided that will check to see if an `App` pointer or a string name was\ncollected on the command line.\n\nFor many cases, however, using an app's callback capabilities may be easier.\nEvery app has a set of callbacks that can be executed at various stages of\nparsing; a `C++` lambda function (with capture to get parsed values) can be used\nas input to the callback definition function. If you throw `CLI::Success` or\n`CLI::RuntimeError(return_value)`, you can even exit the program through the\ncallback.\n\nMultiple subcommands are allowed, to allow [`Click`][click] like series of\ncommands (order is preserved). The same subcommand can be triggered multiple\ntimes but all positional arguments will take precedence over the second and\nfuture calls of the subcommand. `->count()` on the subcommand will return the\nnumber of times the subcommand was called. The subcommand callback will only be\ntriggered once unless the `.immediate_callback()` flag is set or the callback is\nspecified through the `parse_complete_callback()` function. The\n`final_callback()` is triggered only once. In which case the callback executes\non completion of the subcommand arguments but after the arguments for that\nsubcommand have been parsed, and can be triggered multiple times. Note that the\n`parse_complete_callback()` is executed prior to processing any config files.\nThe `final_callback()` is executed after config file processing.\n\nSubcommands may also have an empty name either by calling `add_subcommand` with\nan empty string for the name or with no arguments. Nameless subcommands function\na similarly to groups in the main `App`. See [Option groups](#option-groups) to\nsee how this might work. If an option is not defined in the main App, all\nnameless subcommands are checked as well. This allows for the options to be\ndefined in a composable group. The `add_subcommand` function has an overload for\nadding a `shared_ptr<App>` so the subcommand(s) could be defined in different\ncomponents and merged into a main `App`, or possibly multiple `Apps`. Multiple\nnameless subcommands are allowed. Callbacks for nameless subcommands are only\ntriggered if any options from the subcommand were parsed. Subcommand names given\nthrough the `add_subcommand` method have the same restrictions as option names.\n\nOptions or flags in a subcommand may be directly specified using dot notation\n\n- `--subcommand.long=val` (long subcommand option)\n- `--subcommand.long val` (long subcommand option)\n- `--subcommand.f=val` (short form subcommand option)\n- `--subcommand.f val` (short form subcommand option)\n- `--subcommand.f` (short form subcommand flag)\n- `--subcommand1.subsub.f val` (short form nested subcommand option)\n\nThe use of dot notation in this form is equivalent `--subcommand.long <args>` =>\n`subcommand --long <args> ++`. Nested subcommands also work `sub1.subsub` would\ntrigger the subsub subcommand in `sub1`. This is equivalent to \"sub1 subsub\".\nQuotes around the subcommand names are permitted following the TOML standard for\nsuch specification. This includes allowing escape sequences. For example\n`\"subcommand\".'f'` or `\"subcommand.with.dots\".arg1 = value`.\n\n#### Subcommand options\n\nThere are several options that are supported on the main app and subcommands and\noption_groups. These are:\n\n- `.ignore_case()`: Ignore the case of this subcommand. Inherited by added\n  subcommands, so is usually used on the main `App`.\n- `.ignore_underscore()`: Ignore any underscores in the subcommand name.\n  Inherited by added subcommands, so is usually used on the main `App`.\n- `.allow_windows_style_options()`: Allow command line options to be parsed in\n  the form of `/s /long /file:file_name.ext` This option does not change how\n  options are specified in the `add_option` calls or the ability to process\n  options in the form of `-s --long --file=file_name.ext`.\n- `.allow_non_standard_option_names()`: Allow specification of single `-` long\n  form option names. This is not recommended but is available to enable\n  reworking of existing interfaces. If this modifier is enabled on an app or\n  subcommand, options or flags can be specified like normal but instead of\n  throwing an exception, long form single dash option names will be allowed. It\n  is not allowed to have a single character short option starting with the same\n  character as a single dash long form name; for example, `-s` and `-single` are\n  not allowed in the same application.\n- `.allow_subcommand_prefix_matching()`:ğŸ†• If this modifier is enabled,\n  unambiguious prefix portions of a subcommand will match. For example\n  `upgrade_package` would match on `upgrade_`, `upg`, `u` as long as no other\n  subcommand would also match. It also disallows subcommand names that are full\n  prefixes of another subcommand.\n- `.fallthrough()`: Allow extra unmatched options and positionals to \"fall\n  through\" and be matched on a parent option. Subcommands by default are allowed\n  to \"fall through\" as in they will first attempt to match on the current\n  subcommand and if they fail will progressively check parents for matching\n  subcommands. This can be disabled through `subcommand_fallthrough(false)`.\n- `.subcommand_fallthrough()`: Allow subcommands to \"fall through\" and be\n  matched on a parent option. Disabling this prevents additional subcommands at\n  the same level from being matched. It can be useful in certain circumstances\n  where there might be ambiguity between subcommands and positionals. The\n  default is true.\n- `.configurable()`: Allow the subcommand to be triggered from a configuration\n  file. By default subcommand options in a configuration file do not trigger a\n  subcommand but will just update default values.\n- `.disable()`: Specify that the subcommand is disabled, if given with a bool\n  value it will enable or disable the subcommand or option group.\n- `.disabled_by_default()`: Specify that at the start of parsing the\n  subcommand/option_group should be disabled. This is useful for allowing some\n  Subcommands to trigger others.\n- `.enabled_by_default()`: Specify that at the start of each parse the\n  subcommand/option_group should be enabled. This is useful for allowing some\n  Subcommands to disable others.\n- `.silent()`: Specify that the subcommand is silent meaning that if used it\n  won't show up in the subcommand list. This allows the use of subcommands as\n  modifiers\n- `.validate_positionals()`: Specify that positionals should pass validation\n  before matching. Validation is specified through `transform`, `check`, and\n  `each` for an option. If an argument fails validation it is not an error and\n  matching proceeds to the next available positional or extra arguments.\n- `.validate_optional_arguments()`: Specify that optional arguments should pass\n  validation before being assigned to an option. Validation is specified through\n  `transform`, `check`, and `each` for an option. If an argument fails\n  validation it is not an error and matching proceeds to the next available\n  positional subcommand or extra arguments.\n- `.excludes(option_or_subcommand)`: If given an option pointer or pointer to\n  another subcommand, these subcommands cannot be given together. In the case of\n  options, if the option is passed the subcommand cannot be used and will\n  generate an error.\n- `.needs(option_or_subcommand)`: If given an option pointer or pointer to\n  another subcommand, the subcommands will require the given option to have been\n  given before this subcommand is validated which occurs prior to execution of\n  any callback or after parsing is completed.\n- `.require_option()`: Require 1 or more options or option groups be used.\n- `.require_option(N)`: Require `N` options or option groups, if `N>0`, or up to\n  `N` if `N<0`. `N=0` resets to the default to 0 or more.\n- `.require_option(min, max)`: Explicitly set min and max allowed options or\n  option groups. Setting `max` to 0 implies unlimited options.\n- `.require_subcommand()`: Require 1 or more subcommands.\n- `.require_subcommand(N)`: Require `N` subcommands if `N>0`, or up to `N` if\n  `N<0`. `N=0` resets to the default to 0 or more.\n- `.require_subcommand(min, max)`: Explicitly set min and max allowed\n  subcommands. Setting `max` to 0 is unlimited.\n- `.add_subcommand(name=\"\", description=\"\")`: Add a subcommand, returns a\n  pointer to the internally stored subcommand.\n- `.add_subcommand(shared_ptr<App>)`: Add a subcommand by shared_ptr, returns a\n  pointer to the internally stored subcommand.\n- `.remove_subcommand(App)`: Remove a subcommand from the app or subcommand.\n- `.got_subcommand(App_or_name)`: Check to see if a subcommand was received on\n  the command line.\n- `.get_subcommands(filter)`: The list of subcommands that match a particular\n  filter function.\n- `.add_option_group(name=\"\", description=\"\")`: Add an\n  [option group](#option-groups) to an App, an option group is specialized\n  subcommand intended for containing groups of options or other groups for\n  controlling how options interact.\n- `.get_parent()`: Get the parent App or `nullptr` if called on main App.\n- `.get_option(name)`: Get an option pointer by option name will throw if the\n  specified option is not available, nameless subcommands are also searched\n- `.get_option_no_throw(name)`: Get an option pointer by option name. This\n  function will return a `nullptr` instead of throwing if the option is not\n  available.\n- `.get_options(filter)`: Get the list of all defined option pointers (useful\n  for processing the app for custom output formats).\n- `.parse_order()`: Get the list of option pointers in the order they were\n  parsed (including duplicates).\n- `.formatter(std::shared_ptr<formatterBase> fmt)`: Set a custom formatter for\n  help.\n- `.formatter_fn(fmt)`, with signature\n  `std::string(const App*, std::string, AppFormatMode)`. See [formatting][] for\n  more details.\n- `.config_formatter(std::shared_ptr<Config> fmt)`: set a custom config\n  formatter for generating config files, more details available at [Config\n  files][config]\n- `.description(str)`: Set/change the description.\n- `.get_description()`: Access the description.\n- `.alias(str)`: set an alias for the subcommand, this allows subcommands to be\n  called by more than one name.\n- `.parsed()`: True if this subcommand was given on the command line.\n- `.count()`: Returns the number of times the subcommand was called.\n- `.count(option_name)`: Returns the number of times a particular option was\n  called.\n- `.count_all()`: Returns the total number of arguments a particular subcommand\n  processed, on the main App it returns the total number of processed commands.\n- `.name(name)`: Add or change the name.\n- `.callback(void() function)`: Set the callback for an app. Either sets the\n  `pre_parse_callback` or the `final_callback` depending on the value of\n  `immediate_callback`. See [Subcommand callbacks](#callbacks) for some\n  additional details.\n- `.parse_complete_callback(void() function)`: Set the callback that runs at the\n  completion of parsing. For subcommands this is executed at the completion of\n  the single subcommand and can be executed multiple times. See\n  [Subcommand callbacks](#callbacks) for some additional details.\n- `.final_callback(void() function)`: Set the callback that runs at the end of\n  all processing. This is the last thing that is executed before returning. See\n  [Subcommand callbacks](#callbacks) for some additional details.\n- `.immediate_callback()`: Specifies whether the callback for a subcommand\n  should be run as a `parse_complete_callback`(true) or `final_callback`(false).\n  When used on the main app it will execute the main app callback prior to the\n  callbacks for a subcommand if they do not also have the `immediate_callback`\n  flag set. It is preferable to use the `parse_complete_callback` or\n  `final_callback` directly instead of the `callback` and `immediate_callback`\n  if one wishes to control the ordering and timing of callback. Though\n  `immediate_callback` can be used to swap them if that is needed.\n- `.pre_parse_callback(void(std::size_t) function)`: Set a callback that\n  executes after the first argument of an application is processed. See\n  [Subcommand callbacks](#callbacks) for some additional details.\n- `.allow_extras()`: Do not throw an error if extra arguments are left over.\n- `.allow_extras(CLI::ExtrasMode)`: Specify the method of handling unrecognized\n  arguments.\n  - `CLI::ExtrasMode::Error`: generate an error on unrecognized argument. Same\n    as `.allow_extras(false)`.\n  - `CLI::ExtrasMode::ErrorImmediately`: generate an error immediately on\n    parsing an unrecognized option`.\n  - `CLI::ExtrasMode::Ignore`: ignore any unrecognized argument, do not generate\n    an error.\n  - `CLI::ExtrasMode::AssumeSingleArgument`: After an unrecognized flag or\n    option argument, if the following argument is not a flag or option argument\n    assume it an argument and treat it as also unrecognized even if it would\n    otherwise go to a positional argument\n  - `CLI::ExtrasMode::AssumeMultipleArguments`:After an unrecognized flag or\n    option argument, if the following arguments are not a flag or option\n    argument assume they are arguments and treat them as also unrecognized even\n    if it would otherwise go to a positional argument\n  - `CLI::ExtrasMode::Capture`: capture all unrecognized arguments, same as\n    `true` for `.allow_extras`:\n- `.positionals_at_end()`: Specify that positional arguments occur as the last\n  arguments and throw an error if an unexpected positional is encountered.\n- `.prefix_command()`: Like `allow_extras`, but stop processing immediately on\n  the first unrecognized item. All subsequent arguments are placed in the\n  remaining_arg list. It is ideal for allowing your app or subcommand to be a\n  \"prefix\" to calling another app. Can be called with a `bool` value to turn on\n  or off\n- `.prefix_command(CLI::PrefixCommandMode)`: specify the prefix_command mode to\n  use. `PrefixCommandMode::on` and `PrefixCommandMode::off` are the same as\n  `prefix_command(true)` or `prefix_command(false)`. Calling with\n  `PrefixCommandMode::separator_only` will only trigger prefix command mode by\n  the use of the subcommand separator `--` other unrecognized arguments would be\n  considered an error depending on whether `allow_extras` was set or not.\n- `.usage(message)`: Replace text to appear at the start of the help string\n  after description.\n- `.usage(std::string())`: Set a callback to generate a string that will appear\n  at the start of the help string after description.\n- `.footer(message)`: Set text to appear at the bottom of the help string.\n- `.footer(std::string())`: Set a callback to generate a string that will appear\n  at the end of the help string.\n- `.set_help_flag(name, message)`: Set the help flag name and message, returns a\n  pointer to the created option.\n- `.set_version_flag(name, versionString or callback, help_message)`: Set the\n  version flag name and version string or callback and optional help message,\n  returns a pointer to the created option.\n- `.set_help_all_flag(name, message)`: Set the help all flag name and message,\n  returns a pointer to the created option. Expands subcommands.\n- `.failure_message(func)`: Set the failure message function. Two provided:\n  `CLI::FailureMessage::help` and `CLI::FailureMessage::simple` (the default).\n- `.group(name)`: Set a group name, defaults to `\"Subcommands\"`. Setting an\n  empty string for the name will be hide the subcommand.\n- `[option_name]`: retrieve a const pointer to an option given by `option_name`\n  for Example `app[\"--flag1\"]` will get a pointer to the option for the\n  \"--flag1\" value, `app[\"--flag1\"]->as<bool>()` will get the results of the\n  command line for a flag. The operation will throw an exception if the option\n  name is not valid.\n\n> [!NOTE]\n>\n> If you have a fixed number of required positional options, that will match\n> before subcommand names. `{}` is an empty filter function, and any positional\n> argument will match before repeated subcommand names.\n\n#### Callbacks\n\nA subcommand has three optional callbacks that are executed at different stages\nof processing. The `preparse_callback` is executed once after the first argument\nof a subcommand or application is processed and gives an argument for the number\nof remaining arguments to process. For the main app the first argument is\nconsidered the program name, for subcommands the first argument is the\nsubcommand name. For Option groups and nameless subcommands the first argument\nis after the first argument or subcommand is processed from that group. The\nsecond callback is executed after parsing. This is known as the\n`parse_complete_callback`. For subcommands this is executed immediately after\nparsing and can be executed multiple times if a subcommand is called multiple\ntimes. On the main app this callback is executed after all the\n`parse_complete_callback`s for the subcommands are executed but prior to any\n`final_callback` calls in the subcommand or option groups. If the main app or\nsubcommand has a config file, no data from the config file will be reflected in\n`parse_complete_callback` on named subcommands. For `option_group`s the\n`parse_complete_callback` is executed prior to the `parse_complete_callback` on\nthe main app but after the `config_file` is loaded (if specified). The\n`final_callback` is executed after all processing is complete. After the\n`parse_complete_callback` is executed on the main app, the used subcommand\n`final_callback` are executed followed by the \"final callback\" for option\ngroups. The last thing to execute is the `final_callback` for the `main_app`.\nFor example say an application was set up like\n\n```cpp\napp.parse_complete_callback(ac1);\napp.final_callback(ac2);\nauto sub1=app.add_subcommand(\"sub1\")->parse_complete_callback(c1)->preparse_callback(pc1);\nauto sub2=app.add_subcommand(\"sub2\")->final_callback(c2)->preparse_callback(pc2);\napp.preparse_callback( pa);\n\n... A bunch of other options\n```\n\nThen the command line is given as\n\n```bash\nprogram --opt1 opt1_val  sub1 --sub1opt --sub1optb val sub2 --sub2opt sub1 --sub1opt2 sub2 --sub2opt2 val\n```\n\n- `pa` will be called prior to parsing any values with an argument of 13.\n- `pc1` will be called immediately after processing the `sub1` command with a\n  value of 10.\n- `c1` will be called when the `sub2` command is encountered.\n- `pc2` will be called with value of 6 after the `sub2` command is encountered.\n- `c1` will be called again after the second `sub2` command is encountered.\n- `ac1` will be called after processing of all arguments\n- `c2` will be called once after processing all arguments.\n- `ac2` will be called last after completing all lower level callbacks have been\n  executed.\n\nA subcommand is considered terminated when one of the following conditions are\nmet.\n\n1. There are no more arguments to process\n2. Another subcommand is encountered that would not fit in an optional\n   positional slot of the subcommand\n3. The `positional_mark` (`--`) is encountered and there are no available\n   positional slots in the subcommand.\n4. The `subcommand_terminator` mark (`++`) is encountered\n\nPrior to executed a `parse_complete_callback` all contained options are\nprocessed before the callback is triggered. If a subcommand with a\n`parse_complete_callback` is called again, then the contained options are reset,\nand can be triggered again.\n\n#### Option groups\n\nThe subcommand method\n\n```cpp\n.add_option_group(name,description)\n```\n\nWill create an option group, and return a pointer to it. The argument for\n`description` is optional and can be omitted. An option group allows creation of\na collection of options, similar to the groups function on options, but with\nadditional controls and requirements. They allow specific sets of options to be\ncomposed and controlled as a collective. For an example see\n[range example](https://github.com/CLIUtils/CLI11/blob/main/examples/ranges.cpp).\nOption groups are a specialization of an App so all\n[functions](#subcommand-options) that work with an App or subcommand also work\non option groups. Options can be created as part of an option group using the\nadd functions just like a subcommand, or previously created options can be added\nthrough. The name given in an option group must not contain newlines or null\ncharacters.\n\n```cpp\nogroup->add_option(option_pointer);\nogroup->add_options(option_pointer);\nogroup->add_options(option1,option2,option3,...);\n```\n\nThe option pointers used in this function must be options defined in the parent\napplication of the option group otherwise an error will be generated.\nSubcommands can also be added via\n\n```cpp\nogroup->add_subcommand(subcom_pointer);\n```\n\nThis results in the subcommand being moved from its parent into the option\ngroup.\n\nOptions in an option group are searched for a command line match after any\noptions in the main app, so any positionals in the main app would be matched\nfirst. So care must be taken to make sure of the order when using positional\narguments and option groups. Option groups work well with `excludes` and\n`require_options` methods, as an application will treat an option group as a\nsingle option for the purpose of counting and requirements, and an option group\nwill be considered used if any of the options or subcommands contained in it are\nused. Option groups allow specifying requirements such as requiring 1 of 3\noptions in one group and 1 of 3 options in a different group. Option groups can\ncontain other groups as well. Disabling an option group will turn off all\noptions within the group.\n\nThe `CLI::TriggerOn` and `CLI::TriggerOff` methods are helper functions to allow\nthe use of options/subcommands from one group to trigger another group on or\noff.\n\n```cpp\nCLI::TriggerOn(group1_pointer, triggered_group);\nCLI::TriggerOff(group2_pointer, disabled_group);\n```\n\nThese functions make use of `preparse_callback`, `enabled_by_default()` and\n`disabled_by_default`. The triggered group may be a vector of group pointers.\nThese methods should only be used once per group and will override any previous\nuse of the underlying functions. More complex arrangements can be accomplished\nusing similar methodology with a custom `preparse_callback` function that does\nmore.\n\nAdditional helper functions `deprecate_option` and `retire_option` are available\nto deprecate or retire options\n\n```cpp\nCLI::deprecate_option(option *, replacement_name=\"\");\nCLI::deprecate_option(App,option_name,replacement_name=\"\");\n```\n\nwill specify that the option is deprecated which will display a message in the\nhelp and a warning on first usage. Deprecated options function normally but will\nadd a message in the help and display a warning on first use.\n\n```cpp\nCLI::retire_option(App,option *);\nCLI::retire_option(App,option_name);\n```\n\nwill create an option that does nothing by default and will display a warning on\nfirst usage that the option is retired and has no effect. If the option exists\nit is replaces with a dummy option that takes the same arguments.\n\nIf an empty string is passed the option group name the entire group will be\nhidden in the help results. For example.\n\n```cpp\nauto hidden_group=app.add_option_group(\"\");\n```\n\nwill create a group such that no options in that group are displayed in the help\nstring. For the purposes of help display, if the option group name starts with a\n'+' it is treated as if it were not in a group for help and get_options. For\nexample:\n\n```cpp\nauto added_group=app.add_option_group(\"+sub\");\n```\n\nIn this case the help output will not reference the option group and options\ninside of it will be treated for most purposes as if they were part of the\nparent.\n\n### Configuration file\n\n```cpp\napp.set_config(option_name=\"\",\n               default_file_name=\"\",\n               help_string=\"Read an ini file\",\n               required=false)\n```\n\nIf this is called with no arguments, it will remove the configuration file\noption (like `set_help_flag`). Setting a configuration option is special. If it\nis present, it will be read along with the normal command line arguments. The\nfile will be read if it exists, and does not throw an error unless `required` is\n`true`. Configuration files are in [TOML][] format by default, though the\ndefault reader can also accept files in INI format as well. The config reader\ncan read most aspects of TOML files including strings both literal and with\npotential escape sequences, digit separators, and multi-line strings, and run\nthem through the CLI11 parser. Other formats can be added by an adept user, some\nvariations are available through customization points in the default formatter.\nAn example of a TOML file:\n\n```toml\n# Comments are supported, using a #\n# The default section is [default], case-insensitive\n\nvalue = 1\nvalue2 = 123_456 # a string with separators\nstr = \"A string\"\nstr2 = \"A string\\nwith new lines\"\nstr3 = 'A literal \"string\"'\nvector = [1,2,3]\nstr_vector = [\"one\",\"two\",\"and three\"]\n\n# Sections map to subcommands\n[subcommand]\nin_subcommand = Wow\nsub.subcommand = true\n\"sub\".\"subcommand2\" = \"string_value\"\n```\n\nor equivalently in INI format\n\n```ini\n; Comments are supported, using a ;\n; The default section is [default], case-insensitive\n\nvalue = 1\nstr = \"A string\"\nvector = 1 2 3\nstr_vector = \"one\" \"two\" \"and three\"\n\n; Sections map to subcommands\n[subcommand]\nin_subcommand = Wow\nsub.subcommand = true\n```\n\nSpaces before and after the name and argument are ignored. Multiple arguments\nare separated by spaces. One set of quotes will be removed, preserving spaces\n(the same way the command line works). Boolean options can be `true`, `on`, `1`,\n`yes`, `enable`; or `false`, `off`, `0`, `no`, `disable` (case-insensitive).\nSections (and `.` separated names) are treated as subcommands (note: this does\nnot necessarily mean that subcommand was passed, it just sets the \"defaults\").\nYou cannot set positional-only arguments. Subcommands can be triggered from\nconfiguration files if the `configurable` flag was set on the subcommand. Then\nthe use of `[subcommand]` notation will trigger a subcommand and cause it to act\nas if it were on the command line.\n\nTo print a configuration file from the passed arguments, use\n`.config_to_str(default_also=false, write_description=false)`, where\n`default_also` will also show any defaulted arguments, and `write_description`\nwill include the app and option descriptions. See\n[Config files](https://cliutils.github.io/CLI11/book/chapters/config.html) for\nsome additional details and customization points.\n\nIf it is desired that multiple configuration be allowed. Use\n\n```cpp\napp.set_config(\"--config\")->expected(1, X);\n```\n\nWhere X is some positive number and will allow up to `X` configuration files to\nbe specified by separate `--config` arguments. Value strings with quote\ncharacters in it will be printed with a single quote. All other arguments will\nuse double quote. Empty strings will use a double quoted argument. Numerical or\nboolean values are not quoted.\n\nFor options or flags which allow 0 arguments to be passed using an empty string\nin the config file, `{}`, or `[]` will convert the result to the default value\nspecified via `default_str` or `default_val` on the option. If no user specified\ndefault is given the result is an empty string or the converted value of an\nempty string.\n\nNOTE: Transforms and checks can be used with the option pointer returned from\nset_config like any other option to validate the input if needed. It can also be\nused with the built in transform `CLI::FileOnDefaultPath` to look in a default\npath as well as the current one. For example\n\n```cpp\napp.set_config(\"--config\")->transform(CLI::FileOnDefaultPath(\"/to/default/path/\"));\n```\n\nSee [Transforming Validators](#transforming-validators) for additional details\non this validator. Multiple transforms or validators can be used either by\nmultiple calls or using `|` operations with the transform.\n\n### Inheriting defaults\n\nMany of the defaults for subcommands and even options are inherited from their\ncreators. The inherited default values for subcommands are `allow_extras`,\n`prefix_command`, `ignore_case`, `ignore_underscore`, `fallthrough`, `group`,\n`usage`, `footer`, `immediate_callback` and maximum number of required\nsubcommands. The help flag existence, name, and description are inherited, as\nwell.\n\nOptions have defaults for `group`, `required`, `multi_option_policy`,\n`ignore_case`, `ignore_underscore`, `delimiter`, and `disable_flag_override`. To\nset these defaults, you should set the `option_defaults()` object, for example:\n\n```cpp\napp.option_defaults()->required();\n// All future options will be required\n```\n\nThe default settings for options are inherited to subcommands, as well.\n\n### Formatting\n\nThe job of formatting help printouts is delegated to a formatter object. You are\nfree to replace the formatter with a custom one by calling `formatter(fmt)` on\nan `App`. CLI11 comes with a default App formatter, `Formatter`. You can\nretrieve the formatter via `.get_formatter()` this will return a pointer to the\ncurrent `Formatter`. It is customizable; you can set `label(key, value)` to\nreplace the default labels like `REQUIRED`, and `OPTIONS`. You can also set\n`column_width(n)` to set the width of the columns before you add the functional\nto the app or option. Several other configuration options are also available in\nthe `Formatter`. You can also override almost any stage of the formatting\nprocess in a subclass of either formatter. If you want to make a new formatter\nfrom scratch, you can do that too; you just need to implement the correct\nsignature. see [formatting][] for more details.\n\n### Subclassing\n\nThe App class was designed allow toolkits to subclass it, to provide preset\ndefault options (see above) and setup/teardown code. Subcommands remain an\nunsubclassed `App`, since those are not expected to need setup and teardown. The\ndefault `App` only adds a help flag, `-h,--help`, than can removed/replaced\nusing `.set_help_flag(name, help_string)`. You can also set a help-all flag with\n`.set_help_all_flag(name, help_string)`; this will expand the subcommands (one\nlevel only). You can remove options if you have pointers to them using\n`.remove_option(opt)`. You can add a `pre_callback` override to customize the\nafter parse but before run behavior, while still giving the user freedom to\n`callback` on the main app.\n\nThe most important parse function is `parse(std::vector<std::string>)`, which\ntakes a reversed list of arguments (so that `pop_back` processes the args in the\ncorrect order). `get_help_ptr` and `get_config_ptr` give you access to the\nhelp/config option pointers. The standard `parse` manually sets the name from\nthe first argument, so it should not be in this vector. You can also use\n`parse(string, bool)` to split up and parse a single string; the optional\nboolean should be set to true if you are including the program name in the\nstring, and false otherwise. The program name can contain spaces if it is an\nexisting file, otherwise can be enclosed in quotes(single quote, double quote or\nbacktick). Embedded quote characters can be escaped with `\\`.\n\nAlso, in a related note, the `App` you get a pointer to is stored in the parent\n`App` in a `shared_ptr`s (similar to `Option`s) and are deleted when the main\n`App` goes out of scope unless the object has another owner.\n\n### How it works\n\nEvery `add_` option you have seen so far depends on one method that takes a\nlambda function. Each of these methods is just making a different lambda\nfunction with capture to populate the option. The function has full access to\nthe vector of strings, so it knows how many times an option was passed or how\nmany arguments it received. The lambda returns `true` if it could validate the\noption strings, and `false` if it failed.\n\nOther values can be added as long as they support `operator>>` (and defaults can\nbe printed if they support `operator<<`). To add a new type, for example,\nprovide a custom `operator>>` with an `istream` (inside the CLI namespace is\nfine if you don't want to interfere with an existing `operator>>`).\n\nIf you wanted to extend this to support a completely new type, use a lambda or\nadd an overload of the `lexical_cast` function in the namespace of the type you\nneed to convert to. Some examples of some new parsers for `complex<double>` that\nsupport all of the features of a standard `add_options` call are in\n[one of the tests](./tests/NewParseTest.cpp). A simpler example is shown below:\n\n```cpp\napp.add_option(\"--fancy-count\", [](std::vector<std::string> val){\n    std::cout << \"This option was given \" << val.size() << \" times.\" << std::endl;\n    return true;\n    });\n```\n\n### Unicode support\n\nCLI11 supports Unicode and wide strings as defined in the\n[UTF-8 Everywhere](http://utf8everywhere.org/) manifesto. In particular:\n\n- The library can parse a wide version of command-line arguments on Windows,\n  which are converted internally to UTF-8 (more on this below);\n- You can store option values in `std::wstring`, in which case they will be\n  converted to a correct wide string encoding on your system (UTF-16 on Windows\n  and UTF-32 on most other systems);\n- Instead of storing wide strings, it is recommended to use provided `widen` and\n  `narrow` functions to convert to and from wide strings when actually necessary\n  (such as when calling into Windows APIs).\n\nWhen using the command line on Windows with unicode arguments, your `main`\nfunction may already receive broken Unicode. Parsing `argv` at that point will\nnot give you a correct string. To fix this, you have three options; the first is\nrecommended for cross-platform support:\n\n1\\. Replace `argv` with `app.ensure_utf8(argv)` before any arguments are parsed.\n`ensure_utf8` will do nothing on systems where `argv` is already in UTF-8 (Such\nas Linux or macOS) and return `argv` unmodified. On Windows, it will discard\n`argv` and replace it with a correctly decoded array or arguments from win32\nAPI.\n\n```cpp\nint main(int argc, char** argv) {\n    CLI::App app;\n    argv = app.ensure_utf8(argv);  // new argv memory is held by app\n    // ...\n    CLI11_PARSE(app, argc, argv);\n}\n```\n\nBe sure you do not modify `argv` before this function call, as the correct\nvalues will be reconstructed using Windows APIs and produced by this call. It\nhas no effect on other platforms and just passes through `argv`.\n\n<details><summary>Other options (click to expand)</summary><p>\n\n2\\. Use the Windows-only non-standard `wmain` function, which accepts\n`wchar_t *argv[]` instead of `char* argv[]`. Parsing this will allow CLI to\nconvert wide strings to UTF-8 without losing information.\n\n```cpp\nint wmain(int argc, wchar_t *argv[]) {\n    CLI::App app;\n    // ...\n    CLI11_PARSE(app, argc, argv);\n}\n```\n\n3\\. Retrieve arguments yourself by using Windows APIs like\n[CommandLineToArgvW](https://learn.microsoft.com/en-us/windows/win32/api/shellapi/nf-shellapi-commandlinetoargvw)\nand pass them to CLI. This is what the library is doing under the hood in\n`ensure_utf8`.\n\n</p></details>\n</br>\n\nThe library provides functions to convert between UTF-8 and wide strings:\n\n```cpp\nnamespace CLI {\n    std::string narrow(const std::wstring &str);\n    std::string narrow(const wchar_t *str);\n    std::string narrow(const wchar_t *str, std::size_t size);\n    std::string narrow(std::wstring_view str);  // C++17\n\n    std::wstring widen(const std::string &str);\n    std::wstring widen(const char *str);\n    std::wstring widen(const char *str, std::size_t size);\n    std::wstring widen(std::string_view str);  // C++17\n}\n```\n\n#### Note on using Unicode paths\n\nWhen creating a `filesystem::path` from a UTF-8 path on Windows, you need to\nconvert it to a wide string first. CLI11 provides a platform-independent\n`to_path` function, which will convert a UTF-8 string to path, the right way:\n\n```cpp\nstd::string utf8_name = \"Hello HallÃ³ ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ ä½ å¥½ ğŸ‘©â€ğŸš€â¤ï¸.txt\";\n\nstd::filesystem::path p = CLI::to_path(utf8_name);\nstd::ifstream stream(CLI::to_path(utf8_name));\n// etc.\n```\n\n### Utilities\n\nThere are a few other utilities that are often useful in CLI programming. These\nare in separate headers, and do not appear in `CLI11.hpp`, but are completely\nindependent and can be used as needed. The `Timer`/`AutoTimer` class allows you\nto easily time a block of code, with custom print output.\n\n```cpp\n{\nCLI::AutoTimer timer {\"My Long Process\", CLI::Timer::Big};\nsome_long_running_process();\n}\n```\n\nThis will create a timer with a title (default: `Timer`), and will customize the\noutput using the predefined `Big` output (default: `Simple`). Because it is an\n`AutoTimer`, it will print out the time elapsed when the timer is destroyed at\nthe end of the block. If you use `Timer` instead, you can use `to_string` or\n`std::cout << timer << std::endl;` to print the time. The print function can be\nany function that takes two strings, the title and the time, and returns a\nformatted string for printing.\n\n### Other libraries\n\nIf you use the excellent [Rang][] library to add color to your terminal in a\nsafe, multi-platform way, you can combine it with CLI11 nicely:\n\n```cpp\nstd::atexit([](){std::cout << rang::style::reset;});\ntry {\n    app.parse(argc, argv);\n} catch (const CLI::ParseError &e) {\n    std::cout << (e.get_exit_code()==0 ? rang::fg::blue : rang::fg::red);\n    return app.exit(e);\n}\n```\n\nThis will print help in blue, errors in red, and will reset before returning the\nterminal to the user.\n\nIf you are on a Unix-like system, and you'd like to handle control-c and color,\nyou can add:\n\n```cpp\n #include <csignal>\n void signal_handler(int s) {\n     std::cout << std::endl << rang::style::reset << rang::fg::red << rang::fg::bold;\n     std::cout << \"Control-C detected, exiting...\" << rang::style::reset << std::endl;\n     std::exit(1); // will call the correct exit func, no unwinding of the stack though\n }\n```\n\nAnd, in your main function:\n\n```cpp\n     // Nice Control-C\n     struct sigaction sigIntHandler;\n     sigIntHandler.sa_handler = signal_handler;\n     sigemptyset(&sigIntHandler.sa_mask);\n     sigIntHandler.sa_flags = 0;\n     sigaction(SIGINT, &sigIntHandler, nullptr);\n```\n\n## API\n\nThe API is [documented here][api-docs]. Also see the [CLI11 tutorial\nGitBook][gitbook].\n\n## Examples\n\nSeveral short examples of different features are included in the repository. A\nbrief description of each is included here\n\n- [arg_capture](https://github.com/CLIUtils/CLI11/blob/main/examples/arg_capture.cpp):\n  Example of capturing all remaining arguments after a specific option, using\n  subcommand and prefix_command() with an alias.\n- [callback_passthrough](https://github.com/CLIUtils/CLI11/blob/main/examples/callback_passthrough.cpp):\n  Example of directly passing remaining arguments through to a callback function\n  which generates a CLI11 application based on existing arguments.\n- [custom_parse](https://github.com/CLIUtils/CLI11/blob/main/examples/custom_parse.cpp):\n  Based on [Issue #566](https://github.com/CLIUtils/CLI11/issues/566), example\n  of custom parser\n- [digit_args](https://github.com/CLIUtils/CLI11/blob/main/examples/digit_args.cpp):\n  Based on [Issue #123](https://github.com/CLIUtils/CLI11/issues/123), uses\n  digit flags to pass a value\n- [enum](https://github.com/CLIUtils/CLI11/blob/main/examples/enum.cpp): Using\n  enumerations in an option, and the use of\n  [CheckedTransformer](#transforming-validators)\n- [enum_ostream](https://github.com/CLIUtils/CLI11/blob/main/examples/enum_ostream.cpp):\n  In addition to the contents of example enum.cpp, this example shows how a\n  custom ostream operator overrides CLI11's enum streaming.\n- [formatter](https://github.com/CLIUtils/CLI11/blob/main/examples/formatter.cpp):\n  Illustrating usage of a custom formatter\n- [groups](https://github.com/CLIUtils/CLI11/blob/main/examples/groups.cpp):\n  Example using groups of options for help grouping and a timer helper class\n- [inter_argument_order](https://github.com/CLIUtils/CLI11/blob/main/examples/inter_argument_order.cpp):\n  An app to practice mixing unlimited arguments, but still recover the original\n  order.\n- [json](https://github.com/CLIUtils/CLI11/blob/main/examples/json.cpp): Using\n  JSON as a config file parser\n- [modhelp](https://github.com/CLIUtils/CLI11/blob/main/examples/modhelp.cpp):\n  How to modify the help flag to do something other than default\n- [nested](https://github.com/CLIUtils/CLI11/blob/main/examples/nested.cpp):\n  Nested subcommands\n- [option_groups](https://github.com/CLIUtils/CLI11/blob/main/examples/option_groups.cpp):\n  Illustrating the use of option groups and a required number of options. Based\n  on [Issue #88](https://github.com/CLIUtils/CLI11/issues/88) to set interacting\n  groups of options\n- [positional_arity](https://github.com/CLIUtils/CLI11/blob/main/examples/positional_arity.cpp):\n  Illustrating use of `preparse_callback` to handle situations where the number\n  of arguments can determine which should get parsed, Based on\n  [Issue #166](https://github.com/CLIUtils/CLI11/issues/166)\n- [positional_validation](https://github.com/CLIUtils/CLI11/blob/main/examples/positional_validation.cpp):\n  Example of how positional arguments are validated using the\n  `validate_positional` flag, also based on\n  [Issue #166](https://github.com/CLIUtils/CLI11/issues/166)\n- [prefix_command](https://github.com/CLIUtils/CLI11/blob/main/examples/prefix_command.cpp):\n  Illustrating use of the `prefix_command` flag.\n- [ranges](https://github.com/CLIUtils/CLI11/blob/main/examples/ranges.cpp): App\n  to demonstrate exclusionary option groups based on\n  [Issue #88](https://github.com/CLIUtils/CLI11/issues/88)\n- [shapes](https://github.com/CLIUtils/CLI11/blob/main/examples/shapes.cpp):\n  Illustrating how to set up repeated subcommands Based on\n  [gitter discussion](https://gitter.im/CLI11gitter/Lobby?at=5c7af6b965ffa019ea788cd5)\n- [simple](https://github.com/CLIUtils/CLI11/blob/main/examples/simple.cpp): A\n  simple example of how to set up a CLI11 Application with different flags and\n  options\n- [subcom_help](https://github.com/CLIUtils/CLI11/blob/main/examples/subcom_help.cpp):\n  Configuring help for subcommands\n- [subcom_partitioned](https://github.com/CLIUtils/CLI11/blob/main/examples/subcom_partitioned.cpp):\n  Example with a timer and subcommands generated separately and added to the\n  main app later.\n- [subcommands](https://github.com/CLIUtils/CLI11/blob/main/examples/subcommands.cpp):\n  Short example of subcommands\n- [validators](https://github.com/CLIUtils/CLI11/blob/main/examples/validators.cpp):\n  Example illustrating use of validators\n- [custom validators](https://github.com/CLIUtils/CLI11/blob/main/examples/custom_validator.cpp):\n  Example illustrating use of validators\n- [date validators](https://github.com/CLIUtils/CLI11/blob/main/examples/date_validator.cpp):\n  Example illustrating use of validators\n\n## Contribute\n\nTo contribute, open an [issue][github issues] or [pull\nrequest][github pull requests] on GitHub, or ask a question on [gitter][]. There\nis also a [short note to contributors](./.github/CONTRIBUTING.md). This readme\nroughly follows the [Standard Readme Style][] and includes a mention of almost\nevery feature of the library. More complex features are documented in more\ndetail in the [CLI11 tutorial GitBook][gitbook].\n\nThis project was created by [Henry Schreiner](https://github.com/henryiii) and\nmajor features were added by [Philip Top](https://github.com/phlptp). Special\nthanks to all the contributors\n([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://profiles.sussex.ac.uk/p281168-alex-dewar/publications\"><img src=\"https://avatars.githubusercontent.com/u/23149834?v=4?s=100\" width=\"100px;\" alt=\"Alex Dewar\"/><br /><sub><b>Alex Dewar</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=alexdewar\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://nvidia.com/\"><img src=\"https://avatars.githubusercontent.com/u/91539962?v=4?s=100\" width=\"100px;\" alt=\"Alexander Galanin\"/><br /><sub><b>Alexander Galanin</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=agalanin-at-nvidia\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/andrew-hardin\"><img src=\"https://avatars0.githubusercontent.com/u/16496326?v=4?s=100\" width=\"100px;\" alt=\"Andrew Hardin\"/><br /><sub><b>Andrew Hardin</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=andrew-hardin\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/andreasxp\"><img src=\"https://avatars.githubusercontent.com/u/28830446?v=4?s=100\" width=\"100px;\" alt=\"Andrey Zhukov\"/><br /><sub><b>Andrey Zhukov</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=andreasxp\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/bindreams\"><img src=\"https://avatars.githubusercontent.com/u/28830446?v=4?s=100\" width=\"100px;\" alt=\"Anna Zhukova\"/><br /><sub><b>Anna Zhukova</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=bindreams\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/SX91\"><img src=\"https://avatars2.githubusercontent.com/u/754754?v=4?s=100\" width=\"100px;\" alt=\"Anton\"/><br /><sub><b>Anton</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=SX91\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/trokhymchuk\"><img src=\"https://avatars.githubusercontent.com/u/66204814?v=4?s=100\" width=\"100px;\" alt=\"Artem Trokhymchuk \"/><br /><sub><b>Artem Trokhymchuk </b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=trokhymchuk\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/BenjaminBeichler\"><img src=\"https://avatars.githubusercontent.com/u/1441492?v=4?s=100\" width=\"100px;\" alt=\"Benjamin Beichler\"/><br /><sub><b>Benjamin Beichler</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=BenjaminBeichler\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/calebzulawski\"><img src=\"https://avatars.githubusercontent.com/u/563826?v=4?s=100\" width=\"100px;\" alt=\"Caleb Zulawski\"/><br /><sub><b>Caleb Zulawski</b></sub></a><br /><a href=\"#platform-calebzulawski\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/krico\"><img src=\"https://avatars.githubusercontent.com/u/6952185?v=4?s=100\" width=\"100px;\" alt=\"Christian Asmussen\"/><br /><sub><b>Christian Asmussen</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=krico\" title=\"Documentation\">ğŸ“–</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=krico\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.linkedin.com/in/cbachhuber/\"><img src=\"https://avatars0.githubusercontent.com/u/27212661?v=4?s=100\" width=\"100px;\" alt=\"Christoph Bachhuber\"/><br /><sub><b>Christoph Bachhuber</b></sub></a><br /><a href=\"#example-cbachhuber\" title=\"Examples\">ğŸ’¡</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=cbachhuber\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ComixHe\"><img src=\"https://avatars.githubusercontent.com/u/54773474?v=4?s=100\" width=\"100px;\" alt=\"Comix\"/><br /><sub><b>Comix</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=ComixHe\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dfleury2\"><img src=\"https://avatars1.githubusercontent.com/u/4805384?v=4?s=100\" width=\"100px;\" alt=\"D. Fleury\"/><br /><sub><b>D. Fleury</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=dfleury2\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dbarowy\"><img src=\"https://avatars3.githubusercontent.com/u/573142?v=4?s=100\" width=\"100px;\" alt=\"Dan Barowy\"/><br /><sub><b>Dan Barowy</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=dbarowy\" title=\"Documentation\">ğŸ“–</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mensinda\"><img src=\"https://avatars3.githubusercontent.com/u/3407462?v=4?s=100\" width=\"100px;\" alt=\"Daniel Mensinger\"/><br /><sub><b>Daniel Mensinger</b></sub></a><br /><a href=\"#platform-mensinda\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/DarkWingMcQuack\"><img src=\"https://avatars.githubusercontent.com/u/38857302?v=4?s=100\" width=\"100px;\" alt=\"DarkWingMcQuack\"/><br /><sub><b>DarkWingMcQuack</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=DarkWingMcQuack\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/tnixeu\"><img src=\"https://avatars.githubusercontent.com/u/4436784?v=4?s=100\" width=\"100px;\" alt=\"Dominik Nussbaumer\"/><br /><sub><b>Dominik Nussbaumer</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=tnixeu\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ZeeD26\"><img src=\"https://avatars2.githubusercontent.com/u/2487468?v=4?s=100\" width=\"100px;\" alt=\"Dominik Steinberger\"/><br /><sub><b>Dominik Steinberger</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=ZeeD26\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dvj\"><img src=\"https://avatars2.githubusercontent.com/u/77217?v=4?s=100\" width=\"100px;\" alt=\"Doug Johnston\"/><br /><sub><b>Doug Johnston</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/issues?q=author%3Advj\" title=\"Bug reports\">ğŸ›</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=dvj\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://recursiveascent.blogspot.com/\"><img src=\"https://avatars.githubusercontent.com/u/1779595?v=4?s=100\" width=\"100px;\" alt=\"Dylan Baker\"/><br /><sub><b>Dylan Baker</b></sub></a><br /><a href=\"#platform-dcbaker\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/eli-schwartz\"><img src=\"https://avatars.githubusercontent.com/u/6551424?v=4?s=100\" width=\"100px;\" alt=\"Eli Schwartz\"/><br /><sub><b>Eli Schwartz</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=eli-schwartz\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/sifferman\"><img src=\"https://avatars.githubusercontent.com/u/43790149?v=4?s=100\" width=\"100px;\" alt=\"Ethan Sifferman\"/><br /><sub><b>Ethan Sifferman</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=sifferman\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/helmesjo\"><img src=\"https://avatars0.githubusercontent.com/u/2501070?v=4?s=100\" width=\"100px;\" alt=\"Fred HelmesjÃ¶\"/><br /><sub><b>Fred HelmesjÃ¶</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/issues?q=author%3Ahelmesjo\" title=\"Bug reports\">ğŸ›</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=helmesjo\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/gblanco92\"><img src=\"https://avatars.githubusercontent.com/u/3957977?v=4?s=100\" width=\"100px;\" alt=\"Guillem Blanco\"/><br /><sub><b>Guillem Blanco</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=gblanco92\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://iscinumpy.gitlab.io\"><img src=\"https://avatars1.githubusercontent.com/u/4616906?v=4?s=100\" width=\"100px;\" alt=\"Henry Schreiner\"/><br /><sub><b>Henry Schreiner</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/issues?q=author%3Ahenryiii\" title=\"Bug reports\">ğŸ›</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=henryiii\" title=\"Documentation\">ğŸ“–</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=henryiii\" title=\"Code\">ğŸ’»</a> <a href=\"#example-henryiii\" title=\"Examples\">ğŸ’¡</a> <a href=\"#platform-henryiii\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://izzys.casa\"><img src=\"https://avatars0.githubusercontent.com/u/63051?v=4?s=100\" width=\"100px;\" alt=\"Isabella Muerte\"/><br /><sub><b>Isabella Muerte</b></sub></a><br /><a href=\"#platform-slurps-mad-rips\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://izzys.casa/\"><img src=\"https://avatars.githubusercontent.com/u/63051?v=4?s=100\" width=\"100px;\" alt=\"Izzy Muerte\"/><br /><sub><b>Izzy Muerte</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=bruxisma\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jakoblover\"><img src=\"https://avatars0.githubusercontent.com/u/14160441?v=4?s=100\" width=\"100px;\" alt=\"Jakob Lover\"/><br /><sub><b>Jakob Lover</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=jakoblover\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jgerityneurala\"><img src=\"https://avatars2.githubusercontent.com/u/57360646?v=4?s=100\" width=\"100px;\" alt=\"James Gerity\"/><br /><sub><b>James Gerity</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=jgerityneurala\" title=\"Documentation\">ğŸ“–</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jbriales\"><img src=\"https://avatars1.githubusercontent.com/u/6850478?v=4?s=100\" width=\"100px;\" alt=\"Jesus Briales\"/><br /><sub><b>Jesus Briales</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=jbriales\" title=\"Code\">ğŸ’»</a> <a href=\"https://github.com/CLIUtils/CLI11/issues?q=author%3Ajbriales\" title=\"Bug reports\">ğŸ›</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://jonathangzzben.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/47956254?v=4?s=100\" width=\"100px;\" alt=\"Jonark\"/><br /><sub><b>Jonark</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=JonathanGzzBen\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/SkyToGround\"><img src=\"https://avatars1.githubusercontent.com/u/58835?v=4?s=100\" width=\"100px;\" alt=\"Jonas Nilsson\"/><br /><sub><b>Jonas Nilsson</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/issues?q=author%3ASkyToGround\" title=\"Bug reports\">ğŸ›</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=SkyToGround\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/j-rivero\"><img src=\"https://avatars.githubusercontent.com/u/2098802?v=4?s=100\" width=\"100px;\" alt=\"Jose Luis Rivero\"/><br /><sub><b>Jose Luis Rivero</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=j-rivero\" title=\"Code\">ğŸ’»</a> <a href=\"#platform-j-rivero\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jsoref\"><img src=\"https://avatars0.githubusercontent.com/u/2119212?v=4?s=100\" width=\"100px;\" alt=\"Josh Soref\"/><br /><sub><b>Josh Soref</b></sub></a><br /><a href=\"#tool-jsoref\" title=\"Tools\">ğŸ”§</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=jsoref\" title=\"Documentation\">ğŸ“–</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.effibem.com\"><img src=\"https://avatars.githubusercontent.com/u/5479063?v=4?s=100\" width=\"100px;\" alt=\"Julien Marrec\"/><br /><sub><b>Julien Marrec</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=jmarrec\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/KOLANICH\"><img src=\"https://avatars1.githubusercontent.com/u/240344?v=4?s=100\" width=\"100px;\" alt=\"KOLANICH\"/><br /><sub><b>KOLANICH</b></sub></a><br /><a href=\"#platform-KOLANICH\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/skannan89\"><img src=\"https://avatars0.githubusercontent.com/u/11918764?v=4?s=100\" width=\"100px;\" alt=\"Kannan\"/><br /><sub><b>Kannan</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/issues?q=author%3Askannan89\" title=\"Bug reports\">ğŸ›</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=skannan89\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://himvis.com\"><img src=\"https://avatars3.githubusercontent.com/u/465279?v=4?s=100\" width=\"100px;\" alt=\"Khem Raj\"/><br /><sub><b>Khem Raj</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=kraj\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/looopTools\"><img src=\"https://avatars.githubusercontent.com/u/1943536?v=4?s=100\" width=\"100px;\" alt=\"Lars Nielsen\"/><br /><sub><b>Lars Nielsen</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=looopTools\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://lucas-czech.de\"><img src=\"https://avatars0.githubusercontent.com/u/4741887?v=4?s=100\" width=\"100px;\" alt=\"Lucas Czech\"/><br /><sub><b>Lucas Czech</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/issues?q=author%3Alczech\" title=\"Bug reports\">ğŸ›</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=lczech\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.mogigoma.com/\"><img src=\"https://avatars2.githubusercontent.com/u/130862?v=4?s=100\" width=\"100px;\" alt=\"Mak Kolybabi\"/><br /><sub><b>Mak Kolybabi</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=mogigoma\" title=\"Documentation\">ğŸ“–</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/LostInCompilation\"><img src=\"https://avatars.githubusercontent.com/u/12819635?v=4?s=100\" width=\"100px;\" alt=\"Marc\"/><br /><sub><b>Marc</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=LostInCompilation\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/cetius\"><img src=\"https://avatars.githubusercontent.com/u/6552472?v=4?s=100\" width=\"100px;\" alt=\"Marcin Ropa\"/><br /><sub><b>Marcin Ropa</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=cetius\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://lambdafu.net/\"><img src=\"https://avatars1.githubusercontent.com/u/1138455?v=4?s=100\" width=\"100px;\" alt=\"Marcus Brinkmann\"/><br /><sub><b>Marcus Brinkmann</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/issues?q=author%3Alambdafu\" title=\"Bug reports\">ğŸ›</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=lambdafu\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://msoeken.github.io\"><img src=\"https://avatars0.githubusercontent.com/u/1998245?v=4?s=100\" width=\"100px;\" alt=\"Mathias Soeken\"/><br /><sub><b>Mathias Soeken</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=msoeken\" title=\"Documentation\">ğŸ“–</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://www.mmmccormick.com/\"><img src=\"https://avatars.githubusercontent.com/u/25432?v=4?s=100\" width=\"100px;\" alt=\"Matt McCormick\"/><br /><sub><b>Matt McCormick</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=thewtex\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/AnticliMaxtic\"><img src=\"https://avatars.githubusercontent.com/u/43995389?v=4?s=100\" width=\"100px;\" alt=\"Max\"/><br /><sub><b>Max</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=AnticliMaxtic\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://mbh.sh\"><img src=\"https://avatars3.githubusercontent.com/u/20403931?v=4?s=100\" width=\"100px;\" alt=\"Michael Hall\"/><br /><sub><b>Michael Hall</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=mbhall88\" title=\"Documentation\">ğŸ“–</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nathanhourt\"><img src=\"https://avatars2.githubusercontent.com/u/271977?v=4?s=100\" width=\"100px;\" alt=\"Nathan Hourt\"/><br /><sub><b>Nathan Hourt</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/issues?q=author%3Anathanhourt\" title=\"Bug reports\">ğŸ›</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=nathanhourt\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nathanielhourt\"><img src=\"https://avatars.githubusercontent.com/u/271977?v=4?s=100\" width=\"100px;\" alt=\"Nathaniel Hourt\"/><br /><sub><b>Nathaniel Hourt</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=nathanielhourt\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/paddy-hack\"><img src=\"https://avatars.githubusercontent.com/u/6804372?v=4?s=100\" width=\"100px;\" alt=\"Olaf Meeuwissen\"/><br /><sub><b>Olaf Meeuwissen</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=paddy-hack\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://lunarwatcher.github.io\"><img src=\"https://avatars.githubusercontent.com/u/29489988?v=4?s=100\" width=\"100px;\" alt=\"Olivia (Zoe)\"/><br /><sub><b>Olivia (Zoe)</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=LunarWatcher\" title=\"Documentation\">ğŸ“–</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://ondrejcertik.com/\"><img src=\"https://avatars3.githubusercontent.com/u/20568?v=4?s=100\" width=\"100px;\" alt=\"OndÅ™ej ÄŒertÃ­k\"/><br /><sub><b>OndÅ™ej ÄŒertÃ­k</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/issues?q=author%3Acertik\" title=\"Bug reports\">ğŸ›</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/pleroux0\"><img src=\"https://avatars2.githubusercontent.com/u/39619854?v=4?s=100\" width=\"100px;\" alt=\"Paul le Roux\"/><br /><sub><b>Paul le Roux</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=pleroux0\" title=\"Code\">ğŸ’»</a> <a href=\"#platform-pleroux0\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/chfast\"><img src=\"https://avatars1.githubusercontent.com/u/573380?v=4?s=100\" width=\"100px;\" alt=\"PaweÅ‚ Bylica\"/><br /><sub><b>PaweÅ‚ Bylica</b></sub></a><br /><a href=\"#platform-chfast\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/PeteAudinate\"><img src=\"https://avatars.githubusercontent.com/u/99274874?v=4?s=100\" width=\"100px;\" alt=\"PeteAudinate\"/><br /><sub><b>PeteAudinate</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=PeteAudinate\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/peterazmanov\"><img src=\"https://avatars0.githubusercontent.com/u/15322318?v=4?s=100\" width=\"100px;\" alt=\"Peter Azmanov\"/><br /><sub><b>Peter Azmanov</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=peterazmanov\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/peterh\"><img src=\"https://avatars.githubusercontent.com/u/79339?v=4?s=100\" width=\"100px;\" alt=\"Peter Harris\"/><br /><sub><b>Peter Harris</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=peterh\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://ptheywood.uk/\"><img src=\"https://avatars.githubusercontent.com/u/628937?v=4?s=100\" width=\"100px;\" alt=\"Peter Heywood\"/><br /><sub><b>Peter Heywood</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=ptheywood\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/phlptp\"><img src=\"https://avatars0.githubusercontent.com/u/20667153?v=4?s=100\" width=\"100px;\" alt=\"Philip Top\"/><br /><sub><b>Philip Top</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/issues?q=author%3Aphlptp\" title=\"Bug reports\">ğŸ›</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=phlptp\" title=\"Documentation\">ğŸ“–</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=phlptp\" title=\"Code\">ğŸ’»</a> <a href=\"#example-phlptp\" title=\"Examples\">ğŸ’¡</a> <a href=\"#platform-phlptp\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/pmundkur\"><img src=\"https://avatars.githubusercontent.com/u/103736?v=4?s=100\" width=\"100px;\" alt=\"Prashanth Mundkur\"/><br /><sub><b>Prashanth Mundkur</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=pmundkur\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/radimkrcmar\"><img src=\"https://avatars.githubusercontent.com/u/4678534?v=4?s=100\" width=\"100px;\" alt=\"Radim KrÄmÃ¡Å™\"/><br /><sub><b>Radim KrÄmÃ¡Å™</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=radimkrcmar\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/rafiw\"><img src=\"https://avatars3.githubusercontent.com/u/3034707?v=4?s=100\" width=\"100px;\" alt=\"Rafi Wiener\"/><br /><sub><b>Rafi Wiener</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=rafiw\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/RangeMachine\"><img src=\"https://avatars.githubusercontent.com/u/11577601?v=4?s=100\" width=\"100px;\" alt=\"RangeMachine\"/><br /><sub><b>RangeMachine</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=RangeMachine\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/Krzmbrzl\"><img src=\"https://avatars.githubusercontent.com/u/12751591?v=4?s=100\" width=\"100px;\" alt=\"Robert Adam\"/><br /><sub><b>Robert Adam</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=Krzmbrzl\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://www.ratml.org/\"><img src=\"https://avatars0.githubusercontent.com/u/1845039?v=4?s=100\" width=\"100px;\" alt=\"Ryan Curtin\"/><br /><sub><b>Ryan Curtin</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=rcurtin\" title=\"Documentation\">ğŸ“–</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/SherlockInSpace\"><img src=\"https://avatars.githubusercontent.com/u/5507786?v=4?s=100\" width=\"100px;\" alt=\"Ryan Sherlock\"/><br /><sub><b>Ryan Sherlock</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=SherlockInSpace\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://sam.hocevar.net/\"><img src=\"https://avatars2.githubusercontent.com/u/245089?v=4?s=100\" width=\"100px;\" alt=\"Sam Hocevar\"/><br /><sub><b>Sam Hocevar</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=samhocevar\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://seanfisk.com/\"><img src=\"https://avatars0.githubusercontent.com/u/410322?v=4?s=100\" width=\"100px;\" alt=\"Sean Fisk\"/><br /><sub><b>Sean Fisk</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/issues?q=author%3Aseanfisk\" title=\"Bug reports\">ğŸ›</a> <a href=\"https://github.com/CLIUtils/CLI11/commits?author=seanfisk\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/delpinux\"><img src=\"https://avatars0.githubusercontent.com/u/35096584?v=4?s=100\" width=\"100px;\" alt=\"StÃ©phane Del Pino\"/><br /><sub><b>StÃ©phane Del Pino</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=delpinux\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/The0Dev\"><img src=\"https://avatars.githubusercontent.com/u/98645239?v=4?s=100\" width=\"100px;\" alt=\"The0Dev\"/><br /><sub><b>The0Dev</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=The0Dev\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://theoparis.com/\"><img src=\"https://avatars.githubusercontent.com/u/11761863?v=4?s=100\" width=\"100px;\" alt=\"Theo Paris\"/><br /><sub><b>Theo Paris</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=theoparis\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/TheodorNEngoy\"><img src=\"https://avatars.githubusercontent.com/u/140903820?v=4?s=100\" width=\"100px;\" alt=\"Theodor Nesfeldt EngÃ¸y\"/><br /><sub><b>Theodor Nesfeldt EngÃ¸y</b></sub></a><br /><a href=\"#infra-TheodorNEngoy\" title=\"Infrastructure (Hosting, Build-Tools, etc)\">ğŸš‡</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://uilianries.github.io/\"><img src=\"https://avatars.githubusercontent.com/u/4870173?v=4?s=100\" width=\"100px;\" alt=\"Uilian Ries\"/><br /><sub><b>Uilian Ries</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=uilianries\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/metopa\"><img src=\"https://avatars2.githubusercontent.com/u/3974178?v=4?s=100\" width=\"100px;\" alt=\"Viacheslav Kroilov\"/><br /><sub><b>Viacheslav Kroilov</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=metopa\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/VolkerChristian\"><img src=\"https://avatars.githubusercontent.com/u/18554540?v=4?s=100\" width=\"100px;\" alt=\"Volker Christian\"/><br /><sub><b>Volker Christian</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=VolkerChristian\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/almikhayl\"><img src=\"https://avatars2.githubusercontent.com/u/6747040?v=4?s=100\" width=\"100px;\" alt=\"almikhayl\"/><br /><sub><b>almikhayl</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=almikhayl\" title=\"Code\">ğŸ’»</a> <a href=\"#platform-almikhayl\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ayum\"><img src=\"https://avatars.githubusercontent.com/u/6747040?v=4?s=100\" width=\"100px;\" alt=\"ayum\"/><br /><sub><b>ayum</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=ayum\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/captainurist\"><img src=\"https://avatars.githubusercontent.com/u/73941350?v=4?s=100\" width=\"100px;\" alt=\"captainurist\"/><br /><sub><b>captainurist</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=captainurist\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://cs.odu.edu/~ctsolakis\"><img src=\"https://avatars0.githubusercontent.com/u/6725596?v=4?s=100\" width=\"100px;\" alt=\"christos\"/><br /><sub><b>christos</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=ChristosT\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/deining\"><img src=\"https://avatars3.githubusercontent.com/u/18169566?v=4?s=100\" width=\"100px;\" alt=\"deining\"/><br /><sub><b>deining</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=deining\" title=\"Documentation\">ğŸ“–</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dherrera-fb\"><img src=\"https://avatars.githubusercontent.com/u/89840711?v=4?s=100\" width=\"100px;\" alt=\"dherrera-fb\"/><br /><sub><b>dherrera-fb</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=dherrera-fb\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dherrera-meta\"><img src=\"https://avatars.githubusercontent.com/u/161629485?v=4?s=100\" width=\"100px;\" alt=\"dherrera-meta\"/><br /><sub><b>dherrera-meta</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=dherrera-meta\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/djerius\"><img src=\"https://avatars.githubusercontent.com/u/196875?v=4?s=100\" width=\"100px;\" alt=\"djerius\"/><br /><sub><b>djerius</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=djerius\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/dryleev\"><img src=\"https://avatars.githubusercontent.com/u/83670813?v=4?s=100\" width=\"100px;\" alt=\"dryleev\"/><br /><sub><b>dryleev</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=dryleev\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/elszon\"><img src=\"https://avatars0.githubusercontent.com/u/2971495?v=4?s=100\" width=\"100px;\" alt=\"elszon\"/><br /><sub><b>elszon</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=elszon\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ferdymercury\"><img src=\"https://avatars3.githubusercontent.com/u/10653970?v=4?s=100\" width=\"100px;\" alt=\"ferdymercury\"/><br /><sub><b>ferdymercury</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=ferdymercury\" title=\"Documentation\">ğŸ“–</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/fpeng1985\"><img src=\"https://avatars1.githubusercontent.com/u/87981?v=4?s=100\" width=\"100px;\" alt=\"fpeng1985\"/><br /><sub><b>fpeng1985</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=fpeng1985\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/geir-t\"><img src=\"https://avatars3.githubusercontent.com/u/35292136?v=4?s=100\" width=\"100px;\" alt=\"geir-t\"/><br /><sub><b>geir-t</b></sub></a><br /><a href=\"#platform-geir-t\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/gostefan\"><img src=\"https://avatars.githubusercontent.com/u/2479455?v=4?s=100\" width=\"100px;\" alt=\"gostefan\"/><br /><sub><b>gostefan</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=gostefan\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/huangqinjin\"><img src=\"https://avatars.githubusercontent.com/u/8402260?v=4?s=100\" width=\"100px;\" alt=\"huangqinjin\"/><br /><sub><b>huangqinjin</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=huangqinjin\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jaefunk\"><img src=\"https://avatars.githubusercontent.com/u/8777543?v=4?s=100\" width=\"100px;\" alt=\"jaefunk\"/><br /><sub><b>jaefunk</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=jaefunk\" title=\"Documentation\">ğŸ“–</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ncihnegn\"><img src=\"https://avatars3.githubusercontent.com/u/12021721?v=4?s=100\" width=\"100px;\" alt=\"ncihnegn\"/><br /><sub><b>ncihnegn</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=ncihnegn\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nshaheed\"><img src=\"https://avatars.githubusercontent.com/u/6963603?v=4?s=100\" width=\"100px;\" alt=\"nshaheed\"/><br /><sub><b>nshaheed</b></sub></a><br /><a href=\"#platform-nshaheed\" title=\"Packaging/porting to new platform\">ğŸ“¦</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/nurelin\"><img src=\"https://avatars3.githubusercontent.com/u/5276274?v=4?s=100\" width=\"100px;\" alt=\"nurelin\"/><br /><sub><b>nurelin</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=nurelin\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://polistern.i2p/\"><img src=\"https://avatars.githubusercontent.com/u/55511995?v=4?s=100\" width=\"100px;\" alt=\"polistern\"/><br /><sub><b>polistern</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=polistern\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/romanholidaypancakes\"><img src=\"https://avatars.githubusercontent.com/u/51652878?v=4?s=100\" width=\"100px;\" alt=\"romanholidaypancakes\"/><br /><sub><b>romanholidaypancakes</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=romanholidaypancakes\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/ryan4729\"><img src=\"https://avatars3.githubusercontent.com/u/40183301?v=4?s=100\" width=\"100px;\" alt=\"ryan4729\"/><br /><sub><b>ryan4729</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=ryan4729\" title=\"Tests\">âš ï¸</a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/shameekganguly\"><img src=\"https://avatars.githubusercontent.com/u/2412842?v=4?s=100\" width=\"100px;\" alt=\"shameekganguly\"/><br /><sub><b>shameekganguly</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=shameekganguly\" title=\"Code\">ğŸ’»</a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/tansy\"><img src=\"https://avatars.githubusercontent.com/u/40426919?v=4?s=100\" width=\"100px;\" alt=\"tansy\"/><br /><sub><b>tansy</b></sub></a><br /><a href=\"https://github.com/CLIUtils/CLI11/commits?author=tansy\" title=\"Code\">ğŸ’»</a></td>\n    </tr>\n  </tbody>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nThis project follows the\n[all-contributors](https://github.com/all-contributors/all-contributors)\nspecification. Contributions of any kind welcome!\n\n## License\n\nAs of version 1.0, this library is available under a 3-Clause BSD license. See\nthe [LICENSE](./LICENSE) file for details.\n\nCLI11 was developed at the [University of Cincinnati][] to support of the\n[GooFit][] library under [NSF Award 1414736][]. Version 0.9 was featured in a\n[DIANA/HEP][] meeting at CERN ([see the slides][diana slides]). Please give it a\ntry! Feedback is always welcome.\n\n[doi-badge]: https://zenodo.org/badge/80064252.svg\n[doi-link]: https://zenodo.org/badge/latestdoi/80064252\n[azure-badge]:\n  https://dev.azure.com/CLIUtils/CLI11/_apis/build/status/CLIUtils.CLI11?branchName=main\n[azure]: https://dev.azure.com/CLIUtils/CLI11\n[actions-link]: https://github.com/CLIUtils/CLI11/actions\n[actions-badge]:\n  https://github.com/CLIUtils/CLI11/actions/workflows/tests.yml/badge.svg\n[repology-badge]: https://repology.org/badge/latest-versions/cli11.svg\n[repology]: https://repology.org/project/cli11/versions\n[codecov-badge]:\n  https://codecov.io/gh/CLIUtils/CLI11/branch/main/graph/badge.svg?token=2O4wfs8NJO\n[codecov]: https://codecov.io/gh/CLIUtils/CLI11\n[gitter-badge]: https://badges.gitter.im/CLI11gitter/Lobby.svg\n[gitter]: https://gitter.im/CLI11gitter/Lobby\n[license-badge]: https://img.shields.io/badge/License-BSD-blue.svg\n[conan-badge]: https://img.shields.io/badge/conan-io-blue\n[conan-link]: https://conan.io/center/cli11\n[conda-badge]: https://img.shields.io/conda/vn/conda-forge/cli11.svg\n[conda-link]: https://github.com/conda-forge/cli11-feedstock\n[github releases]: https://github.com/CLIUtils/CLI11/releases\n[github issues]: https://github.com/CLIUtils/CLI11/issues\n[github pull requests]: https://github.com/CLIUtils/CLI11/pulls\n[goofit]: https://GooFit.github.io\n[plumbum]: https://plumbum.readthedocs.io/en/latest/\n[click]: http://click.pocoo.org\n[api-docs]: https://CLIUtils.github.io/CLI11/index.html\n[rang]: https://github.com/agauniyal/rang\n[boost program options]:\n  http://www.boost.org/doc/libs/1_63_0/doc/html/program_options.html\n[the lean mean c++ option parser]: http://optionparser.sourceforge.net\n[tclap]: http://tclap.sourceforge.net\n[cxxopts]: https://github.com/jarro2783/cxxopts\n[docopt]: https://github.com/docopt/docopt.cpp\n[gflags]: https://gflags.github.io/gflags\n[getopt]: https://www.gnu.org/software/libc/manual/html_node/Getopt.html\n[diana/hep]: http://diana-hep.org\n[nsf award 1414736]: https://nsf.gov/awardsearch/showAward?AWD_ID=1414736\n[university of cincinnati]: http://www.uc.edu\n[gitbook]: https://cliutils.github.io/CLI11/book/\n[cli11 advanced topics/custom converters]:\n  https://cliutils.gitlab.io/CLI11Tutorial/chapters/advanced-topics.html#custom-converters\n[programoptions.hxx]: https://github.com/Fytch/ProgramOptions.hxx\n[argument aggregator]: https://github.com/vietjtnguyen/argagg\n[args]: https://github.com/Taywee/args\n[argh!]: https://github.com/adishavit/argh\n[fmt]: https://github.com/fmtlib/fmt\n[catch]: https://github.com/philsquared/Catch\n[clara]: https://github.com/philsquared/Clara\n[version 1.0 post]: https://iscinumpy.gitlab.io/post/announcing-cli11-10/\n[version 1.3 post]: https://iscinumpy.gitlab.io/post/announcing-cli11-13/\n[version 1.6 post]: https://iscinumpy.gitlab.io/post/announcing-cli11-16/\n[version 2.0 post]: https://iscinumpy.gitlab.io/post/announcing-cli11-20/\n[wandbox-badge]: https://img.shields.io/badge/try_2.1-online-blue.svg\n[wandbox-link]: https://wandbox.org/permlink/9eQyaD1DchlzukRv\n[releases-badge]: https://img.shields.io/github/release/CLIUtils/CLI11.svg\n[cli11-po-compare]:\n  https://iscinumpy.gitlab.io/post/comparing-cli11-and-boostpo/\n[diana slides]:\n  https://indico.cern.ch/event/619465/contributions/2507949/attachments/1448567/2232649/20170424-diana-2.pdf\n[awesome c++]: https://github.com/fffaraz/awesome-cpp/blob/master/README.md#cli\n[cli]: https://codesynthesis.com/projects/cli/\n[single file libs]:\n  https://github.com/nothings/single_file_libs/blob/master/README.md\n[codacy-badge]:\n  https://app.codacy.com/project/badge/Grade/2796b969c1b54321a02ad08affec0800\n[codacy-link]:\n  https://www.codacy.com/gh/CLIUtils/CLI11/dashboard?utm_source=github.com&utm_medium=referral&utm_content=CLIUtils/CLI11&utm_campaign=Badge_Grade\n[standard readme style]: https://github.com/RichardLitt/standard-readme\n[argparse]: https://github.com/p-ranav/argparse\n[toml]: https://toml.io\n[lyra]: https://github.com/bfgroup/Lyra\n[installation]: https://cliutils.github.io/CLI11/book/chapters/installation.html\n[formatting]: https://cliutils.github.io/CLI11/book/chapters/formatting.html\n[config]: https://cliutils.github.io/CLI11/book/chapters/config.html\n",
      "stars_today": 1
    },
    {
      "id": 143079594,
      "name": "swift-syntax",
      "full_name": "swiftlang/swift-syntax",
      "description": "A set of Swift libraries for parsing, inspecting, generating, and transforming Swift source code.",
      "html_url": "https://github.com/swiftlang/swift-syntax",
      "stars": 3596,
      "forks": 480,
      "language": "Swift",
      "topics": [],
      "created_at": "2018-07-31T23:19:58Z",
      "updated_at": "2026-01-13T21:03:20Z",
      "pushed_at": "2026-01-11T08:55:18Z",
      "open_issues": 145,
      "owner": {
        "login": "swiftlang",
        "avatar_url": "https://avatars.githubusercontent.com/u/42816656?v=4"
      },
      "readme": "# Swift Syntax\n\nThe swift-syntax package is a set of libraries that work on a source-accurate tree representation of Swift source code, called the SwiftSyntax tree. The SwiftSyntax tree forms the backbone of Swiftâ€™s macro system â€“ the macro expansion nodes are represented as SwiftSyntax nodes and a macro generates a SwiftSyntax tree to be inserted into the source file.\n\n## Documentation\n\nYou can read SwiftSyntaxâ€™s documentation on [swiftpackageindex.com](https://swiftpackageindex.com/swiftlang/swift-syntax/documentation).\n\nA great way to interactively explore the SwiftSyntax tree of a source file is https://swift-ast-explorer.com, developed by [@kishikawakatsumi](https://github.com/kishikawakatsumi).\n\nA set of example usages of swift-syntax can be found in [Examples](Examples).\n\n## Releases\n\nReleases of SwiftSyntax are aligned with corresponding language and tooling releases, for example the major version 509 of swift-syntax is aligned with Swift 5.9. \n \nTo depend on swift-syntax in a SwiftPM package, add the following to your `Package.swift`.\n\n\n```swift\ndependencies: [\n  .package(url: \"https://github.com/swiftlang/swift-syntax.git\", from: \"<#latest swift-syntax tag#>\"),\n],\n```\n \nTo add swift-syntax as a dependency of your Xcode project, go to the *Package Dependencies* tab of your Xcode project, click the plus button and search for https://github.com/swiftlang/swift-syntax.git.\n\n## Reporting Issues\n\nIf you should hit any issues while using SwiftSyntax, we appreciate bug reports on [GitHub Issue](https://github.com/swiftlang/swift-syntax/issues).\n\n## Contributing\n\nStart contributing to SwiftSyntax see [this guide](CONTRIBUTING.md) for more information.\n\n## Bazel\n\nSwiftSyntax provides an experimental [Bazel](https://bazel.build) build configuration, maintained by Keith Smiley. \nTo use it, you can pull the source archive from the relevant release tag\ninto your `MODULE.bazel` file (preferred and recommended) with `bazel_dep`. Bzlmod support was added starting release of `509.0.0` and above. All available versions can be found in the [Bazel Central Registry](https://registry.bazel.build/modules/swift-syntax)\n\n```python3\nbazel_dep(name = \"swift-syntax\", version = \"600.0.1\")\n```\n\nYou can also pull source archive with `WORKSPACE` but note that it is preferred to use `MODULE.bazel`. To use `WORKSPACE` and swift-syntax, you can use `http_archive` as such\n\n```python3\nhttp_archive(\n    name = \"SwiftSyntax\",\n    sha256 = \"f070fd44db9b33f430fd5b5d2700f1e2001c0028711859600e80cc975074fab0\",\n    strip_prefix = \"swift-syntax-509.1.0\",\n    url = \"https://github.com/apple/swift-syntax/archive/refs/tags/509.1.0.tar.gz\",\n)\n```\n\nand depend on the libraries you need from the\n[`BUILD.bazel`](BUILD.bazel) file. Each library also has an associated\n`Library_opt` target (such as `SwiftSyntax_opt`) which forces\nSwiftSyntax to always build with optimizations enabled. This may help\nlocal runtime performance at the cost of debuggability, and initial\nbuild time. Please tag any [issues](https://github.com/swiftlang/swift-syntax/issues) related to the Bazel configuration with the label \"Bazel\".\n\n## License\n\nPlease see [LICENSE](LICENSE.txt) for more information.\n",
      "stars_today": 1
    },
    {
      "id": 341374920,
      "name": "solr",
      "full_name": "apache/solr",
      "description": "Apache Solr open-source search software",
      "html_url": "https://github.com/apache/solr",
      "stars": 1547,
      "forks": 804,
      "language": "Java",
      "topics": [
        "backend",
        "information-retrieval",
        "java",
        "lucene",
        "nosql",
        "search",
        "search-engine",
        "solr"
      ],
      "created_at": "2021-02-23T00:12:42Z",
      "updated_at": "2026-01-14T19:09:51Z",
      "pushed_at": "2026-01-14T22:13:33Z",
      "open_issues": 252,
      "owner": {
        "login": "apache",
        "avatar_url": "https://avatars.githubusercontent.com/u/47359?v=4"
      },
      "readme": "<!--\n    Licensed to the Apache Software Foundation (ASF) under one or more\n    contributor license agreements.  See the NOTICE file distributed with\n    this work for additional information regarding copyright ownership.\n    The ASF licenses this file to You under the Apache License, Version 2.0\n    the \"License\"); you may not use this file except in compliance with\n    the License.  You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n -->\n\n# Welcome to the Apache Solr project!\n-----------------------------------\n\nSolr is the blazing-fast, open source, multi-modal search platform built on [Apache Lucene](https://lucene.apache.org/).\nIt powers full-text, vector, and geospatial search at many of the world's largest organizations.\n\n[![Build Status](https://ci-builds.apache.org/job/Solr/job/Solr-Artifacts-main/badge/icon?subject=Solr%20Artifacts)](https://ci-builds.apache.org/job/Solr/job/Solr-Artifacts-main/)\n[![Build Status](https://ci-builds.apache.org/job/Solr/job/Solr-Lint-main/badge/icon?subject=Solr%20Lint)](https://ci-builds.apache.org/job/Solr/job/Solr-Lint-main/)\n\nFor a complete description of the Solr project, team composition, source\ncode repositories, and other details, please see the Solr web site at\nhttps://solr.apache.org/\n\n## Download\n\nDownloads for Apache Solr distributions are available at https://solr.apache.org/downloads.html.\n\n## Running Solr\n\n### Installing Solr\n\nThe Reference Guide contains an entire [Deployment Guide](https://solr.apache.org/guide/solr/latest/deployment-guide/system-requirements.html) to walk you through installing Solr.\n\n### Running Solr in Docker\n\nYou can run Solr in Docker via the [official image](https://hub.docker.com/_/solr).\nLearn more about [Solr in Docker](https://solr.apache.org/guide/solr/latest/deployment-guide/solr-in-docker.html)\n\n### Running Solr on Kubernetes\n\nSolr has official support for running on Kubernetes, in the official Docker image.\nPlease refer to the [Solr Operator](https://solr.apache.org/operator) home for details, tutorials and instructions.\n\n## How to Use\n\nSolr includes a few examples to help you get started. To run a specific example, enter:\n\n```\n  bin/solr start -e <EXAMPLE> where <EXAMPLE> is one of:\n    cloud:         SolrCloud example\n    techproducts:  Comprehensive example illustrating many of Solr's core capabilities\n    schemaless:    Schema-less example (schema is inferred from data during indexing)\n    films:         Example of starting with _default configset and adding explicit fields dynamically    \n```\n\nFor instance, if you want to run the techproducts example, enter:\n\n```\n  bin/solr start -e techproducts\n```\n\nFor a more in-depth introduction, please check out the [tutorials in the Solr Reference\nGuide](https://solr.apache.org/guide/solr/latest/getting-started/solr-tutorial.html).\n\n\n## Support\n\n- [Users Mailing List](https://solr.apache.org/community.html#mailing-lists-chat)\n- Slack: Solr Community Channel.  Sign up at https://s.apache.org/solr-slack\n- IRC: `#solr` on [libera.chat](https://web.libera.chat/?channels=#solr)\n\n## Developer Documentation\n\nLearn more about developing Solr by reading through the developer docs in [./dev-docs](./dev-docs) source tree or building Solr from source in [./dev-docs/solr-source-code.adoc](./dev-docs/solr-source-code.adoc)\n\n### Quickstart\n\nSolr uses [Gradle](https://gradle.org/) for its build system. Here are some useful hints to build and run Solr locally:\n\n- To build a Solr dev distribution:\n\n```\n./gradlew dev\n```\n\n- To run the Solr dev distribution locally:\n\n```\ncd ./solr/packaging/build/dev\nbin/solr start\n```\n\n- Open a web browser and go to http://localhost:8983/solr/ to access the Solr Admin interface. You can also use the `bin/solr` script to create and manage Solr collections. For example use the `bin/solr post` tool to index some sample data.\n\n## Get Involved\nPlease review [CONTRIBUTING.md](CONTRIBUTING.md) for information on contributing to the project.\n\nTo get involved in the developer community:\n\n- [Mailing Lists](https://solr.apache.org/community.html#mailing-lists-chat)\n- Slack: `#solr-dev` in the `the-asf` organization.  Sign up at https://the-asf.slack.com/messages/CE70MDPMF\n- [Issue Tracker (JIRA)](https://issues.apache.org/jira/browse/SOLR)\n- IRC: `#solr-dev` on [libera.chat](https://web.libera.chat/?channels=#solr-dev)\n",
      "stars_today": 1
    },
    {
      "id": 53432472,
      "name": "sonic-buildimage",
      "full_name": "sonic-net/sonic-buildimage",
      "description": "Scripts which perform an installable binary image build for SONiC",
      "html_url": "https://github.com/sonic-net/sonic-buildimage",
      "stars": 907,
      "forks": 1694,
      "language": "C",
      "topics": [
        "hacktoberfest",
        "sonic"
      ],
      "created_at": "2016-03-08T17:42:37Z",
      "updated_at": "2026-01-14T21:17:23Z",
      "pushed_at": "2026-01-14T21:17:15Z",
      "open_issues": 2541,
      "owner": {
        "login": "sonic-net",
        "avatar_url": "https://avatars.githubusercontent.com/u/102750714?v=4"
      },
      "readme": "*master builds*:\n\n[![Broadcom](https://dev.azure.com/mssonic/build/_apis/build/status/broadcom/Azure.sonic-buildimage.official.broadcom?branchName=master&label=Broadcom)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=138&branchName=master)\n[![Mellanox](https://dev.azure.com/mssonic/build/_apis/build/status/mellanox/Azure.sonic-buildimage.official.mellanox?branchName=master&label=Mellanox)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=139&branchName=master)\n[![Marvell-Teralynx](https://dev.azure.com/mssonic/build/_apis/build/status/innovium/Azure.sonic-buildimage.official.marvell-teralynx?branchName=master&label=Marvell-Teralynx)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=2432&branchName=master)\n[![Marvell-Prestera(armhf)](https://dev.azure.com/mssonic/build/_apis/build/status/marvell/Azure.sonic-buildimage.official.marvell-prestera-armhf?branchName=master&label=Marvell-Prestera-armhf)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=141&branchName=master)\n[![Marvell-Prestera(arm64)](https://dev.azure.com/mssonic/build/_apis/build/status/marvell/Azure.sonic-buildimage.official.marvell-prestera-arm64?branchName=master&label=Marvell-Prestera-arm64)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=999&branchName=master)\n[![Nvidia-Bluefield](https://dev.azure.com/mssonic/build/_apis/build/status/nvidia/Azure.sonic-buildimage.official.nvidia-bluefield?branchName=master&label=Nvidia-Bluefield)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=1665&branchName=master)\n[![VS](https://dev.azure.com/mssonic/build/_apis/build/status/vs/Azure.sonic-buildimage.official.vs?branchName=master&label=VS)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=142&branchName=master)\n\n*202511 builds*:\n\n[![Broadcom](https://dev.azure.com/mssonic/build/_apis/build/status/broadcom/Azure.sonic-buildimage.official.broadcom?branchName=202511&label=Broadcom)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=138&branchName=202511)\n[![Mellanox](https://dev.azure.com/mssonic/build/_apis/build/status/mellanox/Azure.sonic-buildimage.official.mellanox?branchName=202511&label=Mellanox)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=139&branchName=202511)\n[![Marvell-Teralynx](https://dev.azure.com/mssonic/build/_apis/build/status/innovium/Azure.sonic-buildimage.official.marvell-teralynx?branchName=202511&label=Marvell-Teralynx)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=2432&branchName=202511)\n[![Marvell-Prestera(armhf)](https://dev.azure.com/mssonic/build/_apis/build/status/marvell/Azure.sonic-buildimage.official.marvell-prestera-armhf?branchName=202511&label=Marvell-Prestera-armhf)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=141&branchName=202511)\n[![Marvell-Prestera(arm64)](https://dev.azure.com/mssonic/build/_apis/build/status/marvell/Azure.sonic-buildimage.official.marvell-prestera-arm64?branchName=202511&label=Marvell-Prestera-arm64)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=999&branchName=202511)\n[![Nvidia-Bluefield](https://dev.azure.com/mssonic/build/_apis/build/status/nvidia/Azure.sonic-buildimage.official.nvidia-bluefield?branchName=202511&label=Nvidia-Bluefield)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=1665&branchName=202511)\n[![VS](https://dev.azure.com/mssonic/build/_apis/build/status/vs/Azure.sonic-buildimage.official.vs?branchName=202511&label=VS)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=142&branchName=202511)\n\n*202505 builds*:\n\n[![Broadcom](https://dev.azure.com/mssonic/build/_apis/build/status/broadcom/Azure.sonic-buildimage.official.broadcom?branchName=202505&label=Broadcom)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=138&branchName=202505)\n[![Mellanox](https://dev.azure.com/mssonic/build/_apis/build/status/mellanox/Azure.sonic-buildimage.official.mellanox?branchName=202505&label=Mellanox)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=139&branchName=202505)\n[![Marvell(armhf)](https://dev.azure.com/mssonic/build/_apis/build/status/marvell/Azure.sonic-buildimage.official.marvell-armhf?branchName=202505&label=Marvell-armhf)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=141&branchName=202505)\n[![Marvell(arm64)](https://dev.azure.com/mssonic/build/_apis/build/status/marvell/Azure.sonic-buildimage.official.marvell-arm64?branchName=202505&label=Marvell-arm64)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=999&branchName=202505)\n[![Nvidia-Bluefield](https://dev.azure.com/mssonic/build/_apis/build/status/nvidia/Azure.sonic-buildimage.official.nvidia-bluefield?branchName=202505&label=Nvidia-Bluefield)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=1665&branchName=202505)\n[![VS](https://dev.azure.com/mssonic/build/_apis/build/status/vs/Azure.sonic-buildimage.official.vs?branchName=202505&label=VS)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=142&branchName=202505)\n\n*202411 builds*:\n\n[![Broadcom](https://dev.azure.com/mssonic/build/_apis/build/status/broadcom/Azure.sonic-buildimage.official.broadcom?branchName=202411&label=Broadcom)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=138&branchName=202411)\n[![Centec](https://dev.azure.com/mssonic/build/_apis/build/status/centec/Azure.sonic-buildimage.official.centec?branchName=202411&label=Centec)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=143&branchName=202411)\n[![Centec(arm64)](https://dev.azure.com/mssonic/build/_apis/build/status/centec/Azure.sonic-buildimage.official.centec-arm64?branchName=202411&label=Centec-arm64)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=140&branchName=202411)\n[![Mellanox](https://dev.azure.com/mssonic/build/_apis/build/status/mellanox/Azure.sonic-buildimage.official.mellanox?branchName=202411&label=Mellanox)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=139&branchName=202411)\n[![Marvell(armhf)](https://dev.azure.com/mssonic/build/_apis/build/status/marvell/Azure.sonic-buildimage.official.marvell-armhf?branchName=202411&label=Marvell-armhf)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=141&branchName=202411)\n[![Marvell(arm64)](https://dev.azure.com/mssonic/build/_apis/build/status/marvell/Azure.sonic-buildimage.official.marvell-arm64?branchName=202411&label=Marvell-arm64)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=999&branchName=202411)\n[![Nvidia-Bluefield](https://dev.azure.com/mssonic/build/_apis/build/status/nvidia/Azure.sonic-buildimage.official.nvidia-bluefield?branchName=202411&label=Nvidia-Bluefield)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=1665&branchName=202411)\n[![VS](https://dev.azure.com/mssonic/build/_apis/build/status/vs/Azure.sonic-buildimage.official.vs?branchName=202411&label=VS)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=142&branchName=202411)\n\n*202405 builds*:\n\n[![Broadcom](https://dev.azure.com/mssonic/build/_apis/build/status/broadcom/Azure.sonic-buildimage.official.broadcom?branchName=202405&label=Broadcom)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=138&branchName=202405)\n[![Centec](https://dev.azure.com/mssonic/build/_apis/build/status/centec/Azure.sonic-buildimage.official.centec?branchName=202405&label=Centec)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=143&branchName=202405)\n[![Centec(arm64)](https://dev.azure.com/mssonic/build/_apis/build/status/centec/Azure.sonic-buildimage.official.centec-arm64?branchName=202405&label=Centec-arm64)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=140&branchName=202405)\n[![Mellanox](https://dev.azure.com/mssonic/build/_apis/build/status/mellanox/Azure.sonic-buildimage.official.mellanox?branchName=202405&label=Mellanox)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=139&branchName=202405)\n[![Marvell(armhf)](https://dev.azure.com/mssonic/build/_apis/build/status/marvell/Azure.sonic-buildimage.official.marvell-armhf?branchName=202405&label=Marvell-armhf)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=141&branchName=202405)\n[![Marvell(arm64)](https://dev.azure.com/mssonic/build/_apis/build/status/marvell/Azure.sonic-buildimage.official.marvell-arm64?branchName=202405&label=Marvell-arm64)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=999&branchName=202405)\n[![Nvidia-Bluefield](https://dev.azure.com/mssonic/build/_apis/build/status/nvidia/Azure.sonic-buildimage.official.nvidia-bluefield?branchName=202405&label=Nvidia-Bluefield)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=1665&branchName=202405)\n[![VS](https://dev.azure.com/mssonic/build/_apis/build/status/vs/Azure.sonic-buildimage.official.vs?branchName=202405&label=VS)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=142&branchName=202405)\n\n## SONiC Image Azure Pipelines\n\nAll SONiC project build pipelines can be found at [Download Portal for SONiC Images](https://sonic-build.azurewebsites.net/pipelines)\n\n# sonic-buildimage\n\n## Build SONiC Switch Images\n\n# Description\n\nFollowing are the instructions on how to build an [(ONIE)](https://github.com/opencomputeproject/onie)\ncompatible network operating system (NOS) installer image for network switches,\nand also how to build docker images running inside the NOS.\nNote that SONiC images are build per ASIC platform.\nSwitches using the same ASIC platform share a common image.\nFor a list of supported switches and ASIC, please refer to this [list](https://github.com/sonic-net/SONiC/wiki/Supported-Devices-and-Platforms)\n\n# Hardware\n\nAny server can be a build image server as long as it has:\n\n* Multiple cores to increase build speed\n* Plenty of RAM (less than 8 GiB is likely to cause issues)\n* 300G of free disk space\n* KVM Virtualization Support.\n\n> Note: If you are in a VM, make sure you have support for nested virtualization.\n> Some cases (e.g. building OVS image) also requires extra configuration\n> options to expose the full KVM interface to the VM\n> (e.g. [the KVM paravirtualization support on VirtualBox](https://www.virtualbox.org/manual/ch10.html#gimproviders)).\n\nA good choice of OS for building SONiC is currently Ubuntu 22.04.\n\n## Automated prerequisites installation and repository cloning\n\nFor convenience, you can use the automated prerequisites script to handle both prerequisites installation and repository cloning:\n\n```shell\ncurl -sSL https://raw.githubusercontent.com/sonic-net/sonic-buildimage/master/scripts/prerequisites.sh | bash\n```\n\nThis script will automatically:\n* Install required packages (pip, jinja, Docker)\n* Configure Docker for non-root usage\n* Clone the repository with all submodules\n\nAfter completing this step, proceed to the [Usage](#usage) section below.\n\n## Manual prerequisites installation\n\n* Install pip and jinja in host build machine, execute below commands\n   if j2/jinjanator is not available:\n\n```shell\nsudo apt install -y python3-pip\npip3 install --user jinjanator\n```\n\n> **Note:** If you cannot run the `j2` command after installation, this is likely because the `~/.local/bin` directory was just created and is not yet included in your `$PATH`. Please log out and log back in to refresh your environment, then test the command again.\n\n\n* Install [Docker](https://docs.docker.com/engine/install/) and configure your\n  system to allow running the 'docker' command without 'sudo':\n  * Add current user to the docker group: `sudo gpasswd -a ${USER} docker`\n  * Log out and log back in so that your group membership is re-evaluated\n  * If you are using Linux kernel 5.3 or newer, then you must use Docker 20.10.10 or newer. This is because older Docker versions did not allow the `clone3` syscall, which is now used in Bookworm.\n\n> Note: If a previous installation of Docker using snap was present on the\n> system, remove it and also remove docker from snap before reinstallating docker.\n> This will avoid [known bugs that falsely report read-only filesystems issues](https://stackoverflow.com/questions/52526219/docker-mkdir-read-only-file-system)\n> during the build process.\n\n## Manual clone the repository with all the git submodules\n\nTo clone the code repository recursively:\n\n```shell\ngit clone --recurse-submodules https://github.com/sonic-net/sonic-buildimage.git\n```\n\n## Usage\n\nTo build SONiC installer image and docker images, run the following commands:\n\n```shell\n# Ensure the 'overlay' module is loaded on your development system\nsudo modprobe overlay\n\n# Enter the source directory\ncd sonic-buildimage\n\n# (Optional) Checkout a specific branch. By default, it uses master branch.\n# For example, to checkout the branch 201911, use \"git checkout 201911\"\ngit checkout [branch_name]\n\n# Execute make init once after cloning the repo,\n# or after fetching remote repo with submodule updates\nmake init\n\n# Execute make configure once to configure ASIC\nmake configure PLATFORM=[ASIC_VENDOR]\n\n# Build SONiC image with 4 jobs in parallel.\n# Note: You can set this higher, but 4 is a good number for most cases\n#       and is well-tested.\nmake SONIC_BUILD_JOBS=4 all\n```\n\nThe supported ASIC vendors are:\n\n* PLATFORM=barefoot\n* PLATFORM=broadcom\n* PLATFORM=marvell-prestera\n* PLATFORM=marvell-teralynx\n* PLATFORM=mellanox\n* PLATFORM=centec\n* PLATFORM=nephos\n* PLATFORM=nvidia-bluefield\n* PLATFORM=vs\n\n## Usage for ARM Architecture\n\n```shell\nsudo apt-get install --allow-downgrades -y docker-ce=5:18.09.0~3-0~ubuntu-xenial\nsudo apt-get install --allow-downgrades -y docker-ce-cli=5:18.09.0~3-0~ubuntu-xenial\n```\n\nTo build Arm32 bit for (ARMHF) platform\n\n```shell\n# Execute make configure once to configure ASIC and ARCH\nmake configure PLATFORM=[ASIC_VENDOR] PLATFORM_ARCH=armhf\nmake target/sonic-[ASIC_VENDER]-armhf.bin\n```\n\n_example:_\n\n```shell\nmake configure PLATFORM=marvell-prestera PLATFORM_ARCH=armhf\nmake target/sonic-marvell-prestera-armhf.bin\n```\n\nTo build Arm32 bit for (ARMHF) Marvell Prestera platform on amd64 host for debian buster\nusing cross-compilation, run the following commands:\n\n```shell\n# Execute make configure once to configure ASIC and ARCH for cross-compilation build\n\nNOJESSIE=1 NOSTRETCH=1 BLDENV=buster CROSS_BLDENV=1 \\\nmake configure PLATFORM=marvell-prestera PLATFORM_ARCH=armhf\n\n# Execute Arm32 build using cross-compilation environment\n\nNOJESSIE=1 NOSTRETCH=1 BLDENV=buster CROSS_BLDENV=1 make target/sonic-marvell-prestera-armhf.bin\n```\n\nRunning the above Arm32 build using cross-compilation instead of qemu emulator\ndrastically reduces the build time.\n\nTo build Arm64 bit for platform\n\n```shell\n# Execute make configure once to configure ASIC and ARCH\n\nmake configure PLATFORM=[ASIC_VENDOR] PLATFORM_ARCH=arm64\n\n# example:\n\nmake configure PLATFORM=marvell-prestera PLATFORM_ARCH=arm64\n```\n\n **NOTE**:\n\n* Recommend reserving at least 100G free space to build one platform\n  with a single job.\n  The build process will use more disk if you are setting `SONIC_BUILD_JOBS`\n  to more than 1.\n* If Docker's workspace folder, `/var/lib/docker`,\n  resides on a partition without sufficient free space,\n  you may encounter an error like the following during a Docker container build job:\n\n    `/usr/bin/tar: /path/to/sonic-buildimage/<some_file>:\n     Cannot write: No space left on device`\n\n    The solution is to [move the directory](https://www.ibm.com/docs/en/z-logdata-analytics/5.1.0?topic=compose-relocating-docker-root-directory)\n    to a partition with more free space.\n* Use\n  `http_proxy=[your_proxy] https_proxy=[your_proxy] no_proxy=[your_no_proxy] make`\n  to enable http(s) proxy in the build process.\n* Add your user account to `docker` group and use your user account to make.\n  `root` or `sudo` are not supported.\n* For more details on cross-compilation errors, please refer to [README.arm64_build_on_amd64.md](https://github.com/sonic-net/sonic-buildimage/blob/master/README.arm64_build_on_amd64.md)\n\nThe SONiC installer contains all docker images needed.\nSONiC uses one image for all devices of a same ASIC vendor.\n\nFor Broadcom ASIC, we build ONIE and EOS image.\nEOS image is used for Arista devices,\nONIE image is used for all other Broadcom ASIC based devices.\n\n```shell\nmake configure PLATFORM=broadcom\n# build debian stretch required targets\nBLDENV=stretch make stretch\n# build ONIE image\nmake target/sonic-broadcom.bin\n# build EOS image\nmake target/sonic-aboot-broadcom.swi\n```\n\nYou may find the rules/config file useful.\nIt contains configuration options for the build process,\nlike adding more verbosity or showing dependencies,\nusername and password for base image etc.\n\nEvery docker image is built and saved to target/ directory.\nSo, for instance, to build only docker-database, execute:\n\n```shell\nmake target/docker-database.gz\n```\n\nSame goes for debian packages, which are under target/debs/:\n\n```shell\nmake target/debs/swss_1.0.0_amd64.deb\n```\n\nEvery target has a clean target, so in order to clean swss, execute:\n\n```shell\nmake target/debs/swss_1.0.0_amd64.deb-clean\n```\n\nIt is recommended to use clean targets to clean all packages that are built together,\nlike dev packages for instance.\nIn order to be more familiar with build process and make some changes to it,\nit is recommended to read this short [Documentation](README.buildsystem.md).\n\n## Build debug dockers and debug SONiC installer image\n\nSONiC build system supports building dockers and ONIE-image with debug tools\nand debug symbols, to help with live & core debugging.\nFor details refer to [SONiC Buildimage Guide](https://github.com/sonic-net/sonic-buildimage/blob/master/README.buildsystem.md).\n\n## SAI Version\n\nPlease refer to [SONiC roadmap](https://github.com/sonic-net/SONiC/wiki/Sonic-Roadmap-Planning)\non the SAI version for each SONiC release.\n\n## Notes\n\n* If you are running make for the first time, a sonic-slave-${USER} docker image\n  will be built automatically.\n  This may take a while, but it is a one-time action, so please be patient.\n* The root user account is disabled. However, the created user can `sudo`.\n* The target directory is `./target`, containing the NOS installer image\n  and docker images.\n  * sonic-generic.bin: SONiC switch installer image (ONIE compatible)\n  * sonic-aboot.bin: SONiC switch installer image (Aboot compatible)\n  * docker-base.gz: base docker image where other docker images are built from,\n    only used in build process (gzip tar archive)\n  * docker-database.gz: docker image for in-memory key-value store,\n    used as inter-process communication (gzip tar archive)\n  * docker-fpm.gz: docker image for quagga with fpm module enabled\n    (gzip tar archive)\n  * docker-orchagent.gz: docker image for SWitch State Service (SWSS)\n    (gzip tar archive)\n  * docker-syncd-brcm.gz: docker image for the daemon to sync database\n    and Broadcom switch ASIC (gzip tar archive)\n  * docker-syncd-cavm.gz: docker image for the daemon to sync database\n    and Cavium switch ASIC (gzip tar archive)\n  * docker-syncd-mlnx.gz: docker image for the daemon to sync database\n    and Mellanox switch ASIC (gzip tar archive)\n  * docker-syncd-nephos.gz: docker image for the daemon to sync database\n    and Nephos switch ASIC (gzip tar archive)\n  * docker-syncd-mrvl-teralynx.gz: docker image for the daemon to sync database\n    and Marvell-Teralynx switch ASIC (gzip tar archive)\n  * docker-syncd-mrvl-prestera.gz: docker image for the daemon to sync database\n    and Marvell-Prestera switch ASIC (gzip tar archive)\n  * docker-sonic-p4.gz: docker image for all-in-one for p4 software switch\n    (gzip tar archive)\n  * docker-sonic-vs.gz: docker image for all-in-one for software virtual switch\n    (gzip tar archive)\n  * docker-sonic-mgmt.gz: docker image for\n    [managing, configuring and monitoring SONiC](https://github.com/sonic-net/sonic-mgmt)\n    (gzip tar archive)\n\n## Contribution Guide\n\nAll contributors must sign a contribution license agreement before contributions\ncan be accepted.\nVisit [EasyCLA - Linux Foundation](https://easycla.lfx.linuxfoundation.org).\n\n## GitHub Workflow\n\nWe're following basic GitHub Flow.\nIf you have no idea what we're talking about, check out [GitHub's official guide](https://guides.github.com/introduction/flow/).\nNote that merge is only performed by the repository maintainer.\n\nGuide for performing commits:\n\n* Isolate each commit to one component/bugfix/issue/feature\n* Use a standard commit message format:\n\n> [component/folder touched]: Description intent of your changes\n>\n> [List of changes]\n>\n> Signed-off-by: Your Name your@email.com\n\nFor example:\n\n> swss-common: Stabilize the ConsumerTable\n>\n> * Fixing autoreconf\n> * Fixing unit-tests by adding checkers and initialize the DB before start\n> * Adding the ability to select from multiple channels\n> * Health-Monitor - The idea of the patch is that if something went wrong\n>   with the notification channel,\n>   we will have the option to know about it (Query the LLEN table length).\n>\n> Signed-off-by: user@dev.null\n\n* Each developer should fork this repository and [add the team as a Contributor](https://help.github.com/articles/adding-collaborators-to-a-personal-repository)\n* Push your changes to your private fork and do \"pull-request\" to this repository\n* Use a pull request to do code review\n* Use issues to keep track of what is going on\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com)\nwith any additional questions or comments.\n",
      "stars_today": 1
    },
    {
      "id": 30017750,
      "name": "ComplexHeatmap",
      "full_name": "jokergoo/ComplexHeatmap",
      "description": "Make Complex Heatmaps ",
      "html_url": "https://github.com/jokergoo/ComplexHeatmap",
      "stars": 1458,
      "forks": 243,
      "language": "R",
      "topics": [
        "clustering",
        "complex-heatmaps",
        "heatmap"
      ],
      "created_at": "2015-01-29T11:45:58Z",
      "updated_at": "2026-01-14T17:52:30Z",
      "pushed_at": "2025-06-23T15:51:50Z",
      "open_issues": 227,
      "owner": {
        "login": "jokergoo",
        "avatar_url": "https://avatars.githubusercontent.com/u/449218?v=4"
      },
      "readme": "# Make Complex Heatmaps <a href=\"https://jokergoo.github.io/ComplexHeatmap-reference/book/\"><img src=\"https://jokergoo.github.io/ComplexHeatmap-reference/book/complexheatmap-cover.jpg\" width=240 align=\"right\" style=\"border:2px solid black;\" ></a>\n\n[![R-CMD-check](https://github.com/jokergoo/ComplexHeatmap/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/ComplexHeatmap/actions)\n[![codecov](https://img.shields.io/codecov/c/github/jokergoo/ComplexHeatmap.svg)](https://codecov.io/github/jokergoo/ComplexHeatmap) \n[![bioc](http://www.bioconductor.org/shields/downloads/devel/ComplexHeatmap.svg)](https://bioconductor.org/packages/stats/bioc/ComplexHeatmap/) \n[![bioc](http://www.bioconductor.org/shields/years-in-bioc/ComplexHeatmap.svg)](http://bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html)\n\n<img src=\"http://jokergoo.github.io/complexheatmap_logo.svg\" width=\"550\">\n\n\nComplex heatmaps are efficient to visualize associations between different\nsources of data sets and reveal potential patterns. Here the\n**ComplexHeatmap** package provides a highly flexible way to arrange multiple\nheatmaps and supports various annotation graphics.\n\nThe [**InteractiveComplexHeatmap**](https://github.com/jokergoo/InteractiveComplexHeatmap) package can directly export static complex heatmaps into an interactive Shiny app. Have a try!\n\n## Citation\n\nZuguang Gu, et al., [Complex heatmaps reveal patterns and correlations in multidimensional genomic data](http://bioinformatics.oxfordjournals.org/content/early/2016/05/20/bioinformatics.btw313.abstract), Bioinformatics, 2016.\n\nZuguang Gu. [Complex Heatmap Visualization](https://doi.org/10.1002/imt2.43), iMeta, 2022. \n\n\n## Install\n\n`ComplexHeatmap` is available on [Bioconductor](http://www.bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html), you can install it by:\n\n```r\nif (!requireNamespace(\"BiocManager\", quietly=TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"ComplexHeatmap\")\n```\n\nIf you want the latest version, install it directly from GitHub:\n\n```r\nlibrary(devtools)\ninstall_github(\"jokergoo/ComplexHeatmap\")\n```\n\n## Usage\n\nMake a single heatmap:\n\n```r\nHeatmap(mat, ...)\n```\n\nA single Heatmap with column annotations:\n\n```r\nha = HeatmapAnnotation(df = anno1, anno_fun = anno2, ...)\nHeatmap(mat, ..., top_annotation = ha)\n```\n\nMake a list of heatmaps:\n\n```r\nHeatmap(mat1, ...) + Heatmap(mat2, ...)\n```\n\nMake a list of heatmaps and row annotations:\n\n```r\nha = HeatmapAnnotation(df = anno1, anno_fun = anno2, ..., which = \"row\")\nHeatmap(mat1, ...) + Heatmap(mat2, ...) + ha\n```\n\n## Documentation\n\nThe full documentations are available at https://jokergoo.github.io/ComplexHeatmap-reference/book/ and the website is at https://jokergoo.github.io/ComplexHeatmap.\n\n## Blog posts\n\nThere are following blog posts focusing on specific topics:\n\n- [Make 3D heatmap](https://jokergoo.github.io/2021/03/24/3d-heatmap/)\n- [Translate from pheatmap to ComplexHeatmap](https://jokergoo.github.io/2020/05/06/translate-from-pheatmap-to-complexheatmap/)\n- [Set cell width/height in the heatmap](https://jokergoo.github.io/2020/05/11/set-cell-width/height-in-the-heatmap/)\n- [Interactive ComplexHeatmap](https://jokergoo.github.io/2020/05/15/interactive-complexheatmap/)\n- [Word cloud as heatmap annotation](https://jokergoo.github.io/2020/05/31/word-cloud-as-heatmap-annotation/)\n- [Which heatmap function is faster?](https://jokergoo.github.io/2020/06/19/which-heatmap-function-is-faster/)\n- [Rasterization in ComplexHeatmap](https://jokergoo.github.io/2020/06/30/rasterization-in-complexheatmap/)\n- [Block annotation over several slices](https://jokergoo.github.io/2020/07/06/block-annotation-over-several-slices/)\n- [Integrate ComplexHeatmap with cowplot package](https://jokergoo.github.io/2020/07/14/integrate-complexheatmap-with-cowplot-package/)\n\n\n## Examples\n\n### Visualize Methylation Profile with Complex Annotations\n\n![complexheatmap_example4](https://user-images.githubusercontent.com/449218/47718635-2ec22980-dc49-11e8-9f01-37becb19e0d5.png)\n\n### Correlations between methylation, expression and other genomic features\n\n![complexheatmap_example3](https://user-images.githubusercontent.com/449218/47718636-2ec22980-dc49-11e8-8db0-1659c27dcf40.png)\n\n### Visualize Cell Heterogeneity from Single Cell RNASeq\n\n![complexheatmap_example2](https://user-images.githubusercontent.com/449218/47718637-2ec22980-dc49-11e8-925e-955c16cfa982.png)\n\n### Making Enhanced OncoPrint\n\n![complexheatmap_example1](https://user-images.githubusercontent.com/449218/47718638-2ec22980-dc49-11e8-845e-21e51d3b8e73.png)\n\n### UpSet plot\n\n<img src=\"https://user-images.githubusercontent.com/449218/102615477-48c76a80-4136-11eb-98d9-3c528844fbe8.png\" width=500 />\n\n### 3D heatmap\n\n![image](https://user-images.githubusercontent.com/449218/112284448-8c77c600-8c89-11eb-8d38-c5538900df20.png)\n\n\n\n## License\n\nMIT @ Zuguang Gu\n\n",
      "stars_today": 1
    },
    {
      "id": 177906917,
      "name": "SwiftTerm",
      "full_name": "migueldeicaza/SwiftTerm",
      "description": "Xterm/VT100 Terminal emulator in Swift",
      "html_url": "https://github.com/migueldeicaza/SwiftTerm",
      "stars": 1258,
      "forks": 204,
      "language": "Swift",
      "topics": [
        "appkit",
        "imgcat",
        "ios",
        "macos",
        "sixel",
        "ssh-connection",
        "swift",
        "terminal-engine",
        "uikit",
        "vt100",
        "xterm",
        "xterm-256color"
      ],
      "created_at": "2019-03-27T02:52:33Z",
      "updated_at": "2026-01-13T19:09:23Z",
      "pushed_at": "2026-01-13T14:30:48Z",
      "open_issues": 62,
      "owner": {
        "login": "migueldeicaza",
        "avatar_url": "https://avatars.githubusercontent.com/u/36863?v=4"
      },
      "readme": "\nSwiftTerm\n=========\n\nSwiftTerm is a VT100/Xterm terminal emulator library for Swift applications that can be \nembedded into macOS, iOS applications, text-based, headless applications or other \ncustom scenarios. It has been used in several commercially available SSH clients, including \n[Secure Shellfish](https://apps.apple.com/us/app/secure-shellfish-ssh-files/id1336634154), \n [La Terminal](https://apps.apple.com/us/app/la-terminal-ssh-client/id1629902861) and [CodeEdit](https://github.com/CodeEditApp/CodeEdit)\n\nCheck the [API Documentation](https://migueldeicaza.github.io/SwiftTermDocs/documentation/swiftterm/)\n\nThis repository contains both a terminal emulator engine that is UI agnostic, as well as\nfront-ends for this engine for iOS using UIKit, and macOS using AppKit.   A curses-based\nterminal emulator (to emulate an xterm inside a console application) is available as\npart of the [TermKit](https://github.com/migueldeicaza/TermKit) library. \n\n**Sample Code** There are a couple of minimal sample apps for Mac and iOS showing how to \nuse the library inside the `TerminalApp` directory.   \n\n* The sample Mac app has much of the functionality of MacOS' Terminal.app, but without the configuration UI.   \n* The sample iOS application uses an SSH library to connect to a remote system (as there is no native shell \non iOS to run), and the sample happens to be hardcoded to my home machine, you can change that in the source\ncode. \n\n## Companion Apps\n\n[SwiftTermApp](https://github.com/migueldeicaza/SwiftTermApp) builds\nan actual iOS app that uses this library and is more complete than the\ntesting apps in this module and provides a proper configuration UI.\nIt is a proof of concept for what you would need to do.\n\n[Pane](https://github.com/migueldeicaza/pane) is a terminal\nmultiplexor, similar to tmux.\n\n## History\n\nThis is a port of my original\n[XtermSharp](https://github.com/migueldeicaza/XtermSharp), which was itself\nbased on [xterm.js](https://xtermjs.org).  At this point, I consider SwiftTerm\nto be a more advanced terminal emulator than both of those (modulo\nSelection/Accessibility) as it handles UTF, Unicode and grapheme clusters better\nthan those and has a more complete coverage of terminal emulation.   XtermSharp\nis generally attempting to keep up, but has lagged behind.\n\nFeatures\n========\n\n* Pretty decent terminal emulation, on or better than XtermSharp and xterm.js (and more comprehensive in many ways)\n* Unicode rendering (including Emoji, and combining characters and emoji)\n* Reusable and pluggable engine allows multiple user interfaces to be built on top of it:\n   *  Bundled MacOS and iOS\n   *  Bundled Headless terminal.\n   *  [TermKit](https://github.com/migueldeicaza/TermKit) contains a terminal-over-a-terminal\n   *  [Pane](https://github.com/migueldeicaza/pane) implements a terminal multiplexor\n* Selection engine (with macOS support in the view)\n* Supports colors (ANSI, 256, TrueColor)\n* Supports mouse events\n* Supports terminal resizing operations (controlled by remote host, or locally)\n* [Hyperlinks](https://gist.github.com/egmontkob/eb114294efbcd5adb1944c9f3cb5feda) in terminal output\n* Local process and SSH connection support (some assembly required for the last one)\n* Proper CoreText rendering can munch through the hardened Unicode test suites.\n* Graphics support:\n  * Sixel (Use img2sixel to test)\n  * iTerm2-style graphic rendering (Use imgcat to test)\n  * Kitty graphics (Use kittyimg to test)\n* Terminal session recording and playback with termcast\n* Fuzzed and abused\n* Seems pretty fast to me\n\n# SwiftTerm library\n\nThe SwiftTerm library itself contains the source code for both\nthe engine and the front-ends.  The front-ends are conditionally\ncompiled based on the target platform.\n\nThe engine is in this directory, while code for macOS lives under `Mac`, and\ncode for iOS, lives under `iOS`.    Given that those two share a lot of common \ntraits, the shared code is under `Apple`.\n\n## Using SwiftTerm\n\nSwiftTerm uses the Swift Package Manager for its build, and you can\nadd the library to your project by using the url for this project or a\nfork of it.\n\n## MacOS NSView \nThe macOS AppKit NSView implementation [`TerminalView`](https://migueldeicaza.github.io/SwiftTermDocs/documentation/swiftterm/terminalview) is a reusable\nNSView control that can be connected to any source by implementing the\n[`TerminalViewDelegate`](https://migueldeicaza.github.io/SwiftTermDocs/documentation/swiftterm/terminalviewdelegate).  \nI anticipate that a common scenario will be\nto host a local Unix command, so I have included\n[`LocalProcessTerminalView`](https://migueldeicaza.github.io/SwiftTermDocs/documentation/swiftterm/localprocessterminalview)\n which is an implementation that connects\nthe `TerminalView` to a Unix pseudo-terminal and runs a command there.\n\n## iOS UIView\nThere is an equivalent UIKit UIView implementation for\n[`TerminalView`](https://migueldeicaza.github.io/SwiftTermDocs/documentation/swiftterm/terminalview)\nwhich like its NSView companion is an embeddable and reusable view\nthat can be connected to your application by implementing the same\nTerminalViewDelegate.  Unlike the NSView case running on a Mac, where\na common scenario will be to run local commands, given that iOS does\nnot offer access to processes, the most common scenario will be to\nwire up this terminal to a remote host.  And the safest way of\nconnecting to a remote system is with SSH.\n\n## Shared Code between MacOS and iOS\n\nThe iOS and UIKit code share a lot of the code, that code lives under the Apple directory.\n\n## Using SSH\nThe core library currently does not provide a convenient way to connect to SSH, purely\nto avoid the additional dependency.   But this git module references a module that pulls\na precompiled SSH client ([Frugghi's SwiftSH](https://github.com/migueldeicaza/SwiftSH)), along with \na [`UIKitSsshTerminalView`](https://github.com/migueldeicaza/SwiftTerm/blob/main/TerminalApp/iOSTerminal/UIKitSshTerminalView.swift)\nin the iOS sample that that connects the `TerminalView` for iOS to an SSH connection.\n\n## Termcast - Terminal Recording and Playback\n\nSwiftTerm includes a `termcast` command-line tool that can record and playback terminal sessions in the [asciinema](https://asciinema.org/) `.cast` format. This tool is built using SwiftTerm's `LocalProcess` functionality.\n\n### Recording Sessions\n\nTo record a terminal session:\n\n```bash\nswift run termcast record output.cast\n```\n\nOptions:\n- `--command` / `-c`: Specify a command to run (defaults to your shell)\n- `--timeout` / `-t`: Set an automatic timeout in seconds\n\nExamples:\n```bash\n# Record an interactive shell session\nswift run termcast record my-session.cast\n\n# Record a specific command\nswift run termcast record -c \"ls -la && echo 'Done'\" command-demo.cast\n\n# Record with a 30-second timeout\nswift run termcast record --timeout 30 timed-session.cast\n```\n\n### Playing Back Sessions\n\nTo playback a recorded session:\n\n```bash\nswift run termcast playback my-session.cast\n```\n\nThe playback will show the recorded terminal session with proper timing, including both input and output as they occurred during recording.\n\n### Features\n\n- **Full input/output capture**: Records both user input and program output with precise timing\n- **Raw terminal mode**: Properly handles terminal control sequences and special keys\n- **asciinema compatibility**: Uses the standard `.cast` format for interoperability\n- **Live display**: Shows the session live while recording\n- **Proper terminal handling**: Maintains correct line endings and terminal state  \n\nWorking on SwiftTerm\n====================\n\nIf you are using Xcode, there are two toplevel projects, one for Mac\nand one for iOS in the TerminalApp directory, one called \"iOSTerminal.xcodeproj\"\nand one called \"MacTerminal.xcodeproj\".  \n\nThis is needed because Xcode does not provide code completion for iOS if you \nhave a Mac project in the project.   So I had to split them up.   Both \nprojects reference the same SwiftTerm package.\n\nWhen working with these projects, if you choose the terminal application\nit will run this one.   To run the test suite, select the 'SwiftTerm' target\ninstead, and you can use 'SwiftTermFuzz' to run the fuzzer.\n\nYou can use `swift build` to build the package, and `swift test` to\nrun the test suite.  For better test coverage, clone the esctest\nrepository which contains comprehensive terminal emulator tests:\n\n```\nmake clone-esctest\nswift test\n```\n\nThis clones the [esctest](https://github.com/migueldeicaza/esctest)\nrepository (Python 3 branch) and enables the full terminal compliance\ntest suite to run.\n\nIf using Xcode, you can select the \"SwiftTerm\" project, and then use Command-U \nto run the test suite.\n\nScreenshots\n===========\n\n24 Bit Color \n\n<img width=\"1246\" alt=\"24 bit color\" src=\"https://user-images.githubusercontent.com/36863/79060395-82181400-7c52-11ea-8f48-cd02323a8284.png\">\n\nMidnight Commander\n\n<img width=\"969\" alt=\"Screen Shot 2020-04-12 at 12 17 49 AM\" src=\"https://user-images.githubusercontent.com/36863/79060466-49c50580-7c53-11ea-8514-bb4a31359662.png\">\n\nSolid UTF-8 support, excellent rendering:\n<img width=\"799\" alt=\"Screen Shot 2020-04-22 at 11 25 30 PM\" src=\"https://user-images.githubusercontent.com/36863/80055786-95e43580-84f0-11ea-86dd-8dfb7f062b39.png\">\n\n<img width=\"799\" alt=\"Screen Shot 2020-04-22 at 11 25 24 PM\" src=\"https://user-images.githubusercontent.com/36863/80055792-9977bc80-84f0-11ea-8cac-735d4a516a80.png\">\n\nSupports hyperlinks emitted by modern apps:\n\n<img width=\"674\" alt=\"image\" src=\"https://user-images.githubusercontent.com/36863/80055972-0b500600-84f1-11ea-9c57-41cadce67162.png\">\n\niOS support:\n\n<img width=\"981\" alt=\"image\" src=\"https://user-images.githubusercontent.com/36863/80056069-54a05580-84f1-11ea-8597-5a227c9c64a7.png\">\n\nSixel support:\n\n<img width=\"770\" alt=\"image\" src=\"https://user-images.githubusercontent.com/36863/115647346-97a62c00-a2f1-11eb-929a-f9d942cc0c09.png\">\n\n<img width=\"568\" alt=\"image\" src=\"https://user-images.githubusercontent.com/36863/115647706-4e0a1100-a2f2-11eb-9bba-2a82503bca33.png\">\n\n\nResources \n========= \n\n* [Digital's VT100 User Guide](https://geoffg.net/Downloads/Terminal/VT100_User_Guide.pdf)\n* [Terminal Guide](https://terminalguide.namepad.de) - very nice and visual, but not normative\n* [Xterm Control Sequences](https://invisible-island.net/xterm/ctlseqs/ctlseqs.html#h2-Mouse-Tracking)\n* [VT510 Video Terminal Programmer Information](https://vt100.net/docs/vt510-rm/contents.html])\n\nAdditional and useful documents:\n* [VT330/VT340 Programmer Reference Manual Volume 2: Graphics Programming](https://vt100.net/docs/vt3xx-gp/contents.html)\n* [A parser for DECâ€™s ANSI-compatible video terminals](https://vt100.net/emu/dec_ansi_parser)\n* [Codes and Standards](https://vt100.net/emu/)\n* [Linux Console Docs](http://man7.org/linux/man-pages/man4/console_codes.4.html) they are a subset of vt100, but often simple to follow.\n* [Sixel Graphics](https://github.com/saitoha/libsixel)\n\nTest suites:\n* [VTTest](https://invisible-island.net/vttest/) - old, but still good\n* [EscTest](https://gitlab.freedesktop.org/terminal-wg/esctest) - fantastic: George Nachman, the author of iTerm, created this test suite, and it became a FreeDesktop standard.  Since then, Thomas E. Dickey, the xterm maintainer and maintainer of many text apps has contributed to this effort.\n\n# Authors\n\n* Thanks go to the [xterm.js](https://xtermjs.org/) developers that originally wrote a terminal emulator\nthat was licensed under a license that allowed for maximum reuse.   \n* [Marcin Krzyzanowski](https://krzyzanowskim.com) who masterfully improved and curated the rendering engine on AppKit/CoreText to be the glorious renderer that it is today - and for his contributions to the rendering engine\n* Greg Munn that did a lot of work in XtermSharp to support the needs of Visual Studio for\nMac\n* [Anders Borum](https://github.com/palmin) has contributed reliability fixes, the sixel parser and changes required to put SwiftTerm to use in production.\n* [Miguel de Icaza](https://tirania.org/) -me- who have been looking for an excuse to write some Swift code.\n",
      "stars_today": 1
    },
    {
      "id": 423362956,
      "name": "unitree_mujoco",
      "full_name": "unitreerobotics/unitree_mujoco",
      "description": null,
      "html_url": "https://github.com/unitreerobotics/unitree_mujoco",
      "stars": 777,
      "forks": 227,
      "language": "C++",
      "topics": [],
      "created_at": "2021-11-01T06:41:58Z",
      "updated_at": "2026-01-14T16:17:35Z",
      "pushed_at": "2025-11-07T03:38:55Z",
      "open_issues": 50,
      "owner": {
        "login": "unitreerobotics",
        "avatar_url": "https://avatars.githubusercontent.com/u/44998897?v=4"
      },
      "readme": "# Introduction\n## Unitree mujoco\n`unitree_mujoco` is a simulator developed based on `Unitree sdk2` and `mujoco`. Users can easily integrate the control programs developed with `Unitree_sdk2`, `unitree_ros2`, and `unitree_sdk2_python` into this simulator, enabling a seamless transition from simulation to physical development. The repository includes two versions of the simulator implemented in C++ and Python, with a structure as follows:\n![](./doc/func.png)\n\n## Directory Structure\n- `simulate`: Simulator implemented based on unitree_sdk2 and mujoco (C++, recommended)\n- `simulate_python`: Simulator implemented based on unitree_sdk2_python and mujoco (Python)\n- `unitree_robots`: MJCF description files for robots supported by unitree_sdk2\n- `terrain_tool`: Tool for generating terrain in simulation scenarios\n- `example`: Example programs\n\n## Supported Unitree sdk2 Messages:\n**Current version only supports low-level development, mainly used for sim to real verification of controller**\n- `LowCmd`: Motor control commands\n- `LowState`: Motor state information\n- `SportModeState`: Robot position and velocity data\n- `IMUState`: Torso IMU state at `rt/secondary_imu` topic (G1 only)\n\nNote:\n1. The numbering of the motors corresponds to the actual robot hardware. Specific details can be found in the [Unitree documentation](https://support.unitree.com/home/zh/developer).\n2. In the actual robot hardware, the `SportModeState` message is not readable after the built-in motion control service is turned off. However, the simulator retains this message to allow users to utilize the position and velocity information for analyzing the developed control programs.\n\n## Related links\n- [unitree_sdk2](https://github.com/unitreerobotics/unitree_sdk2)\n- [unitree_sdk2_python](https://github.com/unitreerobotics/unitree_sdk2_python)\n- [unitree_ros2](https://github.com/unitreerobotics/unitree_ros2)\n- [Unitree Doc](https://support.unitree.com/home/zh/developer)\n- [Mujoco Doc](https://mujoco.readthedocs.io/en/stable/overview.html)\n\n## Message (DDS idl) type description\n- Unitree Go2, B2, H1, B2w, Go2w robots use unitree_go idl for low-level communication.\n- Unitree G1, H1-2 robot uses unitree_hg idl for low-level communication.\n\n\n# Installation\n## C++ Simulator (simulate)\n### 1. Dependencies\n\n```bash\nsudo apt install libyaml-cpp-dev libspdlog-dev libboost-all-dev libglfw3-dev\n```\n\n#### unitree_sdk2\nIt is recommended to install `unitree_sdk2` in `/opt/unitree_robotics` path.\n```bash\ngit clone https://github.com/unitreerobotics/unitree_sdk2.git\ncd unitree_sdk2/\nmkdir build\ncd build\ncmake .. -DCMAKE_INSTALL_PREFIX=/opt/unitree_robotics\nsudo make install\n```\nFor more details, see: https://github.com/unitreerobotics/unitree_sdk2\n#### mujoco\n\nDownload the mujoco [release](https://github.com/google-deepmind/mujoco/releases), and extract it to the `~/.mujoco` directory;\n\n```\ncd unitree_mujoco/simulate/\nln -s ~/.mujoco/mujoco-3.3.6 mujoco\n```\n\n### 2. Compile unitree_mujoco\n```bash\ncd unitree_mujoco/simulate\nmkdir build && cd build\ncmake ..\nmake -j4\n```\n### 3. Test:\nRun:\n```bash\n./unitree_mujoco -r go2 -s scene_terrain.xml\n```\nYou should see the mujoco simulator with the Go2 robot loaded.\nIn a new terminal, run:\n```bash\n./test\n```\nThe program will output the robot's pose and position information in the simulator, and each motor of the robot will continuously output 1Nm of torque.\n\n**Note:** The testing program sends the unitree_go message. If you want to test G1 robot, you need to modify the program to use the unitree_hg message.\n\n## Python Simulator (simulate_python)\n### 1. Dependencies\n#### unitree_sdk2_python\n```bash\ncd ~\nsudo apt install python3-pip\ngit clone https://github.com/unitreerobotics/unitree_sdk2_python.git\ncd unitree_sdk2_python\npip3 install -e .\n```\nIf you encounter an error during installation:\n```bash\nCould not locate cyclonedds. Try to set CYCLONEDDS_HOME or CMAKE_PREFIX_PATH\n```\nRefer to: https://github.com/unitreerobotics/unitree_sdk2_python\n#### mujoco-python\n```bash\npip3 install mujoco\n```\n\n#### joystick\n```bash\npip3 install pygame\n```\n\n### 2. Test\n```bash\ncd ./simulate_python\npython3 ./unitree_mujoco.py\n```\nYou should see the mujoco simulator with the Go2 robot loaded.\nIn a new terminal, run:\n```bash\npython3 ./test/test_unitree_sdk2.py\n```\nThe program will output the robot's pose and position information in the simulator, and each motor of the robot will continuously output 1Nm of torque.\n\n**Note:** The testing program sends the unitree_go message. If you want to test G1 robot, you need to modify the program to use the unitree_hg message.\n\n\n# Usage\n## 1. Simulation Configuration\n### C++ Simulator\nThe configuration file for the C++ simulator is located at `/simulate/config.yaml`:\n```yaml\n# Robot name loaded by the simulator\n# \"go2\", \"b2\", \"b2w\", \"h1\"\nrobot: \"go2\"\n# Robot simulation scene file\n# For example, for go2, it refers to the scene.xml file in the /unitree_robots/go2/ folder\nrobot_scene: \"scene.xml\"\n# DDS domain id, it is recommended to distinguish from the real robot (default is 0 on the real robot)\ndomain_id: 1\n\nuse_joystick: 1 # Simulate Unitree WirelessController using a gamepad\njoystick_type: \"xbox\" # support \"xbox\" and \"switch\" gamepad layout\njoystick_device: \"/dev/input/js0\" # Device path\njoystick_bits: 16 # Some game controllers may only have 8-bit accuracy\n\n# Network interface name, for simulation, it is recommended to use the local loopback \"lo\"\ninterface: \"lo\"\n# Whether to output robot link, joint, sensor information, 1 for output\nprint_scene_information: 1\n# Whether to use virtual tape, 1 to enable\n# Mainly used to simulate the hanging process of H1 robot initialization\nenable_elastic_band: 0 # For H1\n```\n### Python Simulator\nThe configuration file for the Python simulator is located at `/simulate_python/config.py`:\n```python\n# Robot name loaded by the simulator\n# \"go2\", \"b2\", \"b2w\", \"h1\"\nROBOT = \"go2\"\n# Robot simulation scene file\nROBOT_SCENE = \"../unitree_robots/\" + ROBOT + \"/scene.xml\"  # Robot scene\n# DDS domain id, it is recommended to distinguish from the real robot (default is 0 on the real robot)\nDOMAIN_ID = 1  # Domain id\n# Network interface name, for simulation, it is recommended to use the local loopback \"lo\"\nINTERFACE = \"lo\"  # Interface\n# Whether to output robot link, joint, sensor information, True for output\nPRINT_SCENE_INFORMATION = True\n\nUSE_JOYSTICK = 1 # Simulate Unitree WirelessController using a gamepad\nJOYSTICK_TYPE = \"xbox\" # support \"xbox\" and \"switch\" gamepad layout\nJOYSTICK_DEVICE = 0 # Joystick number\n\n# Whether to use virtual tape, 1 to enable\n# Mainly used to simulate the hanging process of H1 robot initialization\nENABLE_ELASTIC_BAND = False\n# Simulation time step (unit: s)\n# To ensure the reliability of the simulation, it needs to be greater than the time required for viewer.sync() to render once\nSIMULATE_DT = 0.003  \n# Visualization interface runtime step, 0.02 corresponds to 50fps/s\nVIEWER_DT = 0.02\n```\n### Joystick\nThe simulator will use an Xbox or Switch gamepad  to simulate the wireless controller of the robot. The button and joystick information of the wireless controller will be published through \"rt/wireless_controller\" topic. `use_joystick/USE_JOYSTICK` in `config.yaml/config.py` needs to be set to 0, when there is no gamepad. If your gamepad is not in Xbox or Switch layout, you can modify it in the source code (The button and joystick IDs can be  determined  using `jstest`):\n\nIn `simulate/src/unitree_sdk2_bridge/unitree_sdk2_bridge.cc`: \n```C++\n if (js_type == \"xbox\")\n{\n    js_id_.axis[\"LX\"] = 0; // Left stick axis x\n    js_id_.axis[\"LY\"] = 1; // Left stick axis y\n    js_id_.axis[\"RX\"] = 3; // Right stick axis x\n    js_id_.axis[\"RY\"] = 4; // Right stick axis y\n    js_id_.axis[\"LT\"] = 2; // Left trigger\n    js_id_.axis[\"RT\"] = 5; // Right trigger\n    js_id_.axis[\"DX\"] = 6; // Directional pad x\n    js_id_.axis[\"DY\"] = 7; // Directional pad y\n\n    js_id_.button[\"X\"] = 2;\n    js_id_.button[\"Y\"] = 3;\n    js_id_.button[\"B\"] = 1;\n    js_id_.button[\"A\"] = 0;\n    js_id_.button[\"LB\"] = 4;\n    js_id_.button[\"RB\"] = 5;\n    js_id_.button[\"SELECT\"] = 6;\n    js_id_.button[\"START\"] = 7;\n}\n```\n\nIn `simulate_python/unitree_sdk2_bridge.py`: \n```python\nif js_type == \"xbox\":\n    self.axis_id = {\n        \"LX\": 0,  # Left stick axis x\n        \"LY\": 1,  # Left stick axis y\n        \"RX\": 3,  # Right stick axis x\n        \"RY\": 4,  # Right stick axis y\n        \"LT\": 2,  # Left trigger\n        \"RT\": 5,  # Right trigger\n        \"DX\": 6,  # Directional pad x\n        \"DY\": 7,  # Directional pad y\n    }\n\n    self.button_id = {\n        \"X\": 2,\n        \"Y\": 3,\n        \"B\": 1,\n        \"A\": 0,\n        \"LB\": 4,\n        \"RB\": 5,\n        \"SELECT\": 6,\n        \"START\": 7,\n    }\n```\n\n### Elastic band for humanoid \nConsider humanoid robots are not suitable for starting in ground, a virtual elastic band was designed to simulate the lifting and lowering of humanoid robots. Setting ` enable_elastic_mand/ENABLE_ELSTIC_BAND=1 ` can enable the virtual elastic band. After loading the robot, press' 9 'to activate or release the strap, press' 7' to lower the robot, and press' 8 'to lift the robot.\n\n## 2. Terrain Generation Tool\nWe provide a tool to parametrically create simple terrains in the mujoco simulator, including stairs, rough ground, and height maps. The program is located in the `terrain_tool` folder. For specific usage instructions, refer to the README file in the `terrain_tool` folder.\n![Terrain Generation Example](./doc/terrain.png)\n\n## 3. Sim to Real\nThe `example` folder contains simple examples of using different interfaces to make the Go2 robot stand up and then lie down. These examples demonstrate how to implement the transition from simulation to reality using interfaces provided by Unitree. Here is an explanation of each folder name:\n- `cpp`: Based on C++, using `unitree_sdk2` interface\n- `python`: Based on Python, using  `unitree_sdk2_python` interface\n- `ros2`: Based on ROS2, using `unitree_ros2` interface\n\n### unitree_sdk2\n1. Compile\n```bash\ncd example/cpp\nmkdir build && cd build\ncmake ..\nmake -j4\n```\n2. Run:\n```bash\n./stand_go2 # Control the robot in the simulation (make sure the Go2 simulation scene has been loaded)\n./stand_go2 enp3s0 # Control the physical robot, where enp3s0 is the name of the network card connected to the robot\n```\n3. Sim to Real\n```cpp\nif (argc < 2)\n{   \n    // If no network card is input, use the simulated domain id and the local network card\n    ChannelFactory::Instance()->Init(1, \"lo\");\n}\nelse\n{   \n    // Otherwise, use the specified network card\n    ChannelFactory::Instance()->Init(0, argv[1]);\n}\n```\n### unitree_sdk2_python\n1. Run\n```bash\npython3 ./stand_go2.py # Control the robot in the simulation (make sure the Go2 simulation scene has been loaded)\npython3 ./stand_go2.py enp3s0 # Control the physical robot, where enp3s0 is the name of the network card connected to the robot\n```\n2. Sim to Real\n```python\nif len(sys.argv) < 2:\n    // If no network card is input, use the simulated domain id and the local network card\n    ChannelFactoryInitialize(1, \"lo\")\nelse:\n    // Otherwise, use the specified network card\n    ChannelFactoryInitialize(0, sys.argv[1])\n```\n### unitree_ros2\n\n1. Compile\nFirst, ensure that the unitree_ros2 environment has been properly configured, see [unitree_ros2](https://github.com/unitreerobotics/unitree_ros2).\n\n```bash\nsource ~/unitree_ros2/setup.sh\ncd example/ros2\ncolcon build\n```\n\n2. Run simulation\n```bash\nsource ~/unitree_ros2/setup_local.sh # Use the local network card\nexport ROS_DOMAIN_ID=1 # Modify the domain id to match the simulation\n./install/stand_go2/bin/stand_go2 # Run\n```\n\n3. Run real robot\n```bash\nsource ~/unitree_ros2/setup.sh # Use the network card connected to the robot\nexport ROS_DOMAIN_ID=0 # Use the default domain id\n./install/stand_go2/bin/stand_go2 # Run\n```",
      "stars_today": 1
    },
    {
      "id": 738491,
      "name": "facebook-ios-sdk",
      "full_name": "facebook/facebook-ios-sdk",
      "description": "Used to integrate the Facebook Platform with your iOS & tvOS apps.",
      "html_url": "https://github.com/facebook/facebook-ios-sdk",
      "stars": 8008,
      "forks": 3678,
      "language": "Swift",
      "topics": [],
      "created_at": "2010-06-24T22:11:03Z",
      "updated_at": "2026-01-14T14:26:30Z",
      "pushed_at": "2026-01-05T18:29:31Z",
      "open_issues": 427,
      "owner": {
        "login": "facebook",
        "avatar_url": "https://avatars.githubusercontent.com/u/69631?v=4"
      },
      "readme": "# Facebook SDK for iOS\n\n[![Platforms](https://img.shields.io/cocoapods/p/FBSDKCoreKit.svg)](https://cocoapods.org/pods/FBSDKCoreKit)\n[![circleci](https://circleci.com/gh/facebook/facebook-ios-sdk/tree/main.svg?style=shield)](https://circleci.com/gh/facebook/facebook-ios-sdk/tree/main)\n\n[![CocoaPods](https://img.shields.io/cocoapods/v/FBSDKCoreKit.svg)](https://cocoapods.org/pods/FBSDKCoreKit)\n[![Carthage compatible](https://img.shields.io/badge/Carthage-compatible-4BC51D.svg?style=flat)](https://github.com/Carthage/Carthage)\n\nThis open-source library allows you to integrate Facebook into your iOS app.\n\nLearn more about the provided samples, documentation, integrating the SDK into your app, accessing source code, and more\nat https://developers.facebook.com/docs/ios\n\nPlease take a moment and [subscribe to releases](https://docs.github.com/en/enterprise/2.15/user/articles/watching-and-unwatching-repositories) so that you can be notified about new features, deprecations, and critical fixes. To see information about the latest release, consult our [changelog](CHANGELOG.md).\n\n|:warning: Be Advised :warning:|\n|:---|\n|<p>We have begun rewriting the iOS SDK in Swift in order to modernize the code base.</p><p>Please monitor the changelog for updates to existing interfaces but keep in mind that some interfaces will be unstable during this process. As such, updating to a minor version may introduce compilation issues related to language interoperability. Using symbols now defined in Swift may require using `@import` syntax from Objective-C and using C++ will likely require workarounds like creating wrappers in Objective-C.</p>Please bear with us as we work towards providing an improved experience for integrating with the Facebook platform.|\n\n## TRY IT OUT\n\n### Swift Package Manager\n\n1. In Xcode, select File > Swift Packages > Add Package Dependency.\n1. Follow the prompts using the URL for this repository\n1. Select the `Facebook`-prefixed libraries you want to use\n1. Check-out the tutorials available online at: <https://developers.facebook.com/docs/ios/getting-started>\n1. Start coding! Visit <https://developers.facebook.com/docs/ios> for tutorials and reference documentation.\n\n## iOS 14 CHANGES\n\n### Data Disclosure\n\nDue to the release of iOS 14, tracking events that your app collects and sends to Facebook may require you to disclosed these data types in the App Store Connect questionnaire. It is your responsibility to ensure this is reflected in your applicationâ€™s privacy policy. Visit our blogpost for information on affected Facebook SDKs, APIs, and products and the Apple App Store Privacy Details article to learn more about the data types you will need to disclose.\n\nlink to FB blogpost https://developers.facebook.com/blog/post/2020/10/22/preparing-for-apple-app-store-data-disclosure-requirements/\n\napple store details https://developer.apple.com/app-store/app-privacy-details/\n\n## FEATURES\n\n- Login - <https://developers.facebook.com/docs/facebook-login>\n- Sharing - <https://developers.facebook.com/docs/sharing>\n- App Links - <https://developers.facebook.com/docs/applinks>\n- Graph API - <https://developers.facebook.com/docs/ios/graph>\n- Analytics - <https://developers.facebook.com/docs/analytics>\n\n## GIVE FEEDBACK\n\nPlease report bugs or issues to our designated developer support team -- <https://developers.facebook.com/support/bugs/> -- as this will help us resolve them more quickly.\n\nYou can also visit our [Facebook Developer Community Forum](https://developers.facebook.com/community/),\njoin the [Facebook Developers Group on Facebook](https://www.facebook.com/groups/fbdevelopers/),\nask questions on [Stack Overflow](https://stackoverflow.com/questions/tagged/facebook-ios-sdk),\nor open an issue in this repository.\n\n## CONTRIBUTE\n\nFacebook welcomes contributions to our SDKs. Please see the [CONTRIBUTING](CONTRIBUTING.md) file.\n\n## LICENSE\n\nSee the [LICENSE](LICENSE) file.\n\nCopyright Â© Meta Platforms, Inc\n\n## SECURITY POLICY\n\nSee the [SECURITY POLICY](SECURITY.md) for more info on our bug bounty program.\n\n## DEVELOPER TERMS\n\n- By enabling Facebook integrations, including through this SDK, you can share information with Facebook, including\n  information about peopleâ€™s use of your app. Facebook will use information received in accordance with our\n  [Data Use Policy](https://www.facebook.com/about/privacy/), including to provide you with insights about the\n  effectiveness of your ads and the use of your app. These integrations also enable us and our partners to serve ads on\n  and off Facebook.\n- You may limit your sharing of information with us by updating the Insights control in the developer tool\n  `https://developers.facebook.com/apps/{app_id}/settings/advanced`.\n- If you use a Facebook integration, including to share information with us, you agree and confirm that you have\n  provided appropriate and sufficiently prominent notice to and obtained the appropriate consent from your users\n  regarding such collection, use, and disclosure (including, at a minimum, through your privacy policy). You further\n  agree that you will not share information with us about children under the age of 13.\n- You agree to comply with all applicable laws and regulations and also agree to our Terms\n  <https://www.facebook.com/policies/>, including our Platform Policies <https://developers.facebook.com/policy/> and\n  Advertising Guidelines, as applicable <https://www.facebook.com/ad_guidelines.php>.\n\nBy using the Facebook SDK for iOS you agree to these terms.\n",
      "stars_today": 0
    },
    {
      "id": 17101828,
      "name": "swirl_courses",
      "full_name": "swirldev/swirl_courses",
      "description": ":mortar_board: A collection of interactive courses for the swirl R package.",
      "html_url": "https://github.com/swirldev/swirl_courses",
      "stars": 4519,
      "forks": 7243,
      "language": "R",
      "topics": [],
      "created_at": "2014-02-23T04:48:04Z",
      "updated_at": "2026-01-12T21:17:04Z",
      "pushed_at": "2024-01-10T17:38:19Z",
      "open_issues": 206,
      "owner": {
        "login": "swirldev",
        "avatar_url": "https://avatars.githubusercontent.com/u/5671732?v=4"
      },
      "readme": "# swirl courses\n\nThis is a collection of interactive courses for use with the [swirl R package](http://swirlstats.com). You'll find instructions for installing courses further down on this page. Some courses are still in development and we'd love to hear any [suggestions](https://github.com/swirldev/swirl_courses/issues/new) you have as you work through them.\n\nFor more information regarding swirl, visit [swirlstats.com](http://swirlstats.com) or the [swirl GitHub repository](https://github.com/swirldev/swirl). If you'd like to write your own interactive content, please visit the [Instructors page](http://swirlstats.com/instructors.html) of our website.\n\nHere are our current offerings, organized by level of difficulty:\n\n#### Beginner\n\n- **R Programming**: The basics of programming in R\n- [**R Programming E**](https://github.com/swirldev/R_Programming_E): Same as the original, but modified slightly for in-class use (see below ***)\n- [**The R Programming Environment**](https://swirlstats.com/scn/rpe.html)\n<!-- - **Data Analysis**: Basic ideas in statistics and data visualization -->\n<!-- - **Mathematical Biostatistics Boot Camp**: One- and two-sample t-tests, power, and sample size -->\n<!-- - **Open Intro**: A very basic introduction to statistics, data analysis, and data visualization -->\n\n\\*\\*\\* *R Programming E is identical to R Programming, except we've eliminated the prompts for Coursera credentials at the end of each lesson and instead give students the option to send an email to their instructor notifying them of completion. Admittedly, it's sort of a hack until we come up with a more robust solution for in-class use (i.e. an instructor \"dashboard\").*\n\n#### Intermediate\n\n- **Regression Models**: The basics of regression modeling in R\n- **Getting and Cleaning Data**: dplyr, tidyr, lubridate, oh my!\n\n#### Advanced\n\n- **Statistical Inference**: This intermediate to advanced level course closely follows the\n[Statistical Inference course](https://www.coursera.org/course/statinference) of the Johns Hopkins \n[Data Science Specialization](https://www.coursera.org/specialization/jhudatascience/1) on Coursera. It\nintroduces the student to basic concepts of statistical inference\nincluding probability, hypothesis testing, confidence intervals and\np-values. It concludes with an initiation to topics of particular\nrelevance to big data, issues of multiple testing and resampling.\n- [**Advanced R Programming**](https://swirlstats.com/scn/arp.html)\n\nSince our users come from a variety backgrounds, it's very hard to label material as **Beginner**, **Intermediate**, or **Advanced**. If you find something that is labelled **Beginner** to be challenging, please don't be discouraged. The first step of learning anything is to acknowledge that you are capable of understanding it. True understanding will come with time and practice.\n\n#### Course Authors\n\n- **Writing swirl Courses**: An interactive guides and example \n  for swirl course authors. The first group of lessons cover basics. The rest cover \n  special topics useful primarily as samples--points of departure for one's own material.\n  For more comprehensive documentation about writing your own swirl courses see http://swirlstats.com/swirlify/.\n\n## Install and run a course automatically from swirl\n\n**This is the preferred method of installing courses.** It automates the process by allowing you to do everything right from the R console.\n\n1) Make sure you have a recent version version of swirl:\n\n```\ninstall.packages(\"swirl\")\n```\n\n2) Enter the following from the R console, **substituting the name of the course** that you wish to install:\n\n```\nlibrary(swirl)\ninstall_course(\"Course Name Here\")\nswirl()\n```\n\nFor example, `install_course(\"R Programming\")` will install the R Programming course. **Please note that course names are case sensitive!**\n\nIf that doesn't work for you...\n\n## Install and run a course manually\n\nIf the automatic course installation method outlined above does not work for you, then there's a simple alternative.\n\n1. Find the course you want to install on the [Swirl Course network website](https://swirlstats.com/scn/title.html).\n2. Follow the manual installation instructions on the course page.\n\nIf that does not work for you, consider taking a look at the \n[legacy manual install instructions](https://github.com/swirldev/swirl_courses/wiki/Legacy-Manual-Install-Instructions-for-Swirl-Courses).\n\n## Uninstall a course\n\nIf you'd like to remove a course at any time, you can use `uninstall_course(\"Course Name Here\")`.\n\n## Using swirl in the classroom\n\nInstructors around the world are using swirl in their classrooms. We think this is awesome. If you're an instructor, please feel free to do the same -- free of charge. While your students may be paying to take your course or attend your institution, we simply ask that you don't charge people *directly* for the use of our software or instructional content.\n\nIf you are not sure about a particular use case, don't hesitate to post a\nquestion to our [Google Group](https://groups.google.com/forum/#!forum/swirl-discuss).\n",
      "stars_today": 0
    },
    {
      "id": 120519269,
      "name": "atlantis",
      "full_name": "runatlantis/atlantis",
      "description": "Terraform Pull Request Automation",
      "html_url": "https://github.com/runatlantis/atlantis",
      "stars": 8759,
      "forks": 1208,
      "language": "Go",
      "topics": [
        "atlantis",
        "automation",
        "devops",
        "go",
        "golang",
        "hacktoberfest",
        "sre",
        "tacos",
        "terraform"
      ],
      "created_at": "2018-02-06T20:28:06Z",
      "updated_at": "2026-01-14T19:43:55Z",
      "pushed_at": "2026-01-14T06:43:33Z",
      "open_issues": 784,
      "owner": {
        "login": "runatlantis",
        "avatar_url": "https://avatars.githubusercontent.com/u/32311288?v=4"
      },
      "readme": "# Atlantis <!-- omit in toc -->\n\n[![Latest Release](https://img.shields.io/github/release/runatlantis/atlantis.svg)](https://github.com/runatlantis/atlantis/releases/latest)\n[![SuperDopeBadge](./runatlantis.io/public/hightower-super-dope.svg)](https://twitter.com/kelseyhightower/status/893260922222813184)\n[![Go Report Card](https://goreportcard.com/badge/github.com/runatlantis/atlantis)](https://goreportcard.com/report/github.com/runatlantis/atlantis)\n[![Go Reference](https://pkg.go.dev/badge/github.com/runatlantis/atlantis.svg)](https://pkg.go.dev/github.com/runatlantis/atlantis)\n[![Slack](https://img.shields.io/badge/Join-Atlantis%20Community%20Slack-red)](https://slack.cncf.io/)\n[![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/runatlantis/atlantis/badge)](https://scorecard.dev/viewer/?uri=github.com/runatlantis/atlantis)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/9428/badge)](https://www.bestpractices.dev/projects/9428)\n\n<p align=\"center\">\n  <img src=\"./runatlantis.io/public/hero.png\" alt=\"Atlantis Logo\"/><br><br>\n  <b>Terraform Pull Request Automation</b>\n</p>\n\n- [Resources](#resources)\n- [What is Atlantis?](#what-is-atlantis)\n- [What does it do?](#what-does-it-do)\n- [Why should you use it?](#why-should-you-use-it)\n- [Stargazers over time](#stargazers-over-time)\n\n## Resources\n* How to get started: [www.runatlantis.io/guide](https://www.runatlantis.io/guide)\n* Full documentation: [www.runatlantis.io/docs](https://www.runatlantis.io/docs)\n* Download the latest release: [github.com/runatlantis/atlantis/releases/latest](https://github.com/runatlantis/atlantis/releases/latest)\n* Get help in our [Slack channel](https://slack.cncf.io/) in channel #atlantis and development in #atlantis-contributors\n* Start Contributing: [CONTRIBUTING.md](CONTRIBUTING.md)\n\n## What is Atlantis?\nA self-hosted golang application that listens for Terraform pull request events via webhooks.\n\n## What does it do?\nRuns `terraform plan`, `import`, `apply` remotely and comments back on the pull request with the output.\n\n## Why should you use it?\n* Make Terraform changes visible to your whole team.\n* Enable non-operations engineers to collaborate on Terraform.\n* Standardize your Terraform workflows.\n\n## Stargazers over time\n\n[![Stargazers over time](https://starchart.cc/runatlantis/atlantis.svg)](https://starchart.cc/runatlantis/atlantis)\n",
      "stars_today": 0
    },
    {
      "id": 6427813,
      "name": "dplyr",
      "full_name": "tidyverse/dplyr",
      "description": "dplyr: A grammar of data manipulation",
      "html_url": "https://github.com/tidyverse/dplyr",
      "stars": 4981,
      "forks": 2132,
      "language": "R",
      "topics": [
        "data-manipulation",
        "grammar",
        "r"
      ],
      "created_at": "2012-10-28T13:39:17Z",
      "updated_at": "2026-01-09T01:48:36Z",
      "pushed_at": "2025-12-19T19:01:08Z",
      "open_issues": 76,
      "owner": {
        "login": "tidyverse",
        "avatar_url": "https://avatars.githubusercontent.com/u/22032646?v=4"
      },
      "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# dplyr <a href=\"https://dplyr.tidyverse.org\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" /></a>\n\n<!-- badges: start -->\n\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/dplyr)](https://cran.r-project.org/package=dplyr)\n[![R-CMD-check](https://github.com/tidyverse/dplyr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidyverse/dplyr/actions/workflows/R-CMD-check.yaml)\n[![Codecov test\ncoverage](https://codecov.io/gh/tidyverse/dplyr/graph/badge.svg)](https://app.codecov.io/gh/tidyverse/dplyr)\n<!-- badges: end -->\n\n## Overview\n\ndplyr is a grammar of data manipulation, providing a consistent set of\nverbs that help you solve the most common data manipulation challenges:\n\n- `mutate()` adds new variables that are functions of existing variables\n- `select()` picks variables based on their names.\n- `filter()` picks cases based on their values.\n- `summarise()` reduces multiple values down to a single summary.\n- `arrange()` changes the ordering of the rows.\n\nThese all combine naturally with `group_by()` which allows you to\nperform any operation â€œby groupâ€. You can learn more about them in\n`vignette(\"dplyr\")`. As well as these single-table verbs, dplyr also\nprovides a variety of two-table verbs, which you can learn about in\n`vignette(\"two-table\")`.\n\nIf you are new to dplyr, the best place to start is the [data\ntransformation chapter](https://r4ds.hadley.nz/data-transform) in R for\nData Science.\n\n## Backends\n\nIn addition to data frames/tibbles, dplyr makes working with other\ncomputational backends accessible and efficient. Below is a list of\nalternative backends:\n\n- [arrow](https://arrow.apache.org/docs/r/) for larger-than-memory\n  datasets, including on remote cloud storage like AWS S3, using the\n  Apache Arrow C++ engine,\n  [Acero](https://arrow.apache.org/docs/cpp/acero/overview.html).\n\n- [dbplyr](https://dbplyr.tidyverse.org/) for data stored in a\n  relational database. Translates your dplyr code to SQL.\n\n- [dtplyr](https://dtplyr.tidyverse.org/) for large, in-memory datasets.\n  Translates your dplyr code to high performance\n  [data.table](https://rdatatable.gitlab.io/data.table/) code.\n\n- [duckplyr](https://duckplyr.tidyverse.org/) for large, in-memory\n  datasets. Translates your dplyr code to high performance\n  [duckdb](https://duckdb.org) queries with zero extra copies and an\n  automatic R fallback when translation isnâ€™t possible.\n\n- [sparklyr](https://spark.posit.co/) for very large datasets stored in\n  [Apache Spark](https://spark.apache.org).\n\n## Installation\n\n``` r\n# The easiest way to get dplyr is to install the whole tidyverse:\ninstall.packages(\"tidyverse\")\n\n# Alternatively, install just dplyr:\ninstall.packages(\"dplyr\")\n```\n\n### Development version\n\nTo get a bug fix or to use a feature from the development version, you\ncan install the development version of dplyr from GitHub.\n\n``` r\n# install.packages(\"pak\")\npak::pak(\"tidyverse/dplyr\")\n```\n\n## Cheat Sheet\n\n<a href=\"https://github.com/rstudio/cheatsheets/blob/main/data-transformation.pdf\"><img src=\"https://raw.githubusercontent.com/rstudio/cheatsheets/main/pngs/thumbnails/data-transformation-cheatsheet-thumbs.png\" width=\"630\" height=\"252\"/></a>\n\n## Usage\n\n``` r\nlibrary(dplyr)\n\nstarwars |>\n  filter(species == \"Droid\")\n#> # A tibble: 6 Ã— 14\n#>   name   height  mass hair_color skin_color  eye_color birth_year sex   gender  \n#>   <chr>   <int> <dbl> <chr>      <chr>       <chr>          <dbl> <chr> <chr>   \n#> 1 C-3PO     167    75 <NA>       gold        yellow           112 none  masculiâ€¦\n#> 2 R2-D2      96    32 <NA>       white, blue red               33 none  masculiâ€¦\n#> 3 R5-D4      97    32 <NA>       white, red  red               NA none  masculiâ€¦\n#> 4 IG-88     200   140 none       metal       red               15 none  masculiâ€¦\n#> 5 R4-P17     96    NA none       silver, red red, blue         NA none  feminine\n#> # â„¹ 1 more row\n#> # â„¹ 5 more variables: homeworld <chr>, species <chr>, films <list>,\n#> #   vehicles <list>, starships <list>\n\nstarwars |>\n  select(name, ends_with(\"color\"))\n#> # A tibble: 87 Ã— 4\n#>   name           hair_color skin_color  eye_color\n#>   <chr>          <chr>      <chr>       <chr>    \n#> 1 Luke Skywalker blond      fair        blue     \n#> 2 C-3PO          <NA>       gold        yellow   \n#> 3 R2-D2          <NA>       white, blue red      \n#> 4 Darth Vader    none       white       yellow   \n#> 5 Leia Organa    brown      light       brown    \n#> # â„¹ 82 more rows\n\nstarwars |>\n  mutate(name, bmi = mass / ((height / 100)^2)) |>\n  select(name:mass, bmi)\n#> # A tibble: 87 Ã— 4\n#>   name           height  mass   bmi\n#>   <chr>           <int> <dbl> <dbl>\n#> 1 Luke Skywalker    172    77  26.0\n#> 2 C-3PO             167    75  26.9\n#> 3 R2-D2              96    32  34.7\n#> 4 Darth Vader       202   136  33.3\n#> 5 Leia Organa       150    49  21.8\n#> # â„¹ 82 more rows\n\nstarwars |>\n  arrange(desc(mass))\n#> # A tibble: 87 Ã— 14\n#>   name      height  mass hair_color skin_color eye_color birth_year sex   gender\n#>   <chr>      <int> <dbl> <chr>      <chr>      <chr>          <dbl> <chr> <chr> \n#> 1 Jabba Deâ€¦    175  1358 <NA>       green-tanâ€¦ orange         600   hermâ€¦ mascuâ€¦\n#> 2 Grievous     216   159 none       brown, whâ€¦ green, yâ€¦       NA   male  mascuâ€¦\n#> 3 IG-88        200   140 none       metal      red             15   none  mascuâ€¦\n#> 4 Darth Vaâ€¦    202   136 none       white      yellow          41.9 male  mascuâ€¦\n#> 5 Tarfful      234   136 brown      brown      blue            NA   male  mascuâ€¦\n#> # â„¹ 82 more rows\n#> # â„¹ 5 more variables: homeworld <chr>, species <chr>, films <list>,\n#> #   vehicles <list>, starships <list>\n\nstarwars |>\n  group_by(species) |>\n  summarise(\n    n = n(),\n    mass = mean(mass, na.rm = TRUE)\n  ) |>\n  filter(\n    n > 1,\n    mass > 50\n  )\n#> # A tibble: 9 Ã— 3\n#>   species      n  mass\n#>   <chr>    <int> <dbl>\n#> 1 Droid        6  69.8\n#> 2 Gungan       3  74  \n#> 3 Human       35  81.3\n#> 4 Kaminoan     2  88  \n#> 5 Mirialan     2  53.1\n#> # â„¹ 4 more rows\n```\n\n## Getting help\n\nIf you encounter a clear bug, please file an issue with a minimal\nreproducible example on\n[GitHub](https://github.com/tidyverse/dplyr/issues). For questions and\nother discussion, please use [forum.posit.co](https://forum.posit.co/).\n\n## Code of conduct\n\nPlease note that this project is released with a [Contributor Code of\nConduct](https://dplyr.tidyverse.org/CODE_OF_CONDUCT). By participating\nin this project you agree to abide by its terms.\n",
      "stars_today": 0
    },
    {
      "id": 120498971,
      "name": "swift-nio",
      "full_name": "apple/swift-nio",
      "description": "Event-driven network application framework for high performance protocol servers & clients, non-blocking.",
      "html_url": "https://github.com/apple/swift-nio",
      "stars": 8379,
      "forks": 723,
      "language": "Swift",
      "topics": [
        "asynchronous-io",
        "event-driven",
        "high-performance",
        "networking",
        "non-blocking",
        "non-blocking-io",
        "swift",
        "swift-server",
        "swift5",
        "swiftnio"
      ],
      "created_at": "2018-02-06T17:47:31Z",
      "updated_at": "2026-01-14T20:58:59Z",
      "pushed_at": "2026-01-14T13:42:45Z",
      "open_issues": 247,
      "owner": {
        "login": "apple",
        "avatar_url": "https://avatars.githubusercontent.com/u/10639145?v=4"
      },
      "readme": "[![sswg:graduated|104x20](https://img.shields.io/badge/sswg-graduated-green.svg)](https://github.com/swift-server/sswg/blob/main/process/incubation.md#graduated-level)\n\n# SwiftNIO\n\nSwiftNIO is a cross-platform asynchronous event-driven network application framework\nfor rapid development of maintainable high performance protocol servers & clients.\n\nIt's like [Netty](https://netty.io), but written for Swift.\n\n### Repository organization\n\nThe SwiftNIO project is split across multiple repositories:\n\nRepository | NIO 2\n--- | ---\n[https://github.com/apple/swift-nio][repo-nio] <br> SwiftNIO core | `from: \"2.0.0\"`\n[https://github.com/apple/swift-nio-ssl][repo-nio-ssl] <br> TLS (SSL) support | `from: \"2.0.0\"`\n[https://github.com/apple/swift-nio-http2][repo-nio-http2]<br> HTTP/2 support | `from: \"1.0.0\"`\n[https://github.com/apple/swift-nio-extras][repo-nio-extras] <br>useful additions around SwiftNIO | `from: \"1.0.0\"`\n[https://github.com/apple/swift-nio-transport-services][repo-nio-transport-services] <br> first-class support for macOS, iOS, tvOS, and watchOS | `from: \"1.0.0\"`\n[https://github.com/apple/swift-nio-ssh][repo-nio-ssh] <br> SSH support | `.upToNextMinor(from: \"0.2.0\")`\n\nWithin this repository we have a number of products that provide different functionality. This package contains the following products:\n\n- `NIO`. This is an umbrella module exporting `NIOCore`, `NIOEmbedded` and `NIOPosix`.\n- `NIOCore`. This provides the core abstractions and types for using SwiftNIO (see [\"Conceptual Overview\"](#conceptual-overview) for more details). Most NIO extension projects that provide things like new [`EventLoop`s][el] and [`Channel`s][c] or new protocol implementations should only need to depend on `NIOCore`.\n- `NIOPosix`. This provides the primary [`EventLoopGroup`], [`EventLoop`][el], and [`Channel`s][c] for use on POSIX-based systems. This is our high performance core I/O layer. In general, this should only be imported by projects that plan to do some actual I/O, such as high-level protocol implementations or applications.\n- `NIOEmbedded`. This provides [`EmbeddedChannel`][ec] and [`EmbeddedEventLoop`][eel], implementations of the `NIOCore` abstractions that provide fine-grained control over their execution. These are most often used for testing, but can also be used to drive protocol implementations in a way that is decoupled from networking altogether.\n- `NIOConcurrencyHelpers`. This provides a few low-level concurrency primitives that are used by NIO implementations, such as locks and atomics.\n- `NIOFoundationCompat`. This extends a number of NIO types for better interoperation with Foundation data types. If you are working with Foundation data types such as `Data`, you should import this.\n- `NIOTLS`. This provides a few common abstraction types for working with multiple TLS implementations. Note that this doesn't provide TLS itself: please investigate [swift-nio-ssl][repo-nio-ssl] and [swift-nio-transport-services][repo-nio-transport-services] for concrete implementations.\n- `NIOHTTP1`. This provides a low-level HTTP/1.1 protocol implementation.\n- `NIOWebSocket`. This provides a low-level WebSocket protocol implementation.\n- `NIOTestUtils`. This provides a number of helpers for testing projects that use SwiftNIO.\n- `NIOFileSystem`. This provides `async` APIs for interacting with the file system.\n\n### Protocol Implementations\n\nBelow you can find a list of a few protocol implementations that are done with SwiftNIO. This is a non-exhaustive list of protocols that are either part of the SwiftNIO project or are accepted into the [SSWG](https://swift.org/server)'s incubation process. All of the libraries listed below do all of their I/O in a non-blocking fashion using SwiftNIO.\n\n#### Low-level protocol implementations\n\nLow-level protocol implementations are often a collection of [`ChannelHandler`][ch]s that implement a protocol but still require the user to have a good understanding of SwiftNIO. Often, low-level protocol implementations will then be wrapped in high-level libraries with a nicer, more user-friendly API.\n\nProtocol | Client<br />(Sends requests) | Server<br />(Responds to requests) | Repository | Module | Comment\n--- |  --- | --- | --- | --- | ---\nHTTP/1 | âœ…| âœ… | [apple/swift-nio](https://github.com/apple/swift-nio) | [`NIOHTTP1`][nioh1] | official NIO project\nHTTP/2 | âœ…| âœ… | [apple/swift-nio-http2](https://github.com/apple/swift-nio-http2) | [`NIOHTTP2`][nioh2] | official NIO project\nWebSocket | âœ…| âœ… | [apple/swift-nio](https://github.com/apple/swift-nio) | [`NIOWebSocket`][niows] | official NIO project\nTLS | âœ… | âœ… | [apple/swift-nio-ssl](https://github.com/apple/swift-nio-ssl) | [`NIOSSL`][niossl] | official NIO project\nSSH | âœ… | âœ… | [apple/swift-nio-ssh][repo-nio-ssh] | [`NIOSSH`][niossh] | official NIO project\n\n\n#### High-level implementations\n\nHigh-level implementations are usually libraries that come with an API that doesn't expose SwiftNIO's [`ChannelPipeline`][cp] and can therefore be used with very little (or no) SwiftNIO-specific knowledge. The implementations listed below do still do all of their I/O in SwiftNIO and integrate really well with the SwiftNIO ecosystem.\n\nProtocol | Client<br />(Sends requests) | Server<br />(Responds to requests) | Repository | Module | Comment\n--- |  --- | --- | --- | --- | ---\nHTTP | âœ…| âŒ | [swift-server/async-http-client](https://github.com/swift-server/async-http-client) | `AsyncHTTPClient` | SSWG community project\ngRPC | âœ…| âœ… | [grpc/grpc-swift](https://github.com/grpc/grpc-swift) | `GRPC` | also offers a low-level API; SSWG community project\nAPNS | âœ… | âŒ | [swift-server-community/APNSwift](https://github.com/swift-server-community/APNSwift) | `APNSwift` | SSWG community project\nPostgreSQL | âœ… | âŒ | [vapor/postgres-nio](https://github.com/vapor/postgres-nio) | `PostgresNIO` | SSWG community project\nRedis | âœ… | âŒ | [swift-server/RediStack](https://github.com/swift-server/RediStack) | `RediStack` | SSWG community project\n\n### Supported Versions\n\n### SwiftNIO 2\n\nThis is the current version of SwiftNIO and will be supported for the foreseeable future.\n\n### Swift Versions\n\nWe commit to support the most recently released Swift version and the last two minor releases before that unless this is impossible to do in one codebase.\nIn addition checks are run against the latest beta release (if any) as well as the nightly Swift builds and the intent is that these should pass.\n\nThe minimum Swift version supported by SwiftNIO releases are detailed below:\n\nSwiftNIO            | Minimum Swift Version\n--------------------|----------------------\n`2.0.0 ..< 2.30.0`  | 5.0\n`2.30.0 ..< 2.40.0` | 5.2\n`2.40.0 ..< 2.43.0` | 5.4\n`2.43.0 ..< 2.51.0` | 5.5.2\n`2.51.0 ..< 2.60.0` | 5.6\n`2.60.0 ..< 2.65.0` | 5.7\n`2.65.0 ..< 2.76.0` | 5.8\n`2.76.0 ..< 2.83.0` | 5.9\n`2.83.0 ..< 2.87.0` | 5.10\n`2.87.0 ...       ` | 6.0\n\n### SwiftNIO 1\nSwiftNIO 1 is considered end of life - it is strongly recommended that you move to a newer version.  The Core NIO team does not actively work on this version.  No new features will be added to this version but PRs which fix bugs or security vulnerabilities will be accepted until the end of May 2022.\n\nIf you have a SwiftNIO 1 application or library that you would like to migrate to SwiftNIO 2, please check out the [migration guide](docs/migration-guide-NIO1-to-NIO2.md) we prepared for you.\n\nThe latest released SwiftNIO 1 versionÂ supports Swift 4.0, 4.1, 4.2, and 5.0.\n\n### Supported Platforms\n\nSwiftNIO aims to support all of the platforms where Swift is supported. Currently, it is developed and tested on macOS and Linux, and is known to support the following operating system versions:\n\n* Ubuntu 18.04+\n* macOS 10.9+, iOS 7+; (macOS 10.14+, iOS 12+, tvOS 12+ or watchOS 6+ with [swift-nio-transport-services][repo-nio-transport-services])\n\nSwiftNIO has experimental support on OpenBSD for all SwiftNIO libraries _except_ for NIOFileSystem, which is not yet supported. You can use all other SwiftNIO libraries on OpenBSD by adding them as dependencies in `Package.swift`.\n\n### Compatibility\n\nSwiftNIO follows [SemVer 2.0.0](https://semver.org/#semantic-versioning-200) with a separate document declaring [SwiftNIO's Public API](docs/public-api.md).\n\nWhat this means for you is that you should depend on SwiftNIO with a version range that covers everything from the minimum SwiftNIO version you require up to the next major version.\nIn SwiftPM that can be easily done specifying for example `from: \"2.0.0\"` meaning that you support SwiftNIO in every version starting from 2.0.0 up to (excluding) 3.0.0.\nSemVer and SwiftNIO's Public API guarantees should result in a working program without having to worry about testing every single version for compatibility.\n\n\n## Conceptual Overview\n\nSwiftNIO is fundamentally a low-level tool for building high-performance networking applications in Swift. It particularly targets those use-cases where using a \"thread-per-connection\" model of concurrency is inefficient or untenable. This is a common limitation when building servers that use a large number of relatively low-utilization connections, such as HTTP servers.\n\nTo achieve its goals, SwiftNIO extensively uses \"non-blocking I/O\": hence the name! Non-blocking I/O differs from the more common blocking I/O model because the application does not wait for data to be sent to or received from the network: instead, SwiftNIO asks for the kernel to notify it when I/O operations can be performed without waiting.\n\nSwiftNIO does not aim to provide high-level solutions like, for example, web frameworks do. Instead, SwiftNIO is focused on providing the low-level building blocks for these higher-level applications. When it comes to building a web application, most users will not want to use SwiftNIO directly: instead, they'll want to use one of the many great web frameworks available in the Swift ecosystem. Those web frameworks, however, may choose to use SwiftNIO under the covers to provide their networking support.\n\nThe following sections will describe the low-level tools that SwiftNIO provides, and provide a quick overview of how to work with them. If you feel comfortable with these concepts, then you can skip right ahead to the other sections of this README.\n\n### Basic Architecture\n\nThe basic building blocks of SwiftNIO are the following 8 types of objects:\n\n- [`EventLoopGroup`][elg], a protocol, provided by `NIOCore`.\n- [`EventLoop`][el], a protocol, provided by `NIOCore`.\n- [`Channel`][c], a protocol, provided by `NIOCore`.\n- [`ChannelHandler`][ch], a protocol, provided by `NIOCore`.\n- `Bootstrap`, several related structures, provided by `NIOCore`.\n- [`ByteBuffer`][bb], a struct, provided by `NIOCore`.\n- [`EventLoopFuture`][elf], a generic class, provided by `NIOCore`.\n- [`EventLoopPromise`][elp], a generic struct, provided by `NIOCore`.\n\nAll SwiftNIO applications are ultimately constructed of these various components.\n\n#### EventLoops and EventLoopGroups\n\nThe basic I/O primitive of SwiftNIO is the event loop. The event loop is an object that waits for events (usually I/O related events, such as \"data received\") to happen and then fires some kind of callback when they do. In almost all SwiftNIO applications there will be relatively few event loops: usually only one or two per CPU core the application wants to use. Generally speaking, event loops run for the entire lifetime of your application, spinning in an endless loop dispatching events.\n\nEvent loops are gathered together into event loop *groups*. These groups provide a mechanism to distribute work around the event loops. For example, when listening for inbound connections the listening socket will be registered on one event loop. However, we don't want all connections that are accepted on that listening socket to be registered with the same event loop, as that would potentially overload one event loop while leaving the others empty. For that reason, the event loop group provides the ability to spread load across multiple event loops.\n\nIn SwiftNIO today there is one [`EventLoopGroup`][elg] implementation, and two [`EventLoop`][el] implementations. For production applications there is the [`MultiThreadedEventLoopGroup`][mtelg], an [`EventLoopGroup`][elg] that creates a number of threads (using the POSIX [`pthreads`][pthreads] library) and places one `SelectableEventLoop` on each one. The `SelectableEventLoop` is an event loop that uses a selector (either [`kqueue`][kqueue] or [`epoll`][epoll] depending on the target system) to manage I/O events from file descriptors and to dispatch work. These [`EventLoop`s][el] and [`EventLoopGroup`s][elg] are provided by the `NIOPosix` module. Additionally, there is the [`EmbeddedEventLoop`][eel], which is a dummy event loop that is used primarily for testing purposes, provided by the `NIOEmbedded` module.\n\n[`EventLoop`][el]s have a number of important properties. Most vitally, they are the way all work gets done in SwiftNIO applications. In order to ensure thread-safety, any work that wants to be done on almost any of the other objects in SwiftNIO must be dispatched via an [`EventLoop`][el]. [`EventLoop`][el] objects own almost all the other objects in a SwiftNIO application, and understanding their execution model is critical for building high-performance SwiftNIO applications.\n\n#### Channels, Channel Handlers, Channel Pipelines, and Channel Contexts\n\nWhile [`EventLoop`][el]s are critical to the way SwiftNIO works, most users will not interact with them substantially beyond asking them to create [`EventLoopPromise`][elp]s and to schedule work. The parts of a SwiftNIO application most users will spend the most time interacting with are [`Channel`][c]s and [`ChannelHandler`][ch]s.\n\nAlmost every file descriptor that a user interacts with in a SwiftNIO program is associated with a single [`Channel`][c]. The [`Channel`][c] owns this file descriptor, and is responsible for managing its lifetime. It is also responsible for processing inbound and outbound events on that file descriptor: whenever the event loop has an event that corresponds to a file descriptor, it will notify the [`Channel`][c] that owns that file descriptor.\n\n[`Channel`][c]s by themselves, however, are not useful. After all, it is a rare application that doesn't want to do anything with the data it sends or receives on a socket! So the other important part of the [`Channel`][c] is the [`ChannelPipeline`][cp].\n\nA [`ChannelPipeline`][cp] is a sequence of objects, called [`ChannelHandler`][ch]s, that process events on a [`Channel`][c]. The [`ChannelHandler`][ch]s process these events one after another, in order, mutating and transforming events as they go. This can be thought of as a data processing pipeline; hence the name [`ChannelPipeline`][cp].\n\nAll [`ChannelHandler`][ch]s are either Inbound or Outbound handlers, or both. Inbound handlers process \"inbound\" events: events like reading data from a socket, reading socket close, or other kinds of events initiated by remote peers. Outbound handlers process \"outbound\" events, such as writes, connection attempts, and local socket closes.\n\nEach handler processes the events in order. For example, read events are passed from the front of the pipeline to the back, one handler at a time, while write events are passed from the back of the pipeline to the front. Each handler may, at any time, generate either inbound or outbound events that will be sent to the next handler in whichever direction is appropriate. This allows handlers to split up reads, coalesce writes, delay connection attempts, and generally perform arbitrary transformations of events.\n\nIn general, [`ChannelHandler`][ch]s are designed to be highly re-usable components. This means they tend to be designed to be as small as possible, performing one specific data transformation. This allows handlers to be composed together in novel and flexible ways, which helps with code reuse and encapsulation.\n\n[`ChannelHandler`][ch]s are able to keep track of where they are in a [`ChannelPipeline`][cp] by using a [`ChannelHandlerContext`][chc]. These objects contain references to the previous and next channel handler in the pipeline, ensuring that it is always possible for a [`ChannelHandler`][ch] to emit events while it remains in a pipeline.\n\nSwiftNIO ships with many [`ChannelHandler`][ch]s built in that provide useful functionality, such as HTTP parsing. In addition, high-performance applications will want to provide as much of their logic as possible in [`ChannelHandler`][ch]s, as it helps avoid problems with context switching.\n\nAdditionally, SwiftNIO ships with a few [`Channel`][c] implementations. In particular, it ships with `ServerSocketChannel`, a [`Channel`][c] for sockets that accept inbound connections; `SocketChannel`, a [`Channel`][c] for TCP connections; and `DatagramChannel`, a [`Channel`][c] for UDP sockets. All of these are provided by the `NIOPosix` module. It also provides [`EmbeddedChannel`][ec], a [`Channel`][c] primarily used for testing, provided by the `NIOEmbedded` module.\n\n##### A Note on Blocking\n\nOne of the important notes about [`ChannelPipeline`][cp]s is that they are thread-safe. This is very important for writing SwiftNIO applications, as it allows you to write much simpler [`ChannelHandler`][ch]s in the knowledge that they will not require synchronization.\n\nHowever, this is achieved by dispatching all code on the [`ChannelPipeline`][cp] on the same thread as the [`EventLoop`][el]. This means that, as a general rule, [`ChannelHandler`][ch]s **must not** call blocking code without dispatching it to a background thread. If a [`ChannelHandler`][ch] blocks for any reason, all [`Channel`][c]s attached to the parent [`EventLoop`][el] will be unable to progress until the blocking call completes.\n\nThis is a common concern while writing SwiftNIO applications. If it is useful to write code in a blocking style, it is highly recommended that you dispatch work to a different thread when you're done with it in your pipeline.\n\n#### Bootstrap\n\nWhile it is possible to configure and register [`Channel`][c]s with [`EventLoop`][el]s directly, it is generally more useful to have a higher-level abstraction to handle this work.\n\nFor this reason, SwiftNIO ships a number of `Bootstrap` objects whose purpose is to streamline the creation of channels. Some `Bootstrap` objects also provide other functionality, such as support for Happy Eyeballs for making TCP connection attempts.\n\nCurrently SwiftNIO ships with three `Bootstrap` objects in the `NIOPosix` module: [`ServerBootstrap`][sbootstrap], for bootstrapping listening channels; [`ClientBootstrap`][cbootstrap], for bootstrapping client TCP channels; and [`DatagramBootstrap`][dbootstrap] for bootstrapping UDP channels.\n\n#### ByteBuffer\n\nThe majority of the work in a SwiftNIO application involves shuffling buffers of bytes around. At the very least, data is sent and received to and from the network in the form of buffers of bytes. For this reason it's very important to have a high-performance data structure that is optimized for the kind of work SwiftNIO applications perform.\n\nFor this reason, SwiftNIO provides [`ByteBuffer`][bb], a fast copy-on-write byte buffer that forms a key building block of most SwiftNIO applications. This type is provided by the `NIOCore` module.\n\n[`ByteBuffer`][bb] provides a number of useful features, and in addition provides a number of hooks to use it in an \"unsafe\" mode. This turns off bounds checking for improved performance, at the cost of potentially opening your application up to memory correctness problems.\n\nIn general, it is highly recommended that you use the [`ByteBuffer`][bb] in its safe mode at all times.\n\nFor more details on the API of [`ByteBuffer`][bb], please see our API documentation, linked below.\n\n#### Promises and Futures\n\nOne major difference between writing concurrent code and writing synchronous code is that not all actions will complete immediately. For example, when you write data on a channel, it is possible that the event loop will not be able to immediately flush that write out to the network. For this reason, SwiftNIO provides [`EventLoopPromise<T>`][elp] and [`EventLoopFuture<T>`][elf] to manage operations that complete *asynchronously*. These types are provided by the `NIOCore` module.\n\nAn [`EventLoopFuture<T>`][elf] is essentially a container for the return value of a function that will be populated *at some time in the future*. Each [`EventLoopFuture<T>`][elf] has a corresponding [`EventLoopPromise<T>`][elp], which is the object that the result will be put into. When the promise is succeeded, the future will be fulfilled.\n\nIf you had to poll the future to detect when it completed that would be quite inefficient, so [`EventLoopFuture<T>`][elf] is designed to have managed callbacks. Essentially, you can chain callbacks off the future that will be executed when a result is available. The [`EventLoopFuture<T>`][elf] will even carefully arrange the scheduling to ensure that these callbacks always execute on the event loop that initially created the promise, which helps ensure that you don't need too much synchronization around [`EventLoopFuture<T>`][elf] callbacks.\n\nAnother important topic for consideration is the difference between how the promise passed to `close` works as opposed to `closeFuture` on a [`Channel`][c]. For example, the promise passed into `close` will succeed after the [`Channel`][c] is closed down but before the [`ChannelPipeline`][cp] is completely cleared out. This will allow you to take action on the [`ChannelPipeline`][cp] before it is completely cleared out, if needed. If it is desired to wait for the [`Channel`][c] to close down and the [`ChannelPipeline`][cp] to be cleared out without any further action, then the better option would be to wait for the `closeFuture` to succeed.\n\nThere are several functions for applying callbacks to [`EventLoopFuture<T>`][elf], depending on how and when you want them to execute. Details of these functions is left to the API documentation.\n\n### Design Philosophy\n\nSwiftNIO is designed to be a powerful tool for building networked applications and frameworks, but it is not intended to be the perfect solution for all levels of abstraction. SwiftNIO is tightly focused on providing the basic I/O primitives and protocol implementations at low levels of abstraction, leaving more expressive but slower abstractions to the wider community to build. The intention is that SwiftNIO will be a building block for server-side applications, not necessarily the framework those applications will use directly.\n\nApplications that need extremely high performance from their networking stack may choose to use SwiftNIO directly in order to reduce the overhead of their abstractions. These applications should be able to maintain extremely high performance with relatively little maintenance cost. SwiftNIO also focuses on providing useful abstractions for this use-case, such that extremely high performance network servers can be built directly.\n\nThe core SwiftNIO repository will contain a few extremely important protocol implementations, such as HTTP, directly in tree. However, we believe that most protocol implementations should be decoupled from the release cycle of the underlying networking stack, as the release cadence is likely to be very different (either much faster or much slower). For this reason, we actively encourage the community to develop and maintain their protocol implementations out-of-tree. Indeed, some first-party SwiftNIO protocol implementations, including our TLS and HTTP/2 bindings, are developed out-of-tree!\n\n## Documentation\n\n - [API documentation](https://swiftpackageindex.com/apple/swift-nio/documentation/nio)\n\n## Example Usage\n\nThere are currently several example projects that demonstrate how to use SwiftNIO.\n\n- **chat client** https://github.com/apple/swift-nio/tree/main/Sources/NIOChatClient\n- **chat server** https://github.com/apple/swift-nio/tree/main/Sources/NIOChatServer\n- **echo client** https://github.com/apple/swift-nio/tree/main/Sources/NIOEchoClient\n- **echo server** https://github.com/apple/swift-nio/tree/main/Sources/NIOEchoServer\n- **UDP echo client** https://github.com/apple/swift-nio/tree/main/Sources/NIOUDPEchoClient\n- **UDP echo server** https://github.com/apple/swift-nio/tree/main/Sources/NIOUDPEchoServer\n- **HTTP client** https://github.com/apple/swift-nio/tree/main/Sources/NIOHTTP1Client\n- **HTTP server** https://github.com/apple/swift-nio/tree/main/Sources/NIOHTTP1Server\n- **WebSocket client** https://github.com/apple/swift-nio/tree/main/Sources/NIOWebSocketClient\n- **WebSocket server** https://github.com/apple/swift-nio/tree/main/Sources/NIOWebSocketServer\n\nTo build & run them, run following command, replace TARGET_NAME with the folder name under `./Sources`\n\n```bash\nswift run TARGET_NAME\n```\n\nFor example, to run NIOHTTP1Server, run following command:\n\n```bash\nswift run NIOHTTP1Server\n```\n\n## Getting Started\n\nSwiftNIO primarily uses [SwiftPM](https://swift.org/package-manager/) as its build tool, so we recommend using that as well. If you want to depend on SwiftNIO in your own project, it's as simple as adding a `dependencies` clause to your `Package.swift`:\n\n```swift\ndependencies: [\n    .package(url: \"https://github.com/apple/swift-nio.git\", from: \"2.0.0\")\n]\n```\n\nand then adding the appropriate SwiftNIO module(s) to your target dependencies.\nThe syntax for adding target dependencies differs slightly between Swift\nversions. For example, if you want to depend on the `NIOCore`, `NIOPosix` and\n`NIOHTTP1` modules, specify the following dependencies:\n\n#### Swift 5.4 and newer (`swift-tools-version:5.4`)\n\n    dependencies: [.product(name: \"NIOCore\", package: \"swift-nio\"),\n                   .product(name: \"NIOPosix\", package: \"swift-nio\"),\n                   .product(name: \"NIOHTTP1\", package: \"swift-nio\")]\n\n### Using Xcode Package support\n\nIf your project is set up as an Xcode project and you're using Xcode 11+, you can add SwiftNIO as a dependency to your\nXcode project by clicking File -> Swift Packages -> Add Package Dependency. In the upcoming dialog, please enter\n`https://github.com/apple/swift-nio.git` and click Next twice. Finally, select the targets you are planning to use (for\nexample `NIOCore`, `NIOHTTP1`, and `NIOFoundationCompat`) and click finish. Now will be able to `import NIOCore` (as well as all\nthe other targets you have selected) in your project.\n\nTo work on SwiftNIO itself, or to investigate some of the demonstration applications, you can clone the repository directly and use SwiftPM to help build it. For example, you can run the following commands to compile and run the example echo server:\n\n```bash\nswift build\nswift test\nswift run NIOEchoServer\n```\n\nTo verify that it is working, you can use another shell to attempt to connect to it:\n\n```bash\necho \"Hello SwiftNIO\" | nc localhost 9999\n```\n\nIf all goes well, you'll see the message echoed back to you.\n\nTo work on SwiftNIO in Xcode, you can just open the `Package.swift`\nfile in Xcode and use Xcode's support for SwiftPM Packages.\n\n## Developing SwiftNIO\n\n*Note*: This section is only relevant if you would like to develop SwiftNIO yourself. You can ignore the information here if you just want to use SwiftNIO as a SwiftPM package.\n\nFor the most part, SwiftNIO development is as straightforward as any other SwiftPM project. With that said, we do have a few processes that are worth understanding before you contribute. For details, please see `CONTRIBUTING.md` in this repository.\n\n### Prerequisites\n\nSwiftNIO's `main` branch is the development branch for the next releases of SwiftNIO 2, it's Swift 5-only.\n\nTo be able to compile and run SwiftNIO and the integration tests, you need to\nhave a few prerequisites installed on your system.\n\n#### macOS\n\n- Xcode 11.4 or newer, Xcode 12 recommended.\n\n### Linux\n\n- Swift 5.7 or newer from [swift.org/download](https://swift.org/download/#releases). We always recommend to use the latest released version.\n- netcat (for integration tests only)\n- lsof (for integration tests only)\n- shasum (for integration tests only)\n\n#### Ubuntu 18.04\n\n```\n# install swift tarball from https://swift.org/downloads\napt-get install -y git curl libatomic1 libxml2 netcat-openbsd lsof perl\n```\n\n\n### Fedora 28+\n\n```\ndnf install swift-lang /usr/bin/nc /usr/bin/lsof /usr/bin/shasum\n```\n\n[ch]: https://swiftpackageindex.com/apple/swift-nio/documentation/niocore/channelhandler\n[c]: https://swiftpackageindex.com/apple/swift-nio/documentation/niocore/channel\n[chc]: https://swiftpackageindex.com/apple/swift-nio/documentation/niocore/channelhandlercontext\n[ec]: https://swiftpackageindex.com/apple/swift-nio/documentation/nioembedded/embeddedchannel\n[el]: https://swiftpackageindex.com/apple/swift-nio/documentation/niocore/eventloop\n[eel]: https://swiftpackageindex.com/apple/swift-nio/documentation/nioembedded/embeddedeventloop\n[elg]: https://swiftpackageindex.com/apple/swift-nio/documentation/niocore/eventloopgroup\n[bb]: https://swiftpackageindex.com/apple/swift-nio/documentation/niocore/bytebuffer\n[elf]: https://swiftpackageindex.com/apple/swift-nio/documentation/niocore/eventloopfuture\n[elp]: https://swiftpackageindex.com/apple/swift-nio/documentation/niocore/eventlooppromise\n[cp]: https://swiftpackageindex.com/apple/swift-nio/documentation/niocore/channelpipeline\n[sbootstrap]: https://swiftpackageindex.com/apple/swift-nio/documentation/nioposix/serverbootstrap\n[cbootstrap]: https://swiftpackageindex.com/apple/swift-nio/documentation/nioposix/clientbootstrap\n[dbootstrap]: https://swiftpackageindex.com/apple/swift-nio/documentation/nioposix/datagrambootstrap\n[mtelg]: https://swiftpackageindex.com/apple/swift-nio/documentation/nioposix/multithreadedeventloopgroup\n[nioh1]: https://swiftpackageindex.com/apple/swift-nio/documentation/niohttp1\n[nioh2]: https://swiftpackageindex.com/apple/swift-nio-http2/documentation/niohttp2\n[niows]: https://swiftpackageindex.com/apple/swift-nio/documentation/niowebsocket\n[niossl]: https://swiftpackageindex.com/apple/swift-nio-ssl/documentation/niossl\n[niossh]: https://swiftpackageindex.com/apple/swift-nio-ssh/documentation/niossh\n[pthreads]: https://en.wikipedia.org/wiki/POSIX_Threads\n[kqueue]: https://en.wikipedia.org/wiki/Kqueue\n[epoll]: https://en.wikipedia.org/wiki/Epoll\n[repo-nio]: https://github.com/apple/swift-nio\n[repo-nio-extras]: https://github.com/apple/swift-nio-extras\n[repo-nio-http2]: https://github.com/apple/swift-nio-http2\n[repo-nio-ssl]: https://github.com/apple/swift-nio-ssl\n[repo-nio-transport-services]: https://github.com/apple/swift-nio-transport-services\n[repo-nio-ssh]: https://github.com/apple/swift-nio-ssh\n",
      "stars_today": 0
    },
    {
      "id": 2928948,
      "name": "azure-sdk-for-java",
      "full_name": "Azure/azure-sdk-for-java",
      "description": "This repository is for active development of the Azure SDK for Java. For consumers of the SDK we recommend visiting our public developer docs at https://docs.microsoft.com/java/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-java. ",
      "html_url": "https://github.com/Azure/azure-sdk-for-java",
      "stars": 2548,
      "forks": 2149,
      "language": "Java",
      "topics": [
        "azure",
        "azure-resources",
        "azure-sdk",
        "azure-services",
        "hacktoberfest",
        "java",
        "media"
      ],
      "created_at": "2011-12-06T23:33:56Z",
      "updated_at": "2026-01-14T23:26:06Z",
      "pushed_at": "2026-01-14T23:38:53Z",
      "open_issues": 572,
      "owner": {
        "login": "Azure",
        "avatar_url": "https://avatars.githubusercontent.com/u/6844498?v=4"
      },
      "readme": "\n# Azure SDK for Java\n\n[![Packages](https://img.shields.io/badge/packages-latest-blue.svg)](https://azure.github.io/azure-sdk/releases/latest/java.html) [![Build Documentation](https://img.shields.io/badge/documentation-published-blue.svg)](https://azure.github.io/azure-sdk-for-java)\n\nThis repository is for active development of the Azure SDK for Java. For consumers of the SDK we recommend visiting our [public developer docs](https://docs.microsoft.com/azure/developer/java/sdk/) or our versioned [developer docs](https://azure.github.io/azure-sdk-for-java).\n\n## Getting started\n\nTo get started with a specific service library, see the **README.md** file located in the library's project folder. You can find service libraries in the `/sdk` directory. For a list of all the services we support access our [list of all existing libraries](https://azure.github.io/azure-sdk/releases/latest/all/java.html).\n\nFor tutorials, samples, quick starts and other documentation, visit [Azure for Java Developers](https://docs.microsoft.com/java/azure/).\n\n### Prerequisites\n\nAll libraries baseline on Java 8, with testing and forward support up until the latest Java long-term support release.\n\n## Available packages\n\nEach service can have both 'client' and 'management' libraries. 'Client' libraries are used to consume the service, whereas 'management' libraries are used to configure and manage the service.\n\n### Client Libraries\n\nOur client libraries follow the [Azure SDK Design Guidelines for Java](https://azure.github.io/azure-sdk/java/guidelines/), and share a number of core features such as HTTP retries, logging, transport protocols, authentication protocols, etc., so that once you learn how to use these features in one client library, you will know how to use them in other client libraries. You can learn about these shared features [here](https://docs.microsoft.com/azure/developer/java/sdk/overview).\nThese libraries can be easily identified by folder, package, and namespaces names starting with `azure-`, e.g. `azure-keyvault`.\n\nYou can find the **[most up-to-date list of new packages on our page](https://azure.github.io/azure-sdk/releases/latest/index.html#java)**. This list includes the most recent releases: both stable and beta.\n\n> NOTE: If you need to ensure your code is ready for production use one of the stable, non-beta libraries.\n\n### Management Libraries\n\nSimilar to our client libraries, the management libraries follow the [Azure SDK Design Guidelines for Java](https://azure.github.io/azure-sdk/java/guidelines/). These libraries provide a high-level, object-oriented API for _managing_ Azure resources, that are optimized for ease of use, succinctness, and consistency. You can find the list of management libraries **[on this page](https://azure.github.io/azure-sdk/releases/latest/mgmt/java.html)**.\n\n**For general documentation on how to use the new libraries for Azure Resource Management, please [visit here](https://aka.ms/azsdk/java/mgmt)**. We have also prepared **[plenty of code samples](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/resourcemanager/docs/SAMPLE.md)** as well as **[migration guide](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/resourcemanager/docs/MIGRATION_GUIDE.md)** in case you are upgrading from previous versions.\n\nThe management libraries can be identified by namespaces that start with `azure-resourcemanager`, e.g. `azure-resourcemanager-compute`.\n\n### Historical Releases\n\nNote that the latest libraries from Microsoft are in the `com.azure` Maven group ID, and have the package naming pattern of beginning with `com.azure`. If you're using libraries that are in `com.microsoft.azure` Maven group ID, or have this as the package structure, please consider migrating to the latest libraries. You can find a mapping table from these historical releases to their equivalent [here](https://azure.github.io/azure-sdk/releases/deprecated/index.html#java).\n\n### Android Support\n\nThe Azure SDKs for Java do not provide support for Android. While we attempt to allow the SDKs to be used on Android, we do not test or support this scenario.\n\n## Need help?\n\n- For reference documentation visit the [Azure SDK for Java documentation](https://aka.ms/java-docs).\n- For tutorials, samples, quick starts and other documentation, visit [Azure for Java Developers](https://docs.microsoft.com/java/azure/).\n- For build reports on code quality, test coverage, etc., visit [Azure Java SDK](https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-java/index.html).\n- File an issue via [GitHub Issues](https://github.com/Azure/azure-sdk-for-java/issues/new/choose).\n- Check [previous questions](https://stackoverflow.com/questions/tagged/azure-java-sdk) or ask new ones on StackOverflow using `azure-java-sdk` tag.\n\n## Navigating the repository\n\n### Main branch\n\nThe main branch has the most recent code with new features and bug fixes. It does **not** represent latest released **stable** SDK.\n\n### Release branches (Release tagging)\n\nFor each package we release there will be a unique git tag created that contains the name and the version of the package to mark the commit of the code that produced the package. This tag will be used for servicing via hotfix branches as well as debugging the code for a particular beta or stable release version.\nFormat of the release tags are `<package-name>_<package-version>`. For more information please see [our branching strategy](https://github.com/Azure/azure-sdk/blob/main/docs/policies/repobranching.md#release-tagging).\n\n## Contributing\n\nFor details on contributing to this repository, see the [contributing guide](https://github.com/Azure/azure-sdk-for-java/blob/main/CONTRIBUTING.md).\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, view [Microsoft's CLA](https://cla.microsoft.com).\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Additional Helpful Links for Contributors\n\nMany people all over the world have helped make this project better.  You'll want to check out:\n\n- [What are some good first issues for new contributors to the repo?](https://github.com/azure/azure-sdk-for-java/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)\n- [How to build and test your change](https://github.com/Azure/azure-sdk-for-java/blob/main/CONTRIBUTING.md#developer-guide)\n- [How you can make a change happen!](https://github.com/Azure/azure-sdk-for-java/blob/main/CONTRIBUTING.md#pull-requests)\n- Frequently Asked Questions (FAQ) and Conceptual Topics in the detailed [Azure SDK for Java wiki](https://github.com/azure/azure-sdk-for-java/wiki).\n\n### Reporting security issues and security bugs\n\nSecurity issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) <secure@microsoft.com>. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the [Security TechCenter](https://www.microsoft.com/msrc/faqs-report-an-issue).\n\n### License\n\nAzure SDK for Java is licensed under the [MIT](https://github.com/Azure/azure-sdk-for-java/blob/main/LICENSE.txt) license.\n\n<!-- Links -->\n[java_guidelines]: https://azure.github.io/azure-sdk/java_introduction.html\n[latest_release_page]: https://azure.github.io/azure-sdk/releases/2020-03/java.html\n[feb_20_release_page]: https://azure.github.io/azure-sdk/releases/2020-02/java.html\n[jan_20_release_page]: https://azure.github.io/azure-sdk/releases/2020-01/java.html\n[dec_19_release_page]: https://azure.github.io/azure-sdk/releases/2019-12/java.html\n[nov_19_release_page]: https://azure.github.io/azure-sdk/releases/2019-11/java.html\n[oct_19_release_page]: https://azure.github.io/azure-sdk/releases/2019-10-11/java.html\n[sep_19_release_page]: https://azure.github.io/azure-sdk/releases/2019-09-17/java.html\n[aug_19_release_page]: https://azure.github.io/azure-sdk/releases/2019-08-06/java.html\n[jul_19_release_page]: https://azure.github.io/azure-sdk/releases/2019-07-10/java.html\n\n\n",
      "stars_today": 0
    },
    {
      "id": 341631350,
      "name": "lucene",
      "full_name": "apache/lucene",
      "description": "Apache Lucene open-source search software",
      "html_url": "https://github.com/apache/lucene",
      "stars": 3302,
      "forks": 1267,
      "language": "Java",
      "topics": [
        "backend",
        "information-retrieval",
        "java",
        "lucene",
        "nosql",
        "search",
        "search-engine"
      ],
      "created_at": "2021-02-23T17:16:56Z",
      "updated_at": "2026-01-14T21:07:25Z",
      "pushed_at": "2026-01-14T22:03:51Z",
      "open_issues": 2502,
      "owner": {
        "login": "apache",
        "avatar_url": "https://avatars.githubusercontent.com/u/47359?v=4"
      },
      "readme": "<!--\n    Licensed to the Apache Software Foundation (ASF) under one or more\n    contributor license agreements.  See the NOTICE file distributed with\n    this work for additional information regarding copyright ownership.\n    The ASF licenses this file to You under the Apache License, Version 2.0\n    the \"License\"); you may not use this file except in compliance with\n    the License.  You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n-->\n\n# Apache Lucene\n\n![Lucene Logo](https://lucene.apache.org/theme/images/lucene/lucene_logo_green_300.png?v=0e493d7a)\n\nApache Lucene is a high-performance, full-featured text search engine library\nwritten in Java.\n\n[![Build Status](https://ci-builds.apache.org/job/Lucene/job/Lucene-Artifacts-main/badge/icon?subject=Lucene)](https://ci-builds.apache.org/job/Lucene/job/Lucene-Artifacts-main/)\n[![Revved up by Develocity](https://img.shields.io/badge/Revved%20up%20by-Develocity-06A0CE?logo=Gradle&labelColor=02303A)](https://develocity.apache.org/scans?search.buildToolType=gradle&search.rootProjectNames=lucene-root)\n\n## Online Documentation\n\nThis README file only contains basic setup instructions.  For more\ncomprehensive documentation, visit:\n\n- Latest Releases: <https://lucene.apache.org/core/documentation.html>\n- Nightly: <https://ci-builds.apache.org/job/Lucene/job/Lucene-Artifacts-main/javadoc/>\n- New contributors should start by reading [Contributing Guide](./CONTRIBUTING.md)\n- Build System Documentation: [help/](./help/)\n- Migration Guide: [lucene/MIGRATE.md](./lucene/MIGRATE.md)\n\n## Building\n\n### Basic steps:\n\n1. Install JDK 25 using your package manager or download manually from\n[OpenJDK](https://jdk.java.net/),\n[Adoptium](https://adoptium.net/temurin/releases),\n[Azul](https://www.azul.com/downloads/),\n[Oracle](https://www.oracle.com/java/technologies/downloads/) or any other JDK provider.\n2. Clone Lucene's git repository (or download the source distribution).\n3. Run gradle launcher script (`gradlew`).\n\nWe'll assume that you know how to get and set up the JDK - if you don't, then we suggest starting at https://jdk.java.net/ and learning more about Java, before returning to this README.\n\n## Contributing\n\nBug fixes, improvements and new features are always welcome!\nPlease review the [Contributing to Lucene\nGuide](./CONTRIBUTING.md) for information on\ncontributing.\n\n- Additional Developer Documentation: [dev-docs/](./dev-docs/)\n\n## Discussion and Support\n\n- [Users Mailing List](https://lucene.apache.org/core/discussion.html#java-user-list-java-userluceneapacheorg)\n- [Developers Mailing List](https://lucene.apache.org/core/discussion.html#developer-lists)\n- IRC: `#lucene` and `#lucene-dev` on freenode.net\n",
      "stars_today": 0
    },
    {
      "id": 96496250,
      "name": "swift-snapshot-testing",
      "full_name": "pointfreeco/swift-snapshot-testing",
      "description": "ğŸ“¸ Delightful Swift snapshot testing.",
      "html_url": "https://github.com/pointfreeco/swift-snapshot-testing",
      "stars": 4111,
      "forks": 641,
      "language": "Swift",
      "topics": [
        "screenshot-testing",
        "snapshot-testing",
        "swift",
        "testing"
      ],
      "created_at": "2017-07-07T03:38:51Z",
      "updated_at": "2026-01-13T15:28:50Z",
      "pushed_at": "2025-11-17T17:51:33Z",
      "open_issues": 194,
      "owner": {
        "login": "pointfreeco",
        "avatar_url": "https://avatars.githubusercontent.com/u/29466629?v=4"
      },
      "readme": "# ğŸ“¸ SnapshotTesting\n\n[![CI](https://github.com/pointfreeco/swift-snapshot-testing/workflows/CI/badge.svg)](https://actions-badge.atrox.dev/pointfreeco/swift-snapshot-testing/goto)\n[![Slack](https://img.shields.io/badge/slack-chat-informational.svg?label=Slack&logo=slack)](http://pointfree.co/slack-invite)\n[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fpointfreeco%2Fswift-snapshot-testing%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/pointfreeco/swift-snapshot-testing)\n[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fpointfreeco%2Fswift-snapshot-testing%2Fbadge%3Ftype%3Dplatforms)](https://swiftpackageindex.com/pointfreeco/swift-snapshot-testing)\n\nDelightful Swift snapshot testing.\n\n## Usage\n\nOnce [installed](#installation), _no additional configuration is required_. You can import the\n`SnapshotTesting` module and call the `assertSnapshot` function.\n\n``` swift\nimport SnapshotTesting\nimport Testing\n\n@MainActor\nstruct MyViewControllerTests {\n  @Test func myViewController() {\n    let vc = MyViewController()\n\n    assertSnapshot(of: vc, as: .image)\n  }\n}\n```\n\nWhen an assertion first runs, a snapshot is automatically recorded to disk and the test will fail,\nprinting out the file path of any newly-recorded reference.\n\n> âŒ failed - No reference was found on disk. Automatically recorded snapshot: â€¦\n>\n> open \"â€¦/MyAppTests/\\_\\_Snapshots\\_\\_/MyViewControllerTests/testMyViewController.png\"\n>\n> Re-run \"testMyViewController\" to test against the newly-recorded snapshot.\n\nRepeat test runs will load this reference and compare it with the runtime value. If they don't\nmatch, the test will fail and describe the difference. Failures can be inspected from Xcode's Report\nNavigator or by inspecting the file URLs of the failure.\n\nYou can record a new reference by customizing snapshots inline with the assertion, or using the\n`withSnapshotTesting` tool:\n\n```swift\n// Record just this one snapshot\nassertSnapshot(of: vc, as: .image, record: .all)\n\n// Record all snapshots in a scope:\nwithSnapshotTesting(record: .all) {\n  assertSnapshot(of: vc1, as: .image)\n  assertSnapshot(of: vc2, as: .image)\n  assertSnapshot(of: vc3, as: .image)\n}\n\n// Record all snapshot failures in a Swift Testing suite:\n@Suite(.snapshots(record: .failed))\nstruct FeatureTests {}\n\n// Record all snapshot failures in an 'XCTestCase' subclass:\nclass FeatureTests: XCTestCase {\n  override func invokeTest() {\n    withSnapshotTesting(record: .failed) {\n      super.invokeTest()\n    }\n  }\n}\n```\n\n## Snapshot Anything\n\nWhile most snapshot testing libraries in the Swift community are limited to `UIImage`s of `UIView`s,\nSnapshotTesting can work with _any_ format of _any_ value on _any_ Swift platform!\n\nThe `assertSnapshot` function accepts a value and any snapshot strategy that value supports. This\nmeans that a view or view controller can be tested against an image representation _and_ against a\ntextual representation of its properties and subview hierarchy.\n\n``` swift\nassertSnapshot(of: vc, as: .image)\nassertSnapshot(of: vc, as: .recursiveDescription)\n```\n\nView testing is highly configurable. You can override trait collections (for specific size classes\nand content size categories) and generate device-agnostic snapshots, all from a single simulator.\n\n``` swift\nassertSnapshot(of: vc, as: .image(on: .iPhoneSe))\nassertSnapshot(of: vc, as: .recursiveDescription(on: .iPhoneSe))\n\nassertSnapshot(of: vc, as: .image(on: .iPhoneSe(.landscape)))\nassertSnapshot(of: vc, as: .recursiveDescription(on: .iPhoneSe(.landscape)))\n\nassertSnapshot(of: vc, as: .image(on: .iPhoneX))\nassertSnapshot(of: vc, as: .recursiveDescription(on: .iPhoneX))\n\nassertSnapshot(of: vc, as: .image(on: .iPadMini(.portrait)))\nassertSnapshot(of: vc, as: .recursiveDescription(on: .iPadMini(.portrait)))\n```\n\n> **Warning**\n> Snapshots must be compared using the exact same simulator that originally took the reference to\n> avoid discrepancies between images.\n\nBetter yet, SnapshotTesting isn't limited to views and view controllers! There are a number of\navailable snapshot strategies to choose from.\n\nFor example, you can snapshot test URL requests (_e.g._, those that your API client prepares).\n\n``` swift\nassertSnapshot(of: urlRequest, as: .raw)\n// POST http://localhost:8080/account\n// Cookie: pf_session={\"userId\":\"1\"}\n//\n// email=blob%40pointfree.co&name=Blob\n```\n\nAnd you can snapshot test `Encodable` values against their JSON _and_ property list representations.\n\n``` swift\nassertSnapshot(of: user, as: .json)\n// {\n//   \"bio\" : \"Blobbed around the world.\",\n//   \"id\" : 1,\n//   \"name\" : \"Blobby\"\n// }\n\nassertSnapshot(of: user, as: .plist)\n// <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n// <!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\"\n//  \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n// <plist version=\"1.0\">\n// <dict>\n//   <key>bio</key>\n//   <string>Blobbed around the world.</string>\n//   <key>id</key>\n//   <integer>1</integer>\n//   <key>name</key>\n//   <string>Blobby</string>\n// </dict>\n// </plist>\n```\n\nIn fact, _any_ value can be snapshot-tested by default using its\n[mirror](https://developer.apple.com/documentation/swift/mirror)!\n\n``` swift\nassertSnapshot(of: user, as: .dump)\n// â–¿ User\n//   - bio: \"Blobbed around the world.\"\n//   - id: 1\n//   - name: \"Blobby\"\n```\n\nIf your data can be represented as an image, text, or data, you can write a snapshot test for it!\n\n## Documentation\n\nThe latest documentation is available\n[here](https://swiftpackageindex.com/pointfreeco/swift-snapshot-testing/main/documentation/snapshottesting).\n\n## Installation\n\n### Xcode\n\n> **Warning**\n> By default, Xcode will try to add the SnapshotTesting package to your project's main\n> application/framework target. Please ensure that SnapshotTesting is added to a _test_ target\n> instead, as documented in the last step, below.\n\n 1. From the **File** menu, navigate through **Swift Packages** and select\n    **Add Package Dependencyâ€¦**.\n 2. Enter package repository URL: `https://github.com/pointfreeco/swift-snapshot-testing`.\n 3. Confirm the version and let Xcode resolve the package.\n 4. On the final dialog, update SnapshotTesting's **Add to Target** column to a test target that\n    will contain snapshot tests (if you have more than one test target, you can later add\n    SnapshotTesting to them by manually linking the library in its build phase).\n\n### Swift Package Manager\n\nIf you want to use SnapshotTesting in any other project that uses\n[SwiftPM](https://swift.org/package-manager/), add the package as a dependency in `Package.swift`:\n\n```swift\ndependencies: [\n  .package(\n    url: \"https://github.com/pointfreeco/swift-snapshot-testing\",\n    from: \"1.12.0\"\n  ),\n]\n```\n\nNext, add `SnapshotTesting` as a dependency of your test target:\n\n```swift\ntargets: [\n  .target(name: \"MyApp\"),\n  .testTarget(\n    name: \"MyAppTests\",\n    dependencies: [\n      \"MyApp\",\n      .product(name: \"SnapshotTesting\", package: \"swift-snapshot-testing\"),\n    ]\n  )\n]\n```\n\n## Features\n\n  - [**Dozens of snapshot strategies**][available-strategies]. Snapshot\n    testing isn't just for `UIView`s and `CALayer`s. Write snapshots against _any_ value.\n  - [**Write your own snapshot strategies**][defining-strategies].\n    If you can convert it to an image, string, data, or your own diffable format, you can snapshot\n    test it! Build your own snapshot strategies from scratch or transform existing ones.\n  - **No configuration required.** Don't fuss with scheme settings and environment variables.\n    Snapshots are automatically saved alongside your tests.\n  - **More hands-off.** New snapshots are recorded whether `isRecording` mode is `true` or not.\n  - **Subclass-free.** Assert from any XCTest case or Quick spec.\n  - **Device-agnostic snapshots.** Render views and view controllers for specific devices and trait\n    collections from a single simulator.\n  - **First-class Xcode support.** Image differences are captured as XCTest attachments. Text\n    differences are rendered in inline error messages.\n  - **Supports any platform that supports Swift.** Write snapshot tests for iOS, Linux, macOS, and\n    tvOS.\n  - **SceneKit, SpriteKit, and WebKit support.** Most snapshot testing libraries don't support these\n    view subclasses.\n  - **`Codable` support**. Snapshot encodable data structures into their JSON and property list\n    representations.\n  - **Custom diff tool integration**. Configure failure messages to print diff commands for\n    [Kaleidoscope](https://kaleidoscope.app) or your diff tool of choice.\n    ``` swift\n    SnapshotTesting.diffToolCommand = { \"ksdiff \\($0) \\($1)\" }\n    ```\n\n[available-strategies]: https://swiftpackageindex.com/pointfreeco/swift-snapshot-testing/main/documentation/snapshottesting/snapshotting\n[defining-strategies]: https://swiftpackageindex.com/pointfreeco/swift-snapshot-testing/main/documentation/snapshottesting/customstrategies\n\n## Plug-ins\n\n  - [AccessibilitySnapshot](https://github.com/cashapp/AccessibilitySnapshot) adds easy regression\n    testing for iOS accessibility.\n    \n  - [AccessibilitySnapshotColorBlindness](https://github.com/Sherlouk/AccessibilitySnapshotColorBlindness)\n    adds snapshot strategies for color blindness simulation on iOS views, view controllers and images.\n\n  - [GRDBSnapshotTesting](https://github.com/SebastianOsinski/GRDBSnapshotTesting) adds snapshot\n    strategy for testing SQLite database migrations made with [GRDB](https://github.com/groue/GRDB.swift).\n\n  - [Nimble-SnapshotTesting](https://github.com/tahirmt/Nimble-SnapshotTesting) adds \n    [Nimble](https://github.com/Quick/Nimble) matchers for SnapshotTesting to be used by Swift\n    Package Manager.\n\n  - [Prefire](https://github.com/BarredEwe/Prefire) generating Snapshot Tests via\n    [Swift Package Plugins](https://github.com/apple/swift-package-manager/blob/main/Documentation/Plugins.md)\n    using SwiftUI `Preview`\n  \n  - [PreviewSnapshots](https://github.com/doordash-oss/swiftui-preview-snapshots) share `View`\n    configurations between SwiftUI Previews and snapshot tests and generate several snapshots with a\n    single test assertion.\n\n  - [swift-html](https://github.com/pointfreeco/swift-html) is a Swift DSL for type-safe,\n    extensible, and transformable HTML documents and includes an `HtmlSnapshotTesting` module to\n    snapshot test its HTML documents.\n\n  - [swift-snapshot-testing-nimble](https://github.com/Killectro/swift-snapshot-testing-nimble) adds\n    [Nimble](https://github.com/Quick/Nimble) matchers for SnapshotTesting.\n\n  - [swift-snapshot-testing-stitch](https://github.com/Sherlouk/swift-snapshot-testing-stitch/) adds\n    the ability to stitch multiple UIView's or UIViewController's together in a single test.\n\n  - [SnapshotTestingDump](https://github.com/tahirmt/swift-snapshot-testing-dump) Adds support to\n    use [swift-custom-dump](https://github.com/pointfreeco/swift-custom-dump/) by using `customDump`\n    strategy for `Any`\n\n  - [SnapshotTestingHEIC](https://github.com/alexey1312/SnapshotTestingHEIC) adds image support\n  using the HEIC storage format which reduces file sizes in comparison to PNG.\n\n  - [SnapshotVision](https://github.com/gregersson/swift-snapshot-testing-vision) adds snapshot\n    strategy for text recognition on views and images. Uses Apples Vision framework.\n\nHave you written your own SnapshotTesting plug-in?\n[Add it here](https://github.com/pointfreeco/swift-snapshot-testing/edit/master/README.md) and\nsubmit a pull request!\n\n## Related Tools\n\n  - [`iOSSnapshotTestCase`](https://github.com/uber/ios-snapshot-test-case/) helped introduce screen\n    shot testing to a broad audience in the iOS community. Experience with it inspired the creation\n    of this library.\n\n  - [Jest](https://jestjs.io) brought generalized snapshot testing to the JavaScript community with\n    a polished user experience. Several features of this library (diffing, automatically capturing\n    new snapshots) were directly influenced.\n\n## Learn More\n\nSnapshotTesting was designed with [witness-oriented programming](https://www.pointfree.co/episodes/ep39-witness-oriented-library-design).\n\nThis concept (and more) are explored thoroughly in a series of episodes on\n[Point-Free](https://www.pointfree.co), a video series exploring functional programming and Swift\nhosted by [Brandon Williams](https://twitter.com/mbrandonw) and\n[Stephen Celis](https://twitter.com/stephencelis).\n\nWitness-oriented programming and the design of this library was explored in the following\n[Point-Free](https://www.pointfree.co) episodes:\n\n  - [Episode 33](https://www.pointfree.co/episodes/ep33-protocol-witnesses-part-1): Protocol Witnesses: Part 1\n  - [Episode 34](https://www.pointfree.co/episodes/ep34-protocol-witnesses-part-1): Protocol Witnesses: Part 2\n  - [Episode 35](https://www.pointfree.co/episodes/ep35-advanced-protocol-witnesses-part-1): Advanced Protocol Witnesses: Part 1\n  - [Episode 36](https://www.pointfree.co/episodes/ep36-advanced-protocol-witnesses-part-2): Advanced Protocol Witnesses: Part 2\n  - [Episode 37](https://www.pointfree.co/episodes/ep37-protocol-oriented-library-design-part-1): Protocol-Oriented Library Design: Part 1\n  - [Episode 38](https://www.pointfree.co/episodes/ep38-protocol-oriented-library-design-part-2): Protocol-Oriented Library Design: Part 2\n  - [Episode 39](https://www.pointfree.co/episodes/ep39-witness-oriented-library-design): Witness-Oriented Library Design\n  - [Episode 40](https://www.pointfree.co/episodes/ep40-async-functional-refactoring): Async Functional Refactoring\n  - [Episode 41](https://www.pointfree.co/episodes/ep41-a-tour-of-snapshot-testing): A Tour of Snapshot Testing ğŸ†“\n\n<a href=\"https://www.pointfree.co/episodes/ep41-a-tour-of-snapshot-testing\">\n  <img alt=\"video poster image\" src=\"https://d3rccdn33rt8ze.cloudfront.net/episodes/0041.jpeg\" width=\"480\">\n</a>\n\n## License\n\nThis library is released under the MIT license. See [LICENSE](LICENSE) for details.\n",
      "stars_today": 0
    },
    {
      "id": 643909,
      "name": "devtools",
      "full_name": "r-lib/devtools",
      "description": "Tools to make an R developer's life easier",
      "html_url": "https://github.com/r-lib/devtools",
      "stars": 2486,
      "forks": 764,
      "language": "R",
      "topics": [
        "package-creation",
        "r"
      ],
      "created_at": "2010-05-03T04:08:49Z",
      "updated_at": "2026-01-13T23:32:52Z",
      "pushed_at": "2026-01-13T23:45:16Z",
      "open_issues": 45,
      "owner": {
        "login": "r-lib",
        "avatar_url": "https://avatars.githubusercontent.com/u/22618716?v=4"
      },
      "readme": "# devtools <a href=\"https://devtools.r-lib.org/\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" alt=\"\"/></a>\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/r-lib/devtools/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/r-lib/devtools/actions/workflows/R-CMD-check.yaml)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/devtools)](https://cran.r-project.org/package=devtools)\n[![Codecov test coverage](https://codecov.io/gh/r-lib/devtools/graph/badge.svg)](https://app.codecov.io/gh/r-lib/devtools)\n<!-- badges: end -->\n\nThe aim of devtools is to make package development easier by providing R\nfunctions that simplify and expedite common tasks. [R\nPackages](https://r-pkgs.org/) is a book based around this workflow.\n\n## Installation\n\n```r\n# Install devtools from CRAN\ninstall.packages(\"devtools\")\n\n# Or the development version from GitHub:\n# install.packages(\"pak\")\npak::pak(\"r-lib/devtools\")\n```\n\n## Cheatsheet\n\n<a href=\"https://raw.githubusercontent.com/rstudio/cheatsheets/main/package-development.pdf\"><img src=\"https://github.com/rstudio/cheatsheets/raw/main/pngs/thumbnails/package-development-thumbs.png\" height=\"252\" alt=\"thumbnail of package development cheatsheet\"/></a>\n\n\n## Usage\n\nAll devtools functions accept a path as an argument, e.g.\n`load_all(\"path/to/mypkg\")`. If you don't specify a path, devtools will\nlook in the current working directory - this is a recommended practice.\n\n### Frequent development tasks:\n\n* `load_all()` simulates installing and reloading your package, loading R code\n  in `R/`, compiled shared objects in `src/` and data files in `data/`. During\n  development you would usually want to access all functions (even un-exported\n  internal ones) so `load_all()` works as if all functions were exported in the\n  package `NAMESPACE`.\n\n* `document()` updates generated documentation in `man/`, file collation and\n  `NAMESPACE`.\n\n* `test()` reloads your code with `load_all()`, then runs all `testthat` tests.\n\n* `test_coverage()` runs test coverage on your package with\n  [covr](https://github.com/r-lib/covr). This makes it easy to see what parts of your\n  package could use more tests!\n\n### Building and installing:\n\n* `install()` reinstalls the package, detaches the currently loaded version\n  then reloads the new version with `library()`. Reloading a package is not\n  guaranteed to work: see the documentation for `unload()` for caveats.\n\n* `build()` builds a package file from package sources. You can use it to build\n  a binary version of your package.\n\n* `install_*` functions install an R package:\n   * `install_github()` from GitHub\n   * `install_gitlab()` from GitLab\n   * `install_bitbucket()` from Bitbucket\n   * `install_url()` from an arbitrary url\n   * `install_git()` and `install_svn()` from an arbitrary git or SVN repository\n   * `install_local()` from a local file on disk\n   * `install_version()` from a specific version on CRAN\n\n* `update_packages()` updates a package to the latest version. This works\n  both on packages installed from CRAN as well as those installed from any of\n  the `install_*` functions.\n\n### Check and release:\n\n* `check()` updates the documentation, then builds and checks the package locally.\n* `check_win_release()`, `check_win_devel()`, and `check_mac_release()` check\n  a package using [win-builder](https://win-builder.r-project.org/) or\n  <https://mac.r-project.org/macbuilder/submit.html>.\n* `release()` and `submit_cran()` handle the mechanics of CRAN submission with\n  or without, respectively, (re)-running lots of local checks.\n\n## Learning more\n\nR package development can be intimidating, however there are now a number of\nvaluable resources to help!\n\n<a href=\"https://r-pkgs.org\"><img src=\"http://r-pkgs.org/images/cover-2e-small.png\" height=\"252\" align = \"right\" alt=\"Cover image of R Packages book\"/></a>\n\n1. R Packages is a book that gives a comprehensive treatment of all common parts\n   of package development and uses devtools throughout.\n    * The first edition is no longer available online, but it is still in print. Note that it has grown somewhat out of sync with the current version of devtools.\n    * A second edition that reflects the current state of devtools, plus new topics such as package websites and GitHub Actions, is available at <https://r-pkgs.org> and in paperback format.\n    * The [Whole Game](https://r-pkgs.org/whole-game.html) and\n      [Package structure](https://r-pkgs.org/package-structure-state.html) chapters\n      make great places to start.\n\n2. [Posit Community - package\n   development](https://forum.posit.co/c/package-development/11)\n   is a great place to ask specific questions related to package development.\n\n3. [rOpenSci packages](https://devguide.ropensci.org/) has\n   extensive documentation on best practices for R packages looking to be\n   contributed to rOpenSci, but also very useful general recommendations\n   for package authors.\n\n4. There are a number of fantastic blog posts on writing your first package, including\n   - [Writing an R package from scratch - Hilary Parker](https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/)\n   - [How to develop good R packages - MaÃ«lle Salmon](https://masalmon.eu/2017/12/11/goodrpackages/)\n   - [Making your first R package - Fong Chun Chan](https://tinyheero.github.io/jekyll/update/2015/07/26/making-your-first-R-package.html)\n   - [Writing an R package from scratch - Tomas Westlake](https://r-mageddon.netlify.app/post/writing-an-r-package-from-scratch/)\n\n5. [Writing R\n   Extensions](https://cran.r-project.org/doc/manuals/r-release/R-exts.html) is\n   the exhaustive, canonical reference for writing R packages, maintained by\n   the R core developers.\n\n## Conscious uncoupling\n\ndevtools started off as a lean-and-mean package to facilitate local package\ndevelopment, but over the years it accumulated more and more functionality.\ndevtools has undergone a [conscious\nuncoupling](https://web.archive.org/web/20140326060230/https://www.goop.com/journal/be/conscious-uncoupling)\nto split out functionality into smaller, more tightly focussed packages. This\nincludes:\n\n* [testthat](https://github.com/r-lib/testthat): Writing and running tests\n  (i.e. `test()`).\n\n* [roxygen2](https://github.com/r-lib/roxygen2): Function and package documentation\n  (i.e. `document()`).\n\n* [remotes](https://github.com/r-lib/remotes): Installing packages (i.e.\n  `install_github()`).\n\n* [pkgbuild](https://github.com/r-lib/pkgbuild): Building binary packages\n  (including checking if build tools are available) (i.e. `build()`).\n\n* [pkgload](https://github.com/r-lib/pkgload): Simulating package loading (i.e.\n  `load_all()`).\n\n* [rcmdcheck](https://github.com/r-lib/rcmdcheck): Running R CMD check and\n  reporting the results (i.e. `check()`).\n\n* [revdepcheck](https://github.com/r-lib/revdepcheck): Running R CMD check on\n  all reverse dependencies, and figuring out what's changed since the last CRAN\n  release (i.e. `revdep_check()`).\n\n* [sessioninfo](https://github.com/r-lib/sessioninfo): R session info (i.e.\n  `session_info()`).\n\n* [usethis](https://github.com/r-lib/usethis): Automating package setup (i.e.\n  `use_test()`).\n\nGenerally, you would not need to worry about these different packages, because\ndevtools installs all of them automatically. You will need to care, however, if\nyou're filing a bug because reporting it at the correct place will lead to a\nspeedier resolution.\n\nYou may also need to care if you are trying to use some devtools functionality\nin your own package or deployed application. Generally in these cases it\nis better to depend on the particular package directly rather than depend on devtools,\ne.g. use `sessioninfo::session_info()` rather than `devtools::session_info()`,\nor `remotes::install_github()` vs `devtools::install_github()`.\n\nHowever for day to day development we recommend you continue to use\n`library(devtools)` to quickly load all needed development tools, just like\n`library(tidyverse)` quickly loads all the tools necessary for data exploration\nand visualization.\n\n## Code of conduct\n\nPlease note that the devtools project is released with a [Contributor Code of Conduct](https://github.com/r-lib/devtools/blob/main/.github/CODE_OF_CONDUCT.md). By contributing to this project, you agree to abide by its terms.\n",
      "stars_today": 0
    },
    {
      "id": 14579179,
      "name": "plotly.R",
      "full_name": "plotly/plotly.R",
      "description": "An interactive graphing library for R",
      "html_url": "https://github.com/plotly/plotly.R",
      "stars": 2656,
      "forks": 635,
      "language": "R",
      "topics": [
        "d3js",
        "data-visualization",
        "ggplot2",
        "javascript",
        "plotly",
        "r",
        "r-package",
        "rstats",
        "shiny",
        "webgl"
      ],
      "created_at": "2013-11-21T05:56:51Z",
      "updated_at": "2026-01-13T07:07:27Z",
      "pushed_at": "2025-12-30T22:01:43Z",
      "open_issues": 757,
      "owner": {
        "login": "plotly",
        "avatar_url": "https://avatars.githubusercontent.com/u/5997976?v=4"
      },
      "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n<img src=\"man/figures/plotly.png\" width=\"200\" />\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/ropensci/plotly/workflows/R-CMD-check/badge.svg)](https://github.com/plotly/plotly.R/actions)\n[![CRAN Status](https://www.r-pkg.org/badges/version/plotly)](https://cran.r-project.org/package=plotly)\n[![CRAN Downloads](https://cranlogs.r-pkg.org/badges/grand-total/plotly)](https://cranlogs.r-pkg.org/badges/grand-total/plotly)\n[![monthly](https://cranlogs.r-pkg.org/badges/plotly)](https://cranlogs.r-pkg.org/badges/plotly)\n<!-- badges: end -->\n\nAn R package for creating interactive web graphics via the open source\nJavaScript graphing library\n[plotly.js](https://github.com/plotly/plotly.js).\n\n<div align=\"center\">\n  <a href=\"https://dash.plotly.com/project-maintenance\">\n    <img src=\"https://dash.plotly.com/assets/images/maintained-by-community.png\" width=\"400px\" alt=\"Maintained by the Plotly Community\">\n  </a>\n</div>\n\n## Installation\n\nInstall from CRAN:\n\n``` r\ninstall.packages(\"plotly\")\n```\n\nOr install the latest development version (on GitHub) via `{remotes}`:\n\n``` r\nremotes::install_github(\"plotly/plotly\")\n```\n\n## Getting started\n\n### Web-based ggplot2 graphics\n\nIf you use [ggplot2](https://github.com/tidyverse/ggplot2), `ggplotly()`\nconverts your static plots to an interactive web-based version\\!\n\n``` r\nlibrary(plotly)\ng <- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") + \n  xlim(1, 6) + ylim(40, 100)\nggplotly(g)\n```\n\n![<https://i.imgur.com/G1rSArP.gifv>](https://i.imgur.com/G1rSArP.gif)\n\nBy default, `ggplotly()` tries to replicate the static ggplot2 version\nexactly (before any interaction occurs), but sometimes you need greater\ncontrol over the interactive behavior. The `ggplotly()` function itself\nhas some convenient â€œhigh-levelâ€ arguments, such as `dynamicTicks`,\nwhich tells plotly.js to dynamically recompute axes, when appropriate.\nThe `style()` function also comes in handy for *modifying* the\nunderlying trace\nattributes (e.g. [hoveron](https://plotly.com/r/reference/#scatter-hoveron)) used to generate the plot:\n\n``` r\ngg <- ggplotly(g, dynamicTicks = \"y\")\nstyle(gg, hoveron = \"points\", hoverinfo = \"x+y+text\", hoverlabel = list(bgcolor = \"white\"))\n```\n\n![<https://i.imgur.com/qRvLgea.gifv>](https://imgur.com/qRvLgea.gif)\n\nMoreover, since `ggplotly()` returns a plotly object, you can apply\nessentially any function from the R package on that object. Some useful\nones include `layout()` (for [customizing the\nlayout](https://plotly-r.com/improving-ggplotly.html#modifying-layout)),\n`add_traces()` (and its higher-level `add_*()` siblings, for example\n`add_polygons()`, for [adding new\ntraces/data](https://plotly-r.com/improving-ggplotly.html#leveraging-statistical-output)),\n`subplot()` (for [combining multiple plotly\nobjects](https://plotly-r.com/arranging-views.html#arranging-plotly-objects)),\nand `plotly_json()` (for inspecting the underlying JSON sent to\nplotly.js).\n\nThe `ggplotly()` function will also respect some â€œunofficialâ€\n**ggplot2** aesthetics, namely `text` (for [customizing the\ntooltip](https://plotly-r.com/controlling-tooltips.html#tooltip-text-ggplotly)),\n`frame` (for [creating\nanimations](https://plotly-r.com/animating-views.html)),\nand `ids` (for ensuring sensible smooth transitions).\n\n### Using plotly without ggplot2\n\nThe `plot_ly()` function provides a more direct interface to plotly.js\nso you can leverage more specialized chart types (e.g., [parallel\ncoordinates](https://plotly.com/r/parallel-coordinates-plot/) or\n[maps](https://plotly.com/r/maps/)) or even some visualization that the\nggplot2 API wonâ€™t ever support (e.g., surface,\n[mesh](https://plotly.com/r/3d-mesh/),\n[trisurf](https://plotly.com/r/trisurf/), etc).\n\n``` r\nplot_ly(z = ~volcano, type = \"surface\")\n```\n\n![<https://plotly.com/~brnvg/1134>](https://plotly.com/~brnvg/1134.png)\n\n## Learn more\n\nTo learn more about special features that the plotly R package provides (e.g., [client-side linking](https://plotly-r.com/client-side-linking.html), [**shiny** integration](https://plotly-r.com/linking-views-with-shiny.html), [editing and generating static images](https://plotly-r.com/publish.html), [custom events in JavaScript](https://plotly-r.com/javascript.html), and more), see <https://plotly-r.com>. You may already be familiar with existing plotly documentation (e.g., <https://plotly.com/r/>), which is essentially a language-agnostic how-to guide for learning plotly.js, whereas <https://plotly-r.com> is meant to be more wholistic tutorial written by and for the R user. The package itself ships with a number of demos (list them by running `demo(package = \"plotly\")`) and shiny/rmarkdown examples (list them by running `plotly_example(\"shiny\")` or `plotly_example(\"rmd\")`). [Carson](https://cpsievert.me) also keeps numerous [slide decks](https://talks.cpsievert.me) with useful examples and concepts.\n\n## Contributing\n\nPlease read through our [contributing\nguidelines](https://github.com/plotly/plotly.R/blob/master/CONTRIBUTING.md).\nIncluded are directions for opening issues, asking questions,\ncontributing changes to plotly, and our code of\nconduct.\n",
      "stars_today": 0
    },
    {
      "id": 45709704,
      "name": "sf",
      "full_name": "r-spatial/sf",
      "description": "Simple Features for R",
      "html_url": "https://github.com/r-spatial/sf",
      "stars": 1415,
      "forks": 299,
      "language": "R",
      "topics": [
        "gdal",
        "geos",
        "proj",
        "r",
        "r-package",
        "rstats",
        "spatial"
      ],
      "created_at": "2015-11-06T21:49:34Z",
      "updated_at": "2026-01-14T15:50:23Z",
      "pushed_at": "2026-01-14T15:55:22Z",
      "open_issues": 72,
      "owner": {
        "login": "r-spatial",
        "avatar_url": "https://avatars.githubusercontent.com/u/25086656?v=4"
      },
      "readme": "<!-- badges: start -->\n[![R-CMD-check](https://github.com/r-spatial/sf/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/r-spatial/sf/actions/workflows/R-CMD-check.yaml)\n[![tic-db](https://github.com/r-spatial/sf/actions/workflows/tic-db.yml/badge.svg)](https://github.com/r-spatial/sf/actions/workflows/tic-db.yml)\n[![Coverage Status](https://img.shields.io/codecov/c/github/r-spatial/sf/main.svg)](https://app.codecov.io/gh/r-spatial/sf)\n[![License](http://img.shields.io/badge/license-GPL%20%28%3E=%202%29-brightgreen.svg?style=flat)](http://www.gnu.org/licenses/gpl-2.0.html)\n[![CRAN](https://www.r-pkg.org/badges/version/sf)](https://cran.r-project.org/package=sf)\n[![cran checks](https://badges.cranchecks.info/worst/sf.svg)](https://cran.r-project.org/web/checks/check_results_sf.html)\n[![Downloads](https://cranlogs.r-pkg.org/badges/sf?color=brightgreen)](https://www.r-pkg.org/pkg/sf)\n[![status](https://tinyverse.netlify.app/badge/sf)](https://CRAN.R-project.org/package=sf)\n<!-- badges: end -->\n\n# Simple Features for R\n\n<a href=\"https://gist.github.com/edzer/f461a3a95570c4ab7edf3125c2f19d20\"><img align=\"right\" src=\"https://user-images.githubusercontent.com/520851/34887433-ce1d130e-f7c6-11e7-83fc-d60ad4fae6bd.gif\" /></a>\n\nA package that provides [simple features access](https://en.wikipedia.org/wiki/Simple_Features) for R. \n\n[Blogs, links](#blogs-presentations-vignettes-sp-sf-wiki) â€¢ [Cheatsheet](#cheatsheet) â€¢ [Installing](#installing)\nâ€¢ [Contributing](#contributing) â€¢ [Acknowledgment](#acknowledgment) â€¢ [How to cite](#how-to-cite)\n\nPackage sf:\n\n* represents simple features as records in a `data.frame` or `tibble` with a geometry list-column\n* represents natively in R all 17 simple feature types for all dimensions (XY, XYZ, XYM, XYZM)\n* interfaces to [GEOS](https://libgeos.org) for geometrical operations on projected coordinates, and (through R package [s2](https://cran.r-project.org/package=s2)) to [s2geometry](http://s2geometry.io/) for geometrical operations on ellipsoidal coordinates\n* interfaces to [GDAL](https://gdal.org/), supporting all driver options, `Date` and `POSIXct` and list-columns\n* interfaces to [PRÃ˜J](http://proj.org/) for coordinate reference system conversion and transformation\n* uses [well-known-binary](https://en.wikipedia.org/wiki/Well-known_text#Well-known_binary) serialisations written in C++/Rcpp for fast I/O with GDAL and GEOS \n* reads from and writes to spatial databases such as [PostGIS](http://postgis.net/) using [DBI](https://cran.r-project.org/package=DBI)\n* is extended by \n    * [lwgeom](https://github.com/r-spatial/lwgeom/) for selected liblwgeom/PostGIS functions\n    * [stars](https://github.com/r-spatial/stars/) for raster data, and raster or vector data cubes (spatial time series)\n    * [sfnetworks](https://luukvdmeer.github.io/sfnetworks/) for geospatial network data\n\n<a href=\"https://gist.github.com/edzer/442d74a5775abcd5068cf3e73b23687b\"><img align=\"left\" src=\"https://user-images.githubusercontent.com/520851/50280460-e35c1880-044c-11e9-9ed7-cc46754e49db.jpg\" /></a>\n\n(Illustration (c) 2018 by <a href=\"https://twitter.com/allison_horst/status/1071456081308614656\">Allison Horst</a>)\n\n## Books, journal articles, blogs, presentations, vignettes, sp-sf wiki\n\n* an open access [R Journal article](https://journal.r-project.org/archive/2018/RJ-2018-009/index.html) summarizes the package\n* two books: [Spatial Data Science: with applications in R](https://r-spatial.org/book/), [Geocomputation with R](https://r.geocompx.org/)\n* package vignettes: [first](https://r-spatial.github.io/sf/articles/sf1.html), [second](https://r-spatial.github.io/sf/articles/sf2.html), [third](https://r-spatial.github.io/sf/articles/sf3.html), [fourth](https://r-spatial.github.io/sf/articles/sf4.html), [fifth](https://r-spatial.github.io/sf/articles/sf5.html), [sixth](https://r-spatial.github.io/sf/articles/sf6.html), [seventh](https://r-spatial.github.io/sf/articles/sf7.html)\n* blog posts: [first](https://r-spatial.org/r/2016/02/15/simple-features-for-r.html), [second](https://r-spatial.org/r/2016/07/18/sf2.html), [third](https://r-spatial.org/r/2016/11/02/sfcran.html), [fourth](https://r-spatial.org/r/2017/01/12/newssf.html)\n* the original R Consortium ISC [proposal](PROPOSAL.md), the R Consortium [blog post](https://www.r-consortium.org/blog/2017/01/03/simple-features-now-on-cran)\n* presentations: [rstudio::conf 2018](https://edzer.github.io/rstudio_conf/#1) ([video](https://posit.co/resources/videos/tidy-spatial-data-analysis/)), [UseR! 2016](http://pebesma.staff.ifgi.de/pebesma_sfr.pdf)\n* wiki page describing [sp-sf migration](https://github.com/r-spatial/sf/wiki/Migrating)\n\n## Cheatsheet\n[CC 4.0](https://creativecommons.org/licenses/by/4.0/) BY [Ryan Garnett](https://github.com/ryangarnett)  \n\n<a href=\"https://github.com/rstudio/cheatsheets/blob/main/sf.pdf\"><img src=\"https://raw.githubusercontent.com/rstudio/cheatsheets/main/pngs/sf.png\" /></a>\n\n## Installing\n\nInstall either from CRAN with:\n```r\ninstall.packages(\"sf\")\n```\nThis will install binary packages on Windows and MacOS, unless you configured R such that it tries to install source packages; in that case, see below.\n\nInstall development versions from GitHub with:\n```r\nlibrary(remotes)\ninstall_github(\"r-spatial/sf\")\n```\n\n### Windows\n\nInstalling sf from source works under Windows when [Rtools](https://cran.r-project.org/bin/windows/Rtools/) is installed.\n\n### MacOS\n\nMacOS users are strongly encouraged to install the `sf` binary packages from CRAN, unless they are familiar with compilers, linking, C++ source code, and homebrew. If you experience that R tries to install `sf` from source (or otherwise your install fails but you don't understand what is going on) try again by explicitly installing the binary, using\n\n```r\ninstall.packages(\"sf\", type = \"binary\")\n```\n\nThe remainder of this section is for those who understand what source installs mean, and imply.\n\nPerhaps the easiest way of an install from source is to first install `gdal` using Homebrew. Recent versions of Homebrew include a full-featured up-to-date [gdal formula](https://github.com/Homebrew/homebrew-core/blob/master/Formula/g/gdal.rb), which installs `proj` and `gdal` at the same time:\n\n```\nbrew install pkg-config\nbrew install gdal\n```\n\nOnce gdal is installed, you may be able to install `sf` package from source in R. With the current version of `proj` on homebrew, installation requires additional configuration:\n\n```r\ninstall.packages(\"sf\", type = \"source\", configure.args = \"--with-proj-lib=$(brew --prefix)/lib/\")\n```\n\nOr the development version:\n\n```r\nlibrary(remotes)\ninstall_github(\"r-spatial/sf\", configure.args = \"--with-proj-lib=$(brew --prefix)/lib/\")\n```\n\nAlternatively, [these instructions](https://stat.ethz.ch/pipermail/r-sig-mac/2017-June/012429.html) explain how to install gdal using kyngchaos frameworks.\n\nFor Mac OS 11 Big Sur source install instruction, see [here](https://github.com/r-spatial/sf/issues/1536#issuecomment-727342736)\n\n### Linux\n\nFor Unix-alikes, GDAL (>= 2.0.1), GEOS (>= 3.4.0) and Proj.4 (>= 4.8.0) are required.\n\n#### Ubuntu\n\nDependencies for recent versions of Ubuntu (18.04 and later) are available in the official repositories; you can install them with:\n\n```sh\nsudo apt -y update && apt install -y libudunits2-dev libgdal-dev libgeos-dev libproj-dev libsqlite3-dev\n```\n\nHowever, to get more up-to-date versions of dependencies such as GDAL, GEOS and PROJ we recommend adding the [ubuntugis-unstable](http://ppa.launchpad.net/ubuntugis/ubuntugis-unstable/ubuntu/) PPA to the package repositories and installing them as follows:\n\n```sh\nsudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable\nsudo apt update\nsudo apt install libudunits2-dev libgdal-dev libgeos-dev libproj-dev libsqlite3-dev\n```\n\nAdding this PPA is required for installing `sf` on older versions of Ubuntu (e.g. Xenial).\n\nAnother option, for advanced users, is to install dependencies from source; see e.g. an older [Travis](https://github.com/r-spatial/sf/blob/593ee48b34001fe3b383ea73ea57063ecf690732/.travis.yml) config file for hints.\n\n#### Fedora\nThe following command installs all required dependencies:\n```sh\nsudo dnf install gdal-devel proj-devel geos-devel sqlite-devel udunits2-devel\n```\n\n#### Arch\n\nGet gdal, proj, geos and podofo from the main repos, and udunits from the AUR:\n\n```\npacman -S gdal proj geos arrow podofo\nyay/pacaur/yaourt/whatever -S udunits\n```\n\n#### `renv` or `conda`\n\nThere are several reports that `sf` fails to install as a source package when R is used with `renv`, or when R is installed in a `conda` environment. If you experience this, please do not raise an issue here, but \n\n* try to sort this out with the `renv` developers or the `conda` maintainers, or\n* try to use binary installs of the `sf` package, e.g. from [r2u](https://github.com/eddelbuettel/r2u), or the Posit package manager\n\n#### Other\n\nTo install on Debian, the [rocker geospatial](https://github.com/rocker-org/geospatial) Dockerfiles may be helpful. Ubuntu Dockerfiles are found [here](https://github.com/r-spatial/sf/tree/main/inst/docker).\n\n### Multiple GDAL, GEOS and/or PROJ versions on your system\n\nIf you use dynamic linking (installation from source) and have multiple versions of these libraries installed (e.g. one from ubuntugis-unstable, another installed from source in `/usr/local/lib`) then this will in general not work, even when setting `LD_LIBRARY_PATH` manually. See [here](https://github.com/r-spatial/sf/issues/844) for the reason why. \n\n### lwgeom\n\nFunctions and methods that require `liblwgeom`, including ellipsoidal (not spherical or Euclidean) metrics (area, distances), are provide by and used from [lwgeom](https://github.com/r-spatial/lwgeom), which is also on [CRAN](https://cran.r-project.org/package=lwgeom).\n\n## Contributing\n\n* Contributions of all sorts are most welcome, issues and pull requests are the preferred ways of sharing them.\n* When contributing pull requests, please adhere to the package style (in package code use `=` rather than `<-`; don't change indentation; tab stops of 4 spaces are preferred).\n* This project is released with a [Contributor Code of Conduct](CONDUCT.md). By participating in this project, you agree to abide by its terms.\n\n## How to cite\n\nPackage `sf` can be cited as: \n\n* Edzer Pebesma, 2018.  Simple Features for R: Standardized Support\nfor Spatial Vector Data. The R Journal [10:1, 439-446.](https://journal.r-project.org/archive/2018/RJ-2018-009/index.html)\n\n* Pebesma, E.; Bivand, R. (2023). [Spatial Data Science: With Applications in R](https://r-spatial.org/book/) \n(1st ed.). 314 pages. [Chapman and Hall/CRC](https://doi.org/10.1201/9780429459016).\n\n## Acknowledgment\n\nThis project gratefully acknowledges financial [support](https://www.r-consortium.org/projects) from the\n\n<a href=\"https://r-consortium.org/all-projects/2016-group-1.html#simple-features-for-r\">\n<img src=\"https://r-consortium.org/images/RConsortium_Horizontal_Pantone.webp\" width=\"300\">\n</a>\n<!--\n<img src=\"http://pebesma.staff.ifgi.de/RConsortium_Horizontal_Pantone.png\" width=\"300\">\n-->\n\n",
      "stars_today": 0
    },
    {
      "id": 159560389,
      "name": "renv",
      "full_name": "rstudio/renv",
      "description": "renv: Project environments for R.",
      "html_url": "https://github.com/rstudio/renv",
      "stars": 1122,
      "forks": 162,
      "language": "R",
      "topics": [],
      "created_at": "2018-11-28T20:25:39Z",
      "updated_at": "2026-01-10T14:35:21Z",
      "pushed_at": "2026-01-06T17:48:32Z",
      "open_issues": 211,
      "owner": {
        "login": "rstudio",
        "avatar_url": "https://avatars.githubusercontent.com/u/513560?v=4"
      },
      "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# renv <img src=\"man/figures/logo.svg\" align=\"right\" height=\"115\"/>\n\n<!-- badges: start -->\n\n[![Lifecycle:\nstable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/renv)](https://CRAN.R-project.org/package=renv)\n[![R-CMD-check](https://github.com/rstudio/renv/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rstudio/renv/actions/workflows/R-CMD-check.yaml)\n\n<!-- badges: end -->\n\n## Overview\n\nThe renv package[^1] helps you create **r**eproducible **env**ironments\nfor your R projects. Use renv to make your R projects more isolated,\nportable and reproducible.\n\n- **Isolated**: Installing a new or updated package for one project\n  wonâ€™t break your other projects, and vice versa. Thatâ€™s because renv\n  gives each project its own private library.\n- **Portable**: Easily transport your projects from one computer to\n  another, even across different platforms. renv makes it easy to\n  install the packages your project depends on.\n- **Reproducible**: renv records the exact package versions you depend\n  on, and ensures those exact versions are the ones that get installed\n  wherever you go.\n\n## Installation\n\nInstall the latest version of renv from CRAN with:\n\n``` r\ninstall.packages(\"renv\")\n```\n\nAlternatively, install the development version from\n[r-universe](https://rstudio.r-universe.dev/renv) with:\n\n``` r\ninstall.packages(\"renv\", repos = \"https://rstudio.r-universe.dev\")\n```\n\n## Workflow\n\n<img src=\"vignettes/renv.png\" alt=\"A diagram showing the most important verbs and nouns of renv. Projects start with init(), which creates a project library using packages from the system library. snapshot() updates the lockfile using the packages installed in the project library, where restore() installs packages into the project library using the metadata from the lockfile, and status() compares the lockfile to the project library. You install and update packages from CRAN and GitHub using install() and update(), but because you'll need to do this for multiple projects, renv uses cache to make this fast.\" width=\"408\" style=\"display: block; margin: auto;\" />\n\nUse `renv::init()` to initialize renv in a new or existing project. This\nwill set up a **project library**, containing all the packages youâ€™re\ncurrently using. The packages (and all the metadata needed to reinstall\nthem) are recorded into a **lockfile**, `renv.lock`, and a `.Rprofile`\nensures that the library is used every time you open that project.\n\nAs you continue to work on your project, you will install and upgrade\npackages, either using `install.packages()` and `update.packages()` or\n`renv::install()` and `renv::update()`. After youâ€™ve confirmed your code\nworks as expected, use `renv::snapshot()` to record the packages and\ntheir sources in the lockfile.\n\nLater, if you need to share your code with someone else or run your code\non new machine, your collaborator (or you) can call `renv::restore()` to\nreinstall the specific package versions recorded in the lockfile.\n\n## Learning more\n\nIf this is your first time using renv, we strongly recommend starting\nwith the [Introduction to\nrenv](https://rstudio.github.io/renv/articles/renv.html) vignette: this\nwill help you understand the most important verbs and nouns of renv.\n\nIf you have a question about renv, please first check the\n[FAQ](https://rstudio.github.io/renv/articles/faq.html) to see whether\nyour question has already been addressed. If it hasnâ€™t, please feel free\nto ask on the [Posit Forum](https://forum.posit.co).\n\nIf you believe youâ€™ve found a bug in renv, please file a bug (and, if\npossible, a [reproducible example](https://reprex.tidyverse.org)) at\n<https://github.com/rstudio/renv/issues>.\n\n[^1]: Pronounced â€œRâ€ â€œenvâ€\n",
      "stars_today": 0
    },
    {
      "id": 259225467,
      "name": "CellChat",
      "full_name": "sqjin/CellChat",
      "description": "R toolkit for inference, visualization and analysis of cell-cell communication from single-cell data",
      "html_url": "https://github.com/sqjin/CellChat",
      "stars": 756,
      "forks": 167,
      "language": "R",
      "topics": [
        "cell-cell-communication",
        "cell-cell-interaction",
        "microenvironment",
        "single-cell-analysis"
      ],
      "created_at": "2020-04-27T06:28:33Z",
      "updated_at": "2026-01-11T17:20:28Z",
      "pushed_at": "2024-01-06T18:23:14Z",
      "open_issues": 455,
      "owner": {
        "login": "sqjin",
        "avatar_url": "https://avatars.githubusercontent.com/u/32399212?v=4"
      },
      "readme": "\n<p align=\"center\">\n  <img width=\"200\"  src=\"https://github.com/sqjin/CellChat/blob/master/CellChat_Logo.png\">\n</p>\n\n# CAUTION\nWe have updated CellChat to v2 and migrated CellChat to a new repository. This repository will be NOT updated and maintained any more. Please check the new repository [jinworks/CellChat](https://github.com/jinworks/CellChat) for the new updates, and the [CellChat v2 paper](https://biorxiv.org/cgi/content/short/2023.11.05.565674v1) for a comprehensive protocol of CellChat.  \n\n# About CellChat and CellChatDB\nCellChat is an R package designed for inference, analysis, and visualization of cell-cell communication from single-cell data. CellChat aims to enable users to identify and interpret cell-cell communication within an easily interpretable framework, with the emphasis of clear, attractive, and interpretable visualizations.  \n\nCellChatDB is a manually curated database of literature-supported ligand-receptor interactions in mutiple species, leading to a comprehensive recapitulation of known molecular interaction mechanisms including multi-subunit structure of ligand-receptor complexes and co-factors.\n\nIf you use CellChat in your research, please considering citing our papers: \n- [Suoqin Jin et al., CellChat for systematic analysis of cell-cell communication from single-cell and spatially resolved transcriptomics, bioRxiv 2023](https://biorxiv.org/cgi/content/short/2023.11.05.565674v1) [CellChat v2]\n- [Suoqin Jin et al., Inference and analysis of cell-cell communication using CellChat, Nature Communications 2021](https://www.nature.com/articles/s41467-021-21246-9) [CellChat v1]\n\n# Capabilities\nIn addition to infer the intercellular communication from any given single-cell data, CellChat provides functionality for further data exploration, analysis, and visualization. \n\n- It can quantitatively characterize and compare the inferred cell-cell communication networks using a systems approach by combining social network analysis, pattern recognition, and manifold learning approaches.\n- It provides an easy-to-use tool for extracting and visualizing high-order information of the inferred networks. For example, it allows ready prediction of major signaling inputs and outputs for all cell populations and how these populations and signals coordinate together for functions.\n- It enables comparative analysis of cell-cell communication across different conditions and identification of altered signaling and cell populations. \n- It provides several visualization outputs to facilitate intuitive user-guided data interpretation.\n\n<p align=\"center\">\n  <img width=\"700\"  src=\"https://github.com/sqjin/CellChat/blob/master/overview_CellChat.png\">\n</p>\n\n\n<p align=\"center\">\n  <a href=\"https://clustrmaps.com/site/1bpq2\">\n     <img width=\"200\"  src=\"https://clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=n&d=42WqeykSXznN_NSaBlpf6CtSXQxhqmIs6QusUsguFdY\" />\n   </a>\n</p>\n<p align=\"center\">\n  <a href=\"#\">\n     <img src=\"https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fsqjin%2FCellChat&labelColor=%233499cc&countColor=%2370c168\" />\n   </a>\n</p>\n\n\n\n\n\n",
      "stars_today": 0
    },
    {
      "id": 53894616,
      "name": "infercnv",
      "full_name": "broadinstitute/infercnv",
      "description": "Inferring CNV from Single-Cell RNA-Seq",
      "html_url": "https://github.com/broadinstitute/infercnv",
      "stars": 648,
      "forks": 177,
      "language": "R",
      "topics": [],
      "created_at": "2016-03-14T21:53:54Z",
      "updated_at": "2026-01-12T23:27:31Z",
      "pushed_at": "2025-11-14T17:35:17Z",
      "open_issues": 236,
      "owner": {
        "login": "broadinstitute",
        "avatar_url": "https://avatars.githubusercontent.com/u/393552?v=4"
      },
      "readme": "\n***********************************************\n>InferCNV is no longer supported. Please explore alternatives such as InferCNA (https://jlaffy.github.io/infercna/)[https://jlaffy.github.io/infercna/], CopyKAT (https://github.com/navinlabcode/copykat)[https://github.com/navinlabcode/copykat], and Numbat (https://github.com/kharchenkolab/numbat)[https://github.com/kharchenkolab/numbat]\n***********************************************\n\n# Subclustering\n\nSubclustering resolution is one of the primary settings that will need to be adjusted in most runs to avoid oversplitting. The tutorial below explains how it works and details about it can also be found on the [wiki](https://github.com/broadinstitute/infercnv/wiki/infercnv-tumor-subclusters#tumor-subclustering-by-leiden-clustering-preferred).\n\n# Documentation\n### Full documentation\n\nVisit project [wiki](https://github.com/broadinstitute/inferCNV/wiki) for InferCNV documentation.\n\n\n### Infercnv video tutorial\n\nA **video** tutorial giving on overview of infercnv features and how to run an analysis can be found below **(click on the image)**:\n\n[![Tutorial: Running infercnv](http://img.youtube.com/vi/-qOcHAavZT8/0.jpg)](http://www.youtube.com/watch?v=-qOcHAavZT8 \"Tutorial: Running infercnv\")\n\n\n\n",
      "stars_today": 0
    },
    {
      "id": 120286519,
      "name": "nichenetr",
      "full_name": "saeyslab/nichenetr",
      "description": "NicheNet: predict active ligand-target links between interacting cells",
      "html_url": "https://github.com/saeyslab/nichenetr",
      "stars": 602,
      "forks": 134,
      "language": "R",
      "topics": [
        "cell-cell-communication",
        "data-integration",
        "gene-expression",
        "intercellular-communication",
        "ligand-receptor",
        "ligand-target",
        "network-inference",
        "rna-seq",
        "single-cell-omics",
        "single-cell-rna-seq"
      ],
      "created_at": "2018-02-05T09:58:45Z",
      "updated_at": "2026-01-12T00:44:16Z",
      "pushed_at": "2025-11-12T09:07:45Z",
      "open_issues": 25,
      "owner": {
        "login": "saeyslab",
        "avatar_url": "https://avatars.githubusercontent.com/u/18485264?v=4"
      },
      "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n<!-- github markdown built using\nrmarkdown::render(\"README.Rmd\",output_format = \"md_document\")\n-->\n\n# nichenetr\n\n<!-- badges: start -->\n\n[![R build\nstatus](https://github.com/saeyslab/nichenetr/workflows/R-CMD-check-bioc/badge.svg)](https://github.com/saeyslab/nichenetr/actions)\n[![Coverage\nStatus](https://codecov.io/gh/saeyslab/nichenetr/branch/master/graph/badge.svg)](https://codecov.io/gh/saeyslab/nichenetr)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3260758.svg)](https://doi.org/10.5281/zenodo.3260758)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7074291.svg)](https://doi.org/10.5281/zenodo.7074291)\n<!-- badges: end -->\n\n**nichenetr: the R implementation of the NicheNet method.** The goal of\nNicheNet is to study intercellular communication from a computational\nperspective. NicheNet uses human or mouse gene expression data of\ninteracting cells as input and combines this with a prior model that\nintegrates existing knowledge on ligand-to-target signaling paths. This\nallows to predict ligand-receptor interactions that might drive gene\nexpression changes in cells of interest.\n\nWe describe the NicheNet algorithm in the following paper: [NicheNet:\nmodeling intercellular communication by linking ligands to target\ngenes](https://www.nature.com/articles/s41592-019-0667-5).\n\nTo help users **customize NicheNet** to their specific biological use-case, we have recently published a **best practices workflow** cultivated over four years of experience and user feedback  \n[Unraveling cell-cell communication with NicheNet by inferring active\nligands from transcriptomics\ndata](https://www.nature.com/articles/s41596-024-01121-9). In the\nstep-by-step protocol, we describe both a â€˜sender-agnosticâ€™ approach\nthat considers ligands from the entire microenvironment and a\nâ€˜sender-focusedâ€™ approach that only considers ligands from cell\npopulations of interest. We also include a new downstream procedure for\nprioritizing cell type-specific ligand-receptor pairs. The code to\nreproduce this protocol and the resulting figures can be found on\n<https://github.com/saeyslab/nichenet_protocol>.\n\n## Installation of nichenetr\n\nInstallation typically takes a few minutes, depending on the number of\ndependencies that has already been installed on your PC. You can install\nnichenetr (and required dependencies) from github with:\n\n``` r\nif(!requireNamespace(\"devtools\", quietly = TRUE)) {\n  install.packages(\"devtools\") \n}\n\ndevtools::install_github(\"saeyslab/nichenetr\")\n```\n\nnichenetr was tested on both Windows and Linux (most recently tested R\nversion: R 4.3.2)\n\n## Overview of NicheNet\n\n<details>\n<summary>\n<h3>\nBackground\n</h3>\n</summary>\n\nNicheNet strongly differs from most computational approaches to study\ncell-cell communication (CCC), as summarized conceptually by the figure\nbelow (**top panel:** current ligand-receptor inference approaches;\n**bottom panel:** NicheNet). Many approaches to study CCC from\nexpression data involve linking ligands expressed by sender cells to\ntheir corresponding receptors expressed by receiver cells. However,\nfunctional understanding of a CCC process also requires knowing how\nthese inferred ligand-receptor interactions result in changes in the\nexpression of downstream target genes within the receiver cells.\nTherefore, we developed NicheNet to consider the gene regulatory effects\nof ligands. <br><br>\n<img src=\"vignettes/images/comparison_other_approaches_2.jpg\"\nwidth=\"450\" /> <br><br>\n\nAt the core of NicheNet is a prior knowledge model, created by\nintegrating three types of databasesâ€”ligand-receptor interactions,\nsignaling pathways, and transcription factor (TF) regulationâ€”to form a\ncomplete communication network spanning from ligands to their downstream\ntarget genes (see figure below). Therefore, this model goes beyond\nligand-receptor interactions and incorporates intracellular signaling\nand transcriptional regulation as well. As a result, NicheNet is able to\npredict which ligands influence the expression in another cell, which\ntarget genes are affected by each ligand, and which signaling mediators\nmay be involved. By generating these novel types of hypotheses, NicheNet\ncan drive an improved functional understanding of a CCC process of\ninterest. Note that although we provide a pre-built prior model, it is\nalso possible to construct your own model (see vignettes below).\n\n<img src=\"vignettes/images/nichenet_prior_model.png\"\nstyle=\"width:70.0%\" />\n</details>\n<details>\n<summary>\n<h3>\nMain functionalities of nichenetr\n</h3>\n</summary>\n\n- Assessing how well ligands expressed by a sender cell can predict\n  changes in gene expression in the receiver cell\n- Prioritizing ligands based on their effect on gene expression\n- Inferring putative ligand-target links active in the system under\n  study\n- Inferring potential signaling paths between ligands and target genes\n  of interest: to generate causal hypotheses and check which data\n  sources support the predictions\n- Validation of the prior ligand-target model\n- Construction of user-defined prior ligand-target models\n\nMoreover, we provide instructions on how to make intuitive\nvisualizations of the main predictions (e.g., via circos plots as shown\nhere below).\n\n<br><br>\n<img src=\"vignettes/images/circos_plot_adapted.jpg\" width=\"600\" />\n\n</details>\n\nAs input to NicheNet, users must provide cell type-annotated expression\ndata that reflects a cell-cell communication (CCC) event. The input can\nbe single-cell or sorted bulk data from human or mouse. As output,\nNicheNet returns the ranking of ligands that best explain the CCC event\nof interest, as well as candidate target genes with high potential to be\nregulated by these ligands. As an intermediate step, we extract the\nthree features required for the analysis: a list of potential ligands, a\ngene set that captures the downstream effects of the CCC event of\ninterest, and a background set of genes. Further explanation on each\nfeature can be found in the introductory vignette.\n\n![](vignettes/images/figure1.svg) <br><br>\n\n## Learning to use nichenetr\n\nThe following vignettes contain the explanation on how to perform a\nbasic NicheNet analysis on a Seurat object. This includes prioritizing\nligands and predicting target genes of prioritized ligands. We recommend\nstarting with the step-by-step analysis, but we also demonstrate the use\nof a single wrapper function. This demo analysis takes only a few\nminutes to run.\n\n- [Perform NicheNet analysis starting from a Seurat object: step-by-step\n  analysis](vignettes/seurat_steps.md):`vignette(\"seurat_steps\", package=\"nichenetr\")`\n- [Perform NicheNet analysis starting from a Seurat\n  object](vignettes/seurat_wrapper.md):`vignette(\"seurat_wrapper\", package=\"nichenetr\")`\n\nCase study on HNSCC tumor which demonstrates the flexibility of\nNicheNet. Here, the gene set of interest was determined by the original\nauthors, and the expression data is a matrix rather than a Seurat\nobject.\n\n- [NicheNetâ€™s ligand activity analysis on a gene set of\n  interest](vignettes/ligand_activity_geneset.md):\n  `vignette(\"ligand_activity_geneset\", package=\"nichenetr\")`\n\nThe following vignettes contain explanation on how to do some follow-up\nanalyses after performing the most basic analysis:\n\n- [Prioritization of ligands based on expression\n  values](vignettes/seurat_steps_prioritization.md):\n  `vignette(\"seurat_steps_prioritization\", package=\"nichenetr\")`\n- [Inferring ligand-to-target signaling\n  paths](vignettes/ligand_target_signaling_path.md):\n  `vignette(\"ligand_target_signaling_path\", package=\"nichenetr\")`\n- [Assess how well top-ranked ligands can predict a gene set of\n  interest](vignettes/target_prediction_evaluation_geneset.md):\n  `vignette(\"target_prediction_evaluation_geneset\", package=\"nichenetr\")`\n- [Single-cell NicheNetâ€™s ligand activity\n  analysis](vignettes/ligand_activity_single_cell.md):\n  `vignette(\"ligand_activity_single_cell\", package=\"nichenetr\")`\n\nIf you want to make a circos plot visualization of the NicheNet output\nto show active ligand-target links between interacting cells, you can\ncheck following vignettes:\n\n- [Seurat Wrapper + circos\n  visualization](vignettes/seurat_wrapper_circos.md):`vignette(\"seurat_wrapper_circos\", package=\"nichenetr\")`.\n- [HNSCC case study + double circos\n  visualization](vignettes/circos.md):`vignette(\"circos\", package=\"nichenetr\")`.\n\nPeople interested in building their own models or benchmarking their own\nmodels against NicheNet can read one of the following vignettes:\n\n- [Model construction](vignettes/model_construction.md):\n  `vignette(\"model_construction\", package=\"nichenetr\")`\n- [Using LIANA ligand-receptor databases to construct the ligand-target\n  model](vignettes/model_construction_with_liana.md):\n  `vignette(\"model_construction_with_liana\", package=\"nichenetr\")`\n- [Model evaluation: target gene and ligand activity\n  prediction](vignettes/model_evaluation.md):\n  `vignette(\"model_evaluation\", package=\"nichenetr\")`\n- [Parameter optimization via\n  NSGAII-R](vignettes/parameter_optimization.md):\n  `vignette(\"parameter_optimization\", package=\"nichenetr\")`\n\n## FAQ\n\nCheck the FAQ page at [FAQ NicheNet](vignettes/faq.md):\n`vignette(\"faq\", package=\"nichenetr\")`\n\n<details>\n<summary>\n<h2>\nPrevious updates\n</h2>\n</summary>\n\n**20-06-2023:**\n\n- MultiNicheNet - a multi-sample, multi-condition extension of\n  NicheNet - is now available on\n  [biorxiv](https://www.biorxiv.org/content/10.1101/2023.06.13.544751v1)\n  and [Github](https://github.com/saeyslab/multinichenetr).\n- MultiNicheNet uses an [updated prior model\n  (v2)](https://zenodo.org/record/7074291/) consisting of additional\n  ligand-receptor interactions from the [Omnipath\n  database](https://omnipathdb.org/) and from [Verschueren et\n  al.Â (2020)](https://www.sciencedirect.com/science/article/pii/S0092867420306942?via%3Dihub).\n  We have now also updated the vignettes of NicheNet to use the new\n  model instead.\n- **New functionality:** we have included additional functions to\n  prioritize ligands not only based on the ligand activity, but also on\n  the ligand and receptor expression, cell type specificity, and\n  condition specificity. This is similar to the criteria used in\n  Differential NicheNet and MultiNicheNet. See the [Prioritizing ligands\n  based on expression values](vignettes/seurat_steps_prioritization.md)\n  vignette for more information.\n- Due to this more generalizable prioritization scheme, we will no\n  longer provide support for Differential NicheNet.\n- We included code for making a ligand-receptor-target circos plot in\n  the [Circos plot visualization](vignettes/circos.md) vignette.\n\n<h5>\nDeprecated vignettes\n</h5>\n\nDifferential NicheNet has been deprecated: we will not longer provide\nsupport or code fixes on Differential NicheNet and its vignettes. You\nmay want to consider using the [general prioritization\nscheme](vignettes/seurat_steps_prioritization.md) instead.\n\n- [Differential NicheNet analysis between niches of\n  interest](vignettes/differential_nichenet.md):`vignette(\"differential_nichenet\", package=\"nichenetr\")`\n- [Differential NicheNet analysis between conditions of\n  interest](vignettes/differential_nichenet_pEMT.md):`vignette(\"differential_nichenet_pEMT\", package=\"nichenetr\")`\n\nIn NicheNet v2, the mouse and human ligand-target models are uploaded\nseparately so symbol conversion is not necessary. If you are still using\nthe NicheNet v1 model, you can check the following vignette on how to\nconvert the model (given in human symbols) to mouse symbols:\n\n- [Converting NicheNetâ€™s model from human to mouse\n  symbols](vignettes/symbol_conversion.md):\n  `vignette(\"symbol_conversion\", package=\"nichenetr\")`\n\n**12-01-2022:** In the Liver Atlas paper from Guilliams et al.: [Spatial\nproteogenomics reveals distinct and evolutionarily conserved hepatic\nmacrophage\nniches](https://www.sciencedirect.com/science/article/pii/S0092867421014811),\nwe used Differential NicheNet, an extension to the default NicheNet\nalgorithm. **Differential NicheNet** can be used to compare cell-cell\ninteractions between different niches and better predict niche-specific\nligand-receptor (L-R) pairs. It was used in that paper to predict\nligand-receptor pairs specific for the Kupffer cell niche in mouse and\nhuman.\n\nThe main difference between the classic NicheNet pipeline and the\nDifferential NicheNet pipeline is that Differential NicheNet also uses\nthe differential expression between the conditions/niches of the\nligand-receptor pairs for prioritization in addition to the ligand\nactivities. The classic NicheNet pipeline on the contrary uses only\nligand acivity for prioritization (and shows differential expression\nonly in visualizations).\n\nSo if you have data of multiple conditions or niches, and you want to\ninclude differential expression of the ligand-receptor pairs in the\nprioritization, we recommend you check out Differential NicheNet (update\nnichenetr to the 1.1.0 version). At the bottom of this page, you can\nfind the links to two vignettes illustrating a Differential NicheNet\nanalysis. We recommend these vignettes if you want to apply Differential\nNicheNet on your own data. If you want to see the code used for the\nanalyses used in the Guilliams et al.Â paper, see\n<https://github.com/saeyslab/NicheNet_LiverCellAtlas>.\n\n**15-10-2019:** Bonnardel, Tâ€™Jonck et al.Â used NicheNet to predict\nupstream niche signals driving Kupffer cell differentiation [Stellate\nCells, Hepatocytes, and Endothelial Cells Imprint the Kupffer Cell\nIdentity on Monocytes Colonizing the Liver Macrophage\nNiche](https://www.cell.com/immunity/fulltext/S1074-7613(19)30368-1).\n\n</details>\n\n## References\n\nBrowaeys, R., Saelens, W. & Saeys, Y. NicheNet: modeling intercellular\ncommunication by linking ligands to target genes. Nat Methods (2019)\n<doi:10.1038/s41592-019-0667-5>\n\nBonnardel et al.Â Stellate Cells, Hepatocytes, and Endothelial Cells\nImprint the Kupffer Cell Identity on Monocytes Colonizing the Liver\nMacrophage Niche. Immunity (2019) <doi:10.1016/j.immuni.2019.08.017>\n\nGuilliams et al.Â Spatial proteogenomics reveals distinct and\nevolutionarily conserved hepatic macrophage niches. Cell (2022)\n<doi:10.1016/j.cell.2021.12.018>\n",
      "stars_today": 0
    },
    {
      "id": 28114218,
      "name": "dada2",
      "full_name": "benjjneb/dada2",
      "description": "Accurate sample inference from amplicon data with single nucleotide resolution",
      "html_url": "https://github.com/benjjneb/dada2",
      "stars": 532,
      "forks": 162,
      "language": "R",
      "topics": [
        "amplicon",
        "bioconductor",
        "bioinformatics",
        "metabarcoding",
        "metagenomics",
        "microbiome",
        "taxonomy"
      ],
      "created_at": "2014-12-17T00:53:58Z",
      "updated_at": "2026-01-07T21:19:32Z",
      "pushed_at": "2025-12-15T19:22:15Z",
      "open_issues": 193,
      "owner": {
        "login": "benjjneb",
        "avatar_url": "https://avatars.githubusercontent.com/u/5797204?v=4"
      },
      "readme": "\n[![Build Status](https://app.travis-ci.com/benjjneb/dada2.svg?branch=master)](https://app.travis-ci.com/benjjneb/dada2)\n\n# dada2\n\nExact sample inference from high-throughput amplicon data. Resolves real variants differing by as little as one nucleotide. Visit [the DADA2 website](https://benjjneb.github.io/dada2/index.html) for the most detailed and up-to-date documentation.\n\n### Installation\n\nThe dada2 package binaries are available through Bioconductor:\n\n```S\n## try http:// if https:// URLs are not supported\nif (!requireNamespace(\"BiocManager\", quietly=TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"dada2\")\n```\n\nIn order to install dada2 from source (and get the latest and greatest new features) see our [installation from source instructions](https://benjjneb.github.io/dada2/dada-installation.html).\n\n### Documentation\n\nThe [tutorial walkthrough of the DADA2 pipeline on paired end Illumina Miseq data](https://benjjneb.github.io/dada2/tutorial.html). \n\nThe [dada2 R package manual](https://www.bioconductor.org/packages/3.6/bioc/manuals/dada2/man/dada2.pdf).\n\nFurther documentation is available on [the DADA2 front page](http://benjjneb.github.io/dada2/). \n\n### DADA2 Articles\n\n[DADA2: High resolution sample inference from Illumina amplicon data. Nature Methods, 2016.](http://dx.doi.org/10.1038/nmeth.3869) [(Open Access link.)](http://rdcu.be/ipGh)\n\n[Bioconductor workflow for microbiome data analysis: from raw reads to community analyses. F1000 Research, 2016.](https://f1000research.com/articles/5-1492)\n\n[Exact sequence variants should replace operational taxonomic units in marker-gene data analysis. ISMEJ, 2017.](http://dx.doi.org/10.1038/ismej.2017.119)\n\n[High-throughput amplicon sequencing of the full-length 16S rRNA gene with single-nucleotide resolution. Nucleic Acids Research, 2019.](http://dx.doi.org/10.1093/nar/gkz569)\n\n### Other Resources\n\nPlanned feature improvements are publicly catalogued at the main DADA2 development site on github, specifically on the \"Issues\" page for DADA2:\n\nhttps://github.com/benjjneb/dada2/issues\n\nIf the feature you are hoping for is not listed, you are welcome to add it as a feature request \"issue\" on this page. This request will be publicly available and listed on the page.\n\nBugs and difficulties in using DADA2 are also welcome on [the issue tracker](https://github.com/benjjneb/dada2/issues).\n",
      "stars_today": 0
    },
    {
      "id": 138660553,
      "name": "DoubletFinder",
      "full_name": "chris-mcginnis-ucsf/DoubletFinder",
      "description": "R package for detecting doublets in single-cell RNA sequencing data",
      "html_url": "https://github.com/chris-mcginnis-ucsf/DoubletFinder",
      "stars": 517,
      "forks": 123,
      "language": "R",
      "topics": [],
      "created_at": "2018-06-25T23:32:45Z",
      "updated_at": "2026-01-10T20:29:11Z",
      "pushed_at": "2025-03-21T11:20:52Z",
      "open_issues": 23,
      "owner": {
        "login": "chris-mcginnis-ucsf",
        "avatar_url": "https://avatars.githubusercontent.com/u/40582930?v=4"
      },
      "readme": "~~ Announcement (11/24/21) ~~\nI'm now a postdoc at Stanford and my UCSF email will be decommissioned soon. I also only check my github repos about once per month, so please reach out directly at cmcginni@stanford[dot]edu if you run into any issues. \n\n# DoubletFinder\n\nDoubletFinder is an R package that predicts doublets in single-cell RNA sequencing data. \n\nDoubletFinder is implemented to interface with Seurat >= 2.0 (https://satijalab.org/seurat/) \n\nDoubletFinder was published by Cell Systems in April, 2019: https://www.cell.com/cell-systems/fulltext/S2405-4712(19)30073-0\n\n## Updates\n\n(02/02/2025) Haibo Liu (Senior Bioinformatician at UMass, @haibol2016) added as maintainer after his much-needed improvement updates to the package.\n\n(11/21/2023) Made compatible with Seurat v5 and removed '_v3' flag from relevant function names.\n\n(03/31/2020) Internalized functions normally in 'modes' package to enable compatibility with R v3.6 and highger.\n\n(06/21/2019) Added parallelization to paramSweep_v3 (thanks NathanSkeen!) -- Note: progress no longer updated, but the process is much faster! Fixed bug with smaller datasets. Updated readme.\n\n(04/12/2019) Added SCTransform compatibilities to 'paramSweep_v3' and 'doubletFinder_v3'\n\n(04/08/2019) Added 'PCs' argument to 'doubletFinder', 'doubletFinder_v3', 'paramSweep', and 'paramSweep_v3' to avoid conflicts with dimension reduction preferences. Updated readme.\n\n(01/12/2019) Seurat V3 compatibility: 'doubletFinder_v3' and 'paramSweep_v3' functions added, other functions for parameter estimation remain compatible.  \n\n## DoubletFinder V2.0 (11/28/2018) \n\nNew Features:\n1. Increased computational efficiency during pANN computation\n2. Implemented strategy for determining optimal pK values for any scRNA-seq data using pN-pK parameter sweeps and mean-variance-normalized bimodality coefficient (BCmvn)\n3. Included vignette describing 'best-practices' for applying DoubletFinder to scRNA-seq data generated without sample multiplexing\n\n## Installation (in R/RStudio)\n\n```{r}\nremotes::install_github('chris-mcginnis-ucsf/DoubletFinder', force = TRUE)\n```\n\n## Dependencies\n\nDoubletFinder requires the following R packages: \n* Seurat (>= 2.0) \n* Matrix (1.2.14) \n* fields (9.6) \n* KernSmooth (2.23-15)\n* ROCR (1.0-7)\n* parallel (3.5.1)\n* NOTE: These package versions were used in the bioRxiv paper, but other versions may work, as well.\n\n## Frequently Asked Questions\n\nQuestion: What is my anticipated doublet rate? \nAnswer: This is dependent on your platform (10x, parse, etc.) and will vary with the number of input cells. It will not always be 7.5% as is used in the tutorial. This information is available in the user guides for each technology. See https://github.com/chris-mcginnis-ucsf/DoubletFinder/issues/76 and https://github.com/chris-mcginnis-ucsf/DoubletFinder/issues/156\n\nQuestion: Can I run DoubletFinder on merged data from multiple 10x lanes?\nAnswer: Technically yes but I would only do this if you were splitting the same sample across multiple lanes. You want to avoid instances where DoubletFinder is attempting to find doublets that do not actually exist in the data. I would also not advise running DF on integrated Seurat objects. See https://github.com/chris-mcginnis-ucsf/DoubletFinder/issues/101 \n\nQuestion: I see multiple potential pK values when visualizing BCmvn -- what should I do?\nAnswer: I would spot check the results in GEX space to see what makes the most sense given your understanding of the data. See https://github.com/chris-mcginnis-ucsf/DoubletFinder/issues/62 and https://github.com/chris-mcginnis-ucsf/DoubletFinder/issues/40\n\n# DoubletFinder Overview\n\nDoubletFinder can be broken up into 4 steps:\n\n(1) Generate artificial doublets from existing scRNA-seq data \n\n(2) Pre-process merged real-artificial data\n\n(3) Perform PCA and use the PC distance matrix to find each cell's proportion of artificial k nearest neighbors (pANN)\n\n(4) Rank order and threshold pANN values according to the expected number of doublets\n\n![alternativetext](DF.screenshots/DoubletFinderOverview.png)\n\nDoubletFinder takes the following arguments:\n\nseu ~ This is a fully-processed Seurat object (i.e., after NormalizeData, FindVariableGenes, ScaleData, RunPCA, and RunTSNE have all been run).\n\nPCs ~ The number of statistically-significant principal components, specified as a range (e.g., PCs = 1:10)\n\npN ~ This defines the number of generated artificial doublets, expressed as a proportion of the merged real-artificial data. Default is set to 25%, based on observation that DoubletFinder performance is largely pN-invariant (see McGinnis, Murrow and Gartner 2019, Cell Systems).\n\npK ~ This defines the PC neighborhood size used to compute pANN, expressed as a proportion of the merged real-artificial data. No default is set, as pK should be adjusted for each scRNA-seq dataset. Optimal pK values should be estimated using the strategy described below.\n\nnExp ~ This defines the pANN threshold used to make final doublet/singlet predictions. This value can best be estimated from cell loading densities into the 10X/Drop-Seq device, and adjusted according to the estimated proportion of homotypic doublets.\n\n## Application to Cell Hashing and Demuxlet data\n\nDoubletFinder successfully recapitulates ground-truth doublet classifications determined using antibody-barcode sample multiplexing (Cell Hashing) and SNP deconvolution (Demuxlet). DoubletFinder identifies false-negative Demuxlet classifications caused by doublets formed from cells with identical SNP profiles. DoubletFinder is insensitive to homotypic doublets -- i.e., doublets dervied from transcriptionally-similar cell states. \n\n![alternativetext](DF.screenshots/Results_Demux.png)\n![alternativetext](DF.screenshots/Results_Hashing.png)\n\n# 'Best-Practices' for scRNA-seq data generated without sample multiplexing\n\n## Input scRNA-seq Data\n\n* Do not apply DoubletFinder to aggregated scRNA-seq data representing multiple *distinct* samples (e.g., multiple 10X lanes). For example, if you run DoubletFinder on aggregated data representing WT and mutant cell lines sequenced across different 10X lanes, artificial doublets will be generated from WT and mutant cells, which cannot exist in your data. These artificial doublets will skew results. Notably, it is okay to run DoubletFinder on data generated by splitting a single sample across multiple 10X lanes. \n\n* Ensure that input data is cleared of low-quality cell clusters. There are a variety of ways to do this, but I usually use the following workflow:\n1. Manually threshold raw gene expression matrices according to RNA nUMIs (especially important when dealing with super-loaded 10X data because of the way CellRanger threholds data -- See Lun et al., 2019, Genome Biology.\n2. Pre-process data using standard workflow.\n3. Identify clusters with (A) low RNA UMIs, (B) High % mitochondrial reads, and/or (C) Uninformative marker genes.\n4. Remove clusters, pre-process again, and run DoubletFinder.\n\n## pK Selection\n\nROC analysis across pN-pK parameter sweeps for Cell Hashing and Demuxlet datasets demonstrate that DoubletFinder performance is largely invariant of pN value selection:\n\n![alternativetext](DF.screenshots/ParamSweep_Schematic.png)\n![alternativetext](DF.screenshots/ParamSweep_HeatMap.png)\n\nROC analysis across pN-pK parameter sweeps for simulated scRNA-seq data with (I) Variable numbers of cell states and (II) Variable magnitudes of transcriptional heterogeneity demonstrates that (I) Optimal pK value selection depends on the total number of cell states and (II) DoubletFinder performance suffers when applied to transcriptionally-homogenous data. Simulated data was generated using a strategy similar to as described in Wolock, Lopex & Klein 2019, Cell Systems.\n\n![alternativetext](DF.screenshots/Simulation_Schematic.png)\n![alternativetext](DF.screenshots/Results_Simulation.png)\n\nSimulated and sample-multiplexed data are unique in that ground-truth doublet classifications can be leveraged to characterize how DoubletFinder parameters must be 'fit' to distinct scRNA-seq datasets. However, doublets remain unknown in real-world contexts -- which is likely why you are interested in DoubletFinder, at all!\n\nTo maximize the accuracy of DoubletFinder predictions, we sought a ground-truth-agnostic metric that coincides with pK values that maximize AUC in Cell Hashing and Demuxlet data. Mean-variance normalized bimodality coefficient (BCmvn) achieves this goal, featuring a single, easily-discernible maximum at pK values that optimize AUC. \n\n![alternativetext](DF.screenshots/BCmvn.png)\n\nBCmvn distributions also feature a single maximum for scRNA-seq datasets generated without sample-multiplexing (e.g., Mouse pancreas, Byrnes et al., 2018, Nature Communcations; Mouse kidney, Park et al., 2018, Science), enabling pK selection.\n\n## Doublet Number Estimation\n\nDoubletFinder is sensitive to heterotypic doublets -- i.e., doublets formed from transcriptionally-distinct cell states -- but is insensitive to homotypic doublets -- i.e., doublets formed from transcriptionally-similar cell states. In our original manuscript, we suggested using DoubletFinder to predict the number of doublets expected from Poisson statistical estimates realting to the droplet microfluidics cell loading density. However, Poisson estimates are agnostic of homotypic doublets, and will thus invariably overestimate the number of *detectable* doublets.\n\nTo address this issue, we suggest users utilize literature-supported cell type annotations to model the proportion of homotypic doublets present in their data. As an example, we present an analysis of mouse kidney scRNA-seq data (Park et al., 2018, Science):\n\n![alternativetext](DF.screenshots/HomotypicAdjustment.png)\n\nNotably, it is conceivable that literature-suppoted cell type annotations may not accurately recapitulate the magnitude of transcriptional divergence necessary for DoubletFinder sensitivity. For example, nominally-homogenous cells (e.g., CD4+ T-cells) may exist along a spectrum of gene expression states (e.g., distinct anatomical locations, disease states, naive/Tregs/Th17 cells, etc.), and doublets formed by cell sub-types may be detectable by DoubletFinder. Thus, we consider doublet number estimates based on Poisson statistics with and without homotypic doublet proportion adjustment to 'bookend' the real detectable doublet rate. \n\n## Example code for 'real-world' applications\n\n```R\n## Pre-process Seurat object (standard) --------------------------------------------------------------------------------------\nseu_kidney <- CreateSeuratObject(kidney.data)\nseu_kidney <- NormalizeData(seu_kidney)\nseu_kidney <- FindVariableFeatures(seu_kidney, selection.method = \"vst\", nfeatures = 2000)\nseu_kidney <- ScaleData(seu_kidney)\nseu_kidney <- RunPCA(seu_kidney)\nseu_kidney <- RunUMAP(seu_kidney, dims = 1:10)\n\n## Pre-process Seurat object (sctransform) -----------------------------------------------------------------------------------\nseu_kidney <- CreateSeuratObject(kidney.data)\nseu_kidney <- SCTransform(seu_kidney)\nseu_kidney <- RunPCA(seu_kidney)\nseu_kidney <- RunUMAP(seu_kidney, dims = 1:10)\n\n## pK Identification (no ground-truth) ---------------------------------------------------------------------------------------\nsweep.res.list_kidney <- paramSweep(seu_kidney, PCs = 1:10, sct = FALSE)\nsweep.stats_kidney <- summarizeSweep(sweep.res.list_kidney, GT = FALSE)\nbcmvn_kidney <- find.pK(sweep.stats_kidney)\n\n## pK Identification (ground-truth) ------------------------------------------------------------------------------------------\nsweep.res.list_kidney <- paramSweep(seu_kidney, PCs = 1:10, sct = FALSE)\ngt.calls <- seu_kidney@meta.data[rownames(sweep.res.list_kidney[[1]]), \"GT\"].   ## GT is a vector containing \"Singlet\" and \"Doublet\" calls recorded using sample multiplexing classification and/or in silico geneotyping results \nsweep.stats_kidney <- summarizeSweep(sweep.res.list_kidney, GT = TRUE, GT.calls = gt.calls)\nbcmvn_kidney <- find.pK(sweep.stats_kidney)\n\n## Homotypic Doublet Proportion Estimate -------------------------------------------------------------------------------------\nhomotypic.prop <- modelHomotypic(annotations)           ## ex: annotations <- seu_kidney@meta.data$ClusteringResults\nnExp_poi <- round(0.075*nrow(seu_kidney@meta.data))  ## Assuming 7.5% doublet formation rate - tailor for your dataset\nnExp_poi.adj <- round(nExp_poi*(1-homotypic.prop))\n\n## Run DoubletFinder with varying classification stringencies ----------------------------------------------------------------\nseu_kidney <- doubletFinder(seu_kidney, PCs = 1:10, pN = 0.25, pK = 0.09, nExp = nExp_poi, reuse.pANN = NULL, sct = FALSE)\nseu_kidney <- doubletFinder(seu_kidney, PCs = 1:10, pN = 0.25, pK = 0.09, nExp = nExp_poi.adj, reuse.pANN = \"pANN_0.25_0.09_913\", sct = FALSE)\n```\n\n![alternativetext](DF.screenshots/DFkidney_low.vs.high.png)\n\n## Other Doublet Detection Methods\n[Scrublet (Py)](https://github.com/AllonKleinLab/scrublet)\n[DoubletDecon (R)](https://github.com/EDePasquale/DoubletDecon)\n[DoubletDetection (Py)](https://github.com/JonathanShor/DoubletDetection)\n[Solo (Py)](https://github.com/calico/solo)\n[scds (R)](https://github.com/kostkalab/scds)\n[scDblFinder (R)](https://github.com/plger/scDblFinder)\n\n## References\n\n1.\tStoeckius M, Zheng S, Houck-Loomis B, Hao S, Yeung BZ, Smibert P, Satija R. Cell Hashing with barcoded antibodies enables multiplexing and doublet detection for single cell genomics. Genome Biology. 2018. 19:224.\n\n2.  Kang HM, Subramaniam M, Targ S, Nguyen M, Maliskova L, McCarthy E, Wan E, Wong S, Byrnes L, Lanata CM, Gate RE, Mostafavi S, Marson A, Zaitlen N, Criswell LA, Ye JC. Multiplexed droplet single-cell RNA-sequencing using natural genetic variation. Nature Biotechnology. 2018. 36(1):89-94. \n\n3.  Wolock SL, Lopez R, Klein AM. Scrublet: Computational Identification of Cell Doublets in Single-Cell Transcriptomic Data. Cell Systems. 2019. 8(4):281-291.e9.\n\n4.  Park J, Shrestha R, Qiu C, Kondo A, Huang S, Werth M, Li M, Barasch J, SusztÃ¡k K. Single-cell transcriptomics of the mouse kidney reveals potential cellular targets of kidney disease. Science. 2018. 360(6390):758-63.\n\n5.  Byrnes LE, Wong DM, Subramaniam M, Meyer NP, Gilchrist CL, Knox SM, Tward AD, Ye CJ, Sneddon JB. Lineage dynamics of murine pancreatic development at single-cell resolution. Nature Communications. 2018; 9:3922.\n\n6.  Bais AS, Kostka D. scds: computational annotation of doublets in single-cell RNA sequencing data. Bioinformatics. 2020. 36(4):1150-8.\n\n7.  Bernstein NJ, Fong NL, Lam I, Roy MA, Hendrickson DG, Kelley DR. Solo: Doublet Identification in Single-Cell RNA-Seq via Semi-Supervised Deep Learning. Cell Systems. 2020. S2405-4712(20)30195-2.\n\n8.  DePasquale EAK, Schnell DJ, Van Camp PJ, Valiente-Alandi I, Blaxall BC, Grimes HL, Singh H, Salomonis N. DoubletDecon: Deconvoluting Doublets from Single-Cell RNA-Sequencing Data. Cell Reports. 2019. 29(6):1718-27.e8.\n",
      "stars_today": 0
    },
    {
      "id": 216123064,
      "name": "ArchR",
      "full_name": "GreenleafLab/ArchR",
      "description": "ArchR : Analysis of Regulatory Chromatin in R (www.ArchRProject.com)",
      "html_url": "https://github.com/GreenleafLab/ArchR",
      "stars": 442,
      "forks": 152,
      "language": "R",
      "topics": [],
      "created_at": "2019-10-18T23:35:41Z",
      "updated_at": "2026-01-06T15:44:55Z",
      "pushed_at": "2025-02-18T21:19:00Z",
      "open_issues": 177,
      "owner": {
        "login": "GreenleafLab",
        "avatar_url": "https://avatars.githubusercontent.com/u/8398169?v=4"
      },
      "readme": "<p align=\"center\"><a href =\"https://www.archrproject.com\"><img src=\"Figures/ArchR_Logo_Integrated.png\" alt=\"\" width=\"350\"></a></p>\n<hr>\n\n[![Lifecycle: maturing](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://www.tidyverse.org/lifecycle/#maturing)\n\n### ArchR has new features available for scATAC-seq Analysis\n\n**Paired scATAC-seq and scRNA-seq Analysis**\n\nArchR now supports paired scATAC-seq and scRNA-seq Analysis! <br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See updates with importFeatureMatrix, addGeneExpressionMatrix, addIterativeLSI, addCombinedDims <br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For a brief tutorial of these features : https://greenleaflab.github.io/ArchR_2020/Ex-Analyze-Multiome.html\n\n**Trajectory Analysis**\n\nArchR now directly supports both monocle3 and Slingshot based trajectory analysis! <br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See updates with getMonocleTrajectories, addMonocleTrajectory, addSlingShotTrajectories <br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For a brief tutorial of these features : https://greenleaflab.github.io/ArchR_2020/Ex-Analyze-Trajectory.html\n\nAdditionally ArchR now enables export of a peak matrix that is compatible with STREAM!<br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See updates with exportPeakMatrixForSTREAM <br />\n\n### ArchR is currently in Beta and will be in active development through the peer review process.\n\nArchR is a full-featured R package for processing and analyzing single-cell ATAC-seq data. ArchR provides the most extensive suite of scATAC-seq analysis tools of any software available. Additionally, ArchR excels in both speed and resource usage, making it possible to analyze 1 million cells in 8 hours on a MacBook Pro laptop.\n\n### For installation instructions and full documentation, visit www.ArchRProject.com.\n\n<hr>\n\n![](Figures/ArchR_Workflow_Horizontal.png)\n\n# Quick Installation of ArchR\nFor a full walk through of installation and frequently related issues please visit www.ArchRProject.com.\n\n**First, install devtools (for installing GitHub packages) if it isn't already installed:**\n``` r\nif (!requireNamespace(\"devtools\", quietly = TRUE)) install.packages(\"devtools\")\n```\n\n**Then, install BiocManager (for installing bioconductor packages) if it isn't already installed:**\n``` r\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\")\n```\n\n**Then, install ArchR:**\n``` r\ndevtools::install_github(\"GreenleafLab/ArchR\", ref=\"master\", repos = BiocManager::repositories())\n```\n\n**Lastly, install all of the ArchR dependencies that aren't installed by default:**\n``` r\nlibrary(ArchR)\nArchR::installExtraPackages()\n```\nIf any of these steps fails, you should identify the offending package and troubleshoot that individual installation before proceeding. Additionally, please see the ArchR website (www.ArchRProject.com) where we have installation troubleshooting tips.\n\n# Pre-compiled ArchR environment\nWe provide two methods in which a user can manage R dependencies.  \n\n### Using renv to manage dependencies\nThe first is by using renv to manage a project's dependencies. To utilize this, make sure that the renv package is installed and loaded.  Before you are ready to use `renv`, you must ensure that you are working on the same R version that we used for the provided renv environment. \nThe R versions we currently support are:\n```\n- R 4.4\n- R 4.1\n```\nSecondly, make sure that the renv package is installed and loaded.\n```\ninstall.packages(\"renv\")\nlibrary(renv)\n```\nSet a working directory for your project\n```\ndir.create(path = \"./<project_name>\", showWarnings = FALSE)\nsetwd(\"./<project_name>\")\n```\nThen, lets download the lock file for the current master branch of ArchR.\nFor R 4.4:\n```\ndownload.file(url = \"https://pub-9ae435458ecc412abbbc9420a502ec38.r2.dev/renv.lock\", destfile = \"./renv.lock\")\n```\nFor R 4.1:\n```\ndownload.file(url = \"https://pub-9ae435458ecc412abbbc9420a502ec38.r2.dev/renv_4_1.lock\", destfile = \"./renv.lock\")\n```\n\nNow, we can initiate our renv project environment, utilizing the renv.lock to bootstrap a new renv environment.\n```\nrenv::init()\n```\n\n### Using Docker to manage dependencies\nWe also provide Docker images, built off of `rocker/rstudio`, that already have ArchR and all dependencies pre-loaded.\n\nThe latest version can be found at:\n```\ngreenleaflab/archr:latest\n```\nand other versions, including images built with differing R versions, can be found at:\n```\nhttps://hub.docker.com/r/greenleaflab/archr/tags\n```\n\nTo utilize these images, the user can first install Docker as mentioned in their [documentation](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository)\n\nFollowing, create a container using the following command:\n```\ndocker image pull greenleaflab/archr:latest\ndocker run -it --rm -v <your_workspace>:/workspace -p <your_port_of_interest>:8787\n```\nThis will spin up a container that has Rstudio turned on by default. Rstudio can be accessed through:\n```\nlocalhost:<your_port_of_interest>\n```\nIf you would like an interactive bash console instead, the following command can instead be called:\n```\ndocker run -it --rm -v <your_workspace>:/workspace -p <your_port_of_interest>:8787 bash\n```\n\n# Issues using ArchR?\n\nArchR is currently in __beta__. We expect there to be bumps in the road. If you think you have found a bug, please first install the latest version of ArchR via\n``` r\ndevtools::install_github(\"GreenleafLab/ArchR\", ref=\"master\", repos = BiocManager::repositories())\n```\nIf this does not fix your problem, please [report an issue on Github](https://github.com/GreenleafLab/ArchR/issues) with the __Bug Report__ form.\n\nIf you have questions about ArchR usage, please refer to the [the searchable full user's manual](https://www.archrproject.com/bookdown/index.html), [the FAQ section](https://www.archrproject.com/articles/Articles/faq.html), and the [publication](https://greenleaf.stanford.edu/assets/pdf/). If you think the documentation on this website or in the function annotations is unclear, please [submit an issue on Github](https://github.com/GreenleafLab/ArchR/issues) with the __Documentation Request__ form. If there is a feature that you think is missing from ArchR _and you have already searched the user's manual_, [submit an issue on Github](https://github.com/GreenleafLab/ArchR/issues) with the __Feature Request__ form. If none of these options help, [send us an email](mailto:archr.devs@gmail.com). We will do our best to respond to questions that are not otherwise answered in the documentation.\n\n\n",
      "stars_today": 0
    },
    {
      "id": 53515475,
      "name": "sonic-swss",
      "full_name": "sonic-net/sonic-swss",
      "description": "SONiC Switch State Service (SwSS)",
      "html_url": "https://github.com/sonic-net/sonic-swss",
      "stars": 208,
      "forks": 662,
      "language": "C++",
      "topics": [
        "hacktoberfest",
        "sonic"
      ],
      "created_at": "2016-03-09T17:01:09Z",
      "updated_at": "2026-01-15T00:50:46Z",
      "pushed_at": "2026-01-15T00:50:42Z",
      "open_issues": 631,
      "owner": {
        "login": "sonic-net",
        "avatar_url": "https://avatars.githubusercontent.com/u/102750714?v=4"
      },
      "readme": "*static analysis:*\n\n[![Total alerts](https://img.shields.io/lgtm/alerts/g/sonic-net/sonic-swss.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/sonic-net/sonic-swss/alerts/)\n[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/sonic-net/sonic-swss.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/sonic-net/sonic-swss/context:python)\n[![Language grade: C/C++](https://img.shields.io/lgtm/grade/cpp/g/sonic-net/sonic-swss.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/sonic-net/sonic-swss/context:cpp)\n\n*sonic-swss builds:*\n\n[![master build](https://dev.azure.com/mssonic/build/_apis/build/status/Azure.sonic-swss?branchName=master&label=master)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=15&branchName=master)\n[![202205 build](https://dev.azure.com/mssonic/build/_apis/build/status/Azure.sonic-swss?branchName=202205&label=202205)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=15&branchName=202205)\n[![202111 build](https://dev.azure.com/mssonic/build/_apis/build/status/Azure.sonic-swss?branchName=202111&label=202111)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=15&branchName=202111)\n[![202106 build](https://dev.azure.com/mssonic/build/_apis/build/status/Azure.sonic-swss?branchName=202106&label=202106)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=15&branchName=202106)\n[![202012 build](https://dev.azure.com/mssonic/build/_apis/build/status/Azure.sonic-swss?branchName=202012&label=202012)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=15&branchName=202012)\n[![201911 build](https://dev.azure.com/mssonic/build/_apis/build/status/Azure.sonic-swss?branchName=201911&label=201911)](https://dev.azure.com/mssonic/build/_build/latest?definitionId=15&branchName=201911)\n\n\n# SONiC - SWitch State Service - SWSS\n\n## Description\nThe SWitch State Service (SWSS) is a collection of software that provides a database interface for communication with and state representation of network applications and network switch hardware.\n\n## Getting Started\n\n### Prerequisites\n\nInstall the following dependencies:\n```\nsudo apt install redis-server\nsudo apt install libhiredis0.14\nsudo apt install libzmq5 libzmq3-dev\nsudo apt install libboost-serialization1.74.0\nsudo apt install libboost1.71-dev\nsudo apt install libasan6\n```\n**Note:** If your are using Ubuntu 18.04, install `libhiredis0.13` instead.\n\nVisit the [official sonic-buildimage Azure pipeline for the VS platform](https://dev.azure.com/mssonic/build/_build?definitionId=142&view=branches) and choose the branch that matches the sonic-swss branch you are trying to build or install. Then select the latest successful build.\nFrom the Summary tab, access build artifacts.\n<img width=\"1048\" alt=\"image\" src=\"https://github.com/user-attachments/assets/faa6f08d-788b-4801-8439-3f31a52efaa1\">\nDownload the folder `sonic-buildimage.vs/target/debs/{your host machine's Debian code name}`. You can check the Debian code name of your machine by running `cat /etc/debian_version`.\n<img width=\"1022\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1ad750eb-252c-4913-b14f-91b5533a1295\">\nExtract the downloaded zip file using `unzip sonic-buildimage.vs.zip`. Then navigate to `sonic-buildimage.vs/target/debs/{Debian code name}/` and install the following Debian packages:\n```\nsudo dpkg -i libdashapi_1.0.0_amd64.deb libnl-3-200_3.5.0-1_amd64.deb libnl-3-dev_3.5.0-1_amd64.deb libnl-cli-3-200_3.5.0-1_amd64.deb libnl-cli-3-dev_3.5.0-1_amd64.deb libnl-genl-3-200_3.5.0-1_amd64.deb libnl-genl-3-dev_3.5.0-1_amd64.deb libnl-nf-3-200_3.5.0-1_amd64.deb libnl-nf-3-dev_3.5.0-1_amd64.deb libnl-route-3-200_3.5.0-1_amd64.deb libnl-route-3-dev_3.5.0-1_amd64.deb libprotobuf32_3.21.12-3_amd64.deb libsaimetadata_1.0.0_amd64.deb  libsaimetadata-dev_1.0.0_amd64.deb libsairedis_1.0.0_amd64.deb libsairedis-dev_1.0.0_amd64.deb libsaivs_1.0.0_amd64.deb libsaivs-dev_1.0.0_amd64.deb  libswsscommon_1.0.0_amd64.deb libswsscommon-dev_1.0.0_amd64.deb libteam5_1.31-1_amd64.deb libteamdctl0_1.31-1_amd64.deb libyang_1.0.73_amd64.deb libyang-dev_1.0.73_amd64.deb python3-swsscommon_1.0.0_amd64.deb\n```\n**Note:** You can also [build these packages yourself (for the VS platform)](https://github.com/sonic-net/sonic-buildimage/blob/master/README.md).\n\nNow, you can either directly install the SONiC SWSS package or you can build it from source and then install it. To install the SONiC SWSS package that is already in `sonic-buildimage.vs/target/debs/{Debian code name}/`, simply run the following command:\n```\nsudo dpkg -i swss_1.0.0_amd64.deb\n```\n\n#### Install from Source\n\nInstall build dependencies:\n```\nsudo apt install libtool\nsudo apt install autoconf automake\nsudo apt install dh-exec\nsudo apt install nlohmann-json3-dev\nsudo apt install libgmock-dev\n```\n\nClone the `sonic-swss` repository on your host machine: `git clone https://github.com/sonic-net/sonic-swss.git`.\n\nMake sure that SAI header files exist in `/usr/include/sai`. Since you have already installed `libsairedis-dev`, `libsaimetadata-dev`, and `libsaivs-dev`, this should already be the case. If you have compiled `libsairedis` yourself, make sure that the SAI header files are copied to `/usr/include/sai`.\n\nYou can compile and install from source using:\n```\n./autogen.sh\n./configure\nmake && sudo make install\n```\n**Note:** This will NOT run the mock tests located under `tests/mock_tests`.\n\nYou can also build a debian package using:\n```\n./autogen.sh\nfakeroot debian/rules binary\n```\n## Common issues\n\n#### Cannot find `libboost-serialization1.74.0`\n\nUnfortunately, `libboost-serialization1.74.0` is not officially supported on Ubuntu 20.04 (focal) even though it is supported on Debian 11 (bullseye). Therefore, you must build this package from source. You can use a script similar to [this one](https://github.com/ulikoehler/deb-buildscripts/blob/master/deb-boost.sh), but you only need to create a package for the Boost serialization library. You should also make sure that the generated package is named `libboost-serialization1.74.0`. After the package is created, you can install it by running `sudo dpkg -i libboost-serialization1.74.0_1.74.0_amd64.deb`.\n\n#### Dependency issue when installing `libzmq3-dev`\n\nIf you cannot install `libzmq3-dev` because of dependency issues, please check the version of `libkrb5` packages installed on your host machine:\n```\n    sudo dpkg -l | grep \"libkrb5\"\n```\nIf the version is not `1.17-6ubuntu4.7`, then you need to install the correct version:\n\n    sudo apt install libkrb5support0=1.17-6ubuntu4.7\n    sudo apt install libzmq3-dev\n\n**Warning:** This may remove many packages that are already installed on your system. Please take note of what is being removed.\n\n**Note:** Do NOT install `*krb5*` packages that are located in the `sonic-buildimage.vs` folder that you downloaded. These packages have a higher version and will cause dependency issues.\n\n#### Dependency issues when installing some package\n\nIf you run into dependency issues during the installation of a package, you can run `sudo apt -f install` to fix the issue. But note that if `apt` is unable to fix the dependency problem, it will attempt to remove the broken package(s).\n\n#### Too many open files\n\nIf you get a C++ exception with the description \"Too many open files\" during the mock tests, you should check the maximum number of open files that are permitted on your system:\n```\nulimit -a | grep \"open files\"\n```\nYou can increase it by executing this command: `ulimit -n 8192`. Feel free to change `8192`. This value worked fine for me.\n\n**Note:** This change is only valid for the current terminal session. If you want a persistent change, append `ulimit -n 8192` to `~/.bashrc`.\n\n## Need Help?\n\nFor general questions, setup help, or troubleshooting:\n- [sonicproject on Google Groups](https://groups.google.com/g/sonicproject)\n\nFor bug reports or feature requests, please open an Issue.\n\n## Contribution guide\n\nSee the [contributors guide](https://github.com/sonic-net/SONiC/wiki/Becoming-a-contributor) for information about how to contribute.\n\n### GitHub Workflow\n\nWe're following basic GitHub Flow. If you have no idea what we're talking about, check out [GitHub's official guide](https://guides.github.com/introduction/flow/). Note that merge is only performed by the repository maintainer.\n\nGuide for performing commits:\n\n* Isolate each commit to one component/bugfix/issue/feature\n* Use a standard commit message format:\n\n>     [component/folder touched]: Description intent of your changes\n>\n>     [List of changes]\n>\n> \t  Signed-off-by: Your Name your@email.com\n\nFor example:\n\n>     swss-common: Stabilize the ConsumerTable\n>\n>     * Fixing autoreconf\n>     * Fixing unit-tests by adding checkers and initialize the DB before start\n>     * Adding the ability to select from multiple channels\n>     * Health-Monitor - The idea of the patch is that if something went wrong with the notification channel,\n>       we will have the option to know about it (Query the LLEN table length).\n>\n>       Signed-off-by: user@dev.null\n\n\n* Each developer should fork this repository and [add the team as a Contributor](https://help.github.com/articles/adding-collaborators-to-a-personal-repository)\n* Push your changes to your private fork and do \"pull-request\" to this repository\n* Use a pull request to do code review\n* Use issues to keep track of what is going on\n\n",
      "stars_today": 0
    },
    {
      "id": 185880527,
      "name": "signac",
      "full_name": "stuart-lab/signac",
      "description": "R toolkit for the analysis of single-cell chromatin data",
      "html_url": "https://github.com/stuart-lab/signac",
      "stars": 400,
      "forks": 102,
      "language": "R",
      "topics": [
        "atac",
        "bioinformatics",
        "single-cell"
      ],
      "created_at": "2019-05-09T22:32:26Z",
      "updated_at": "2026-01-10T20:34:16Z",
      "pushed_at": "2025-12-11T05:17:49Z",
      "open_issues": 19,
      "owner": {
        "login": "stuart-lab",
        "avatar_url": "https://avatars.githubusercontent.com/u/102445397?v=4"
      },
      "readme": "# Signac <img align=\"right\" src=\"man/figures/logo.svg\" style=\"height:100px;\" />\n\n[![R-CMD-check](https://github.com/stuart-lab/signac/workflows/R-CMD-check/badge.svg)](https://github.com/stuart-lab/signac/actions)\n[![CRAN\nVersion](https://www.r-pkg.org/badges/version/Signac)](https://cran.r-project.org/package=Signac)\n[![CRAN\nDownloads](https://cranlogs.r-pkg.org/badges/Signac)](https://cran.r-project.org/package=Signac)\n\n## Overview\n\nSignac is a comprehensive R package for the analysis of single-cell\nchromatin data. Signac includes functions for quality control,\nnormalization, dimension reduction, clustering, differential activity,\nand more.\n\nDocumentation and tutorials can be found at\n<https://stuartlab.org/signac/>\n\n## Installation\n\nTo install the latest release of Signac from CRAN:\n\n``` r\nsetRepositories(ind=1:3) # needed to automatically install Bioconductor dependencies\ninstall.packages(\"Signac\")\n```\n\nTo release the latest develop version from GitHub:\n\n``` r\nif (!requireNamespace(\"remotes\", quietly = TRUE))\n    install.packages(\"remotes\")\nremotes::install_github(\"stuart-lab/signac\", ref = \"develop\")\n```\n\n## Release notes\n\nFor a changelog please see the [NEWS\nfile](https://github.com/stuart-lab/signac/blob/develop/NEWS.md), also\navailable on the [Signac\nwebsite](https://stuartlab.org/signac/news/index.html).\n\n## Contributing\n\nWe welcome contributions to the Signac package. Please see the\n[contribution guide](https://github.com/stuart-lab/signac/blob/develop/CONTRIBUTING.md)\nfor more information.\n\n## Getting help\n\nIf you encounter a bug or have a feature request, please open an\n[issue](https://github.com/stuart-lab/signac/issues).\n\nIf you would like to discuss questions related to single-cell analysis,\nyou can open a\n[discussion](https://github.com/stuart-lab/signac/discussions).\n\n## Citing Signac\n\nIf you use the Signac package in your work please cite [Stuart et\nal. 2021](https://doi.org/10.1038/s41592-021-01282-5)\n\n```\n@ARTICLE{signac,\n  title     = \"Single-cell chromatin state analysis with Signac\",\n  author    = \"Stuart, Tim and Srivastava, Avi and Madad, Shaista and Lareau,\n               Caleb A and Satija, Rahul\",\n  journal   = \"Nat. Methods\",\n  publisher = \"Nature Publishing Group\",\n  pages     = \"1--9\",\n  month     =  nov,\n  year      =  2021,\n  url       = \"https://www.nature.com/articles/s41592-021-01282-5\",\n  language  = \"en\"\n}\n```\n\n## Related packages\n\n-   [Seurat](https://github.com/satijalab/seurat)\n-   [SeuratObject](https://github.com/satijalab/seurat-object)\n-   [SeuratDisk](https://github.com/mojaveazure/seurat-disk)\n-   [SeuratData](https://github.com/satijalab/seurat-data)\n-   [SeuratWrappers](https://github.com/satijalab/seurat-wrappers)\n-   [Azimuth](https://github.com/satijalab/azimuth)\n",
      "stars_today": 0
    },
    {
      "id": 452908115,
      "name": "nflverse-data",
      "full_name": "nflverse/nflverse-data",
      "description": "Automated nflverse data repository",
      "html_url": "https://github.com/nflverse/nflverse-data",
      "stars": 324,
      "forks": 33,
      "language": "R",
      "topics": [],
      "created_at": "2022-01-28T02:01:18Z",
      "updated_at": "2026-01-11T19:19:29Z",
      "pushed_at": "2026-01-01T03:28:54Z",
      "open_issues": 5,
      "owner": {
        "login": "nflverse",
        "avatar_url": "https://avatars.githubusercontent.com/u/79467114?v=4"
      },
      "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# nflverse-data\n\n<!-- badges: start -->\n<!-- badges: end -->\n\nThis repository holds automated data releases for nflverse projects\n(i.e.Â all of the data powered/scraped via GitHub Actions).\n\n## Usage\n\nYou can download data hosted here with the `{nflreadr}` package, or\nmanually download and access the\n[releases](https://github.com/nflverse/nflverse-data/releases) page.\nReleases are roughly organized along the [main\nfunctions](https://nflreadr.nflverse.com/reference/) of nflreadr.\n\n## Automation Status\n\nPlease see https://nflreadr.nflverse.com/articles/nflverse_data_schedule.html#automation-status for the status table. \n",
      "stars_today": 0
    }
  ],
  "created_at": "2026-01-15T01:04:14.934743667Z"
}