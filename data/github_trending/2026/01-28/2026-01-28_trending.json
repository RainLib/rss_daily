{
  "date": "2026-01-28",
  "name": "trending",
  "repositories": [
    {
      "id": 274495425,
      "name": "remotion",
      "full_name": "remotion-dev/remotion",
      "description": "ğŸ¥      Make videos programmatically with React",
      "html_url": "https://github.com/remotion-dev/remotion",
      "stars": 32894,
      "forks": 1968,
      "language": "TypeScript",
      "topics": [
        "javascript",
        "react",
        "video"
      ],
      "created_at": "2020-06-23T19:49:10Z",
      "updated_at": "2026-01-28T02:13:09Z",
      "pushed_at": "2026-01-28T00:53:51Z",
      "open_issues": 95,
      "owner": {
        "login": "remotion-dev",
        "avatar_url": "https://avatars.githubusercontent.com/u/85344006?v=4"
      },
      "readme": "<p align=\"center\">\n  <a href=\"https://github.com/remotion-dev/logo\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/remotion-dev/logo/raw/main/animated-logo-banner-dark.apng\">\n      <img alt=\"Animated Remotion Logo\" src=\"https://github.com/remotion-dev/logo/raw/main/animated-logo-banner-light.gif\">\n    </picture>\n  </a>\n</p>\n\n[![Discord Shield](https://img.shields.io/discord/809501355504959528?color=000000&label=Discord&logo=fdgssdf)](https://remotion.dev/discord)\n[![NPM Version](https://img.shields.io/npm/v/remotion.svg?style=flat&color=black)](https://www.npmjs.org/package/remotion)\n[![NPM Downloads](https://img.shields.io/npm/dm/remotion.svg?style=flat&color=black&label=Downloads)](https://npmcharts.com/compare/remotion?minimal=true)\n[![Open Bounties](https://img.shields.io/endpoint?url=https%3A%2F%2Fconsole.algora.io%2Fapi%2Fshields%2Fremotion%2Fbounties%3Fstatus%3Dopen&style=flat&color=black&labelColor=grey&label=Open+Bounties)](https://github.com/remotion-dev/remotion/issues?q=is%3Aopen+label%3A%22%F0%9F%92%8E+Bounty%22+sort%3Aupdated-desc)\n<a href=\"https://twitter.com/remotion\"><img src=\"https://img.shields.io/twitter/follow/remotion?label=Twitter&color=black\" alt=\"Twitter\"></a>\n\nRemotion is a framework for **creating videos programmatically using React.**\n\n## Why create videos in React?\n\n- **Leverage web technologies**: Use all of CSS, Canvas, SVG, WebGL, etc.\n- **Leverage programming**: Use variables, functions, APIs, math and algorithms to create new effects\n- **Leverage React**: Reusable components, Powerful composition, Fast Refresh, Package ecosystem\n\n## Created with Remotion\n\n<table>\n<tr>\n<td align=\"center\">\n<img style=\"width: 290px\" src=\"https://pub-646d808d9cb240cea53bedc76dd3cd0c.r2.dev/fireship-quick.gif\" />\n<p>\"This video was made with code\" <em>- Fireship</em> <a href=\"https://youtu.be/deg8bOoziaE\">Watch</a> â€¢ <a href=\"https://github.com/wcandillon/remotion-fireship\">Source</a></p>\n</td>\n<td align=\"center\">\n<img style=\"width: 240px\" src=\"https://pub-646d808d9cb240cea53bedc76dd3cd0c.r2.dev/unwrapped-2023.gif\" />\n<p>GitHub Unwrapped - Personalized Year in Review <a href=\"https://www.githubunwrapped.com\">Try</a> â€¢ <a href=\"https://github.com/remotion-dev/github-unwrapped\">Source</a></p>\n</td>\n<td align=\"center\">\n<em>View more in the <a href=\"https://remotion.dev/showcase\">Remotion Showcase</a>!</em>\n</td>\n</tr>\n</table>\n\n## Get started\n\nIf you already have Node.JS installed, type\n\n```console\nnpx create-video@latest\n```\n\nto get started. Otherwise, read the [installation page](https://www.remotion.dev/docs/) in the documentation.\n\n## Documentation\n\nDocumentation: [**remotion.dev/docs**](https://www.remotion.dev/docs)  \nAPI Reference: [**remotion.dev/api**](https://www.remotion.dev/api)\n\n## License\n\nBe aware of that Remotion has a special license and requires obtaining a company license in some cases. Read the [LICENSE](LICENSE.md) page for more information.\n\n## Contributing\n\nPlease read [CONTRIBUTING.md](CONTRIBUTING.md) to learn about contributing to this project.\n",
      "stars_today": 1204
    },
    {
      "id": 764287615,
      "name": "supermemory",
      "full_name": "supermemoryai/supermemory",
      "description": "Memory engine and app that is extremely fast, scalable. The Memory API for the AI era.",
      "html_url": "https://github.com/supermemoryai/supermemory",
      "stars": 15596,
      "forks": 1589,
      "language": "TypeScript",
      "topics": [
        "cloudflare-kv",
        "cloudflare-pages",
        "cloudflare-workers",
        "drizzle-orm",
        "postgres",
        "remix",
        "tailwindcss",
        "typescript",
        "vite"
      ],
      "created_at": "2024-02-27T20:10:04Z",
      "updated_at": "2026-01-28T02:11:13Z",
      "pushed_at": "2026-01-27T22:20:10Z",
      "open_issues": 14,
      "owner": {
        "login": "supermemoryai",
        "avatar_url": "https://avatars.githubusercontent.com/u/171979587?v=4"
      },
      "readme": "<p align=\"center\" style=\"padding-bottom:20px;padding-top:20px\">\n  <picture>\n    <source srcset=\"apps/web/public/logo-fullmark.svg\" media=\"(prefers-color-scheme: dark)\">\n    <source srcset=\"apps/web/public/logo-light-fullmark.svg\" media=\"(prefers-color-scheme: light)\">\n    <img src=\"apps/web/public/logo-fullmark.svg\" alt=\"supermemory Logo\" width=\"400\" />\n  </picture>\n  <br/><br/>\n  <em>Your AI second brain for saving and organizing everything that matters.</em>\n  <br/><br/>\n  <a href=\"https://app.supermemory.ai\" style=\"text-decoration: none;\">\n    <img src=\"https://img.shields.io/badge/Web-App-000000?style=for-the-badge\" alt=\"Web App\" />\n  </a>\n  &nbsp;\n  <a href=\"https://chromewebstore.google.com/detail/supermemory/afpgkkipfdpeaflnpoaffkcankadgjfc\" style=\"text-decoration: none;\">\n    <img src=\"https://img.shields.io/badge/Chrome-Extension-4285F4?style=for-the-badge&logo=googlechrome&logoColor=white\" alt=\"Chrome Extension\" />\n  </a>\n  &nbsp;\n  <a href=\"https://www.raycast.com/supermemory/supermemory\" style=\"text-decoration: none;\">\n    <img src=\"https://img.shields.io/badge/Raycast-Extension-FF6363?style=for-the-badge&logo=raycast&logoColor=white\" alt=\"Raycast Extension\" />\n  </a>\n  &nbsp;\n  <a href=\"https://supermemory.link/discord\" style=\"text-decoration: none;\">\n    <img src=\"https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white\" alt=\"Discord\" />\n  </a>\n</p>\n\n<p style=\"font-size: 0.9em; color: #666;\">\n    <strong>Building with Supermemory?</strong> Check out the <a href=\"https://console.supermemory.ai?utm_source=github&utm_medium=readme&utm_campaign=consumer_app\">Developer Console</a> and <a href=\"https://docs.supermemory.ai?utm_source=github&utm_medium=readme&utm_campaign=consumer_app\">Documentation</a> for API access.\n</p>\n<p style=\"font-size: 0.9em; color: #666;\">\n    <strong>Want to self-host?</strong> See our <a href=\"https://supermemory.ai/docs/deployment/self-hosting#self-hosting\">Self-Hosting Guide</a> for enterprise deployment options.\n</p>\n<br/>\n\n<div align=\"center\" style=\"padding-bottom:10px;padding-top:10px\">\n  <img src=\"apps/web/public/landing-page.jpeg\" alt=\"supermemory\" width=\"100%\" />\n</div>\n\n## Features\n\n### Core Functionality\n\n- **[Add Memories from Any Content](#add-memory)**: Easily add memories from URLs, PDFs, and plain textâ€”just paste, upload, or link.\n- **[Chat with Your Memories](#chat-memories)**: Converse with your stored content using natural language chat.\n- **[Supermemory MCP Integration](#mcp-integration)**: Seamlessly connect with all major AI tools (Claude, Cursor, etc.) via Supermemory MCP.\n- **[Browser Extension](#browser-extension)**: Save memories directly from your browser with integrations for ChatGPT, Claude, and Twitter/X.\n- **[Raycast Extension](#raycast-extension)**: Add and search memories directly from Raycast with keyboard shortcuts.\n\n## How do I use this?\n\nGo to [app.supermemory.ai](https://app.supermemory.ai) and sign in with your account\n\n1. <a id=\"add-memory\"></a>Start Adding Memory with your choice of format (Note, Link, File)\n<div align=\"center\" style=\"padding-bottom:10px;padding-top:10px\">\n  <img src=\"apps/web/public/add-memory.png\" alt=\"supermemory\" width=\"100%\" />\n</div>\n\n2. You can also Connect to your favourite services (Notion, Google Drive, OneDrive)\n<div align=\"center\" style=\"padding-bottom:10px;padding-top:10px\">\n  <img src=\"apps/web/public/add-connections.png\" alt=\"supermemory\" width=\"100%\" />\n</div>\n\n3. <a id=\"chat-memories\"></a>Once Memories are added, you can chat with Supermemory by clicking on \"Open Chat\" and retrieve info from your saved memories\n<div align=\"center\" style=\"padding-bottom:10px;padding-top:10px\">\n  <img src=\"apps/web/public/chat.png\" alt=\"supermemory\" width=\"100%\" />\n</div>\n\n4. <a id=\"mcp-integration\"></a>Add MCP to your AI Tools (by clicking on \"Connect to your AI\" and select the AI tool you are trying to integrate)\n<div align=\"center\" style=\"padding-bottom:10px;padding-top:10px\">\n  <img src=\"apps/web/public/mcp.png\" alt=\"supermemory\" width=\"100%\" />\n</div>\n\n5. <a id=\"browser-extension\"></a>**Browser Extension**: Install the [Chrome/Edge extension](https://chromewebstore.google.com/detail/supermemory/afpgkkipfdpeaflnpoaffkcankadgjfc) to save memories directly from any webpage, integrate with ChatGPT and Claude conversations, and import from Twitter/X. Right-click on any content or use the extension popup to save memories instantly.\n\n6. <a id=\"raycast-extension\"></a>**Raycast Extension**: Install the [Raycast extension](https://www.raycast.com/supermemory/supermemory) to add and search memories directly from Raycast. Use the \"Add Memory\" command to quickly save content, or \"Search Memories\" to find and retrieve your saved information with keyboard shortcuts.\n\n## Support\n\nHave questions or feedback? We're here to help:\n\n- Email: [support@supermemory.ai](mailto:support@supermemory.ai)\n- Discord: [Join our Discord server](https://supermemory.link/discord)\n- Documentation: [docs.supermemory.ai](https://docs.supermemory.ai)\n\n## Contributing\n\nWe welcome contributions from developers of all skill levels! Whether you're fixing bugs, adding features, or improving documentation, your help makes supermemory better for everyone.\n\nFor detailed guidelines, development setup, coding standards, and the complete contribution workflow, please see our [**Contributing Guide**](CONTRIBUTING.md).\n\n### Ways to Contribute\n\n- ğŸ› **Bug fixes** - Help us squash those pesky issues\n- âœ¨ **New features** - Add functionality that users will love\n- ğŸ¨ **UI/UX improvements** - Make the interface more intuitive\n- âš¡ **Performance optimizations** - Help us make supermemory faster\n\nCheck out our [Issues](https://github.com/supermemoryai/supermemory/issues) page for `good first issue` and `help wanted` labels to get started!\n\n## Updates & Roadmap\n\nStay up to date with the latest improvements:\n\n- [Changelog](https://docs.supermemory.ai/changelog/overview)\n- [X](https://x.com/supermemory).\n",
      "stars_today": 435
    },
    {
      "id": 122758193,
      "name": "video2x",
      "full_name": "k4yt3x/video2x",
      "description": "A machine learning-based video super resolution and frame interpolation framework. Est. Hack the Valley II, 2018.",
      "html_url": "https://github.com/k4yt3x/video2x",
      "stars": 18294,
      "forks": 1616,
      "language": "C++",
      "topics": [
        "anime4k",
        "frame-interpolation",
        "machine-learning",
        "neural-networks",
        "realcugan",
        "realesrgan",
        "rife",
        "super-resoluion",
        "upscale-video",
        "vulkan"
      ],
      "created_at": "2018-02-24T16:34:41Z",
      "updated_at": "2026-01-28T02:13:01Z",
      "pushed_at": "2026-01-27T08:04:06Z",
      "open_issues": 85,
      "owner": {
        "login": "k4yt3x",
        "avatar_url": "https://avatars.githubusercontent.com/u/21986859?v=4"
      },
      "readme": "<p align=\"center\">\n   <img src=\"https://github.com/user-attachments/assets/5cd63373-e806-474f-94ec-6e04963bf90f\"\n        alt=\"Video2X: A machine learning-based video super resolution and frame interpolation framework.\"/>\n   </br>\n   <img src=\"https://img.shields.io/github/v/release/k4yt3x/video2x?style=flat-square\"/>\n   <img src=\"https://img.shields.io/github/downloads/k4yt3x/video2x/total?style=flat-square\"/>\n   <img src=\"https://img.shields.io/github/license/k4yt3x/video2x?style=flat-square\"/>\n   <img src=\"https://img.shields.io/github/sponsors/k4yt3x?style=flat-square&link=https%3A%2F%2Fgithub.com%2Fsponsors%2Fk4yt3x\"/>\n   <img src=\"https://img.shields.io/badge/dynamic/json?color=%23e85b46&label=Patreon&query=data.attributes.patron_count&suffix=%20patrons&url=https%3A%2F%2Fwww.patreon.com%2Fapi%2Fcampaigns%2F4507807&style=flat-square\"/>\n</p>\n\n## ğŸŒŸ Version 6.0.0\n\nVideo2X 6.0.0 highlights:\n\n- Complete rewrite of the Video2X project in C/C++.\n- Faster and more efficient architecture.\n- Cross-platform support for Windows and Linux.\n- Vastly improved output quality.\n- New GUI and installer for easy setup on Windows.\n\n<details>\n<summary>Click to see more details</summary>\n\nVersion 6.0.0 is a complete rewrite of this project in C/C++. It:\n\n- genuinely works this time, with much less hassle compared to the 5.0.0 beta;\n- is blazing fast, thanks to the new optimized pipeline and the efficiency of C/C++;\n- is cross-platform, available now for both Windows and Linux;\n- offers significantly better output quality with Anime4K v4, Real-ESRGAN, Real-CUGAN, and RIFE;\n- supports two modes: filtering (upscaling) and frame interpolation;\n- supports Anime4K v4 and all custom MPV-compatible GLSL shaders;\n- supports Real-ESRGAN, Real-CUGAN, and RIFE (all models) via ncnn and Vulkan;\n- requires zero additional disk space during processing, just space for the final output.\n\n</details>\n\n![6.4.0-screenshot](https://github.com/user-attachments/assets/9b1cc8a7-2903-4d2c-80a2-8d81f007e45b)\n\n## ğŸ–¥ï¸ Hardware Requirements\n\nYour system must meet the minimum hardware requirements below to run Video2X.\n\n- **CPU**\n  - The precompiled binaries require CPUs with AVX2 support.\n  - **Intel**: Haswell (Q2 2013) or newer\n  - **AMD**: Excavator (Q2 2015) or newer\n- **GPU**\n  - The GPU must support Vulkan.\n  - **NVIDIA**: Kepler (GTX 600 series, Q2 2012) or newer\n  - **AMD**: GCN 1.0 (Radeon HD 7000 series, Q1 2012) or newer\n  - **Intel**: HD Graphics 4000 (Q2 2012) or newer\n\n## [ğŸªŸ Install on Windows](https://docs.video2x.org/installing/windows-qt6.html)\n\n**[Download the Latest Windows Installer Executable (6.4.0)](https://github.com/k4yt3x/video2x/releases/download/6.4.0/video2x-qt6-windows-amd64-installer.exe)**\n\nYou can download the latest Windows release on the [releases page](https://github.com/k4yt3x/video2x/releases/latest). For basic GUI usage, refer to the [documentation](https://docs.video2x.org/running/desktop.html). If you're unable to download directly from GitHub, try the [mirror site](https://files.k4yt3x.com). The GUI currently supports the following languages:\n\n- English (United States)\n- ç®€ä½“ä¸­æ–‡ï¼ˆä¸­å›½ï¼‰\n- æ—¥æœ¬èªï¼ˆæ—¥æœ¬ï¼‰\n- PortuguÃªs (Portugal)\n- FranÃ§ais (France)\n- Deutsch (Deutschland)\n\n## [ğŸ§ Install on Linux](https://docs.video2x.org/installing/linux.html)\n\nVideo2X packages are available for the Linux distros listed below. A universal AppImage is also available for other distros. If you'd like to build it from source code, refer to the [PKGBUILD](packaging/arch/PKGBUILD) file for a general overview of the required dependencies and commands.\n\n- Arch Linux: AUR packages, maintained by [@K4YT3X](https://github.com/k4yt3x).\n  - [aur/video2x](https://aur.archlinux.org/packages/video2x)\n  - [aur/video2x-git](https://aur.archlinux.org/packages/video2x-git)\n  - [aur/video2x-qt6](https://aur.archlinux.org/packages/video2x-qt6)\n  - [aur/video2x-qt6-git](https://aur.archlinux.org/packages/video2x-qt6-git)\n- Arch Linux (Chinese Mainland): archlinuxcn packages, maintained by [@Integral-Tech](https://github.com/Integral-Tech).\n  - [archlinuxcn/video2x](https://github.com/archlinuxcn/repo/tree/master/archlinuxcn/video2x)\n  - [archlinuxcn/video2x-git](https://github.com/archlinuxcn/repo/tree/master/archlinuxcn/video2x-git)\n  - [archlinuxcn/video2x-qt6](https://github.com/archlinuxcn/repo/tree/master/archlinuxcn/video2x-qt6)\n  - [archlinuxcn/video2x-qt6-git](https://github.com/archlinuxcn/repo/tree/master/archlinuxcn/video2x-qt6-git)\n- Other distros: `Video2X-x86_64.AppImage` on the [releases page](https://github.com/k4yt3x/video2x/releases/latest).\n\n## [ğŸ“¦ Container Image](https://docs.video2x.org/running/container.html)\n\nVideo2X [container images](https://github.com/k4yt3x/video2x/pkgs/container/video2x) are available on the GitHub Container Registry for easy deployment on Linux and macOS. If you already have Docker/Podman installed, only one command is needed to start upscaling a video. For more information on how to use Video2X's Docker image, please refer to the [documentation](https://docs.video2x.org/running/container.html).\n\n## [ğŸ“” Google Colab](https://colab.research.google.com/drive/1gWEwcA9y57EsxwOjmLNmNMXPsafw0kGo)\n\nYou can use Video2X on [Google Colab](https://colab.research.google.com/) **for free** if you don't have a powerful GPU of your own. You can borrow a powerful GPU (NVIDIA T4, L4, or A100) on Google's server for free for a maximum of 12 hours per session. **Please use the free resource fairly** and do not create sessions back-to-back and run upscaling 24/7. This might result in you getting banned. You can get [Colab Pro/Pro+](https://colab.research.google.com/signup/pricing) if you'd like to use better GPUs and get longer runtimes. Usage instructions are embedded in the [Colab Notebook](https://colab.research.google.com/drive/1gWEwcA9y57EsxwOjmLNmNMXPsafw0kGo).\n\n## [ğŸ’¬ Telegram Discussion Group](https://t.me/video2x)\n\nJoin our Telegram discussion group to ask any questions you have about Video2X, chat directly with the developers, or discuss super resolution, frame interpolation technologies, or the future of Video2X in general.\n\n## [ğŸ“– Documentation](https://docs.video2x.org/)\n\nComprehensive documentation for Video2X is available at [https://docs.video2x.org/](https://docs.video2x.org/). It offers detailed instructions on how to [build](https://docs.video2x.org/building/index.html), [install](https://docs.video2x.org/installing/index.html), [use](https://docs.video2x.org/running/index.html), and [develop](https://docs.video2x.org/developing/index.html) with this program.\n\n## ğŸ“½ï¸ Video Demos (Outdated)\n\n![Spirited Away Demo](https://user-images.githubusercontent.com/21986859/49412428-65083280-f73a-11e8-8237-bb34158a545e.png)\\\n_Upscale demo: Spirited Away's movie trailer_\n\n- **Spirited Away**: [YouTube](https://youtu.be/mGEfasQl2Zo) | [Bilibili](https://www.bilibili.com/video/BV1V5411471i/)\n  - 360P to 4K\n  - The [original video](https://www.youtube.com/watch?v=ByXuk9QqQkk)'s copyright belongs to æ ªå¼ä¼šç¤¾ã‚¹ã‚¿ã‚¸ã‚ªã‚¸ãƒ–ãƒª\n- **Bad Apple!!**: [YouTube](https://youtu.be/A81rW_FI3cw) | [Bilibili](https://www.bilibili.com/video/BV16K411K7ue)\n  - 384P 30 FPS to 4K 120 FPS with waifu2x and DAIN\n  - The [original video](https://www.nicovideo.jp/watch/sm8628149)'s copyright belongs to ã‚ã«ã‚‰\n- **The Pet Girl of Sakurasou**: [YouTube](https://youtu.be/M0vDI1HH2_Y) | [Bilibili](https://www.bilibili.com/video/BV14k4y167KP/)\n  - 240P 29.97 to 1080P 60 FPS with waifu2x and DAIN\n  - The original video's copyright belongs to ASCII Media Works\n\n### Standard Test Clip\n\nThe following clip can be used to test if your setup works properly. This is also the standard clip used for running performance benchmarks.\n\n- [Standard Test Clip (240P)](https://files.k4yt3x.com/resources/videos/standard-test.mp4) 4.54 MiB\n- [Real-CUGAN Upscaled Sample (1704P)](https://files.k4yt3x.com/resources/videos/standard-realcugan.mp4) 3.5 MiB\n- [Real-ESRGAN Upscaled Sample (1704P)](https://files.k4yt3x.com/resources/videos/standard-realesrgan.mp4) 3.1 MiB\n- [waifu2x Upscaled Sample (1080P)](https://files.k4yt3x.com/resources/videos/standard-waifu2x.mp4) 4.54 MiB\n- [Ground Truth (1080P)](https://files.k4yt3x.com/resources/videos/standard-original.mp4) 22.2 MiB\n\nThe original clip came from the anime \"ã•ãã‚‰è˜ã®ãƒšãƒƒãƒˆãªå½¼å¥³.\"\\\nCopyright of this clip belongs to æ ªå¼ä¼šç¤¾ã‚¢ãƒ‹ãƒ—ãƒ¬ãƒƒã‚¯ã‚¹.\n\n## âš–ï¸ License\n\nThis project is licensed under [GNU AGPL version 3](https://www.gnu.org/licenses/agpl-3.0.txt).\\\nCopyright (C) 2018-2025 K4YT3X and [contributors](https://github.com/k4yt3x/video2x/graphs/contributors).\n\n![AGPLv3](https://www.gnu.org/graphics/agplv3-155x51.png)\n\nThis project includes or depends on these following projects:\n\n| Project                                                                               | License         |\n| ------------------------------------------------------------------------------------- | --------------- |\n| [FFmpeg/FFmpeg](https://www.ffmpeg.org/)                                              | LGPLv2.1, GPLv2 |\n| [Tencent/ncnn](https://github.com/Tencent/ncnn)                                       | BSD 3-Clause    |\n| [bloc97/Anime4K](https://github.com/bloc97/Anime4K)                                   | MIT License     |\n| [nihui/realcugan-ncnn-vulkan](https://github.com/nihui/realcugan-ncnn-vulkan)         | MIT License     |\n| [nihui/rife-ncnn-vulkan](https://github.com/nihui/rife-ncnn-vulkan)                   | MIT License     |\n| [xinntao/Real-ESRGAN-ncnn-vulkan](https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan) | MIT License     |\n\nMore licensing information can be found in the [NOTICE](NOTICE) file.\n\n## ğŸŒº Special Thanks\n\nSpecial thanks to the following individuals for their significant contributions to the project, listed in alphabetical order.\n\n- [@ArchieMeng](https://github.com/archiemeng)\n- [@BrianPetkovsek](https://github.com/BrianPetkovsek)\n- [@Integral-Tech](https://github.com/Integral-Tech)\n- [@ddouglas87](https://github.com/ddouglas87)\n- [@lhanjian](https://github.com/lhanjian)\n- [@nihui](https://github.com/nihui)\n- [@sat3ll](https://github.com/sat3ll)\n",
      "stars_today": 354
    },
    {
      "id": 1031912724,
      "name": "cc-switch",
      "full_name": "farion1231/cc-switch",
      "description": "A cross-platform desktop All-in-One assistant tool for Claude Code, Codex, OpenCode & Gemini CLI.",
      "html_url": "https://github.com/farion1231/cc-switch",
      "stars": 14254,
      "forks": 901,
      "language": "Rust",
      "topics": [
        "ai-tools",
        "claude-code",
        "codex",
        "desktop-app",
        "kimi-k2-thiking",
        "mcp",
        "minimax",
        "open-source",
        "opencode",
        "provider-management",
        "rust",
        "skills",
        "skills-management",
        "tauri",
        "typescript",
        "wsl-support"
      ],
      "created_at": "2025-08-04T14:16:16Z",
      "updated_at": "2026-01-28T02:11:54Z",
      "pushed_at": "2026-01-27T15:39:55Z",
      "open_issues": 136,
      "owner": {
        "login": "farion1231",
        "avatar_url": "https://avatars.githubusercontent.com/u/44939412?v=4"
      },
      "readme": "<div align=\"center\">\n\n# All-in-One Assistant for Claude Code, Codex & Gemini CLI\n\n[![Version](https://img.shields.io/badge/version-3.10.2-blue.svg)](https://github.com/farion1231/cc-switch/releases)\n[![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20macOS%20%7C%20Linux-lightgrey.svg)](https://github.com/farion1231/cc-switch/releases)\n[![Built with Tauri](https://img.shields.io/badge/built%20with-Tauri%202-orange.svg)](https://tauri.app/)\n[![Downloads](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/farion1231/cc-switch/total)](https://github.com/farion1231/cc-switch/releases/latest)\n\n<a href=\"https://trendshift.io/repositories/15372\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/15372\" alt=\"farion1231%2Fcc-switch | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\nEnglish | [ä¸­æ–‡](README_ZH.md) | [æ—¥æœ¬èª](README_JA.md) | [Changelog](CHANGELOG.md)\n\n</div>\n\n## â¤ï¸Sponsor\n\n[![Zhipu GLM](assets/partners/banners/glm-en.jpg)](https://z.ai/subscribe?ic=8JVLJQFSKB)\n\nThis project is sponsored by Z.ai, supporting us with their GLM CODING PLAN.GLM CODING PLAN is a subscription service designed for AI coding, starting at just $3/month. It provides access to their flagship GLM-4.6 model across 10+ popular AI coding tools (Claude Code, Cline, Roo Code, etc.), offering developers top-tier, fast, and stable coding experiences.Get 10% OFF the GLM CODING PLAN with [this link](https://z.ai/subscribe?ic=8JVLJQFSKB)!\n\n---\n\n<table>\n<tr>\n<td width=\"180\"><a href=\"https://www.packyapi.com/register?aff=cc-switch\"><img src=\"assets/partners/logos/packycode.png\" alt=\"PackyCode\" width=\"150\"></a></td>\n<td>Thanks to PackyCode for sponsoring this project! PackyCode is a reliable and efficient API relay service provider, offering relay services for Claude Code, Codex, Gemini, and more. PackyCode provides special discounts for our software users: register using <a href=\"https://www.packyapi.com/register?aff=cc-switch\">this link</a> and enter the \"cc-switch\" promo code during recharge to get 10% off.</td>\n</tr>\n\n<tr>\n<td width=\"180\"><a href=\"https://aigocode.com/invite/CC-SWITCH\"><img src=\"assets/partners/logos/aigocode.png\" alt=\"AIGoCode\" width=\"150\"></a></td>\n<td>Thanks to AIGoCode for sponsoring this project! AIGoCode is an all-in-one platform that integrates Claude Code, Codex, and the latest Gemini models, providing you with stable, efficient, and highly cost-effective AI coding services. The platform offers flexible subscription plans, zero risk of account suspension, direct access with no VPN required, and lightning-fast responses. AIGoCode has prepared a special benefit for CC Switch users: if you register via <a href=\"https://aigocode.com/invite/CC-SWITCH\">this link</a>, you'll receive an extra 10% bonus credit on your first top-up!</td>\n</tr>\n\n<tr>\n<td width=\"180\"><a href=\"https://www.aicodemirror.com/register?invitecode=9915W3\"><img src=\"assets/partners/logos/aicodemirror.jpg\" alt=\"AICodeMirror\" width=\"150\"></a></td>\n<td>Thanks to AICodeMirror for sponsoring this project! AICodeMirror provides official high-stability relay services for Claude Code / Codex / Gemini CLI, with enterprise-grade concurrency, fast invoicing, and 24/7 dedicated technical support.\nClaude Code / Codex / Gemini official channels at 38% / 2% / 9% of original price, with extra discounts on top-ups! AICodeMirror offers special benefits for CC Switch users: register via <a href=\"https://www.aicodemirror.com/register?invitecode=9915W3\">this link</a> to enjoy 20% off your first top-up, and enterprise customers can get up to 25% off!</td>\n</tr>\n\n<tr>\n<td width=\"180\"><a href=\"https://cubence.com/signup?code=CCSWITCH&source=ccs\"><img src=\"assets/partners/logos/cubence.png\" alt=\"Cubence\" width=\"150\"></a></td>\n<td>Thanks to Cubence for sponsoring this project! Cubence is a reliable and efficient API relay service provider, offering relay services for Claude Code, Codex, Gemini, and more with flexible billing options including pay-as-you-go and monthly plans. Cubence provides special discounts for CC Switch users: register using <a href=\"https://cubence.com/signup?code=CCSWITCH&source=ccs\">this link</a> and enter the \"CCSWITCH\" promo code during recharge to get 10% off every top-up!</td>\n</tr>\n\n<tr>\n<td width=\"180\"><a href=\"https://www.dmxapi.cn/register?aff=bUHu\"><img src=\"assets/partners/logos/dmx-en.jpg\" alt=\"DMXAPI\" width=\"150\"></a></td>\n<td>Thanks to DMXAPI for sponsoring this project! DMXAPI provides global large model API services to 200+ enterprise users. One API key for all global models. Features include: instant invoicing, unlimited concurrency, starting from $0.15, 24/7 technical support. GPT/Claude/Gemini all at 32% off, domestic models 20-50% off, Claude Code exclusive models at 66% off! <a href=\"https://www.dmxapi.cn/register?aff=bUHu\">Register here</a></td>\n</tr>\n\n</table>\n\n## Screenshots\n\n|                  Main Interface                   |                  Add Provider                  |\n| :-----------------------------------------------: | :--------------------------------------------: |\n| ![Main Interface](assets/screenshots/main-en.png) | ![Add Provider](assets/screenshots/add-en.png) |\n\n## Features\n\n### Current Version: v3.10.2 | [Full Changelog](CHANGELOG.md) | [Release Notes](docs/release-note-v3.9.0-en.md)\n\n**v3.8.0 Major Update (2025-11-28)**\n\n**Persistence Architecture Upgrade & Brand New UI**\n\n- **SQLite + JSON Dual-layer Architecture**\n  - Migrated from JSON file storage to SQLite + JSON dual-layer structure\n  - Syncable data (providers, MCP, Prompts, Skills) stored in SQLite\n  - Device-level data (window state, local paths) stored in JSON\n  - Lays the foundation for future cloud sync functionality\n  - Schema version management for database migrations\n\n- **Brand New User Interface**\n  - Completely redesigned interface layout\n  - Unified component styles and smoother animations\n  - Optimized visual hierarchy\n  - Tailwind CSS downgraded from v4 to v3.4 for better browser compatibility\n\n- **Japanese Language Support**\n  - Added Japanese interface support (now supports Chinese/English/Japanese)\n\n- **Auto Launch on Startup**\n  - One-click enable/disable in settings\n  - Platform-native APIs (Registry/LaunchAgent/XDG autostart)\n\n- **Skills Recursive Scanning**\n  - Support for multi-level directory structures\n  - Allow same-named skills from different repositories\n\n- **Critical Bug Fixes**\n  - Fixed custom endpoints lost when updating providers\n  - Fixed Gemini configuration write issues\n  - Fixed Linux WebKitGTK rendering issues\n\n**v3.7.0 Highlights**\n\n**Six Core Features, 18,000+ Lines of New Code**\n\n- **Gemini CLI Integration**\n  - Third supported AI CLI (Claude Code / Codex / Gemini)\n  - Dual-file configuration support (`.env` + `settings.json`)\n  - Complete MCP server management\n  - Presets: Google Official (OAuth) / PackyCode / Custom\n\n- **Claude Skills Management System**\n  - Auto-scan skills from GitHub repositories (3 pre-configured curated repos)\n  - One-click install/uninstall to `~/.claude/skills/`\n  - Custom repository support + subdirectory scanning\n  - Complete lifecycle management (discover/install/update)\n\n- **Prompts Management System**\n  - Multi-preset system prompt management (unlimited presets, quick switching)\n  - Cross-app support (Claude: `CLAUDE.md` / Codex: `AGENTS.md` / Gemini: `GEMINI.md`)\n  - Markdown editor (CodeMirror 6 + real-time preview)\n  - Smart backfill protection, preserves manual modifications\n\n- **MCP v3.7.0 Unified Architecture**\n  - Single panel manages MCP servers across three applications\n  - New SSE (Server-Sent Events) transport type\n  - Smart JSON parser + Codex TOML format auto-correction\n  - Unified import/export + bidirectional sync\n\n- **Deep Link Protocol**\n  - `ccswitch://` protocol registration (all platforms)\n  - One-click import provider configs via shared links\n  - Security validation + lifecycle integration\n\n- **Environment Variable Conflict Detection**\n  - Auto-detect cross-app configuration conflicts (Claude/Codex/Gemini/MCP)\n  - Visual conflict indicators + resolution suggestions\n  - Override warnings + backup before changes\n\n**Core Capabilities**\n\n- **Provider Management**: One-click switching between Claude Code, Codex, and Gemini API configurations\n- **Speed Testing**: Measure API endpoint latency with visual quality indicators\n- **Import/Export**: Backup and restore configs with auto-rotation (keep 10 most recent)\n- **i18n Support**: Complete Chinese/English localization (UI, errors, tray)\n- **Claude Plugin Sync**: One-click apply/restore Claude plugin configurations\n\n**v3.6 Highlights**\n\n- Provider duplication & drag-and-drop sorting\n- Multi-endpoint management & custom config directory (cloud sync ready)\n- Granular model configuration (4-tier: Haiku/Sonnet/Opus/Custom)\n- WSL environment support with auto-sync on directory change\n- 100% hooks test coverage & complete architecture refactoring\n\n**System Features**\n\n- System tray with quick switching\n- Single instance daemon\n- Built-in auto-updater\n- Atomic writes with rollback protection\n\n## Download & Installation\n\n### System Requirements\n\n- **Windows**: Windows 10 and above\n- **macOS**: macOS 10.15 (Catalina) and above\n- **Linux**: Ubuntu 22.04+ / Debian 11+ / Fedora 34+ and other mainstream distributions\n\n### Windows Users\n\nDownload the latest `CC-Switch-v{version}-Windows.msi` installer or `CC-Switch-v{version}-Windows-Portable.zip` portable version from the [Releases](../../releases) page.\n\n### macOS Users\n\n**Method 1: Install via Homebrew (Recommended)**\n\n```bash\nbrew tap farion1231/ccswitch\nbrew install --cask cc-switch\n```\n\nUpdate:\n\n```bash\nbrew upgrade --cask cc-switch\n```\n\n**Method 2: Manual Download**\n\nDownload `CC-Switch-v{version}-macOS.zip` from the [Releases](../../releases) page and extract to use.\n\n> **Note**: Since the author doesn't have an Apple Developer account, you may see an \"unidentified developer\" warning on first launch. Please close it first, then go to \"System Settings\" â†’ \"Privacy & Security\" â†’ click \"Open Anyway\", and you'll be able to open it normally afterwards.\n\n### Arch Linux Users\n\n**Install via paru (Recommended)**\n\n```bash\nparu -S cc-switch-bin\n```\n\n### Linux Users\n\nDownload the latest Linux build from the [Releases](../../releases) page:\n\n- `CC-Switch-v{version}-Linux.deb` (Debian/Ubuntu)\n- `CC-Switch-v{version}-Linux.rpm` (Fedora/RHEL/openSUSE)\n- `CC-Switch-v{version}-Linux.AppImage` (Universal)\n- `CC-Switch-v{version}-Linux.flatpak` (Flatpak)\n\nFlatpak install & run:\n\n```bash\nflatpak install --user ./CC-Switch-v{version}-Linux.flatpak\nflatpak run com.ccswitch.desktop\n```\n\n## Quick Start\n\n### Basic Usage\n\n1. **Add Provider**: Click \"Add Provider\" â†’ Choose preset or create custom configuration\n2. **Switch Provider**:\n   - Main UI: Select provider â†’ Click \"Enable\"\n   - System Tray: Click provider name directly (instant effect)\n3. **Takes Effect**: Restart your terminal or Claude Code / Codex / Gemini clients to apply changes\n4. **Back to Official**: Select the \"Official Login\" preset (Claude/Codex) or \"Google Official\" preset (Gemini), restart the corresponding client, then follow its login/OAuth flow\n\n### MCP Management\n\n- **Location**: Click \"MCP\" button in top-right corner\n- **Add Server**:\n  - Use built-in templates (mcp-fetch, mcp-filesystem, etc.)\n  - Support stdio / http / sse transport types\n  - Configure independent MCP servers for different apps\n- **Enable/Disable**: Toggle switches to control which servers sync to live config\n- **Sync**: Enabled servers auto-sync to each app's live files\n- **Import/Export**: Import existing MCP servers from Claude/Codex/Gemini config files\n\n### Skills Management (v3.7.0 New)\n\n- **Location**: Click \"Skills\" button in top-right corner\n- **Discover Skills**:\n  - Auto-scan pre-configured GitHub repositories (Anthropic official, ComposioHQ, community, etc.)\n  - Add custom repositories (supports subdirectory scanning)\n- **Install Skills**: Click \"Install\" to one-click install to `~/.claude/skills/`\n- **Uninstall Skills**: Click \"Uninstall\" to safely remove and clean up state\n- **Manage Repositories**: Add/remove custom GitHub repositories\n\n### Prompts Management (v3.7.0 New)\n\n- **Location**: Click \"Prompts\" button in top-right corner\n- **Create Presets**:\n  - Create unlimited system prompt presets\n  - Use Markdown editor to write prompts (syntax highlighting + real-time preview)\n- **Switch Presets**: Select preset â†’ Click \"Activate\" to apply immediately\n- **Sync Mechanism**:\n  - Claude: `~/.claude/CLAUDE.md`\n  - Codex: `~/.codex/AGENTS.md`\n  - Gemini: `~/.gemini/GEMINI.md`\n- **Protection Mechanism**: Auto-save current prompt content before switching, preserves manual modifications\n\n### Configuration Files\n\n**Claude Code**\n\n- Live config: `~/.claude/settings.json` (or `claude.json`)\n- API key field: `env.ANTHROPIC_AUTH_TOKEN` or `env.ANTHROPIC_API_KEY`\n- MCP servers: `~/.claude.json` â†’ `mcpServers`\n\n**Codex**\n\n- Live config: `~/.codex/auth.json` (required) + `config.toml` (optional)\n- API key field: `OPENAI_API_KEY` in `auth.json`\n- MCP servers: `~/.codex/config.toml` â†’ `[mcp_servers]` tables\n\n**Gemini**\n\n- Live config: `~/.gemini/.env` (API key) + `~/.gemini/settings.json` (auth mode)\n- API key field: `GEMINI_API_KEY` or `GOOGLE_GEMINI_API_KEY` in `.env`\n- Environment variables: Support `GOOGLE_GEMINI_BASE_URL`, `GEMINI_MODEL`, etc.\n- MCP servers: `~/.gemini/settings.json` â†’ `mcpServers`\n- Tray quick switch: Each provider switch rewrites `~/.gemini/.env`, no need to restart Gemini CLI\n\n**CC Switch Storage (v3.8.0 New Architecture)**\n\n- Database (SSOT): `~/.cc-switch/cc-switch.db` (SQLite, stores providers, MCP, Prompts, Skills)\n- Local settings: `~/.cc-switch/settings.json` (device-level settings)\n- Backups: `~/.cc-switch/backups/` (auto-rotate, keep 10)\n\n### Cloud Sync Setup\n\n1. Go to Settings â†’ \"Custom Configuration Directory\"\n2. Choose your cloud sync folder (Dropbox, OneDrive, iCloud, etc.)\n3. Restart app to apply\n4. Repeat on other devices to enable cross-device sync\n\n> **Note**: First launch auto-imports existing Claude/Codex configs as default provider.\n\n## Architecture Overview\n\n### Design Principles\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Frontend (React + TS)                    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚ Components  â”‚  â”‚    Hooks     â”‚  â”‚  TanStack Query  â”‚    â”‚\nâ”‚  â”‚   (UI)      â”‚â”€â”€â”‚ (Bus. Logic) â”‚â”€â”€â”‚   (Cache/Sync)   â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                         â”‚ Tauri IPC\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                  Backend (Tauri + Rust)                     â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚  Commands   â”‚  â”‚   Services   â”‚  â”‚  Models/Config   â”‚    â”‚\nâ”‚  â”‚ (API Layer) â”‚â”€â”€â”‚ (Bus. Layer) â”‚â”€â”€â”‚     (Data)       â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Core Design Patterns**\n\n- **SSOT** (Single Source of Truth): All data stored in `~/.cc-switch/cc-switch.db` (SQLite)\n- **Dual-layer Storage**: SQLite for syncable data, JSON for device-level settings\n- **Dual-way Sync**: Write to live files on switch, backfill from live when editing active provider\n- **Atomic Writes**: Temp file + rename pattern prevents config corruption\n- **Concurrency Safe**: Mutex-protected database connection avoids race conditions\n- **Layered Architecture**: Clear separation (Commands â†’ Services â†’ DAO â†’ Database)\n\n**Key Components**\n\n- **ProviderService**: Provider CRUD, switching, backfill, sorting\n- **McpService**: MCP server management, import/export, live file sync\n- **ConfigService**: Config import/export, backup rotation\n- **SpeedtestService**: API endpoint latency measurement\n\n**v3.6 Refactoring**\n\n- Backend: 5-phase refactoring (error handling â†’ command split â†’ tests â†’ services â†’ concurrency)\n- Frontend: 4-stage refactoring (test infra â†’ hooks â†’ components â†’ cleanup)\n- Testing: 100% hooks coverage + integration tests (vitest + MSW)\n\n## Development\n\n### Environment Requirements\n\n- Node.js 18+\n- pnpm 8+\n- Rust 1.85+\n- Tauri CLI 2.8+\n\n### Development Commands\n\n```bash\n# Install dependencies\npnpm install\n\n# Dev mode (hot reload)\npnpm dev\n\n# Type check\npnpm typecheck\n\n# Format code\npnpm format\n\n# Check code format\npnpm format:check\n\n# Run frontend unit tests\npnpm test:unit\n\n# Run tests in watch mode (recommended for development)\npnpm test:unit:watch\n\n# Build application\npnpm build\n\n# Build debug version\npnpm tauri build --debug\n```\n\n### Rust Backend Development\n\n```bash\ncd src-tauri\n\n# Format Rust code\ncargo fmt\n\n# Run clippy checks\ncargo clippy\n\n# Run backend tests\ncargo test\n\n# Run specific tests\ncargo test test_name\n\n# Run tests with test-hooks feature\ncargo test --features test-hooks\n```\n\n### Testing Guide (v3.6 New)\n\n**Frontend Testing**:\n\n- Uses **vitest** as test framework\n- Uses **MSW (Mock Service Worker)** to mock Tauri API calls\n- Uses **@testing-library/react** for component testing\n\n**Test Coverage**:\n\n- Hooks unit tests (100% coverage)\n  - `useProviderActions` - Provider operations\n  - `useMcpActions` - MCP management\n  - `useSettings` series - Settings management\n  - `useImportExport` - Import/export\n- Integration tests\n  - App main application flow\n  - SettingsDialog complete interaction\n  - MCP panel functionality\n\n**Running Tests**:\n\n```bash\n# Run all tests\npnpm test:unit\n\n# Watch mode (auto re-run)\npnpm test:unit:watch\n\n# With coverage report\npnpm test:unit --coverage\n```\n\n## Tech Stack\n\n**Frontend**: React 18 Â· TypeScript Â· Vite Â· TailwindCSS 4 Â· TanStack Query v5 Â· react-i18next Â· react-hook-form Â· zod Â· shadcn/ui Â· @dnd-kit\n\n**Backend**: Tauri 2.8 Â· Rust Â· serde Â· tokio Â· thiserror Â· tauri-plugin-updater/process/dialog/store/log\n\n**Testing**: vitest Â· MSW Â· @testing-library/react\n\n## Project Structure\n\n```\nâ”œâ”€â”€ src/                      # Frontend (React + TypeScript)\nâ”‚   â”œâ”€â”€ components/           # UI components (providers/settings/mcp/ui)\nâ”‚   â”œâ”€â”€ hooks/                # Custom hooks (business logic)\nâ”‚   â”œâ”€â”€ lib/\nâ”‚   â”‚   â”œâ”€â”€ api/              # Tauri API wrapper (type-safe)\nâ”‚   â”‚   â””â”€â”€ query/            # TanStack Query config\nâ”‚   â”œâ”€â”€ i18n/locales/         # Translations (zh/en)\nâ”‚   â”œâ”€â”€ config/               # Presets (providers/mcp)\nâ”‚   â””â”€â”€ types/                # TypeScript definitions\nâ”œâ”€â”€ src-tauri/                # Backend (Rust)\nâ”‚   â””â”€â”€ src/\nâ”‚       â”œâ”€â”€ commands/         # Tauri command layer (by domain)\nâ”‚       â”œâ”€â”€ services/         # Business logic layer\nâ”‚       â”œâ”€â”€ app_config.rs     # Config data models\nâ”‚       â”œâ”€â”€ provider.rs       # Provider domain models\nâ”‚       â”œâ”€â”€ mcp.rs            # MCP sync & validation\nâ”‚       â””â”€â”€ lib.rs            # App entry & tray menu\nâ”œâ”€â”€ tests/                    # Frontend tests\nâ”‚   â”œâ”€â”€ hooks/                # Unit tests\nâ”‚   â””â”€â”€ components/           # Integration tests\nâ””â”€â”€ assets/                   # Screenshots & partner resources\n```\n\n## Changelog\n\nSee [CHANGELOG.md](CHANGELOG.md) for version update details.\n\n## Legacy Electron Version\n\n[Releases](../../releases) retains v2.0.3 legacy Electron version\n\nIf you need legacy Electron code, you can pull the electron-legacy branch\n\n## Contributing\n\nIssues and suggestions are welcome!\n\nBefore submitting PRs, please ensure:\n\n- Pass type check: `pnpm typecheck`\n- Pass format check: `pnpm format:check`\n- Pass unit tests: `pnpm test:unit`\n- ğŸ’¡ For new features, please open an issue for discussion before submitting a PR\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=farion1231/cc-switch&type=Date)](https://www.star-history.com/#farion1231/cc-switch&Date)\n\n## License\n\nMIT Â© Jason Young\n",
      "stars_today": 261
    },
    {
      "id": 976921297,
      "name": "system_prompts_leaks",
      "full_name": "asgeirtj/system_prompts_leaks",
      "description": "Collection of extracted System Prompts from popular chatbots like ChatGPT, Claude & Gemini",
      "html_url": "https://github.com/asgeirtj/system_prompts_leaks",
      "stars": 25663,
      "forks": 3993,
      "language": "JavaScript",
      "topics": [
        "ai",
        "anthropic",
        "chatbots",
        "chatgpt",
        "claude",
        "gemini",
        "generative-ai",
        "google-deepmind",
        "large-language-models",
        "llm",
        "openai",
        "prompt-engineering",
        "prompt-injection",
        "prompts"
      ],
      "created_at": "2025-05-03T02:43:56Z",
      "updated_at": "2026-01-28T01:59:24Z",
      "pushed_at": "2026-01-16T20:29:21Z",
      "open_issues": 26,
      "owner": {
        "login": "asgeirtj",
        "avatar_url": "https://avatars.githubusercontent.com/u/27446620?v=4"
      },
      "readme": "![CleanShot 2025-09-03 at 02 37 49](https://github.com/user-attachments/assets/22d32e2d-e0c9-4afc-9e72-44b779dac659)\n\n\n# System Prompts Leaks\n\nCollection of system prompts/system messages/developer messages.\n\nFeel free to do Pull Requests\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=asgeirtj/system_prompts_leaks&type=Date)](https://www.star-history.com/#asgeirtj/system_prompts_leaks&Date)\n",
      "stars_today": 241
    },
    {
      "id": 626805178,
      "name": "dify",
      "full_name": "langgenius/dify",
      "description": "Production-ready platform for agentic workflow development.",
      "html_url": "https://github.com/langgenius/dify",
      "stars": 127607,
      "forks": 19892,
      "language": "TypeScript",
      "topics": [
        "agent",
        "agentic-ai",
        "agentic-framework",
        "agentic-workflow",
        "ai",
        "automation",
        "gemini",
        "genai",
        "gpt",
        "gpt-4",
        "llm",
        "low-code",
        "mcp",
        "nextjs",
        "no-code",
        "openai",
        "orchestration",
        "python",
        "rag",
        "workflow"
      ],
      "created_at": "2023-04-12T07:40:24Z",
      "updated_at": "2026-01-28T02:13:52Z",
      "pushed_at": "2026-01-28T01:58:25Z",
      "open_issues": 706,
      "owner": {
        "login": "langgenius",
        "avatar_url": "https://avatars.githubusercontent.com/u/127165244?v=4"
      },
      "readme": "![cover-v5-optimized](./images/GitHub_README_if.png)\n\n<p align=\"center\">\n  ğŸ“Œ <a href=\"https://dify.ai/blog/introducing-dify-workflow-file-upload-a-demo-on-ai-podcast\">Introducing Dify Workflow File Upload: Recreate Google NotebookLM Podcast</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://cloud.dify.ai\">Dify Cloud</a> Â·\n  <a href=\"https://docs.dify.ai/getting-started/install-self-hosted\">Self-hosting</a> Â·\n  <a href=\"https://docs.dify.ai\">Documentation</a> Â·\n  <a href=\"https://dify.ai/pricing\">Dify edition overview</a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://dify.ai\" target=\"_blank\">\n        <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/Product-F04438\"></a>\n    <a href=\"https://dify.ai/pricing\" target=\"_blank\">\n        <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/free-pricing?logo=free&color=%20%23155EEF&label=pricing&labelColor=%20%23528bff\"></a>\n    <a href=\"https://discord.gg/FngNHpbcY7\" target=\"_blank\">\n        <img src=\"https://img.shields.io/discord/1082486657678311454?logo=discord&labelColor=%20%235462eb&logoColor=%20%23f5f5f5&color=%20%235462eb\"\n            alt=\"chat on Discord\"></a>\n    <a href=\"https://reddit.com/r/difyai\" target=\"_blank\">  \n        <img src=\"https://img.shields.io/reddit/subreddit-subscribers/difyai?style=plastic&logo=reddit&label=r%2Fdifyai&labelColor=white\"\n            alt=\"join Reddit\"></a>\n    <a href=\"https://twitter.com/intent/follow?screen_name=dify_ai\" target=\"_blank\">\n        <img src=\"https://img.shields.io/twitter/follow/dify_ai?logo=X&color=%20%23f5f5f5\"\n            alt=\"follow on X(Twitter)\"></a>\n    <a href=\"https://www.linkedin.com/company/langgenius/\" target=\"_blank\">\n        <img src=\"https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff\"\n            alt=\"follow on LinkedIn\"></a>\n    <a href=\"https://hub.docker.com/u/langgenius\" target=\"_blank\">\n        <img alt=\"Docker Pulls\" src=\"https://img.shields.io/docker/pulls/langgenius/dify-web?labelColor=%20%23FDB062&color=%20%23f79009\"></a>\n    <a href=\"https://github.com/langgenius/dify/graphs/commit-activity\" target=\"_blank\">\n        <img alt=\"Commits last month\" src=\"https://img.shields.io/github/commit-activity/m/langgenius/dify?labelColor=%20%2332b583&color=%20%2312b76a\"></a>\n    <a href=\"https://github.com/langgenius/dify/\" target=\"_blank\">\n        <img alt=\"Issues closed\" src=\"https://img.shields.io/github/issues-search?query=repo%3Alanggenius%2Fdify%20is%3Aclosed&label=issues%20closed&labelColor=%20%237d89b0&color=%20%235d6b98\"></a>\n    <a href=\"https://github.com/langgenius/dify/discussions/\" target=\"_blank\">\n        <img alt=\"Discussion posts\" src=\"https://img.shields.io/github/discussions/langgenius/dify?labelColor=%20%239b8afb&color=%20%237a5af8\"></a>\n    <a href=\"https://insights.linuxfoundation.org/project/langgenius-dify\" target=\"_blank\">\n        <img alt=\"LFX Health Score\" src=\"https://insights.linuxfoundation.org/api/badge/health-score?project=langgenius-dify\"></a>\n    <a href=\"https://insights.linuxfoundation.org/project/langgenius-dify\" target=\"_blank\">\n        <img alt=\"LFX Contributors\" src=\"https://insights.linuxfoundation.org/api/badge/contributors?project=langgenius-dify\"></a>\n    <a href=\"https://insights.linuxfoundation.org/project/langgenius-dify\" target=\"_blank\">\n        <img alt=\"LFX Active Contributors\" src=\"https://insights.linuxfoundation.org/api/badge/active-contributors?project=langgenius-dify\"></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"./README.md\"><img alt=\"README in English\" src=\"https://img.shields.io/badge/English-d9d9d9\"></a>\n  <a href=\"./docs/zh-TW/README.md\"><img alt=\"ç¹é«”ä¸­æ–‡æ–‡ä»¶\" src=\"https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-d9d9d9\"></a>\n  <a href=\"./docs/zh-CN/README.md\"><img alt=\"ç®€ä½“ä¸­æ–‡æ–‡ä»¶\" src=\"https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-d9d9d9\"></a>\n  <a href=\"./docs/ja-JP/README.md\"><img alt=\"æ—¥æœ¬èªã®README\" src=\"https://img.shields.io/badge/æ—¥æœ¬èª-d9d9d9\"></a>\n  <a href=\"./docs/es-ES/README.md\"><img alt=\"README en EspaÃ±ol\" src=\"https://img.shields.io/badge/EspaÃ±ol-d9d9d9\"></a>\n  <a href=\"./docs/fr-FR/README.md\"><img alt=\"README en FranÃ§ais\" src=\"https://img.shields.io/badge/FranÃ§ais-d9d9d9\"></a>\n  <a href=\"./docs/tlh/README.md\"><img alt=\"README tlhIngan Hol\" src=\"https://img.shields.io/badge/Klingon-d9d9d9\"></a>\n  <a href=\"./docs/ko-KR/README.md\"><img alt=\"README in Korean\" src=\"https://img.shields.io/badge/í•œêµ­ì–´-d9d9d9\"></a>\n  <a href=\"./docs/ar-SA/README.md\"><img alt=\"README Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\" src=\"https://img.shields.io/badge/Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©-d9d9d9\"></a>\n  <a href=\"./docs/tr-TR/README.md\"><img alt=\"TÃ¼rkÃ§e README\" src=\"https://img.shields.io/badge/TÃ¼rkÃ§e-d9d9d9\"></a>\n  <a href=\"./docs/vi-VN/README.md\"><img alt=\"README Tiáº¿ng Viá»‡t\" src=\"https://img.shields.io/badge/Ti%E1%BA%BFng%20Vi%E1%BB%87t-d9d9d9\"></a>\n  <a href=\"./docs/de-DE/README.md\"><img alt=\"README in Deutsch\" src=\"https://img.shields.io/badge/German-d9d9d9\"></a>\n  <a href=\"./docs/bn-BD/README.md\"><img alt=\"README in à¦¬à¦¾à¦‚à¦²à¦¾\" src=\"https://img.shields.io/badge/à¦¬à¦¾à¦‚à¦²à¦¾-d9d9d9\"></a>\n</p>\n\nDify is an open-source platform for developing LLM applications. Its intuitive interface combines agentic AI workflows, RAG pipelines, agent capabilities, model management, observability features, and moreâ€”allowing you to quickly move from prototype to production.\n\n## Quick start\n\n> Before installing Dify, make sure your machine meets the following minimum system requirements:\n>\n> - CPU >= 2 Core\n> - RAM >= 4 GiB\n\n<br/>\n\nThe easiest way to start the Dify server is through [Docker Compose](docker/docker-compose.yaml). Before running Dify with the following commands, make sure that [Docker](https://docs.docker.com/get-docker/) and [Docker Compose](https://docs.docker.com/compose/install/) are installed on your machine:\n\n```bash\ncd dify\ncd docker\ncp .env.example .env\ndocker compose up -d\n```\n\nAfter running, you can access the Dify dashboard in your browser at [http://localhost/install](http://localhost/install) and start the initialization process.\n\n#### Seeking help\n\nPlease refer to our [FAQ](https://docs.dify.ai/getting-started/install-self-hosted/faqs) if you encounter problems setting up Dify. Reach out to [the community and us](#community--contact) if you are still having issues.\n\n> If you'd like to contribute to Dify or do additional development, refer to our [guide to deploying from source code](https://docs.dify.ai/getting-started/install-self-hosted/local-source-code)\n\n## Key features\n\n**1. Workflow**:\nBuild and test powerful AI workflows on a visual canvas, leveraging all the following features and beyond.\n\n**2. Comprehensive model support**:\nSeamless integration with hundreds of proprietary / open-source LLMs from dozens of inference providers and self-hosted solutions, covering GPT, Mistral, Llama3, and any OpenAI API-compatible models. A full list of supported model providers can be found [here](https://docs.dify.ai/getting-started/readme/model-providers).\n\n![providers-v5](https://github.com/langgenius/dify/assets/13230914/5a17bdbe-097a-4100-8363-40255b70f6e3)\n\n**3. Prompt IDE**:\nIntuitive interface for crafting prompts, comparing model performance, and adding additional features such as text-to-speech to a chat-based app.\n\n**4. RAG Pipeline**:\nExtensive RAG capabilities that cover everything from document ingestion to retrieval, with out-of-box support for text extraction from PDFs, PPTs, and other common document formats.\n\n**5. Agent capabilities**:\nYou can define agents based on LLM Function Calling or ReAct, and add pre-built or custom tools for the agent. Dify provides 50+ built-in tools for AI agents, such as Google Search, DALLÂ·E, Stable Diffusion and WolframAlpha.\n\n**6. LLMOps**:\nMonitor and analyze application logs and performance over time. You could continuously improve prompts, datasets, and models based on production data and annotations.\n\n**7. Backend-as-a-Service**:\nAll of Dify's offerings come with corresponding APIs, so you could effortlessly integrate Dify into your own business logic.\n\n## Using Dify\n\n- **Cloud <br/>**\n  We host a [Dify Cloud](https://dify.ai) service for anyone to try with zero setup. It provides all the capabilities of the self-deployed version, and includes 200 free GPT-4 calls in the sandbox plan.\n\n- **Self-hosting Dify Community Edition<br/>**\n  Quickly get Dify running in your environment with this [starter guide](#quick-start).\n  Use our [documentation](https://docs.dify.ai) for further references and more in-depth instructions.\n\n- **Dify for enterprise / organizations<br/>**\n  We provide additional enterprise-centric features. [Send us an email](mailto:business@dify.ai?subject=%5BGitHub%5DBusiness%20License%20Inquiry) to discuss your enterprise needs. <br/>\n\n  > For startups and small businesses using AWS, check out [Dify Premium on AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-t22mebxzwjhu6) and deploy it to your own AWS VPC with one click. It's an affordable AMI offering with the option to create apps with custom logo and branding.\n\n## Staying ahead\n\nStar Dify on GitHub and be instantly notified of new releases.\n\n![star-us](https://github.com/langgenius/dify/assets/13230914/b823edc1-6388-4e25-ad45-2f6b187adbb4)\n\n## Advanced Setup\n\n### Custom configurations\n\nIf you need to customize the configuration, please refer to the comments in our [.env.example](docker/.env.example) file and update the corresponding values in your `.env` file. Additionally, you might need to make adjustments to the `docker-compose.yaml` file itself, such as changing image versions, port mappings, or volume mounts, based on your specific deployment environment and requirements. After making any changes, please re-run `docker-compose up -d`. You can find the full list of available environment variables [here](https://docs.dify.ai/getting-started/install-self-hosted/environments).\n\n#### Customizing Suggested Questions\n\nYou can now customize the \"Suggested Questions After Answer\" feature to better fit your use case. For example, to generate longer, more technical questions:\n\n```bash\n# In your .env file\nSUGGESTED_QUESTIONS_PROMPT='Please help me predict the five most likely technical follow-up questions a developer would ask. Focus on implementation details, best practices, and architecture considerations. Keep each question between 40-60 characters. Output must be JSON array: [\"question1\",\"question2\",\"question3\",\"question4\",\"question5\"]'\nSUGGESTED_QUESTIONS_MAX_TOKENS=512\nSUGGESTED_QUESTIONS_TEMPERATURE=0.3\n```\n\nSee the [Suggested Questions Configuration Guide](docs/suggested-questions-configuration.md) for detailed examples and usage instructions.\n\n### Metrics Monitoring with Grafana\n\nImport the dashboard to Grafana, using Dify's PostgreSQL database as data source, to monitor metrics in granularity of apps, tenants, messages, and more.\n\n- [Grafana Dashboard by @bowenliang123](https://github.com/bowenliang123/dify-grafana-dashboard)\n\n### Deployment with Kubernetes\n\nIf you'd like to configure a highly-available setup, there are community-contributed [Helm Charts](https://helm.sh/) and YAML files which allow Dify to be deployed on Kubernetes.\n\n- [Helm Chart by @LeoQuote](https://github.com/douban/charts/tree/master/charts/dify)\n- [Helm Chart by @BorisPolonsky](https://github.com/BorisPolonsky/dify-helm)\n- [Helm Chart by @magicsong](https://github.com/magicsong/ai-charts)\n- [YAML file by @Winson-030](https://github.com/Winson-030/dify-kubernetes)\n- [YAML file by @wyy-holding](https://github.com/wyy-holding/dify-k8s)\n- [ğŸš€ NEW! YAML files (Supports Dify v1.6.0) by @Zhoneym](https://github.com/Zhoneym/DifyAI-Kubernetes)\n\n#### Using Terraform for Deployment\n\nDeploy Dify to Cloud Platform with a single click using [terraform](https://www.terraform.io/)\n\n##### Azure Global\n\n- [Azure Terraform by @nikawang](https://github.com/nikawang/dify-azure-terraform)\n\n##### Google Cloud\n\n- [Google Cloud Terraform by @sotazum](https://github.com/DeNA/dify-google-cloud-terraform)\n\n#### Using AWS CDK for Deployment\n\nDeploy Dify to AWS with [CDK](https://aws.amazon.com/cdk/)\n\n##### AWS\n\n- [AWS CDK by @KevinZhao (EKS based)](https://github.com/aws-samples/solution-for-deploying-dify-on-aws)\n- [AWS CDK by @tmokmss (ECS based)](https://github.com/aws-samples/dify-self-hosted-on-aws)\n\n#### Using Alibaba Cloud Computing Nest\n\nQuickly deploy Dify to Alibaba cloud with [Alibaba Cloud Computing Nest](https://computenest.console.aliyun.com/service/instance/create/default?type=user&ServiceName=Dify%E7%A4%BE%E5%8C%BA%E7%89%88)\n\n#### Using Alibaba Cloud Data Management\n\nOne-Click deploy Dify to Alibaba Cloud with [Alibaba Cloud Data Management](https://www.alibabacloud.com/help/en/dms/dify-in-invitational-preview/)\n\n#### Deploy to AKS with Azure Devops Pipeline\n\nOne-Click deploy Dify to AKS with [Azure Devops Pipeline Helm Chart by @LeoZhang](https://github.com/Ruiruiz30/Dify-helm-chart-AKS)\n\n## Contributing\n\nFor those who'd like to contribute code, see our [Contribution Guide](https://github.com/langgenius/dify/blob/main/CONTRIBUTING.md).\nAt the same time, please consider supporting Dify by sharing it on social media and at events and conferences.\n\n> We are looking for contributors to help translate Dify into languages other than Mandarin or English. If you are interested in helping, please see the [i18n README](https://github.com/langgenius/dify/blob/main/web/i18n-config/README.md) for more information, and leave us a comment in the `global-users` channel of our [Discord Community Server](https://discord.gg/8Tpq4AcN9c).\n\n## Community & contact\n\n- [GitHub Discussion](https://github.com/langgenius/dify/discussions). Best for: sharing feedback and asking questions.\n- [GitHub Issues](https://github.com/langgenius/dify/issues). Best for: bugs you encounter using Dify.AI, and feature proposals. See our [Contribution Guide](https://github.com/langgenius/dify/blob/main/CONTRIBUTING.md).\n- [Discord](https://discord.gg/FngNHpbcY7). Best for: sharing your applications and hanging out with the community.\n- [X(Twitter)](https://twitter.com/dify_ai). Best for: sharing your applications and hanging out with the community.\n\n**Contributors**\n\n<a href=\"https://github.com/langgenius/dify/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=langgenius/dify\" />\n</a>\n\n## Star history\n\n[![Star History Chart](https://api.star-history.com/svg?repos=langgenius/dify&type=Date)](https://star-history.com/#langgenius/dify&Date)\n\n## Security disclosure\n\nTo protect your privacy, please avoid posting security issues on GitHub. Instead, report issues to security@dify.ai, and our team will respond with detailed answer.\n\n## License\n\nThis repository is licensed under the [Dify Open Source License](LICENSE), based on Apache 2.0 with additional conditions.\n",
      "stars_today": 238
    },
    {
      "id": 157616880,
      "name": "iptv",
      "full_name": "iptv-org/iptv",
      "description": "Collection of publicly available IPTV channels from all over the world",
      "html_url": "https://github.com/iptv-org/iptv",
      "stars": 110582,
      "forks": 5459,
      "language": "TypeScript",
      "topics": [
        "iptv",
        "m3u",
        "playlist",
        "streams",
        "tv"
      ],
      "created_at": "2018-11-14T22:00:57Z",
      "updated_at": "2026-01-28T02:13:54Z",
      "pushed_at": "2026-01-28T02:01:40Z",
      "open_issues": 324,
      "owner": {
        "login": "iptv-org",
        "avatar_url": "https://avatars.githubusercontent.com/u/55937028?v=4"
      },
      "readme": "# IPTV [![update](https://github.com/iptv-org/iptv/actions/workflows/update.yml/badge.svg)](https://github.com/iptv-org/iptv/actions/workflows/update.yml)\r\n\r\nCollection of publicly available IPTV (Internet Protocol television) channels from all over the world.\r\n\r\n## Table of contents\r\n\r\n- ğŸš€ [How to use?](#how-to-use)\r\n- ğŸ“º [Playlists](#playlists)\r\n- ğŸ—“ [EPG](#epg)\r\n- ğŸ—„ [Database](#database)\r\n- ğŸ‘¨â€ğŸ’» [API](#api)\r\n- ğŸ“š [Resources](#resources)\r\n- ğŸ’¬ [Discussions](#discussions)\r\n- â“ [FAQ](#faq)\r\n- ğŸ›  [Contribution](#contribution)\r\n- âš– [Legal](#legal)\r\n- Â© [License](#license)\r\n\r\n## How to use?\r\n\r\nSimply paste the link to one of the playlists into [any video player](https://github.com/iptv-org/awesome-iptv#apps) that supports live streaming and press _Open_.\r\n\r\n![VLC Network Panel](https://github.com/iptv-org/iptv/raw/master/.readme/preview.png)\r\n\r\n## Playlists\r\n\r\nThe main playlist containing all channels available in the repository can be found at:\r\n\r\n```\r\nhttps://iptv-org.github.io/iptv/index.m3u\r\n```\r\n\r\nLinks to other playlists can be found in the [PLAYLISTS.md](PLAYLISTS.md) file.\r\n\r\n## EPG\r\n\r\n[Electronic Program Guide](https://en.wikipedia.org/wiki/Electronic_program_guide) for most of the channels can be downloaded using utilities published in the [iptv-org/epg](https://github.com/iptv-org/epg) repository.\r\n\r\n## Database\r\n\r\nAll channel data is taken from the [iptv-org/database](https://github.com/iptv-org/database) repository. If you find any errors please open a new [issue](https://github.com/iptv-org/database/issues) there.\r\n\r\n## API\r\n\r\nThe API documentation can be found in the [iptv-org/api](https://github.com/iptv-org/api) repository.\r\n\r\n## Resources\r\n\r\nLinks to other useful IPTV-related resources can be found in the [iptv-org/awesome-iptv](https://github.com/iptv-org/awesome-iptv) repository.\r\n\r\n## Discussions\r\n\r\nIf you have a question or idea, welcome to the [Discussions](https://github.com/orgs/iptv-org/discussions).\r\n\r\n## FAQ\r\n\r\nThe answers to the most popular questions can be found in the [FAQ.md](FAQ.md) file.\r\n\r\n## Contribution\r\n\r\nPlease make sure to read the [Contributing Guide](CONTRIBUTING.md) before sending an issue or making a pull request.\r\n\r\nAnd thank you to everyone who has already contributed!\r\n\r\n### Backers\r\n\r\n<a href=\"https://opencollective.com/iptv-org\"><img src=\"https://opencollective.com/iptv-org/backers.svg?width=890\" /></a>\r\n\r\n### Contributors\r\n\r\n<a href=\"https://github.com/iptv-org/iptv/graphs/contributors\"><img src=\"https://opencollective.com/iptv-org/contributors.svg?width=890\" /></a>\r\n\r\n## Legal\r\n\r\nNo video files are stored in this repository. The repository simply contains user-submitted links to publicly available video stream URLs, which to the best of our knowledge have been intentionally made publicly by the copyright holders. If any links in these playlists infringe on your rights as a copyright holder, they may be removed by sending a [pull request](https://github.com/iptv-org/iptv/pulls) or opening an [issue](https://github.com/iptv-org/iptv/issues/new?assignees=freearhey&labels=removal+request&template=--removal-request.yml&title=Remove%3A+). However, note that we have **no control** over the destination of the link, and just removing the link from the playlist will not remove its contents from the web. Note that linking does not directly infringe copyright because no copy is made on the site providing the link, and thus this is **not** a valid reason to send a DMCA notice to GitHub. To remove this content from the web, you should contact the web host that's actually hosting the content (**not** GitHub, nor the maintainers of this repository).\r\n\r\n## License\r\n\r\n[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](LICENSE)\r\n",
      "stars_today": 177
    },
    {
      "id": 965415649,
      "name": "codex",
      "full_name": "openai/codex",
      "description": "Lightweight coding agent that runs in your terminal",
      "html_url": "https://github.com/openai/codex",
      "stars": 57869,
      "forks": 7497,
      "language": "Rust",
      "topics": [],
      "created_at": "2025-04-13T05:37:54Z",
      "updated_at": "2026-01-28T02:09:35Z",
      "pushed_at": "2026-01-28T02:09:31Z",
      "open_issues": 891,
      "owner": {
        "login": "openai",
        "avatar_url": "https://avatars.githubusercontent.com/u/14957082?v=4"
      },
      "readme": "<p align=\"center\"><code>npm i -g @openai/codex</code><br />or <code>brew install --cask codex</code></p>\n<p align=\"center\"><strong>Codex CLI</strong> is a coding agent from OpenAI that runs locally on your computer.\n<p align=\"center\">\n  <img src=\"./.github/codex-cli-splash.png\" alt=\"Codex CLI splash\" width=\"80%\" />\n</p>\n</br>\nIf you want Codex in your code editor (VS Code, Cursor, Windsurf), <a href=\"https://developers.openai.com/codex/ide\">install in your IDE.</a>\n</br>If you are looking for the <em>cloud-based agent</em> from OpenAI, <strong>Codex Web</strong>, go to <a href=\"https://chatgpt.com/codex\">chatgpt.com/codex</a>.</p>\n\n---\n\n## Quickstart\n\n### Installing and running Codex CLI\n\nInstall globally with your preferred package manager:\n\n```shell\n# Install using npm\nnpm install -g @openai/codex\n```\n\n```shell\n# Install using Homebrew\nbrew install --cask codex\n```\n\nThen simply run `codex` to get started.\n\n<details>\n<summary>You can also go to the <a href=\"https://github.com/openai/codex/releases/latest\">latest GitHub Release</a> and download the appropriate binary for your platform.</summary>\n\nEach GitHub Release contains many executables, but in practice, you likely want one of these:\n\n- macOS\n  - Apple Silicon/arm64: `codex-aarch64-apple-darwin.tar.gz`\n  - x86_64 (older Mac hardware): `codex-x86_64-apple-darwin.tar.gz`\n- Linux\n  - x86_64: `codex-x86_64-unknown-linux-musl.tar.gz`\n  - arm64: `codex-aarch64-unknown-linux-musl.tar.gz`\n\nEach archive contains a single entry with the platform baked into the name (e.g., `codex-x86_64-unknown-linux-musl`), so you likely want to rename it to `codex` after extracting it.\n\n</details>\n\n### Using Codex with your ChatGPT plan\n\nRun `codex` and select **Sign in with ChatGPT**. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan. [Learn more about what's included in your ChatGPT plan](https://help.openai.com/en/articles/11369540-codex-in-chatgpt).\n\nYou can also use Codex with an API key, but this requires [additional setup](https://developers.openai.com/codex/auth#sign-in-with-an-api-key).\n\n## Docs\n\n- [**Codex Documentation**](https://developers.openai.com/codex)\n- [**Contributing**](./docs/contributing.md)\n- [**Installing & building**](./docs/install.md)\n- [**Open source fund**](./docs/open-source-fund.md)\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n",
      "stars_today": 156
    },
    {
      "id": 1024118326,
      "name": "WeKnora",
      "full_name": "Tencent/WeKnora",
      "description": "LLM-powered framework for deep document understanding, semantic retrieval, and context-aware answers using RAG paradigm.",
      "html_url": "https://github.com/Tencent/WeKnora",
      "stars": 12548,
      "forks": 1410,
      "language": "Go",
      "topics": [
        "agent",
        "agentic",
        "ai",
        "chatbot",
        "chatbots",
        "embeddings",
        "evaluation",
        "generative-ai",
        "golang",
        "knowledge-base",
        "llm",
        "multi-tenant",
        "multimodel",
        "ollama",
        "openai",
        "question-answering",
        "rag",
        "reranking",
        "semantic-search",
        "vector-search"
      ],
      "created_at": "2025-07-22T08:01:23Z",
      "updated_at": "2026-01-28T02:11:58Z",
      "pushed_at": "2026-01-27T10:15:54Z",
      "open_issues": 83,
      "owner": {
        "login": "Tencent",
        "avatar_url": "https://avatars.githubusercontent.com/u/18461506?v=4"
      },
      "readme": "<p align=\"center\">\n  <picture>\n    <img src=\"./docs/images/logo.png\" alt=\"WeKnora Logo\" height=\"120\"/>\n  </picture>\n</p>\n\n<p align=\"center\">\n  <picture>\n    <a href=\"https://trendshift.io/repositories/15289\" target=\"_blank\">\n      <img src=\"https://trendshift.io/api/badge/repositories/15289\" alt=\"Tencent%2FWeKnora | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/>\n    </a>\n  </picture>\n</p>\n<p align=\"center\">\n    <a href=\"https://weknora.weixin.qq.com\" target=\"_blank\">\n        <img alt=\"å®˜æ–¹ç½‘ç«™\" src=\"https://img.shields.io/badge/å®˜æ–¹ç½‘ç«™-WeKnora-4e6b99\">\n    </a>\n    <a href=\"https://chatbot.weixin.qq.com\" target=\"_blank\">\n        <img alt=\"å¾®ä¿¡å¯¹è¯å¼€æ”¾å¹³å°\" src=\"https://img.shields.io/badge/å¾®ä¿¡å¯¹è¯å¼€æ”¾å¹³å°-5ac725\">\n    </a>\n    <a href=\"https://github.com/Tencent/WeKnora/blob/main/LICENSE\">\n        <img src=\"https://img.shields.io/badge/License-MIT-ffffff?labelColor=d4eaf7&color=2e6cc4\" alt=\"License\">\n    </a>\n    <a href=\"./CHANGELOG.md\">\n        <img alt=\"Version\" src=\"https://img.shields.io/badge/version-0.2.10-2e6cc4?labelColor=d4eaf7\">\n    </a>\n</p>\n\n<p align=\"center\">\n| <b>English</b> | <a href=\"./README_CN.md\"><b>ç®€ä½“ä¸­æ–‡</b></a> | <a href=\"./README_JA.md\"><b>æ—¥æœ¬èª</b></a> |\n</p>\n\n<p align=\"center\">\n  <h4 align=\"center\">\n\n  [Overview](#-overview) â€¢ [Architecture](#-architecture) â€¢ [Key Features](#-key-features) â€¢ [Getting Started](#-getting-started) â€¢ [API Reference](#-api-reference) â€¢ [Developer Guide](#-developer-guide)\n  \n  </h4>\n</p>\n\n# ğŸ’¡ WeKnora - LLM-Powered Document Understanding & Retrieval Framework\n\n## ğŸ“Œ Overview\n\n[**WeKnora**](https://weknora.weixin.qq.com) is an LLM-powered framework designed for deep document understanding and semantic retrieval, especially for handling complex, heterogeneous documents. \n\nIt adopts a modular architecture that combines multimodal preprocessing, semantic vector indexing, intelligent retrieval, and large language model inference. At its core, WeKnora follows the **RAG (Retrieval-Augmented Generation)** paradigm, enabling high-quality, context-aware answers by combining relevant document chunks with model reasoning.\n\n**Website:** https://weknora.weixin.qq.com\n\n## âœ¨ Latest Updates\n\n**v0.2.0 Highlights:**\n\n- ğŸ¤– **Agent Mode**: New ReACT Agent mode that can call built-in tools, MCP tools, and web search, providing comprehensive summary reports through multiple iterations and reflection\n- ğŸ“š **Multi-Type Knowledge Bases**: Support for FAQ and document knowledge base types, with new features including folder import, URL import, tag management, and online entry\n- âš™ï¸ **Conversation Strategy**: Support for configuring Agent models, normal mode models, retrieval thresholds, and Prompts, with precise control over multi-turn conversation behavior\n- ğŸŒ **Web Search**: Support for extensible web search engines with built-in DuckDuckGo search engine\n- ğŸ”Œ **MCP Tool Integration**: Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting multiple transport methods\n- ğŸ¨ **New UI**: Optimized conversation interface with Agent mode/normal mode switching, tool call process display, and comprehensive knowledge base management interface upgrade\n- âš¡ **Infrastructure Upgrade**: Introduced MQ async task management, support for automatic database migration, and fast development mode\n\n## ğŸ”’ Security Notice\n\n**Important:** Starting from v0.1.3, WeKnora includes login authentication functionality to enhance system security. For production deployments, we strongly recommend:\n\n- Deploy WeKnora services in internal/private network environments rather than public internet\n- Avoid exposing the service directly to public networks to prevent potential information leakage\n- Configure proper firewall rules and access controls for your deployment environment\n- Regularly update to the latest version for security patches and improvements\n\n## ğŸ—ï¸ Architecture\n\n![weknora-architecture.png](./docs/images/architecture.png)\n\nWeKnora employs a modern modular design to build a complete document understanding and retrieval pipeline. The system primarily includes document parsing, vector processing, retrieval engine, and large model inference as core modules, with each component being flexibly configurable and extendable.\n\n## ğŸ¯ Key Features\n\n- **ğŸ¤– Agent Mode**: Support for ReACT Agent mode that can use built-in tools to retrieve knowledge bases, MCP tools, and web search tools to access external services, providing comprehensive summary reports through multiple iterations and reflection\n- **ğŸ” Precise Understanding**: Structured content extraction from PDFs, Word documents, images and more into unified semantic views\n- **ğŸ§  Intelligent Reasoning**: Leverages LLMs to understand document context and user intent for accurate Q&A and multi-turn conversations\n- **ğŸ“š Multi-Type Knowledge Bases**: Support for FAQ and document knowledge base types, with folder import, URL import, tag management, and online entry capabilities\n- **ğŸ”§ Flexible Extension**: All components from parsing and embedding to retrieval and generation are decoupled for easy customization\n- **âš¡ Efficient Retrieval**: Hybrid retrieval strategies combining keywords, vectors, and knowledge graphs, with cross-knowledge base retrieval support\n- **ğŸŒ Web Search**: Support for extensible web search engines with built-in DuckDuckGo search engine\n- **ğŸ”Œ MCP Tool Integration**: Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting multiple transport methods\n- **âš™ï¸ Conversation Strategy**: Support for configuring Agent models, normal mode models, retrieval thresholds, and Prompts, with precise control over multi-turn conversation behavior\n- **ğŸ¯ User-Friendly**: Intuitive web interface and standardized APIs for zero technical barriers\n- **ğŸ”’ Secure & Controlled**: Support for local deployment and private cloud, ensuring complete data sovereignty\n\n## ğŸ“Š Application Scenarios\n\n| Scenario | Applications | Core Value |\n|---------|----------|----------|\n| **Enterprise Knowledge Management** | Internal document retrieval, policy Q&A, operation manual search | Improve knowledge discovery efficiency, reduce training costs |\n| **Academic Research Analysis** | Paper retrieval, research report analysis, scholarly material organization | Accelerate literature review, assist research decisions |\n| **Product Technical Support** | Product manual Q&A, technical documentation search, troubleshooting | Enhance customer service quality, reduce support burden |\n| **Legal & Compliance Review** | Contract clause retrieval, regulatory policy search, case analysis | Improve compliance efficiency, reduce legal risks |\n| **Medical Knowledge Assistance** | Medical literature retrieval, treatment guideline search, case analysis | Support clinical decisions, improve diagnosis quality |\n\n## ğŸ§© Feature Matrix\n\n| Module | Support                                                                        | Description                                                                                                                                                        |\n|---------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Agent Mode | âœ… ReACT Agent Mode                                                             | Support for using built-in tools to retrieve knowledge bases, MCP tools, and web search, with cross-knowledge base retrieval and multiple iterations               |\n| Knowledge Base Types | âœ… FAQ / Document                                                               | Support for creating FAQ and document knowledge base types, with folder import, URL import, tag management, and online entry                                       |\n| Document Formats | âœ… PDF / Word / Txt / Markdown / Images (with OCR / Caption)                    | Support for structured and unstructured documents with text extraction from images                                                                                 |\n| Model Management | âœ… Centralized configuration, built-in model sharing                            | Centralized model configuration with model selection in knowledge base settings, support for multi-tenant shared built-in models                                   |\n| Embedding Models | âœ… Local models, BGE / GTE APIs, etc.                                           | Customizable embedding models, compatible with local deployment and cloud vector generation APIs                                                                   |\n| Vector DB Integration | âœ… PostgreSQL (pgvector), Elasticsearch                                         | Support for mainstream vector index backends, flexible switching for different retrieval scenarios                                                                 |\n| Retrieval Strategies | âœ… BM25 / Dense Retrieval / GraphRAG                                            | Support for sparse/dense recall and knowledge graph-enhanced retrieval with customizable retrieve-rerank-generate pipelines                                        |\n| LLM Integration | âœ… Support for Qwen, DeepSeek, etc., with thinking/non-thinking mode switching  | Compatible with local models (e.g., via Ollama) or external API services with flexible inference configuration                                                     |\n| Conversation Strategy | âœ… Agent models, normal mode models, retrieval thresholds, Prompt configuration | Support for configuring Agent models, normal mode models, retrieval thresholds, online Prompt configuration, precise control over multi-turn conversation behavior |\n| Web Search | âœ… Extensible search engines, DuckDuckGo / Google                               | Support for extensible web search engines with built-in DuckDuckGo search engine                                                                                   |\n| MCP Tools | âœ… uvx, npx launchers, Stdio/HTTP Streamable/SSE                                | Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting three transport methods                                      |\n| QA Capabilities | âœ… Context-aware, multi-turn dialogue, prompt templates                         | Support for complex semantic modeling, instruction control and chain-of-thought Q&A with configurable prompts and context windows                                  |\n| E2E Testing | âœ… Retrieval+generation process visualization and metric evaluation             | End-to-end testing tools for evaluating recall hit rates, answer coverage, BLEU/ROUGE and other metrics                                                            |\n| Deployment Modes | âœ… Support for local deployment / Docker images                                 | Meets private, offline deployment and flexible operation requirements, with fast development mode support                                                          |\n| User Interfaces | âœ… Web UI + RESTful API                                                         | Interactive interface and standard API endpoints, with Agent mode/normal mode switching and tool call process display                                              |\n| Task Management | âœ… MQ async tasks, automatic database migration                                 | MQ-based async task state maintenance, support for automatic database schema and data migration during version upgrades                                            |\n\n## ğŸš€ Getting Started\n\n### ğŸ›  Prerequisites\n\nMake sure the following tools are installed on your system:\n\n* [Docker](https://www.docker.com/)\n* [Docker Compose](https://docs.docker.com/compose/)\n* [Git](https://git-scm.com/)\n\n### ğŸ“¦ Installation\n\n#### â‘  Clone the repository\n\n```bash\n# Clone the main repository\ngit clone https://github.com/Tencent/WeKnora.git\ncd WeKnora\n```\n\n#### â‘¡ Configure environment variables\n\n```bash\n# Copy example env file\ncp .env.example .env\n\n# Edit .env and set required values\n# All variables are documented in the .env.example comments\n```\n\n#### â‘¢ Start the services (include Ollama)\n\nCheck the images that need to be started in the .env file.\n\n```bash\n./scripts/start_all.sh\n```\n\nor\n\n```bash\nmake start-all\n```\n\n#### â‘¢.0 Start ollama services (Optional)\n\n```bash\nollama serve > /dev/null 2>&1 &\n```\n\n#### â‘¢.1 Activate different combinations of features\n\n- Minimum core services\n```bash\ndocker compose up -d\n```\n\n- All features enabled\n```bash\ndocker-compose --profile full up -d\n```\n\n- Tracing logs required\n```bash\ndocker-compose --profile jaeger up -d\n```\n\n- Neo4j knowledge graph required\n```bash\ndocker-compose --profile neo4j up -d\n```\n\n- Minio file storage service required\n```bash\ndocker-compose --profile minio up -d\n```\n\n- Multiple options combination\n```bash\ndocker-compose --profile neo4j --profile minio up -d\n```\n\n#### â‘£ Stop the services\n\n```bash\n./scripts/start_all.sh --stop\n# Or\nmake stop-all\n```\n\n### ğŸŒ Access Services\n\nOnce started, services will be available at:\n\n* Web UI: `http://localhost`\n* Backend API: `http://localhost:8080`\n* Jaeger Tracing: `http://localhost:16686`\n\n### ğŸ”Œ Using WeChat Dialog Open Platform\n\nWeKnora serves as the core technology framework for the [WeChat Dialog Open Platform](https://chatbot.weixin.qq.com), providing a more convenient usage approach:\n\n- **Zero-code Deployment**: Simply upload knowledge to quickly deploy intelligent Q&A services within the WeChat ecosystem, achieving an \"ask and answer\" experience\n- **Efficient Question Management**: Support for categorized management of high-frequency questions, with rich data tools to ensure accurate, reliable, and easily maintainable answers\n- **WeChat Ecosystem Integration**: Through the WeChat Dialog Open Platform, WeKnora's intelligent Q&A capabilities can be seamlessly integrated into WeChat Official Accounts, Mini Programs, and other WeChat scenarios, enhancing user interaction experiences\n\n### ğŸ”— Access WeKnora via MCP Server\n\n#### 1ï¸âƒ£ Clone the repository\n```\ngit clone https://github.com/Tencent/WeKnora\n```\n\n#### 2ï¸âƒ£ Configure MCP Server\n> It is recommended to directly refer to the [MCP Configuration Guide](./mcp-server/MCP_CONFIG.md) for configuration.\n\nConfigure the MCP client to connect to the server:\n```json\n{\n  \"mcpServers\": {\n    \"weknora\": {\n      \"args\": [\n        \"path/to/WeKnora/mcp-server/run_server.py\"\n      ],\n      \"command\": \"python\",\n      \"env\":{\n        \"WEKNORA_API_KEY\":\"Enter your WeKnora instance, open developer tools, check the request header x-api-key starting with sk\",\n        \"WEKNORA_BASE_URL\":\"http(s)://your-weknora-address/api/v1\"\n      }\n    }\n  }\n}\n```\n\nRun directly using stdio command:\n```\npip install weknora-mcp-server\npython -m weknora-mcp-server\n```\n\n## ğŸ”§ Initialization Configuration Guide\n\nTo help users quickly configure various models and reduce trial-and-error costs, we've improved the original configuration file initialization method by adding a Web UI interface for model configuration. Before using, please ensure the code is updated to the latest version. The specific steps are as follows:\nIf this is your first time using this project, you can skip steps â‘ â‘¡ and go directly to steps â‘¢â‘£.\n\n### â‘  Stop the services\n\n```bash\n./scripts/start_all.sh --stop\n```\n\n### â‘¡ Clear existing data tables (recommended when no important data exists)\n\n```bash\nmake clean-db\n```\n\n### â‘¢ Compile and start services\n\n```bash\n./scripts/start_all.sh\n```\n\n### â‘£ Access Web UI\n\nhttp://localhost\n\nOn your first visit, you will be automatically redirected to the registration/login page. After completing registration, please create a new knowledge base and finish the relevant settings on its configuration page.\n\n## ğŸ“± Interface Showcase\n\n### Web UI Interface\n\n<table>\n  <tr>\n    <td><b>Knowledge Base Management</b><br/><img src=\"./docs/images/knowledgebases.png\" alt=\"Knowledge Base Management\"></td>\n    <td><b>Conversation Settings</b><br/><img src=\"./docs/images/settings.png\" alt=\"Conversation Settings\"></td>\n  </tr>\n  <tr>\n    <td colspan=\"2\"><b>Agent Mode Tool Call Process</b><br/><img src=\"./docs/images/agent-qa.png\" alt=\"Agent Mode Tool Call Process\"></td>\n  </tr>\n</table>\n\n**Knowledge Base Management:** Support for creating FAQ and document knowledge base types, with multiple import methods including drag-and-drop, folder import, and URL import. Automatically identifies document structures and extracts core knowledge to establish indexes. Supports tag management and online entry. The system clearly displays processing progress and document status, achieving efficient knowledge base management.\n\n**Agent Mode:** Support for ReACT Agent mode that can use built-in tools to retrieve knowledge bases, call user-configured MCP tools and web search tools to access external services, providing comprehensive summary reports through multiple iterations and reflection. Supports cross-knowledge base retrieval, allowing selection of multiple knowledge bases for simultaneous retrieval.\n\n**Conversation Strategy:** Support for configuring Agent models, normal mode models, retrieval thresholds, and online Prompt configuration, with precise control over multi-turn conversation behavior and retrieval execution methods. The conversation input box supports Agent mode/normal mode switching, enabling/disabling web search, and selecting conversation models.\n\n### Document Knowledge Graph\n\nWeKnora supports transforming documents into knowledge graphs, displaying the relationships between different sections of the documents. Once the knowledge graph feature is enabled, the system analyzes and constructs an internal semantic association network that not only helps users understand document content but also provides structured support for indexing and retrieval, enhancing the relevance and breadth of search results.\n\nFor detailed configuration, please refer to the [Knowledge Graph Configuration Guide](./docs/KnowledgeGraph.md).\n\n### MCP Server\n\nPlease refer to the [MCP Configuration Guide](./mcp-server/MCP_CONFIG.md) for the necessary setup.\n\n## ğŸ“˜ API Reference\n\nTroubleshooting FAQ: [Troubleshooting FAQ](./docs/QA.md)\n\nDetailed API documentation is available at: [API Docs](./docs/api/README.md)\n\n## ğŸ§­ Developer Guide\n\n### âš¡ Fast Development Mode (Recommended)\n\nIf you need to frequently modify code, **you don't need to rebuild Docker images every time**! Use fast development mode:\n\n```bash\n# Method 1: Using Make commands (Recommended)\nmake dev-start      # Start infrastructure\nmake dev-app        # Start backend (new terminal)\nmake dev-frontend   # Start frontend (new terminal)\n\n# Method 2: One-click start\n./scripts/quick-dev.sh\n\n# Method 3: Using scripts\n./scripts/dev.sh start     # Start infrastructure\n./scripts/dev.sh app       # Start backend (new terminal)\n./scripts/dev.sh frontend  # Start frontend (new terminal)\n```\n\n**Development Advantages:**\n- âœ… Frontend modifications auto hot-reload (no restart needed)\n- âœ… Backend modifications quick restart (5-10 seconds, supports Air hot-reload)\n- âœ… No need to rebuild Docker images\n- âœ… Support IDE breakpoint debugging\n\n**Detailed Documentation:** [Development Environment Quick Start](./docs/å¼€å‘æŒ‡å—.md)\n\n### ğŸ“ Directory Structure\n\n```\nWeKnora/\nâ”œâ”€â”€ client/      # go client\nâ”œâ”€â”€ cmd/         # Main entry point\nâ”œâ”€â”€ config/      # Configuration files\nâ”œâ”€â”€ docker/      # docker images files\nâ”œâ”€â”€ docreader/   # Document parsing app\nâ”œâ”€â”€ docs/        # Project documentation\nâ”œâ”€â”€ frontend/    # Frontend app\nâ”œâ”€â”€ internal/    # Core business logic\nâ”œâ”€â”€ mcp-server/  # MCP server\nâ”œâ”€â”€ migrations/  # DB migration scripts\nâ””â”€â”€ scripts/     # Shell scripts\n```\n\n## ğŸ¤ Contributing\n\nWe welcome community contributions! For suggestions, bugs, or feature requests, please submit an [Issue](https://github.com/Tencent/WeKnora/issues) or directly create a Pull Request.\n\n### ğŸ¯ How to Contribute\n\n- ğŸ› **Bug Fixes**: Discover and fix system defects\n- âœ¨ **New Features**: Propose and implement new capabilities\n- ğŸ“š **Documentation**: Improve project documentation\n- ğŸ§ª **Test Cases**: Write unit and integration tests\n- ğŸ¨ **UI/UX Enhancements**: Improve user interface and experience\n\n### ğŸ“‹ Contribution Process\n\n1. **Fork the project** to your GitHub account\n2. **Create a feature branch** `git checkout -b feature/amazing-feature`\n3. **Commit changes** `git commit -m 'Add amazing feature'`\n4. **Push branch** `git push origin feature/amazing-feature`\n5. **Create a Pull Request** with detailed description of changes\n\n### ğŸ¨ Code Standards\n\n- Follow [Go Code Review Comments](https://github.com/golang/go/wiki/CodeReviewComments)\n- Format code using `gofmt`\n- Add necessary unit tests\n- Update relevant documentation\n\n### ğŸ“ Commit Guidelines\n\nUse [Conventional Commits](https://www.conventionalcommits.org/) standard:\n\n```\nfeat: Add document batch upload functionality\nfix: Resolve vector retrieval precision issue\ndocs: Update API documentation\ntest: Add retrieval engine test cases\nrefactor: Restructure document parsing module\n```\n\n## ğŸ‘¥ Contributors\n\nThanks to these excellent contributors:\n\n[![Contributors](https://contrib.rocks/image?repo=Tencent/WeKnora)](https://github.com/Tencent/WeKnora/graphs/contributors)\n\n## ğŸ“„ License\n\nThis project is licensed under the [MIT License](./LICENSE).\nYou are free to use, modify, and distribute the code with proper attribution.\n\n## ğŸ“ˆ Project Statistics\n\n<a href=\"https://www.star-history.com/#Tencent/WeKnora&type=date&legend=top-left\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=Tencent/WeKnora&type=date&theme=dark&legend=top-left\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=Tencent/WeKnora&type=date&legend=top-left\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=Tencent/WeKnora&type=date&legend=top-left\" />\n </picture>\n</a>\n",
      "stars_today": 123
    },
    {
      "id": 1035029907,
      "name": "pi-mono",
      "full_name": "badlogic/pi-mono",
      "description": "AI agent toolkit: coding agent CLI, unified LLM API, TUI & web UI libraries, Slack bot, vLLM pods",
      "html_url": "https://github.com/badlogic/pi-mono",
      "stars": 2751,
      "forks": 340,
      "language": "TypeScript",
      "topics": [],
      "created_at": "2025-08-09T14:03:50Z",
      "updated_at": "2026-01-28T02:14:21Z",
      "pushed_at": "2026-01-28T02:03:15Z",
      "open_issues": 24,
      "owner": {
        "login": "badlogic",
        "avatar_url": "https://avatars.githubusercontent.com/u/514052?v=4"
      },
      "readme": "<p align=\"center\">\n  <a href=\"https://shittycodingagent.ai\">\n    <img src=\"https://shittycodingagent.ai/logo.svg\" alt=\"pi logo\" width=\"128\">\n  </a>\n</p>\n<p align=\"center\">\n  <a href=\"https://discord.com/invite/nKXTsAcmbT\"><img alt=\"Discord\" src=\"https://img.shields.io/badge/discord-community-5865F2?style=flat-square&logo=discord&logoColor=white\" /></a>\n  <a href=\"https://github.com/badlogic/pi-mono/actions/workflows/ci.yml\"><img alt=\"Build status\" src=\"https://img.shields.io/github/actions/workflow/status/badlogic/pi-mono/ci.yml?style=flat-square&branch=main\" /></a>\n</p>\n\n# Pi Monorepo\n\n> **Looking for the pi coding agent?** See **[packages/coding-agent](packages/coding-agent)** for installation and usage.\n\nTools for building AI agents and managing LLM deployments.\n\n## Packages\n\n| Package | Description |\n|---------|-------------|\n| **[@mariozechner/pi-ai](packages/ai)** | Unified multi-provider LLM API (OpenAI, Anthropic, Google, etc.) |\n| **[@mariozechner/pi-agent-core](packages/agent)** | Agent runtime with tool calling and state management |\n| **[@mariozechner/pi-coding-agent](packages/coding-agent)** | Interactive coding agent CLI |\n| **[@mariozechner/pi-mom](packages/mom)** | Slack bot that delegates messages to the pi coding agent |\n| **[@mariozechner/pi-tui](packages/tui)** | Terminal UI library with differential rendering |\n| **[@mariozechner/pi-web-ui](packages/web-ui)** | Web components for AI chat interfaces |\n| **[@mariozechner/pi-pods](packages/pods)** | CLI for managing vLLM deployments on GPU pods |\n\n## Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for contribution guidelines and [AGENTS.md](AGENTS.md) for project-specific rules (for both humans and agents).\n\n## Development\n\n```bash\nnpm install          # Install all dependencies\nnpm run build        # Build all packages\nnpm run check        # Lint, format, and type check\n./test.sh            # Run tests (skips LLM-dependent tests without API keys)\n./pi-test.sh         # Run pi from sources (must be run from repo root)\n```\n\n> **Note:** `npm run check` requires `npm run build` to be run first. The web-ui package uses `tsc` which needs compiled `.d.ts` files from dependencies.\n\n## License\n\nMIT",
      "stars_today": 118
    },
    {
      "id": 699532645,
      "name": "uv",
      "full_name": "astral-sh/uv",
      "description": "An extremely fast Python package and project manager, written in Rust.",
      "html_url": "https://github.com/astral-sh/uv",
      "stars": 77949,
      "forks": 2490,
      "language": "Rust",
      "topics": [
        "packaging",
        "python",
        "resolver",
        "uv"
      ],
      "created_at": "2023-10-02T20:24:11Z",
      "updated_at": "2026-01-28T02:05:08Z",
      "pushed_at": "2026-01-28T02:02:15Z",
      "open_issues": 2590,
      "owner": {
        "login": "astral-sh",
        "avatar_url": "https://avatars.githubusercontent.com/u/115962839?v=4"
      },
      "readme": "# uv\n\n[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)\n[![image](https://img.shields.io/pypi/v/uv.svg)](https://pypi.python.org/pypi/uv)\n[![image](https://img.shields.io/pypi/l/uv.svg)](https://pypi.python.org/pypi/uv)\n[![image](https://img.shields.io/pypi/pyversions/uv.svg)](https://pypi.python.org/pypi/uv)\n[![Actions status](https://github.com/astral-sh/uv/actions/workflows/ci.yml/badge.svg)](https://github.com/astral-sh/uv/actions)\n[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&logoColor=white)](https://discord.gg/astral-sh)\n\nAn extremely fast Python package and project manager, written in Rust.\n\n<p align=\"center\">\n  <picture align=\"center\">\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/astral-sh/uv/assets/1309177/03aa9163-1c79-4a87-a31d-7a9311ed9310\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github.com/astral-sh/uv/assets/1309177/629e59c0-9c6e-4013-9ad4-adb2bcf5080d\">\n    <img alt=\"Shows a bar chart with benchmark results.\" src=\"https://github.com/astral-sh/uv/assets/1309177/629e59c0-9c6e-4013-9ad4-adb2bcf5080d\">\n  </picture>\n</p>\n\n<p align=\"center\">\n  <i>Installing <a href=\"https://trio.readthedocs.io/\">Trio</a>'s dependencies with a warm cache.</i>\n</p>\n\n## Highlights\n\n- A single tool to replace `pip`, `pip-tools`, `pipx`, `poetry`, `pyenv`, `twine`, `virtualenv`, and\n  more.\n- [10-100x faster](https://github.com/astral-sh/uv/blob/main/BENCHMARKS.md) than `pip`.\n- Provides [comprehensive project management](#projects), with a\n  [universal lockfile](https://docs.astral.sh/uv/concepts/projects/layout#the-lockfile).\n- [Runs scripts](#scripts), with support for\n  [inline dependency metadata](https://docs.astral.sh/uv/guides/scripts#declaring-script-dependencies).\n- [Installs and manages](#python-versions) Python versions.\n- [Runs and installs](#tools) tools published as Python packages.\n- Includes a [pip-compatible interface](#the-pip-interface) for a performance boost with a familiar\n  CLI.\n- Supports Cargo-style [workspaces](https://docs.astral.sh/uv/concepts/projects/workspaces) for\n  scalable projects.\n- Disk-space efficient, with a [global cache](https://docs.astral.sh/uv/concepts/cache) for\n  dependency deduplication.\n- Installable without Rust or Python via `curl` or `pip`.\n- Supports macOS, Linux, and Windows.\n\nuv is backed by [Astral](https://astral.sh), the creators of\n[Ruff](https://github.com/astral-sh/ruff) and [ty](https://github.com/astral-sh/ty).\n\n## Installation\n\nInstall uv with our standalone installers:\n\n```bash\n# On macOS and Linux.\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n\n```bash\n# On Windows.\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n\nOr, from [PyPI](https://pypi.org/project/uv/):\n\n```bash\n# With pip.\npip install uv\n```\n\n```bash\n# Or pipx.\npipx install uv\n```\n\nIf installed via the standalone installer, uv can update itself to the latest version:\n\n```bash\nuv self update\n```\n\nSee the [installation documentation](https://docs.astral.sh/uv/getting-started/installation/) for\ndetails and alternative installation methods.\n\n## Documentation\n\nuv's documentation is available at [docs.astral.sh/uv](https://docs.astral.sh/uv).\n\nAdditionally, the command line reference documentation can be viewed with `uv help`.\n\n## Features\n\n### Projects\n\nuv manages project dependencies and environments, with support for lockfiles, workspaces, and more,\nsimilar to `rye` or `poetry`:\n\n```console\n$ uv init example\nInitialized project `example` at `/home/user/example`\n\n$ cd example\n\n$ uv add ruff\nCreating virtual environment at: .venv\nResolved 2 packages in 170ms\n   Built example @ file:///home/user/example\nPrepared 2 packages in 627ms\nInstalled 2 packages in 1ms\n + example==0.1.0 (from file:///home/user/example)\n + ruff==0.5.0\n\n$ uv run ruff check\nAll checks passed!\n\n$ uv lock\nResolved 2 packages in 0.33ms\n\n$ uv sync\nResolved 2 packages in 0.70ms\nAudited 1 package in 0.02ms\n```\n\nSee the [project documentation](https://docs.astral.sh/uv/guides/projects/) to get started.\n\nuv also supports building and publishing projects, even if they're not managed with uv. See the\n[publish guide](https://docs.astral.sh/uv/guides/publish/) to learn more.\n\n### Scripts\n\nuv manages dependencies and environments for single-file scripts.\n\nCreate a new script and add inline metadata declaring its dependencies:\n\n```console\n$ echo 'import requests; print(requests.get(\"https://astral.sh\"))' > example.py\n\n$ uv add --script example.py requests\nUpdated `example.py`\n```\n\nThen, run the script in an isolated virtual environment:\n\n```console\n$ uv run example.py\nReading inline script metadata from: example.py\nInstalled 5 packages in 12ms\n<Response [200]>\n```\n\nSee the [scripts documentation](https://docs.astral.sh/uv/guides/scripts/) to get started.\n\n### Tools\n\nuv executes and installs command-line tools provided by Python packages, similar to `pipx`.\n\nRun a tool in an ephemeral environment using `uvx` (an alias for `uv tool run`):\n\n```console\n$ uvx pycowsay 'hello world!'\nResolved 1 package in 167ms\nInstalled 1 package in 9ms\n + pycowsay==0.0.0.2\n  \"\"\"\n\n  ------------\n< hello world! >\n  ------------\n   \\   ^__^\n    \\  (oo)\\_______\n       (__)\\       )\\/\\\n           ||----w |\n           ||     ||\n```\n\nInstall a tool with `uv tool install`:\n\n```console\n$ uv tool install ruff\nResolved 1 package in 6ms\nInstalled 1 package in 2ms\n + ruff==0.5.0\nInstalled 1 executable: ruff\n\n$ ruff --version\nruff 0.5.0\n```\n\nSee the [tools documentation](https://docs.astral.sh/uv/guides/tools/) to get started.\n\n### Python versions\n\nuv installs Python and allows quickly switching between versions.\n\nInstall multiple Python versions:\n\n```console\n$ uv python install 3.12 3.13 3.14\nInstalled 3 versions in 972ms\n + cpython-3.12.12-macos-aarch64-none (python3.12)\n + cpython-3.13.9-macos-aarch64-none (python3.13)\n + cpython-3.14.0-macos-aarch64-none (python3.14)\n\n```\n\nDownload Python versions as needed:\n\n```console\n$ uv venv --python 3.12.0\nUsing Python 3.12.0\nCreating virtual environment at: .venv\nActivate with: source .venv/bin/activate\n\n$ uv run --python pypy@3.8 -- python --version\nPython 3.8.16 (a9dbdca6fc3286b0addd2240f11d97d8e8de187a, Dec 29 2022, 11:45:30)\n[PyPy 7.3.11 with GCC Apple LLVM 13.1.6 (clang-1316.0.21.2.5)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>>\n```\n\nUse a specific Python version in the current directory:\n\n```console\n$ uv python pin 3.11\nPinned `.python-version` to `3.11`\n```\n\nSee the [Python installation documentation](https://docs.astral.sh/uv/guides/install-python/) to get\nstarted.\n\n### The pip interface\n\nuv provides a drop-in replacement for common `pip`, `pip-tools`, and `virtualenv` commands.\n\nuv extends their interfaces with advanced features, such as dependency version overrides,\nplatform-independent resolutions, reproducible resolutions, alternative resolution strategies, and\nmore.\n\nMigrate to uv without changing your existing workflows â€” and experience a 10-100x speedup â€” with the\n`uv pip` interface.\n\nCompile requirements into a platform-independent requirements file:\n\n```console\n$ uv pip compile docs/requirements.in \\\n   --universal \\\n   --output-file docs/requirements.txt\nResolved 43 packages in 12ms\n```\n\nCreate a virtual environment:\n\n```console\n$ uv venv\nUsing Python 3.12.3\nCreating virtual environment at: .venv\nActivate with: source .venv/bin/activate\n```\n\nInstall the locked requirements:\n\n```console\n$ uv pip sync docs/requirements.txt\nResolved 43 packages in 11ms\nInstalled 43 packages in 208ms\n + babel==2.15.0\n + black==24.4.2\n + certifi==2024.7.4\n ...\n```\n\nSee the [pip interface documentation](https://docs.astral.sh/uv/pip/index/) to get started.\n\n## Contributing\n\nWe are passionate about supporting contributors of all levels of experience and would love to see\nyou get involved in the project. See the\n[contributing guide](https://github.com/astral-sh/uv?tab=contributing-ov-file#contributing) to get\nstarted.\n\n## FAQ\n\n#### How do you pronounce uv?\n\nIt's pronounced as \"you - vee\" ([`/juË viË/`](https://en.wikipedia.org/wiki/Help:IPA/English#Key))\n\n#### How should I stylize uv?\n\nJust \"uv\", please. See the [style guide](./STYLE.md#styling-uv) for details.\n\n#### What platforms does uv support?\n\nSee uv's [platform support](https://docs.astral.sh/uv/reference/platforms/) document.\n\n#### Is uv ready for production?\n\nYes, uv is stable and widely used in production. See uv's\n[versioning policy](https://docs.astral.sh/uv/reference/versioning/) document for details.\n\n## Acknowledgements\n\nuv's dependency resolver uses [PubGrub](https://github.com/pubgrub-rs/pubgrub) under the hood. We're\ngrateful to the PubGrub maintainers, especially [Jacob Finkelman](https://github.com/Eh2406), for\ntheir support.\n\nuv's Git implementation is based on [Cargo](https://github.com/rust-lang/cargo).\n\nSome of uv's optimizations are inspired by the great work we've seen in [pnpm](https://pnpm.io/),\n[Orogene](https://github.com/orogene/orogene), and [Bun](https://github.com/oven-sh/bun). We've also\nlearned a lot from Nathaniel J. Smith's [Posy](https://github.com/njsmith/posy) and adapted its\n[trampoline](https://github.com/njsmith/posy/tree/main/src/trampolines/windows-trampolines/posy-trampoline)\nfor Windows support.\n\n## License\n\nuv is licensed under either of\n\n- Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or\n  <https://www.apache.org/licenses/LICENSE-2.0>)\n- MIT license ([LICENSE-MIT](LICENSE-MIT) or <https://opensource.org/licenses/MIT>)\n\nat your option.\n\nUnless you explicitly state otherwise, any contribution intentionally submitted for inclusion in uv\nby you, as defined in the Apache-2.0 license, shall be dually licensed as above, without any\nadditional terms or conditions.\n\n<div align=\"center\">\n  <a target=\"_blank\" href=\"https://astral.sh\" style=\"background:none\">\n    <img src=\"https://raw.githubusercontent.com/astral-sh/uv/main/assets/svg/Astral.svg\" alt=\"Made by Astral\">\n  </a>\n</div>\n",
      "stars_today": 112
    },
    {
      "id": 1012625755,
      "name": "git-ai",
      "full_name": "git-ai-project/git-ai",
      "description": "A Git extension for tracking the AI-generated code in your repos",
      "html_url": "https://github.com/git-ai-project/git-ai",
      "stars": 722,
      "forks": 48,
      "language": "Rust",
      "topics": [
        "ai",
        "ai-blame",
        "coding-agents"
      ],
      "created_at": "2025-07-02T16:09:26Z",
      "updated_at": "2026-01-28T02:04:42Z",
      "pushed_at": "2026-01-28T02:04:37Z",
      "open_issues": 72,
      "owner": {
        "login": "git-ai-project",
        "avatar_url": "https://avatars.githubusercontent.com/u/238678734?v=4"
      },
      "readme": "<div>\n<img src=\"https://github.com/acunniffe/git-ai/raw/main/assets/docs/git-ai.png\" align=\"right\"\n     alt=\"Git AI by acunniffe/git-ai\" width=\"100\" height=\"100\" />\n\n</div>\n<div>\n<h1 align=\"left\"><b>git-ai</b></h1>\n</div>\n<p align=\"left\">Track the AI Code in your repositories</p>\n\n<video src=\"https://github.com/user-attachments/assets/68304ca6-b262-4638-9fb6-0a26f55c7986\" muted loop controls autoplay></video>\n\n## Quick Start\n\n#### Mac, Linux, Windows (WSL)\n\n```bash\ncurl -sSL https://usegitai.com/install.sh | bash\n```\n\n#### Windows (non-WSL)\n\n```powershell\npowershell -NoProfile -ExecutionPolicy Bypass -Command \"irm http://usegitai.com/install.ps1 | iex\"\n```\n\nğŸŠ That's it! **No per-repo setup.** Once installed Git AI will work OOTB with any of these **Supported Agents**:\n\n<img src=\"https://github.com/acunniffe/git-ai/raw/main/assets/docs/supported-agents.png\" width=\"320\" />\n\n### Documentation https://usegitai.com/docs\n- [AI Blame](https://usegitai.com/docs/cli/ai-blame)\n- [Cross-Agent Prompt Saving](https://usegitai.com/docs/cli/prompt-storage)\n- [CLI Reference](https://usegitai.com/docs/cli/reference)\n- [Configuring Git AI for the enterprise](https://usegitai.com/docs/cli/configuration)\n\n### Just Install and Commit\n\nBuild as usual. Just prompt, edit and commit. Git AI will track every line of AI-Code and record the Coding Agent, Model, and prompt that generated it. \n\n<img src=\"https://github.com/acunniffe/git-ai/raw/main/assets/docs/graph.jpg\" width=\"400\" />\n\n#### How Does it work? \n\nSupported Coding Agents call Git AI and mark the lines they insert as AI-generated. \n\nOn commit, Git AI saves the final AI-attributions into a Git Note. These notes power AI-Blame, AI contribution stats, and more. The CLI makes sure these notes are preserved through rebases, merges, squashes, cherry-picks, etc.\n\n![Git Tree](https://github.com/user-attachments/assets/edd20990-ec0b-4a53-afa4-89fa33de9541)\n\nThe format of the notes is outlined here in the [Git AI Standard v3.0.0](https://github.com/git-ai-project/git-ai/blob/main/specs/git_ai_standard_v3.0.0.md)\n\n## Goals of `git-ai` project\n\nğŸ¤– **Track AI code in a Multi-Agent** world. Because developers get to choose their tools, engineering teams need a **vendor agnostic** way to track AI impact in their repos.\n\nğŸ¯ **Accurate attribution** from Laptop â†’ Pull Request â†’ Merged. Claude Code, Cursor and Copilot cannot track code after generationâ€”Git AI follows it through the entire workflow.\n\nğŸ”„ **Support real-world git workflows** by making sure AI-Authorship annotations survive a `merge --squash`, `rebase`, `reset`, `cherry-pick` etc.\n\nğŸ”— **Maintain link between prompts and code** - there is valuable context and requirements in team promptsâ€”preserve them alongside code.\n\nğŸš€ **Git-native + Fast** - `git-ai` is built on git plumbing commands. Negligible impact even in large repos (&lt;100ms). Tested in [Chromium](https://github.com/chromium/chromium).\n\n## Agent Support\n\n`git-ai` automatically sets up all supported agent hooks using the `git-ai install-hooks` command\n\n| Agent/IDE                                                                                  | Authorship | Prompts |\n| ------------------------------------------------------------------------------------------ | ---------- | ------- |\n| Cursor &gt;1.7                                                                             | âœ…         | âœ…      |\n| Claude Code                                                                                | âœ…         | âœ…      |\n| GitHub Copilot in VSCode via Extension                                                     | âœ…         | âœ…      |\n| Google Gemini CLI                                                                          | âœ…         | âœ…      |\n| Continue CLI                                                                               | âœ…         | âœ…      |\n| OpenCode                                                                                   | âœ…         | âœ…      |\n| Atlassian RovoDev CLI                                                                      | âœ…         | âœ…      |\n| GitHub Copilot in Jetbrains IDEs (IntelliJ, etc.)                                          | ğŸ”„         | ğŸ”„      |\n| Jetbrains Junie                                                                            | ğŸ”„         | ğŸ”„      |\n| AWS Kiro (in-progress)                                                                     | ğŸ”„         | ğŸ”„      |\n| Continue VS Code/IntelliJ (in-progress)                                                    | ğŸ”„         | ğŸ”„      |\n| Windsurf                                                                                   | ğŸ”„         | ğŸ”„      |\n| Augment Code                                                                               | ğŸ”„         | ğŸ”„      |\n| OpenAI Codex (waiting on [openai/codex #2109](https://github.com/openai/codex/issues/2109)) |            |         |\n| Ona                                                                                        |            |         |\n| Sourcegraph Cody + Amp                                                                     |            |         |\n| Google Antigravity                                                                         |            |         |\n\n\n> **Building a Coding Agent?** [Add support for Git AI by following this guide](https://usegitai.com/docs/cli/add-your-agent)\n\n## Installing the Stats Bot (early access)\n\nAggregate `git-ai` data at the PR, developer, Repository and Organization levels:\n\n- AI authorship breakdown for every Pull Request\n- Measure % of code that is AI generated through the entire SDLC\n- Compare accepted-rate for code written by each Agent + Model. \n- AI-Code Halflife (how durable is the AI code)\n> [Get early access by chatting with the maintainers](https://calendly.com/acunniffe/meeting-with-git-ai-authors)\n\n![alt](https://github.com/acunniffe/git-ai/raw/main/assets/docs/dashboard.png)\n\n",
      "stars_today": 101
    },
    {
      "id": 805155266,
      "name": "cherry-studio",
      "full_name": "CherryHQ/cherry-studio",
      "description": "AI Agent + Coding Agent + 300+ assistants: agentic AI desktop with autonomous coding, intelligent automation, and unified access to frontier LLMs.",
      "html_url": "https://github.com/CherryHQ/cherry-studio",
      "stars": 38617,
      "forks": 3558,
      "language": "TypeScript",
      "topics": [
        "ai-agent",
        "claude-code",
        "clawdbot",
        "code-agent",
        "codex",
        "moltbot",
        "opencode",
        "skills",
        "supermemory",
        "vibe-coding"
      ],
      "created_at": "2024-05-24T01:56:26Z",
      "updated_at": "2026-01-28T02:14:10Z",
      "pushed_at": "2026-01-27T19:11:05Z",
      "open_issues": 630,
      "owner": {
        "login": "CherryHQ",
        "avatar_url": "https://avatars.githubusercontent.com/u/187777663?v=4"
      },
      "readme": "<div align=\"right\" >\n  <details>\n    <summary >ğŸŒ Language</summary>\n    <div>\n      <div align=\"right\">\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=en\">English</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=zh-CN\">ç®€ä½“ä¸­æ–‡</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=zh-TW\">ç¹é«”ä¸­æ–‡</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=ja\">æ—¥æœ¬èª</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=ko\">í•œêµ­ì–´</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=hi\">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=th\">à¹„à¸—à¸¢</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=fr\">FranÃ§ais</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=de\">Deutsch</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=es\">EspaÃ±ol</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=it\">Italiano</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=ru\">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=pt\">PortuguÃªs</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=nl\">Nederlands</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=pl\">Polski</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=ar\">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=fa\">ÙØ§Ø±Ø³ÛŒ</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=tr\">TÃ¼rkÃ§e</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=vi\">Tiáº¿ng Viá»‡t</a></p>\n        <p><a href=\"https://openaitx.github.io/view.html?user=CherryHQ&project=cherry-studio&lang=id\">Bahasa Indonesia</a></p>\n      </div>\n    </div>\n  </details>\n</div>\n\n<h1 align=\"center\">\n  <a href=\"https://github.com/CherryHQ/cherry-studio/releases\">\n    <img src=\"https://github.com/CherryHQ/cherry-studio/blob/main/build/icon.png?raw=true\" width=\"150\" height=\"150\" alt=\"banner\" /><br>\n  </a>\n</h1>\n\n<p align=\"center\">English | <a href=\"./docs/zh/README.md\">ä¸­æ–‡</a> | <a href=\"https://cherry-ai.com\">Official Site</a> | <a href=\"https://docs.cherry-ai.com/docs/en-us\">Documents</a> | <a href=\"./docs/en/guides/development.md\">Development</a> | <a href=\"https://github.com/CherryHQ/cherry-studio/issues\">Feedback</a><br></p>\n\n<div align=\"center\">\n\n[![][deepwiki-shield]][deepwiki-link]\n[![][twitter-shield]][twitter-link]\n[![][discord-shield]][discord-link]\n[![][telegram-shield]][telegram-link]\n\n</div>\n<div align=\"center\">\n\n[![][github-release-shield]][github-release-link]\n[![][github-nightly-shield]][github-nightly-link]\n[![][github-contributors-shield]][github-contributors-link]\n[![][license-shield]][license-link]\n[![][commercial-shield]][commercial-link]\n[![][sponsor-shield]][sponsor-link]\n\n</div>\n\n<div align=\"center\">\n <a href=\"https://hellogithub.com/repository/1605492e1e2a4df3be07abfa4578dd37\" target=\"_blank\" style=\"text-decoration: none\"><img src=\"https://api.hellogithub.com/v1/widgets/recommend.svg?rid=1605492e1e2a4df3be07abfa4578dd37\" alt=\"Featuredï½œHelloGitHub\"  width=\"220\" height=\"55\" /></a>\n <a href=\"https://trendshift.io/repositories/14318\" target=\"_blank\" style=\"text-decoration: none\"><img src=\"https://trendshift.io/api/badge/repositories/14318\" alt=\"CherryHQ%2Fcherry-studio | Trendshift\" width=\"220\" height=\"55\" /></a>\n <a href=\"https://www.producthunt.com/posts/cherry-studio?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-cherry&#0045;studio\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=496640&theme=light\" alt=\"Cherry&#0032;Studio - AI&#0032;Chatbots&#0044;&#0032;AI&#0032;Desktop&#0032;Client | Product Hunt\" width=\"220\" height=\"55\" /></a>\n</div>\n\n# ğŸ’ Cherry Studio\n\nCherry Studio is a desktop client that supports multiple LLM providers, available on Windows, Mac and Linux.\n\nğŸ‘ Join [Telegram Group](https://t.me/CherryStudioAI)ï½œ[Discord](https://discord.gg/wez8HtpxqQ) | [QQ Group(575014769)](https://qm.qq.com/q/lo0D4qVZKi)\n\nâ¤ï¸ Like Cherry Studio? Give it a star ğŸŒŸ or [Sponsor](docs/zh/guides/sponsor.md) to support the development!\n\n# ğŸŒ  Screenshot\n\n![](https://github.com/user-attachments/assets/36dddb2c-e0fb-4a5f-9411-91447bab6e18)\n\n![](https://github.com/user-attachments/assets/f549e8a0-2385-40b4-b52b-2039e39f2930)\n\n![](https://github.com/user-attachments/assets/58e0237c-4d36-40de-b428-53051d982026)\n\n# ğŸŒŸ Key Features\n\n1. **Diverse LLM Provider Support**:\n\n- â˜ï¸ Major LLM Cloud Services: OpenAI, Gemini, Anthropic, and more\n- ğŸ”— AI Web Service Integration: Claude, Perplexity, [Poe](https://poe.com/), and others\n- ğŸ’» Local Model Support with Ollama, LM Studio\n\n2. **AI Assistants & Conversations**:\n\n- ğŸ“š 300+ Pre-configured AI Assistants\n- ğŸ¤– Custom Assistant Creation\n- ğŸ’¬ Multi-model Simultaneous Conversations\n\n3. **Document & Data Processing**:\n\n- ğŸ“„ Supports Text, Images, Office, PDF, and more\n- â˜ï¸ WebDAV File Management and Backup\n- ğŸ“Š Mermaid Chart Visualization\n- ğŸ’» Code Syntax Highlighting\n\n4. **Practical Tools Integration**:\n\n- ğŸ” Global Search Functionality\n- ğŸ“ Topic Management System\n- ğŸ”¤ AI-powered Translation\n- ğŸ¯ Drag-and-drop Sorting\n- ğŸ”Œ Mini Program Support\n- âš™ï¸ MCP(Model Context Protocol) Server\n\n5. **Enhanced User Experience**:\n\n- ğŸ–¥ï¸ Cross-platform Support for Windows, Mac, and Linux\n- ğŸ“¦ Ready to Use - No Environment Setup Required\n- ğŸ¨ Light/Dark Themes and Transparent Window\n- ğŸ“ Complete Markdown Rendering\n- ğŸ¤² Easy Content Sharing\n\n# ğŸ“ Roadmap\n\nWe're actively working on the following features and improvements:\n\n1. ğŸ¯ **Core Features**\n\n- Selection Assistant with smart content selection enhancement\n- Deep Research with advanced research capabilities\n- Memory System with global context awareness\n- Document Preprocessing with improved document handling\n- MCP Marketplace for Model Context Protocol ecosystem\n\n2. ğŸ—‚ **Knowledge Management**\n\n- Notes and Collections\n- Dynamic Canvas visualization\n- OCR capabilities\n- TTS (Text-to-Speech) support\n\n3. ğŸ“± **Platform Support**\n\n- HarmonyOS Edition (PC)\n- Android App (Phase 1)\n- iOS App (Phase 1)\n- Multi-Window support\n- Window Pinning functionality\n- Intel AI PC (Core Ultra) Support\n\n4. ğŸ”Œ **Advanced Features**\n\n- Plugin System\n- ASR (Automatic Speech Recognition)\n- Assistant and Topic Interaction Refactoring\n\nTrack our progress and contribute on our [project board](https://github.com/orgs/CherryHQ/projects/7).\n\nWant to influence our roadmap? Join our [GitHub Discussions](https://github.com/CherryHQ/cherry-studio/discussions) to share your ideas and feedback!\n\n# ğŸŒˆ Theme\n\n- Theme Gallery: <https://cherrycss.com>\n- Aero Theme: <https://github.com/hakadao/CherryStudio-Aero>\n- PaperMaterial Theme: <https://github.com/rainoffallingstar/CherryStudio-PaperMaterial>\n- Claude dynamic-style: <https://github.com/bjl101501/CherryStudio-Claudestyle-dynamic>\n- Maple Neon Theme: <https://github.com/BoningtonChen/CherryStudio_themes>\n\nWelcome PR for more themes\n\n# ğŸ¤ Contributing\n\nWe welcome contributions to Cherry Studio! Here are some ways you can contribute:\n\n1. **Contribute Code**: Develop new features or optimize existing code.\n2. **Fix Bugs**: Submit fixes for any bugs you find.\n3. **Maintain Issues**: Help manage GitHub issues.\n4. **Product Design**: Participate in design discussions.\n5. **Write Documentation**: Improve user manuals and guides.\n6. **Community Engagement**: Join discussions and help users.\n7. **Promote Usage**: Spread the word about Cherry Studio.\n\nRefer to the [Branching Strategy](docs/en/guides/branching-strategy.md) for contribution guidelines\n\n## Getting Started\n\n1. **Fork the Repository**: Fork and clone it to your local machine.\n2. **Create a Branch**: For your changes.\n3. **Submit Changes**: Commit and push your changes.\n4. **Open a Pull Request**: Describe your changes and reasons.\n\nFor more detailed guidelines, please refer to our [Contributing Guide](CONTRIBUTING.md).\n\nThank you for your support and contributions!\n\n# ğŸ”§ Developer Co-creation Program\n\nWe are launching the Cherry Studio Developer Co-creation Program to foster a healthy and positive-feedback loop within the open-source ecosystem. We believe that great software is built collaboratively, and every merged pull request breathes new life into the project.\n\nWe sincerely invite you to join our ranks of contributors and shape the future of Cherry Studio with us.\n\n## Contributor Rewards Program\n\nTo give back to our core contributors and create a virtuous cycle, we have established the following long-term incentive plan.\n\n**The inaugural tracking period for this program will be Q3 2025 (July, August, September). Rewards for this cycle will be distributed on October 1st.**\n\nWithin any tracking period (e.g., July 1st to September 30th for the first cycle), any developer who contributes more than **30 meaningful commits** to any of Cherry Studio's open-source projects on GitHub will be eligible for the following benefits:\n\n- **Cursor Subscription Sponsorship**: Receive a **$70 USD** credit or reimbursement for your [Cursor](https://cursor.sh/) subscription, making AI your most efficient coding partner.\n- **Unlimited Model Access**: Get **unlimited** API calls for the **DeepSeek** and **Qwen** models.\n- **Cutting-Edge Tech Access**: Enjoy occasional perks, including API access to models like **Claude**, **Gemini**, and **OpenAI**, keeping you at the forefront of technology.\n\n## Growing Together & Future Plans\n\nA vibrant community is the driving force behind any sustainable open-source project. As Cherry Studio grows, so will our rewards program. We are committed to continuously aligning our benefits with the best-in-class tools and resources in the industry. This ensures our core contributors receive meaningful support, creating a positive cycle where developers, the community, and the project grow together.\n\n**Moving forward, the project will also embrace an increasingly open stance to give back to the entire open-source community.**\n\n## How to Get Started?\n\nWe look forward to your first Pull Request!\n\nYou can start by exploring our repositories, picking up a `good first issue`, or proposing your own enhancements. Every commit is a testament to the spirit of open source.\n\nThank you for your interest and contributions.\n\nLet's build together.\n\n# ğŸ¢ Enterprise Edition\n\nBuilding on the Community Edition, we are proud to introduce **Cherry Studio Enterprise Edition**â€”a privately-deployable AI productivity and management platform designed for modern teams and enterprises.\n\nThe Enterprise Edition addresses core challenges in team collaboration by centralizing the management of AI resources, knowledge, and data. It empowers organizations to enhance efficiency, foster innovation, and ensure compliance, all while maintaining 100% control over their data in a secure environment.\n\n## Core Advantages\n\n- **Unified Model Management**: Centrally integrate and manage various cloud-based LLMs (e.g., OpenAI, Anthropic, Google Gemini) and locally deployed private models. Employees can use them out-of-the-box without individual configuration.\n- **Enterprise-Grade Knowledge Base**: Build, manage, and share team-wide knowledge bases. Ensures knowledge retention and consistency, enabling team members to interact with AI based on unified and accurate information.\n- **Fine-Grained Access Control**: Easily manage employee accounts and assign role-based permissions for different models, knowledge bases, and features through a unified admin backend.\n- **Fully Private Deployment**: Deploy the entire backend service on your on-premises servers or private cloud, ensuring your data remains 100% private and under your control to meet the strictest security and compliance standards.\n- **Reliable Backend Services**: Provides stable API services and enterprise-grade data backup and recovery mechanisms to ensure business continuity.\n\n## âœ¨ Online Demo\n\n**ğŸ”— [Cherry Studio Enterprise](https://www.cherry-ai.com/enterprise)**\n\n## Version Comparison\n\n| Feature           | Community Edition                                                                    | Enterprise Edition                                                                                                                      |\n| :---------------- | :----------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------- |\n| **Open Source**   | âœ… Yes                                                                               | â­•ï¸ Partially released to customers                                                                                                      |\n| **Cost**          | [AGPL-3.0 License](https://github.com/CherryHQ/cherry-studio?tab=AGPL-3.0-1-ov-file) | Buyout / Subscription Fee                                                                                                               |\n| **Admin Backend** | â€”                                                                                    | â— Centralized **Model** Access<br>â— **Employee** Management<br>â— Shared **Knowledge Base**<br>â— **Access** Control<br>â— **Data** Backup |\n| **Server**        | â€”                                                                                    | âœ… Dedicated Private Deployment                                                                                                         |\n\n## Get the Enterprise Edition\n\nWe believe the Enterprise Edition will become your team's AI productivity engine. If you are interested in Cherry Studio Enterprise Edition and would like to learn more, request a quote, or schedule a demo, please feel free to contact us.\n\n- **For Business Inquiries & Purchasing**:\n  **ğŸ“§ [bd@cherry-ai.com](mailto:bd@cherry-ai.com)**\n\n# ğŸ”— Related Projects\n\n- [new-api](https://github.com/QuantumNous/new-api): The next-generation LLM gateway and AI asset management system supports multiple languages.\n\n- [one-api](https://github.com/songquanpeng/one-api): LLM API management and distribution system supporting mainstream models like OpenAI, Azure, and Anthropic. Features a unified API interface, suitable for key management and secondary distribution.\n\n- [Poe](https://poe.com/): Poe gives you access to the best AI, all in one place. Explore GPT-5, Claude Opus 4.1, DeepSeek-R1, Veo 3, ElevenLabs, and millions of others.\n\n- [ublacklist](https://github.com/iorate/ublacklist): Blocks specific sites from appearing in Google search results\n\n# ğŸš€ Contributors\n\n<a href=\"https://github.com/CherryHQ/cherry-studio/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=CherryHQ/cherry-studio\" />\n</a>\n<br /><br />\n\n# ğŸ“Š GitHub Stats\n\n![Stats](https://repobeats.axiom.co/api/embed/a693f2e5f773eed620f70031e974552156c7f397.svg \"Repobeats analytics image\")\n\n# â­ï¸ Star History\n\n<a href=\"https://www.star-history.com/#CherryHQ/cherry-studio&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=CherryHQ/cherry-studio&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=CherryHQ/cherry-studio&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=CherryHQ/cherry-studio&type=Date\" />\n </picture>\n</a>\n\n# ğŸ“œ License\n\nThe Cherry Studio Community Edition is governed by the standard GNU Affero General Public License v3.0 (AGPL-3.0), available at https://www.gnu.org/licenses/agpl-3.0.html.\n\nUse of the Cherry Studio Community Edition for commercial purposes is permitted, subject to full compliance with the terms and conditions of the AGPL-3.0 license.\n\nShould you require a commercial license that provides an exemption from the AGPL-3.0 requirements, please contact us at bd@cherry-ai.com.\n\n<!-- Links & Images -->\n\n[deepwiki-shield]: https://img.shields.io/badge/Deepwiki-CherryHQ-0088CC?logo=data:image/svg+xml;base64,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\n[deepwiki-link]: https://deepwiki.com/CherryHQ/cherry-studio\n[twitter-shield]: https://img.shields.io/badge/Twitter-CherryStudioApp-0088CC?logo=x\n[twitter-link]: https://twitter.com/CherryStudioHQ\n[discord-shield]: https://img.shields.io/badge/Discord-@CherryStudio-0088CC?logo=discord\n[discord-link]: https://discord.gg/wez8HtpxqQ\n[telegram-shield]: https://img.shields.io/badge/Telegram-@CherryStudioAI-0088CC?logo=telegram\n[telegram-link]: https://t.me/CherryStudioAI\n\n<!-- Links & Images -->\n\n[github-release-shield]: https://img.shields.io/github/v/release/CherryHQ/cherry-studio?logo=github\n[github-release-link]: https://github.com/CherryHQ/cherry-studio/releases\n[github-nightly-shield]: https://img.shields.io/github/actions/workflow/status/CherryHQ/cherry-studio/nightly-build.yml?label=nightly%20build&logo=github\n[github-nightly-link]: https://github.com/CherryHQ/cherry-studio/actions/workflows/nightly-build.yml\n[github-contributors-shield]: https://img.shields.io/github/contributors/CherryHQ/cherry-studio?logo=github\n[github-contributors-link]: https://github.com/CherryHQ/cherry-studio/graphs/contributors\n\n<!-- Links & Images -->\n\n[license-shield]: https://img.shields.io/badge/License-AGPLv3-important.svg?logo=gnu\n[license-link]: https://www.gnu.org/licenses/agpl-3.0\n[commercial-shield]: https://img.shields.io/badge/License-Contact-white.svg?logoColor=white&logo=telegram&color=blue\n[commercial-link]: mailto:license@cherry-ai.com?subject=Commercial%20License%20Inquiry\n[sponsor-shield]: https://img.shields.io/badge/Sponsor-FF6699.svg?logo=githubsponsors&logoColor=white\n[sponsor-link]: https://github.com/CherryHQ/cherry-studio/blob/main/docs/sponsor.md\n",
      "stars_today": 100
    },
    {
      "id": 890668799,
      "name": "servers",
      "full_name": "modelcontextprotocol/servers",
      "description": "Model Context Protocol Servers",
      "html_url": "https://github.com/modelcontextprotocol/servers",
      "stars": 77353,
      "forks": 9370,
      "language": "TypeScript",
      "topics": [],
      "created_at": "2024-11-19T01:10:17Z",
      "updated_at": "2026-01-28T02:12:47Z",
      "pushed_at": "2026-01-27T23:03:07Z",
      "open_issues": 281,
      "owner": {
        "login": "modelcontextprotocol",
        "avatar_url": "https://avatars.githubusercontent.com/u/182288589?v=4"
      },
      "readme": "# Model Context Protocol servers\n\nThis repository is a collection of *reference implementations* for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP), as well as references to community-built servers and additional resources.\n\n> [!IMPORTANT]\n> If you are looking for a list of MCP servers, you can browse published servers on [the MCP Registry](https://registry.modelcontextprotocol.io/). The repository served by this README is dedicated to housing just the small number of reference servers maintained by the MCP steering group.\n\n> [!WARNING]\n> The servers in this repository are intended as **reference implementations** to demonstrate MCP features and SDK usage. They are meant to serve as educational examples for developers building their own MCP servers, not as production-ready solutions. Developers should evaluate their own security requirements and implement appropriate safeguards based on their specific threat model and use case.\n\nThe servers in this repository showcase the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.\nTypically, each MCP server is implemented with an MCP SDK:\n\n- [C# MCP SDK](https://github.com/modelcontextprotocol/csharp-sdk)\n- [Go MCP SDK](https://github.com/modelcontextprotocol/go-sdk)\n- [Java MCP SDK](https://github.com/modelcontextprotocol/java-sdk)\n- [Kotlin MCP SDK](https://github.com/modelcontextprotocol/kotlin-sdk)\n- [PHP MCP SDK](https://github.com/modelcontextprotocol/php-sdk)\n- [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk)\n- [Ruby MCP SDK](https://github.com/modelcontextprotocol/ruby-sdk)\n- [Rust MCP SDK](https://github.com/modelcontextprotocol/rust-sdk)\n- [Swift MCP SDK](https://github.com/modelcontextprotocol/swift-sdk)\n- [TypeScript MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n\n## ğŸŒŸ Reference Servers\n\nThese servers aim to demonstrate MCP features and the official SDKs.\n\n- **[Everything](src/everything)** - Reference / test server with prompts, resources, and tools.\n- **[Fetch](src/fetch)** - Web content fetching and conversion for efficient LLM usage.\n- **[Filesystem](src/filesystem)** - Secure file operations with configurable access controls.\n- **[Git](src/git)** - Tools to read, search, and manipulate Git repositories.\n- **[Memory](src/memory)** - Knowledge graph-based persistent memory system.\n- **[Sequential Thinking](src/sequentialthinking)** - Dynamic and reflective problem-solving through thought sequences.\n- **[Time](src/time)** - Time and timezone conversion capabilities.\n\n### Archived\n\nThe following reference servers are now archived and can be found at [servers-archived](https://github.com/modelcontextprotocol/servers-archived).\n\n- **[AWS KB Retrieval](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime.\n- **[Brave Search](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/brave-search)** - Web and local search using Brave's Search API.  Has been replaced by the [official server](https://github.com/brave/brave-search-mcp-server).\n- **[EverArt](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/everart)** - AI image generation using various models.\n- **[GitHub](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/github)** - Repository management, file operations, and GitHub API integration.\n- **[GitLab](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/gitlab)** - GitLab API, enabling project management.\n- **[Google Drive](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/gdrive)** - File access and search capabilities for Google Drive.\n- **[Google Maps](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/google-maps)** - Location services, directions, and place details.\n- **[PostgreSQL](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/postgres)** - Read-only database access with schema inspection.\n- **[Puppeteer](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/puppeteer)** - Browser automation and web scraping.\n- **[Redis](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/redis)** - Interact with Redis key-value stores.\n- **[Sentry](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/sentry)** - Retrieving and analyzing issues from Sentry.io.\n- **[Slack](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/slack)** - Channel management and messaging capabilities. Now maintained by [Zencoder](https://github.com/zencoderai/slack-mcp-server)\n- **[SQLite](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/sqlite)** - Database interaction and business intelligence capabilities.\n\n## ğŸ¤ Third-Party Servers\n\n> [!NOTE]\nThe server lists in this README are no longer maintained and will eventually be removed.\n\n### ğŸ–ï¸ Official Integrations\n\nOfficial integrations are maintained by companies building production ready MCP servers for their platforms.\n\n- <img height=\"12\" width=\"12\" src=\"https://www.21st.dev/favicon.ico\" alt=\"21st.dev Logo\" /> **[21st.dev Magic](https://github.com/21st-dev/magic-mcp)** - Create crafted UI components inspired by the best 21st.dev design engineers.\n- <img height=\"12\" width=\"12\" src=\"https://www.2slides.com/images/2slides-red.svg\" alt=\"2slides Logo\" /> **[2slides](https://github.com/2slides/2slides-mcp)** - An MCP server that provides tools to convert content into slides/PPT/presentation or generate slides/PPT/presentation with user intention.\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/LpSK1tSZweomrAHOMAj9Gea96lA.svg\" alt=\"Paragon Logo\" /> **[ActionKit by Paragon](https://github.com/useparagon/paragon-mcp)** - Connect to 130+ SaaS integrations (e.g. Slack, Salesforce, Gmail) with Paragonâ€™s [ActionKit](https://www.useparagon.com/actionkit) API.\n- <img height=\"12\" width=\"12\" src=\"https://invoxx-public-bucket.s3.eu-central-1.amazonaws.com/frontend-resources/adfin-logo-small.svg\" alt=\"Adfin Logo\" /> **[Adfin](https://github.com/Adfin-Engineering/mcp-server-adfin)** - The only platform you need to get paid - all payments in one place, invoicing and accounting reconciliations with [Adfin](https://www.adfin.com/).\n- <img height=\"12\" width=\"12\" src=\"https://github.com/AgentOps-AI/agentops/blob/main/docs/favicon.png\" alt=\"AgentOps Logo\" /> **[AgentOps](https://github.com/AgentOps-AI/agentops-mcp)** - Provide observability and tracing for debugging AI agents with [AgentOps](https://www.agentops.ai/) API.\n- <img height=\"12\" width=\"12\" src=\"https://www.agentql.com/favicon/favicon.png\" alt=\"AgentQL Logo\" /> **[AgentQL](https://github.com/tinyfish-io/agentql-mcp)** - Enable AI agents to get structured data from unstructured web with [AgentQL](https://www.agentql.com/).\n- <img height=\"12\" width=\"12\" src=\"https://agentrpc.com/favicon.ico\" alt=\"AgentRPC Logo\" /> **[AgentRPC](https://github.com/agentrpc/agentrpc)** - Connect to any function, any language, across network boundaries using [AgentRPC](https://www.agentrpc.com/).\n- **[Agentset](https://github.com/agentset-ai/mcp-server)** - RAG for your knowledge base connected to [Agentset](https://agentset.ai).\n- <img height=\"12\" width=\"12\" src=\"https://www.airwallex.com/favicon.ico\" alt=\"Airwallex Logo\" /> **[Airwallex Developer](https://www.npmjs.com/package/@airwallex/developer-mcp)** - Empowers AI coding agents with the tools they need to assist developers integrating with [Airwallex APIs](https://www.airwallex.com/docs/api/)\n- <img height=\"12\" width=\"12\" src=\"https://aiven.io/favicon.ico\" alt=\"Aiven Logo\" /> **[Aiven](https://github.com/Aiven-Open/mcp-aiven)** - Navigate your [Aiven projects](https://go.aiven.io/mcp-server) and interact with the PostgreSQLÂ®, Apache KafkaÂ®, ClickHouseÂ® and OpenSearchÂ® services\n- <img height=\"12\" width=\"12\" src=\"https://www.alation.com/resource-center/download/7p3vnbbznfiw/34FMtBTex5ppvs2hNYa9Fc/c877c37e88e5339878658697c46d2d58/Alation-Logo-Bug-Primary.svg\" alt=\"Alation Logo\" /> **[Alation](https://github.com/Alation/alation-ai-agent-sdk)** - Unlock the power of the enterprise Data Catalog by harnessing tools provided by the Alation MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://i.postimg.cc/5NYw9qjS/alby-icon-head-yellow-500x500.png\" alt=\"Alby Logo\" /> **[Alby Bitcoin Payments](https://github.com/getAlby/mcp)** - Connect any bitcoin lightning wallet to your agent to send and receive instant payments globally with your agent.\n- **[Algolia](https://github.com/algolia/mcp)** - Use AI agents to provision, configure, and query your [Algolia](https://algolia.com) search indices.\n- <img height=\"12\" width=\"12\" src=\"https://img.alicdn.com/imgextra/i4/O1CN01epkXwH1WLAXkZfV6N_!!6000000002771-2-tps-200-200.png\" alt=\"Alibaba Cloud AnalyticDB for MySQL Logo\" /> **[Alibaba Cloud AnalyticDB for MySQL](https://github.com/aliyun/alibabacloud-adb-mysql-mcp-server)** - Connect to an [AnalyticDB for MySQL](https://www.alibabacloud.com/en/product/analyticdb-for-mysql) cluster for getting database or table metadata, querying and analyzing data. It will be supported to add the OpenAPI for cluster operation in the future.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/aliyun/alibabacloud-adbpg-mcp-server/blob/master/images/AnalyticDB.png\" alt=\"Alibaba Cloud AnalyticDB for PostgreSQL Logo\" /> **[Alibaba Cloud AnalyticDB for PostgreSQL](https://github.com/aliyun/alibabacloud-adbpg-mcp-server)** - An MCP server to connect to [AnalyticDB for PostgreSQL](https://github.com/aliyun/alibabacloud-adbpg-mcp-server) instances, query and analyze data.\n- <img height=\"12\" width=\"12\" src=\"https://img.alicdn.com/imgextra/i3/O1CN0101UWWF1UYn3rAe3HU_!!6000000002530-2-tps-32-32.png\" alt=\"DataWorks Logo\" /> **[Alibaba Cloud DataWorks](https://github.com/aliyun/alibabacloud-dataworks-mcp-server)** - A Model Context Protocol (MCP) server that provides tools for AI, allowing it to interact with the [DataWorks](https://www.alibabacloud.com/help/en/dataworks/) Open API through a standardized interface. This implementation is based on the Alibaba Cloud Open API and enables AI agents to perform cloud resources operations seamlessly.\n- <img height=\"12\" width=\"12\" src=\"https://opensearch-shanghai.oss-cn-shanghai.aliyuncs.com/ouhuang/aliyun-icon.png\" alt=\"Alibaba Cloud OpenSearch Logo\" /> **[Alibaba Cloud OpenSearch](https://github.com/aliyun/alibabacloud-opensearch-mcp-server)** - This MCP server equips AI Agents with tools to interact with [OpenSearch](https://help.aliyun.com/zh/open-search/?spm=5176.7946605.J_5253785160.6.28098651AaYZXC) through a standardized and extensible interface.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/aliyun/alibaba-cloud-ops-mcp-server/blob/master/image/alibaba-cloud.png\" alt=\"Alibaba Cloud OPS Logo\" /> **[Alibaba Cloud OPS](https://github.com/aliyun/alibaba-cloud-ops-mcp-server)** - Manage the lifecycle of your Alibaba Cloud resources with [CloudOps Orchestration Service](https://www.alibabacloud.com/en/product/oos) and Alibaba Cloud OpenAPI.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/aliyun/alibabacloud-rds-openapi-mcp-server/blob/main/assets/alibabacloudrds.png\" alt=\"Alibaba Cloud RDS MySQL Logo\" /> **[Alibaba Cloud RDS](https://github.com/aliyun/alibabacloud-rds-openapi-mcp-server)** - An MCP server designed to interact with the Alibaba Cloud RDS OpenAPI, enabling programmatic management of RDS resources via an LLM.\n- <img height=\"12\" width=\"12\" src=\"https://www.alipayplus.com/favicon.ico\" alt=\"AlipayPlus Logo\" /> **[AlipayPlus](https://github.com/alipay/global-alipayplus-mcp)** - Connect your AI Agents to AlipayPlus Checkout Payment.\n- <img height=\"12\" width=\"12\" src=\"https://datalab.alkemi.ai/favicon.png\" alt=\"Alkemi Logo\" /> **[Alkemi](https://github.com/alkemi-ai/alkemi-mcp)** - Query Snowflake, Google BigQuery, DataBricks Data Products through Alkemi.ai.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.allvoicelab.com/resources/workbench/dist/icon-dark.ico\" alt=\"AllVoiceLab Logo\" /> **[AllVoiceLab](https://www.allvoicelab.com/mcp)** - An AI voice toolkit with TTS, voice cloning, and video translation, now available as an MCP server for smarter agent integration.\n- <img height=\"12\" width=\"12\" src=\"https://files.alpaca.markets/webassets/favicon-32x32.png\" alt=\"Alpaca Logo\" /> **[Alpaca](https://github.com/alpacahq/alpaca-mcp-server)** â€“ Alpaca's MCP server lets you trade stocks and options, analyze market data, and build strategies through [Alpaca's Trading API](https://alpaca.markets/)\n- <img height=\"12\" width=\"12\" src=\"https://www.alphavantage.co/logo.png/\" alt=\"AlphaVantage Logo\" /> **[AlphaVantage](https://mcp.alphavantage.co/)** - Connect to 100+ APIs for financial market data, including stock prices, fundamentals, and more from [AlphaVantage](https://www.alphavantage.co)\n- <img height=\"12\" width=\"12\" src=\"https://alttester.com/app/themes/alttester-sage-theme/public/images/logo-alttester.038ec8.png\" alt=\"AltTester Logo\" /> **[AltTesterÂ®](https://alttester.com/docs/desktop/latest/pages/ai-extension.html)** - Use AltTesterÂ® capabilities to connect and test your Unity or Unreal game. Write game test automation faster and smarter, using [AltTester](https://alttester.com) and the AltTesterÂ® MCP server. \n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/amplitude/mcp-server-guide/refs/heads/main/amplitude-logo.svg\" alt=\"Amplitude Logo\" /> **[Amplitude](https://amplitude.com/docs/analytics/amplitude-mcp)** - The Amplitude MCP server enables seamless integration between AI assistants and your product data, allowing you to search, analyze, and query charts, dashboards, experiments, feature flags, and metrics directly from your AI interface.\n- <img height=\"12\" width=\"12\" src=\"https://www.antom.com/favicon.ico\" alt=\"Antom Logo\" /> **[Antom](https://github.com/alipay/global-antom-mcp)** - Connect your AI Agents to Antom Checkout Payment.\n- <img height=\"12\" width=\"12\" src=\"https://developers.anytype.io/img/favicon.ico\" alt=\"Anytype Logo\" /> **[Anytype](https://github.com/anyproto/anytype-mcp)** - An MCP server enabling AI assistants to interact with [Anytype](https://anytype.io) - a local and collaborative wiki - to organize objects, lists, and more through natural language.\n- <img height=\"12\" width=\"12\" src=\"https://doris.apache.org/images/favicon.ico\" alt=\"Apache Doris Logo\" /> **[Apache Doris](https://github.com/apache/doris-mcp-server)** - MCP Server For [Apache Doris](https://doris.apache.org/), an MPP-based real-time data warehouse.\n- <img height=\"12\" width=\"12\" src=\"https://iotdb.apache.org/img/logo.svg\" alt=\"Apache IoTDB Logo\" /> **[Apache IoTDB](https://github.com/apache/iotdb-mcp-server)** - MCP Server for [Apache IoTDB](https://github.com/apache/iotdb) database and its tools\n- **[Apache Pinot](https://github.com/startreedata/mcp-pinot)** â€“ MCP server for running real - time analytics queries on Apache Pinot, an open-source OLAP database built for high-throughput, low-latency powering real-time applications.\n- <img height=\"12\" width=\"12\" src=\"https://apify.com/favicon.ico\" alt=\"Apify Logo\" /> **[Apify](https://github.com/apify/apify-mcp-server)** - Use 6,000+ pre-built cloud tools to extract data from websites, e-commerce, social media, search engines, maps, and more\n- <img height=\"12\" width=\"12\" src=\"https://2052727.fs1.hubspotusercontent-na1.net/hubfs/2052727/cropped-cropped-apimaticio-favicon-1-32x32.png\" alt=\"APIMatic Logo\" /> **[APIMatic MCP](https://github.com/apimatic/apimatic-validator-mcp)** - APIMatic MCP Server is used to validate OpenAPI specifications using [APIMatic](https://www.apimatic.io/). The server processes OpenAPI files and returns validation summaries by leveraging APIMatic's API.\n- <img height=\"12\" width=\"12\" src=\"https://apollo-server-landing-page.cdn.apollographql.com/_latest/assets/favicon.png\" alt=\"Apollo Graph Logo\" /> **[Apollo MCP Server](https://github.com/apollographql/apollo-mcp-server/)** - Connect your GraphQL APIs to AI agents\n- <img height=\"12\" width=\"12\" src=\"https://appium.io/docs/en/latest/assets/images/appium-logo-horiz.png\" alt=\"Appium Logo\" /> **[Appium MCP Server](https://github.com/appium/appium-mcp.git)** - MCP server for Mobile Development and Automation | iOS, Android, Simulator, Emulator, and Real Devices \n- <img height=\"12\" width=\"12\" src=\"https://developer.aqara.com/favicon.ico\" alt=\"Aqara Logo\" /> **[Aqara MCP Server](https://github.com/aqara/aqara-mcp-server/)** - Control  [Aqara](https://www.aqara.com/) smart home devices, query status, execute scenes, and much more using natural language.\n- <img height=\"12\" width=\"12\" src=\"https://media.licdn.com/dms/image/v2/C4D0BAQEeD7Dxbpadkw/company-logo_200_200/company-logo_200_200/0/1644692667545/archbee_logo?e=2147483647&v=beta&t=lTi9GRIoqzG6jN3kJC26uZWh0q3uiQelsH6mGoq_Wfw\" alt=\"Archbee Logo\" /> **[Archbee](https://www.npmjs.com/package/@archbee/mcp)** - Write and publish documentation that becomes the trusted source for instant answers with AI. Stop cobbling tools and use [Archbee](https://www.archbee.com/) â€” the first complete documentation platform.\n- <img height=\"12\" width=\"12\" src=\"https://phoenix.arize.com/wp-content/uploads/2023/04/cropped-Favicon-32x32.png\" alt=\"Arize-Phoenix Logo\" /> **[Arize Phoenix](https://github.com/Arize-ai/phoenix/tree/main/js/packages/phoenix-mcp)** - Inspect traces, manage prompts, curate datasets, and run experiments using [Arize Phoenix](https://github.com/Arize-ai/phoenix), an open-source AI and LLM observability tool.\n- <img height=\"12\" width=\"12\" src=\"https://731523176-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FaVUBXRZbpAgtjYf5HsvO%2Fuploads%2FaRRrVVocXCTr6GkepfCx%2Flogo_color.svg?alt=media&token=3ba24089-0ab2-421f-a9d9-41f2f94f954a\" alt=\"Armor Logo\" /> **[Armor Crypto MCP](https://github.com/armorwallet/armor-crypto-mcp)** - MCP to interface with multiple blockchains, staking, DeFi, swap, bridging, wallet management, DCA, Limit Orders, Coin Lookup, Tracking and more.\n- <img height=\"12\" width=\"12\" src=\"https://console.asgardeo.io/app/libs/themes/wso2is/assets/images/branding/favicon.ico\" alt=\"Asgardeo Logo\" /> **[Asgardeo](https://github.com/asgardeo/asgardeo-mcp-server)** - MCP server to interact with your [Asgardeo](https://wso2.com/asgardeo) organization through LLM tools.\n- <img height=\"12\" width=\"12\" src=\"https://www.datastax.com/favicon-32x32.png\" alt=\"DataStax logo\" /> **[Astra DB](https://github.com/datastax/astra-db-mcp)** - Comprehensive tools for managing collections and documents in a [DataStax Astra DB](https://www.datastax.com/products/datastax-astra) NoSQL database with a full range of operations such as create, update, delete, find, and associated bulk actions.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/66598898fd13d51606c3215d/66ccbfef13bd8bc19d587578_favicon-32x32.png\" alt=\"Atla Logo\" /> **[Atla](https://github.com/atla-ai/atla-mcp-server)** - Enable AI agents to interact with the [Atla API](https://docs.atla-ai.com/) for state-of-the-art LLMJ evaluation.\n- <img height=\"12\" width=\"12\" src=\"https://assets.atlan.com/assets/atlan-a-logo-blue-background.png\" alt=\"Atlan Logo\" /> **[Atlan](https://github.com/atlanhq/agent-toolkit/tree/main/modelcontextprotocol)** - The Atlan Model Context Protocol server allows you to interact with the [Atlan](https://www.atlan.com/) services through multiple tools.\n- <img height=\"12\" width=\"12\" src=\"https://www.atlassian.com/favicon.ico\" alt=\"Atlassian Logo\" /> **[Atlassian](https://www.atlassian.com/platform/remote-mcp-server)** - Securely interact with Jira work items and Confluence pages, and search across both.\n- <img height=\"12\" width=\"12\" src=\"https://res.oafimg.cn/-/737b3b3ffed9b19e/logo.png\" alt=\"AtomGit Logo\" /> **[AtomGit](https://atomgit.com/atomgit-open-source-ecosystem/atomgit-mcp-server)** - Official AtomGit server for integration with repository management, PRs, issues, branches, labels, and more.\n- <img height=\"12\" width=\"12\" src=\"https://atono.io/favicon.ico\" alt=\"Atono Logo\" /> **[Atono](https://docs.atono.io/docs/mcp-server-for-atono/)** - Modern product teams connect their AI assistant to Atono to create and update stories, bugs, assignments and fixes.\n- <img height=\"12\" width=\"12\" src=\"https://resources.audiense.com/hubfs/favicon-1.png\" alt=\"Audiense Logo\" /> **[Audiense Insights](https://github.com/AudienseCo/mcp-audiense-insights)** - Marketing insights and audience analysis from [Audiense](https://www.audiense.com/products/audiense-insights) reports, covering demographic, cultural, influencer, and content engagement analysis.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.auth0.com/website/website/favicons/auth0-favicon.svg\" alt=\"Auth0 Logo\" /> **[Auth0](https://github.com/auth0/auth0-mcp-server)** - MCP server for interacting with your Auth0 tenant, supporting creating and modifying actions, applications, forms, logs, resource servers, and more.\n- <img height=\"12\" width=\"12\" src=\"https://firstorder.ai/favicon_auth.ico\" alt=\"Authenticator App Logo\" /> **[Authenticator App Â· 2FA](https://github.com/firstorderai/authenticator_mcp)** - A secure MCP (Model Context Protocol) server that enables AI agents to interact with the Authenticator App.\n- <img height=\"12\" width=\"12\" src=\"https://a0.awsstatic.com/libra-css/images/site/fav/favicon.ico\" alt=\"AWS Logo\" /> **[AWS](https://github.com/awslabs/mcp)** -  Specialized MCP servers that bring AWS best practices directly to your development workflow.\n- <img height=\"12\" width=\"12\" src=\"https://axiom.co/favicon.ico\" alt=\"Axiom Logo\" /> **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze your Axiom logs, traces, and all other event data in natural language\n- <img height=\"12\" width=\"12\" src=\"https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/acom_social_icon_azure\" alt=\"Microsoft Azure Logo\" /> **[Azure](https://github.com/microsoft/mcp/tree/main/servers/Azure.Mcp.Server)** - The Azure MCP Server gives MCP Clients access to key Azure services and tools like Azure Storage, Cosmos DB, the Azure CLI, and more.\n- <img height=\"12\" width=\"12\" src=\"https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/1062064-Products-1.2-24x24\" alt=\"Microsoft Azure DevOps Logo\" /> **[Azure DevOps](https://github.com/microsoft/azure-devops-mcp)** - Interact with Azure DevOps services like repositories, work items, builds, releases, test plans, and code search.\n- <img height=\"12\" width=\"12\" src=\"https://application.backdocket.com/favicon.ico\" alt=\"Backdocket Logo\" /> **[Backdocket](https://ai.backdocket.com)** - Search, Retrieve, and Update your **[Backdocket](https://backdocket.com)** data. This currently includes Claims, Matters, Contacts, Tasks and Advanced Searches. To easily use the Remote Mcp Server utilize the following url: **[https://ai.backdocket.com/mcp]([https://backdocket.com](https://ai.backdocket.com/mcp))**\n- <img height=\"12\" width=\"12\" src=\"https://mapopen-website-wiki.cdn.bcebos.com/LOGO/lbsyunlogo_icon.ico\" alt=\"Baidu Map Logo\" /> **[Baidu Map](https://github.com/baidu-maps/mcp)** - [Baidu Map MCP Server](https://lbsyun.baidu.com/faq/api?title=mcpserver/base) provides tools for AI agents to interact with Baidu Maps APIs, enabling location-based services and geospatial data analysis.\n- <img height=\"12\" width=\"12\" src=\"https://www.bankless.com/favicon.ico\" alt=\"Bankless Logo\" /> **[Bankless Onchain](https://github.com/bankless/onchain-mcp)** - Query Onchain data, like ERC20 tokens, transaction history, smart contract state.\n- <img height=\"12\" width=\"12\" src=\"https://baserow.io/img/logo_baserow_square_large.png\" alt=\"Baserow Logo\" /> **[Baserow](https://gitlab.com/baserow/baserow/-/tree/develop/backend/src/baserow/api/mcp)** - Query data from Baserow self-hosted or SaaS databases using MCP integration.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6815c48ebd95a588d14e383b/68582c01f6420d9777922095_xAsset%20114rt.avif\" alt=\"Bauplan Logo\" /> **[Bauplan](https://github.com/BauplanLabs/bauplan-mcp-server)** - Manage the Bauplan lakehouse: query tables, create data branches, run pipelines, retrieve logs.\n- <img height=\"12\" width=\"12\" src=\"https://bicscan.io/favicon.png\" alt=\"BICScan Logo\" /> **[BICScan](https://github.com/ahnlabio/bicscan-mcp)** - Risk score / asset holdings of EVM blockchain address (EOA, CA, ENS) and even domain names.\n- <img height=\"12\" width=\"12\" src=\"https://www.bitnovo.com/favicons/favicon-196x196.png\" alt=\"Bitnovo Logo\" /> **[Bitnovo Pay](https://github.com/bitnovo/mcp-bitnovo-pay)** - Cryptocurrency payment integration enabling AI agents to create payments, manage QR codes, and process transactions through the Bitnovo Pay API with support for Bitcoin, Ethereum, and other cryptocurrencies.\n- <img height=\"12\" width=\"12\" src=\"https://web-cdn.bitrise.io/favicon.ico\" alt=\"Bitrise Logo\" /> **[Bitrise](https://github.com/bitrise-io/bitrise-mcp)** - Chat with your builds, CI, and [more](https://bitrise.io/blog/post/chat-with-your-builds-ci-and-more-introducing-the-bitrise-mcp-server).\n- <img height=\"12\" width=\"12\" src=\"https://boikot.xyz/assets/favicon.svg\" alt=\"boikot Logo\" /> **[Boikot](https://github.com/boikot-xyz/boikot)** - Learn about the ethical and unethical actions of major companies with [boikot.xyz](https://boikot.xyz/).\n- <img height=\"12\" width=\"12\" src=\"https://boldsign.com/favicon.ico\" alt=\"BoldSign Logo\" /> **[BoldSign](https://github.com/boldsign/boldsign-mcp)** - Search, request, and manage e-signature contracts effortlessly with [BoldSign](https://boldsign.com/).\n- <img height=\"12\" width=\"12\" src=\"https://boost.space/favicon.ico\" alt=\"Boost.space Logo\" /> **[Boost.space](https://github.com/boostspace/boostspace-mcp-server)** - An MCP server integrating with [Boost.space](https://boost.space) for centralized, automated business data from 2000+ sources.\n- <img height=\"12\" width=\"12\" src=\"https://boostsecurity.io/hs-fs/hubfs/blue-logo.png\" alt=\"BoostSecurity Logo\" /> **[BoostSecurity](https://github.com/boost-community/boost-mcp)** - Powered by [BoostSecurity](https://boostsecurity.io/), the MCP guardrails coding agents against introducing dependencies with vulnerabilities, malware or typosquatting.\n- <img height=\"12\" width=\"12\" src=\"https://www.box.com/favicon.ico\" alt=\"Box Logo\" /> **[Box](https://github.com/box-community/mcp-server-box)** - Interact with the Intelligent Content Management platform through Box AI.\n- <img height=\"12\" width=\"12\" src=\"https://www.brightdata.com/favicon.ico\" alt=\"BrightData Logo\" /> **[BrightData](https://github.com/luminati-io/brightdata-mcp)** - Discover, extract, and interact with the web - one interface powering automated access across the public internet.\n- <img height=\"12\" width=\"12\" src=\"https://browserbase.com/favicon.ico\" alt=\"Browserbase Logo\" /> **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)\n- <img height=\"12\" width=\"12\" src=\"https://browserstack.wpenginepowered.com/wp-content/themes/browserstack/img/favicons/favicon.ico\" alt=\"BrowserStack Logo\" /> **[BrowserStack](https://github.com/browserstack/mcp-server)** - Access BrowserStack's [Test Platform](https://www.browserstack.com/test-platform) to debug, write and fix tests, do accessibility testing and more.\n- <img height=\"12\" width=\"12\" src=\"https://bldbl.dev/favico.png\" alt=\"Buildable Logo\" />**[Buildable](https://github.com/chunkydotdev/bldbl-mcp)** (TypeScript) - Official MCP server for Buildable AI-powered development platform. Enables AI assistants to manage tasks, track progress, get project context, and collaborate with humans on software projects.\n- <img height=\"12\" width=\"12\" src=\"https://www.google.com/s2/favicons?domain=buildkite.com&sz=24\" alt=\"Buildkite Logo\" /> **[Buildkite](https://github.com/buildkite/buildkite-mcp-server)** - Exposing Buildkite data (pipelines, builds, jobs, tests) to AI tooling and editors.\n- <img height=\"12\" width=\"12\" src=\"https://builtwith.com/favicon.ico\" alt=\"BuiltWith Logo\" /> **[BuiltWith](https://github.com/builtwith/mcp)** - Identify the technology stack behind any website.\n- <img height=\"12\" width=\"12\" src=\"https://portswigger.net/favicon.ico\" alt=\"PortSwigger Logo\" /> **[Burp Suite](https://github.com/PortSwigger/mcp-server)** - MCP Server extension allowing AI clients to connect to [Burp Suite](https://portswigger.net)\n- <img src=\"https://app.cal.com/favicon.ico\" alt=\"Cal.com\" width=\"12\" height=\"12\"> **[Cal.com](https://www.npmjs.com/package/@calcom/cal-mcp?activeTab=readme)** - Connect to the Cal.com API to schedule and manage bookings and appointments.\n- <img height=\"12\" width=\"12\" src=\"https://campertunity.com/assets/icon/favicon.ico\" alt=\"Campertunity Logo\" /> **[Campertunity](https://github.com/campertunity/mcp-server)** - Search campgrounds around the world on campertunity, check availability, and provide booking links.\n- <img height=\"12\" width=\"12\" src=\"https://static.canva.com/static/images/favicon.ico\" alt=\"Canva logo\" /> **[Canva](https://www.canva.dev/docs/apps/mcp-server/)** â€” Provide AI - powered development assistance for [Canva](https://canva.com) apps and integrations.\n- <img height=\"12\" width=\"12\" src=\"https://carbonvoice.app/favicon.ico\" alt=\"Carbon Voice Logo\" /> **[Carbon Voice](https://github.com/PhononX/cv-mcp-server)** - MCP Server that connects AI Agents to [Carbon Voice](https://getcarbon.app). Create, manage, and interact with voice messages, conversations, direct messages, folders, voice memos, AI actions and more in [Carbon Voice](https://getcarbon.app).\n- <img height=\"12\" width=\"12\" src=\"https://play.cartesia.ai/icon.png\" alt=\"Cartesia logo\" /> **[Cartesia](https://github.com/cartesia-ai/cartesia-mcp)** - Connect to the [Cartesia](https://cartesia.ai/) voice platform to perform text-to-speech, voice cloning etc.\n- <img height=\"12\" width=\"12\" src=\"https://www.cashfree.com/favicon.ico\" alt=\"Cashfree logo\" /> **[Cashfree](https://github.com/cashfree/cashfree-mcp)** - [Cashfree Payments](https://www.cashfree.com/) official MCP server.\n- **[CB Insights](https://github.com/cbinsights/cbi-mcp-server)** - Use the [CB Insights](https://www.cbinsights.com) MCP Server to connect to [ChatCBI](https://www.cbinsights.com/chatcbi/)\n- <img height=\"12\" width=\"12\" src=\"https://chainaware.ai/assets/brand/chainawareai-logo.svg\" alt=\"ChainAware.ai Logo\" /> **[Behavioural Prediction](https://github.com/ChainAware/behavioral-prediction-mcp)** - AI-powered tools to analyze wallet behaviour prediction,fraud detection and rug pull prediction powered by [ChainAware.ai](https://www.chainaware.ai).\n- <img height=\"12\" width=\"12\" src=\"https://www.chargebee.com/static/resources/brand/favicon.png\" alt=\"Chargebee Logo\" /> **[Chargebee](https://github.com/chargebee/agentkit/tree/main/modelcontextprotocol)** - MCP Server that connects AI agents to [Chargebee platform](https://www.chargebee.com).\n- <img height=\"12\" width=\"12\" src=\"https://cheqd.io/wp-content/uploads/2023/03/logo_cheqd_favicon.png\" alt=\"Cheqd Logo\" /> **[Cheqd](https://github.com/cheqd/mcp-toolkit)** - Enable AI Agents to be trusted, verified, prevent fraud, protect your reputation, and more through [cheqd's](https://cheqd.io) Trust Registries and Credentials.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.chiki.studio/brand/logo.png\" alt=\"Chiki StudIO Logo\" /> **[Chiki StudIO](https://chiki.studio/galimybes/mcp/)** - Create your own configurable MCP servers purely via configuration (no code), with instructions, prompts, and tools support.\n- <img height=\"12\" width=\"12\" src=\"https://trychroma.com/_next/static/media/chroma-logo.ae2d6e4b.svg\" alt=\"Chroma Logo\" /> **[Chroma](https://github.com/chroma-core/chroma-mcp)** - Embeddings, vector search, document storage, and full-text search with the open-source AI application database\n- <img height=\"12\" width=\"12\" src=\"https://www.google.com/chrome/static/images/favicons/favicon-32x32.png\" alt=\"Chrome\" /> **[Chrome DevTools](https://github.com/ChromeDevTools/chrome-devtools-mcp)** - Enable AI coding assistants to debug web pages directly in Chrome, providing runtime insights and debugging capabilities.\n- <img height=\"12\" width=\"12\" src=\"https://www.chronulus.com/favicon/chronulus-logo-blue-on-alpha-square-128x128.ico\" alt=\"Chronulus AI Logo\" /> **[Chronulus AI](https://github.com/ChronulusAI/chronulus-mcp)** - Predict anything with Chronulus AI forecasting and prediction agents.\n- <img height=\"12\" width=\"12\" src=\"https://circleci.com/favicon.ico\" alt=\"CircleCI Logo\" /> **[CircleCI](https://github.com/CircleCI-Public/mcp-server-circleci)** - Enable AI Agents to fix build failures from CircleCI.\n- <img height=\"12\" width=\"12\" src=\"https://assets.zilliz.com/Zilliz_Logo_Mark_White_20230223_041013_86057436cc.png\" alt=\"Claude Context Logo\" /> **[Claude Context](https://github.com/zilliztech/claude-context)** - Bring your codebase as context to Claude Code\n- <img height=\"12\" width=\"12\" src=\"https://cleanupcrew.ai/favicon-light.png\" alt=\"Cleanup Crew logo\" /> **[Cleanup Crew](https://cleanupcrew.ai/install)** - Real-time human support service for non-technical founders using AI coding tools. When AI hits a wall, request instant human help directly from your IDE.\n- <img height=\"12\" width=\"12\" src=\"https://clickhouse.com/favicon.ico\" alt=\"ClickHouse Logo\" /> **[ClickHouse](https://github.com/ClickHouse/mcp-clickhouse)** - Query your [ClickHouse](https://clickhouse.com/) database server.\n- <img height=\"12\" width=\"12\" src=\"https://brand.clicksend.com/_ipx/s_794x608/img/clicksend_icon_only.svg\" alt=\"ClickSend Logo\" /> **[ClickSend](https://github.com/ClickSend/clicksend-mcp-server/)** - This is the official ClickSend MCP Server developed by ClickSend team.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/206176626?s=200&v=4\" alt=\"Clix Logo\" /> **[Clix MCP Server](https://github.com/clix-so/clix-mcp-server)** - Clix MCP Server that enables AI agents to provide real-time, trusted Clix documentation and SDK code examples for seamless integrations.\n- <img height=\"12\" width=\"12\" src=\"https://7463-tcb-advanced-a656fc-1257967285.tcb.qcloud.la/mcp/cloudbase-logo.svg\" alt=\"CloudBase Logo\" /> **[CloudBase](https://github.com/TencentCloudBase/CloudBase-AI-ToolKit)** - One-stop backend services for WeChat Mini-Programs and full-stack apps with serverless cloud functions and databases by [Tencent CloudBase](https://tcb.cloud.tencent.com/)\n- <img height=\"12\" width=\"12\" src=\"https://www.cloudbees.com/favicon.ico\" alt=\"CloudBees Logo\" /> **[CloudBees CI](https://docs.cloudbees.com/docs/cloudbees-ci-mcp-router/latest/)** - Enable AI access to your [CloudBees CI](https://www.cloudbees.com/capabilities/continuous-integration) cluster, the Enterprise-grade JenkinsÂ®-based solution. \n- <img height=\"12\" width=\"12\" src=\"https://www.cloudbees.com/favicon.ico\" alt=\"CloudBees Logo\" /> **[CloudBees Unify](https://docs.cloudbees.com/docs/cloudbees-unify-mcp-server/latest/install/mcp-server)** - Enable AI access to your [CloudBees Unify](https://www.cloudbees.com/unify) environment.\n- <img height=\"12\" width=\"12\" src=\"https://www.cloudbet.com/favicon.ico\" alt=\"Cloudbet Logo\" /> **[Cloudbet](https://github.com/cloudbet/sports-mcp-server)** - Structured sports and esports data via Cloudbet API: fixtures, live odds, stake limits, and markets.\n- <img src=\"http://www.google.com/s2/favicons?domain=www.cloudera.com\" alt=\"Cloudera Iceberg\" width=\"12\" height=\"12\"> **[Cloudera Iceberg](https://github.com/cloudera/iceberg-mcp-server)** - enabling AI on the [Open Data Lakehouse](https://www.cloudera.com/products/open-data-lakehouse.html).\n- <img height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/cloudflare\" /> **[Cloudflare](https://github.com/cloudflare/mcp-server-cloudflare)** - Deploy, configure & interrogate your resources on the Cloudflare developer platform (e.g. Workers/KV/R2/D1)\n- <img src=\"https://cdn.prod.website-files.com/64d41aab8183c7c3324ddb29/67c0f1e272e51cf3c511c17c_Gyph.svg\" alt=\"Cloudinary\" width=\"12\" height=\"12\"> **[Cloudinary](https://github.com/cloudinary/mcp-servers)** - Exposes Cloudinary's media upload, transformation, AI analysis, management, optimization and delivery as tools usable by AI agents\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/Cloudsway-AI/smartsearch/refs/heads/main/plugin_cloudsway.ico\" alt=\"Cloudsway Logo\" /> **[Cloudsway SmartSearch](https://github.com/Cloudsway-AI/smartsearch)** - Web search MCP server powered by Cloudsway, supporting keyword search, language, and safety options. Returns structured JSON results.\n- <img height=\"12\" width=\"12\" src=\"https://app.codacy.com/static/images/favicon-16x16.png\" alt=\"Codacy Logo\" /> **[Codacy](https://github.com/codacy/codacy-mcp-server/)** - Interact with [Codacy](https://www.codacy.com) API to query code quality issues, vulnerabilities, and coverage insights about your code.\n- <img height=\"12\" width=\"12\" src=\"https://codelogic.com/wp-content/themes/codelogic/assets/img/favicon.png\" alt=\"CodeLogic Logo\" /> **[CodeLogic](https://github.com/CodeLogicIncEngineering/codelogic-mcp-server)** - Interact with [CodeLogic](https://codelogic.com), a Software Intelligence platform that graphs complex code and data architecture dependencies, to boost AI accuracy and insight.\n- <img height=\"12\" width=\"12\" src=\"https://www.coinex.com/_assets/img/brand/svg/day-1.svg\" alt=\"Coinex Logo\" /> **[Coinex](https://github.com/coinexcom/coinex_mcp_server)** - Official [Coinex API](https://docs.coinex.com/api/v2). An MCP Server to interface with the CoinEx cryptocurrency exchange, enabling retrieve of market data, K-line data, order book depth, account balance queries, order placement and more.\n- <img height=\"12\" width=\"12\" src=\"https://www.coingecko.com/favicon.ico\" alt=\"CoinGecko Logo\" /> **[CoinGecko](https://github.com/coingecko/coingecko-typescript/tree/main/packages/mcp-server)** - Official [CoinGecko API](https://www.coingecko.com/en/api) MCP Server for Crypto Price & Market Data, across 200+ Blockchain Networks and 8M+ Tokens.\n- <img height=\"12\" width=\"12\" src=\"https://coinstats.app/favicon.ico\" alt=\"CoinStats Logo\" /> **[CoinStats](https://github.com/CoinStatsHQ/coinstats-mcp)** - MCP Server for the [CoinStats API](https://coinstats.app/api-docs/mcp/connecting). Provides access to cryptocurrency market data, portfolio tracking and news.\n- <img height=\"12\" width=\"12\" src=\"https://www.comet.com/favicon.ico\" alt=\"Comet Logo\" /> **[Comet Opik](https://github.com/comet-ml/opik-mcp)** - Query and analyze your [Opik](https://github.com/comet-ml/opik) logs, traces, prompts and all other telemetry data from your LLMs in natural language.\n- <img height=\"12\" width=\"12\" src=\"https://www.commercelayer.io/favicon.ico\" alt=\"Commerce Layer Logo\" /> **[Commerce Layer](https://github.com/commercelayer/mcp-server-metrics)** - Interact with Commerce Layer Metrics API.\n- <img height=\"12\" width=\"12\" src=\"https://platform.composio.dev/favicon.ico\" alt=\"Composio Logo\" /> **[Composio](https://docs.composio.dev/docs/mcp-overview#-getting-started)** â€“ Use [Composio](https://composio.dev) to connect 100+ tools. Zero setup. Auth built-in. Made for agents, works for humans.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6572bd8c27ee5db3eb91f4b3/6572bd8d27ee5db3eb91f55e_favicon-dashflow-webflow-template.svg\" alt=\"OSS Conductor Logo\" /> <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/68c3f472828bb14d0564ad4a/68c3f472828bb14d0564b0ab_Orkes%20Logo%20Symbol.svg\" alt=\"Orkes Conductor Logo\" />**[Conductor](https://github.com/conductor-oss/conductor-mcp)** - Interact with Conductor (OSS and Orkes) REST APIs.\n- <img height=\"12\" width=\"12\" src=\"https://configcat.com/favicon.ico\" alt=\"ConfigCat Logo\" /> **[ConfigCat](https://github.com/configcat/mcp-server)** - Enables AI tools to interact with [ConfigCat](https://configcat.com), a feature flag service for teams. Supports managing ConfigCat feature flags, configs, environments, products and organizations. Helps to integrate ConfigCat SDK, implement feature flags and remove zombie (stale) flags.\n- <img height=\"12\" width=\"12\" src=\"https://www.confluent.io/favicon.ico\" alt=\"Confluent Logo\" /> **[Confluent](https://github.com/confluentinc/mcp-confluent)** - Interact with Confluent Kafka and Confluent Cloud REST APIs.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/mattjoyce.png\" alt=\"Construe Logo\" /> **[Construe](https://github.com/mattjoyce/mcp-construe)** - FastMCP server for intelligent Obsidian vault context management with frontmatter filtering, automatic chunking, and secure bidirectional knowledge operations.\n- <img height=\"12\" width=\"12\" src=\"https://ginylil.com/favicon.ico\" alt=\"Ginylil Logo\" /> **[Context Templates](https://github.com/ginylil/context-templates)** - An open-source collection of reusable context templates designed to assist developers in structuring prompts, configurations, and workflows across various development tasks. Community contributions are encouraged to expand and refine available templates.\n- <img src=\"https://contrastsecurity.com/favicon.ico\" alt=\"Contrast Security\" width=\"12\" height=\"12\"> **[Contrast Security](https://github.com/Contrast-Security-OSS/mcp-contrast)** - Brings Contrast's vulnerability and SCA data into your coding agent to quickly remediate vulnerabilities.\n- <img height=\"12\" width=\"12\" src=\"https://www.convex.dev/favicon.ico\" alt=\"Convex Logo\" /> **[Convex](https://stack.convex.dev/convex-mcp-server)** - Introspect and query your apps deployed to Convex.\n- <img height=\"12\" width=\"12\" src=\"https://www.cortex.io/favicon.ico\" alt=\"Cortex Logo\" /> **[Cortex](https://github.com/cortexapps/cortex-mcp)** - Official MCP server for [Cortex](https://www.cortex.io).\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/605755?s=200&v=4\" alt=\"Couchbase Logo\" /> **[Couchbase](https://github.com/Couchbase-Ecosystem/mcp-server-couchbase)** - Interact with the data stored in Couchbase clusters.\n- <img height=\"12\" width=\"12\" src=\"https://www.courier.com/favicon.ico\" alt=\"Courier Logo\" /> **[Courier](https://www.courier.com/docs/tools/mcp)** - Build, update, and send multi-channel notifications across email, sms, push, Slack, and Microsoft Teams.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/user-attachments/assets/b256f9fa-2020-4b37-9644-c77229ef182b\" alt=\"CRIC å…‹è€Œç‘ LOGO\"> **[CRIC Wuye AI](https://github.com/wuye-ai/mcp-server-wuye-ai)** - Interact with capabilities of the CRIC Wuye AI platform, an intelligent assistant specifically for the property management industry.\n- <img height=\"12\" width=\"12\" src=\"https://www.crowdstrike.com/etc.clientlibs/crowdstrike/clientlibs/crowdstrike-common/resources/favicon.ico\" alt=\"CrowdStrike Logo\" /> **[CrowdStrike Falcon](https://github.com/CrowdStrike/falcon-mcp)** - Connects AI agents with the CrowdStrike Falcon platform for intelligent security analysis, providing programmatic access to detections, incidents, behaviors, threat intelligence, hosts, vulnerabilities, and identity protection capabilities.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/58433296\" alt=\"CTERA Edge Filer\" /> **[CTERA Edge Filer](https://github.com/ctera/mcp-ctera-edge)** - CTERA Edge Filer delivers intelligent edge caching and multiprotocol file access, enabling fast, secure access to files across core and remote sites.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/58433296\" alt=\"CTERA Portal\" /> **[CTERA Portal](https://github.com/ctera/mcp-ctera-core)** - CTERA Portal is a multi-tenant, multi-cloud platform that delivers a global namespace and unified management across petabytes of distributed content.\n- <img height=\"12\" width=\"12\" src=\"https://customer.io/favicon.ico\" alt=\"Customer.io Logo\" /> **[Customer.io](https://docs.customer.io/ai/mcp-server/)** - Let any LLM work directly with your Customer.io workspace to create segments, inspect user profiles, search for customers, and access workspace data. Analyze customer attributes, manage audience targeting, and explore your workspace without switching tabs.\n- <img height=\"12\" width=\"12\" src=\"https://app.cycode.com/img/favicon.ico\" alt=\"Cycode Logo\" /> **[Cycode](https://github.com/cycodehq/cycode-cli#mcp-command-experiment)** - Boost security in your dev lifecycle via SAST, SCA, Secrets & IaC scanning with [Cycode](https://cycode.com/).\n- <img height=\"12\" width=\"12\" src=\"http://app.itsdart.com/static/img/favicon.png\" alt=\"Dart Logo\" /> **[Dart](https://github.com/its-dart/dart-mcp-server)** - Interact with task, doc, and project data in [Dart](https://itsdart.com), an AI-native project management tool\n- <img height=\"12\" width=\"12\" src=\"https://cdn.bfldr.com/9AYANS2F/at/k8bgnnxhb4bggjk88r4x9snf/databricks-symbol-color.svg?auto=webp&format=png&width=12&height=13\" alt=\"Databricks Logo\" /> **[Databricks](https://docs.databricks.com/aws/en/generative-ai/mcp/)** - Connect to data, AI tools & agents, and the rest of the Databricks platform using turnkey managed MCP servers. Or, host your own custom MCP servers within the Databricks security and data governance boundary.\n- <img height=\"12\" width=\"12\" src=\"https://datahub.com/wp-content/uploads/2025/04/cropped-Artboard-1-32x32.png\" alt=\"DataHub Logo\" /> **[DataHub](https://github.com/acryldata/mcp-server-datahub)** - Search your data assets, traverse data lineage, write SQL queries, and more using [DataHub](https://datahub.com/) metadata.\n- <img height=\"12\" width=\"12\" src=\"https://www.datawrapper.de/favicon-32x32.png\" alt=\"Datawrapper logo\"> **[Datawrapper](https://github.com/palewire/datawrapper-mcp)** - A  Model Context Protocol (MCP) server for creating [Datawrapper](https://datawrapper.de) charts using AI assistants.\n- <img height=\"12\" width=\"12\" src=\"https://www.daytona.io/brand/social-daytona-icon.png\" alt=\"Daytona Logo\" /> **[Daytona](https://github.com/daytonaio/daytona/tree/main/apps/cli/mcp)** - Fast and secure execution of your AI generated code with [Daytona](https://daytona.io) sandboxes\n- <img height=\"12\" width=\"12\" src=\"https://debugg.ai/favicon.svg\" alt=\"Debugg AI Logo\" /> **[Debugg.AI](https://github.com/debugg-ai/debugg-ai-mcp)** - Zero-Config, Fully AI-Managed End-to-End Testing for any code gen platform via [Debugg.AI](https://debugg.ai) remote browsing test agents.\n- <img height=\"12\" width=\"12\" src=\"https://www.deepl.com/img/logo/deepl-logo-blue.svg\" alt=\"DeepL Logo\" /> **[DeepL](https://github.com/DeepLcom/deepl-mcp-server)** - Translate or rewrite text with [DeepL](https://deepl.com)'s very own AI models using [the DeepL API](https://developers.deepl.com/docs)\n- <img height=\"12\" width=\"12\" src=\"https://web-st.oss-cn-shanghai.aliyuncs.com/www/static/icon/bitbug_favicon.ico\" alt=\"DeepQ Logo\"> **[DeepQ](https://github.com/shenqingtech/deepq-financial-toolkit-mcp-server)** - DeepQ Technology's Financial Toolkit MCP Server is an Chinese Financial AI toolkit provides comprehensive financial data and analytical tool support for AI large language models.\n- <img height=\"12\" width=\"12\" src=\"https://defang.io/_next/static/media/defang-icon-dark-colour.25f95b77.svg\" alt=\"Defang Logo\" /> **[Defang](https://github.com/DefangLabs/defang/blob/main/src/pkg/mcp/README.md)** - Deploy your project to the cloud seamlessly with the [Defang](https://www.defang.io) platform without leaving your integrated development environment\n- <img height=\"12\" width=\"12\" src=\"https://deployhq.com/assets/favicon-357ebe39b58f28869358da83948e76e7cadfb0791c97af34abfe346f5e3ef634.png\" alt=\"DeployHQ Logo\" /> **[DeployHQ](https://github.com/deployhq/deployhq-mcp-server)** â€“ MCP server for DeployHQ API integration, enabling AI assistants to manage deployments, list projects, and monitor deployment status.\n- <img height=\"12\" width=\"12\" src=\"https://destinia.com/headers/ilusion/sunrise/dist/favicon/favicon-16x16.png?v=PCJysKzN\" alt=\"Destinia Logo\" /> **[Destinia](https://destinia.com/developers)** - Provider tools to search for hotels in Destinia and get listing details.\n- <img height=\"12\" width=\"12\" src=\"https://detailer.ginylil.com/favicon.ico\" alt=\"Detailer Logo\" /> **[Detailer](https://detailer.ginylil.com/)** â€“ Instantly generate rich, AI-powered documentation for your GitHub repositories. Designed for AI agents to gain deep project context before taking action.\n- <img height=\"12\" width=\"12\" src=\"https://devcycle.com/_next/image?url=%2Fassets%2Fbrand%2FColor-logo-mark.png&w=384&q=75\" alt=\"DevCycle Logo\" /> **[DevCycle](https://docs.devcycle.com/cli-mcp/mcp-getting-started)** - Create and monitor feature flags using natural language in your AI coding assistant.\n- <img height=\"12\" width=\"12\" src=\"https://www.devexpress.com/Content/Core/favicon.ico\" alt=\"DevExpress Logo\" /> **[DevExpress](https://docs.devexpress.com/GeneralInformation/405551/help-resources/dev-express-documentation-mcp-server-configure-an-ai-powered-assistant)** Documentation MCP server â€” Get instant, AI-powered access to 300,000+ help topics on [DevExpress](https://www.devexpress.com) UI Component APIs â€” right in the AI Coding Assistant/IDE of your choice.\n- <img height=\"12\" width=\"12\" src=\"https://www.devhub.com/img/upload/favicon-196x196-dh.png\" alt=\"DevHub Logo\" /> **[DevHub](https://github.com/devhub/devhub-cms-mcp)** - Manage and utilize website content within the [DevHub](https://www.devhub.com) CMS platform\n- <img height=\"12\" width=\"12\" src=\"https://devrev.ai/favicon.ico\" alt=\"DevRev Logo\" /> **[DevRev](https://github.com/devrev/mcp-server)** - An MCP server to integrate with DevRev APIs to search through your DevRev Knowledge Graph where objects can be imported from diff. Sources listed [here](https://devrev.ai/docs/import#available-sources).\n- <img height=\"12\" width=\"12\" src=\"https://dexpaprika.com/favicon.ico\" alt=\"DexPaprika Logo\" /> **[DexPaprika (CoinPaprika)](https://github.com/coinpaprika/dexpaprika-mcp)** - Access real-time DEX data, liquidity pools, token information, and trading analytics across multiple blockchain networks with [DexPaprika](https://dexpaprika.com) by CoinPaprika.\n- **[Diffusion](https://github.com/diffusiondata/diffusion-mcp-server)** - Connect to any Diffusion server to explore topics, create/update topics, manage sessions, configure features like topic views and metrics, and monitor the server.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/dolthub/dolt/raw/main/images/Dolt-Logo@3x.svg\" alt=\"Dolt Logo\" /> **[Dolt](https://github.com/dolthub/dolt-mcp)** - The official MCP server for version-controlled [Dolt](https://doltdb.com/) databases.\n- <img height=\"12\" width=\"12\" src=\"https://eu.getdot.ai/favicon.ico\" alt=\"GetDot.ai Logo\" /> **[Dot (GetDot.ai)](https://docs.getdot.ai/dot/integrations/mcp)** - Fetch, analyze or visualize data from your favorite database or data warehouse (Snowflake, BigQuery, Redshift, Databricks, Clickhouse, ...) with [Dot](https://getdot.ai), your AI Data Analyst. This remote MCP server is a one-click integration for user that have setup Dot.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/65421071?s=200&v=4\" alt=\"Drata Logo\" /> **[Drata](https://drata.com/mcp)** - Get hands-on with our experimental MCP serverâ€”bringing real-time compliance intelligence into your AI workflows.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/204530939?s=200&v=4\" alt=\"Dumpling AI Logo\" /> **[Dumpling AI](https://github.com/Dumpling-AI/mcp-server-dumplingai)** - Access data, web scraping, and document conversion APIs by [Dumpling AI](https://www.dumplingai.com/)\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/58178984\" alt=\"Dynatrace Logo\" /> **[Dynatrace](https://github.com/dynatrace-oss/dynatrace-mcp)** - Manage and interact with the [Dynatrace Platform ](https://www.dynatrace.com/platform) for real-time observability and monitoring.\n- <img height=\"12\" width=\"12\" src=\"https://e2b.dev/favicon.ico\" alt=\"E2B Logo\" /> **[E2B](https://github.com/e2b-dev/mcp-server)** - Run code in secure sandboxes hosted by [E2B](https://e2b.dev)\n- <img height=\"12\" width=\"12\" src=\"https://www.edgee.cloud/favicon.ico\" alt=\"Edgee Logo\" /> **[Edgee](https://github.com/edgee-cloud/mcp-server-edgee)** - Deploy and manage [Edgee](https://www.edgee.cloud) components and projects\n- <img height=\"12\" width=\"12\" src=\"https://static.edubase.net/media/brand/favicon/favicon-32x32.png\" alt=\"EduBase Logo\" /> **[EduBase](https://github.com/EduBase/MCP)** - Interact with [EduBase](https://www.edubase.net), a comprehensive e-learning platform with advanced quizzing, exam management, and content organization capabilities\n- <img height=\"12\" width=\"12\" src=\"https://www.elastic.co/favicon.ico\" alt=\"Elasticsearch Logo\" /> **[Elasticsearch](https://github.com/elastic/mcp-server-elasticsearch)** - Query your data in [Elasticsearch](https://www.elastic.co/elasticsearch)\n- <img height=\"12\" width=\"12\" src=\"https://www.elastic.co/favicon.ico\" alt=\"Elasticsearch Memory Logo\" /> **[Elasticsearch Memory](https://github.com/fredac100/elasticsearch-memory-mcp)** - Persistent memory with hierarchical categorization, semantic search, and intelligent auto-detection. Install via [PyPI](https://pypi.org/project/elasticsearch-memory-mcp/).\n- <img height=\"12\" width=\"12\" src=\"https://elasticemail.com/favicon.ico\" alt=\"Elastic Email Logo\" /> **[Elastic Email](https://github.com/ElasticEmail/elasticemail-mcp-server)** - Elastic Email MCP Server delivers full-scale email capabilities to the next generation of AI agents and MCP-compatible environments.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/EmberAGI/arbitrum-vibekit/blob/main/img/Ember%20Black.png?raw=true\" alt=\"Ember AI Logo\" /> **[Ember AI](https://docs.emberai.xyz/)** - A unified MCP server that enables AI agents to execute cross-chain DeFi strategies.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/656eaf5c6da3527caf362363/656ecc07555afac40df4c40e_Facicon.png\" alt=\"Endor Labs Logo\" /> **[Endor Labs](https://docs.endorlabs.com/deployment/ide/mcp/)** - Find and fix security risks in you code. Integrate [Endor Labs](https://endorlabs.com) to scan and secure your code from vulnerabilities and secret leaks.\n- <img height=\"12\" width=\"12\" src=\"https://esignatures.com/favicon.ico\" alt=\"eSignatures Logo\" /> **[eSignatures](https://github.com/esignaturescom/mcp-server-esignatures)** - Contract and template management for drafting, reviewing, and sending binding contracts.\n- <img height=\"12\" width=\"12\" src=\"https://rainmaker.espressif.com/favicon.ico\" alt=\"ESP RainMaker Logo\" /> **[ESP RainMaker](https://github.com/espressif/esp-rainmaker-mcp)** - Official Espressif MCP Server to Control and Manage ESP RainMaker Devices.\n- <img height=\"12\" width=\"12\" src=\"https://exa.ai/images/favicon-32x32.png\" alt=\"Exa Logo\" /> **[Exa](https://github.com/exa-labs/exa-mcp-server)** - Search Engine made for AIs by [Exa](https://exa.ai)\n- <img height=\"12\" width=\"12\" src=\"https://www.explorium.ai/wp-content/uploads/2025/04/Favicon-Purple-512x512-1-150x150.png\" alt=\"Explorium Logo\" /> **[Explorium](https://github.com/explorium-ai/mcp-explorium)** - B2B data and infrastructure for AI SDR & GTM Agents [Explorium](https://www.explorium.ai)\n- **[FalkorDB](https://github.com/FalkorDB/FalkorDB-MCPServer)** - FalkorDB graph database server get schema and read/write-cypher [FalkorDB](https://www.falkordb.com)\n- <img height=\"12\" width=\"12\" src=\"https://fetchserp.com/icon.png\" alt=\"fetchSERP Logo\" /> **[fetchSERP](https://github.com/fetchSERP/fetchserp-mcp-server-node)** - All-in-One SEO & Web Intelligence Toolkit API [fetchSERP](https://www.fetchserp.com/)\n- <img height=\"12\" width=\"12\" src=\"https://fewsats.com/favicon.svg\" alt=\"Fewsats Logo\" /> **[Fewsats](https://github.com/Fewsats/fewsats-mcp)** - Enable AI Agents to purchase anything in a secure way using [Fewsats](https://fewsats.com)\n- <img height=\"12\" width=\"12\" src=\"https://fibery.io/favicon.svg\" alt=\"Fibery Logo\" /> **[Fibery](https://github.com/Fibery-inc/fibery-mcp-server)** - Perform queries and entity operations in your [Fibery](https://fibery.io) workspace.\n- <img height=\"12\" width=\"12\" src=\"https://financialdatasets.ai/favicon.ico\" alt=\"Financial Datasets Logo\" /> **[Financial Datasets](https://github.com/financial-datasets/mcp-server)** - Stock market API made for AI agents\n- <img height=\"12\" width=\"12\" src=\"https://www.gstatic.com/devrel-devsite/prod/v7aeef7f1393bb1d75a4489145c511cdd5aeaa8e13ad0a83ec1b5b03612e66330/firebase/images/favicon.png\" alt=\"Firebase Logo\" /> **[Firebase](https://github.com/firebase/firebase-tools/blob/master/src/mcp)** - Firebase's experimental [MCP Server](https://firebase.google.com/docs/cli/mcp-server) to power your AI Tools\n- <img height=\"12\" width=\"12\" src=\"https://firecrawl.dev/favicon.ico\" alt=\"Firecrawl Logo\" /> **[Firecrawl](https://github.com/firecrawl/firecrawl-mcp-server)** - Extract web data with [Firecrawl](https://firecrawl.dev)\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/100200663?s=200&v=4\" alt=\"Firefly Logo\" /> **[Firefly](https://github.com/gofireflyio/firefly-mcp)** - Integrates, discovers, manages, and codifies cloud resources with [Firefly](https://firefly.ai).\n- <img height=\"12\" width=\"12\" src=\"https://fireproof.storage/favicon.ico\" alt=\"Fireproof Logo\" /> **[Fireproof](https://github.com/fireproof-storage/mcp-database-server)** - Immutable ledger database with live synchronization\n- <img height=\"12\" width=\"12\" src=\"https://fixparser.dev/favicon.ico\" alt=\"FIXParser Logo\" /> **[FIXParser](https://gitlab.com/logotype/fixparser/-/tree/main/packages/fixparser-plugin-mcp)** - A modern FIX Protocol engine for AI-powered trading agents\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/52471808\" alt=\"Fluid Attacks Logo\" /> **[Fluid Attacks](https://github.com/fluidattacks/mcp)** - Interact with the [Fluid Attacks](https://fluidattacks.com/) API, enabling vulnerability management, organization insights, and GraphQL query execution.\n- <img height=\"12\" width=\"12\" src=\"https://flutterwave.com/favicon.ico\" alt=\"Flutterwave Logo\" /> **[Flutterwave](https://github.com/bajoski34/mcp-flutterwave/tree/main)** - Interact with Flutterwave payment solutions API, to manage transactions, payment links and more.\n- <img height=\"12\" width=\"12\" src=\"https://forevervm.com/icon.png\" alt=\"ForeverVM Logo\" /> **[ForeverVM](https://github.com/jamsocket/forevervm/tree/main/javascript/mcp-server)** - Run Python in a code sandbox.\n- <img height=\"12\" width=\"12\" src=\"https://gcore.com/assets/favicon/favicon-16x16.png\" alt=\"Gcore Logo\" /> **[Gcore](https://github.com/G-Core/gcore-mcp-server)** - Interact with Gcore platform services via LLM assistants, providing unified access to CDN, GPU Cloud & AI Inference, Video Streaming, WAAP, and cloud resources including instances and networks.\n- <img height=\"12\" width=\"12\" src=\"https://app.gibsonai.com/favicon.ico\" alt=\"GibsonAI Logo\" /> **[GibsonAI](https://github.com/GibsonAI/mcp)** - AI-Powered Cloud databases: Build, migrate, and deploy database instances with AI\n- <img height=\"12\" width=\"12\" src=\"https://gitea.com/assets/img/favicon.svg\" alt=\"Gitea Logo\" /> **[Gitea](https://gitea.com/gitea/gitea-mcp)** - Interact with Gitea instances with MCP.\n- <img height=\"12\" width=\"12\" src=\"https://gitee.com/favicon.ico\" alt=\"Gitee Logo\" /> **[Gitee](https://github.com/oschina/mcp-gitee)** - Gitee API integration, repository, issue, and pull request management, and more.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/5ee25cbe47310017adf964da/6323888a9b9f4e22a7bc766b_GG%20Favicon.svg\" alt=\"GitGuardian Logo\" /> **[GitGuardian](https://github.com/GitGuardian/gg-mcp)** - GitGuardian official MCP server - Scan projects using GitGuardian's industry-leading API, which features over 500 secret detectors to prevent credential leaks before they reach public repositories. Resolve security incidents directly with rich contextual data for rapid, automated remediation.\n- <img height=\"12\" width=\"12\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" alt=\"GitHub Logo\" /> **[GitHub](https://github.com/github/github-mcp-server)** - GitHub's official MCP Server.\n- <img height=\"12\" width=\"12\" src=\"https://www.gitkraken.com/wp-content/uploads/2021/03/android-chrome-144x144-1.png\" alt=\"GitKraken Logo\" /> **[GitKraken](https://github.com/gitkraken/gk-cli?tab=readme-ov-file#mcp-server)** - A CLI for interacting with GitKraken APIs. Includes an MCP server via `gk mcp` that not only wraps GitKraken APIs, but also Jira, GitHub, GitLab, and more.\n- <img height=\"12\" width=\"12\" src=\"https://gitlab.com/favicon.ico\" alt=\"GitLab Logo\" /> **[GitLab](https://docs.gitlab.com/user/gitlab_duo/model_context_protocol/mcp_server/)** - GitLab's official MCP server enabling AI tools to securely access GitLab project data, manage issues, and perform repository operations via OAuth 2.0.\n- <img height=\"12\" width=\"12\" src=\"https://app.glean.com/images/favicon3-196x196.png\" alt=\"Glean Logo\" /> **[Glean](https://github.com/gleanwork/mcp-server)** - Enterprise search and chat using Glean's API.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.jsdelivr.net/gh/jsdelivr/globalping-media@refs/heads/master/icons/android-chrome-192x192.png\" alt=\"Globalping Logo\" /> **[Globalping](https://github.com/jsdelivr/globalping-mcp-server)** - Access a network of thousands of probes to run network commands like ping, traceroute, mtr, http and DNS resolve.\n- <img height=\"12\" width=\"12\" src=\"https://gnucleus.ai/favicon.ico\" alt=\"gNucleus Logo\" /> **[gNucleus Text-To-CAD](https://github.com/gNucleus/text-to-cad-mcp)** - Generate CAD parts and assemblies from text using gNucleus AI models.\n- <img height=\"12\" width=\"12\" src=\"https://api.gologin.com/favicon.ico\" alt=\"GoLogin Logo\" /> **[GoLogin MCP server](https://github.com/gologinapp/gologin-mcp)** - Manage your GoLogin browser profiles and automation directly through AI conversations!\n- <img height=\"12\" width=\"12\" src=\"https://www.gstatic.com/cgc/favicon.ico\" alt=\"Google Cloud Logo\" /> **[Google Cloud Run](https://github.com/GoogleCloudPlatform/cloud-run-mcp)** - Deploy code to Google Cloud Run\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/3717923?s=200&v=4\" alt=\"Google Maps Platform Logo\" /> **[Google Maps Platform Code Assist](https://github.com/googlemaps/platform-ai/tree/main/packages/code-assist)** - Ground agents on fresh, official documentation and code samples for optimal geo-related guidance and code..\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6605a2979ff17b2cd1939cd4/6605a460de47e7596ed84f06_icon256.png\" alt=\"gotoHuman Logo\" /> **[gotoHuman](https://github.com/gotohuman/gotohuman-mcp-server)** - Human-in-the-loop platform - Allow AI agents and automations to send requests for approval to your [gotoHuman](https://www.gotohuman.com) inbox.\n- <img height=\"12\" width=\"12\" src=\"https://grafana.com/favicon.ico\" alt=\"Grafana Logo\" /> **[Grafana](https://github.com/grafana/mcp-grafana)** - Search dashboards, investigate incidents and query datasources in your Grafana instance\n- <img height=\"12\" width=\"12\" src=\"https://grafbase.com/favicon.ico\" alt=\"Grafbase Logo\" /> **[Grafbase](https://github.com/grafbase/grafbase/tree/main/crates/mcp)** - Turn your GraphQL API into an efficient MCP server with schema intelligence in a single command.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/5f5e90c17e7c9eb95c7acb17/61d3457a519242f2c75c725c_favicon.png\" alt=\"Grain Logo\" /> **[Grain](https://grain.com/release-note/06-18-2025)** - Access your Grain meetings notes & transcripts directly in claude and generate reports with native Claude Prompts.\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/KCOWBYLKunDff1Dr452y6EfjiU.png\" alt=\"Graphlit Logo\" /> **[Graphlit](https://github.com/graphlit/graphlit-mcp-server)** - Ingest anything from Slack to Gmail to podcast feeds, in addition to web crawling, into a searchable [Graphlit](https://www.graphlit.com) project.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/64a5291e7847ac04fe1531ad/64a529af2f1fc7debc26f2a6_favicon-32x32.avif\" alt=\"Gremlin favicon\" /> **[Gremlin](https://github.com/gremlin/mcp)** - The official [Gremlin](https://www.gremlin.com) MCP server. Analyze your reliability posture, review recent tests and chaos engineering experiments, and create detailed reports.\n- <img height=\"12\" width=\"12\" src=\"https://greptime.com/favicon.ico\" alt=\"Greptime Logo\" /> **[GreptimeDB](https://github.com/GreptimeTeam/greptimedb-mcp-server)** - Provides AI assistants with a secure and structured way to explore and analyze data in [GreptimeDB](https://github.com/GreptimeTeam/greptimedb).\n- <img height=\"12\" width=\"12\" src=\"https://growi.org/assets/images/favicon.ico\" alt=\"GROWI Logo\" /> **[GROWI](https://github.com/growilabs/growi-mcp-server)** - Official MCP Server to integrate with GROWI APIs.\n- <img height=\"12\" width=\"12\" src=\"https://gyazo.com/favicon.ico\" alt=\"Gyazo Logo\" /> **[Gyazo](https://github.com/nota/gyazo-mcp-server)** - Search, fetch, upload, and interact with Gyazo images, including metadata and OCR data.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6374050260446c42f94dc90f/63d828be3e13d32ee6973f35_favicon-32x32.png\" alt=\"Harper Logo\" /> **[Harper](https://github.com/HarperDB/mcp-server)** - An MCP server providing an interface for MCP clients to access data within [Harper](https://www.harpersystems.dev/).\n- <img height=\"12\" width=\"12\" src=\"https://www.herokucdn.com/favicons/favicon.ico\" alt=\"Heroku Logo\" /> **[Heroku](https://github.com/heroku/heroku-mcp-server)** - Interact with the Heroku Platform through LLM-driven tools for managing apps, add-ons, dynos, databases, and more.\n- <img height=\"12\" width=\"12\" src=\"https://heyoncall.com/favicon.ico\" alt=\"HeyOnCall Logo\" /> **[HeyOnCall](https://heyoncall.com/blog/mcp-server-for-paging-a-human)** - Page a human, sending critical or non-critical alerts to the free [HeyOnCall](https://heyoncall.com/) iOS or Android apps.\n- <img height=\"12\" width=\"12\" src=\"https://hillnote.com/favicon.ico\" alt=\"Hillnote Logo\" /> **[Hillnote](https://github.com/Rajathbail/hillnote-mcp-server)** - search, edit, save and create documents to your [Hillnote](https://hillnote.com) workspace, a markdown-first editor that stores files locally.\n- <img height=\"12\" width=\"12\" src=\"https://hiveintelligence.xyz/favicon.ico\" alt=\"Hive Intelligence Logo\" /> **[Hive Intelligence](https://github.com/hive-intel/hive-crypto-mcp)** - Ultimate cryptocurrency MCP for AI assistants with unified access to crypto, DeFi, and Web3 analytics\n- <img height=\"12\" width=\"12\" src=\"https://www.hiveflow.ai/favicon.ico\" alt=\"Hiveflow Logo\" /> **[Hiveflow](https://github.com/hiveflowai/hiveflow-mcp-server)** - Create, manage, and execute agentic AI workflows directly from your assistant.\n- <img height=\"12\" width=\"12\" src=\"https://img.alicdn.com/imgextra/i3/O1CN01d9qrry1i6lTNa2BRa_!!6000000004364-2-tps-218-200.png\" alt=\"Hologres Logo\" /> **[Hologres](https://github.com/aliyun/alibabacloud-hologres-mcp-server)** - Connect to a [Hologres](https://www.alibabacloud.com/en/product/hologres) instance, get table metadata, query and analyze data.\n- <img height=\"12\" width=\"12\" src=\"https://brew.sh/assets/img/favicon.ico\" alt=\"Homebrew Logo\" /> **[Homebrew](https://docs.brew.sh/MCP-Server)** Allows [Homebrew](https://brew.sh) users to run Homebrew commands locally.\n- <img height=\"12\" width=\"12\" src=\"https://www.honeycomb.io/favicon.ico\" alt=\"Honeycomb Logo\" /> **[Honeycomb](https://github.com/honeycombio/honeycomb-mcp)** Allows [Honeycomb](https://www.honeycomb.io/) Enterprise customers to query and analyze their data, alerts, dashboards, and more; and cross-reference production behavior with the codebase.\n- <img height=\"12\" width=\"12\" src=\"https://hopx.ai/favicon.ico\" alt=\"HOPX Logo\" /> **[HOPX](https://github.com/hopx-ai/mcp)** - Execute Python, JavaScript, Bash, and Go code in isolated cloud containers with sub-150ms startup times. Pre-installed data science libraries (pandas, numpy, matplotlib) for AI-powered data analysis and code testing.\n- <img height=\"12\" width=\"12\" src=\"https://static.hsinfrastatic.net/StyleGuideUI/static-3.438/img/sprocket/favicon-32x32.png\" alt=\"HubSpot Logo\" /> **[HubSpot](https://developer.hubspot.com/mcp)** - Connect, manage, and interact with [HubSpot](https://www.hubspot.com/) CRM data\n- <img height=\"12\" width=\"12\" src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.svg\" alt=\"HuggingFace Logo\" /> **[Hugging Face](https://huggingface.co/settings/mcp)** - Connect to the Hugging Face Hub APIs programmatically: semantic search for spaces and papers, exploration of datasets and models, and access to all compatible MCP Gradio tool spaces!\n- <img height=\"12\" width=\"12\" src=\"https://hunter.io/favicon.ico\" alt=\"Hunter Logo\" /> **[Hunter](https://github.com/hunter-io/hunter-mcp)** - Interact with the [Hunter API](https://hunter.io) to get B2B data using natural language.\n- <img height=\"12\" width=\"12\" src=\"https://app.hyperbolic.xyz/hyperbolic-logo.svg\" alt=\"Hyperbolic Labs Logo\" /> **[Hyperbolic](https://github.com/HyperbolicLabs/hyperbolic-mcp)** - Interact with Hyperbolic's GPU cloud, enabling agents and LLMs to view and rent available GPUs, SSH into them, and run GPU-powered workloads for you.\n- <img height=\"12\" width=\"12\" src=\"https://hyperbrowser-assets-bucket.s3.us-east-1.amazonaws.com/Hyperbrowser-logo.png\" alt=\"Hyperbrowsers23 Logo\" /> **[Hyperbrowser](https://github.com/hyperbrowserai/mcp)** - [Hyperbrowser](https://www.hyperbrowser.ai/) is the next-generation platform empowering AI agents and enabling effortless, scalable browser automation.\n- **[IBM watsonx.data intelligence](https://github.com/IBM/data-intelligence-mcp-server)** - Find, understand, and work with your data in the watsonx.data intelligence governance & catalog, data quality, data lineage, and data product hub\n- **[IBM wxflows](https://github.com/IBM/wxflows/tree/main/examples/mcp/javascript)** - Tool platform by IBM to build, test and deploy tools for any data source\n- <img height=\"12\" width=\"12\" src=\"https://improvedigital.com/favicon.ico\" alt=\"Improve Digital Icon\" /> **[Improve Digital Publisher MCP](https://github.com/azerion/improvedigital-publisher-mcp-server)** - An MCP server that enables publishers to integrate [Improve Digitalâ€™s](https://improvedigital.com/) inventory management system with their AI tools or agents.\n- <img height=\"12\" width=\"12\" src=\"https://www.getinboxzero.com/icon.png\" alt=\"Inbox Zero Logo\" /> **[Inbox Zero](https://github.com/elie222/inbox-zero/tree/main/apps/mcp-server)** - AI personal assistant for email [Inbox Zero](https://www.getinboxzero.com)\n- <img height=\"12\" width=\"12\" src=\"https://www.inflectra.com/Favicon.ico\" alt=\"Inflectra Logo\" /> **[Inflectra Spira](https://github.com/Inflectra/mcp-server-spira)** - Connect to your instance of the SpiraTest, SpiraTeam or SpiraPlan application lifecycle management platform by [Inflectra](https://www.inflectra.com)\n- <img height=\"12\" width=\"12\" src=\"https://cdn-web.infobip.com/uploads/2025/05/infobip-symbol-orange.png\" alt=\"Infobip Logo\" /> **[Infobip](https://github.com/infobip/mcp)** - MCP server for integrating [Infobip](https://www.infobip.com/) global cloud communication platform. It equips AI agents with communication superpowers, allowing them to send and receive SMS and RCS messages, interact with WhatsApp and Viber, automate communication workflows, and manage customer data, all in a production-ready environment.\n- <img height=\"12\" width=\"12\" src=\"https://inkeep.com/favicon.ico\" alt=\"Inkeep Logo\" /> **[Inkeep](https://github.com/inkeep/mcp-server-python)** - RAG Search over your content powered by [Inkeep](https://inkeep.com)\n- <img height=\"12\" width=\"12\" src=\"https://integration.app/favicon.ico\" alt=\"Integration App Icon\" /> **[Integration App](https://github.com/integration-app/mcp-server)** - Interact with any other SaaS applications on behalf of your customers.\n- <img height=\"12\" width=\"12\" src=\"https://www.ip2location.io/favicon.ico\" alt=\"IP2Location.io Icon\" /> **[IP2Location.io](https://github.com/ip2location/mcp-ip2location-io)** - Interact with IP2Location.io API to retrieve the geolocation information for an IP address.\n- <img height=\"12\" width=\"12\" src=\"https://static.iplocate.io/custom/logo-square-rounded.png\" alt=\"IPLocate Icon\" /> **[IPLocate](https://github.com/iplocate/mcp-server-iplocate)** - Look up IP address geolocation, network information, detect proxies and VPNs, and find abuse contact details using [IPLocate.io](https://www.iplocate.io)\n- <img height=\"12\" width=\"12\" src=\"https://jellyfish.co/favicon.ico\" alt=\"Jellyfish Logo\" /> **[Jellyfish](https://github.com/Jellyfish-AI/jellyfish-mcp)** â€“ Give your AI agent context about your team's software engineering allocations and workflow via the [Jellyfish](https://jellyfish.co) platform\n- <img height=\"12\" width=\"12\" src=\"https://jenkins.io/images/logos/jenkins/jenkins.svg\" alt=\"Jenkins Logo\" /> **[Jenkins](https://plugins.jenkins.io/mcp-server/)** - Official Jenkins MCP Server plugin enabling AI assistants to manage builds, check job statuses, retrieve logs, and integrate with CI/CD pipelines through standardized MCP interface.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.simpleicons.org/jetbrains\" /> **[JetBrains](https://www.jetbrains.com/help/idea/mcp-server.html)** â€“ Work on your code with JetBrains IDEs: IntelliJ IDEA, PhpStorm, etc.\n- <img height=\"12\" width=\"12\" src=\"https://speedmedia.jfrog.com/08612fe1-9391-4cf3-ac1a-6dd49c36b276/media.jfrog.com/wp-content/uploads/2019/04/20131046/Jfrog16-1.png\" alt=\"JFrog Logo\" /> **[JFrog](https://github.com/jfrog/mcp-jfrog)** - Model Context Protocol (MCP) Server for the [JFrog](https://jfrog.com/) Platform API, enabling repository management, build tracking, release lifecycle management, and more.\n- <img height=\"12\" width=\"12\" src=\"https://kagi.com/favicon.ico\" alt=\"Kagi Logo\" /> **[Kagi Search](https://github.com/kagisearch/kagimcp)** - Search the web using Kagi's search API\n- ğŸ“… **[Kalendis](https://github.com/kalendis-dev/kalendis-mcp)** - Generate TypeScript clients and API route handlers for the Kalendis scheduling API across multiple frameworks (Next.js, Express, Fastify, NestJS), streamlining integration of availability management and booking functionality.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/319096?s=48&v=4\" alt=\"Kaltura Logo\" /> **[Kaltura](https://github.com/kaltura/mcp-events)** - Manage [Kaltura Event Platform](https://corp.kaltura.com/blog/best-virtual-event-platform/#what-is-a-virtual-event-platform-0). Provide tools and resources for creating, managing, and interacting with Kaltura virtual events.\n- <img height=\"12\" width=\"12\" src=\"https://kash.click/favicon.ico\" alt=\"Kash Logo\" /> **[Kash.click](https://github.com/paracetamol951/caisse-enregistreuse-mcp-server)** - Gives AI access to your sales, clients, orders, tax information, payments, and all the insights on your business\n- <img height=\"12\" width=\"12\" src=\"https://connection.keboola.com/favicon.ico\" alt=\"Keboola Logo\" /> **[Keboola](https://github.com/keboola/keboola-mcp-server)** - Build robust data workflows, integrations, and analytics on a single intuitive platform.\n- <img height=\"12\" width=\"12\" src=\"https://mcp.onkernel.com/favicon.svg\" alt=\"Kernel Logo\" /> **[Kernel](https://github.com/onkernel/kernel-mcp-server)** â€“ Access Kernel's cloudâ€‘based browsers via MCP.\n- <img height=\"12\" width=\"12\" src=\"https://keywordseverywhere.com/favicon.ico\" alt=\"Keywords Everywhere Logo\" /> **[Keywords Everywhere](https://api.keywordseverywhere.com/docs/#/mcp_integration)** â€“ Access SEO data through the official Keywords Everywhere API MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://keywordspeopleuse.com/favicon.ico\" alt=\"KeywordsPeopleUse Logo\" /> **[KeywordsPeopleUse.com](https://github.com/data-skunks/kpu-mcp)** - Find questions people ask online with [KeywordsPeopleUse](https://keywordspeopleuse.com).\n- <img height=\"12\" width=\"12\" src=\"https://kiln.tech/images/animated_logo.svg\" alt=\"Kiln Logo\" /> **[Kiln](https://github.com/Kiln-AI/Kiln)** - A free open-source platform for building production-ready AI systems. It supports RAG pipelines, AI agents, MCP tool-calling, evaluations, synthetic data generation, and fine-tuning â€” all in one unified framework by [Kiln-AI](https://kiln.tech/).\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/4815054\" alt=\"Kintone Logo\" /> **[Kintone](https://github.com/kintone/mcp-server)** - The official local MCP server for [Kintone](https://kintone.com).\n- <img height=\"12\" width=\"12\" src=\"https://kirokuforms.com/favicon.svg\" alt=\"KirokuForms Logo\" /> **[KirokuForms](https://www.kirokuforms.com/ai/mcp)** - [KirokuForms](https://www.kirokuforms.com) is an AI-powered form platform combining professional form building with Human-in-the-Loop (HITL) capabilities. Create custom forms, collect submissions, and integrate human oversight into AI workflows through [MCP integration](https://kirokuforms.com/ai/mcp).\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/kiteworks/mcp/main/docs/img/kiteworks_logo-small.png\" alt=\"Kiteworks Logo\" /> **[Kiteworks](https://github.com/kiteworks/mcp)** - Official MCP server to interact with the [Kiteworks Private Data Network (PDN) platform](https://kiteworks.com).\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/klavis-ai/klavis/main/static/klavis-ai.png\" alt=\"Klavis Logo\" /> **[Klavis ReportGen](https://github.com/Klavis-AI/klavis/tree/main/mcp_servers/report_generation)** - Create professional reports from a simple user query.\n- <img height=\"12\" width=\"12\" src=\"https://www.klaviyo.com/media/Favicon-16by16.png\" alt=\"Klaviyo Logo\" /> **[Klaviyo](https://developers.klaviyo.com/en/docs/klaviyo_mcp_server)** - Interact with your [Klaviyo](https://www.klaviyo.com/) marketing data.\n- <img height=\"12\" width=\"12\" src=\"https://platform.kluster.ai/logo-light.svg\" alt=\"kluster.ai Logo\" /> **[kluster.ai](https://docs.kluster.ai/get-started/mcp/overview/)** - kluster.ai provides MCP servers that bring AI services directly into your development workflow, including guardrails like hallucination detection.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6347ea26001f0287c592ff91/649953ef7a9ffe1f3e492b5a_Knit%20Logo.svg\" alt=\"Knit Logo\" /> **[Knit MCP Server](https://developers.getknit.dev/docs/knit-mcp-server-getting-started)** - Production-ready remote MCP servers that enable you to connect with 10000+ tools across CRM, HRIS, Payroll, Accounting, ERP, Calendar, Expense Management, and Chat categories.\n- <img height=\"12\" width=\"12\" src=\"https://knock.app/favicon/favicon-dark.svg\" alt=\"Knock Logo\" /> **[Knock MCP Server](https://github.com/knocklabs/agent-toolkit#model-context-protocol-mcp)** - Send product and customer messaging across email, in-app, push, SMS, Slack, MS Teams.\n- <img height=\"12\" width=\"12\" src=\"https://kumo-sdk-public.s3.us-west-2.amazonaws.com/rfm-colabs/kumo_ai_logo.jpeg\" alt=\"Kumo Logo\" /> **[Kumo](https://github.com/kumo-ai/kumo-rfm-mcp)** - MCP Server to interact with KumoRFM, a foundation model for generating predictions from your relational data.\n- <img height=\"12\" width=\"12\" src=\"https://www.kurrent.io/favicon.ico\" alt=\"Kurrent Logo\" /> **[KurrentDB](https://github.com/kurrent-io/mcp-server)** - This is a simple MCP server to help you explore data and prototype projections faster on top of KurrentDB.\n- <img height=\"12\" width=\"12\" src=\"https://kuzudb.com/favicon.ico\" alt=\"Kuzu Logo\" /> **[Kuzu](https://github.com/kuzudb/kuzu-mcp-server)** - This server enables LLMs to inspect database schemas and execute queries on the provided Kuzu graph database. See [blog](https://blog.kuzudb.com/post/2025-03-23-kuzu-mcp-server/)) for a debugging use case.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/187484914\" alt=\"KWDB Logo\" /> **[KWDB](https://github.com/KWDB/kwdb-mcp-server)** - Reading, writing, querying, modifying data, and performing DDL operations with data in your KWDB Database.\n- <img height=\"12\" width=\"12\" src=\"https://kweenkl.com/favicon.ico\" alt=\"kweenkl Logo\" /> **[kweenkl](https://github.com/antoinedelorme/kweenkl-mcp)** - Send push notifications from AI assistants using natural language. Pre-launch demo available with example webhook token.\n- <img height=\"12\" width=\"12\" src=\"https://labelstud.io/favicon-16x16.png\" alt=\"Label Studio Logo\" /> **[Label Studio](https://github.com/HumanSignal/label-studio-mcp-server)** - Open Source data labeling platform.\n- <img src=\"https://avatars.githubusercontent.com/u/188884511?s=48&v=4\" alt=\"Lambda Capture\" width=\"12\" height=\"12\"> **[Lambda Capture](https://github.com/lambda-capture/mcp-server)** - Macroeconomic Forecasts & Semantic Context from Federal Reserve, Bank of England, ECB.\n- <img src=\"https://www.lambdatest.com/resources/images/header/professional-service.svg\" alt=\"LambdaTest MCP server\" width=\"12\" height=\"12\"> **[LambdaTest](https://www.lambdatest.com/mcp)** - LambdaTest MCP Servers ranging from Accessibility, SmartUI, Automation, and HyperExecute allows you to connect AI assistants with your testing workflow, streamlining setup, analyzing failures, and generating fixes to speed up testing and improve efficiency.\n- <img height=\"12\" width=\"12\" src=\"https://langfuse.com/favicon.ico\" alt=\"Langfuse Logo\" /> **[Langfuse Prompt Management](https://github.com/langfuse/mcp-server-langfuse)** - Open-source tool for collaborative editing, versioning, evaluating, and releasing prompts.\n- <img height=\"12\" width=\"12\" src=\"https://laratranslate.com/favicon.ico\" alt=\"Lara Translate Logo\" /> **[Lara Translate](https://github.com/translated/lara-mcp)** - MCP Server for Lara Translate API, enabling powerful translation capabilities with support for language detection and context-aware translations.\n- <img height=\"12\" width=\"12\" src=\"https://last9.io/favicon.png\" alt=\"Last9 Logo\" /> **[Last9](https://github.com/last9/last9-mcp-server)** - Seamlessly bring real-time production contextâ€”logs, metrics, and tracesâ€”into your local environment to auto-fix code faster.\n- <img height=\"12\" width=\"12\" src=\"https://www.launchdarkly.com/favicon.ico\" alt=\"LaunchDarkly Logo\" /> **[LaunchDarkly](https://github.com/launchdarkly/mcp-server)** - LaunchDarkly is a continuous delivery platform that provides feature flags as a service and allows developers to iterate quickly and safely.\n- <img height=\"12\" width=\"12\" src=\"https://www.line.me/favicon-32x32.png\" alt=\"LINE Logo\" /> **[LINE](https://github.com/line/line-bot-mcp-server)** - Integrates the LINE Messaging API to connect an AI Agent to the LINE Official Account.\n- <img height=\"12\" width=\"12\" src=\"https://linear.app/favicon.ico\" alt=\"Linear Logo\" /> **[Linear](https://linear.app/docs/mcp)** - Search, create, and update Linear issues, projects, and comments.\n- <img height=\"12\" width=\"12\" src=\"https://lingo.dev/favicon.ico\" alt=\"Lingo.dev Logo\" /> **[Lingo.dev](https://github.com/lingodotdev/lingo.dev/blob/main/mcp.md)** - Make your AI agent speak every language on the planet, using [Lingo.dev](https://lingo.dev) Localization Engine.\n- <img height=\"12\" width=\"12\" src=\"https://ligo.ertiqah.com/favicon.avif\" alt=\"LiGo Logo\" /> **[LinkedIn MCP Runner](https://github.com/ertiqah/linkedin-mcp-runner)** - Write, edit, and schedule LinkedIn posts right from ChatGPT and Claude with [LiGo](https://ligo.ertiqah.com/).\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/175112039?s=200&v=4\" alt=\"Linkup Logo\" /> **[Linkup](https://github.com/LinkupPlatform/js-mcp-server)** - (JS version) MCP server that provides web search capabilities through Linkup's advanced search API. This server enables AI assistants and development tools to perform intelligent web searches with natural language queries.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/175112039?s=200&v=4\" alt=\"Linkup Logo\" /> **[Linkup](https://github.com/LinkupPlatform/python-mcp-server)** - (Python version) MCP server that provides web search capabilities through Linkup's advanced search API. This server enables AI assistants and development tools to perform intelligent web searches with natural language queries.\n- <img src=\"https://avatars.githubusercontent.com/u/149083471\" alt=\"Lippia.io\" width=\"12\" height=\"12\"> **[Lippia](https://github.com/Lippia-io/Lippia-MCP-Server/blob/main/getting-started.md)** - MCP Server to accelerate Test Automation using Lippia Framework.\n- <img src=\"https://gornschool.com/gorn.png\" alt=\"Lisply\" width=\"12\" height=\"12\"> **[Lisply](https://github.com/gornskew/lisply-mcp)** - Flexible frontend for compliant Lisp-speaking backends.\n- <img height=\"12\" width=\"12\" src=\"https://litmus.io/favicon.ico\" alt=\"Litmus.io Logo\" /> **[Litmus.io](https://github.com/litmusautomation/litmus-mcp-server)** - Official MCP server for configuring [Litmus](https://litmus.io) Edge for Industrial Data Collection, Edge Analytics & Industrial AI.\n- <img height=\"12\" width=\"12\" src=\"https://liveblocks.io/favicon.ico\" alt=\"Liveblocks Logo\" /> **[Liveblocks](https://github.com/liveblocks/liveblocks-mcp-server)** - Readyâ€‘made features for AI & human collaborationâ€”use this to develop your [Liveblocks](https://liveblocks.io) app quicker.\n- <img height=\"12\" width=\"12\" src=\"https://logfire.pydantic.dev/favicon.ico\" alt=\"Logfire Logo\" /> **[Logfire](https://github.com/pydantic/logfire-mcp)** - Provides access to OpenTelemetry traces and metrics through Logfire.\n- <img height=\"12\" width=\"12\" src=\"https://make.magicmealkits.com/favicon.ico\" alt=\"Magic Meal Kits Logo\" /> **[Magic Meal Kits](https://github.com/pureugong/mmk-mcp)** - Unleash Make's Full Potential by [Magic Meal Kits](https://make.magicmealkits.com/)\n- <img height=\"12\" width=\"12\" src=\"https://www.mailgun.com/favicon.ico\" alt=\"Mailgun Logo\" /> **[Mailgun](https://github.com/mailgun/mailgun-mcp-server)** - Interact with Mailgun API.\n- <img height=\"12\" width=\"12\" src=\"https://www.mailjet.com/favicon.ico\" alt=\"Mailjet Logo\" /> **[Mailjet](https://github.com/mailgun/mailjet-mcp-server)** - Official MCP server which allows AI agents to interact with contact, campaign, segmentation, statistics, workflow (and more) APIs from [Sinch Mailjet](https://www.mailjet.com).\n- <img height=\"12\" width=\"12\" src=\"https://www.make.com/favicon.ico\" alt=\"Make Logo\" /> **[Make](https://github.com/integromat/make-mcp-server)** - Turn your [Make](https://www.make.com/) scenarios into callable tools for AI assistants.\n- <img height=\"12\" width=\"12\" src=\"https://static-assets.mapbox.com/branding/favicon/v1/favicon.ico\" alt=\"Mapbox Logo\" /> **[Mapbox](https://github.com/mapbox/mcp-server)** - Unlock geospatial intelligence through Mapbox APIs like geocoding, POI search, directions, isochrones and more.\n- <img height=\"12\" width=\"12\" src=\"https://www.mariadb.com/favicon.ico\" alt=\"MariaDB Logo\" /> **[MariaDB](https://github.com/mariadb/mcp)** - A standard interface for managing and querying MariaDB databases, supporting both standard SQL operations and advanced vector/embedding-based search.\n- <img height=\"14\" width=\"14\" src=\"https://raw.githubusercontent.com/rust-mcp-stack/mcp-discovery/refs/heads/main/docs/_media/mcp-discovery-logo.png\" alt=\"mcp-discovery logo\" /> **[MCP Discovery](https://github.com/rust-mcp-stack/mcp-discovery)** - A lightweight CLI tool built in Rust for discovering MCP server capabilities.\n- <img height=\"12\" width=\"12\" src=\"https://woocommerce.com/favicon.ico\" alt=\"WooCommerce Logo\" /> **[MCP for WooCommerce](https://github.com/iOSDevSK/mcp-for-woocommerce)** - Connect your WooCommerce store to AI assistants with read-only access to products, categories, reviews, and WordPress content. [WordPress plugin](https://wordpress.org/plugins/mcp-for-woocommerce/)\n- <img height=\"12\" width=\"12\" src=\"https://googleapis.github.io/genai-toolbox/favicons/favicon.ico\" alt=\"MCP Toolbox for Databases Logo\" /> **[MCP Toolbox for Databases](https://github.com/googleapis/genai-toolbox)** - Open source MCP server specializing in easy, fast, and secure tools for Databases. Supports  AlloyDB, BigQuery, Bigtable, Cloud SQL, Dgraph, Looker, MySQL, Neo4j, Postgres, Spanner, and more.\n- <img height=\"12\" width=\"12\" src=\"https://www.meilisearch.com/favicon.ico\" alt=\"Meilisearch Logo\" /> **[Meilisearch](https://github.com/meilisearch/meilisearch-mcp)** - Interact & query with Meilisearch (Full-text & semantic search API)\n- <img height=\"12\" width=\"12\" src=\"https://github.com/nfergu/memalot/blob/main/logo.png?raw=true\" alt=\"Memalot Logo\" /> **[Memalot](https://github.com/nfergu/memalot?tab=readme-ov-file#mcp-server)** - Finds memory leaks in Python programs.\n- <img height=\"12\" width=\"12\" src=\"https://memgraph.com/favicon.png\" alt=\"Memgraph Logo\" /> **[Memgraph](https://github.com/memgraph/ai-toolkit/tree/main/integrations/mcp-memgraph)** - Query your data in [Memgraph](https://memgraph.com/) graph database.\n- <img height=\"12\" width=\"12\" src=\"https://www.mercadolibre.com.ar/favicon.ico\" alt=\"MercadoLibre Logo\" /> **[Mercado Libre](https://mcp.mercadolibre.com/)** - Mercado Libre's official MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://www.mercadopago.com/favicon.ico\" alt=\"MercadoPago Logo\" /> **[Mercado Pago](https://mcp.mercadopago.com/)** - Mercado Pago's official MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://metoro.io/static/images/logos/MetoroLogo.png\" alt=\"Metoro Logo\" /> **[Metoro](https://github.com/metoro-io/metoro-mcp-server)** - Query and interact with kubernetes environments monitored by Metoro\n- <img height=\"12\" width=\"12\" src=\"https://knowall.ai/favicon.ico\" alt=\"Microsoft Business Central Logo\" /> **[Microsoft Business Central](https://github.com/knowall-ai/mcp-business-central)** - Manage Dynamics 365 Business Central customers, contacts, sales opportunities, invoices, and vendors\n- <img height=\"12\" width=\"12\" src=\"https://claritystatic.azureedge.net/images/logo.ico\" alt=\"Microsoft Clarity Logo\"/> **[Microsoft Clarity](https://github.com/microsoft/clarity-mcp-server)** - Official MCP Server to get your behavioral analytics data and insights from [Clarity](https://clarity.microsoft.com)\n- <img height=\"12\" width=\"12\" src=\"https://conn-afd-prod-endpoint-bmc9bqahasf3grgk.b01.azurefd.net/releases/v1.0.1735/1.0.1735.4099/commondataserviceforapps/icon.png\" alt=\"Microsoft Dataverse Logo\" /> **[Microsoft Dataverse](https://go.microsoft.com/fwlink/?linkid=2320176)** - Chat over your business data using NL - Discover tables, run queries, retrieve data, insert or update records, and execute custom prompts grounded in business knowledge and context.\n- <img height=\"12\" width=\"12\" src=\"https://learn.microsoft.com/favicon.ico\" alt=\"Microsoft Learn Logo\" /> **[Microsoft Learn Docs](https://github.com/microsoftdocs/mcp)** - An MCP server that provides structured access to Microsoft's official documentation. Retrieves accurate, authoritative, and context-aware technical content for code generation, question answering, and workflow grounding.\n- <img height=\"12\" width=\"12\" src=\"https://statics.teams.microsoft.com/hashedassets/favicon/prod/favicon-9f45b466.ico\" alt=\"Microsoft Teams Logo\" /> **[Microsoft Teams](https://devblogs.microsoft.com/microsoft365dev/announcing-the-updated-teams-ai-library-and-mcp-support/)** - Official Microsoft Teams AI Library with MCP support enabling advanced agent orchestration, multi-agent collaboration, and seamless integration with Teams messaging and collaboration features.\n- <img height=\"12\" width=\"12\" src=\"https://milvus.io/favicon-32x32.png\" /> **[Milvus](https://github.com/zilliztech/mcp-server-milvus)** - Search, Query and interact with data in your Milvus Vector Database.\n- <img src=\"https://www.mimilabs.ai/logos/mimilabsSquare.svg\" alt=\"mimilabs\" width=\"12\" height=\"12\"> **[mimilabs](https://www.mimilabs.ai/mcp)** - A US healthcare data discovery guide for 50+ gov sources and thousands of publicly available US healthcare datasets regarding gov-funded programs, policies, drug pricings, clinical trials, etc.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.mxpnl.com/marketing-site/static/favicons/favicon-32x32.png\" alt=\"Mixpanel Logo\" /> **[Mixpanel](https://docs.mixpanel.com/docs/features/mcp)** - Query and analyze your product analytics data through natural language. This Mixpanel MCP connects AI assistants to your Mixpanel workspace, enabling conversational access to user behavior insights, funnels, retention analysis, and custom reports.\n- <img src=\"https://avatars.githubusercontent.com/u/94089762?s=48&v=4\" alt=\"Mobb\" width=\"12\" height=\"12\"> **[Mobb](https://github.com/mobb-dev/bugsy?tab=readme-ov-file#model-context-protocol-mcp-server)** - The [Mobb Vibe Shield](https://vibe.mobb.ai/) MCP server identifies and remediates vulnerabilities in both human and AI-written code, ensuring your applications remain secure without slowing development.\n- <img height=\"12\" width=\"12\" src=\"https://console.gomomento.com/favicon.ico\" /> **[Momento](https://github.com/momentohq/mcp-momento)** - Momento Cache lets you quickly improve your performance, reduce costs, and handle load at any scale.\n- <img height=\"12\" width=\"12\" src=\"https://www.monday.com/favicon.ico\" alt=\"Monday.com Logo\" /> **[Monday.com](https://github.com/mondaycom/mcp)** - Interact with Monday.com boards, items, accounts and work forms.\n- <img height=\"12\" width=\"12\" src=\"https://www.mongodb.com/favicon.ico\" /> **[MongoDB](https://github.com/mongodb-js/mongodb-mcp-server)** - Both MongoDB Community Server and MongoDB Atlas are supported.\n- <img height=\"12\" width=\"12\" src=\"https://moorcheh.ai/Moorcheh-mcp.ico\" alt=\"Moorcheh Logo\" /> **[Moorcheh](https://github.com/moorcheh-ai/moorcheh-mcp)** - Provides seamless integration with Moorcheh's Embedding, Vector Store, Search, and Gen AI Answer services.\n- <img height=\"12\" width=\"12\" src=\"https://www.motherduck.com/favicon.ico\" alt=\"MotherDuck Logo\" /> **[MotherDuck](https://github.com/motherduckdb/mcp-server-motherduck)** - Query and analyze data with MotherDuck and local DuckDB\n- <img height=\"12\" width=\"12\" src=\"https://docs.mulesoft.com/_/img/favicon.ico\" alt=\"Mulesoft Logo\" /> **[Mulesoft](https://www.npmjs.com/package/@mulesoft/mcp-server)** - Build, deploy, and manage MuleSoft applications with natural language, directly inside any compatible IDE.\n- <img height=\"12\" width=\"12\" src=\"https://www.multiplayer.app/favicon-32x32.png\" alt=\"Multiplayer Logo\" /> **[Multiplayer](https://www.multiplayer.app/docs/ai/mcp-server)** - Analyze your full stack session recordings easily. Record a bug with Multiplayer, analyze and fix it with LLM\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/NangoHQ/nango/refs/heads/master/docs/images/logo/logo-light-mode.svg\" alt=\"Nango Logo\" /> **[Nango](https://nango.dev/docs/guides/use-cases/ai-tool-calling)** - Integrate your AI agent with 500+ APIs: Auth, custom tools, and observability. Open-source.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/38020270\" alt=\"NanoVMs Logo\" /> **[NanoVMs](https://github.com/nanovms/ops-mcp)** - Easily Build and Deploy unikernels to any cloud.\n- <img height=\"12\" width=\"12\" src=\"https://needle-ai.com/images/needle-logo-orange-2-rounded.png\" alt=\"Needle AI Logo\" /> **[Needle](https://github.com/needle-ai/needle-mcp)** - Production-ready RAG out of the box to search and retrieve data from your own documents.\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j](https://github.com/neo4j-contrib/mcp-neo4j/)** - Neo4j graph database server (schema + read/write-cypher) and separate graph database backed memory\n- <img height=\"12\" width=\"12\" src=\"https://knowall.ai/favicon.ico\" alt=\"Neo4j Agent Memory Logo\" /> **[Neo4j Agent Memory](https://github.com/knowall-ai/mcp-neo4j-agent-memory)** - Memory management for AI agents using Neo4j knowledge graphs\n- <img height=\"12\" width=\"12\" src=\"https://neo4j.com/favicon.ico\" alt=\"Neo4j Logo\" /> **[Neo4j GDS](https://github.com/neo4j-contrib/gds-agent)** - Neo4j graph data science server with comprehensive graph algorithms that enables complex graph reasoning and Q&A.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/183852044?s=48&v=4\" alt=\"Neon Logo\" /> **[Neon](https://github.com/neondatabase/mcp-server-neon)** - Interact with the Neon serverless Postgres platform\n- <img height=\"12\" width=\"12\" src=\"https://app.usenerve.com/favicon.ico\" alt=\"Nerve Logo\" /> **[Nerve](https://github.com/nerve-hq/nerve-mcp-server)** - Search and Act on all your company data across all your SaaS apps via [Nerve](https://www.usenerve.com/)\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/370544\" alt=\"NetApp Logo\" /> **[NetApp](https://github.com/NetApp/mcp)** - Query metrics, manage volumes, and search across your NetApp systems and services.\n- <img height=\"12\" width=\"12\" src=\"https://www.netdata.cloud/favicon-32x32.png\" alt=\"Netdata Logo\" /> **[Netdata](https://github.com/netdata/netdata/blob/master/src/web/mcp/README.md)** - Discovery, exploration, reporting and root cause analysis using all observability data, including metrics, logs, systems, containers, processes, and network connections\n- <img height=\"12\" width=\"12\" src=\"https://www.netlify.com/favicon/icon.svg\" alt=\"Netlify Logo\" /> **[Netlify](https://docs.netlify.com/welcome/build-with-ai/netlify-mcp-server/)** - Create, build, deploy, and manage your websites with Netlify web platform.\n- <img height=\"12\" width=\"12\" src=\"https://www.thenile.dev/favicon.ico\" alt=\"Nile Logo\" /> **[Nile](https://github.com/niledatabase/nile-mcp-server)** - An MCP server that talks to Nile - Postgres re-engineered for B2B apps. Manage and query databases, tenants, users, auth using LLMs\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/208441832?s=400&v=4\" alt=\"Nodit Logo\" /> **[Nodit](https://github.com/noditlabs/nodit-mcp-server)** - Official Nodit MCP Server enabling access to multi-chain RPC Nodes and Data APIs for blockchain data.\n- <img height=\"12\" width=\"12\" src=\"https://app.norman.finance/favicons/favicon-32x32.png\" alt=\"Norman Logo\" /> **[Norman Finance](https://github.com/norman-finance/norman-mcp-server)** - MCP server for managing accounting and taxes with Norman Finance.\n- <img height=\"12\" width=\"12\" src=\"https://notifly.tech/favicon.ico\" alt=\"Notifly Logo\" /> **[Notifly](https://github.com/notifly-tech/notifly-mcp-server)** - Notifly MCP Server that enables AI agents to provide real-time, trusted Notifly documentation and SDK code examples for seamless integrations.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/4792552?s=200&v=4\" alt=\"Notion Logo\" /> **[Notion](https://github.com/makenotion/notion-mcp-server#readme)** - This project implements an MCP server for the Notion API.\n- <img height=\"12\" width=\"12\" src=\"https://www.nutrient.io/assets/images/logos/nutrient.svg\" alt=\"Nutrient Logo\" /> **[Nutrient](https://github.com/PSPDFKit/nutrient-dws-mcp-server)** - Create, Edit, Sign, Extract Documents using Natural Language\n- <img height=\"12\" width=\"12\" src=\"https://nx.dev/favicon/favicon.svg\" alt=\"Nx Logo\" /> **[Nx](https://github.com/nrwl/nx-console/blob/master/apps/nx-mcp)** - Makes [Nx's understanding](https://nx.dev/features/enhance-AI) of your codebase accessible to LLMs, providing insights into the codebase architecture, project relationships and runnable tasks thus allowing AI to make precise code suggestions.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/82347605?s=48&v=4\" alt=\"OceanBase Logo\" /> **[OceanBase](https://github.com/oceanbase/mcp-oceanbase)** - MCP Server for OceanBase database and its tools\n- <img height=\"12\" width=\"12\" src=\"https://docs.octagonagents.com/logo.svg\" alt=\"Octagon Logo\" /> **[Octagon](https://github.com/OctagonAI/octagon-mcp-server)** - Deliver real-time investment research with extensive private and public market data.\n- <img height=\"12\" width=\"12\" src=\"https://octoeverywhere.com/img/logo.png\" alt=\"OctoEverywhere Logo\" /> **[OctoEverywhere](https://github.com/OctoEverywhere/mcp)** - A 3D Printing MCP server that allows for querying for live state, webcam snapshots, and 3D printer control.\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/OctopusDeploy/mcp-server/refs/heads/main/images/logo.svg\" alt=\"Octopus Deploy\" /> **[Octopus Deploy](https://github.com/OctopusDeploy/mcp-server)** - Official MCP server for querying, inspecting, and managing your [Octopus Deploy](https://octopus.com/) instance.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/211697972\" alt=\"Offorte Logo\" /> **[Offorte](https://github.com/offorte/offorte-mcp-server#readme)** - Offorte Proposal Software official MCP server enables creation and sending of business proposals.\n- <img height=\"12\" width=\"12\" src=\"https://maps.olakrutrim.com/favicon.ico\" alt=\"Ola Maps\" /> **[OlaMaps](https://pypi.org/project/ola-maps-mcp-server)** - Official Ola Maps MCP Server for services like geocode, directions, place details and many more.\n- <img height=\"12\" width=\"12\" src=\"https://www.olostep.com/favicon.ico\" alt=\"Olostep\" /> **[Olostep](https://github.com/olostep/olostep-mcp-server)** - Search, scrape and crawl content from web. Real-time results in clean markdown.\n- **[OMOP MCP](https://github.com/OHNLP/omop_mcp)** - Map clinical terminology to OMOP concepts using LLMs for healthcare data standardization.\n- <img height=\"12\" width=\"12\" src=\"https://static.onlyoffice.com/images/favicon.ico\" alt=\"ONLYOFFICE DocSpace\" /> **[ONLYOFFICE DocSpace](https://github.com/ONLYOFFICE/docspace-mcp)** - Interact with [ONLYOFFICE DocSpace](https://www.onlyoffice.com/docspace.aspx) API to create rooms, manage files and folders.\n- <img height=\"12\" width=\"12\" src=\"https://op.gg/favicon.ico\" alt=\"OP.GG Logo\" /> **[OP.GG](https://github.com/opgginc/opgg-mcp)** - Access real-time gaming data across popular titles like League of Legends, TFT, and Valorant, offering champion analytics, esports schedules, meta compositions, and character statistics.\n- <img height=\"12\" width=\"12\" src=\"https://open-metadata.org/favicon.ico\" alt=\"OpenMetadata\" /> **[OpenMetadata](https://open-metadata.org/mcp)** - The first Enterprise-grade MCP server for metadata\n- <img height=\"12\" width=\"12\" src=\"https://opensearch.org/wp-content/uploads/2025/01/opensearch_mark_default.svg\" alt=\"OpenSearch Logo\" /> **[OpenSearch](https://github.com/opensearch-project/opensearch-mcp-server-py)** -  MCP server that enables AI agents to perform search and analytics use cases on data stored in [OpenSearch](https://opensearch.org/).\n- <img height=\"12\" width=\"12\" src=\"https://app.opslevel.com/favicon.ico\" alt=\"OpsLevel\" /> **[OpsLevel](https://github.com/opslevel/opslevel-mcp)** - Official MCP Server for [OpsLevel](https://www.opslevel.com).\n- <img height=\"12\" width=\"12\" src=\"https://optuna.org/assets/img/favicon.ico\" alt=\"Optuna Logo\" /> **[Optuna](https://github.com/optuna/optuna-mcp)** - Official MCP server enabling seamless orchestration of hyperparameter search and other optimization tasks with [Optuna](https://optuna.org/).\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/oracle/mcp/refs/heads/main/oracle.svg\" alt=\"Oracle Logo\" /> **[Oracle](https://docs.oracle.com/en/database/oracle/sql-developer-command-line/25.2/sqcug/starting-and-managing-sqlcl-mcp-server.html#GUID-5F916B5D-8670-42BD-9F8B-D3D2424EC47E)** - Official [Oracle Database: SQLcl ](https://www.oracle.com/database/sqldeveloper/technologies/sqlcl/download/) MCP server enabling all access to any Oracle Database via native MCP support directly in SQLcl.\n- <img height=\"12\" width=\"12\" src=\"https://orshot.com/brand/favicon.svg\" alt=\"Orshot Logo\" /> **[Orshot](https://github.com/rishimohan/orshot-mcp-server)** - Official [Orshot](https://orshot.com) MCP server to dynamically generate images from custom design templates.\n- <img height=\"12\" width=\"12\" src=\"https://oxylabs.io/favicon.ico\" alt=\"Oxylabs Logo\" /> **[Oxylabs](https://github.com/oxylabs/oxylabs-mcp)** - Scrape websites with Oxylabs Web API, supporting dynamic rendering and parsing for structured data extraction.\n- <img height=\"12\" width=\"12\" src=\"https://developer.paddle.com/favicon.svg\" alt=\"Paddle Logo\" /> **[Paddle](https://github.com/PaddleHQ/paddle-mcp-server)** - Interact with the Paddle API. Manage product catalog, billing and subscriptions, and reports.\n- **[PaddleOCR](https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/deployment/mcp_server.html)** - An MCP server that brings enterprise-grade OCR and document parsing capabilities to AI applications.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.brandfolder.io/YX9ETPCP/at/266537g8kh6mmvt24jvsjb/P-GreenRGB.svg\" alt=\"PagerDuty Logo\" /> **[PagerDuty](https://github.com/PagerDuty/pagerduty-mcp-server)** - Interact with your PagerDuty account, allowing you to manage incidents, services, schedules, and more directly from your MCP-enabled client.\n- **[Pagos](https://github.com/pagos-ai/pagos-mcp)** - Interact with the Pagos API. Query Credit Card BIN Data with more to come.\n- <img height=\"12\" width=\"12\" src=\"https://paiml.com/favicon.ico\" alt=\"PAIML Logo\" /> **[PAIML MCP Agent Toolkit](https://github.com/paiml/paiml-mcp-agent-toolkit)** - Professional project scaffolding toolkit with zero-configuration AI context generation, template generation for Rust/Deno/Python projects, and hybrid neuro-symbolic code analysis.\n- <img src=\"https://cdn.bfldr.com/7GK1OJLK/at/kq7cwt4vkw5m2x9s4gkvbf7g/android-chrome-512x512-favicon.png?auto=webp&format=png&width=12&height=12\" width=\"12\" height=\"12\" alt=\"PandaDoc\"> **[PandaDoc](https://developers.pandadoc.com/docs/use-pandadoc-mcp-server)** - Configure AI development tools to connect to PandaDoc's Model Context Protocol server and leverage AI-powered PandaDoc integrations.\n- <img height=\"12\" width=\"12\" src=\"https://app.paperinvest.io/favicon.svg\" alt=\"Paper Logo\" /> **[Paper](https://github.com/paperinvest/mcp-server)** - Realistic paper trading platform with market simulation, 22 broker emulations, and professional tools for risk-free trading practice. First trading platform with MCP integration.\n- <img height=\"12\" width=\"12\" src=\"https://parallel.ai/favicon.ico\" alt=\"Parallel Logo\" /> **[Parallel Task MCP](https://github.com/parallel-web/task-mcp)** - Initiate Deep Research and Batch Tasks\n- **[Patronus AI](https://github.com/patronus-ai/patronus-mcp-server)** - Test, evaluate, and optimize AI agents and RAG apps\n- <img height=\"12\" width=\"12\" src=\"https://mcp.paubox.com/paubox.png\" alt=\"Paubox Logo\" />**[Paubox](https://mcp.paubox.com)** - Official MCP server which allows AI agents to interact with Paubox Email API. HITRUST certified.\n- <img height=\"12\" width=\"12\" src=\"https://www.paypalobjects.com/webstatic/icon/favicon.ico\" alt=\"PayPal Logo\" /> **[PayPal](https://mcp.paypal.com)** - PayPal's official MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://www.foxit.com/favicon.ico\" alt=\"Foxit Logo\" /> **[PDFActionInspector](https://github.com/foxitsoftware/PDFActionInspector/tree/develop)** - A Model Context Protocol server for extracting and analyzing JavaScript Actions from PDF files. Provides comprehensive security analysis to detect malicious PDF behaviors, hidden scripts, and potential security threats through AI-assisted risk assessment.\n- <img height=\"12\" width=\"12\" src=\"https://ww2-secure.pearl.com/static/pearl/pearl-logo.svg\" alt=\"Pearl Logo\" /> **[Pearl](https://github.com/Pearl-com/pearl_mcp_server)** - Official MCP Server to interact with Pearl API. Connect your AI Agents with 12,000+ certified experts instantly.\n- <img height=\"12\" width=\"12\" src=\"https://www.perplexity.ai/favicon.ico\" alt=\"Perplexity Logo\" /> **[Perplexity](https://github.com/ppl-ai/modelcontextprotocol)** - An MCP server that connects to Perplexity's Sonar API, enabling real-time web-wide research in conversational AI.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/mattjoyce.png\" alt=\"Persona Sessions Logo\" /> **[Persona Sessions](https://github.com/mattjoyce/mcp-persona-sessions)** - Enable AI assistants to conduct structured, persona-driven sessions including interview preparation, personal reflection, and coaching conversations with built-in timer and evaluation.\n- <img height=\"12\" width=\"12\" src=\"https://www.pga.com/favicon.ico\" alt=\"PGA Logo\" /> **[PGA (Golf)](https://mcp.pga.com)** - PGA's official MCP Server for all things golf-related. Find a coach, play golf, improve your game, and more.\n- <img height=\"12\" width=\"12\" src=\"https://www.pgyer.com/favicon.ico\" alt=\"PGYER Logo\" /> **[PGYER](https://github.com/PGYER/pgyer-mcp-server)** - MCP Server for [PGYER](https://www.pgyer.com/) platform, supports uploading, querying apps, etc.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/54333248\" /> **[Pinecone](https://github.com/pinecone-io/pinecone-mcp)** - [Pinecone](https://docs.pinecone.io/guides/operations/mcp-server)'s developer MCP Server assist developers in searching documentation and managing data within their development environment.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/54333248\" /> **[Pinecone Assistant](https://github.com/pinecone-io/assistant-mcp)** - Retrieves context from your [Pinecone Assistant](https://docs.pinecone.io/guides/assistant/mcp-server) knowledge base.\n- <img height=\"12\" width=\"12\" src=\"https://www.pinmeto.com/hubfs/PinMeTo-Favicon.png\" alt=\"PinMeTo logo\" /> **[PinMeTo](https://github.com/PinMeTo/pinmeto-location-mcp)** - MCP server that enables users with authorized credentials to unlock their location data.\n- <img height=\"12\" width=\"12\" src=\"https://pipedream.com/favicon.ico\" alt=\"Pipedream Logo\" /> **[Pipedream](https://github.com/PipedreamHQ/pipedream/tree/master/modelcontextprotocol)** - Connect with 2,500 APIs with 8,000+ prebuilt tools.\n- <img height=\"12\" width=\"12\" src=\"https://storage.googleapis.com/plainly-static-data/plainly%20-%20logo.png\" alt=\"PlainlyVideos Logo\" /> **[Plainly Videos](https://github.com/plainly-videos/mcp-server)** - The official MCP server for [Plainly Videos](https://plainlyvideos.com), allowing users to browse designs and projects, as well as render videos using various LLM clients.\n- <img height=\"12\" width=\"12\" src=\"https://playcanvas.com/static-assets/images/icons/favicon.png\" alt=\"PlayCanvas Logo\" /> **[PlayCanvas](https://github.com/playcanvas/editor-mcp-server)** - Create interactive 3D web apps with the PlayCanvas Editor.\n- <img height=\"12\" width=\"12\" src=\"https://playwright.dev/img/playwright-logo.ico\" alt=\"Playwright Logo\" /> **[Playwright](https://github.com/microsoft/playwright-mcp)** â€” Browser automation MCP server using Playwright to run tests, navigate pages, capture screenshots, scrape content, and automate web interactions reliably.\n- <img height=\"12\" width=\"12\" src=\"https://www.plugged.in/favicon.ico\" alt=\"Plugged.in Logo\" /> **[Plugged.in](https://github.com/VeriTeknik/pluggedin-mcp)** - A comprehensive proxy that combines multiple MCP servers into a single MCP. It provides discovery and management of tools, prompts, resources, and templates across servers, plus a playground for debugging when building MCP servers.\n- <img height=\"12\" width=\"12\" src=\"https://p-link.io/favicon.ico\" alt=\"P-Link.io Logo\" /> **[P-Link.io](https://github.com/paracetamol951/P-Link-MCP)** - HTTP 402 Protocol implementation on Solana network. Sending & receiving payments for agents\n- <img height=\"12\" width=\"12\" src=\"https://polymarket.com/favicon.ico\" alt=\"Polymarket Logo\" /> **[Polymarket](https://github.com/ozgureyilmaz/polymarket-mcp)** - Real-time prediction market data from Polymarket - search markets, analyze prices, identify trading opportunities.\n- <img height=\"12\" width=\"12\" src=\"https://plusai.com/622ffb3448f15ce7a33c6a2b/652d81ccc31a7d50861db0ef_plus_favicon.ico\" alt=\"Plus AI Logo\" /> **[Plus AI](https://plusai.com/features/mcp)** - A Model Context Protocol (MCP) server for automatically generating professional PowerPoint and Google Slides presentations using the [Plus AI](https://plusai.com/) presentation API.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/port-labs/port-mcp-server/blob/main/assets/port_symbol_white.svg\" alt=\"Port Logo\" /> **[Port IO](https://github.com/port-labs/port-mcp-server)** - Access and manage your software catalog to improve service quality and compliance.\n- **[PostHog](https://github.com/posthog/mcp)** - Interact with PostHog analytics, feature flags, error tracking and more with the official PostHog MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://postidentity.com/favicon.ico\" alt=\"PostIdentity Logo\" /> **[PostIdentity](https://github.com/PostIdentity/mcp-server)** - Generate AI-powered social media posts from any AI assistant. Manage identities, create posts, track referrals, and browse marketplace templates, powered by [PostIdentity](https://postidentity.com).\n- **[Postman API](https://github.com/postmanlabs/postman-api-mcp)** - Manage your Postman resources using the [Postman API](https://www.postman.com/postman/postman-public-workspace/collection/i2uqzpp/postman-api).\n- <img height=\"12\" width=\"12\" src=\"https://powerdrill.ai/_next/static/media/powerdrill.0fa27d00.webp\" alt=\"Powerdrill Logo\" /> **[Powerdrill](https://github.com/powerdrillai/powerdrill-mcp)** - An MCP server that provides tools to interact with Powerdrill datasets, enabling smart AI data analysis and insights.\n- <img height=\"12\" width=\"12\" src=\"https://www.pre.dev/predevlogowhitebackground.png\" alt=\"pre.dev Logo\" /> **[pre.dev Architect](https://docs.pre.dev/mcp-server)** - 10x your coding agent by keeping it on track with pre.dev.\n- <img height=\"12\" width=\"12\" src=\"https://devdocs.prestashop-project.org/images/favicon.png\" alt=\"PrestaShop Logo\" /> **[PrestaShop.com](https://docs.mcp.prestashop.com/)** - Manage your PrestaShop store with AI Assistant by using the official PrestaShop MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://www.prisma.io/images/favicon-32x32.png\" alt=\"Prisma Logo\" /> **[Prisma](https://www.prisma.io/docs/postgres/integrations/mcp-server)** - Create and manage Prisma Postgres databases\n- <img height=\"12\" width=\"12\" src=\"https://probe.dev/favicon.ico\" alt=\"Probe.dev Logo\" /> **[Probe.dev](https://docs.probe.dev/guides/mcp-integration)** - Comprehensive media analysis and validation powered by [Probe.dev](https://probe.dev). Hosted MCP server with FFprobe, MediaInfo, and Probe Report analysis capabilities.\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/FGzpihs4MxmSJhyGZ6n7f2Xj0.png\" alt=\"Prode.ai Logo\" /> **[ProdE](https://github.com/CuriousBox-AI/ProdE-mcp)** - Your 24/7 production engineer that preserves context across multiple codebases.\n- <img height=\"12\" width=\"12\" src=\"https://programintegrity.org/wp-content/uploads/2024/07/PIA-Favicon.svg\" alt=\"Program Integrity Alliance (PIA) Logo\" /> **[Program Integrity Alliance (PIA)](https://github.com/Program-Integrity-Alliance/pia-mcp-local)** - Local and Hosted MCP servers providing AI-friendly access to U.S. Government Open Datasets. Also available on [Docker MCP Catalog](https://hub.docker.com/mcp/explore?search=PIA). See [our website](https://programintegrity.org) for more details.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/newtype-01/prompthouse-mcp/raw/main/prompthouse-logo-12x12.png\" alt=\"PromptHouse Logo\" /> **[PromptHouse](https://github.com/newtype-01/prompthouse-mcp)** - Personal prompt library with MCP integration for AI clients.\n- <img height=\"12\" width=\"12\" src=\"https://docs.speedscale.com/img/favicon.ico\" alt=\"proxymock Logo\" /> **[proxymock](https://docs.speedscale.com/proxymock/reference/mcp/)** - An MCP server that automatically generates tests and mocks by recording a live app.\n- <img src=\"https://www.pubnub.com/favicon/favicon-32x32.png\" alt=\"PubNub\" width=\"12\" height=\"12\"> **[PubNub](https://github.com/pubnub/pubnub-mcp-server)** - Retrieves context for developing with PubNub SDKs and calling APIs.\n- <img height=\"12\" width=\"12\" src=\"https://www.pulumi.com/images/favicon.ico\" alt=\"Pulumi Logo\" /> **[Pulumi](https://github.com/pulumi/mcp-server)** - Deploy and manage cloud infrastructure using [Pulumi](https://pulumi.com).\n- <img height=\"12\" width=\"12\" src=\"https://pure.md/favicon.png\" alt=\"Pure.md Logo\" /> **[Pure.md](https://github.com/puremd/puremd-mcp)** - Reliably access web content in markdown format with [pure.md](https://pure.md) (bot detection avoidance, proxy rotation, and headless JS rendering built in).\n- <img height=\"12\" width=\"12\" src=\"https://put.io/images/favicon.ico\" alt=\"Put.io Logo\" /> **[Put.io](https://github.com/putdotio/putio-mcp-server)** - Interact with your Put.io account to download torrents.\n- <img height=\"12\" width=\"12\" src=\"https://qdrant.tech/img/brand-resources-logos/logomark.svg\" /> **[Qdrant](https://github.com/qdrant/mcp-server-qdrant/)** - Implement semantic memory layer on top of the Qdrant vector search engine\n- <img src=\"https://avatars.githubusercontent.com/u/18053493?s=200&v=4\" alt=\"Qonto\" width=\"12\" height=\"12\"> **[Qonto](https://github.com/qonto/qonto-mcp-server)** - Access and interact your Qonto account through LLMs using MCP.\n- <img src=\"https://api.qoretechnologies.com/api/public/apps/Qorus/qorus-logo.svg\" alt=\"Qorus\" width=\"12\" height=\"12\"> **[Qorus](https://qoretechnologies.com/manual/qorus/current/qorus/sysarch.html#mcp_server)** - Connect to any application, system, or technology and automate your business processes without coding and with AI\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/3912814\" alt=\"QuantConnect Logo\" /> **[QuantConnect](https://github.com/QuantConnect/mcp-server)** - Interact with your [QuantConnect](https://www.quantconnect.com/) account to update projects, write strategies, run backtest, and deploying strategies to production live-trading.\n- **[Quickchat AI](https://github.com/incentivai/quickchat-ai-mcp)** - Launch your conversational [Quickchat AI](https://quickchat.ai) agent as an MCP to give AI apps real-time access to its Knowledge Base and conversational capabilities\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/165178062\" alt=\"Ragie Logo\" /> **[Ragie](https://github.com/ragieai/ragie-mcp-server/)** - Retrieve context from your [Ragie](https://www.ragie.ai) (RAG) knowledge base connected to integrations like Google Drive, Notion, JIRA and more.\n- <img height=\"12\" width=\"12\" src=\"https://www.ramp.com/favicon.ico\" /> **[Ramp](https://github.com/ramp-public/ramp-mcp)** - Interact with [Ramp](https://ramp.com)'s Developer API to run analysis on your spend and gain insights leveraging LLMs\n- **[Raygun](https://github.com/MindscapeHQ/mcp-server-raygun)** - Interact with your crash reporting and real using monitoring data on your Raygun account\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/CU1m0xFonUl76ZeaW0IdkQ0M.png\" alt=\"Razorpay Logo\" /> **[Razorpay](https://github.com/razorpay/razorpay-mcp-server)** - Razorpay's official MCP server\n- <img height=\"12\" width=\"12\" src=\"https://www.recraft.ai/favicons/icon.svg\" alt=\"Recraft Logo\" /> **[Recraft](https://github.com/recraft-ai/mcp-recraft-server)** - Generate raster and vector (SVG) images using [Recraft](https://recraft.ai). Also you can edit, upscale images, create your own styles, and vectorize raster images\n- <img height=\"12\" width=\"12\" src=\"https://www.redhat.com/favicon.ico\" alt=\"Red Hat Logo\" /> **[Red Hat Insights](https://github.com/RedHatInsights/insights-mcp)** - Interact with [Red Hat Insights](https://www.redhat.com/en/technologies/management/insights) - build images, manage vulnerabilities, or view targeted recommendations.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/1529926\" alt=\"Redis Logo\" /> **[Redis](https://github.com/redis/mcp-redis/)** - The Redis official MCP Server offers an interface to manage and search data in Redis.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/1529926\" alt=\"Redis Logo\" /> **[Redis Cloud API](https://github.com/redis/mcp-redis-cloud/)** - The Redis Cloud API MCP Server allows you to manage your Redis Cloud resources using natural language.\n- <img src=\"https://avatars.githubusercontent.com/u/149024635\" alt=\"Reexpress\" width=\"12\" height=\"12\"> **[Reexpress](https://github.com/ReexpressAI/reexpress_mcp_server)** - Enable Similarity-Distance-Magnitude statistical verification for your search, software, and data science workflows\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/68a872edf3df6064de547670/68b7f089c45a6083ce25acb1_reflag-favicon-32.png\" alt=\"Reflag\" /> **[Reflag](https://github.com/reflagcom/javascript/tree/main/packages/cli#model-context-protocol)** - Create and manage feature flags using [Reflag](https://reflag.com)\n- <img height=\"12\" width=\"12\" src=\"https://www.reltio.com/wp-content/uploads/2024/03/cropped-cropped-Reltio_Light_Mode_Dark_Mode_Favicon-270x270.png\" alt=\"Reltio Logo\" /> **[Reltio](https://github.com/reltio-ai/reltio-mcp-server)** - A lightweight, plugin-based MCP server designed to perform advanced entity matching with language models in Reltio environments.\n- <img height=\"12\" width=\"12\" src=\"https://www.rember.com/favicon.ico\" alt=\"Rember Logo\" /> **[Rember](https://github.com/rember/rember-mcp)** - Create spaced repetition flashcards in [Rember](https://rember.com) to remember anything you learn in your chats\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/114033652\" alt=\"Render Logo\" /> **[Render](https://render.com/docs/mcp-server)** - The official Render MCP server: spin up new services, run queries against your databases, and debug rapidly with direct access to service metrics and logs.\n- <img height=\"12\" width=\"12\" src=\"https://reportportal.io/favicon.ico\" alt=\"ReportPortal Logo\" /> **[ReportPortal](https://github.com/reportportal/reportportal-mcp-server)** - explore and analyze automated test results from [ReportPortal](https://reportportal.io) using your favourite LLM.\n- <img height=\"12\" width=\"12\" src=\"http://nonica.io/Nonica-logo.ico\" alt=\"Nonica Logo\" /> **[Revit](https://github.com/NonicaTeam/AI-Connector-for-Revit)** - Connect and interact with your Revit models live.\n- <img height=\"12\" width=\"12\" src=\"https://ui.rilldata.com/favicon.png\" alt=\"Rill Data Logo\" /> **[Rill Data](https://docs.rilldata.com/explore/mcp)** - Interact with Rill Data to query and analyze your data.\n- <img height=\"12\" width=\"12\" src=\"https://riza.io/favicon.ico\" alt=\"Riza logo\" /> **[Riza](https://github.com/riza-io/riza-mcp)** - Arbitrary code execution and tool-use platform for LLMs by [Riza](https://riza.io)\n- <img height=\"12\" width=\"12\" src=\"https://cdn.foundation.roblox.com/current/RobloxStudio.ico\" alt=\"Roblox Studio\" /> **[Roblox Studio](https://github.com/Roblox/studio-rust-mcp-server)** - Roblox Studio MCP Server, create and manipulate scenes, scripts in Roblox Studio\n- <img src=\"https://hyper3d.ai/favicon.ico\" alt=\"Rodin\" width=\"12\" height=\"12\"> **[Rodin](https://github.com/DeemosTech/rodin-api-mcp)** - Generate 3D Models with [Hyper3D Rodin](https://hyper3d.ai)\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/66b7de6a233c04f4dac200a6/66bed52680d689629483c18b_faviconV2%20(2).png\" alt=\"Root Signals Logo\" /> **[Root Signals](https://github.com/root-signals/root-signals-mcp)** - Improve and quality control your outputs with evaluations using LLM-as-Judge\n- **[Roundtable](https://github.com/askbudi/roundtable)** - Unified integration layer that bridges multiple AI coding assistants (Codex, Claude Code, Cursor, Gemini) through zero-configuration auto-discovery and enterprise-ready architecture.\n- **[Routine](https://github.com/routineco/mcp-server)** - MCP server to interact with [Routine](https://routine.co/): calendars, tasks, notes, etc.\n- <img height=\"12\" width=\"12\" src=\"https://platform.composio.dev/favicon.ico\" alt=\"Composio Logo\"> **[Rube](https://github.com/ComposioHQ/Rube)** - Rube is a Model Context Protocol (MCP) server that connects your AI tools to 500+ apps like Gmail, Slack, GitHub, and Notion. Simply install it in your AI client, authenticate once with your apps, and start asking your AI to perform real actions like \"Send an email\" or \"Create a task.\"\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/safedep/.github/refs/heads/main/assets/logo/1.png\" alt=\"SafeDep Logo\" /> **[SafeDep](https://github.com/safedep/vet/blob/main/docs/mcp.md)** - SafeDep `vet-mcp` helps in  vetting open source packages for security risksâ€”such as vulnerabilities and malicious codeâ€”before they're used in your project, especially with AI-generated code suggestions.\n- <img height=\"12\" width=\"12\" src=\"https://waf-ce.chaitin.cn/favicon.ico\" alt=\"SafeLine Logo\" /> **[SafeLine](https://github.com/chaitin/SafeLine/tree/main/mcp_server)** - [SafeLine](https://safepoint.cloud/landing/safeline) is a self-hosted WAF(Web Application Firewall) to protect your web apps from attacks and exploits.\n- <img height=\"12\" width=\"12\" src=\"https://scrapi.tech/favicon.ico\" alt=\"ScrAPI Logo\" /> **[ScrAPI](https://github.com/DevEnterpriseSoftware/scrapi-mcp)** - Web scraping using [ScrAPI](https://scrapi.tech). Extract website content that is difficult to access because of bot detection, captchas or even geolocation restrictions.\n- <img height=\"12\" width=\"12\" src=\"https://upnorthmedia.co/favicon.ico\" alt=\"Up North Media Logo\" /> **[ScreenshotMCP](https://github.com/upnorthmedia/ScreenshotMCP/)** - A Model Context Protocol MCP server for capturing website screenshots with full page, element, and device size features.\n- <img height=\"12\" width=\"12\" src=\"https://screenshotone.com/favicon.ico\" alt=\"ScreenshotOne Logo\" /> **[ScreenshotOne](https://github.com/screenshotone/mcp/)** - Render website screenshots with [ScreenshotOne](https://screenshotone.com/)\n- <img height=\"12\" width=\"12\" src=\"https://pics.fatwang2.com/56912e614b35093426c515860f9f2234.svg\" alt=\"Search1API Logo\" /> **[Search1API](https://github.com/fatwang2/search1api-mcp)** - One API for Search, Crawling, and Sitemaps\n- <img height=\"12\" width=\"12\" src=\"https://www.searchunify.com/favicon.ico\" alt=\"SearchUnify Logo\" /> **[SearchUnify](https://github.com/searchunify/su-mcp/)** - SearchUnify MCP Server (su-mcp) enables seamless integration of SearchUnify with Claude Desktop\n- <img height=\"12\" width=\"12\" src=\"https://secureframe.com/favicon.ico\" alt=\"Secureframe Logo\" /> **[Secureframe](https://github.com/secureframe/secureframe-mcp-server)** - Query security controls, monitor compliance tests, and access audit data across SOC 2, ISO 27001, CMMC, FedRAMP, and other frameworks from [Secureframe](https://secureframe.com).\n- <img height=\"12\" width=\"12\" src=\"https://semgrep.dev/favicon.ico\" alt=\"Semgrep Logo\" /> **[Semgrep](https://github.com/semgrep/semgrep/blob/develop/cli/src/semgrep/mcp/README.md)** - Enable AI agents to secure code with [Semgrep](https://semgrep.dev/).\n- <img height=\"12\" width=\"12\" src=\"https://semilattice.ai/favicon.png\" alt=\"Semilattice icon\" /> **[Semilattice](https://github.com/semilattice-research/mcp)** - Test content, personalise features, and A/B test decisions with accurate audience prediction.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/187640573?s=48&v=4\" alt=\"Sequa Logo\" /> **[Sequa.AI](https://github.com/sequa-ai/sequa-mcp)** - Stop stitching context for Copilot and Cursor. With [Sequa MCP](https://github.com/sequa-ai/sequa-mcp), your AI tools know all your codebases and docs out of the box.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/6372338e5477e047032b37a5/64f85e6388a2a5c8c9525b4d_favLogo.png\" alt=\"Shortcut Logo\" /> **[Shortcut](https://github.com/useshortcut/mcp-server-shortcut)** - Access and implement all of your projects and tasks (Stories) from [Shortcut](https://shortcut.com/).\n- <img height=\"12\" width=\"12\" src=\"https://simplifier.io/favicon.ico\" alt=\"Simplifier Logo\" /> **[Simplifier](https://github.com/simplifier-ag/simplifier-mcp)** - Manage connectors, business objects and more in your [Simplifier](https://simplifier.io/) low code platform.\n- <img height=\"12\" width=\"12\" src=\"https://www.singlestore.com/favicon-32x32.png?v=277b9cbbe31e8bc416504cf3b902d430\"/> **[SingleStore](https://github.com/singlestore-labs/mcp-server-singlestore)** - Interact with the SingleStore database platform\n- <img height=\"12\" width=\"12\" src=\"https://smartbear.com/smartbear/assets/img/favicon.png\" alt=\"SmartBear Logo\" /> **[SmartBear](https://github.com/SmartBear/smartbear-mcp)** - Provides access to multiple capabilities across SmartBear's API Hub, Test Hub, and Insight Hub, all through [dedicated tools and resources](https://developer.smartbear.com/smartbear-mcp/docs/mcp-server).\n- <img src=\"https://smooth-operator.online/logo48.png\" alt=\"Smooth Operator\" width=\"12\" height=\"12\"> **[Smooth Operator](https://smooth-operator.online/agent-tools-api-docs/toolserverdocs)** - Tools to automate Windows via AI Vision, Mouse, Keyboard, Automation Trees, Webbrowser\n- <img height=\"12\" width=\"12\" src=\"https://app.snyk.io/bundle/favicon-faj49uD9.png\" alt=\"Snyk Logo\" /> **[Snyk](https://github.com/snyk/snyk-ls/blob/main/mcp_extension/README.md)** - Enhance security posture by embedding [Snyk](https://snyk.io/) vulnerability scanning directly into agentic workflows.\n- <img height=\"12\" width=\"12\" src=\"https://www.sonarsource.com/favicon.ico\" alt=\"SonarQube Logo\" /> **[SonarQube](https://github.com/SonarSource/sonarqube-mcp-server)** - Enables seamless integration with [SonarQube](https://www.sonarsource.com/) Server or Cloud and allows for code snippet analysis within the agent context.\n- <img src=\"https://sophtron.com/favicon.ico\" alt=\"Sophtron\" width=\"12\" height=\"12\"> **[Sophtron](https://github.com/sophtron/Sophtron-Integration/tree/main/modelcontextprotocol)** - Connect to your bank, credit card, utilities accounts to retrieve account balances and transactions with [Sophtron Bank Integration](https://sophtron.com).\n- <img height=\"12\" width=\"12\" src=\"https://learn.microsoft.com/favicon.ico\" alt=\"Microsoft Learn Logo\" /> **[SQL Server](https://github.com/Azure-Samples/SQL-AI-samples/tree/main/MssqlMcp)** - Official Microsoft SQL Server MCP<sup>[1](https://devblogs.microsoft.com/azure-sql/introducing-mssql-mcp-server/)</sup>\n- <img height=\"12\" width=\"12\" src=\"https://www.stackhawk.com/wp-content/uploads/2025/03/icon-512x512-2-150x150.png\" alt=\"StackHawk Logo\" /> **[StackHawk](https://github.com/stackhawk/stackhawk-mcp)** - Use [StackHawk](https://www.stackhawk.com/) to test for and FIX security problems in your code or vibe coded app.\n- <img height=\"12\" width=\"12\" src=\"https://stackoverflow.com/Content/Sites/stackoverflow/Img/apple-touch-icon@2.png\" alt=\"StackOverflow Logo\" /> **[Stack Overflow](https://api.stackexchange.com/docs/mcp-server)** - Access Stack Overflow's trusted and verified technical questions and answers.\n- <img height=\"12\" width=\"12\" src=\"https://www.stardog.com/img/favicon.ico?_cchid=1cc28b39bd2e8a628edeed79ccd4f49c\" alt=\"Stardog Logo\" /> **[Stardog](https://github.com/stardog-union/stardog-cloud-mcp)** - Provide trusted, contextual answers to both humans and agents using your enterprise knowledge graph with [Stardog](https://www.stardog.com)'s Semantic AI Platform.\n- <img height=\"12\" width=\"12\" src=\"https://www.starrocks.io/favicon.ico\" alt=\"StarRocks Logo\" /> **[StarRocks](https://github.com/StarRocks/mcp-server-starrocks)** - Interact with [StarRocks](https://www.starrocks.io/)\n- <img height=\"12\" width=\"12\" src=\"https://downloads.steadybit.com/logomark.svg\" alt=\"Steadybit Logo\" /> **[Steadybit](https://github.com/steadybit/mcp)** - Interact with [Steadybit](https://www.steadybit.com/)\n- <img height=\"12\" width=\"12\" src=\"https://steuerboard.net/favicon.ico\" alt=\"Steuerboard Logo\" /> **[Steuerboard](https://github.com/steuerboard/steuerboard-mcp-typescript)** - Interact with the accounting data in your business using our official MCP server\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/22632046?s=200&v=4\" alt=\"Storybook Logo\" /> **[Storybook](https://github.com/storybookjs/addon-mcp)** - Interact with [Storybook](https://storybook.js.org/) to automate UI component testing and documentation\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/klavis-ai/klavis/main/static/klavis-ai.png\" alt=\"Strata Logo\" /> **[Strata](https://www.klavis.ai/)** - One MCP server that guides your AI agents through thousands of tools in multiple apps progressively. It eliminates context overload and ensures accurate tool selection, enabling agents to handle complex, multi-app workflows with ease.\n- <img height=\"12\" width=\"12\" src=\"https://stripe.com/favicon.ico\" alt=\"Stripe Logo\" /> **[Stripe](https://github.com/stripe/agent-toolkit)** - Interact with Stripe API\n- <img height=\"12\" width=\"12\" src=\"https://www.success.co/favicon.ico\" alt=\"Success.co Logo\" /> **[Success.co](https://www.success.co/docs/guides/ai-mcp-connector)** - Interact with your Success.co account - enhance your EOSÂ® journey and get insights on your teams and business.\n- <img height=\"12\" width=\"12\" src=\"https://github.com/cdnsteve.png\" alt=\"Sugar Logo\" /> **[Sugar](https://github.com/cdnsteve/sugar)** - Autonomous AI development platform for Claude Code with task management, specialized agents, and workflow automation. Full MCP server bridges Claude with Python CLI for rich task context and autonomous execution.\n- <img height=\"12\" width=\"12\" src=\"https://sunra.ai/favicon.ico\" alt=\"Sunra AI Logo\" /> **[Sunra AI](https://github.com/sunra-ai/sunra-clients/tree/main/mcp-server)** - Search for and run AI models on [Sunra.ai](https://sunra.ai). Discover models, create video, image, and 3D model content, track their status, and manage the generated media.\n- <img height=\"12\" width=\"12\" src=\"https://supabase.com/favicon/favicon.ico\" alt=\"Supabase Logo\" /> **[Supabase](https://github.com/supabase-community/supabase-mcp)** - Interact with Supabase: Create tables, query data, deploy edge functions, and more.\n- <img height=\"12\" width=\"12\" src=\"https://supadata.ai/favicon.ico\" alt=\"Supadata Logo\" /> **[Supadata](https://github.com/supadata-ai/mcp)** - Official MCP server for [Supadata](https://supadata.ai) - YouTube, TikTok, X and Web data for makers.\n- <img height=\"12\" width=\"12\" src=\"https://d12w4pyrrczi5e.cloudfront.net/archive/50eb154ab859c63a8f1c850f9fe094e25d35e929/images/favicon.ico\" alt=\"Tako Logo\" /> **[Tako](https://github.com/TakoData/tako-mcp)** - Use natural language to search [Tako](https://trytako.com) for real-time financial, sports, weather, and public data with visualization\n- <img height=\"12\" width=\"12\" src=\"https://tavily.com/favicon.ico\" alt=\"Tavily Logo\" /> **[Tavily](https://github.com/tavily-ai/tavily-mcp)** - Search engine for AI agents (search + extract) powered by [Tavily](https://tavily.com/)\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/10522416?s=200&v=4\" alt=\"Telnyx Logo\" /> **[Telnyx](https://github.com/team-telnyx/telnyx-mcp-server)** - Official MCP server for building AI-powered communication apps. Create voice assistants, send SMS campaigns, manage phone numbers, and integrate real-time messaging with enterprise-grade reliability. Includes remote [streamable-http](https://api.telnyx.com/v2/mcp) and [sse](https://api.telnyx.com/mcp/sse) servers.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/91520705?s=48&v=4\" alt=\"Tencent RTC Logo\" /> **[Tencent RTC](https://github.com/Tencent-RTC/mcp)** - The MCP Server enables AI IDEs to more effectively understand and use [Tencent's Real-Time Communication](https://trtc.io/) SDKs and APIs, which significantly streamlines the process for developers to build audio/video call applications.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/1615979?s=200&v=4\" alt=\"Teradata Logo\" /> **[Teradata](https://github.com/Teradata/teradata-mcp-server)** - This MCP Server support tools and prompts for multi task data analytics on a [Teradata](https://teradata.com) platform.\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/hashicorp/terraform-mcp-server/main/public/images/Terraform-LogoMark_onDark.svg\" alt=\"Terraform Logo\" /> **[Terraform](https://github.com/hashicorp/terraform-mcp-server)** - Seamlessly integrate with Terraform ecosystem, enabling advanced automation and interaction capabilities for Infrastructure as Code (IaC) development powered by [Terraform](https://www.hashicorp.com/en/products/terraform)\n- <img height=\"12\" width=\"12\" src=\"https://textarttools.com/textarttoolslogo.png\" alt=\"TextArtTools Logo\" /> **[TextArtTools](https://github.com/humanjesse/textarttools-mcp)** - Transform text with 23 Unicode styles and create stylized banners with 322+ figlet fonts.\n- <img height=\"12\" width=\"12\" src=\"https://www.textin.com/favicon.png\" alt=\"TextIn Logo\" /> **[TextIn](https://github.com/intsig-textin/textin-mcp)** - An MCP server for the [TextIn](https://www.textin.com/?from=github_mcp) API, is a tool for extracting text and performing OCR on documents, it also supports converting documents into Markdown\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/106156665?s=200\" alt=\"Thena Logo\" /> **[Thena](https://mcp.thena.ai)** - Thena's MCP server for enabling users and AI agents to interact with Thena's services and manage customers across different channels such as Slack, Email, Web, Discord etc.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/24291394?v=4\" alt=\"ThingsBoard\" /> **[ThingsBoard](https://github.com/thingsboard/thingsboard-mcp)** - The ThingsBoard MCP Server provides a natural language interface for LLMs and AI agents to interact with your ThingsBoard IoT platform.\n- <img height=\"12\" width=\"12\" src=\"https://www.lg.com/favicon.ico\" alt=\"ThinQ Logo\" /> **[ThinQ Connect](https://github.com/thinq-connect/thinqconnect-mcp)** - Interact with LG ThinQ smart home devices and appliances through the ThinQ Connect MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://thirdweb.com/favicon.ico\" alt=\"Thirdweb Logo\" /> **[Thirdweb](https://github.com/thirdweb-dev/ai/tree/main/python/thirdweb-mcp)** - Read/write to over 2k blockchains, enabling data querying, contract analysis/deployment, and transaction execution, powered by [Thirdweb](https://thirdweb.com/)\n- <img height=\"12\" width=\"12\" src=\"https://www.thoughtspot.com/favicon-16x16.png\" alt=\"ThoughtSpot Logo\" /> **[ThoughtSpot](https://github.com/thoughtspot/mcp-server)** - AI is the new BI. A dedicated data analyst for everyone on your team. Bring [ThoughtSpot](https://thoughtspot.com) powers into Claude or any MCP host.\n- <img height=\"12\" width=\"12\" src=\"https://tianji.msgbyte.com/img/dark-brand.svg\" alt=\"Tianji Logo\" /> **[Tianji](https://github.com/msgbyte/tianji/tree/master/apps/mcp-server)** - Interact with Tianji platform whatever selfhosted or cloud platform, powered by [Tianji](https://tianji.msgbyte.com/).\n- <img height=\"12\" width=\"12\" src=\"https://www.pingcap.com/favicon.ico\" alt=\"TiDB Logo\" /> **[TiDB](https://github.com/pingcap/pytidb)** - MCP Server to interact with TiDB database platform.\n- <img height=\"12\" width=\"12\" src=\"https://www.tinybird.co/favicon.ico\" alt=\"Tinybird Logo\" /> **[Tinybird](https://github.com/tinybirdco/mcp-tinybird)** - Interact with Tinybird serverless ClickHouse platform\n- <img height=\"12\" width=\"12\" src=\"https://b2729162.smushcdn.com/2729162/wp-content/uploads/2023/10/cropped-Favicon-1-192x192.png?lossy=1&strip=1&webp=1\" alt=\"Tldv Logo\" /> **[Tldv](https://gitlab.com/tldv/tldv-mcp-server)** - Connect your AI agents to Google-Meet, Zoom & Microsoft Teams through [tl;dv](https://tldv.io)\n- <img height=\"12\" width=\"12\" src=\"https://www.todoist.com/static/favicon-32x32.png\" alt=\"Todoist Logo\" /> **[Todoist](https://github.com/doist/todoist-ai)** - Search, add, and update [Todoist](https://todoist.com) tasks, projects, sections, comments, and more.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.tokenmetrics.com/logo.svg\" alt=\"Token Metrics Logo\" /> **[Token Metrics](https://github.com/token-metrics/mcp)** - [Token Metrics](https://www.tokenmetrics.com/) integration for fetching real-time crypto market data, trading signals, price predictions, and advanced analytics.\n- <img height=\"12\" width=\"12\" src=\"https://di8m9w6rqrh5d.cloudfront.net/2G3TRwfv1w3GTLfmT7Dmco1VddoFTI5P/1920_6b7e7ec2-d897-4cd7-94f3-46a8301212c3.png\" alt=\"TomTom Logo\" /> **[TomTom-MCP](https://github.com/tomtom-international/tomtom-mcp)** - The [TomTom](https://www.tomtom.com/) MCP Server simplifies geospatial development by providing seamless access to TomTom's location services, including search, routing, traffic and static maps data.\n- <img height=\"12\" width=\"12\" src=\"https://images.tradeit.app/trade_agent/logo.svg\" alt=\"Trade It Logo\" /> **[Trade It](https://github.com/trade-it-inc/trade-it-mcp)** - Execute stock, crypto, and options trades on your brokerage via [Trade It](https://tradeit.app). Supports Robinhood, ETrade, Charles Schwab, Webull, Coinbase, and Kraken.\n- <img height=\"18\" width=\"18\" src=\"https://github.com/twelvedata/mcp/raw/develop/favicon.ico\" alt=\"Twelvedata Logo\" /> **[Twelve Data](https://github.com/twelvedata/mcp)** â€” Integrate your AI agents with real-time and historical financial market data through our official [Twelve Data](https://twelvedata.com) MCP server.\n- <img height=\"12\" width=\"12\" src=\"https://www.twilio.com/content/dam/twilio-com/core-assets/social/favicon-16x16.png\" alt=\"Twilio Logo\" /> **[Twilio](https://github.com/twilio-labs/mcp)** - Interact with [Twilio](https://www.twilio.com/en-us) APIs to send SMS messages, manage phone numbers, configure your account, and more.\n- <img height=\"12\" width=\"12\" src=\"https://miniprogram.tcsas-superapp.com/icon_512.png\" alt=\"TCSAS Logo\" /> **[TCSAS](https://github.com/TCMPP-Team/tcsas-devtools-mcp-server)** - Built on the Tencent Mini Program technical framework and fully following the development, powered by [Tencent Cloud Super App as a Service](https://www.tencentcloud.com/products/tcsas?lang=en&pg=).\n- <img height=\"12\" width=\"12\" src=\"https://uberall.com/media/favicon.svg\" alt=\"Uberall Logo\" /> **[Uberall](https://github.com/uberall/uberall-mcp-server)** â€“ Manage multi - location presence, including listings, reviews, and social posting, via [uberall](https://uberall.com).\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/91906527\" alt=\"Unblocked Logo\" /> **[Unblocked](https://docs.getunblocked.com/unblocked-mcp)** Help your AI-powered IDEs generate faster, more accurate code by giving them access to context from Slack, Confluence, Google Docs, JIRA, and more with [Unblocked](https://getunblocked.com).\n- <img height=\"12\" width=\"12\" src=\"https://unifai.network/favicon.ico\" alt=\"UnifAI Logo\" /> **[UnifAI](https://github.com/unifai-network/unifai-mcp-server)** - Dynamically search and call tools using [UnifAI Network](https://unifai.network)\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/plcQevjrOYnyriuGw90NfQBPoQ.jpg\" alt=\"Unstructured Logo\" /> **[Unstructured](https://github.com/Unstructured-IO/UNS-MCP)** - Set up and interact with your unstructured data processing workflows in [Unstructured Platform](https://unstructured.io)\n- <img height=\"12\" width=\"12\" src=\"https://uno-assets.platform.uno/logos/PNG/Uno_Platform_Symbol_RW.png\" alt=\"Uno Platform Logo\" /> **[Uno Platform](https://platform.uno/)** - Connects agents and developers to [Uno Platform's](https://aka.platform.uno/mcp) knowledge base - docs, APIs, and best practices allowing for building cross-platform .NET applications.\n- <img height=\"12\" width=\"12\" src=\"https://upstash.com/icons/favicon-32x32.png\" alt=\"Upstash Logo\" /> **[Upstash](https://github.com/upstash/mcp-server)** - Manage Redis databases and run Redis commands on [Upstash](https://upstash.com/) with natural language.\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/e2e-test-quest/uuv/refs/heads/main/uuv.ico\" alt=\"UUV Logo\" /> **[UUV](https://github.com/e2e-test-quest/uuv/tree/main/packages/mcp-server)** - Generate human readable end to end tests with [UUV](https://e2e-test-quest.github.io/uuv/).\n- <img height=\"12\" width=\"12\" src=\"http://vaadin.com/favicon.ico\" alt=\"Vaadin Logo\" /> **[Vaadin](https://github.com/marcushellberg/vaadin-documentation-services)** - Search Vaadin documentation, get the full documentation, and get version information. Designed for AI agents.\n- <img src=\"https://www.vantage.sh/favicon.ico\" alt=\"Vantage\" width=\"12\" height=\"12\"> **[Vantage](https://github.com/vantage-sh/vantage-mcp-server)** - Interact with your organization's cloud cost spend.\n- <img height=\"12\" width=\"12\" src=\"https://mcp.variflight.com/favicon.ico\" alt=\"VariFlight Logo\" /> **[VariFlight](https://github.com/variflight/variflight-mcp)** - VariFlight's official MCP server provides tools to query flight information, weather data, comfort metrics, the lowest available fares, and other civil aviation-related data.\n- <img height=\"12\" width=\"12\" src=\"https://docs.octagonagents.com/logo.svg\" alt=\"Octagon Logo\" /> **[VCAgents](https://github.com/OctagonAI/octagon-vc-agents)** - Interact with investor agentsâ€”think Wilson or Thielâ€”continuously updated with market intel.\n- **[Vectorize](https://github.com/vectorize-io/vectorize-mcp-server/)** - [Vectorize](https://vectorize.io) MCP server for advanced retrieval, Private Deep Research, Anything-to-Markdown file extraction and text chunking.\n- <img height=\"12\" width=\"12\" src=\"https://static.verbwire.com/favicon-16x16.png\" alt=\"Verbwire Logo\" /> **[Verbwire](https://github.com/verbwire/verbwire-mcp-server)** - Deploy smart contracts, mint NFTs, manage IPFS storage, and more through the Verbwire API\n- <img height=\"12\" width=\"12\" src=\"http://vercel.com/favicon.ico\" alt=\"Vercel Logo\" /> **[Vercel](https://vercel.com/docs/mcp/vercel-mcp)** - Access logs, search docs, and manage projects and deployments.\n- <img height=\"12\" width=\"12\" src=\"https://verodat.io/assets/favicon-16x16.png\" alt=\"Verodat Logo\" /> **[Verodat](https://github.com/Verodat/verodat-mcp-server)** - Interact with Verodat AI Ready Data platform\n- <img height=\"12\" width=\"12\" src=\"https://www.veyrax.com/favicon.ico\" alt=\"VeyraX Logo\" /> **[VeyraX](https://github.com/VeyraX/veyrax-mcp)** - Single tool to control all 100+ API integrations, and UI components\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/174736222?s=200&v=4\" alt=\"VictoriaLogs Logo\" /> **[VictoriaLogs](https://github.com/VictoriaMetrics-Community/mcp-victorialogs)** - Integration with [VictoriaLogs APIs](https://docs.victoriametrics.com/victorialogs/querying/#http-api) and [documentation](https://docs.victoriametrics.com/victorialogs/) for working with logs and debugging tasks related to your VictoriaLogs instances.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/174736222?s=200&v=4\" alt=\"VictoriaMetrics Logo\" /> **[VictoriaMetrics](https://github.com/VictoriaMetrics-Community/mcp-victoriametrics)** - Comprehensive integration with [VictoriaMetrics APIs](https://docs.victoriametrics.com/victoriametrics/url-examples/) and [documentation](https://docs.victoriametrics.com/) for monitoring, observability, and debugging tasks related to your VictoriaMetrics instances.\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/174736222?s=200&v=4\" alt=\"VictoriaTraces Logo\" /> **[VictoriaTraces](https://github.com/VictoriaMetrics-Community/mcp-victoriatraces)** - Integration with [VictoriaTraces APIs](https://docs.victoriametrics.com/victoriatraces/querying/#http-api) and [documentation](https://docs.victoriametrics.com/victoriatraces/) for working with distributed tracing and debugging tasks related to your VictoriaTraces instances.\n- <img height=\"12\" width=\"12\" src=\"https://framerusercontent.com/images/ijlYG00LOcMD6zR1XLMxHbAwZkM.png\" alt=\"VideoDB Director\" /> **[VideoDB Director](https://github.com/video-db/agent-toolkit/tree/main/modelcontextprotocol)** - Create AI-powered video workflows including automatic editing, content moderation, voice cloning, highlight generation, and searchable video momentsâ€”all accessible via simple APIs and intuitive chat-based interfaces.\n- <img height=\"12\" width=\"12\" src=\"https://landing.ai/wp-content/uploads/2024/04/cropped-favicon-192x192.png\" alt=\"LandingAI VisionAgent\" /> **[VisionAgent MCP](https://github.com/landing-ai/vision-agent-mcp)** - A simple MCP server that enables your LLM to better reason over images, video and documents.\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/mckinsey/vizro/main/vizro-core/docs/assets/images/favicon.png\" alt=\"Vizro Logo\" /> **[Vizro](https://github.com/mckinsey/vizro/tree/main/vizro-mcp)** - Tools and templates to create validated and maintainable data charts and dashboards\n- <img height=\"12\" width=\"12\" src=\"https://wavespeed.ai/logo.webp\" alt=\"WaveSpeed Logo\" /> **[WaveSpeed](https://github.com/WaveSpeedAI/mcp-server)** - WaveSpeed MCP server providing AI agents with image and video generation capabilities.\n- <img height=\"12\" width=\"12\" src=\"https://waystation.ai/images/logo.svg\" alt=\"WayStation Logo\" /> **[WayStation](https://github.com/waystation-ai/mcp)** - Universal MCP server to connect to popular productivity tools such as Notion, Monday, AirTable, and many more\n- <img height=\"12\" width=\"12\" src=\"https://www.webflow.com/favicon.ico\" alt=\"Webflow Logo\"> **[Webflow](https://github.com/webflow/mcp-server)** - Interact with Webflow sites, pages, and collections\n- <img height=\"12\" width=\"12\" src=\"https://webscraping.ai/favicon.ico\" alt=\"WebScraping.AI Logo\" /> **[WebScraping.AI](https://github.com/webscraping-ai/webscraping-ai-mcp-server)** - Interact with **[WebScraping.AI](https://WebScraping.AI)** for web data extraction and scraping\n- <img height=\"12\" width=\"12\" src=\"https://static.whatsapp.net/rsrc.php/v3/yz/r/ujTY9i_Jhs1.png\" alt=\"WhatsApp Business Logo\" /> **[WhatsApp Business](https://medium.com/@wassenger/introducing-whatsapp-mcp-ai-connector-3d393b52d1b0)** - WhatsApp Business MCP connector enabling AI agents to send messages, manage conversations, access templates, and integrate with WhatsApp Business API for automated customer communication.\n- <img height=\"12\" width=\"12\" src=\"https://winston-app-production-public.s3.us-east-1.amazonaws.com/winston-ai-favicon-light.svg\" alt=\"Winston.AI Logo\" /> **[Winston AI](https://github.com/gowinston-ai/winston-ai-mcp-server)** - AI detector MCP server with industry leading accuracy rates in detecting use of AI in text and images. The [Winston AI](https://gowinston.ai) MCP server also offers a robust plagiarism checker to help maintain integrity.\n- <img height=\"12\" width=\"12\" src=\"https://woocommerce.com/wp-content/uploads/2024/12/cropped-logo-w-favicon.png\" alt=\"WooCommerce.com Logo\" /> **[WooCommerce.com](https://developer.woocommerce.com/docs/features/mcp/)** - Manaage your WooCommerce.com store, products, and orders with our MCP integration.\n- <img height=\"12\" width=\"12\" src=\"https://developer.wordpress.com/wp-content/uploads/2025/03/cropped-favicon-64x64-from-figma.png\" alt=\"WordPress.com Logo\" /> **[WordPress.com](https://developer.wordpress.com/docs/mcp/)** - Connect your AI assistant to WordPress.com, giving you direct visibility into your site's content, analytics, and settings.\n- <img height=\"12\" width=\"12\" src=\"https://www.xero.com/favicon.ico\" alt=\"Xero Logo\" /> **[Xero](https://github.com/XeroAPI/xero-mcp-server)** - Interact with the accounting data in your business using our official MCP server\n- <img height=\"12\" width=\"12\" src=\"https://storage.yandexcloud.net/ydb-www-prod-site-assets/favicon-202305/favicon.ico\" alt=\"YDB Logo\" /> **[YDB](https://github.com/ydb-platform/ydb-mcp)** - Query [YDB](https://ydb.tech/) databases\n- <img height=\"12\" width=\"12\" src=\"https://fe-resource.yeelight.com/logo-black.jpeg\" alt=\"Yeelight Logo\" /> **[Yeelight MCP Server](https://github.com/Yeelight/yeelight-iot-mcp)** - The official [Yeelight MCP Server](https://github.com/Yeelight/yeelight-iot-mcp) enables users to control and query their [Yeelight](https://en.yeelight.com/) smart devices using natural language, offering a seamless and efficient human-AI interaction experience.\n- <img height=\"12\" width=\"12\" src=\"https://cdn.prod.website-files.com/632cd328ed2b485519c3f689/6334977a5d1a542102d4b9b5_favicon-32x32.png\" alt=\"YepCode Logo\" /> **[YepCode](https://github.com/yepcode/mcp-server-js)** - Run code in a secure, scalable sandbox environment with full support for dependencies, secrets, logs, and access to APIs or databases. Powered by [YepCode](https://yepcode.io)\n- <img height=\"12\" width=\"12\" src=\"https://www.yugabyte.com/favicon-16x16.png\" alt=\"YugabyteDB Logo\" /> **[YugabyteDB](https://github.com/yugabyte/yugabytedb-mcp-server)** -  MCP Server to interact with your [YugabyteDB](https://www.yugabyte.com/) database\n- <img height=\"12\" width=\"12\" src=\"https://avatars.githubusercontent.com/u/14069894\" alt=\"Yunxin Logo\" /> **[Yunxin](https://github.com/netease-im/yunxin-mcp-server)** - An MCP server that connects to Yunxin's IM/RTC/DATA Open-API\n- <img height=\"12\" width=\"12\" src=\"https://cdn.zapier.com/zapier/images/favicon.ico\" alt=\"Zapier Logo\" /> **[Zapier](https://zapier.com/mcp)** - Connect your AI Agents to 8,000 apps instantly.\n- <img height=\"12\" width=\"12\" src=\"https://www.zenable.app/zenable_light.svg\" alt=\"Zenable Logo\" /> **[Zenable](https://docs.zenable.io/integrations/mcp/getting-started)** - Clean up sloppy AI code and prevent vulnerabilities\n- **[ZenML](https://github.com/zenml-io/mcp-zenml)** - Interact with your MLOps and LLMOps pipelines through your [ZenML](https://www.zenml.io) MCP server\n- **[ZettelkastenSpace](https://github.com/joshylchen/zettelkasten_space)** - Built on the proven [Zettelkasten](https://www.zettelkasten.space/) method, enhanced with Claude Desktop integration via Model Context Protocol \n- <img height=\"12\" width=\"12\" src=\"https://www.zine.ai/images/zine-logo.png\" alt=\"Zine Logo\" /> **[Zine](https://www.zine.ai)** - Your memory, everywhere AI goes. Think iPhoto for your knowledge - upload and curate. Like ChatGPT but portable - context that travels with you.\n- <img height=\"12\" width=\"12\" src=\"https://zizai.work/images/logo.jpg\" alt=\"ZIZAI Logo\" /> **[ZIZAI Recruitment](https://github.com/zaiwork/mcp)** - Interact with the next-generation intelligent recruitment platform for employees and employers, powered by [ZIZAI Recruitment](https://zizai.work).\n\n### ğŸŒ Community Servers\n\nA growing set of community-developed and maintained servers demonstrates various applications of MCP across different domains.\n\n> [!NOTE]\n> Community servers are **untested** and should be used at **your own risk**. They are not affiliated with or endorsed by Anthropic.\n\n- **[1mcpserver](https://github.com/particlefuture/1mcpserver)** - MCP of MCPs. Automatically discover, configure, and add MCP servers on your local machine.\n- **[1Panel](https://github.com/1Panel-dev/mcp-1panel)** - MCP server implementation that provides 1Panel interaction.\n- **[A2A](https://github.com/GongRzhe/A2A-MCP-Server)** - An MCP server that bridges the Model Context Protocol (MCP) with the Agent-to-Agent (A2A) protocol, enabling MCP-compatible AI assistants (like Claude) to seamlessly interact with A2A agents.\n- **[Ableton Live](https://github.com/Simon-Kansara/ableton-live-mcp-server)** - an MCP server to control Ableton Live.\n- **[Ableton Live](https://github.com/ahujasid/ableton-mcp)** (by ahujasid) - Ableton integration allowing prompt enabled music creation.\n- **[ActivityPub MCP](https://github.com/cameronrye/activitypub-mcp)** - A comprehensive MCP server that enables LLMs to explore and interact with the Fediverse through ActivityPub protocol, supporting actor discovery, timeline fetching, instance exploration, and WebFinger resolution across decentralized social networks.\n- **[Actor Critic Thinking](https://github.com/aquarius-wing/actor-critic-thinking-mcp)** - Actor-critic thinking for performance evaluation\n- **[Adobe Commerce](https://github.com/rafaelstz/adobe-commerce-dev-mcp)** â€” MCP to interact with Adobe Commerce GraphQL API, including orders, products, customers, etc.\n- **[ADR Analysis](https://github.com/tosin2013/mcp-adr-analysis-server)** - AI-powered Architectural Decision Records (ADR) analysis server that provides architectural insights, technology stack detection, security checks, and TDD workflow enhancement for software development projects.\n- **[Ads MCP](https://github.com/amekala/ads-mcp)** - Remote MCP server for cross-platform ad campaign creation (Google Ads Search & PMax, TikTok). OAuth 2.1 authentication with progress streaming support for long-running operations. [Website](https://www.adspirer.com/)\n- **[Agent Interviews](https://github.com/thinkchainai/agentinterviews_mcp)** - Conduct AI-powered qualitative research interviews and surveys at scale with [Agent Interviews](https://agentinterviews.com).\n- **[AgentBay](https://github.com/Michael98671/agentbay)** - An MCP server for providing serverless cloud infrastructure for AI agents.\n- **[Agentic Framework](https://github.com/Piotr1215/mcp-agentic-framework)** - Multi-agent collaboration framework enabling AI agents to register, discover each other, exchange asynchronous messages via HTTP transport, and work together on complex tasks with persistent message history.\n- **[AgentMode](https://www.agentmode.app)** - Connect to dozens of databases, data warehouses, Github & more, from a single MCP server.  Run the Docker image locally, in the cloud, or on-premise.\n- **[AI Agent Marketplace Index](https://github.com/AI-Agent-Hub/ai-agent-marketplace-index-mcp)** - MCP server to search more than 5000+ AI agents and tools of various categories from [AI Agent Marketplace Index](http://www.deepnlp.org/store/ai-agent) and monitor traffic of AI Agents.\n- **[AI Endurance](https://github.com/ai-endurance/mcp)** - AI-powered training platform for runners, cyclists, and triathletes with over 20 tools for workout management, activity analysis, performance predictions, and recovery tracking.\n- **[AI Tasks](https://github.com/jbrinkman/valkey-ai-tasks)** - Let the AI manage complex plans with integrated task management and tracking tools. Supports STDIO, SSE and Streamable HTTP transports.\n- **[ai-Bible](https://github.com/AdbC99/ai-bible)** - Search the bible reliably and repeatably [ai-Bible Labs](https://ai-bible.com)\n- **[Airbnb](https://github.com/openbnb-org/mcp-server-airbnb)** - Provides tools to search Airbnb and get listing details.\n- **[Airflow](https://github.com/yangkyeongmo/mcp-server-apache-airflow)** - An MCP Server that connects to [Apache Airflow](https://airflow.apache.org/) using official python client.\n- **[Airtable](https://github.com/domdomegg/airtable-mcp-server)** - Read and write access to [Airtable](https://airtable.com/) databases, with schema inspection.\n- **[Airtable](https://github.com/felores/airtable-mcp)** - Airtable Model Context Protocol Server.\n- **[Algorand](https://github.com/GoPlausible/algorand-mcp)** - A comprehensive MCP server for tooling interactions (40+) and resource accessibility (60+) plus many useful prompts for interacting with the Algorand blockchain.\n- **[Amadeus](https://github.com/donghyun-chae/mcp-amadeus)** (by donghyun-chae) - An MCP server to access, explore, and interact with Amadeus Flight Offers Search API for retrieving detailed flight options, including airline, times, duration, and pricing data.\n- **[Amazon Ads](https://github.com/MarketplaceAdPros/amazon-ads-mcp-server)** - MCP Server that provides interaction capabilities with Amazon Advertising through [MarketplaceAdPros](https://marketplaceadpros.com)/\n- **[AniList](https://github.com/yuna0x0/anilist-mcp)** (by yuna0x0) - An MCP server to interact with AniList API, allowing you to search for anime and manga, retrieve user data, and manage your watchlist.\n- **[Anki](https://github.com/scorzeth/anki-mcp-server)** - An MCP server for interacting with your [Anki](https://apps.ankiweb.net) decks and cards.\n- **[Anki](https://github.com/nietus/anki-mcp)** - MCP server to run locally with Anki and Ankiconnect. Supports creating, updating, searching and filtering cards and decks. Include mass update and other advanced tools.\n- **[AntV Chart](https://github.com/antvis/mcp-server-chart)** - A Model Context Protocol server for generating 15+ visual charts using [AntV](https://github.com/antvis).\n- **[Any Chat Completions](https://github.com/pyroprompts/any-chat-completions-mcp)** - Interact with any OpenAI SDK Compatible Chat Completions API like OpenAI, Perplexity, Groq, xAI and many more.\n- **[Apache Gravitino(incubating)](https://github.com/datastrato/mcp-server-gravitino)** - Allow LLMs to explore metadata of structured data and unstructured data with Gravitino, and perform data governance tasks including tagging/classification.\n- **[API Lab MCP](https://github.com/atototo/api-lab-mcp)** - Transform Claude into your AI-powered API testing laboratory. Test, debug, and document APIs through natural conversation with authentication support, response validation, and performance metrics.\n- **[APIWeaver](https://github.com/GongRzhe/APIWeaver)** - An MCP server that dynamically creates MCP  servers from web API configurations. This allows you to easily integrate any REST API, GraphQL endpoint, or web service into an MCP-compatible tool that can be used by AI assistants like Claude.\n- **[Apollo IO MCP Server](https://github.com/AgentX-ai/apollo-io-mcp-server)** - apollo.io mcp server. Get/enrich contact data for people and organizations agentically.\n- **[Apple Books](https://github.com/vgnshiyer/apple-books-mcp)** - Interact with your library on Apple Books, manage your book collection, summarize highlights, notes, and much more.\n- **[Apple Calendar](https://github.com/Omar-v2/mcp-ical)** - An MCP server that allows you to interact with your macOS Calendar through natural language, including features such as event creation, modification, schedule listing, finding free time slots etc.\n- **[Apple Docs](https://github.com/kimsungwhee/apple-docs-mcp)** - A powerful Model Context Protocol (MCP) server that provides seamless access to Apple Developer Documentation through natural language queries. Search, explore, and get detailed information about Apple frameworks, APIs, sample code, and more directly in your AI-powered development environment.\n- **[Apple Script](https://github.com/peakmojo/applescript-mcp)** - MCP server that lets LLM run AppleScript code to to fully control anything on Mac, no setup needed.\n- **[APT MCP](https://github.com/GdMacmillan/apt-mcp-server)** - MCP server which runs debian package manager (apt) commands for you using ai agents.\n- **[Aranet4](https://github.com/diegobit/aranet4-mcp-server)** - MCP Server to manage your Aranet4 CO2 sensor. Fetch data and store in a local SQLite. Ask questions about historical data.\n- **[ArangoDB](https://github.com/ravenwits/mcp-server-arangodb)** - MCP Server that provides database interaction capabilities through [ArangoDB](https://arangodb.com/).\n- **[ArangoDB Graph](https://github.com/PCfVW/mcp-arangodb-async)** - Async-first Python architecture, wrapping the official [python-arango driver](https://github.com/arangodb/python-arango) with graph management capabilities, content conversion utilities (JSON, Markdown, YAML and Table), backup/restore functionality, and graph analytics capabilities; the 33 MCP tools use strict [Pydantic](https://github.com/pydantic/pydantic) validation.\n- **[Archestra.AI](https://github.com/archestra-ai/archestra)** - Open-source enterprise-ready MCP gateway, MCP registry, MCP orchestrator, MCP credentials management, LLM cost management and chat platform.\n- **[Arduino](https://github.com/vishalmysore/choturobo)** - MCP Server that enables AI-powered robotics using Claude AI and Arduino (ESP32) for real-world automation and interaction with robots.\n- **[arXiv API](https://github.com/prashalruchiranga/arxiv-mcp-server)** - An MCP server that enables interacting with the arXiv API using natural language.\n- **[arxiv-latex-mcp](https://github.com/takashiishida/arxiv-latex-mcp)** - MCP server that fetches and processes arXiv LaTeX sources for precise interpretation of mathematical expressions in papers.\n- **[Arr Suite](https://github.com/shaktech786/arr-suite-mcp-server)** - Intelligent MCP server for Plex and the complete *arr media automation suite (Sonarr, Radarr, Prowlarr, Bazarr, Overseerr) with natural language processing for unified media management.\n- **[Atlassian](https://github.com/sooperset/mcp-atlassian)** - Interact with Atlassian Cloud products (Confluence and Jira) including searching/reading Confluence spaces/pages, accessing Jira issues, and project metadata.\n- **[Atlassian Server (by phuc-nt)](https://github.com/phuc-nt/mcp-atlassian-server)** - An MCP server that connects AI agents (Cline, Claude Desktop, Cursor, etc.) to Atlassian Jira & Confluence, enabling data queries and actions through the Model Context Protocol.\n- **[Attestable MCP](https://github.com/co-browser/attestable-mcp-server)** - An MCP server running inside a trusted execution environment (TEE) via Gramine, showcasing remote attestation using [RA-TLS](https://gramine.readthedocs.io/en/stable/attestation.html). This allows an MCP client to verify the server before connecting.\n- **[Audius](https://github.com/glassBead-tc/audius-mcp-atris)** - Audius + AI = Atris. Interact with fans, stream music, tip your favorite artists, and more on Audius: all through Claude.\n- **[AutoML](https://github.com/emircansoftware/MCP_Server_DataScience)** â€“ An MCP server for data analysis workflows including reading, preprocessing, feature engineering, model selection, visualization, and hyperparameter tuning.\n- **[Aviationstack](https://github.com/Pradumnasaraf/aviationstack-mcp)** â€“ An MCP server using the AviationStack API to fetch real-time flight data including airline flights, airport schedules, future flights and aircraft types.\n- **[AWS](https://github.com/rishikavikondala/mcp-server-aws)** - Perform operations on your AWS resources using an LLM.\n- **[AWS Athena](https://github.com/lishenxydlgzs/aws-athena-mcp)** - An MCP server for AWS Athena to run SQL queries on Glue Catalog.\n- **[AWS Cognito](https://github.com/gitCarrot/mcp-server-aws-cognito)** - An MCP server that connects to AWS Cognito for authentication and user management.\n- **[AWS Cost Explorer](https://github.com/aarora79/aws-cost-explorer-mcp-server)** - Optimize your AWS spend (including Amazon Bedrock spend) with this MCP server by examining spend across regions, services, instance types and foundation models ([demo video](https://www.youtube.com/watch?v=WuVOmYLRFmI&feature=youtu.be)).\n- **[AWS Open Data](https://github.com/domdomegg/aws-open-data-mcp)** - Search and explore datasets from the AWS Open Data Registry with fuzzy matching and detailed dataset information.\n- **[AWS Resources Operations](https://github.com/baryhuang/mcp-server-aws-resources-python)** - Run generated python code to securely query or modify any AWS resources supported by boto3.\n- **[AWS S3](https://github.com/aws-samples/sample-mcp-server-s3)** - A sample MCP server for AWS S3 that flexibly fetches objects from S3 such as PDF documents.\n- **[AWS SES](https://github.com/aws-samples/sample-for-amazon-ses-mcp)** Sample MCP Server for Amazon SES (SESv2). See [AWS blog post](https://aws.amazon.com/blogs/messaging - and-targeting/use-ai-agents-and-the-model-context-protocol-with-amazon-ses/) for more details.\n- **[AX-Platform](https://github.com/AX-MCP/PaxAI?tab=readme-ov-file#mcp-setup-guides)** - AI Agent collaboration platform. Collaborate on tasks, share context, and coordinate workflows.\n- **[Azure ADX](https://github.com/pab1it0/adx-mcp-server)** - Query and analyze Azure Data Explorer databases.\n- **[Azure DevOps](https://github.com/Vortiago/mcp-azure-devops)** - An MCP server that provides a bridge to Azure DevOps services, enabling AI assistants to query and manage work items.\n- **[Azure MCP Hub](https://github.com/Azure-Samples/mcp)** - A curated list of all MCP servers and related resources for Azure developers by **[Arun Sekhar](https://github.com/achandmsft)**\n- **[Azure OpenAI DALL-E 3 MCP Server](https://github.com/jacwu/mcp-server-aoai-dalle3)** - An MCP server for Azure OpenAI DALL-E 3 service to generate image from text.\n- **[Azure Wiki Search](https://github.com/coder-linping/azure-wiki-search-server)** - An MCP that enables AI to query the wiki hosted on Azure Devops Wiki.\n- **[Baidu AI Search](https://github.com/baidubce/app-builder/tree/master/python/mcp_server/ai_search)** - Web search with Baidu Cloud's AI Search\n- **[BambooHR MCP](https://github.com/encoreshao/bamboohr-mcp)** - An MCP server that interfaces with the BambooHR APIs, providing access to employee data, time tracking, and HR management features.\n- **[Base Free USDC Transfer](https://github.com/magnetai/mcp-free-usdc-transfer)** - Send USDC on [Base](https://base.org) for free using Claude AI! Built with [Coinbase CDP](https://docs.cdp.coinbase.com/mpc-wallet/docs/welcome).\n- **[Basic Memory](https://github.com/basicmachines-co/basic-memory)** - Local-first knowledge management system that builds a semantic graph from Markdown files, enabling persistent memory across conversations with LLMs.\n- **[BGG MCP](https://github.com/kkjdaniel/bgg-mcp)** (by kkjdaniel) - MCP to enable interaction with the BoardGameGeek API via AI tooling.\n- **[Bible](https://github.com/trevato/bible-mcp)** - Add biblical context to your generative AI applications.\n- **[BigQuery](https://github.com/LucasHild/mcp-server-bigquery)** (by LucasHild) - This server enables LLMs to inspect database schemas and execute queries on BigQuery.\n- **[BigQuery](https://github.com/ergut/mcp-bigquery-server)** (by ergut) - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities\n- **[Bilibili](https://github.com/wangshunnn/bilibili-mcp-server)** - This MCP server provides tools to fetch Bilibili user profiles, video metadata, search videos, and more.\n- **[Binance](https://github.com/ethancod1ng/binance-mcp-server)** - Cryptocurrency trading and market data access through Binance API integration.\n- **[Binance](https://github.com/AnalyticAce/binance-mcp-server)** (by dosseh shalom) - Unofficial tools and server implementation for Binance's Model Context Protocol (MCP). Designed to support developers building crypto trading AI Agents.\n- **[Bing Web Search API](https://github.com/leehanchung/bing-search-mcp)** (by hanchunglee) - Server implementation for Microsoft Bing Web Search API.\n- **[BioMCP](https://github.com/genomoncology/biomcp)** (by imaurer) - Biomedical research assistant server providing access to PubMed, ClinicalTrials.gov, and MyVariant.info.\n- **[bioRxiv](https://github.com/JackKuo666/bioRxiv-MCP-Server)** - ğŸ” Enable AI assistants to search and access bioRxiv papers through a simple MCP interface.\n- **[Bitable MCP](https://github.com/lloydzhou/bitable-mcp)** (by lloydzhou) - MCP server provides access to Lark Bitable through the Model Context Protocol. It allows users to interact with Bitable tables using predefined tools.\n- **[Blender](https://github.com/ahujasid/blender-mcp)** (by ahujasid) - Blender integration allowing prompt enabled 3D scene creation, modeling and manipulation.\n- **[Blender MCP](https://github.com/pranav-deshmukh/blender-mcp)** - MCP server to create professional like 3d scenes on blender using natural language.\n- **[Blockbench MCP Plugin](https://github.com/jasonjgardner/blockbench-mcp-plugin)** (by jasonjgardner) - Blockbench plugin to connect AI agents to Blockbench's JavaScript API. Allows for creating and editing 3D models or pixel art textures with AI in Blockbench.\n- **[Blockchain MCP](https://github.com/tatumio/blockchain-mcp)** - MCP Server for Blockchain Data from **[Tatum](http://tatum.io/mcp)** that instantly unlocks blockchain access for your AI agents. This official Tatum MCP server connects to any LLM in seconds.\n- **[Bluesky](https://github.com/semioz/bluesky-mcp)** (by semioz) - An MCP server for Bluesky, a decentralized social network. It enables automated interactions with the AT Protocol, supporting features like posting, liking, reposting, timeline management, and profile operations.\n- **[Bluetooth MCP Server](https://github.com/Hypijump31/bluetooth-mcp-server)** - Control Bluetooth devices and manage connections through natural language commands, including device discovery, pairing, and audio controls.\n- **[BNBChain MCP](https://github.com/bnb-chain/bnbchain-mcp)** - An MCP server for interacting with BSC, opBNB, and the Greenfield blockchain.\n- **[Braintree](https://github.com/QuentinCody/braintree-mcp-server)** - Unofficial PayPal Braintree payment gateway MCP Server for AI agents to process payments, manage customers, and handle transactions securely.\n- **[Brazilian Law](https://github.com/pdmtt/brlaw_mcp_server/)** (by pdmtt) - Agent-driven research on Brazilian law using official sources.\n- **[BreakoutRoom](https://github.com/agree-able/room-mcp)** - Agents accomplishing goals together in p2p rooms\n- **[Browser MCP](https://github.com/bytedance/UI-TARS-desktop/tree/main/packages/agent-infra/mcp-servers/browser)** (by UI-TARS) - A fast, lightweight MCP server that empowers LLMs with browser automation via Puppeteerâ€™s structured accessibility data, featuring optional vision mode for complex visual understanding and flexible, cross-platform configuration.\n- **[browser-use](https://github.com/co-browser/browser-use-mcp-server)** (by co-browser) - browser-use MCP server with dockerized playwright + chromium + vnc. supports stdio & resumable http.\n- **[BrowserLoop](https://github.com/mattiasw/browserloop)** - An MCP server for taking screenshots of web pages using Playwright. Supports high-quality capture with configurable formats, viewport sizes, cookie-based authentication, and both full page and element-specific screenshots.\n- **[Bsc-mcp](https://github.com/TermiX-official/bsc-mcp)** The first MCP server that serves as the bridge between AI and BNB Chain, enabling AI agents to execute complex on-chain operations through seamless integration with the BNB Chain, including transfer, swap, launch, security check on any token and even more.\n- **[BugBug MCP Server](https://github.com/simplypixi/bugbug-mcp-server)** - Unofficial MCP server for BugBug API.\n- **[BVG MCP Server - (Unofficial) ](https://github.com/svkaizoku/mcp-bvg)** - Unofficial MCP server for Berliner Verkehrsbetriebe Api.\n- **[Bybit](https://github.com/ethancod1ng/bybit-mcp-server)** - A Model Context Protocol (MCP) server for integrating AI assistants with Bybit cryptocurrency exchange APIs, enabling automated trading, market data access, and account management.\n- **[C64 Bridge](https://github.com/chrisgleissner/c64bridge)** - AI command bridge for Commodore 64 hardware. Control Ultimate 64 and C64 Ultimate devices through REST API with BASIC and assembly program creation, real-time memory inspection, SID audio synthesis, and curated retro computing knowledge via local RAG.\n- **[CAD-MCP](https://github.com/daobataotie/CAD-MCP#)** (by daobataotie) - Drawing CAD(Line,Circle,Text,Annotation...) through MCP server, supporting mainstream CAD software.\n- **[Calculator](https://github.com/githejie/mcp-server-calculator)** - This server enables LLMs to use calculator for precise numerical calculations.\n- **[CalDAV MCP](https://github.com/dominik1001/caldav-mcp)** - A CalDAV MCP server to expose calendar operations as tools for AI assistants.\n- **[Calendly-mcp-server](https://github.com/meAmitPatil/calendly-mcp-server)** - Open source calendly mcp server.\n- **[Catalysis Hub](https://github.com/QuentinCody/catalysishub-mcp-server)** - Unofficial MCP server for searching and retrieving scientific data from the Catalysis Hub database, providing access to computational catalysis research and surface reaction data.\n- **[CCTV VMS MCP](https://github.com/jyjune/mcp_vms)** - A Model Context Protocol (MCP) server designed to connect to a CCTV recording program (VMS) to retrieve recorded and live video streams. It also provides tools to control the VMS software, such as showing live or playback dialogs for specific channels at specified times.\n- **[CFBD API](https://github.com/lenwood/cfbd-mcp-server)** - An MCP server for the [College Football Data API](https://collegefootballdata.com/).\n- **[ChatMCP](https://github.com/AI-QL/chat-mcp)** â€“ An Open Source Cross-platform GUI Desktop application compatible with Linux, macOS, and Windows, enabling seamless interaction with MCP servers across dynamically selectable LLMs, by **[AIQL](https://github.com/AI-QL)**\n- **[ChatSum](https://github.com/mcpso/mcp-server-chatsum)** - Query and Summarize chat messages with LLM. by [mcpso](https://mcp.so)\n- **[Chess.com](https://github.com/pab1it0/chess-mcp)** - Access Chess.com player data, game records, and other public information through standardized MCP interfaces, allowing AI assistants to search and analyze chess information.\n- **[Chessagine-mcp](https://github.com/jalpp/chessagine-mcp)** - A chess MCP server that integrates Stockfish engine evaluation, positional theme analysis, Lichess opening databases, and chess knowledgebase.\n- **[ChessPal Chess Engine (stockfish)](https://github.com/wilson-urdaneta/chesspal-mcp-engine)** - A Stockfish-powered chess engine exposed as an MCP server. Calculates best moves and supports both HTTP/SSE and stdio transports.\n- **[Chroma](https://github.com/privetin/chroma)** - Vector database server for semantic document search and metadata filtering, built on Chroma\n- **[Chrome history](https://github.com/vincent-pli/chrome-history-mcp)** - Talk with AI about your browser history, get fun ^_^\n- **[cicada](https://github.com/wende/cicada)** - AST-powered code intelligence for Elixir projects. Provides 9 tools including function search, call site tracking, PR attribution, git history, and semantic search - reducing AI query tokens by 82%.\n- **[CIViC](https://github.com/QuentinCody/civic-mcp-server)** - MCP server for the Clinical Interpretation of Variants in Cancer (CIViC) database, providing access to clinical variant interpretations and genomic evidence for cancer research.\n- **[Claude Thread Continuity](https://github.com/peless/claude-thread-continuity)** - Persistent memory system enabling Claude Desktop conversations to resume with full context across sessions. Maintains conversation history, project states, and user preferences for seamless multi-session workflows.\n- **[claude-faf-mcp](https://github.com/Wolfe-Jam/claude-faf-mcp)** - MCP server for .faf format. Context scoring engine with project context management.\n- **[ClaudePost](https://github.com/ZilongXue/claude-post)** - ClaudePost enables seamless email management for Gmail, offering secure features like email search, reading, and sending.\n- **[CLDGeminiPDF Analyzer](https://github.com/tfll37/CLDGeminiPDF-Analyzer)** - MCP server tool enabling sharing large PDF files to Google LLMs via API for further/additional analysis and response retrieval to Claude Desktop.\n- **[ClearML MCP](https://github.com/prassanna-ravishankar/clearml-mcp)** - Get comprehensive ML experiment context and analysis directly from [ClearML](https://clear.ml) in your AI conversations.\n- **[ClickUp](https://github.com/TaazKareem/clickup-mcp-server)** - MCP server for ClickUp task management, supporting task creation, updates, bulk operations, and markdown descriptions.\n- **[Cloudinary](https://github.com/felores/cloudinary-mcp-server)** - Cloudinary Model Context Protocol Server to upload media to Cloudinary and get back the media link and details.\n- **[CockroachDB](https://github.com/amineelkouhen/mcp-cockroachdb)** - MCP server enabling AI agents and LLMs to manage, monitor, and query **[CockroachDB](https://www.cockroachlabs.com/)** using natural language.\n- **[CockroachDB MCP Server](https://github.com/viragtripathi/cockroachdb-mcp-server)** â€“ Full - featured MCP implementation built with FastAPI and CockroachDB. Supports schema bootstrapping, JSONB storage, LLM-ready CLI, and optional `/debug` endpoints.\n- **[Code Screenshot Generator](https://github.com/MoussaabBadla/code-screenshot-mcp)** - Generate beautiful syntax-highlighted code screenshots with professional themes directly from Claude. Supports file reading, line selection, git diff visualization, and batch processing.\n- **[code-assistant](https://github.com/stippi/code-assistant)** - A coding assistant MCP server that allows to explore a code-base and make changes to code. Should be used with trusted repos only (insufficient protection against prompt injections).\n- **[code-context-provider-mcp](https://github.com/AB498/code-context-provider-mcp)** - MCP server that provides code context and analysis for AI assistants. Extracts directory structure and code symbols using WebAssembly Tree-sitter parsers without Native Dependencies.\n- **[code-executor](https://github.com/bazinga012/mcp_code_executor)** - An MCP server that allows LLMs to execute Python code within a specified Conda environment.\n- **[code-sandbox-mcp](https://github.com/Automata-Labs-team/code-sandbox-mcp)** - An MCP server to create secure code sandbox environment for executing code within Docker containers.\n- **[cognee-mcp](https://github.com/topoteretes/cognee/tree/main/cognee-mcp)** - GraphRAG memory server with customizable ingestion, data processing and search\n- **[coin_api_mcp](https://github.com/longmans/coin_api_mcp)** - Provides access to [coinmarketcap](https://coinmarketcap.com/) cryptocurrency data.\n- **[CoinMarketCap](https://github.com/shinzo-labs/coinmarketcap-mcp)** - Implements the complete [CoinMarketCap](https://coinmarketcap.com/) API for accessing cryptocurrency market data, exchange information, and other blockchain-related metrics.\n- **[commands](https://github.com/g0t4/mcp-server-commands)** - Run commands and scripts. Just like in a terminal.\n- **[Companies House MCP](https://github.com/stefanoamorelli/companies-house-mcp)** (by Stefano Amorelli) - MCP server to connect with the UK Companies House API.\n- **[computer-control-mcp](https://github.com/AB498/computer-control-mcp)** - MCP server that provides computer control capabilities, like mouse, keyboard, OCR, etc. using PyAutoGUI, RapidOCR, ONNXRuntime Without External Dependencies.\n- **[Computer-Use - Remote MacOS Use](https://github.com/baryhuang/mcp-remote-macos-use)** - Open-source out-of-the-box alternative to OpenAI Operator, providing a full desktop experience and optimized for using remote macOS machines as autonomous AI agents.\n- **[computer-use-mcp](https://github.com/domdomegg/computer-use-mcp)** - Control your computer with screen capture, mouse, and keyboard capabilities for automated desktop interaction and task execution.\n- **[Congress.gov API](https://github.com/AshwinSundar/congress_gov_mcp)** - An MCP server to interact with real-time data from the Congress.gov API, which is the official API for the United States Congress.\n- **[Console Automation](https://github.com/ooples/mcp-console-automation)** - Production-ready MCP server for AI-driven console automation and monitoring. 40 tools for session management, SSH, testing, monitoring, and background jobs. Like Playwright for terminal applications.\n- **[consul-mcp](https://github.com/kocierik/consul-mcp-server)** - A consul MCP server for service management, health check and Key-Value Store\n- **[consult7](https://github.com/szeider/consult7)** - Analyze large codebases and document collections using high-context models via OpenRouter, OpenAI, or Google AI -- very useful, e.g., with Claude Code\n- **[Contentful-mcp](https://github.com/ivo-toby/contentful-mcp)** - Read, update, delete, publish content in your [Contentful](https://contentful.com) space(s) from this MCP Server.\n- **[Context Crystallizer](https://github.com/hubertciebiada/context-crystallizer)** - AI Context Engineering tool that transforms large repositories into crystallized, AI-consumable knowledge through systematic analysis and optimization.\n- **[Context Processor](https://github.com/mschultheiss83/context-processor)** - Intelligent context management with configurable pre-processing strategies (clarify, analyze, search, fetch) for enhancing content clarity, searchability, and metadata extraction.\n- **[context-portal](https://github.com/GreatScottyMac/context-portal)** - Context Portal (ConPort) is a memory bank database system that effectively builds a project-specific knowledge graph, capturing entities like decisions, progress, and architecture, along with their relationships. This serves as a powerful backend for Retrieval Augmented Generation (RAG), enabling AI assistants to access precise, up-to-date project information.\n- **[cplusplus-mcp](https://github.com/kandrwmrtn/cplusplus_mcp)** - Semantic C++ code analysis using libclang. Enables Claude to understand C++ codebases through AST parsing rather than text search - find classes, navigate inheritance, trace function calls, and explore code relationships.\n- **[CRASH](https://github.com/nikkoxgonzales/crash-mcp)** - MCP server for structured, iterative reasoning and thinking with flexible validation, confidence tracking, revision mechanisms, and branching support.\n- **[CreateveAI Nexus](https://github.com/spgoodman/createveai-nexus-server)** - Open-Source Bridge Between AI Agents and Enterprise Systems, with simple custom API plug-in capabilities (including close compatibility with ComfyUI nodes), support for Copilot Studio's MCP agent integations, and support for Azure deployment in secure environments with secrets stored in Azure Key Vault, as well as straightforward on-premises deployment.\n- **[Creatify](https://github.com/TSavo/creatify-mcp)** - MCP Server that exposes Creatify AI API capabilities for AI video generation, including avatar videos, URL-to-video conversion, text-to-speech, and AI-powered editing tools.\n- **[Cronlytic](https://github.com/Cronlytic/cronlytic-mcp-server)** - Create CRUD operations for serverless cron jobs through [Cronlytic](https://cronlytic.com) MCP Server\n- **[crypto-feargreed-mcp](https://github.com/kukapay/crypto-feargreed-mcp)**  -  Providing real-time and historical Crypto Fear & Greed Index data.\n- **[crypto-indicators-mcp](https://github.com/kukapay/crypto-indicators-mcp)**  -  An MCP server providing a range of cryptocurrency technical analysis indicators and strategies.\n- **[crypto-sentiment-mcp](https://github.com/kukapay/crypto-sentiment-mcp)**  -  An MCP server that delivers cryptocurrency sentiment analysis to AI agents.\n- **[cryptopanic-mcp-server](https://github.com/kukapay/cryptopanic-mcp-server)** - Providing latest cryptocurrency news to AI agents, powered by CryptoPanic.\n- **[CSV Editor](https://github.com/santoshray02/csv-editor)** - Comprehensive CSV processing with 40+ operations for data manipulation, analysis, and validation. Features auto-save, undo/redo, and handles GB+ files. Built with FastMCP & Pandas.\n- **[Current Time UTC MCP Server](https://github.com/jairampatel/currenttimeutc-mcp)** - A lightweight MCP server that provides accurate UTC time and timezone conversions in real-time.\n- **[Cursor MCP Installer](https://github.com/matthewdcage/cursor-mcp-installer)** - A tool to easily install and configure other MCP servers within Cursor IDE, with support for npm packages, local directories, and Git repositories.\n- **[CV Forge](https://github.com/thechandanbhagat/cv-forge)** - An intelligent MCP (Model Context Protocol) server that analyzes job postings and crafts perfectly-matched CVs (by [Chandan Bhagat](https://me.chandanbhagat.com.np)).\n- **[CVE Intelligence Server](https://github.com/gnlds/mcp-cve-intelligence-server-lite)** â€“ Provides vulnerability intelligence via multi - source CVE data, essential exploit discovery, and EPSS risk scoring through the MCP. Useful for security research, automation, and agent workflows.\n- **[D365FO](https://github.com/mafzaal/d365fo-client)** - A comprehensive MCP server for Microsoft Dynamics 365 Finance & Operations (D365 F&O) that provides easy access to OData endpoints, metadata operations, label management, and AI assistant integration.\n- **[Dagster](https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-dg-cli)** - An MCP server to easily build data pipelines using [Dagster](https://dagster.io/).\n- **[Dappier](https://github.com/DappierAI/dappier-mcp)** - Connect LLMs to real-time, rights-cleared, proprietary data from trusted sources. Access specialized models for Real-Time Web Search, News, Sports, Financial Data, Crypto, and premium publisher content. Explore data models at [marketplace.dappier.com](https://marketplace.dappier.com/marketplace).\n- **[Data Exploration](https://github.com/reading-plus-ai/mcp-server-data-exploration)** - MCP server for autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort. NOTE: Will execute arbitrary Python code on your machine, please use with caution!\n- **[Data4library](https://github.com/isnow890/data4library-mcp)** (by isnow890) - MCP server for Korea's Library Information Naru API, providing comprehensive access to public library data, book searches, loan status, reading statistics, and GPS-based nearby library discovery across South Korea.\n- **[Databricks](https://github.com/JordiNeil/mcp-databricks-server)** - Allows LLMs to run SQL queries, list and get details of jobs executions in a Databricks account.\n- **[Databricks Genie](https://github.com/yashshingvi/databricks-genie-MCP)** - A server that connects to the Databricks Genie, allowing LLMs to ask natural language questions, run SQL queries, and interact with Databricks conversational agents.\n- **[Databricks Smart SQL](https://github.com/RafaelCartenet/mcp-databricks-server)** - Leveraging Databricks Unity Catalog metadata, perform smart efficient SQL queries to solve Ad-hoc queries and explore data.\n- **[DataCite](https://github.com/QuentinCody/datacite-mcp-server)** - Unofficial MCP server for DataCite, providing access to research data and publication metadata through DataCite's REST API and GraphQL interface for scholarly research discovery.\n- **[Datadog](https://github.com/GeLi2001/datadog-mcp-server)** - Datadog MCP Server for application tracing, monitoring, dashboard, incidents queries built on official datadog api.\n- **[Dataset Viewer](https://github.com/privetin/dataset-viewer)** - Browse and analyze Hugging Face datasets with features like search, filtering, statistics, and data export\n- **[Dataverse DevTools MCP Server](https://github.com/vignaesh01/DataverseDevToolsMcpServer)** - An MCP server exposing ready-to-use Dataverse/Dynamics 365 tools for user and security administration, data operations, Web API executions, metadata exploration, and troubleshooting.\n- **[DataWorks](https://github.com/aliyun/alibabacloud-dataworks-mcp-server)** - A Model Context Protocol (MCP) server that provides tools for AI, allowing it to interact with the [DataWorks](https://www.alibabacloud.com/help/en/dataworks/) Open API through a standardized interface. This implementation is based on the Alibaba Cloud Open API and enables AI agents to perform cloud resources operations seamlessly.\n- **[DaVinci Resolve](https://github.com/samuelgursky/davinci-resolve-mcp)** - MCP server integration for DaVinci Resolve providing powerful tools for video editing, color grading, media management, and project control.\n- **[DBHub](https://github.com/bytebase/dbhub/)** - Universal database MCP server connecting to MySQL, MariaDB, PostgreSQL, and SQL Server.\n- **[Deebo](https://github.com/snagasuri/deebo-prototype)** â€“ Agentic debugging MCP server that helps AI coding agents delegate and fix hard bugs through isolated multi-agent hypothesis testing.\n- **[Deep Research](https://github.com/reading-plus-ai/mcp-server-deep-research)** - Lightweight MCP server offering Grok/OpenAI/Gemini/Perplexity-style automated deep research exploration and structured reporting.\n- **[DeepSeek MCP Server](https://github.com/DMontgomery40/deepseek-mcp-server)** - Model Context Protocol server integrating DeepSeek's advanced language models, in addition to [other useful API endpoints](https://github.com/DMontgomery40/deepseek-mcp-server?tab=readme-ov-file#features)\n- **[deepseek-thinker-mcp](https://github.com/ruixingshi/deepseek-thinker-mcp)** - A MCP (Model Context Protocol) provider Deepseek reasoning content to MCP-enabled AI Clients, like Claude Desktop. Supports access to Deepseek's thought processes from the Deepseek API service or from a local Ollama server.\n- **[Deepseek_R1](https://github.com/66julienmartin/MCP-server-Deepseek_R1)** - A Model Context Protocol (MCP) server implementation connecting Claude Desktop with DeepSeek's language models (R1/V3)\n- **[DeFi Rates](https://github.com/qingfeng/defi-rates-mcp)** - Query real-time DeFi lending rates across 13+ protocols (Aave, Morpho, Compound, Venus, Solend, Drift, Jupiter, etc.). Compare rates, search best opportunities, and calculate looping strategies across Ethereum, Arbitrum, Base, BSC, Solana, and HyperEVM.\n- **[Defuddle Fetch](https://github.com/domdomegg/defuddle-fetch-mcp-server)** - Fetch web content with enhanced extraction using Defuddle, converting pages to clean markdown with better results than standard HTML-to-markdown converters.\n- **[deploy-mcp](https://github.com/alexpota/deploy-mcp)** - Universal deployment tracker for AI assistants with live status badges and deployment monitoring.\n- **[Depyler](https://github.com/paiml/depyler/blob/main/docs/mcp-integration.md)** - Energy-efficient Python-to-Rust transpiler with progressive verification, enabling AI assistants to convert Python code to safe, performant Rust while reducing energy consumption by 75-85%.\n- **[Descope](https://github.com/descope-sample-apps/descope-mcp-server)** - An MCP server to integrate with [Descope](https://descope.com) to search audit logs, manage users, and more.\n- **[DesktopCommander](https://github.com/wonderwhy-er/DesktopCommanderMCP)** - Let AI edit and manage files on your computer, run terminal commands, and connect to remote servers via SSH - all powered by one of the most popular local MCP servers.\n- **[Devcontainer](https://github.com/AI-QL/mcp-devcontainers)** - An MCP server for devcontainer to generate and configure development containers directly from devcontainer configuration files.\n- **[DevDb](https://github.com/damms005/devdb-vscode?tab=readme-ov-file#mcp-configuration)** - An MCP server that runs right inside the IDE, for connecting to MySQL, Postgres, SQLite, and MSSQL databases.\n- **[DevOps AI Toolkit](https://github.com/vfarcic/dot-ai)** - AI-powered development productivity platform that enhances software development workflows through intelligent automation and AI-driven assistance.\n- **[DevOps-MCP](https://github.com/wangkanai/devops-mcp)** - Dynamic Azure DevOps MCP server with directory-based authentication switching, supporting work items, repositories, builds, pipelines, and multi-project management with local configuration files.\n- **[DGIdb](https://github.com/QuentinCody/dgidb-mcp-server)** - MCP server for the Drug Gene Interaction Database (DGIdb), providing access to drug-gene interaction data, druggable genome information, and pharmacogenomics research.\n- **[Dicom](https://github.com/ChristianHinge/dicom-mcp)** - An MCP server to query and retrieve medical images and for parsing and reading dicom-encapsulated documents (pdf etc.).\n- **[Dify](https://github.com/YanxingLiu/dify-mcp-server)** - A simple implementation of an MCP server for dify workflows.\n- **[Discogs](https://github.com/cswkim/discogs-mcp-server)** - An MCP server that connects to the Discogs API for interacting with your music collection.\n- **[Discord](https://github.com/v-3/discordmcp)** - An MCP server to connect to Discord guilds through a bot and read and write messages in channels\n- **[Discord](https://github.com/SaseQ/discord-mcp)** - An MCP server, which connects to Discord through a bot, and provides comprehensive integration with Discord.\n- **[Discord](https://github.com/Klavis-AI/klavis/tree/main/mcp_servers/discord)** - For Discord API integration by Klavis AI\n- **[Discourse](https://github.com/AshDevFr/discourse-mcp-server)** - An MCP server to search Discourse posts on a Discourse forum.\n- **[Dispatch Agent](https://github.com/abhinav-mangla/dispatch-agent)** - An intelligent MCP server that provides specialized filesystem operations through ReAct sub-agents.\n- **[DocBase](https://help.docbase.io/posts/3925317)** - Official MCP server for DocBase API integration, enabling post management, user collaboration, group administration, and more.\n- **[Docker](https://github.com/ckreiling/mcp-server-docker)** - Integrate with Docker to manage containers, images, volumes, and networks.\n- **[Docker](https://github.com/0xshariq/docker-mcp-server)** - Docker MCP Server provides advanced, unified Docker management via CLI and MCP workflows, supporting containers, images, volumes, networks, and orchestration.\n- **[Docs](https://github.com/da1z/docsmcp)** - Enable documentation access for the AI agent, supporting llms.txt and other remote or local files.\n- **[documcp](https://github.com/tosin2013/documcp)** - An MCP server for intelligent document processing and management, supporting multiple formats and document operations.\n- **[Docy](https://github.com/oborchers/mcp-server-docy)** - Docy gives your AI direct access to the technical documentation it needs, right when it needs it. No more outdated information, broken links, or rate limits - just accurate, real-time documentation access for more precise coding assistance.\n- **[Dodo Payments](https://github.com/dodopayments/dodopayments-node/tree/main/packages/mcp-server)** - Enables AI agents to securely perform payment operations via a lightweight, serverless-compatible interface to the [Dodo Payments](https://dodopayments.com) API.\n- **[Domain Tools](https://github.com/deshabhishek007/domain-tools-mcp-server)** - A Model Context Protocol (MCP) server for comprehensive domain analysis: WHOIS, DNS records, and DNS health checks.\n- **[Downdetector](https://github.com/domdomegg/downdetector-mcp)** - Check service status and outage information from Downdetector for real-time monitoring of service availability across various platforms and regions.\n- **[DPLP](https://github.com/szeider/mcp-dblp)**  - Searches the [DBLP](https://dblp.org) computer science bibliography database.\n- **[Druid MCP Server](https://github.com/iunera/druid-mcp-server)** - STDIO/SEE MCP Server for Apache Druid by [iunera](https://www.iunera.com) that provides extensive tools, resources, and prompts for managing and analyzing Druid clusters.\n- **[Drupal](https://github.com/Omedia/mcp-server-drupal)** - Server for interacting with [Drupal](https://www.drupal.org/project/mcp) using STDIO transport layer.\n- **[dune-analytics-mcp](https://github.com/kukapay/dune-analytics-mcp)** -  A mcp server that bridges Dune Analytics data to AI agents.\n- **[DynamoDB-Toolbox](https://www.dynamodbtoolbox.com/docs/databases/actions/mcp-toolkit)** - Leverages your Schemas and Access Patterns to interact with your [DynamoDB](https://aws.amazon.com/dynamodb) Database using natural language.\n- **[eBook-mcp](https://github.com/onebirdrocks/ebook-mcp)** - A lightweight MCP server that allows LLMs to read and interact with your personal PDF and EPUB ebooks. Ideal for building AI reading assistants or chat-based ebook interfaces.\n- **[ECharts MCP Server](https://github.com/hustcc/mcp-echarts)** - Generate visual charts using ECharts with AI MCP dynamically, used for chart generation and data analysis.\n- **[EDA MCP Server](https://github.com/NellyW8/mcp-EDA)** - A comprehensive Model Context Protocol server for Electronic Design Automation tools, enabling AI assistants to synthesize Verilog with Yosys, simulate designs with Icarus Verilog, run complete ASIC flows with OpenLane, and view results with GTKWave and KLayout.\n- **[EdgeOne Pages MCP](https://github.com/TencentEdgeOne/edgeone-pages-mcp)** - An MCP service for deploying HTML content to EdgeOne Pages and obtaining a publicly accessible URL.\n- **[Edwin](https://github.com/edwin-finance/edwin/tree/main/examples/mcp-server)** - MCP server for edwin SDK - enabling AI agents to interact with DeFi protocols across EVM, Solana and other blockchains.\n- **[eechat](https://github.com/Lucassssss/eechat)** - An open-source, cross-platform desktop application that seamlessly connects with MCP servers, across Linux, macOS, and Windows.\n- **[Elasticsearch](https://github.com/cr7258/elasticsearch-mcp-server)** - MCP server implementation that provides Elasticsearch interaction.\n- **[ElevenLabs](https://github.com/mamertofabian/elevenlabs-mcp-server)** - A server that integrates with ElevenLabs text-to-speech API capable of generating full voiceovers with multiple voices.\n- **[Email](https://github.com/Shy2593666979/mcp-server-email)** - This server enables users to send emails through various email providers, including Gmail, Outlook, Yahoo, Sina, Sohu, 126, 163, and QQ Mail. It also supports attaching files from specified directories, making it easy to upload attachments along with the email content.\n- **[Email SMTP](https://github.com/egyptianego17/email-mcp-server)** - A simple MCP server that lets your AI agent send emails and attach files through SMTP.\n- **[Enhance Prompt](https://github.com/FelixFoster/mcp-enhance-prompt)** - An MCP service for enhance you prompt.\n- **[Entrez](https://github.com/QuentinCody/entrez-mcp-server)** - Unofficial MCP server for NCBI Entrez databases, providing access to PubMed articles, gene information, protein data, and other biomedical research resources through NCBI's E-utilities API.\n- **[Ergo Blockchain MCP](https://github.com/marctheshark3/ergo-mcp)** -An MCP server to integrate Ergo Blockchain Node and Explorer APIs for checking address balances, analyzing transactions, viewing transaction history, performing forensic analysis of addresses, searching for tokens, and monitoring network status.\n- **[ESP MCP Server](https://github.com/horw/esp-mcp)** - An MCP server that integrates ESP IDF commands like building and flashing code for ESP Microcontrollers using an LLM.\n- **[Eunomia](https://github.com/whataboutyou-ai/eunomia-MCP-server)** - Extension of the Eunomia framework that connects Eunomia instruments with MCP servers\n- **[Everything Search](https://github.com/mamertofabian/mcp-everything-search)** - Fast file searching capabilities across Windows (using [Everything SDK](https://www.voidtools.com/support/everything/sdk/)), macOS (using mdfind command), and Linux (using locate/plocate command).\n- **[EVM MCP Server](https://github.com/mcpdotdirect/evm-mcp-server)** - Comprehensive blockchain services for 30+ EVM networks, supporting native tokens, ERC20, NFTs, smart contracts, transactions, and ENS resolution.\n- **[Excel](https://github.com/haris-musa/excel-mcp-server)** - Excel manipulation including data reading/writing, worksheet management, formatting, charts, and pivot table.\n- **[Excel to JSON MCP by WTSolutions](https://github.com/he-yang/excel-to-json-mcp)** - MCP Server providing a standardized interface for converting (1) Excel or CSV data into JSON format ;(2) Excel(.xlsx) file into Structured JSON.\n- **[Extended Memory](https://github.com/ssmirnovpro/extended-memory-mcp)** - Persistent memory across Claude conversations with multi-project support, automatic importance scoring, and tag-based organization. Production-ready with 400+ tests.\n- **[F1](https://github.com/AbhiJ2706/f1-mcp/tree/main)** - Access to Formula 1 data including race results, driver information, lap times, telemetry, and circuit details.\n- **[Fabi](https://docs.fabi.ai/advanced_features_and_dev_tools/mcp_server)** - MCP server that exposes [Fabi](https://app.fabi.ai/) analyst agent to turn natural-language prompts into insights: navigating connected data, generating safe SQL/Python, running queries, and saving results into dashboards.\n- **[Fabric MCP](https://github.com/aci-labs/ms-fabric-mcp)** - Microsoft Fabric MCP server to accelerate working in your Fabric Tenant with the help of your favorite LLM models.\n- **[Fabric Real-Time Intelligence MCP](https://github.com/Microsoft/fabric-rti-mcp)** - Official Microsoft Fabric RTI server to accelerate working with Eventhouse, Azure Data Explorer(Kusto), Eventstreams and other RTI items using your favorite LLM models.\n- **[fabric-mcp-server](https://github.com/adapoet/fabric-mcp-server)** - The fabric-mcp-server is an MCP server that integrates [Fabric](https://github.com/danielmiessler/fabric) patterns with [Cline](https://cline.bot/), exposing them as tools for AI-driven task execution and enhancing Cline's capabilities.\n- **[Facebook Ads](https://github.com/gomarble-ai/facebook-ads-mcp-server)** - MCP server acting as an interface to the Facebook Ads, enabling programmatic access to Facebook Ads data and management features.\n- **[Facebook Ads 10xeR](https://github.com/fortytwode/10xer)** - Advanced Facebook Ads MCP server with enhanced creative insights, multi-dimensional breakdowns, and comprehensive ad performance analytics.\n- **[Facebook Ads Library](https://github.com/trypeggy/facebook-ads-library-mcp)** - Get any answer from the Facebook Ads Library, conduct deep research including messaging, creative testing and comparisons in seconds.\n- **[Fal MCP Server](https://github.com/raveenb/fal-mcp-server)** - Generate AI images, videos, and music using Fal.ai models (FLUX, Stable Diffusion, MusicGen) directly in Claude\n- **[Fantasy PL](https://github.com/rishijatia/fantasy-pl-mcp)** - Give your coding agent direct access to up-to date Fantasy Premier League data\n- **[Fast Filesystem](https://github.com/efforthye/fast-filesystem-mcp)** - Advanced filesystem operations with large file handling capabilities and Claude-optimized features. Provides fast file reading/writing, sequential reading for large files, directory operations, file search, and streaming writes with backup & recovery.\n- **[Fastmail MCP](https://github.com/MadLlama25/fastmail-mcp)** - Access Fastmail via JMAP: list/search emails, send and move mail, handle attachments/threads, plus contacts and calendar tools.\n- **[fastn.ai â€“ Unified API MCP Server](https://github.com/fastnai/mcp-fastn)** - A remote, dynamic MCP server with a unified API that connects to 1,000+ tools, actions, and workflows, featuring built-in authentication and monitoring.\n- **[FDIC BankFind MCP Server - (Unofficial)](https://github.com/clafollett/fdic-bank-find-mcp-server)** - The is a MCPserver that brings the power of FDIC BankFind APIs straight to your AI tools and workflows. Structured U.S. banking data, delivered with maximum vibes. ğŸ˜ğŸ“Š\n- **[Federal Reserve Economic Data (FRED)](https://github.com/stefanoamorelli/fred-mcp-server)** (by Stefano Amorelli) - Community developed MCP server to interact with the Federal Reserve Economic Data.\n- **[Fetch](https://github.com/zcaceres/fetch-mcp)** - A server that flexibly fetches HTML, JSON, Markdown, or plaintext.\n- **[Feyod](https://github.com/jeroenvdmeer/feyod-mcp)** - A server that answers questions about football matches, and specialised in the football club Feyenoord.\n- **[FHIR](https://github.com/wso2/fhir-mcp-server)** - A Model Context Protocol server that provides seamless, standardized access to Fast Healthcare Interoperability Resources (FHIR) data from any compatible FHIR server. Designed for easy integration with AI tools, developer workflows, and healthcare applications, it enables natural language and programmatic search, retrieval, and analysis of clinical data.\n- **[Fibaro HC3](https://github.com/coding-sailor/mcp-server-hc3)** - MCP server for Fibaro Home Center 3 smart home systems.\n- **[Figma](https://github.com/GLips/Figma-Context-MCP)** - Give your coding agent direct access to Figma file data, helping it one-shot design implementation.\n- **[Figma](https://github.com/paulvandermeijs/figma-mcp)** - A blazingly fast MCP server to read and export your Figma design files.\n- **[Figma to Flutter](https://github.com/mhmzdev/figma-flutter-mcp)** - Write down clean and better Flutter code from Figma design tokens and enrich nodes data in Flutter terminology.\n- **[Files](https://github.com/flesler/mcp-files)** - Enables agents to quickly find and edit code in a codebase with surgical precision. Find symbols, edit them everywhere.\n- **[FileSystem Server](https://github.com/Oncorporation/filesystem_server)** - Local MCP server for Visual Studio 2022 that provides code-workspace functionality by giving AI agents selective access to project folders and files\n- **[finmap.org](https://github.com/finmap-org/mcp-server)** MCP server provides comprehensive historical data from the US, UK, Russian and Turkish stock exchanges. Access sectors, tickers, company profiles, market cap, volume, value, and trade counts, as well as treemap and histogram visualizations.\n- **[Firebase](https://github.com/gannonh/firebase-mcp)** - Server to interact with Firebase services including Firebase Authentication, Firestore, and Firebase Storage.\n- **[Fish Audio](https://github.com/da-okazaki/mcp-fish-audio-server)** - Text-to-Speech integration with Fish Audio's API, supporting multiple voices, streaming, and real-time playback\n- **[FitBit MCP Server](https://github.com/NitayRabi/fitbit-mcp)** - An MCP server that connects to FitBit API using a token obtained from OAuth flow.\n- **[Fleet](https://github.com/SimplyMinimal/fleet-mcp)** - Full Fleet integration for device management, security monitoring, and compliance enforcement. Supports host management, live query execution, policy management, software inventory, vulnerability tracking, and MDM operations. Supports Read-Only and Read-Write modes.\n- **[FlightRadar24](https://github.com/sunsetcoder/flightradar24-mcp-server)** - A Claude Desktop MCP server that helps you track flights in real-time using Flightradar24 data.\n- **[Fluent-MCP](https://github.com/modesty/fluent-mcp)** - MCP server for Fluent (ServiceNow SDK) providing access to ServiceNow SDK CLI, API specifications, code snippets, and more.\n- **[Flyworks Avatar](https://github.com/Flyworks-AI/flyworks-mcp)** - Fast and free zeroshot lipsync MCP server.\n- **[fmp-mcp-server](https://github.com/vipbat/fmp-mcp-server)** - Enable your agent for M&A analysis and investment banking workflows. Access company profiles, financial statements, ratios, and perform sector analysis with the [Financial Modeling Prep APIs]\n- **[FoundationModels](https://github.com/phimage/mcp-foundation-models)** - An MCP server that integrates Apple's [FoundationModels](https://developer.apple.com/documentation/foundationmodels) for text generation.\n- **[Foursquare](https://github.com/foursquare/foursquare-places-mcp)** - Enable your agent to recommend places around the world with the [Foursquare Places API](https://location.foursquare.com/products/places-api/)\n- **[FPE Demo MCP](https://github.com/Horizon-Digital-Engineering/fpe-demo-mcp)** - FF3 Format Preserving Encryption with authentication patterns for secure data protection in LLM workflows.\n- **[FrankfurterMCP](https://github.com/anirbanbasu/frankfurtermcp)** - MCP server acting as an interface to the [Frankfurter API](https://frankfurter.dev/) for currency exchange data.\n- **[freqtrade-mcp](https://github.com/kukapay/freqtrade-mcp)** - An MCP server that integrates with the Freqtrade cryptocurrency trading bot.\n- **[GDAL](https://github.com/Wayfinder-Foundry/gdal-mcp)** - GDAL-style geospatial workflows with built-in reasoning guidance and reference resources to give AI agents catalogue discovery, metadata intelligence, and raster/vector processing.\n- **[GDB](https://github.com/pansila/mcp_server_gdb)** - A GDB/MI protocol server based on the MCP protocol, providing remote application debugging capabilities with AI assistants.\n- **[Gemini Bridge](https://github.com/eLyiN/gemini-bridge)** - Lightweight MCP server that enables Claude to interact with Google's Gemini AI through the official CLI, offering zero API costs and stateless architecture.\n- **[Geolocation](https://github.com/jackyang25/geolocation-mcp-server)** - WalkScore API integration for walkability, transit, and bike scores.\n- **[ggRMCP](https://github.com/aalobaidi/ggRMCP)** - A Go gateway that converts gRPC services into MCP-compatible tools, allowing AI models like Claude to directly call your gRPC services.\n- **[Ghost](https://github.com/MFYDev/ghost-mcp)** - A Model Context Protocol (MCP) server for interacting with Ghost CMS through LLM interfaces like Claude.\n- **[Git](https://github.com/geropl/git-mcp-go)** - Allows LLM to interact with a local git repository, incl. optional push support.\n- **[Git Mob](https://github.com/Mubashwer/git-mob-mcp-server)** - MCP server that interfaces with the [git-mob](https://github.com/Mubashwer/git-mob) CLI app for managing co-authors in git commits during pair/mob programming.\n- **[Github](https://github.com/0xshariq/github-mcp-server)** - A Model Context Protocol (MCP) server that provides 29 Git operations + 11 workflow combinations for AI assistants and developers. This server exposes comprehensive Git repository management through a standardized interface, enabling AI models and developers to safely manage complex version control workflows.\n- **[GitHub Actions](https://github.com/ko1ynnky/github-actions-mcp-server)** - A Model Context Protocol (MCP) server for interacting with GitHub Actions.\n- **[GitHub Enterprise MCP](https://github.com/ddukbg/github-enterprise-mcp)** - A Model Context Protocol (MCP) server for interacting with GitHub Enterprise.\n- **[GitHub GraphQL](https://github.com/QuentinCody/github-graphql-mcp-server)** - Unofficial GitHub MCP server that provides access to GitHub's GraphQL API, enabling more powerful and flexible queries for repository data, issues, pull requests, and other GitHub resources.\n- **[GitHub Projects](https://github.com/redducklabs/github-projects-mcp)** â€” Manage GitHub Projects with full GraphQL API access including items, fields, and milestones.\n- **[GitHub Repos Manager MCP Server](https://github.com/kurdin/github-repos-manager-mcp)** - Token-based GitHub automation management. No Docker, Flexible configuration, 80+ tools with direct API integration.\n- **[GitMCP](https://github.com/idosal/git-mcp)** - gitmcp.io is a generic remote MCP server to connect to ANY GitHub repository or project documentation effortlessly\n- **[Glean](https://github.com/longyi1207/glean-mcp-server)** - A server that uses Glean API to search and chat.\n- **[Gmail](https://github.com/GongRzhe/Gmail-MCP-Server)** - A Model Context Protocol (MCP) server for Gmail integration in Claude Desktop with auto authentication support.\n- **[Gmail](https://github.com/Ayush-k-Shukla/gmail-mcp-server)** - A Simple MCP server for Gmail with support for all basic operations with oauth2.0.\n- **[Gmail Headless](https://github.com/baryhuang/mcp-headless-gmail)** - Remote hostable MCP server that can get and send Gmail messages without local credential or file system setup.\n- **[Gmail MCP](https://github.com/gangradeamitesh/mcp-google-email)** - A Gmail service implementation using MCP (Model Context Protocol) that provides functionality for sending, receiving, and managing emails through Gmail's API.\n- **[Gnuradio](https://github.com/yoelbassin/gnuradioMCP)** - An MCP server for GNU Radio that enables LLMs to autonomously create and modify RF .grc flowcharts.\n- **[Goal Story](https://github.com/hichana/goalstory-mcp)** - a Goal Tracker and Visualization Tool for personal and professional development.\n- **[GOAT](https://github.com/goat-sdk/goat/tree/main/typescript/examples/by-framework/model-context-protocol)** - Run more than +200 onchain actions on any blockchain including Ethereum, Solana and Base.\n- **[Godot](https://github.com/Coding-Solo/godot-mcp)** - An MCP server providing comprehensive Godot engine integration for project editing, debugging, and scene management.\n- **[Golang Filesystem Server](https://github.com/mark3labs/mcp-filesystem-server)** - Secure file operations with configurable access controls built with Go!\n- **[Goodnews](https://github.com/VectorInstitute/mcp-goodnews)** - A simple MCP server that delivers curated positive and uplifting news stories.\n- **[Google Ads](https://github.com/gomarble-ai/google-ads-mcp-server)** - MCP server acting as an interface to the Google Ads, enabling programmatic access to Facebook Ads data and management features.\n- **[Google Analytics](https://github.com/surendranb/google-analytics-mcp)** - Google Analytics MCP Server to bring data across 200+ dimensions & metrics for LLMs to analyse.\n- **[Google Analytics 4](https://github.com/gomakers-ai/mcp-google-analytics)** - MCP server for Google Analytics Data API and Measurement Protocol to read reports and send events.\n- **[Google Calendar](https://github.com/v-3/google-calendar)** - Integration with Google Calendar to check schedules, find time, and add/delete events\n- **[Google Calendar](https://github.com/nspady/google-calendar-mcp)** - Google Calendar MCP Server for managing Google calendar events. Also supports searching for events by attributes like title and location.\n- **[Google Custom Search](https://github.com/adenot/mcp-google-search)** - Provides Google Search results via the Google Custom Search API\n- **[Google Maps](https://github.com/Mastan1301/google_maps_mcp)** - Provides location results using Google Places API.\n- **[Google Sheets](https://github.com/xing5/mcp-google-sheets)** - Access and editing data to your Google Sheets.\n- **[Google Sheets](https://github.com/rohans2/mcp-google-sheets)** - An MCP Server written in TypeScript to access and edit data in your Google Sheets.\n- **[Google Tasks](https://github.com/zcaceres/gtasks-mcp)** - Google Tasks API Model Context Protocol Server.\n- **[Google Vertex AI Search](https://github.com/ubie-oss/mcp-vertexai-search)** - Provides Google Vertex AI Search results by grounding a Gemini model with your own private data\n- **[Google Workspace](https://github.com/taylorwilsdon/google_workspace_mcp)** - Comprehensive Google Workspace MCP with full support for Calendar, Drive, Gmail, and Docs using Streamable HTTP or SSE transport.\n- **[Google-Scholar](https://github.com/JackKuo666/Google-Scholar-MCP-Server)** - Enable AI assistants to search and access Google Scholar papers through a simple MCP interface.\n- **[Google-Scholar](https://github.com/mochow13/google-scholar-mcp)** - An MCP server for Google Scholar written in TypeScript with Streamable HTTP transport, along with a `client` implementations that integrates with the server and interacts with `gemini-2.5-flash`.\n- **[Gopher MCP](https://github.com/cameronrye/gopher-mcp)** - Modern, cross-platform MCP server that enables AI assistants to browse and interact with both Gopher protocol and Gemini protocol resources safely and efficiently.\n- **[Gralio SaaS Database](https://github.com/tymonTe/gralio-mcp)** - Find and compare SaaS products, including data from G2 reviews, Trustpilot, Crunchbase, Linkedin, pricing, features and more, using [Gralio MCP](https://gralio.ai/mcp) server\n- **[GraphQL](https://github.com/drestrepom/mcp_graphql)** - Comprehensive GraphQL API integration that automatically exposes each GraphQL query as a separate tool.\n- **[GraphQL Schema](https://github.com/hannesj/mcp-graphql-schema)** - Allow LLMs to explore large GraphQL schemas without bloating the context.\n- **[Graylog](https://github.com/Pranavj17/mcp-server-graylog)** - Search Graylog logs by absolute/relative timestamps, filter by streams, and debug production issues directly from Claude Desktop.\n- **[Grok-MCP](https://github.com/merterbak/Grok-MCP)** - MCP server for xAIâ€™s API featuring the latest Grok models, image analysis & generation, and web search.\n- **[gx-mcp-server](https://github.com/davidf9999/gx-mcp-server)** - Expose Great Expectations data validation and quality checks as MCP tools for AI agents.\n- **[HackMD](https://github.com/yuna0x0/hackmd-mcp)** (by yuna0x0) - An MCP server for HackMD, a collaborative markdown editor. It allows users to create, read, and update documents in HackMD using the Model Context Protocol.\n- **[HAProxy](https://github.com/tuannvm/haproxy-mcp-server)** - A Model Context Protocol (MCP) server for HAProxy implemented in Go, leveraging HAProxy Runtime API.\n- **[Hashing MCP Server](https://github.com/kanad13/MCP-Server-for-Hashing)** - MCP Server with cryptographic hashing functions e.g. SHA256, MD5, etc.\n- **[HDW LinkedIn](https://github.com/horizondatawave/hdw-mcp-server)** - Access to profile data and management of user account with [HorizonDataWave.ai](https://horizondatawave.ai/).\n- **[HeatPump](https://github.com/jiweiqi/heatpump-mcp-server)** â€” Residential heat - pump sizing & cost-estimation tools by **HeatPumpHQ**.\n- **[Helm Chart CLI](https://github.com/jeff-nasseri/helm-chart-cli-mcp)** - Helm MCP provides a bridge between AI assistants and the Helm package manager for Kubernetes. It allows AI assistants to interact with Helm through natural language requests, executing commands like installing charts, managing repositories, and more.\n- **[Heurist Mesh Agent](https://github.com/heurist-network/heurist-mesh-mcp-server)** - Access specialized web3 AI agents for blockchain analysis, smart contract security, token metrics, and blockchain interactions through the [Heurist Mesh network](https://github.com/heurist-network/heurist-agent-framework/tree/main/mesh).\n- **[HLedger MCP](https://github.com/iiAtlas/hledger-mcp)** - Double entry plain text accounting, right in your LLM! This MCP enables comprehensive read, and (optional) write access to your local [HLedger](https://hledger.org/) accounting journals.\n- **[Holaspirit](https://github.com/syucream/holaspirit-mcp-server)** - Interact with [Holaspirit](https://www.holaspirit.com/).\n- **[Home Assistant](https://github.com/tevonsb/homeassistant-mcp)** - Interact with [Home Assistant](https://www.home-assistant.io/) including viewing and controlling lights, switches, sensors, and all other Home Assistant entities.\n- **[Home Assistant](https://github.com/voska/hass-mcp)** - Docker-ready MCP server for Home Assistant with entity management, domain summaries, automation support, and guided conversations. Includes pre-built container images for easy installation.\n- **[HTML to Markdown](https://github.com/levz0r/html-to-markdown-mcp)** - Fetch web pages and convert HTML to clean, formatted Markdown. Handles large pages with automatic file saving to bypass token limits.\n- **[html2md-mcp](https://github.com/sunshad0w/html2md-mcp)** - MCP server for converting HTML to Markdown with browser support and authentication. Reduces HTML size by 90-95% using trafilatura and BeautifulSoup4, with Playwright integration for JavaScript-rendered content.\n- **[HubSpot](https://github.com/buryhuang/mcp-hubspot)** - HubSpot CRM integration for managing contacts and companies. Create and retrieve CRM data directly through Claude chat.\n- **[HuggingFace Spaces](https://github.com/evalstate/mcp-hfspace)** - Server for using HuggingFace Spaces, supporting Open Source Image, Audio, Text Models and more. Claude Desktop mode for easy integration.\n- **[Human-In-the-Loop](https://github.com/GongRzhe/Human-In-the-Loop-MCP-Server)** - A powerful MCP Server that enables AI assistants like Claude to interact with humans through intuitive GUI dialogs. This server bridges the gap between automated AI processes and human decision-making by providing real-time user input tools, choices, confirmations, and feedback mechanisms.\n- **[Human-use](https://github.com/RapidataAI/human-use)** - Instant human feedback through an MCP, have your AI interact with humans around the world. Powered by [Rapidata](https://www.rapidata.ai/)\n- **[Hyperledger Fabric Agent Suite](https://github.com/padmarajkore/hlf-fabric-agent)** - Modular toolkit for managing Fabric test networks and chaincode lifecycle via MCP tools.\n- **[Hyperliquid](https://github.com/mektigboy/server-hyperliquid)** - An MCP server implementation that integrates the Hyperliquid SDK for exchange data.\n- **[Hypertool](https://github.com/toolprint/hypertool-mcp)** â€“ MCP that let's you create hot - swappable, \"persona toolsets\" from multiple MCP servers to reduce tool overload and improve tool execution.\n- **[hyprmcp](https://github.com/stefanoamorelli/hyprmcp)** (by Stefano Amorelli) - Lightweight MCP server for `hyprland`.\n- **[iFlytek SparkAgent Platform](https://github.com/iflytek/ifly-spark-agent-mcp)** - This is a simple example of using MCP Server to invoke the task chain of the  iFlytek SparkAgent Platform.\n- **[iFlytek Workflow](https://github.com/iflytek/ifly-workflow-mcp-server)** - Connect to iFlytek Workflow via the MCP server and run your own Agent.\n- **[IIIF](https://github.com/code4history/IIIF_MCP)** - Comprehensive IIIF (International Image Interoperability Framework) protocol support for searching, navigating, and manipulating digital collections from museums, libraries, and archives worldwide.\n- **[Image Generation](https://github.com/GongRzhe/Image-Generation-MCP-Server)** - This MCP server provides image generation capabilities using the Replicate Flux model.\n- **[ImageSorcery MCP](https://github.com/sunriseapps/imagesorcery-mcp)** - ComputerVision-based ğŸª„ sorcery of image recognition and editing tools for AI assistants.\n- **[IMAP MCP](https://github.com/dominik1001/imap-mcp)** - ğŸ“§ An IMAP Model Context Protocol (MCP) server to expose IMAP operations as tools for AI assistants.\n- **[iMCP](https://github.com/loopwork-ai/iMCP)** - A macOS app that provides an MCP server for your iMessage, Reminders, and other Apple services.\n- **[InfluxDB](https://github.com/idoru/influxdb-mcp-server)** - Run queries against InfluxDB OSS API v2.\n- **[Inner Monologue MCP](https://github.com/abhinav-mangla/inner-monologue-mcp)** - A cognitive reasoning tool that enables LLMs to engage in private, structured self-reflection and multi-step reasoning before generating responses, improving response quality and problem-solving capabilities.\n- **[Inoyu](https://github.com/sergehuber/inoyu-mcp-unomi-server)** - Interact with an Apache Unomi CDP customer data platform to retrieve and update customer profiles\n- **[Instagram DM](https://github.com/trypeggy/instagram_dm_mcp)** - Send DMs on Instagram via your LLM\n- **[Intelligent Image Generator](https://github.com/shinpr/mcp-image)** - Turn casual prompts into professional-quality images with AI enhancement\n- **[interactive-mcp](https://github.com/ttommyth/interactive-mcp)** - Enables interactive LLM workflows by adding local user prompts and chat capabilities directly into the MCP loop.\n- **[Intercom](https://github.com/raoulbia-ai/mcp-server-for-intercom)** - An MCP-compliant server for retrieving customer support tickets from Intercom. This tool enables AI assistants like Claude Desktop and Cline to access and analyze your Intercom support tickets.\n- **[iOS Simulator](https://github.com/InditexTech/mcp-server-simulator-ios-idb)** - A Model Context Protocol (MCP) server that enables LLMs to interact with iOS simulators (iPhone, iPad, etc.) through natural language commands.\n- **[ipybox](https://github.com/gradion-ai/ipybox)** - Python code execution sandbox based on IPython and Docker. Stateful code execution, file transfer between host and container, configurable network access. See [ipybox MCP server](https://gradion-ai.github.io/ipybox/mcp-server/) for details.\n- **[it-tools-mcp](https://github.com/wrenchpilot/it-tools-mcp)** - A Model Context Protocol server that recreates [CorentinTh it-tools](https://github.com/CorentinTh/it-tools) utilities for AI agents, enabling access to a wide range of developer tools (encoding, decoding, conversions, and more) via MCP.\n- **[itemit MCP](https://github.com/umin-ai/itemit-mcp)** - itemit is Asset Tracking MCP that manage the inventory, monitoring and location tracking that powers over +300 organizations.\n- **[iTerm MCP](https://github.com/ferrislucas/iterm-mcp)** - Integration with iTerm2 terminal emulator for macOS, enabling LLMs to execute and monitor terminal commands.\n- **[iTerm MCP Server](https://github.com/rishabkoul/iTerm-MCP-Server)** - A Model Context Protocol (MCP) server implementation for iTerm2 terminal integration. Able to manage multiple iTerm Sessions.\n- **[Java Decompiler](https://github.com/idachev/mcp-javadc)** - Decompile Java bytecode into readable source code from .class files, package names, or JAR archives using CFR decompiler\n- **[JavaFX](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jfx)** - Make drawings using a JavaFX canvas\n- **[JDBC](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc)** - Connect to any JDBC-compatible database and query, insert, update, delete, and more. Supports MySQL, PostgreSQL, Oracle, SQL Server, SQLite and [more](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc#supported-jdbc-variants).\n- **[Jenkins](https://github.com/jasonkylelol/jenkins-mcp-server)** - This MCP server allow you to create Jenkins tasks.\n- **[JMeter](https://github.com/QAInsights/jmeter-mcp-server)** - Run load testing using Apache JMeter via MCP-compliant tools.\n- **[Job Searcher](https://github.com/0xDAEF0F/job-searchoor)** - A FastMCP server that provides tools for retrieving and filtering job listings based on time period, keywords, and remote work preferences.\n- **[jobswithgpt](https://github.com/jobswithgpt/mcp)** - Job search MCP using jobswithgpt which indexes 500K+ public job listings and refreshed continously.\n- **[joinly](https://github.com/joinly-ai/joinly)** - MCP server to interact with browser-based meeting platforms (Zoom, Teams, Google Meet). Enables AI agents to send bots to online meetings, gather live transcripts, speak text, and send messages in the meeting chat.\n- **[JSON](https://github.com/GongRzhe/JSON-MCP-Server)** - JSON handling and processing server with advanced query capabilities using JSONPath syntax and support for array, string, numeric, and date operations.\n- **[JSON](https://github.com/kehvinbehvin/json-mcp-filter)** - JSON schema generation and filtering server with TypeScript type creation optimised for retrieving relevant context JSON data using quicktype-core and support for shape-based data extraction, nested object filtering, and array processing operations.\n- **[JSON to Excel by WTSolutions](https://github.com/he-yang/json-to-excel-mcp)** - Converting JSON into CSV format string from (1) JSON data, (2) URLs pointing to publiclly available .json files.\n- **[JSON2Video MCP](https://github.com/omergocmen/json2video-mcp-server)** - A Model Context Protocol (MCP) server implementation for programmatically generating videos using the json2video API. This server exposes powerful video generation and status-checking tools for use with LLMs, agents, or any MCP-compatible client.\n- **[jupiter-mcp](https://github.com/kukapay/jupiter-mcp)** - An MCP server for executing token swaps on the Solana blockchain using Jupiter's new Ultra API.\n- **[Jupyter MCP Server](https://github.com/datalayer/jupyter-mcp-server)** â€“ Real-time interaction with Jupyter Notebooks, allowing AI to edit, document and execute code for data analysis, visualization etc. Compatible with any Jupyter deployment (local, JupyterHub, ...).\n- **[Jupyter Notebook](https://github.com/jjsantos01/jupyter-notebook-mcp)** - connects Jupyter Notebook to Claude AI, allowing Claude to directly interact with and control Jupyter Notebooks. This integration enables AI-assisted code execution, data analysis, visualization, and more.\n- **[k8s-multicluster-mcp](https://github.com/razvanmacovei/k8s-multicluster-mcp)** - An MCP server for interact with multiple Kubernetes clusters simultaneously using multiple kubeconfig files.\n- **[Kafka](https://github.com/tuannvm/kafka-mcp-server)** - A Model Context Protocol (MCP) server for Apache Kafka implemented in Go, leveraging [franz-go](https://github.com/twmb/franz-go).\n- **[Kafka Schema Registry MCP](https://github.com/aywengo/kafka-schema-reg-mcp)** \\ - A comprehensive MCP server for Kafka Schema Registry with 48 tools, multi-registry support, authentication, and production safety features. Enables AI-powered schema management with enterprise-grade capabilities including schema contexts, migration tools, and comprehensive export capabilities.\n- **[kafka-mcp](https://github.com/shivamxtech/kafka-mcp)** - An MCP Server for Kafka clusters to interact with kafka environment via tools on messages, topics, offsets, partitions for consumer and producers along with seamless integration with MCP clients.\n- **[Kaggle-mcp](https://github.com/Seif-Sameh/Kaggle-mcp.git)** - An MCP server that provides seamless integration with the Kaggle API. Interact with Kaggle competitions, datasets, kernels, and models through MCP-compatible clients like Claude Desktop.\n- **[Keycloak](https://github.com/idoyudha/mcp-keycloak)** - The Keycloak MCP Server designed for agentic applications to manage and search data in Keycloak efficiently.\n- **[Keycloak MCP](https://github.com/ChristophEnglisch/keycloak-model-context-protocol)** - This MCP server enables natural language interaction with Keycloak for user and realm management including creating, deleting, and listing users and realms.\n- **[Keycloak MCP Server](https://github.com/sshaaf/keycloak-mcp-server)** - designed to work with Keycloak for identity and access management, with about 40+ tools covering, Users, Realms, Clients, Roles, Groups, IDPs, Authentication. Native builds available.\n- **[Kibana MCP](https://github.com/TocharianOU/mcp-server-kibana.git)** (by TocharianOU) - A community-maintained MCP server implementation that allows any MCP-compatible client to access and manage Kibana instances through natural language or programmatic requests.\n- **[Kibela](https://github.com/kiwamizamurai/mcp-kibela-server)** (by kiwamizamurai) - Interact with Kibela API.\n- **[KiCad MCP](https://github.com/lamaalrajih/kicad-mcp)** - MCP server for KiCad on Mac, Windows, and Linux.\n- **[kill-process-mcp](https://github.com/misiektoja/kill-process-mcp)** - List and terminate OS processes via natural language queries\n- **[Kindred Offers & Discounts MCP](https://github.com/kindred-app/mcp-server-kindred-offers)** (by kindred.co) - This MCP server allows you to get live deals and offers/coupons from e-commerce merchant sites all over the world.\n- **[kintone](https://github.com/macrat/mcp-server-kintone)** - Manage records and apps in [kintone](https://kintone.com) through LLM tools.\n- **[KnowAir Weather MCP](https://github.com/shuowang-ai/Weather-MCP)** - A comprehensive Model Context Protocol (MCP) server providing real-time weather data, air quality monitoring, forecasts, and astronomical information powered by Caiyun Weather API.\n- **[Kokoro TTS](https://github.com/mberg/kokoro-tts-mcp)** - Use Kokoro text to speech to convert text to MP3s with optional autoupload to S3.\n- **[Kong Konnect](https://github.com/Kong/mcp-konnect)** - A Model Context Protocol (MCP) server for interacting with Kong Konnect APIs, allowing AI assistants to query and analyze Kong Gateway configurations, traffic, and analytics.\n- **[Korea Stock Analyzer](https://github.com/Mrbaeksang/korea-stock-analyzer-mcp)** - Analyze Korean stocks (KOSPI/KOSDAQ) with 6 legendary investment strategies including Buffett, Lynch, Graham, Greenblatt, Fisher, and Templeton.\n- **[KRS Poland](https://github.com/pkolawa/krs-poland-mcp-server)** - Access to Polish National Court Register (KRS)â€”the government's authoritative registry of all businesses, foundations, and other legal entities.\n- **[Kubeflow Spark History MCP Server](https://github.com/kubeflow/mcp-apache-spark-history-server)** - Enable AI agents to analyze Spark job performance, identify bottlenecks, and provide intelligent insights.\n- **[Kubernetes](https://github.com/Flux159/mcp-server-kubernetes)** - Connect to Kubernetes cluster and manage pods, deployments, and services.\n- **[Kubernetes and OpenShift](https://github.com/manusa/kubernetes-mcp-server)** - A powerful Kubernetes MCP server with additional support for OpenShift. Besides providing CRUD operations for any Kubernetes resource, this server provides specialized tools to interact with your cluster.\n- **[KubeSphere](https://github.com/kubesphere/ks-mcp-server)** - The KubeSphere MCP Server is a Model Context Protocol(MCP) server that provides integration with KubeSphere APIs, enabling to get resources from KubeSphere. Divided into four tools modules: Workspace Management, Cluster Management, User and Roles, Extensions Center.\n- **[Kukapay MCP Servers](https://github.com/kukapay/kukapay-mcp-servers)** - A comprehensive suite of Model Context Protocol (MCP) servers dedicated to cryptocurrency, blockchain, and Web3 data aggregation, analysis, and services from Kukapay.\n- **[kwrds.ai](https://github.com/mkotsollaris/kwrds_ai_mcp)** - Keyword research, people also ask, SERP and other SEO tools for [kwrds.ai](https://www.kwrds.ai/)\n- **[KYC-mcp-server](https://github.com/vishnurudra-ai/KYC-mcp-server)** - Know Your Computer (KYC) - MCP Server compatible with Claude Desktop. Comprehensive system diagnostics for Windows, Mac OS and Linux operating system with AI-powered recommendations.\n- **[Langflow MCP Server](https://github.com/nobrainer-tech/langflow-mcp)** - Comprehensive MCP server providing 90 tools for Langflow workflow automation - manage flows, execute workflows, handle builds, and interact with knowledge bases. Includes Docker support and full API coverage for Langflow 1.6.4.\n- **[Langflow-DOC-QA-SERVER](https://github.com/GongRzhe/Langflow-DOC-QA-SERVER)** - A Model Context Protocol server for document Q&A powered by Langflow. It demonstrates core MCP concepts by providing a simple interface to query documents through a Langflow backend.\n- **[Language Server](https://github.com/isaacphi/mcp-language-server)** - MCP Language Server helps MCP enabled clients navigate codebases more easily by giving them access to semantic tools like get definition, references, rename, and diagnostics.\n- **[Large File MCP](https://github.com/willianpinho/large-file-mcp)** - Intelligent handling of large files with smart chunking, navigation, and streaming capabilities. Features LRU caching, regex\nsearch, and comprehensive file analysis.\n- **[Lark(Feishu)](https://github.com/kone-net/mcp_server_lark)** - A Model Context Protocol(MCP) server for Lark(Feishu) sheet, message, doc and etc.\n- **[Lazy Toggl MCP](https://github.com/movstox/lazy-toggl-mcp)** - Simple unofficial MCP server to track time via Toggl API\n- **[lean-lsp-mcp](https://github.com/oOo0oOo/lean-lsp-mcp)** - Interact with the [Lean theorem prover](https://lean-lang.org/) via the Language Server Protocol.\n- **[librenms-mcp](https://github.com/mhajder/librenms-mcp)** - MCP server for [LibreNMS](https://www.librenms.org/) management\n- **[libvirt-mcp](https://github.com/MatiasVara/libvirt-mcp)** - Allows LLM to interact with libvirt thus enabling to create, destroy or list the Virtual Machines in a system.\n- **[Lightdash](https://github.com/syucream/lightdash-mcp-server)** - Interact with [Lightdash](https://www.lightdash.com/), a BI tool.\n- **[LINE](https://github.com/amornpan/py-mcp-line)** (by amornpan) - Implementation for LINE Bot integration that enables Language Models to read and analyze LINE conversations through a standardized interface. Features asynchronous operation, comprehensive logging, webhook event handling, and support for various message types.\n- **[Linear](https://github.com/tacticlaunch/mcp-linear)** - Interact with Linear project management system.\n- **[Linear](https://github.com/jerhadf/linear-mcp-server)** - Allows LLM to interact with Linear's API for project management, including searching, creating, and updating issues.\n- **[Linear (Go)](https://github.com/geropl/linear-mcp-go)** - Allows LLM to interact with Linear's API via a single static binary.\n- **[Linear MCP](https://github.com/anoncam/linear-mcp)** - Full blown implementation of the Linear SDK to support comprehensive Linear management of projects, initiatives, issues, users, teams and states.\n- **[Linked API MCP](https://github.com/Linked-API/linkedapi-mcp)** - MCP server that lets AI assistants control LinkedIn accounts and retrieve real-time data.\n- **[Listmonk MCP Server](https://github.com/rhnvrm/listmonk-mcp)** (by rhnvrm) - Full API coverage of [Listmonk](https://github.com/knadh/listmonk) email marketing FOSS.\n- **[LlamaCloud](https://github.com/run-llama/mcp-server-llamacloud)** (by marcusschiesser) - Integrate the data stored in a managed index on [LlamaCloud](https://cloud.llamaindex.ai/)\n- **[lldb-mcp](https://github.com/stass/lldb-mcp)** - A Model Context Protocol server for LLDB that provides LLM-driven debugging.\n- **[llm-context](https://github.com/cyberchitta/llm-context.py)** - Provides a repo-packing MCP tool with configurable profiles that specify file inclusion/exclusion patterns and optional prompts.\n- **[Local History](https://github.com/xxczaki/local-history-mcp)** â€“ MCP server for accessing VS Code/Cursor's Local History.\n- **[Local RAG](https://github.com/shinpr/mcp-local-rag)** - Lightweight local document search with minimal setup. Search across PDF, DOCX, TXT, and Markdown files - no Docker, no external services required.\n- **[Locust](https://github.com/QAInsights/locust-mcp-server)** - Allows running and analyzing Locust tests using MCP compatible clients.\n- **[Loki](https://github.com/scottlepp/loki-mcp)** - Golang based MCP Server to query logs from [Grafana Loki](https://github.com/grafana/loki).\n- **[Loki MCP Server](https://github.com/mo-silent/loki-mcp-server)** - Python based MCP Server for querying and analyzing logs from Grafana Loki with advanced filtering and authentication support.\n- **[LottieFiles](https://github.com/junmer/mcp-server-lottiefiles)** - Searching and retrieving Lottie animations from [LottieFiles](https://lottiefiles.com/)\n- **[lsp-mcp](https://github.com/Tritlo/lsp-mcp)** - Interact with Language Servers usint the Language Server Protocol to provide additional context information via hover, code actions and completions.\n- **[Lspace](https://github.com/Lspace-io/lspace-server)** - Turn scattered ChatGPT/Claude/Cursor conversations into persistent, searchable knowledge.\n- **[lucene-mcp-server](https://github.com/VivekKumarNeu/MCP-Lucene-Server)** - spring boot server using Lucene for fast document search and management.\n- **[lucid-mcp-server](https://github.com/smartzan63/lucid-mcp-server)** â€“ An MCP server for Lucidchart and Lucidspark: connect, search, and obtain text representations of your Lucid documents and diagrams via LLM - driven AI Vision analysis. [npm](https://www.npmjs.com/package/lucid-mcp-server)\n- **[LunarCrush Remote MCP](https://github.com/lunarcrush/mcp-server)** - Get the latest social metrics and posts for both current live social context as well as historical metrics in LLM and token optimized outputs. Ideal for automated trading / financial advisory.\n- **[mac-messages-mcp](https://github.com/carterlasalle/mac_messages_mcp)** - An MCP server that securely interfaces with your iMessage database via the Model Context Protocol (MCP), allowing LLMs to query and analyze iMessage conversations. It includes robust phone number validation, attachment processing, contact management, group chat handling, and full support for sending and receiving messages.\n- **[Maestro MCP](https://github.com/maestro-org/maestro-mcp)** - An MCP server for interacting with Bitcoin via the Maestro RPC API.\n- **[Magg: The MCP Aggregator](https://github.com/sitbon/magg)** - A meta-MCP server that acts as a universal hub, allowing LLMs to autonomously discover, install, and orchestrate multiple MCP servers - essentially giving AI assistants the power to extend their own capabilities on-demand. Includes `mbro`, a powerful CLI MCP server browser with scripting capability.\n- **[Mailchimp MCP](https://github.com/AgentX-ai/mailchimp-mcp)** - Allows AI agents to interact with the Mailchimp API (read-only)\n- **[MailNet](https://github.com/Astroa7m/MailNet-MCP-Server)** - Unified Gmail + Outlook MCP server with agentic orchestration, automatic token refresh, standardized base class for new providers, and dedicated email settings endpoints for tone, signature, and thread-aware replies.\n- **[MalwareBazaar_MCP](https://github.com/mytechnotalent/MalwareBazaar_MCP)** (by Kevin Thomas) - An AI-driven MCP server that autonomously interfaces with MalwareBazaar, delivering real-time threat intel and sample metadata for authorized cybersecurity research workflows.\n- **[man-mcp-server](https://github.com/guyru/man-mcp-server)** - MCP to search and access man pages on the local machine.\n- **[Mandoline](https://github.com/mandoline-ai/mandoline-mcp-server)** - Enable AI assistants to reflect on, critique, and continuously improve their own performance using Mandoline's evaluation framework.\n- **[MariaDB](https://github.com/abel9851/mcp-server-mariadb)** - MariaDB database integration with configurable access controls in Python.\n- **[Markdown2doc](https://github.com/Klavis-AI/klavis/tree/main/mcp_servers/pandoc)** - Convert between various file formats using Pandoc\n- **[Markdownify](https://github.com/zcaceres/mcp-markdownify-server)** - MCP to convert almost anything to Markdown (PPTX, HTML, PDF, Youtube Transcripts and more)\n- **[market-fiyati](https://github.com/mtcnbzks/market-fiyati-mcp-server)** - The MCP server for marketfiyati.org.tr, offering grocery price search and comparison across Turkish markets.)\n- **[Markitdown](https://github.com/Klavis-AI/klavis/tree/main/mcp_servers/markitdown)** - Convert files to Markdown\n- **[Masquerade](https://github.com/postralai/masquerade)** - Redact sensitive information from your PDF documents before sending them to Claude. Masquerade serves as a privacy firewall for LLMs.\n- **[MasterGo](https://github.com/mastergo-design/mastergo-magic-mcp)** - The server designed to connect MasterGo design tools with AI models. It enables AI models to directly retrieve DSL data from MasterGo design files.\n- **[Matlab-MCP-Tools](https://github.com/neuromechanist/matlab-mcp-tools)** - An MCP to write and execute MATLAB scripts, maintain workspace context between MCP calls, visualize plots, and perform section-by-section analysis of MATLAB code with full access to MATLAB's computational capabilities.\n- **[Maton](https://github.com/maton-ai/agent-toolkit/tree/main/modelcontextprotocol)** - Connect to your SaaS tools like HubSpot, Salesforce, and more.\n- **[Matrix](https://github.com/mjknowles/matrix-mcp-server)** - Interact with a Matrix homeserver.\n- **[Maven Tools MCP](https://github.com/arvindand/maven-tools-mcp)** - Maven Central dependency intelligence for JVM build tools. Supports all build tools (Maven, Gradle, SBT, Mill) with Context7 integration for documentation support.\n- **[Maybe Don't AI Policy Engine](https://www.maybedont.ai/download/)** - Yet another MCP security gateway, Maybe Don't AI provides policy checks on any call before it reaches downstream MCP servers to protect users from agents behaving poorly.\n- **[MCP Bundles Hub](https://github.com/thinkchainai/mcpbundles)** - Discover, install, and manage 500+ MCP provider integrations and bundles through [MCP Bundles](https://mcpbundles.com).\n- **[MCP Compass](https://github.com/liuyoshio/mcp-compass)** - Suggest the right MCP server for your needs\n- **[MCP Context Provider](https://github.com/doobidoo/MCP-Context-Provider)** - Static server that provides AI models with persistent tool-specific context and rules, preventing context loss between chat sessions and enabling consistent behavior across interactions.\n- **[MCP Create](https://github.com/tesla0225/mcp-create)** - A dynamic MCP server management service that creates, runs, and manages Model Context Protocol servers on-the-fly.\n- **[MCP Documentation Server](https://github.com/andrea9293/mcp-documentation-server)** - Server that provides local-first document management and semantic search via embeddings or Gemini AI (recommended). Optimized for performance with disk persistence, an in-memory index, and caching.\n- **[MCP Dynamic Tool Groups](https://github.com/ECF/MCPToolGroups)** - Example MCP servers that use [annotated](https://github.com/spring-ai-community/mcp-annotations) Java interfaces/classes as 'tool groups'.  Using standard MCP annotations, service implementations can then, at runtime, be used to generate tool specifications, and then dynamically added or removed from MCP servers.   The functionality is demonstrated in a sample tool group, but can be similarly used for any API or service.\n- **[MCP Installer](https://github.com/anaisbetts/mcp-installer)** - This server is a server that installs other MCP servers for you.\n- **[MCP on Android TV](https://github.com/MiddlePoint-Solutions/mcp-on-android-tv)** - A Model Context Protocol (MCP) server running directly on your Android TV with bundeld access to ADB on-device.\n- **[MCP OpenProject Server](https://github.com/boma086/mcp-openproject)** - Comprehensive MCP server for OpenProject integration with GitHub installation, CLI tools, and support for multiple AI assistants including Claude Code and Windsurf.\n- **[MCP ProjectManage OpenProject](https://github.com/boma086/mcp-projectmanage-openproject)** - This server provides the MCP service for project weekly reports, with project management information supplied by OpenProject.\n- **[MCP Proxy Server](https://github.com/TBXark/mcp-proxy)** - An MCP proxy server that aggregates and serves multiple MCP resource servers through a single HTTP server.\n- **[MCP Server Creator](https://github.com/GongRzhe/MCP-Server-Creator)** - A powerful Model Context Protocol (MCP) server that creates other MCP servers! This meta-server provides tools for dynamically generating FastMCP server configurations and Python code.\n- **[MCP Server Generator](https://github.com/SerhatUzbas/mcp-server-generator)** - An MCP server that creates and manages  MCP servers! Helps both non-technical users and developers build custom JavaScript MCP servers with AI guidance, automatic dependency management, and Claude Desktop integration.\n- **[MCP STDIO to Streamable HTTP Adapter](https://github.com/pyroprompts/mcp-stdio-to-streamable-http-adapter)** - Connect to Streamable HTTP MCP Servers even if the MCP Client only supports STDIO.\n- **[MCP Toolz](https://github.com/taylorleese/mcp-toolz)** - Context management, todo persistence, and AI second opinions for Claude Code. Save and restore contexts, code snippets, and todo lists across sessions and get feedback from ChatGPT, Claude, Gemini, and DeepSeek.\n- **[MCP-Airflow-API](https://github.com/call518/MCP-Airflow-API)** - Model Context Protocol (MCP) server for Apache Airflow API integration. Provides comprehensive tools for managing Airflow clusters including service operations, configuration management, status monitoring, and request tracking.\n- **[MCP-Ambari-API](https://github.com/call518/MCP-Ambari-API)** - Model Context Protocol (MCP) server for Apache Ambari API integration. This project provides tools for managing Hadoop clusters, including service operations, configuration management, status monitoring, and request tracking.\n- **[mcp-containerd](https://github.com/jokemanfire/mcp-containerd)** - The containerd MCP implemented by Rust supports the operation of the CRI interface.\n- **[MCP-Database-Server](https://github.com/executeautomation/mcp-database-server)** - Fastest way to interact with your Database such as SQL Server, SQLite and PostgreSQL\n- **[mcp-grep](https://github.com/erniebrodeur/mcp-grep)** - Python-based MCP server that brings grep functionality to LLMs. Supports common grep features including pattern searching, case-insensitive matching, context lines, and recursive directory searches.\n- **[mcp-k8s-go](https://github.com/strowk/mcp-k8s-go)** - Golang-based Kubernetes server for MCP to browse pods and their logs, events, namespaces and more. Built to be extensible.\n- **[mcp-local-rag](https://github.com/nkapila6/mcp-local-rag)** - \"primitive\" RAG-like web search model context protocol (MCP) server that runs locally using Google's MediaPipe Text Embedder and DuckDuckGo Search.\n- **[mcp-mcp](https://github.com/wojtyniak/mcp-mcp)** - Meta-MCP Server that acts as a tool discovery service for MCP clients.\n- **[mcp-meme-sticky](https://github.com/nkapila6/mcp-meme-sticky)** - Make memes or stickers using MCP server for WhatsApp or Telegram.\n- **[mcp-memory-service](https://github.com/doobidoo/mcp-memory-service)** - Universal MCP memory service providing semantic memory search, persistent storage, and autonomous memory consolidation for AI assistants across 13+ AI applications.\n- **[mcp-n8n](https://github.com/gomakers-ai/mcp-n8n)** - Complete n8n API integration with 41 tools for workflow management, execution monitoring, credentials, and 100+ pre-built templates. Control your entire n8n automation infrastructure through AI conversations.\n- **[MCP-NixOS](https://github.com/utensils/mcp-nixos)** - A Model Context Protocol server that provides AI assistants with accurate, real-time information about NixOS packages, system options, Home Manager settings, and nix-darwin macOS configurations.\n- **[mcp-notify](https://github.com/aahl/mcp-notify)** - An MCP server for message push, supporting Weixin, DingTalk, Telegram, Bark, Lark, Feishu, and Home Assistant.\n- **[mcp-open-library](https://github.com/8enSmith/mcp-open-library)** - A Model Context Protocol (MCP) server for the Open Library API that enables AI assistants to search for book and author information.\n- **[MCP-OpenStack-Ops](https://github.com/call518/MCP-OpenStack-Ops)** - Professional OpenStack operations automation via MCP server. Specialized tools for cluster monitoring, instance management, volume control & network analysis. FastMCP + OpenStack SDK + Bearer auth. Claude Desktop ready. Perfect for DevOps & cloud automation.\n- **[MCP-PostgreSQL-Ops](https://github.com/call518/MCP-PostgreSQL-Ops)** - Model Context Protocol (MCP) server for Apache Ambari API integration. This project provides tools for managing Hadoop clusters, including service operations, configuration management, status monitoring, and request tracking.\n- **[mcp-proxy](https://github.com/sparfenyuk/mcp-proxy)** - Connect to MCP servers that run on SSE transport, or expose stdio servers as an SSE server.\n- **[mcp-proxy](https://github.com/mikluko/mcp-proxy)** - Lightweight proxy that handles OAuth 2.0/PKCE authentication and token management for MCP clients lacking native OAuth support.\n- **[mcp-read-website-fast](https://github.com/just-every/mcp-read-website-fast)** - Fast, token-efficient web content extraction that converts websites to clean Markdown. Features Mozilla Readability, smart caching, polite crawling with robots.txt support, and concurrent fetching with minimal dependencies.\n- **[mcp-salesforce](https://github.com/lciesielski/mcp-salesforce-example)** - MCP server with basic demonstration of interactions with your Salesforce instance\n- **[mcp-sanctions](https://github.com/madupay/mcp-sanctions)** - Screen individuals and organizations against global sanctions lists (OFAC, SDN, UN, etc). Query by prompt or document upload.\n- **[mcp-screenshot-website-fast](https://github.com/just-every/mcp-screenshot-website-fast)** - High-quality screenshot capture optimized for Claude Vision API. Automatically tiles full pages into 1072x1072 chunks (1.15 megapixels) with configurable viewports and wait strategies for dynamic content.\n- **[mcp-server-leetcode](https://github.com/doggybee/mcp-server-leetcode)** - Practice and retrieve problems from LeetCode. Automate problem retrieval, solutions, and insights for coding practice and competitions.\n- **[Mcp-Swagger-Server](https://github.com/zaizaizhao/mcp-swagger-server)** (by zaizaizhao) - This MCP server transforms OpenAPI specifications into MCP tools, enabling AI assistants to interact with REST APIs through standardized protocol\n- **[mcp-vision](https://github.com/groundlight/mcp-vision)** - An MCP server exposing HuggingFace computer vision models such as zero-shot object detection as tools, enhancing the vision capabilities of large language or vision-language models.\n- **[mcp-weather](https://github.com/TimLukaHorstmann/mcp-weather)** - Accurate weather forecasts via the AccuWeather API (free tier available).\n- **[mcp-youtube-extract](https://github.com/sinjab/mcp_youtube_extract)** - A Model Context Protocol server for YouTube operations, extracting video information and transcripts with intelligent fallback logic. Features comprehensive logging, error handling, and support for both auto-generated and manual transcripts.\n- **[mcp_weather](https://github.com/isdaniel/mcp_weather_server)** - Get weather information from https://api.open-meteo.com API.\n- **[mcpcap](https://github.com/mcpcap/mcpcap)** - A modular Python MCP (Model Context Protocol) Server for analyzing PCAP files.\n- **[MCPfinder](https://github.com/mcpfinder/server)** - The AI Agent's \"App Store\": Discover, install, and monetize AI capabilities â€” all within the MCP ecosystem.\n- **[MCPIgnore Filesytem](https://github.com/CyberhavenInc/filesystem-mcpignore)** - A Data Security First filesystem MCP server that implements .mcpignore to prevent MCP clients from accessing sensitive data.\n- **[MCPJungle](https://github.com/mcpjungle/MCPJungle)** - Self-hosted MCP Registry and Gateway for enterprise AI Agents\n- **[MCPShell](https://github.com/inercia/mcpshell)** - Tool that allows LLMs to safely execute command-line tools, providing a secure bridge between LLMs and operating system commands.\n- **[Md2doc](https://github.com/Yorick-Ryu/md2doc-mcp)** - Convert Markdown text to DOCX format using an external conversion service\n- **[MeasureSpace MCP](https://github.com/MeasureSpace/measure-space-mcp-server)** - A free [Model Context Protocol (MCP) Server](https://smithery.ai/server/@MeasureSpace/measure-space-mcp-server) that provides global weather, climate, air quality forecast and geocoding services by [measurespace.io](https://measurespace.io).\n- **[MediaWiki](https://github.com/ProfessionalWiki/MediaWiki-MCP-Server)** - A Model Context Protocol (MCP) Server that interacts with any MediaWiki wiki\n- **[MediaWiki MCP adapter](https://github.com/lucamauri/MediaWiki-MCP-adapter)** - A custom Model Context Protocol adapter for MediaWiki and WikiBase APIs\n- **[medRxiv](https://github.com/JackKuo666/medRxiv-MCP-Server)** - Enable AI assistants to search and access medRxiv papers through a simple MCP interface.\n- **[mem0-mcp](https://github.com/mem0ai/mem0-mcp)** - A Model Context Protocol server for Mem0, which helps with managing coding preferences.\n- **[Membase](https://github.com/unibaseio/membase-mcp)** - Save and query your agent memory in distributed way by Membase.\n- **[Meme MCP](https://github.com/lidorshimoni/meme-mcp)** - Generate memes via AI using the Imgflip API through the Model Context Protocol.\n- **[memento-mcp](https://github.com/gannonh/memento-mcp)** - Knowledge graph memory system built on Neo4j with semantic search, temporal awareness.\n- **[memos-api-mcp](https://github.com/MemTensor/memos-api-mcp)** - A Model Context Protocol implementation for the API service of [MemOS](https://memos.openmem.net/), a memory management operating system designed for AI applications.\n- **[Meta Ads Remote MCP](https://github.com/pipeboard-co/meta-ads-mcp)** - Remote MCP server to interact with Meta Ads API - access, analyze, and manage Facebook, Instagram, and other Meta platforms advertising campaigns.\n- **[MetaTrader MCP](https://github.com/ariadng/metatrader-mcp-server)** - Enable AI LLMs to execute trades using MetaTrader 5 platform.\n- **[Metricool MCP](https://github.com/metricool/mcp-metricool)** - A Model Context Protocol server that integrates with Metricool's social media analytics platform to retrieve performance metrics and schedule content across networks like Instagram, Facebook, Twitter, LinkedIn, TikTok and YouTube.\n- **[Microsoft 365](https://github.com/merill/lokka)** - (by Merill) A Model Context Protocol (MCP) server for Microsoft 365. Includes support for all services including Teams, SharePoint, Exchange, OneDrive, Entra, Intune and more. See [Lokka](https://lokka.dev/) for more details.\n- **[Microsoft 365](https://github.com/softeria/ms-365-mcp-server)** - MCP server that connects to Microsoft Office and the whole Microsoft 365 suite using Graph API (including Outlook/mail, files, Excel, calendar)\n- **[Microsoft 365](https://github.com/pnp/cli-microsoft365-mcp-server)** - Single MCP server that allows to manage many different areas of Microsoft 365, for example: Entra ID, OneDrive, OneNote, Outlook, Planner, Power Apps, Power Automate, Power Platform, SharePoint Embedded, SharePoint Online, Teams, Viva Engage, and many more.\n- **[Microsoft 365 Files (SharePoint/OneDrive)](https://github.com/godwin3737/mcp-server-microsoft365-filesearch)** (by godwin3737) - MCP server with tools to search and get file content from Microsoft 365 including Onedrive and SharePoint. Works with Documents (pdf/docx), Presentations, Spreadsheets and Images.\n- **[Microsoft Teams](https://github.com/InditexTech/mcp-teams-server)** - MCP server that integrates Microsoft Teams messaging (read, post, mention, list members and threads)\n- **[Mifos X](https://github.com/openMF/mcp-mifosx)** - An MCP server for the Mifos X Open Source Banking useful for managing clients, loans, savings, shares, financial transactions and generating financial reports.\n- **[Mikrotik](https://github.com/jeff-nasseri/mikrotik-mcp)** - Mikrotik MCP server which cover networking operations (IP, DHCP, Firewall, etc)\n- **[Mindmap](https://github.com/YuChenSSR/mindmap-mcp-server)** (by YuChenSSR) - A server that generates mindmaps from input containing markdown code.\n- **[Minima](https://github.com/dmayboroda/minima)** - MCP server for RAG on local files\n- **[MLflow](https://github.com/kkruglik/mlflow-mcp)** - MLflow MCP server for ML experiment tracking with advanced querying, run comparison, artifact access, and model registry.\n- **[Mobile MCP](https://github.com/mobile-next/mobile-mcp)** (by Mobile Next) - MCP server for Mobile(iOS/Android) automation, app scraping and development using physical devices or simulators/emulators.\n- **[Modao Proto MCP](https://github.com/modao-dev/modao-proto-mcp)** - AI-powered HTML prototype generation server that converts natural language descriptions into complete HTML code with modern design and responsive layouts. Supports design description expansion and seamless integration with Modao workspace.\n- **[Monday.com (unofficial)](https://github.com/sakce/mcp-server-monday)** - MCP Server to interact with Monday.com boards and items.\n- **[MongoDB](https://github.com/kiliczsh/mcp-mongo-server)** - A Model Context Protocol Server for MongoDB.\n- **[MongoDB & Mongoose](https://github.com/nabid-pf/mongo-mongoose-mcp)** - MongoDB MCP Server with Mongoose Schema and Validation.\n- **[MongoDB Lens](https://github.com/furey/mongodb-lens)** - Full Featured MCP Server for MongoDB Databases.\n- **[Monzo](https://github.com/BfdCampos/monzo-mcp-bfdcampos)** - Access and manage your Monzo bank accounts through natural language, including balance checking, pot management, transaction listing, and transaction annotation across multiple account types (personal, joint, flex).\n- **[Morningstar](https://github.com/Morningstar/morningstar-mcp-server)** - MCP Server to interact with Morningstar Research, Editorial and Datapoints\n- **[MSSQL](https://github.com/aekanun2020/mcp-server/)** - MSSQL database integration with configurable access controls and schema inspection\n- **[MSSQL](https://github.com/JexinSam/mssql_mcp_server)** (by jexin) - MCP Server for MSSQL database in Python\n- **[MSSQL-MCP](https://github.com/daobataotie/mssql-mcp)** (by daobataotie) - MSSQL MCP that refer to the official website's SQLite MCP for modifications to adapt to MSSQL\n- **[MSSQL-MCP-Node](https://github.com/mihai-dulgheru/mssql-mcp-node)** (by mihai - dulgheru) â€“ Node.js MCP server for Microsoft SQL Server featuring auto-detected single / multi-database configs, execute-SQL and schema tools, robust Zod validation, and optional Express endpoints for local testing\n- **[MSSQL-Python](https://github.com/amornpan/py-mcp-mssql)** (by amornpan) - A read-only Python implementation for MSSQL database access with enhanced security features, configurable access controls, and schema inspection capabilities. Focuses on safe database interaction through Python ecosystem.\n- **[Multi-Model Advisor](https://github.com/YuChenSSR/multi-ai-advisor-mcp)** - A Model Context Protocol (MCP) server that orchestrates queries across multiple Ollama models, synthesizing their insights to deliver a comprehensive and multifaceted AI perspective on any given query.\n- **[Multicluster-MCP-Sever](https://github.com/yanmxa/multicluster-mcp-server)** - The gateway for GenAI systems to interact with multiple Kubernetes clusters.\n- **[MySQL](https://github.com/benborla/mcp-server-mysql)** (by benborla) - MySQL database integration in NodeJS with configurable access controls and schema inspection\n- **[MySQL](https://github.com/designcomputer/mysql_mcp_server)** (by DesignComputer) - MySQL database integration in Python with configurable access controls and schema inspection\n- **[MySQL-Server](https://github.com/tonycai/mcp-mysql-server)** (by TonyCai) - MySQL Database Integration using Python script with configurable access controls and schema inspection, usng stdio mode to suitable local deployment, you can run it in docker container.\n- **[n8n](https://github.com/leonardsellem/n8n-mcp-server)** - This MCP server provides tools and resources for AI assistants to manage n8n workflows and executions, including listing, creating, updating, and deleting workflows, as well as monitoring their execution status.\n- **[Nacos MCP Router](https://github.com/nacos-group/nacos-mcp-router)** - This MCP(Model Context Protocol) Server provides tools to search, install, proxy other MCP servers.\n- **[Nanana](https://github.com/nanana-app/mcp-server-nano-banana)** - This MCP provides AI text-to-image generator and AI image-to-image editor powered by Google Gemini Nano Banana.\n- **[NASA](https://github.com/ProgramComputer/NASA-MCP-server)** (by ProgramComputer) - Access to a unified gateway of NASA's data sources including but not limited to APOD, NEO, EPIC, GIBS.\n- **[NASA Image MCP Server](https://github.com/adithya1012/NASA-MCP-Server/blob/main/README.md)** - MCP server providing access to NASA's visual data APIs including Mars Rover photos, Earth satellite imagery (EPIC/GIBS), and Astronomy picture of the day. Features built-in image analysis tools with automatic format detection, compression, and base64 conversion for LLM integration.\n- **[NASA Planetary Data System (PDS) MCP Server](https://github.com/NASA-PDS/pds-mcp-server)** - MCP server for connecting to NASA's Planetary Data System (PDS) enabling intelligent data discovery of all of NASA's data products from the 1960s to present day.\n- **[Nasdaq Data Link](https://github.com/stefanoamorelli/nasdaq-data-link-mcp)** (by stefanoamorelli) - An MCP server to access, explore, and interact with Nasdaq Data Link's extensive and valuable financial and economic datasets.\n- **[National Parks](https://github.com/KyrieTangSheng/mcp-server-nationalparks)** - The server provides latest information of park details, alerts, visitor centers, campgrounds, hiking trails, and events for U.S. National Parks.\n- **[NAVER](https://github.com/pfldy2850/py-mcp-naver)** (by pfldy2850) - This MCP server provides tools to interact with various Naver services, such as searching blogs, news, books, and more.\n- **[Naver](https://github.com/isnow890/naver-search-mcp)** (by isnow890) - MCP server for Naver Search API integration, supporting blog, news, shopping search and DataLab analytics features.\n- **[NBA](https://github.com/Taidgh-Robinson/nba-mcp-server)** - This MCP server provides tools to fetch recent and historical NBA games including basic and advanced statistics.\n- **[NCI GDC](https://github.com/QuentinCody/nci-gdc-mcp-server)** - Unofficial MCP server for the National Cancer Institute's Genomic Data Commons (GDC), providing access to harmonized cancer genomic and clinical data for oncology research.\n- **[NCP](https://github.com/portel-dev/ncp)** (Natural Context Provider by portel.dev) - NCP lets your AI dream of a tool and articulate its need as a user story. NCP then intelligently discovers and makes that tool instantly available, streamlining thought processes, eliminating cognitive overload, and slashing token costs by up to 87% (47ms discovery). Experience true on-demand tool access, smart health monitoring, and energy efficiency for your AI agents.\n- **[Neo4j](https://github.com/da-okazaki/mcp-neo4j-server)** - A community built server that interacts with Neo4j Graph Database.\n- **[Neovim](https://github.com/bigcodegen/mcp-neovim-server)** - An MCP Server for your Neovim session.\n- **[Netbird](https://github.com/aantti/mcp-netbird)** - List and analyze Netbird network peers, groups, policies, and more.\n- **[NetMind ParsePro](https://github.com/protagolabs/Netmind-Parse-PDF-MCP)** - The PDF Parser AI service, built and customized by the [NetMind](https://www.netmind.ai/) team.\n- **[NetSuite](https://github.com/dsvantien/netsuite-mcp-server)** - MCP server for NetSuite ERP integration with OAuth 2.0 authentication, enabling natural language access to NetSuite data through SuiteQL queries, reports, saved searches, and REST API operations.\n- **[Nikto MCP](https://github.com/weldpua2008/nikto-mcp)** (by weldpua2008) - A secure MCP server that enables AI agents to interact with Nikto web server scanner](- use with npx or docker).\n- **[NocoDB](https://github.com/edwinbernadus/nocodb-mcp-server)** - Read and write access to NocoDB database.\n- **[Node Code Sandbox](https://github.com/alfonsograziano/node-code-sandbox-mcp)** â€“ A Node.js MCP server that spins up isolated Docker - based sandboxes for executing JavaScript snippets with on-the-fly npm dependency installation\n- **[nomad-mcp](https://github.com/kocierik/mcp-nomad)** - A server that provides a set of tools for managing Nomad clusters through the MCP.\n- **[Notion](https://github.com/suekou/mcp-notion-server)** (by suekou) - Interact with Notion API.\n- **[Notion](https://github.com/v-3/notion-server)** (by v-3) - Notion MCP integration. Search, Read, Update, and Create pages through Claude chat.\n- **[Notion](https://github.com/njbrake/notion-mcp-server)** (by njbrake) - Fork of official Notion MCP Server that returns markdown representation instead of raw json for efficient token usage\n- **[NPM Plus](https://github.com/shacharsol/js-package-manager-mcp)** - AI-powered JavaScript package management with security scanning, bundle analysis, and intelligent dependency management for MCP-compatible editors.\n- **[NS Travel Information](https://github.com/r-huijts/ns-mcp-server)** - Access Dutch Railways (NS) real-time train travel information and disruptions through the official NS API.\n- **[ntfy-mcp](https://github.com/teddyzxcv/ntfy-mcp)** (by teddyzxcv) - The MCP server that keeps you informed by sending the notification on phone using ntfy\n- **[ntfy-me-mcp](https://github.com/gitmotion/ntfy-me-mcp)** (by gitmotion) - An ntfy MCP server for sending/fetching ntfy notifications to your self-hosted ntfy server from AI Agents ğŸ“¤ (supports secure token auth & more - use with npx or docker!)\n- **[oatpp-mcp](https://github.com/oatpp/oatpp-mcp)** - C++ MCP integration for Oat++. Use [Oat++](https://oatpp.io) to build MCP servers.\n- **[Obsidian Markdown Notes](https://github.com/calclavia/mcp-obsidian)** - Read and search through your Obsidian vault or any directory containing Markdown notes\n- **[Obsidian Notes](https://github.com/Piotr1215/mcp-obsidian)** - Direct file system access to Obsidian vaults with security-first design, advanced search capabilities including MOC (Maps of Content) discovery, and support for obsidian.nvim - no Obsidian app required.\n- **[obsidian-mcp](https://github.com/StevenStavrakis/obsidian-mcp)** - (by Steven Stavrakis) An MCP server for Obsidian.md with tools for searching, reading, writing, and organizing notes.\n- **[OceanBase](https://github.com/yuanoOo/oceanbase_mcp_server)** - (by yuanoOo) A Model Context Protocol (MCP) server that enables secure interaction with OceanBase databases.\n- **[Octocode](https://github.com/bgauryy/octocode-mcp)** - (by Guy Bary) AI-powered developer assistant that enables advanced code research, analysis and discovery across GitHub and NPM realms in realtime\n- **[Odoo](https://github.com/ivnvxd/mcp-server-odoo)** - Connect AI assistants to Odoo ERP systems for business data access and workflow automation.\n- **[Office-PowerPoint-MCP-Server](https://github.com/GongRzhe/Office-PowerPoint-MCP-Server)** - A Model Context Protocol (MCP) server for creating, reading, and manipulating Microsoft PowerPoint documents.\n- **[Office-Visio-MCP-Server](https://github.com/GongRzhe/Office-Visio-MCP-Server)** - A Model Context Protocol (MCP) server for creating, reading, and manipulating Microsoft Visio documents.\n- **[Office-Word-MCP-Server](https://github.com/GongRzhe/Office-Word-MCP-Server)** - A Model Context Protocol (MCP) server for creating, reading, and manipulating Microsoft Word documents.\n- **[Okta](https://github.com/kapilduraphe/okta-mcp-server)** - Interact with Okta API.\n- **[OKX-MCP-Server](https://github.com/memetus/okx-mcp-playground)** - An MCP server provides various blockchain data and market price data via the OKX API. The server enables Claude to perform operations like retrieve assets prices, transaction data, account history data and trade instruction data.\n- **[OneCite](https://github.com/HzaCode/OneCite)** - Universal citation management and academic reference toolkit. Generate citations from DOI, arXiv, titles, or URLs in multiple formats (BibTeX, APA, MLA). Supports 7+ literature types and 10+ academic databases with intelligent metadata completion.\n- **[OneNote](https://github.com/rajvirtual/MCP-Servers/tree/master/onenote)** - (by Rajesh Vijay) An MCP server that connects to Microsoft OneNote using the Microsoft Graph API. Reading notebooks, sections, and pages from OneNote,Creating new notebooks, sections, and pages in OneNote.\n- **[Onyx MCP Sandbox](https://github.com/avd1729/Onyx)** â€“ (by Aravind) A secure MCP server that executes code in isolated Docker sandboxes. Supports Python, Java, C, C++, JavaScript, and Rust. Provides the `run_code` tool, enforces CPU/memory limits, includes comprehensive tests, and detailed setup instructions.\n- **[Open Strategy Partners Marketing Tools](https://github.com/open-strategy-partners/osp_marketing_tools)** - Content editing codes, value map, and positioning tools for product marketing.\n- **[Open Targets](https://github.com/QuentinCody/open-targets-mcp-server)** - Unofficial MCP server for the Open Targets Platform, providing access to target-disease associations, drug discovery data, and therapeutic hypothesis generation for biomedical research.\n- **[OpenAI GPT Image](https://github.com/SureScaleAI/openai-gpt-image-mcp)** - OpenAI GPT image generation/editing MCP server.\n- **[OpenAI WebSearch MCP](https://github.com/ConechoAI/openai-websearch-mcp)** - This is a Python-based MCP server that provides OpenAI `web_search` built-in tool.\n- **[OpenAlex.org MCP](https://github.com/drAbreu/alex-mcp)** - Professional MCP server providing ML-powered author disambiguation and comprehensive researcher profiles using the OpenAlex database.\n- **[OpenAPI](https://github.com/snaggle-ai/openapi-mcp-server)** - Interact with [OpenAPI](https://www.openapis.org/) APIs.\n- **[OpenAPI AnyApi](https://github.com/baryhuang/mcp-server-any-openapi)** - Interact with large [OpenAPI](https://www.openapis.org/) docs using built-in semantic search for endpoints. Allows for customizing the MCP server prefix.\n- **[OpenAPI Schema](https://github.com/hannesj/mcp-openapi-schema)** - Allow LLMs to explore large [OpenAPI](https://www.openapis.org/) schemas without bloating the context.\n- **[OpenAPI Schema Explorer](https://github.com/kadykov/mcp-openapi-schema-explorer)** - Token-efficient access to local or remote OpenAPI/Swagger specs via MCP Resources.\n- **[OpenCTI](https://github.com/Spathodea-Network/opencti-mcp)** - Interact with OpenCTI platform to retrieve threat intelligence data including reports, indicators, malware and threat actors.\n- **[OpenCV](https://github.com/GongRzhe/opencv-mcp-server)** - An MCP server providing OpenCV computer vision capabilities. This allows AI assistants and language models to access powerful computer vision tools.\n- **[OpenDigger MCP Server](https://github.com/X-lab2017/open-digger-mcp-server)** - Model Context Protocol (MCP) server for [OpenDigger](https://open-digger.cn/en/), enabling advanced repository analytics and insights through tools and prompts.\n- **[OpenDota](https://github.com/asusevski/opendota-mcp-server)** - Interact with OpenDota API to retrieve Dota 2 match data, player statistics, and more.\n- **[OpenLink Generic Java Database Connectivity](https://github.com/OpenLinkSoftware/mcp-jdbc-server)** - Generic Database Management System (DBMS) access via Open Database Connectivity (ODBC) Connectors (Drivers)\n- **[OpenLink Generic Open Database Connectivity](https://github.com/OpenLinkSoftware/mcp-odbc-server)** - Generic Database Management System (DBMS) access via Open Database Connectivity (ODBC) Connectors (Drivers)\n- **[OpenLink Generic Python Open Database Connectivity](https://github.com/OpenLinkSoftware/mcp-pyodbc-server)** - Generic Database Management System (DBMS) access via Open Database Connectivity (ODBC) Connectors (Drivers) for PyODBC\n- **[OpenLink Generic SQLAlchemy Object-Relational Database Connectivity for PyODBC](https://github.com/OpenLinkSoftware/mcp-sqlalchemy-server)** - Generic Database Management System (DBMS) access via SQLAlchemy (PyODBC) Connectors (Drivers)\n- **[OpenMetadata](https://github.com/yangkyeongmo/mcp-server-openmetadata)** - MCP Server for OpenMetadata, an open-source metadata management platform.\n- **[OpenNeuro](https://github.com/QuentinCody/open-neuro-mcp-server)** - Unofficial MCP server for OpenNeuro, providing access to open neuroimaging datasets, study metadata, and brain imaging data for neuroscience research and analysis.\n- **[OpenReview](https://github.com/anyakors/openreview-mcp-server)** - An MCP server for [OpenReview](https://openreview.net/) to fetch, read and save manuscripts from AI/ML conferences.\n- **[OpenRPC](https://github.com/shanejonas/openrpc-mpc-server)** - Interact with and discover JSON-RPC APIs via [OpenRPC](https://open-rpc.org).\n- **[OpenStack](https://github.com/wangsqly0407/openstack-mcp-server)** - MCP server implementation that provides OpenStack interaction.\n- **[OpenWeather](https://github.com/mschneider82/mcp-openweather)** - Interact with the free openweathermap API to get the current and forecast weather for a location.\n- **[OpenZIM MCP](https://github.com/cameronrye/openzim-mcp)** - Modern, secure, and high-performance MCP server that enables AI models to access and search ZIM format knowledge bases offline, including Wikipedia and educational content archives.\n- **[Operative WebEvalAgent](https://github.com/Operative-Sh/web-eval-agent)** (by [Operative.sh](https://www.operative.sh)) - An MCP server to test, debug, and fix web applications autonomously.\n- **[OPNSense MCP](https://github.com/vespo92/OPNSenseMCP)** - MCP Server for OPNSense Firewall Management and API access\n- **[Optimade MCP](https://github.com/dianfengxiaobo/optimade-mcp-server)** - An MCP server conducts real-time material science data queries with the Optimade database (for example, elemental composition, crystal structure).\n- **[Oracle](https://github.com/marcelo-ochoa/servers)** (by marcelo-ochoa) - Oracle Database integration in NodeJS with configurable access controls, query explain, stats and schema inspection\n- **[Oracle Cloud Infrastructure (OCI)](https://github.com/karthiksuku/oci-mcp)** (by karthiksukumar) - Python MCP server for OCI infrastructure (Compute, Autonomous Database, Object Storage). Read-heavy by default with safe instance actions (start/stop/reset). Includes Claude Desktop config and `.env` compartment scoping.\n- **[Oura MCP server](https://github.com/tomekkorbak/oura-mcp-server)** - MCP server for Oura API to retrieve one's sleep data\n- **[Oura Ring](https://github.com/rajvirtual/oura-mcp-server)** (by Rajesh Vijay) - MCP Server to access and analyze your Oura Ring data. It provides a structured way to fetch and understand your health metrics.\n- **[Outline](https://github.com/Vortiago/mcp-outline)** - MCP Server to interact with [Outline](https://www.getoutline.com) knowledge base to search, read, create, and manage documents and their content, access collections, add comments, and manage document backlinks.\n- **[Outlook Mail + Calendar + OneDrive](https://github.com/Norcim133/OutlookMCPServer) - Virtual assistant with Outlook Mail, Calendar, and early OneDrive support (requires Azure admin).\n- **[Pacman](https://github.com/oborchers/mcp-server-pacman)** - An MCP server that provides package index querying capabilities. This server is able to search and retrieve information from package repositories like PyPI, npm, crates.io, Docker Hub, and Terraform Registry.\n- **[pancakeswap-poolspy-mcp](https://github.com/kukapay/pancakeswap-poolspy-mcp)** - An MCP server that tracks newly created liquidity pools on Pancake Swap.\n- **[Pandoc](https://github.com/vivekVells/mcp-pandoc)** - MCP server for seamless document format conversion using Pandoc, supporting Markdown, HTML, PDF, DOCX (.docx), csv and more.\n- **[Paradex MCP](https://github.com/sv/mcp-paradex-py)** - MCP native server for interacting with Paradex platform, including fully features trading.\n- **[Parliament MCP]([https://github.com/sv/mcp-paradex-py](https://github.com/i-dot-ai/parliament-mcp))** - MCP server for querying UK parliamentary data.\n- **[PDF reader MCP](https://github.com/gpetraroli/mcp_pdf_reader)** - MCP server to read and search text in a local PDF file.\n- **[PDF Tools MCP](https://github.com/Sohaib-2/pdf-mcp-server)** - Comprehensive PDF manipulation toolkit (merge, split, encrypt, optimize and much more)\n- **[PDMT](https://github.com/paiml/pdmt)** - Pragmatic Deterministic MCP Templating - High-performance deterministic templating library with comprehensive todo validation, quality enforcement, and 0.0 temperature generation for reproducible outputs.\n- **[Peacock for VS Code](https://github.com/johnpapa/peacock-mcp)** - MCP Server for the Peacock extension for VS Code, coloring your world, one Code editor at a time. The main goal of the project is to show how an MCP server can be used to interact with APIs.\n- **[persistproc](https://github.com/irskep/persistproc)** - MCP server + command line tool that allows agents to see & control long-running processes like web servers.\n- **[Pexels](https://github.com/garylab/pexels-mcp-server)** - A MCP server providing access to Pexels Free Image API, enabling seamless search, retrieval, and download of high-quality royalty-free images.\n- **[pgtuner_mcp](https://github.com/isdaniel/pgtuner_mcp)** - provides AI-powered PostgreSQL performance tuning capabilities.\n- **[Pharos](https://github.com/QuentinCody/pharos-mcp-server)** - Unofficial MCP server for the Pharos database by the National Center for Advancing Translational Sciences (NCATS), providing access to target, drug, and disease information for drug discovery research.\n- **[Phone MCP](https://github.com/hao-cyber/phone-mcp)** - ğŸ“± A powerful plugin that lets you control your Android phone. Enables AI agents to perform complex tasks like automatically playing music based on weather or making calls and sending texts.\n- **[PIF](https://github.com/hungryrobot1/MCP-PIF)** - A Personal Intelligence Framework (PIF), providing tools for file operations, structured reasoning, and journal-based documentation to support continuity and evolving human-AI collaboration across sessions.\n- **[Pinecone](https://github.com/sirmews/mcp-pinecone)** - MCP server for searching and uploading records to Pinecone. Allows for simple RAG features, leveraging Pinecone's Inference API.\n- **[Pinner MCP](https://github.com/safedep/pinner-mcp)** - An MCP server for pinning GitHub Actions and container base images to their immutable SHA hashes to prevent supply chain attacks.\n- **[Pixelle MCP](https://github.com/AIDC-AI/Pixelle-MCP)** - An omnimodal AIGC framework that seamlessly converts ComfyUI workflows into MCP tools with zero code, enabling full-modal support for Text, Image, Sound, and Video generation with Chainlit-based web interface.\n- **[Placid.app](https://github.com/felores/placid-mcp-server)** - Generate image and video creatives using Placid.app templates\n- **[Plane](https://github.com/kelvin6365/plane-mcp-server)** - This MCP Server will help you to manage projects and issues through Plane's API\n- **[Playwright](https://github.com/executeautomation/mcp-playwright)** - This MCP Server will help you run browser automation and webscraping using Playwright\n- **[Playwright Wizard](https://github.com/oguzc/playwright-wizard-mcp)** - Step-by-step wizard for generating Playwright E2E tests with best practices.\n- **[Podbean](https://github.com/amurshak/podbeanMCP)** - MCP server for managing your podcasts, episodes, and analytics through the Podbean API. Allows for updating, adding, deleting podcasts, querying show description, notes, analytics, and more.\n- **[Polarsteps](https://github.com/remuzel/polarsteps-mcp)** - An MCP server to help you review your previous Trips and plan new ones!\n- **[PostgreSQL](https://github.com/ahmedmustahid/postgres-mcp-server)** - A PostgreSQL MCP server offering dual HTTP/Stdio transports for database schema inspection and read-only query execution with session management and Podman(or Docker) support.\n- **[Postman](https://github.com/shannonlal/mcp-postman)** - MCP server for running Postman Collections locally via Newman. Allows for simple execution of Postman Server and returns the results of whether the collection passed all the tests.\n- **[Powerdrill](https://github.com/powerdrillai/powerdrill-mcp)** - Interact with Powerdrill datasets, authenticated with [Powerdrill](https://powerdrill.ai) User ID and Project API Key.\n- **[predictive-maintenance-mcp](https://github.com/LGDiMaggio/predictive-maintenance-mcp)** - AI-powered predictive maintenance and fault diagnosis. Features vibration analysis, bearing diagnostics, ISO 20816-3. compliance, and ML anomaly detection for industrial machinery.\n- **[Prefect](https://github.com/allen-munsch/mcp-prefect)** - MCP Server for workflow orchestration and ELT/ETL with Prefect Server, and Prefect Cloud [https://www.prefect.io/] using the `prefect` python client.\n- **[Producer Pal](https://github.com/adamjmurray/producer-pal)** - MCP server for controlling Ableton Live, embedded in a Max for Live device for easy drag and drop installation.\n- **[Productboard](https://github.com/kenjihikmatullah/productboard-mcp)** - Integrate the Productboard API into agentic workflows via MCP.\n- **[Prometheus](https://github.com/pab1it0/prometheus-mcp-server)** - Query and analyze Prometheus - open-source monitoring system.\n- **[Prometheus (Golang)](https://github.com/tjhop/prometheus-mcp-server/)** - A Prometheus MCP server with full API support for comprehensive management and deep interaction with Prometheus beyond basic query support. Written in go, it is a single binary install that is capable of STDIO, SSE, and HTTP transports for complex deployments. \n- **[Prometheus (TypeScript)](https://github.com/yanmxa/prometheus-mcp-server)** - Enable AI assistants to query Prometheus using natural language with TypeScript implementation.\n- **[PubChem](https://github.com/sssjiang/pubchem_mcp_server)** - extract drug information from pubchem API.\n- **[PubMed](https://github.com/JackKuo666/PubMed-MCP-Server)** - Enable AI assistants to search, access, and analyze PubMed articles through a simple MCP interface.\n- **[Pulumi](https://github.com/dogukanakkaya/pulumi-mcp-server)** - MCP Server to Interact with Pulumi API, creates and lists Stacks\n- **[Puppeteer vision](https://github.com/djannot/puppeteer-vision-mcp)** - Use Puppeteer to browse a webpage and return a high quality Markdown. Use AI vision capabilities to handle cookies, captchas, and other interactive elements automatically.\n- **[Pushover](https://github.com/ashiknesin/pushover-mcp)** - Send instant notifications to your devices using [Pushover.net](https://pushover.net/)\n- **[py-mcp-qdrant-rag](https://github.com/amornpan/py-mcp-qdrant-rag)** (by amornpan) - A Model Context Protocol server implementation that provides RAG capabilities through Qdrant vector database integration, enabling AI agents to perform semantic search and document retrieval with local or cloud-based embedding generation support across Mac, Linux, and Windows platforms.\n- **[pydantic/pydantic-ai/mcp-run-python](https://github.com/pydantic/pydantic-ai/tree/main/mcp-run-python)** - Run Python code in a secure sandbox via MCP tool calls, powered by Deno and Pyodide\n- **[Python CLI MCP](https://github.com/ofek/pycli-mcp)** - Interact with local Python command line applications.\n- **[qa-use](https://github.com/desplega-ai/qa-use)** - Browser automation and QA testing capabilities. This server integrates with [desplega.ai](https://desplega.ai) to offer automated testing, session monitoring, batch test execution, and intelligent test guidance using the AAA framework.\n- **[QGIS](https://github.com/jjsantos01/qgis_mcp)** - connects QGIS to Claude AI through the MCP. This integration enables prompt-assisted project creation, layer loading, code execution, and more.\n- **[Qiniu MCP Server](https://github.com/qiniu/qiniu-mcp-server)** - The Model Context Protocol (MCP) Server built on Qiniu Cloud products supports users in accessing Qiniu Cloud Storage, intelligent multimedia services, and more through this MCP Server within the context of AI large model clients.\n- **[QuantConnect](https://github.com/taylorwilsdon/quantconnect-mcp)** - QuantConnect Algorithmic Trading Platform Orchestration MCP - Agentic LLM Driven Trading Strategy Design, Research & Implementation.\n- **[Quarkus](https://github.com/quarkiverse/quarkus-mcp-servers)** - MCP servers for the Quarkus Java framework.\n- **[QuickChart](https://github.com/GongRzhe/Quickchart-MCP-Server)** - A Model Context Protocol server for generating charts using QuickChart.io\n- **[Qwen_Max](https://github.com/66julienmartin/MCP-server-Qwen_Max)** - A Model Context Protocol (MCP) server implementation for the Qwen models.\n- **[RabbitMQ](https://github.com/kenliao94/mcp-server-rabbitmq)** - The MCP server that interacts with RabbitMQ to publish and consume messages.\n- **[RAE](https://github.com/rae-api-com/rae-mcp)** - MPC Server to connect your preferred model with rae-api.com, Roya Academy of Spanish Dictionary\n- **[RAG Local](https://github.com/renl/mcp-rag-local)** - This MCP server for storing and retrieving text passages locally based on their semantic meaning.\n- **[RAG Web Browser](https://github.com/apify/mcp-server-rag-web-browser)** An MCP server for Apify's open-source RAG Web Browser [Actor](https://apify.com/apify/rag-web-browser) to perform web searches, scrape URLs, and return content in Markdown.\n- **[Raindrop.io](https://github.com/hiromitsusasaki/raindrop-io-mcp-server)** - An integration that allows LLMs to interact with Raindrop.io bookmarks using the Model Context Protocol (MCP).\n- **[Random Number](https://github.com/zazencodes/random-number-mcp)** - Provides LLMs with essential random generation abilities, built entirely on Python's standard library.\n- **[RCSB PDB](https://github.com/QuentinCody/rcsb-pdb-mcp-server)** - Unofficial MCP server for the Research Collaboratory for Structural Bioinformatics Protein Data Bank (RCSB PDB), providing access to 3D protein structures, experimental data, and structural bioinformatics information.\n- **[Reaper](https://github.com/dschuler36/reaper-mcp-server)** - Interact with your [Reaper](https://www.reaper.fm/) (Digital Audio Workstation) projects.\n- **[Redbee](https://github.com/Tamsi/redbee-mcp)** - Redbee MCP server that provides support for interacting with Redbee API.\n- **[Redfish](https://github.com/nokia/mcp-redfish)** - Redfish MCP server that provides support for interacting with [DMTF Redfish API](https://www.dmtf.org/standards/redfish).\n- **[Redis](https://github.com/GongRzhe/REDIS-MCP-Server)** - Redis database operations and caching microservice server with support for key-value operations, expiration management, and pattern-based key listing.\n- **[Redis](https://github.com/prajwalnayak7/mcp-server-redis)** MCP server to interact with Redis Server, AWS Memory DB, etc for caching or other use-cases where in-memory and key-value based storage is appropriate\n- **[RedNote MCP](https://github.com/ifuryst/rednote-mcp)** - MCP server for accessing RedNote(XiaoHongShu, xhs) content\n- **[Reed Jobs](https://github.com/kld3v/reed_jobs_mcp)** - Search and retrieve job listings from Reed.co.uk.\n- **[Rememberizer AI](https://github.com/skydeckai/mcp-server-rememberizer)** - An MCP server designed for interacting with the Rememberizer data source, facilitating enhanced knowledge retrieval.\n- **[Replicate](https://github.com/deepfates/mcp-replicate)** - Search, run and manage machine learning models on Replicate through a simple tool-based interface. Browse models, create predictions, track their status, and handle generated images.\n- **[Resend](https://github.com/Klavis-AI/klavis/tree/main/mcp_servers/resend)** - Send email using Resend services\n- **[Restream](https://github.com/shaktech786/restream-mcp-server)** - Model Context Protocol server for Restream API integration - manage multi-platform live streams, control channels, and access streaming analytics.\n- **[Revit MCP](https://github.com/revit-mcp)** - A service implementing the MCP protocol for Autodesk Revit.\n- **[Rijksmuseum](https://github.com/r-huijts/rijksmuseum-mcp)** - Interface with the Rijksmuseum API to search artworks, retrieve artwork details, access image tiles, and explore user collections.\n- **[Riot Games](https://github.com/jifrozen0110/mcp-riot)** - MCP server for League of Legends â€“ fetch player info, ranks, champion stats, and match history via Riot API.\n- **[Rohlik](https://github.com/tomaspavlin/rohlik-mcp)** - Shop groceries across the Rohlik Group platforms (Rohlik.cz, Knuspr.de, Gurkerl.at, Kifli.hu, Sezamo.ro)\n- **[Rquest](https://github.com/xxxbrian/mcp-rquest)** - An MCP server providing realistic browser-like HTTP request capabilities with accurate TLS/JA3/JA4 fingerprints for bypassing anti-bot measures.\n- **[Rust MCP Filesystem](https://github.com/rust-mcp-stack/rust-mcp-filesystem)** - Fast, asynchronous MCP server for efficient handling of various filesystem operations built with the power of Rust.\n- **[SafetySearch](https://github.com/surabhya/SafetySearch)** - Real-time FDA food safety data: recalls, adverse events, analysis.\n- **[Salesforce MCP](https://github.com/smn2gnt/MCP-Salesforce)** - Interact with Salesforce Data and Metadata\n- **[Salesforce MCP (AiondaDotCom)](https://github.com/AiondaDotCom/mcp-salesforce)** - Universal Salesforce integration with OAuth authentication, smart learning system, comprehensive backup capabilities, and full CRUD operations for any Salesforce org including custom objects and fields.\n- **[Salesforce MCP Server](https://github.com/tsmztech/mcp-server-salesforce)** - Comprehensive Salesforce integration with tools for querying records, executing Apex, managing fields/objects, and handling debug logs\n- **[Scanova MCP Server](https://github.com/trycon/scanova-mcp)** - MCP server for creating and managing QR codes using the [Scanova](https://scanova.io) API. Provides tools for generating, managing, and downloading QR codes.\n- **[SchemaCrawler](https://github.com/schemacrawler/SchemaCrawler-MCP-Server-Usage)** - Connect to any relational database, and be able to get valid SQL, and ask questions like what does a certain column prefix mean.\n- **[SchemaFlow](https://github.com/CryptoRadi/schemaflow-mcp-server)** - Real-time PostgreSQL & Supabase database schema access for AI-IDEs via Model Context Protocol. Provides live database context through secure SSE connections with three powerful tools: get_schema, analyze_database, and check_schema_alignment. [SchemaFlow](https://schemaflow.dev)\n- **[Scholarly](https://github.com/adityak74/mcp-scholarly)** - An MCP server to search for scholarly and academic articles.\n- **[scrapling-fetch](https://github.com/cyberchitta/scrapling-fetch-mcp)** - Access text content from bot-protected websites. Fetches HTML/markdown from sites with anti-automation measures using Scrapling.\n- **[Screeny](https://github.com/rohanrav/screeny)** - Privacy-first macOS MCP server that provides visual context for AI agents through window screenshots\n- **[ScriptFlow](https://github.com/yanmxa/scriptflow-mcp)** - Transform complex, repetitive AI interactions into persistent, executable scripts with comprehensive script management (add, edit, remove, list, search, execute) and multi-language support (Bash, Python, Node.js, TypeScript).\n- **[SearXNG](https://github.com/ihor-sokoliuk/mcp-searxng)** - A Model Context Protocol Server for [SearXNG](https://docs.searxng.org)\n- **[SearXNG](https://github.com/erhwenkuo/mcp-searxng)** - An MCP server provide web searching via [SearXNG](https://docs.searxng.org) & retrieve url as makrdown.\n- **[SearXNG Public](https://github.com/pwilkin/mcp-searxng-public)** - A Model Context Protocol Server for retrieving data from public [SearXNG](https://docs.searxng.org) instances, with fallback support\n- **[SEC EDGAR](https://github.com/stefanoamorelli/sec-edgar-mcp)** - (by Stefano Amorelli) A community Model Context Protocol Server to access financial filings and data through the U.S. Securities and Exchange Commission ([SEC](https://www.sec.gov/)) `Electronic Data Gathering, Analysis, and Retrieval` ([EDGAR](https://www.sec.gov/submit-filings/about-edgar)) database\n- **[SendGrid](https://github.com/recepyavuz0/sendgrid-mcp-server)** - An MCP server to integrate with SendGrid's API, enabling AI assistants (like Claude, ChatGPT, etc.) to send emails, manage templates, and track email statistics.\n- **[SEO MCP](https://github.com/cnych/seo-mcp)** - A free SEO tool MCP (Model Control Protocol) service based on Ahrefs data. Includes features such as backlinks, keyword ideas, and more. by [claudemcp](https://www.claudemcp.com/servers/seo-mcp).\n- **[Serper](https://github.com/garylab/serper-mcp-server)** - An MCP server that performs Google searches using [Serper](https://serper.dev).\n- **[ServiceNow](https://github.com/osomai/servicenow-mcp)** - An MCP server to interact with a ServiceNow instance\n- **[ShaderToy](https://github.com/wilsonchenghy/ShaderToy-MCP)** - This MCP server lets LLMs to interact with the ShaderToy API, allowing LLMs to learn from compute shaders examples and enabling them to create complex GLSL shaders that they are previously not capable of.\n- **[ShareSeer](https://github.com/shareseer/shareseer-mcp-server)** - MCP to Access SEC filings, financials & insider trading data in real time using [ShareSeer](https://shareseer.com)\n- **[Shell](https://github.com/sonirico/mcp-shell)** - Give hands to AI. MCP server to run shell commands securely, auditably, and on demand\n- **[Shodan MCP](https://github.com/Hexix23/shodan-mcp)** - MCP server to interact with [Shodan](https://www.shodan.io/)\n- **[Shopify](https://github.com/GeLi2001/shopify-mcp)** - MCP to interact with Shopify API including order, product, customers and so on.\n- **[Shopify Storefront](https://github.com/QuentinCody/shopify-storefront-mcp-server)** - Unofficial MCP server that allows AI agents to discover Shopify storefronts and interact with them to fetch products, collections, and other store data through the Storefront API.\n- **[Simple Loki MCP](https://github.com/ghrud92/simple-loki-mcp)** - A simple MCP server to query Loki logs using logcli.\n- **[Siri Shortcuts](https://github.com/dvcrn/mcp-server-siri-shortcuts)** - MCP to interact with Siri Shortcuts on macOS. Exposes all Shortcuts as MCP tools.\n- **[Skyvern](https://github.com/Skyvern-AI/skyvern/tree/main/integrations/mcp)** - MCP to let Claude / Windsurf / Cursor / your LLM control the browser\n- **[Slack](https://github.com/korotovsky/slack-mcp-server)** - The most powerful MCP server for Slack Workspaces. This integration supports both Stdio and SSE transports, proxy settings and does not require any permissions or bots being created or approved by Workspace admins ğŸ˜.\n- **[Slack](https://github.com/zencoderai/slack-mcp-server)** - Slack MCP server which supports both stdio and Streamable HTTP transports. Extended from the original Anthropic's implementation which is now [archived](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/slack)\n- **[Slidespeak](https://github.com/SlideSpeak/slidespeak-mcp)** - Create PowerPoint presentations using the [Slidespeak](https://slidespeak.com/) API.\n- **[Smartlead](https://github.com/jean-technologies/smartlead-mcp-server-local)** - MCP to connect to Smartlead. Additional, tooling, functionality, and connection to workflow automation platforms also available.\n- **[Snowflake](https://github.com/Snowflake-Labs/mcp)** - Open-source MCP server for Snowflake from official Snowflake-Labs supports prompting Cortex Agents, querying structured & unstructured data, object management, SQL execution, semantic view querying, and more. RBAC, fine-grained CRUD controls, and all authentication methods supported.\n- **[Snowflake](https://github.com/isaacwasserman/mcp-snowflake-server)** - This MCP server enables LLMs to interact with Snowflake databases, allowing for secure and controlled data operations.\n- **[Snowflake Cortex MCP Server](https://github.com/thisisbhanuj/Snowflake-Cortex-MCP-Server)** -This Snowflake MCP server provides tooling for Snowflake Cortex AI features, bringing these capabilities to the MCP ecosystem. When connected to an MCP Client (e.g. Claude for Desktop, fast-agent, Agentic Orchestration Framework), users can leverage these Cortex AI features.\n- **[SoccerDataAPI](https://github.com/yeonupark/mcp-soccer-data)** - This MCP server provides real-time football match data based on the SoccerDataAPI.\n- **[Solana Agent Kit](https://github.com/sendaifun/solana-agent-kit/tree/main/examples/agent-kit-mcp-server)** - This MCP server enables LLMs to interact with the Solana blockchain with help of Solana Agent Kit by SendAI, allowing for 40+ protocol actions and growing\n- **[Solr MCP](https://github.com/mjochum64/mcp-solr-search)** - This MCP server offers a basic functionality to perform a search on Solr servers.\n- **[Solver](https://github.com/szeider/mcp-solver)** - Solves constraint satisfaction and optimization problems .\n- **[Solvitor](https://github.com/Adeptus-Innovatio/solvitor-mcp)** â€“ Solvitor MCP server provides tools to access reverse engineering tools that help developers extract IDL files from closed - source Solana smart contracts and decompile them.\n- **[Source to Knowledge Base](https://github.com/vezlo/src-to-kb)** - Convert source code repositories into searchable knowledge bases with AI-powered search using GPT-5, intelligent chunking, and OpenAI embeddings for semantic code understanding.\n- **[Sourcerer](https://github.com/st3v3nmw/sourcerer-mcp)** - MCP for semantic code search & navigation that reduces token waste.\n- **[Specbridge](https://github.com/TBosak/specbridge)** - Easily turn your OpenAPI specs into MCP Tools.\n- **[Splunk](https://github.com/jkosik/mcp-server-splunk)** - Golang MCP server for Splunk (lists saved searches, alerts, indexes, macros...). Supports SSE and STDIO.\n- **[Spotify](https://github.com/varunneal/spotify-mcp)** - This MCP allows an LLM to play and use Spotify.\n- **[Spring Initializr](https://github.com/hpalma/springinitializr-mcp)** - This MCP allows an LLM to create Spring Boot projects with custom configurations. Instead of manually visiting start.spring.io, you can now ask your AI assistant to generate projects with specific dependencies, Java versions, and project structures.\n- **[Squadâ€¯AI](https://github.com/the-basilisk-ai/squad-mcp)** â€“ Productâ€‘discovery and strategy platform integration. Create, query and update opportunities, solutions, outcomes, requirements and feedback from any MCPâ€‘aware LLM.\n- **[SSH](https://github.com/AiondaDotCom/mcp-ssh)** - Agent for managing and controlling SSH connections.\n- **[SSH](https://github.com/classfang/ssh-mcp-server)** - An MCP server that can execute SSH commands remotely, upload files, download files, and so on.\n- **[SSH MCP Server](https://github.com/sinjab/mcp_ssh)** - A production-ready Model Context Protocol server for SSH automation with background execution, file transfers, and comprehensive timeout protection. Features structured output, progress tracking, and enterprise-grade testing (87% coverage).\n- **[sslmon](https://github.com/firesh/sslmon-mcp)** - Domain/HTTPS/SSL domain registration information and SSL certificate monitoring capabilities. Query domain registration and expiration information, and SSL certificate information and validity status for any domain.\n- **[STAC](https://github.com/Wayfinder-Foundry/stac-mcp)** - STAC catalog and item search MCP server for rapid geospatial data discovery.\n- **[Standard Korean Dictionary](https://github.com/privetin/stdict)** - Search the dictionary using API\n- **[Star Wars](https://github.com/johnpapa/mcp-starwars)** -MCP Server for the SWAPI Star Wars API. The main goal of the project is to show how an MCP server can be used to interact with APIs.\n- **[Starknet MCP Server](https://github.com/mcpdotdirect/starknet-mcp-server)** - A comprehensive MCP server for interacting with the Starknet blockchain, providing tools for querying blockchain data, resolving StarknetIDs, and performing token transfers.\n- **[Starling Bank](https://github.com/domdomegg/starling-bank-mcp)** - View and manage Starling Bank accounts and transactions through the Starling Bank API, including account balance checking and transaction history.\n- **[Starwind UI](https://github.com/Boston343/starwind-ui-mcp/)** - This MCP provides relevant commands, documentation, and other information to allow LLMs to take full advantage of Starwind UI's open source Astro components.\n- **[Stellar](https://github.com/syronlabs/stellar-mcp/)** - This MCP server enables LLMs to interact with the Stellar blockchain to create accounts, check address balances, analyze transactions, view transaction history, mint new assets, interact with smart contracts and much more.\n- **[Stitch AI](https://github.com/StitchAI/stitch-ai-mcp/)** - Knowledge management system for AI agents with memory space creation and retrieval capabilities.\n- **[Stockfish](https://github.com/sonirico/mcp-stockfish)** - MCP server connecting AI systems to Stockfish chess engine\n- **[Storybook](https://github.com/stefanoamorelli/storybook-mcp-server)** (by Stefano Amorelli) - Interact with Storybook component libraries, enabling component discovery, story management, prop inspection, and visual testing across different viewports.\n- **[Strava](https://github.com/r-huijts/strava-mcp)** - Connect to the Strava API to access activity data, athlete profiles, segments, and routes, enabling fitness tracking and analysis with Claude.\n- **[Strava API](https://github.com/tomekkorbak/strava-mcp-server)** - MCP server for Strava API to retrieve one's activities\n- **[Stripe](https://github.com/atharvagupta2003/mcp-stripe)** - This MCP allows integration with Stripe for handling payments, customers, and refunds.\n- **[Substack/Medium](https://github.com/jonathan-politzki/mcp-writer-substack)** - Connect Claude to your Substack/Medium writing, enabling semantic search and analysis of your published content.\n- **[System Health](https://github.com/thanhtung0201/mcp-remote-system-health)** - The MCP (Multi-Channel Protocol) System Health Monitoring is a robust, real-time monitoring solution designed to provide comprehensive health metrics and alerts for remote Linux servers.\n- **[SystemSage](https://github.com/Tarusharma1/SystemSage)** - A powerful, cross-platform system management and monitoring tool for Windows, Linux, and macOS.\n- **[Talk To Figma](https://github.com/sonnylazuardi/cursor-talk-to-figma-mcp)** - This MCP server enables LLMs to interact with Figma, allowing them to read and modify designs programmatically.\n- **[Talk To Figma via Claude](https://github.com/gaganmanku96/talk-with-figma-claude)** - TMCP server that provides seamless Figma integration specifically for Claude Desktop, enabling design creation, modification, and real-time collaboration through natural language commands.\n- **[TAM MCP Server](https://github.com/gvaibhav/TAM-MCP-Server)** - Market research and business intelligence with TAM/SAM calculations and integration across 8 economic data sources: Alpha Vantage, BLS, Census Bureau, FRED, IMF, Nasdaq Data Link, OECD, and World Bank.\n- **[Tasks](https://github.com/flesler/mcp-tasks)** - An efficient task manager. Designed to minimize tool confusion and maximize LLM budget efficiency while providing powerful search, filtering, and organization capabilities across multiple file formats (Markdown, JSON, YAML)\n- **[Tavily search](https://github.com/RamXX/mcp-tavily)** - An MCP server for Tavily's search & news API, with explicit site inclusions/exclusions\n- **[TcpSocketMCP](https://github.com/SpaceyKasey/TcpSocketMCP/)** - A Model Context Protocol (MCP) server that provides raw TCP socket access, enabling AI models to interact directly with network services using raw TCP Sockets. Supports multiple concurrent connections, buffering of response data and triggering automatic responses.\n- **[TeamRetro](https://github.com/adepanges/teamretro-mcp-server)** - This MCP server allows LLMs to interact with TeamRetro, allowing LLMs to manage user, team, team member, retrospective, health check, action, agreement and fetch the reports.\n- **[Telegram](https://github.com/chigwell/telegram-mcp)** - An MCP server that provides paginated chat reading, message retrieval, and message sending capabilities for Telegram through Telethon integration.\n- **[Telegram-Client](https://github.com/chaindead/telegram-mcp)** - A Telegram API bridge that manages user data, dialogs, messages, drafts, read status, and more for seamless interactions.\n- **[Telegram-mcp-server](https://github.com/DLHellMe/telegram-mcp-server)** - Access Telegram channels and groups directly in Claude. Features dual-mode operation with API access (100x faster) or web scraping, unlimited post retrieval, and search functionality.\n- **[Template MCP Server](https://github.com/mcpdotdirect/template-mcp-server)** - A CLI tool to create a new Model Context Protocol server project with TypeScript support, dual transport options, and an extensible structure\n- **[Tempo](https://github.com/scottlepp/tempo-mcp-server)** - An MCP server to query traces/spans from [Grafana Tempo](https://github.com/grafana/tempo).\n- **[Tensorboard Query](https://github.com/Alir3z4/tb-query)** - An MCP server for querying and analyzing TensorBoard event files.\n- **[Teradata](https://github.com/arturborycki/mcp-teradata)** - his MCP server enables LLMs to interact with Teradata databases. This MCP Server support tools and prompts for multi task data analytics\n- **[Terminal-Control](https://github.com/GongRzhe/terminal-controller-mcp)** - An MCP server that enables secure terminal command execution, directory navigation, and file system operations through a standardized interface.\n- **[Terraform-Cloud](https://github.com/severity1/terraform-cloud-mcp)** - An MCP server that integrates AI assistants with the Terraform Cloud API, allowing you to manage your infrastructure through natural conversation.\n- **[TFT-Match-Analyzer](https://github.com/GeLi2001/tft-mcp-server)** - MCP server for teamfight tactics match history & match details fetching, providing user the detailed context for every match.\n- **[Thales CDSP CAKM MCP Server](https://github.com/sanyambassi/thales-cdsp-cakm-mcp-server)** - An MCP server for the Thales CipherTrust Data Security Platform (CDSP) Cloud Key Management (CAKM) connector. This MCP server supports Ms SQL and Oracle databases.\n- **[Thales CDSP CRDP MCP Server](https://github.com/sanyambassi/thales-cdsp-crdp-mcp-server)** - A Model Context Protocol (MCP) server that allows interacting with the CipherTrust RestFul Data Protection (CRDP) data protection service.\n- **[Thales CipherTrust Manager MCP Server](https://github.com/sanyambassi/ciphertrust-manager-mcp-server)** - MCP server for Thales CipherTrust Manager integration, enabling secure key management and cryptographic operations.\n- **[thegraph-mcp](https://github.com/kukapay/thegraph-mcp)** - An MCP server that powers AI agents with indexed blockchain data from The Graph.\n- **[TheHive MCP Server](https://github.com/redwaysecurity/the-hive-mcp-server)** - An MCP server for [TheHive](https://strangebee.com/thehive/) Security Incident Response Platform.\n- **[Things3 MCP](https://github.com/urbanogardun/things3-mcp)** - Things3 task management integration for macOS with comprehensive TODO, project, and tag management.\n- **[Think MCP](https://github.com/Rai220/think-mcp)** - Enhances any agent's reasoning capabilities by integrating the think-tools, as described in [Anthropic's article](https://www.anthropic.com/engineering/claude-think-tool).\n- **[Think Node MCP](https://github.com/abhinav-mangla/think-tool-mcp)** - Enhances any agent's reasoning capabilities by integrating the think-tools, as described in [Anthropic's article](https://www.anthropic.com/engineering/claude-think-tool). (Works with Node)\n- **[Ticket-Generator MCP](https://github.com/trycon/ticket-generator-mcp)** - A Model Context Protocol (MCP) server implemented in Streamable HTTP transport that allows AI models to interact with the [Ticket Generator](https://ticket-generator.com/) APIs, enabling fetching active events lists, and generating tickets via 3 different modes.\n- **[Ticketmaster](https://github.com/delorenj/mcp-server-ticketmaster)** - Search for events, venues, and attractions through the Ticketmaster Discovery API\n- **[Ticketmaster MCP Server](https://github.com/mochow13/ticketmaster-mcp-server)** - A Model Context Protocol (MCP) server implemented in Streamable HTTP transport that allows AI models to interact with the Ticketmaster Discovery API, enabling searching events, venues, and attractions.\n- **[TickTick](https://github.com/alexarevalo9/ticktick-mcp-server)** - A Model Context Protocol (MCP) server designed to integrate with the TickTick task management platform, enabling intelligent context-aware task operations and automation.\n- **[Tideways](https://github.com/abuhamza/tideways-mcp-server)** - A Model Context Protocol server that enables AI assistants to query Tideways performance monitoring data and provide conversational performance insights for PHP applications.\n- **[TigerGraph](https://github.com/custom-discoveries/TigerGraph_MCP)** - A community built MCP server that interacts with TigerGraph Graph Database.\n- **[TikTok Ads](https://github.com/AdsMCP/tiktok-ads-mcp-server)** - An MCP server for interacting with TikTok advertising platforms for campaign management, performance analytics, audience targeting, creative management, and custom reporting.\n- **[time-mcp-nuget](https://github.com/domdomegg/time-mcp-nuget)** - Get current UTC time in RFC 3339 format (.NET/NuGet implementation).\n- **[time-mcp-pypi](https://github.com/domdomegg/time-mcp-pypi)** - Get current UTC time in RFC 3339 format (Python/PyPI implementation).\n- **[tip.md](https://github.com/tipdotmd#-mcp-server-for-ai-assistants)** - An MCP server that enables AI assistants to interact with tip.md's crypto tipping functionality, allowing agents or supporters to tip registered developers directly from AI chat interfaces.\n- **[TMD Earthquake](https://github.com/amornpan/tmd-earthquake-server-1.0)** - ğŸŒ Real-time earthquake monitoring from Thai Meteorological Department. Features magnitude filtering, location-based search (Thai/English), today's events tracking, dangerous earthquake alerts, and comprehensive statistics. Covers regional and global seismic activities.\n- **[TMDB](https://github.com/Laksh-star/mcp-server-tmdb)** - This MCP server integrates with The Movie Database (TMDB) API to provide movie information, search capabilities, and recommendations.\n- **[Todoist](https://github.com/abhiz123/todoist-mcp-server)** - Interact with Todoist to manage your tasks.\n- **[Todos](https://github.com/tomelliot/todos-mcp)** - A practical todo list manager to use with your favourite chatbot.\n- **[token-minter-mcp](https://github.com/kukapay/token-minter-mcp)** - An MCP server providing tools for AI agents to mint ERC-20 tokens across multiple blockchains.\n- **[token-revoke-mcp](https://github.com/kukapay/token-revoke-mcp)** - An MCP server for checking and revoking ERC-20 token allowances across multiple blockchains.\n- **[Ton Blockchain MCP](https://github.com/devonmojito/ton-blockchain-mcp)** - An MCP server for interacting with Ton Blockchain.\n- **[Topolograph MCP](https://github.com/Vadims06/topolograph-mcp-server)** â€“ A MCP server that enables LLMs to interact with OSPF and IS - IS protocols and analyze network topologies, query network events, and perform path calculations for OSPF and IS-IS protocols.\n- **[TouchDesigner](https://github.com/8beeeaaat/touchdesigner-mcp)** - An MCP server for TouchDesigner, enabling interaction with TouchDesigner projects, nodes, and parameters.\n- **[Transcribe](https://github.com/transcribe-app/mcp-transcribe)** - An MCP server provides fast and reliable transcriptions for audio/video files and voice memos. It allows LLMs to interact with the text content of audio/video file.\n- **[Travel Planner](https://github.com/GongRzhe/TRAVEL-PLANNER-MCP-Server)** - Travel planning and itinerary management server integrating with Google Maps API for location search, place details, and route calculations.\n- **[Trello MCP Server](https://github.com/lioarce01/trello-mcp-server)** - An MCP server that interact with user Trello boards, modifying them with prompting.\n- **[Trino](https://github.com/tuannvm/mcp-trino)** - A high-performance Model Context Protocol (MCP) server for Trino implemented in Go.\n- **[Tripadvisor](https://github.com/pab1it0/tripadvisor-mcp)** - An MCP server that enables LLMs to interact with Tripadvisor API, supporting location data, reviews, and photos through standardized MCP interfaces\n- **[Triplyfy MCP](https://github.com/helpful-AIs/triplyfy-mcp)** - An MCP server that lets LLMs plan and manage itineraries with interactive maps in Triplyfy; manage itineraries, places and notes, and search/save flights.\n- **[TrueNAS Core MCP](https://github.com/vespo92/TrueNasCoreMCP)** - An MCP server for interacting with TrueNAS Core.\n- **[TuriX Computer Automation MCP](https://github.com/TurixAI/TuriX-CUA/tree/mac_mcp)** - MCP server for helping automation control your computer complete your pre-setting task.\n- **[Tyk API Management](https://github.com/TykTechnologies/tyk-dashboard-mcp)** - Chat with all of your organization's managed APIs and perform other API lifecycle operations, managing tokens, users, analytics, and more.\n- **[Typesense](https://github.com/suhail-ak-s/mcp-typesense-server)** - A Model Context Protocol (MCP) server implementation that provides AI models with access to Typesense search capabilities. This server enables LLMs to discover, search, and analyze data stored in Typesense collections.\n- **[UniFi Dream Machine](https://github.com/sabler/mcp-unifi)** An MCP server that gets your network telemetry from the UniFi Site Manager and your local UniFi router.\n- **[UniProt](https://github.com/QuentinCody/uniprot-mcp-server)** - Unofficial MCP server for UniProt, providing access to protein sequence data, functional annotations, taxonomic information, and cross-references for proteomics and bioinformatics research.\n- **[uniswap-poolspy-mcp](https://github.com/kukapay/uniswap-poolspy-mcp)** - An MCP server that tracks newly created liquidity pools on Uniswap across nine blockchain networks.\n- **[uniswap-trader-mcp](https://github.com/kukapay/uniswap-trader-mcp)** -An MCP server for AI agents to automate token swaps on Uniswap DEX across multiple blockchains.\n- **[Unity Catalog](https://github.com/ognis1205/mcp-server-unitycatalog)** - An MCP server that enables LLMs to interact with Unity Catalog AI, supporting CRUD operations on Unity Catalog Functions and executing them as MCP tools.\n- **[Unity Integration (Advanced)](https://github.com/quazaai/UnityMCPIntegration)** - Advanced Unity3d Game Engine MCP which supports ,Execution of Any Editor Related Code Directly Inside of Unity, Fetch Logs, Get Editor State and Allow File Access of the Project making it much more useful in Script Editing or asset creation.\n- **[Unity MCP (AI Game Developer)](https://github.com/IvanMurzak/Unity-MCP)** - `Unity Editor` and `Unity Runtime` MCP integration. Unit Test, Coding, C# Roslyn, Reflection, Assets. Helps to create games with AI. And helps to run AI logic in the game in runtime. \n- **[Unity3d Game Engine](https://github.com/CoderGamester/mcp-unity)** - An MCP server that enables LLMs to interact with Unity3d Game Engine, supporting access to a variety of the Unit's Editor engine tools (e.g. Console Logs, Test Runner logs, Editor functions, hierarchy state, etc) and executing them as MCP tools or gather them as resources.\n- **[Universal MCP Servers](https://github.com/universal-mcp)** - A collection of MCP servers created using the [AgentR Universal MCP SDK](https://github.com/universal-mcp/universal-mcp).\n- **[Unleash Integration (Feature Toggle)](https://github.com/cuongtl1992/unleash-mcp)** - A Model Context Protocol (MCP) server implementation that integrates with Unleash Feature Toggle system. Provide a bridge between LLM applications and Unleash feature flag system\n- **[Upbit MCP Server](https://github.com/solangii/upbit-mcp-server)** â€“ An MCP server that enables real - time access to cryptocurrency prices, market summaries, and asset listings from the Upbit exchange.\n- **[USA Spending MCP Server](https://github.com/thsmale/usaspending-mcp-server)** â€“ This leverages the official source of government spending data [USASPENDING.gov](https://www.usaspending.gov/). Which enables one to track government spending over time, search government spending by agency, explore government spending to communities, and much more.\n- **[use_aws_mcp](https://github.com/runjivu/use_aws_mcp)** - amazon-q-cli's use_aws tool extracted into independent mcp, for general aws api usage.\n- **[User Feedback](https://github.com/mrexodia/user-feedback-mcp)** - Simple MCP Server to enable a human-in-the-loop workflow in tools like Cline and Cursor.\n- **[Useless Toolkit](https://uselesstoolkit.com/apis/mcp-servers)** - MCP-ready server endpoints for utility APIs, including Password Generator, IP2Geo etc., are provided by UselessToolkit.com, allowing seamless integration with AI agents via secure RapidAPI connections.\n- **[USPTO](https://github.com/riemannzeta/patent_mcp_server)** - MCP server for accessing United States Patent & Trademark Office data through its Open Data Protocol (ODP) API.\n- **[Vectara](https://github.com/vectara/vectara-mcp)** - Query Vectara's trusted RAG-as-a-service platform.\n- **[Vega-Lite](https://github.com/isaacwasserman/mcp-vegalite-server)** - Generate visualizations from fetched data using the VegaLite format and renderer.\n- **[Vertica](https://github.com/nolleh/mcp-vertica)** - Vertica database integration in Python with configurable access controls and schema inspection\n- **[Vibe Check](https://github.com/PV-Bhat/vibe-check-mcp-server)** - An MCP server leveraging an external oversight layer to \"vibe check\" agents, and also self-improve accuracy & user alignment over time. Prevents scope creep, code bloat, misalignment, misinterpretation, tunnel vision, and overcomplication.\n- **[Video Editor](https://github.com/burningion/video-editing-mcp)** - A Model Context Protocol Server to add, edit, and search videos with [Video Jungle](https://www.video-jungle.com/).\n- **[Video Still Capture](https://github.com/13rac1/videocapture-mcp)** - ğŸ“· Capture video stills from an OpenCV-compatible webcam or other video source.\n- **[Virtual location (Google Street View,etc.)](https://github.com/mfukushim/map-traveler-mcp)** - Integrates Google Map, Google Street View, PixAI, Stability.ai, ComfyUI API and Bluesky to provide a virtual location simulation in LLM (written in Effect.ts)\n- **[VMware Fusion](https://github.com/yeahdongcn/vmware-fusion-mcp-server)** - Manage VMware Fusion virtual machines via the Fusion REST API.\n- **[Voice Status Report](https://github.com/tomekkorbak/voice-status-report-mcp-server)** - An MCP server that provides voice status updates using OpenAI's text-to-speech API, to be used with Cursor or Claude Code.\n- **[VoiceMode](https://github.com/mbailey/voicemode)** - Enable voice conversations with Claude using any OpenAI-compatible STT/TTS service [getvoicemode.com](https://getvoicemode.com/)\n- **[VolcEngine TOS](https://github.com/dinghuazhou/sample-mcp-server-tos)** - A sample MCP server for VolcEngine TOS that flexibly get objects from TOS.\n- **[Voyp](https://github.com/paulotaylor/voyp-mcp)** - VOYP MCP server for making calls using Artificial Intelligence.\n- **[vscode-ai-model-detector](https://github.com/thisis-romar/vscode-ai-model-detector)** - Real-time AI model detection for VS Code Copilot with 100% accuracy. Enables proper git attribution by identifying active models (Claude, GPT, Gemini) via Chat Participant API.\n- **[vulnicheck](https://github.com/andrasfe/vulnicheck)** - Real-time Python package vulnerability scanner that checks dependencies against OSV and NVD databases, providing comprehensive security analysis with CVE details, lock file support, and actionable upgrade recommendations.\n- **[Wanaku MCP Router](https://github.com/wanaku-ai/wanaku/)** - The Wanaku MCP Router is a SSE-based MCP server that provides an extensible routing engine that allows integrating your enterprise systems with AI agents.\n- **[weather-mcp-server](https://github.com/devilcoder01/weather-mcp-server)** - Get real-time weather data for any location using weatherapi.\n- **[Web Search MCP](https://github.com/mrkrsl/web-search-mcp)** - A server that provides full web search, summaries and page extration for use with Local LLMs.\n- **[Webex](https://github.com/Kashyap-AI-ML-Solutions/webex-messaging-mcp-server)** - A Model Context Protocol (MCP) server that provides AI assistants with comprehensive access to Cisco Webex messaging capabilities.\n- **[Webflow](https://github.com/kapilduraphe/webflow-mcp-server)** - Interact with the Webflow APIs\n- **[webhook-mcp](https://github.com/noobnooc/webhook-mcp)** (by Nooc) - A Model Context Protocol (MCP) server that sends webhook notifications when called.\n- **[Wekan](https://github.com/namar0x0309/wekan-mcp)** - Unofficial MCP server for Wekan, providing all rest api functionality to add, edit, delete tasks and boards.\n- **[whale-tracker-mcp](https://github.com/kukapay/whale-tracker-mcp)**  -  A mcp server for tracking cryptocurrency whale transactions.\n- **[WhatsApp MCP Server](https://github.com/lharries/whatsapp-mcp)** - MCP server for your personal WhatsApp handling individuals, groups, searching and sending.\n- **[Whois MCP](https://github.com/bharathvaj-ganesan/whois-mcp)** - MCP server that performs whois lookup against domain, IP, ASN and TLD.\n- **[Withings](https://github.com/akutishevsky/withings-mcp)** - Access and analyze Withings health data including sleep analysis, body measurements, workouts, ECG recordings, and fitness goals through natural conversation.\n- **[Wikidata MCP](https://github.com/zzaebok/mcp-wikidata)** - Wikidata MCP server that interact with Wikidata, by searching identifiers, extracting metadata, and executing sparql query.\n- **[Wikidata SPARQL](https://github.com/QuentinCody/wikidata-sparql-mcp-server)** - Unofficial REMOTE MCP server for Wikidata's SPARQL endpoint, providing access to structured knowledge data, entity relationships, and semantic queries for research and data analysis.\n- **[Wikifunctions](https://github.com/Fredibau/wikifunctions-mcp-fredibau)** - Allowing AI models to discover and execute functions from the WikiFunctions library.\n- **[Wikipedia MCP](https://github.com/Rudra-ravi/wikipedia-mcp)** - Access and search Wikipedia articles via MCP for AI-powered information retrieval.\n- **[WildFly MCP](https://github.com/wildfly-extras/wildfly-mcp)** - WildFly MCP server that enables LLM to interact with running WildFly servers (retrieve metrics, logs, invoke operations, ...).\n- **[Windows CLI](https://github.com/SimonB97/win-cli-mcp-server)** - MCP server for secure command-line interactions on Windows systems, enabling controlled access to PowerShell, CMD, and Git Bash shells.\n- **[Windsor](https://github.com/windsor-ai/windsor_mcp)** - Windsor MCP (Model Context Protocol) enables your LLM to query, explore, and analyze your full-stack business data integrated into Windsor.ai with zero SQL writing or custom scripting.\n- **[Wordle MCP](https://github.com/cr2007/mcp-wordle-python)** - MCP Server that gets the Wordle Solution for a particular date.\n- **[WordPress MCP](https://github.com/Automattic/wordpress-mcp)** - Make your WordPress site into a simple MCP server, exposing functionality to LLMs and AI agents.\n- **[WordPress MCP Adapter](https://github.com/WordPress/mcp-adapter)** - An MCP adapter that bridges the Abilities API to the Model Context Protocol, enabling MCP clients to discover and invoke WordPress plugin, theme, and core abilities programmatically.\n- **[Workflowy](https://github.com/danield137/mcp-workflowy)** - A server that interacts with [workflowy](https://workflowy.com/).\n- **[World Bank data API](https://github.com/anshumax/world_bank_mcp_server)** - A server that fetches data indicators available with the World Bank as part of their data API\n- **[Wren Engine](https://github.com/Canner/wren-engine)** - The Semantic Engine for Model Context Protocol(MCP) Clients and AI Agents\n- **[X (Twitter)](https://github.com/EnesCinr/twitter-mcp)** (by EnesCinr) - Interact with twitter API. Post tweets and search for tweets by query.\n- **[X (Twitter)](https://github.com/vidhupv/x-mcp)** (by vidhupv) - Create, manage and publish X/Twitter posts directly through Claude chat.\n- **[Xcode](https://github.com/r-huijts/xcode-mcp-server)** - MCP server that brings AI to your Xcode projects, enabling intelligent code assistance, file operations, project management, and automated development tasks.\n- **[Xcode-mcp-server](https://github.com/drewster99/xcode-mcp-server)** (by drewster99) - Best Xcode integration - ClaudeCode and Cursor can build your project *with* Xcode and see the same errors you do. Fast easy setup.\n- **[xcodebuild](https://github.com/ShenghaiWang/xcodebuild)**  - ğŸ Build iOS Xcode workspace/project and feed back errors to llm.\n- **[Xero-mcp-server](https://github.com/john-zhang-dev/xero-mcp)** - Enabling clients to interact with Xero system for streamlined accounting, invoicing, and business operations.\n- **[Xero-mcp-server](https://github.com/XeroAPI/xero-mcp-server)** - Enabling clients to interact with Xero system for streamlined accounting, invoicing, and business operations.\n- **[XiYan](https://github.com/XGenerationLab/xiyan_mcp_server)** - ğŸ—„ï¸ An MCP server that supports fetching data from a database using natural language queries, powered by XiyanSQL as the text-to-SQL LLM.\n- **[XMind](https://github.com/apeyroux/mcp-xmind)** - Read and search through your XMind directory containing XMind files.\n- **[Yahoo Finance](https://github.com/AgentX-ai/yahoo-finance-server)** - ğŸ“ˆ Lets your AI interact with Yahoo Finance to get comprehensive stock market data, news, financials, and more. Proxy supported.\n- **[YetiBrowser MCP](https://github.com/yetidevworks/yetibrowser-mcp)** - A fully open-source implementation of the Browser MCP workflow with standout features such as optimized screenshots, dom diffs, console access, multi-websocket support + more.\n- **[yfinance](https://github.com/Adity-star/mcp-yfinance-server)** -ğŸ’¹The MCP YFinance Stock Server provides real-time and historical stock data in a standard format, powering dashboards, AI agents,and research tools with seamless financial insights.\n- **[YNAB](https://github.com/ChuckBryan/ynabmcpserver)** - A Model Context Protocol (MCP) server for integrating with YNAB (You Need A Budget), allowing AI assistants to securely access and analyze your financial data.\n- **[YouTrack](https://github.com/tonyzorin/youtrack-mcp)** - A Model Context Protocol (MCP) server implementation for JetBrains YouTrack, allowing AI assistants to interact with YouTrack issue tracking system.\n- **[YouTube](https://github.com/Klavis-AI/klavis/tree/main/mcp_servers/youtube)** - Extract Youtube video information (with proxies support).\n- **[YouTube](https://github.com/ZubeidHendricks/youtube-mcp-server)** - Comprehensive YouTube API integration for video management, Shorts creation, and analytics.\n- **[YouTube DLP](https://github.com/AgentX-ai/youtube-dlp-server)** - Retrieve video information, subtitles, and top comments with proxies.\n- **[YouTube MCP](https://github.com/aardeshir/youtube-mcp)** - Create playlists from song lists with OAuth2. Search videos, manage playlists, let AI curate your YouTube collections.\n- **[Youtube Uploader MCP](https://github.com/anwerj/youtube-uploader-mcp)** - AIâ€‘powered YouTube uploaderâ€”no CLI, no YouTube Studio.\n- **[YouTube Video Summarizer](https://github.com/nabid-pf/youtube-video-summarizer-mcp)** - Summarize lengthy youtube videos.\n- **[yutu](https://github.com/eat-pray-ai/yutu)** - A fully functional MCP server and CLI for YouTube to automate YouTube operation.\n- **[ZapCap](https://github.com/bogdan01m/zapcap-mcp-server)** - MCP server for ZapCap API providing video caption and B-roll generation via natural language\n- **[Zettelkasten](https://github.com/joshylchen/zettelkasten)**- Comprehensive AI-powered knowledge management system implementing the Zettelkasten method. Features atomic note creation, full-text search, AI-powered CEQRC workflows (Captureâ†’Explainâ†’Questionâ†’Refineâ†’Connect), intelligent link discovery, and multi-interface access (CLI, API, Web UI, MCP). Perfect for researchers, students, and knowledge workers.\n- **[ZincBind](https://github.com/QuentinCody/zincbind-mcp-server)** - Unofficial MCP server for ZincBind, providing access to a comprehensive database of zinc binding sites in proteins, structural coordination data, and metalloproteomics research information.\n- **[Zoom](https://github.com/Prathamesh0901/zoom-mcp-server/tree/main)** - Create, update, read and delete your zoom meetings.\n## ğŸ“š Frameworks\n\nThese are high-level frameworks that make it easier to build MCP servers or clients.\n\n### For servers\n\n* **[Anubis MCP](https://github.com/zoedsoupe/anubis-mcp)** (Elixir) - A high-performance and high-level Model Context Protocol (MCP) implementation in Elixir. Think like \"Live View\" for MCP.\n* **[ModelFetch](https://github.com/phuctm97/modelfetch/)** (TypeScript) - Runtime-agnostic SDK to create and deploy MCP servers anywhere TypeScript/JavaScript runs\n* **[EasyMCP](https://github.com/zcaceres/easy-mcp/)** (TypeScript)\n* **[FastAPI to MCP auto generator](https://github.com/tadata-org/fastapi_mcp)** â€“ A zero-configuration tool for automatically exposing FastAPI endpoints as MCP tools by **[Tadata](https://tadata.com/)**\n* **[FastMCP](https://github.com/punkpeye/fastmcp)** (TypeScript)\n* **[Foobara MCP Connector](https://github.com/foobara/mcp-connector)** - Easily expose Foobara commands written in Ruby as tools via MCP\n* **[Foxy Contexts](https://github.com/strowk/foxy-contexts)** â€“ A library to build MCP servers in Golang by **[strowk](https://github.com/strowk)**\n* **[Higress MCP Server Hosting](https://github.com/alibaba/higress/tree/main/plugins/wasm-go/mcp-servers)** - A solution for hosting MCP Servers by extending the API Gateway (based on Envoy) with wasm plugins.\n* **[MCP Declarative Java SDK](https://github.com/codeboyzhou/mcp-declarative-java-sdk)** Annotation-driven MCP servers development with Java, no Spring Framework Required, minimize dependencies as much as possible.\n* **[MCP-Framework](https://mcp-framework.com)** Build MCP servers with elegance and speed in TypeScript. Comes with a CLI to create your project with `mcp create app`. Get started with your first server in under 5 minutes by **[Alex Andru](https://github.com/QuantGeekDev)**\n* **[MCP Plexus](https://github.com/Super-I-Tech/mcp_plexus)**: A secure, **multi-tenant** and Multi-user MCP python server framework built to integrate easily with external services via OAuth 2.1, offering scalable and robust solutions for managing complex AI applications.\n* **[mcp_sse (Elixir)](https://github.com/kEND/mcp_sse)** An SSE implementation in Elixir for rapidly creating MCP servers.\n* **[mxcp](https://github.com/raw-labs/mxcp)** (Python) - Open-source framework for building enterprise-grade MCP servers using just YAML, SQL, and Python, with built-in auth, monitoring, ETL and policy enforcement.\n* **[Next.js MCP Server Template](https://github.com/vercel-labs/mcp-for-next.js)** (Typescript) - A starter Next.js project that uses the MCP Adapter to allow MCP clients to connect and access resources.\n* **[PayMCP](https://github.com/blustAI/paymcp)** (Python & TypeScript) - Lightweight payments layer for MCP servers: turn tools into paid endpoints with a two-line decorator. [PyPI](https://pypi.org/project/paymcp/) Â· [npm](https://www.npmjs.com/package/paymcp) Â· [TS repo](https://github.com/blustAI/paymcp-ts)\n* **[Perl SDK](https://github.com/mojolicious/mojo-mcp)** - An SDK for building MCP servers and clients with the Perl programming language.\n* **[Quarkus MCP Server SDK](https://github.com/quarkiverse/quarkus-mcp-server)** (Java)\n- **[R mcptools](https://github.com/posit-dev/mcptools)** - An R SDK for creating R-based MCP servers and retrieving functionality from third-party MCP servers as R functions.\n* **[SAP ABAP MCP Server SDK](https://github.com/abap-ai/mcp)** - Build SAP ABAP based MCP servers. ABAP 7.52 based with 7.02 downport; runs on R/3 & S/4HANA on-premises, currently not cloud-ready.\n* **[Spring AI MCP Server](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-server-boot-starter-docs.html)** - Provides auto-configuration for setting up an MCP server in Spring Boot applications.\n* **[Template MCP Server](https://github.com/mcpdotdirect/template-mcp-server)** - A CLI tool to create a new Model Context Protocol server project with TypeScript support, dual transport options, and an extensible structure\n* **[AgentR Universal MCP SDK](https://github.com/universal-mcp/universal-mcp)** - A python SDK to build MCP Servers with inbuilt credential management by **[Agentr](https://agentr.dev/home)**\n* **[Vercel MCP Adapter](https://github.com/vercel/mcp-adapter)** (TypeScript) - A simple package to start serving an MCP server on most major JS meta-frameworks including Next, Nuxt, Svelte, and more.\n* **[PHP MCP Server](https://github.com/php-mcp/server)** (PHP) - Core PHP implementation for the Model Context Protocol (MCP) server\n\n### For clients\n\n* **[codemirror-mcp](https://github.com/marimo-team/codemirror-mcp)** - CodeMirror extension that implements the Model Context Protocol (MCP) for resource mentions and prompt commands\n* **[llm-analysis-assistant](https://github.com/xuzexin-hz/llm-analysis-assistant)** <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/xuzexin-hz/llm-analysis-assistant/refs/heads/main/src/llm_analysis_assistant/pages/html/imgs/favicon.ico\" alt=\"Langfuse Logo\" /> - A very streamlined mcp client that supports calling and monitoring stdio/sse/streamableHttp, and can also view request responses through the /logs page. It also supports monitoring and simulation of ollama/openai interface.\n* **[MCP-Agent](https://github.com/lastmile-ai/mcp-agent)** - A simple, composable framework to build agents using Model Context Protocol by **[LastMile AI](https://www.lastmileai.dev)**\n* **[Spring AI MCP Client](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-client-boot-starter-docs.html)** - Provides auto-configuration for MCP client functionality in Spring Boot applications.\n* **[MCP CLI Client](https://github.com/vincent-pli/mcp-cli-host)** - A CLI host application that enables Large Language Models (LLMs) to interact with external tools through the Model Context Protocol (MCP).\n* **[OpenMCP Client](https://github.com/LSTM-Kirigaya/openmcp-client/)** - An all-in-one vscode/trae/cursor plugin for MCP server debugging. [Document](https://kirigaya.cn/openmcp/) & [OpenMCP SDK](https://kirigaya.cn/openmcp/sdk-tutorial/).\n* **[PHP MCP Client](https://github.com/php-mcp/client)** - Core PHP implementation for the Model Context Protocol (MCP) Client\n* **[Runbear](https://runbear.io/solutions/integrations/slack/mcp)** - No-code MCP client for team chat platforms, such as Slack, Microsoft Teams, and Discord.\n\n## ğŸ“š Resources\n\nAdditional resources on MCP.\n\n- **[A2A-MCP Java Bridge](https://github.com/vishalmysore/a2ajava)** - A2AJava brings powerful A2A-MCP integration directly into your Java applications. It enables developers to annotate standard Java methods and instantly expose them as MCP Server, A2A-discoverable actions â€” with no boilerplate or service registration overhead.\n- **[AiMCP](https://www.aimcp.info)** - A collection of MCP clients&servers to find the right mcp tools by **[Hekmon](https://github.com/hekmon8)**\n- **[Awesome Crypto MCP Servers by badkk](https://github.com/badkk/awesome-crypto-mcp-servers)** - A curated list of MCP servers by **[Luke Fan](https://github.com/badkk)**\n- **[Awesome MCP Servers by appcypher](https://github.com/appcypher/awesome-mcp-servers)** - A curated list of MCP servers by **[Stephen Akinyemi](https://github.com/appcypher)**\n- **[Awesome MCP Servers by punkpeye](https://github.com/punkpeye/awesome-mcp-servers)** (**[website](https://glama.ai/mcp/servers)**) - A curated list of MCP servers by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Awesome MCP Servers by wong2](https://github.com/wong2/awesome-mcp-servers)** (**[website](https://mcpservers.org)**) - A curated list of MCP servers by **[wong2](https://github.com/wong2)**\n- **[Awesome Remote MCP Servers by JAW9C](https://github.com/jaw9c/awesome-remote-mcp-servers)** - A curated list of **remote** MCP servers, including their authentication support by **[JAW9C](https://github.com/jaw9c)**\n- **[Discord Server](https://glama.ai/mcp/discord)** â€“ A community discord server dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[Discord Server (ModelContextProtocol)](https://discord.gg/jHEGxQu2a5)** â€“ Connect with developers, share insights, and collaborate on projects in an active Discord community dedicated to the Model Context Protocol by **[Alex Andru](https://github.com/QuantGeekDev)**\n- **[Install This MCP](https://installthismcp.com)** - Reduce Installation Friction with beautiful installation guides\n- <img height=\"12\" width=\"12\" src=\"https://raw.githubusercontent.com/klavis-ai/klavis/main/static/klavis-ai.png\" alt=\"Klavis Logo\" /> **[Klavis AI](https://www.klavis.ai)** - Open Source MCP Infra. Hosted MCP servers and MCP clients on Slack and Discord.\n- **[MCP Badges](https://github.com/mcpx-dev/mcp-badges)** â€“ Quickly highlight your MCP project with clear, eye-catching badges, by **[Ironben](https://github.com/nanbingxyz)**\n- <img height=\"12\" width=\"12\" src=\"https://mcpproxy.app/favicon.svg\" alt=\"MCPProxy Logo\" /> **[MCPProxy](https://github.com/smart-mcp-proxy/mcpproxy-go)** - Open-source local app that enables access to multiple MCP servers and thousands of tools with intelligent discovery via MCP protocol, runs servers in isolated environments, and features automatic quarantine protection against malicious tools.\n- **[MCPRepository.com](https://mcprepository.com/)** - A repository that indexes and organizes all MCP servers for easy discovery.\n- **[mcp-cli](https://github.com/wong2/mcp-cli)** - A CLI inspector for the Model Context Protocol by **[wong2](https://github.com/wong2)**\n- **[mcp-dockmaster](https://mcp-dockmaster.com)** - An Open-Sourced UI to install and manage MCP servers for Windows, Linux and macOS.\n- **[mcp-get](https://mcp-get.com)** - Command line tool for installing and managing MCP servers by **[Michael Latman](https://github.com/michaellatman)**\n- **[mcp-guardian](https://github.com/eqtylab/mcp-guardian)** - GUI application + tools for proxying / managing control of MCP servers by **[EQTY Lab](https://eqtylab.io)**\n- **[MCP Linker](https://github.com/milisp/mcp-linker)** - A cross-platform Tauri GUI tool for one-click setup and management of MCP servers, supporting Claude Desktop, Cursor, Windsurf, VS Code, Cline, and Neovim.\n- **[mcp-manager](https://github.com/zueai/mcp-manager)** - Simple Web UI to install and manage MCP servers for Claude Desktop by **[Zue](https://github.com/zueai)**\n- **[MCP Marketplace Web Plugin](https://github.com/AI-Agent-Hub/mcp-marketplace)** MCP Marketplace is a small Web UX plugin to integrate with AI applications, Support various MCP Server API Endpoint (e.g pulsemcp.com/deepnlp.org and more). Allowing user to browse, paginate and select various MCP servers by different categories. [Pypi](https://pypi.org/project/mcp-marketplace) | [Maintainer](https://github.com/AI-Agent-Hub) | [Website](http://www.deepnlp.org/store/ai-agent/mcp-server)\n- **[mcp.natoma.ai](https://mcp.natoma.ai)** â€“ A Hosted MCP Platform to discover, install, manage and deploy MCP servers by **[Natoma Labs](https://www.natoma.ai)**\n- **[mcp.run](https://mcp.run)** - A hosted registry and control plane to install & run secure + portable MCP Servers.\n- **[MCPHub](https://www.mcphub.com)** - Website to list high quality MCP servers and reviews by real users. Also provide online chatbot for popular LLM models with MCP server support.\n- **[MCP Router](https://mcp-router.net)** â€“ Free Windows and macOS app that simplifies MCP management while providing seamless app authentication and powerful log visualization by **[MCP Router](https://github.com/mcp-router/mcp-router)**\n- **[MCP Servers Hub](https://github.com/apappascs/mcp-servers-hub)** (**[website](https://mcp-servers-hub-website.pages.dev/)**) - A curated list of MCP servers by **[apappascs](https://github.com/apappascs)**\n- **[MCPServers.com](https://mcpservers.com)** - A growing directory of high-quality MCP servers with clear setup guides for a variety of MCP clients. Built by the team behind the **[Highlight MCP client](https://highlightai.com/)**\n- **[MCP Servers Rating and User Reviews](http://www.deepnlp.org/store/ai-agent/mcp-server)** - Website to rate MCP servers, write authentic user reviews, and [search engine for agent & mcp](http://www.deepnlp.org/search/agent)\n- **[MCP Sky](https://bsky.app/profile/brianell.in/feed/mcp)** - Bluesky feed for MCP related news and discussion by **[@brianell.in](https://bsky.app/profile/brianell.in)**\n- **[MCP X Community](https://x.com/i/communities/1861891349609603310)** â€“ A X community for MCP by **[Xiaoyi](https://x.com/chxy)**\n- **[MCPHub](https://github.com/Jeamee/MCPHub-Desktop)** â€“ An Open Source macOS & Windows GUI Desktop app for discovering, installing and managing MCP servers by **[Jeamee](https://github.com/jeamee)**\n- **[mcpm](https://github.com/pathintegral-institute/mcpm.sh)** ([website](https://mcpm.sh)) - MCP Manager (MCPM) is a Homebrew-like service for managing Model Context Protocol (MCP) servers across clients by **[Pathintegral](https://github.com/pathintegral-institute)**\n- **[MCPVerse](https://mcpverse.dev)** - A portal for creating & hosting authenticated MCP servers and connecting to them securely.\n- **[MCP Servers Search](https://github.com/atonomus/mcp-servers-search)** - An MCP server that provides tools for querying and discovering available MCP servers from this list.\n- **[Search MCP Server](https://github.com/krzysztofkucmierz/search-mcp-server)** - Recommends the most relevant MCP servers based on the client's query by searching this README file.\n- **[MCPWatch](https://github.com/kapilduraphe/mcp-watch)** - A comprehensive security scanner for Model Context Protocol (MCP) servers that detects vulnerabilities and security issues in your MCP server implementations.\n- <img height=\"12\" width=\"12\" src=\"https://mkinf.io/favicon-lilac.png\" alt=\"mkinf Logo\" /> **[mkinf](https://mkinf.io)** - An Open Source registry of hosted MCP Servers to accelerate AI agent workflows.\n- **[Open-Sourced MCP Servers Directory](https://github.com/chatmcp/mcp-directory)** - A curated list of MCP servers by **[mcpso](https://mcp.so)**\n- <img height=\"12\" width=\"12\" src=\"https://opentools.com/favicon.ico\" alt=\"OpenTools Logo\" /> **[OpenTools](https://opentools.com)** - An open registry for finding, installing, and building with MCP servers by **[opentoolsteam](https://github.com/opentoolsteam)**\n- **[Programmatic MCP Prototype](https://github.com/domdomegg/programmatic-mcp-prototype)** - Experimental agent prototype demonstrating programmatic MCP tool composition, progressive tool discovery, state persistence, and skill building through TypeScript code execution by **[Adam Jones](https://github.com/domdomegg)**\n- **[PulseMCP](https://www.pulsemcp.com)** ([API](https://www.pulsemcp.com/api)) - Community hub & weekly newsletter for discovering MCP servers, clients, articles, and news by **[Tadas Antanavicius](https://github.com/tadasant)**, **[Mike Coughlin](https://github.com/macoughl)**, and **[Ravina Patel](https://github.com/ravinahp)**\n- **[r/mcp](https://www.reddit.com/r/mcp)** â€“ A Reddit community dedicated to MCP by **[Frank Fiegel](https://github.com/punkpeye)**\n- **[r/modelcontextprotocol](https://www.reddit.com/r/modelcontextprotocol)** â€“ A Model Context Protocol community Reddit page - discuss ideas, get answers to your questions, network with like-minded people, and showcase your projects! by **[Alex Andru](https://github.com/QuantGeekDev)**\n- **[MCP.ing](https://mcp.ing/)** - A list of MCP services for discovering MCP servers in the community and providing a convenient search function for MCP services by **[iiiusky](https://github.com/iiiusky)**\n- **[MCP Hunt](https://mcp-hunt.com)** - Realtime platform for discovering trending MCP servers with momentum tracking, upvoting, and community discussions - like Product Hunt meets Reddit for MCP\n- **[Smithery](https://smithery.ai/)** - A registry of MCP servers to find the right tools for your LLM agents by **[Henry Mao](https://github.com/calclavia)**\n- **[Toolbase](https://gettoolbase.ai)** - Desktop application that manages tools and MCP servers with just a few clicks - no coding required by **[gching](https://github.com/gching)**\n- **[ToolHive](https://github.com/StacklokLabs/toolhive)** - A lightweight utility designed to simplify the deployment and management of MCP servers, ensuring ease of use, consistency, and security through containerization by **[StacklokLabs](https://github.com/StacklokLabs)**\n- **[NetMind](https://www.netmind.ai/AIServices)** - Access powerful AI services via simple APIs or MCP servers to supercharge your productivity.\n- **[Webrix MCP Gateway](https://github.com/webrix-ai/secure-mcp-gateway)** - Enterprise MCP gateway with SSO, RBAC, audit trails, and token vaults for secure, centralized AI agent access control. Deploy via Helm charts on-premise or in your cloud. [webrix.ai](https://webrix.ai)\n\n\n\n## ğŸš€ Getting Started\n\n### Using MCP Servers in this Repository\nTypeScript-based servers in this repository can be used directly with `npx`.\n\nFor example, this will start the [Memory](src/memory) server:\n```sh\nnpx -y @modelcontextprotocol/server-memory\n```\n\nPython-based servers in this repository can be used directly with [`uvx`](https://docs.astral.sh/uv/concepts/tools/) or [`pip`](https://pypi.org/project/pip/). `uvx` is recommended for ease of use and setup.\n\nFor example, this will start the [Git](src/git) server:\n```sh\n# With uvx\nuvx mcp-server-git\n\n# With pip\npip install mcp-server-git\npython -m mcp_server_git\n```\n\nFollow [these](https://docs.astral.sh/uv/getting-started/installation/) instructions to install `uv` / `uvx` and [these](https://pip.pypa.io/en/stable/installation/) to install `pip`.\n\n### Using an MCP Client\nHowever, running a server on its own isn't very useful, and should instead be configured into an MCP client. For example, here's the Claude Desktop configuration to use the above server:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-memory\"]\n    }\n  }\n}\n```\n\nAdditional examples of using the Claude Desktop as an MCP client might look like:\n\n```json\n{\n  \"mcpServers\": {\n    \"filesystem\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/path/to/allowed/files\"]\n    },\n    \"git\": {\n      \"command\": \"uvx\",\n      \"args\": [\"mcp-server-git\", \"--repository\", \"path/to/git/repo\"]\n    },\n    \"github\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-github\"],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n      }\n    },\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@modelcontextprotocol/server-postgres\", \"postgresql://localhost/mydb\"]\n    }\n  }\n}\n```\n\n## ğŸ› ï¸ Creating Your Own Server\n\nInterested in creating your own MCP server? Visit the official documentation at [modelcontextprotocol.io](https://modelcontextprotocol.io/introduction) for comprehensive guides, best practices, and technical details on implementing MCP servers.\n\n## ğŸ¤ Contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information about contributing to this repository.\n\n## ğŸ”’ Security\n\nSee [SECURITY.md](SECURITY.md) for reporting security vulnerabilities.\n\n## ğŸ“œ License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## ğŸ’¬ Community\n\n- [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions)\n\n## â­ Support\n\nIf you find MCP servers useful, please consider starring the repository and contributing new servers or improvements!\n\n---\n\nManaged by Anthropic, but built together with the community. The Model Context Protocol is open source and we encourage everyone to contribute their own servers and improvements!\n",
      "stars_today": 98
    },
    {
      "id": 830077101,
      "name": "wechat-article-exporter",
      "full_name": "wechat-article/wechat-article-exporter",
      "description": "ä¸€æ¬¾åœ¨çº¿çš„ å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ‰¹é‡ä¸‹è½½ å·¥å…·ï¼Œæ”¯æŒå¯¼å‡ºé˜…è¯»é‡ä¸è¯„è®ºæ•°æ®ï¼Œæ— éœ€æ­å»ºä»»ä½•ç¯å¢ƒï¼Œå¯é€šè¿‡ åœ¨çº¿ç½‘ç«™ ä½¿ç”¨ï¼Œæ”¯æŒ docker ç§æœ‰åŒ–éƒ¨ç½²å’Œ Cloudflare éƒ¨ç½²ã€‚  æ”¯æŒä¸‹è½½å„ç§æ–‡ä»¶æ ¼å¼ï¼Œå…¶ä¸­ HTML æ ¼å¼å¯100%è¿˜åŸæ–‡ç« æ’ç‰ˆä¸æ ·å¼ã€‚",
      "html_url": "https://github.com/wechat-article/wechat-article-exporter",
      "stars": 6530,
      "forks": 1122,
      "language": "TypeScript",
      "topics": [
        "download",
        "wechat",
        "wechat-article",
        "wechat-download"
      ],
      "created_at": "2024-07-17T14:41:12Z",
      "updated_at": "2026-01-28T01:49:34Z",
      "pushed_at": "2026-01-21T03:14:17Z",
      "open_issues": 13,
      "owner": {
        "login": "wechat-article",
        "avatar_url": "https://avatars.githubusercontent.com/u/195504668?v=4"
      },
      "readme": "<p align=\"center\">\n  <img src=\"./assets/logo.svg\" alt=\"Logo\">\n</p>\n\n# wechat-article-exporter\n\n![GitHub stars]\n![GitHub forks]\n![GitHub License]\n![Package Version]\n\n\nä¸€æ¬¾åœ¨çº¿çš„ **å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ‰¹é‡ä¸‹è½½** å·¥å…·ï¼Œæ”¯æŒå¯¼å‡ºé˜…è¯»é‡ä¸è¯„è®ºæ•°æ®ï¼Œæ— éœ€æ­å»ºä»»ä½•ç¯å¢ƒï¼Œå¯é€šè¿‡ [åœ¨çº¿ç½‘ç«™] ä½¿ç”¨ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒ docker ç§æœ‰åŒ–éƒ¨ç½²å’Œ Cloudflare éƒ¨ç½²ã€‚\n\næ”¯æŒä¸‹è½½å„ç§æ–‡ä»¶æ ¼å¼ï¼Œå…¶ä¸­ HTML æ ¼å¼å¯100%è¿˜åŸæ–‡ç« æ’ç‰ˆä¸æ ·å¼ã€‚\n\näº¤æµç¾¤(QQ): `991482155`\n\n## :bell: é‡è¦å‘ŠçŸ¥ï¼šé¡¹ç›®åŸŸåè°ƒæ•´\né¡¹ç›®åŸŸåè°ƒæ•´å¦‚ä¸‹ï¼š\n\n|     | ä¸‹è½½ç«™                            | æ–‡æ¡£ç«™                        |\n|-----|--------------------------------|----------------------------|\n| è°ƒæ•´å | https://down.mptext.top        | https://docs.mptext.top    |\n| è°ƒæ•´å‰ | https://exporter.wxdown.online | https://docs.wxdown.online |\n\nå…·ä½“ç»†èŠ‚å¯ä»¥æŸ¥çœ‹ [è¿™é‡Œ](https://docs.mptext.top/misc/domain.html)ã€‚\n\n\n## :books: å¦‚ä½•ä½¿ç”¨ï¼Ÿ\n\nè¯¥å·¥å…·çš„ä½¿ç”¨æ•™ç¨‹å·²ç§»è‡³ [æ–‡æ¡£ç«™ç‚¹](https://docs.mptext.top)ã€‚\n\n\n## :dart: ç‰¹æ€§\n\n- [x] æœç´¢å…¬ä¼—å·ï¼Œæ”¯æŒå…³é”®å­—æœç´¢\n- [x] æ”¯æŒå¯¼å‡º html/json/excel/txt/md/docx æ ¼å¼(html æ ¼å¼æ‰“åŒ…äº†å›¾ç‰‡å’Œæ ·å¼æ–‡ä»¶ï¼Œèƒ½å¤Ÿä¿è¯100%è¿˜åŸæ–‡ç« æ ·å¼)\n- [x] ç¼“å­˜æ–‡ç« åˆ—è¡¨æ•°æ®ï¼Œå‡å°‘æ¥å£è¯·æ±‚æ¬¡æ•°\n- [x] æ”¯æŒæ–‡ç« è¿‡æ»¤ï¼ŒåŒ…æ‹¬ä½œè€…ã€æ ‡é¢˜ã€å‘å¸ƒæ—¶é—´ã€åŸåˆ›æ ‡è¯†ã€æ‰€å±åˆé›†ç­‰\n- [x] æ”¯æŒåˆé›†ä¸‹è½½\n- [x] æ”¯æŒå›¾ç‰‡åˆ†äº«æ¶ˆæ¯\n- [x] æ”¯æŒè§†é¢‘åˆ†äº«æ¶ˆæ¯\n- [x] æ”¯æŒå¯¼å‡ºè¯„è®ºã€è¯„è®ºå›å¤ã€é˜…è¯»é‡ã€è½¬å‘é‡ç­‰æ•°æ® (éœ€è¦æŠ“åŒ…è·å– credentials ä¿¡æ¯ï¼Œ[æŸ¥çœ‹æ“ä½œæ­¥éª¤](https://docs.mptext.top/advanced/wxdown-service.html))\n- [x] æ”¯æŒ Docker éƒ¨ç½²\n- [x] æ”¯æŒ Cloudflare éƒ¨ç½²\n- [x] å¼€æ”¾ API æ¥å£\n\n\n## :heart: æ„Ÿè°¢\n\n- æ„Ÿè°¢ [Deno Deploy]ã€[Cloudflare Workers] æä¾›å…è´¹æ‰˜ç®¡æœåŠ¡\n- æ„Ÿè°¢ [WeChat_Article] é¡¹ç›®æä¾›åŸç†æ€è·¯\n\n\n## :star: æ”¯æŒ\n\nå¦‚æœä½ è§‰å¾—æœ¬é¡¹ç›®å¸®åŠ©åˆ°äº†ä½ ï¼Œè¯·ç»™ä½œè€…ä¸€ä¸ªå…è´¹çš„ Starï¼Œæ„Ÿè°¢ä½ çš„æ”¯æŒï¼\n\n\n## :bulb: åŸç†\n\nåœ¨å…¬ä¼—å·åå°å†™æ–‡ç« æ—¶æ”¯æŒæœç´¢å…¶ä»–å…¬ä¼—å·çš„æ–‡ç« åŠŸèƒ½ï¼Œä»¥æ­¤æ¥å®ç°æŠ“å–æŒ‡å®šå…¬ä¼—å·æ‰€æœ‰æ–‡ç« çš„ç›®çš„ã€‚\n\n\n## :memo: è®¸å¯\n\nMIT\n\n## :red_circle: å£°æ˜\n\næœ¬ç¨‹åºæ‰¿è¯ºï¼Œä¸ä¼šåˆ©ç”¨æ‚¨æ‰«ç ç™»å½•çš„å…¬ä¼—å·è¿›è¡Œä»»ä½•å½¢å¼çš„ç§æœ‰çˆ¬è™«ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸å­˜åœ¨æŠŠä½ çš„è´¦å·ä½œä¸ºå…¬å…±è´¦å·ä¸ºåˆ«äººçˆ¬å–æ–‡ç« çš„è¡Œä¸ºï¼Œä¹Ÿä¸å­˜åœ¨ç±»ä¼¼è´¦å·æ± çš„ä¸œè¥¿ã€‚\n\næ‚¨çš„å…¬ä¼—å·åªä¼šæœåŠ¡äºæ‚¨è‡ªå·±çš„æŠ“å–æ–‡ç« çš„ç›®çš„ã€‚\n\né€šè¿‡æœ¬ç¨‹åºè·å–çš„å…¬ä¼—å·æ–‡ç« å†…å®¹ï¼Œç‰ˆæƒå½’æ–‡ç« åŸä½œè€…æ‰€æœ‰ï¼Œè¯·åˆç†ä½¿ç”¨ã€‚è‹¥å‘ç°ä¾µæƒè¡Œä¸ºï¼Œè¯·è”ç³»æˆ‘ä»¬å¤„ç†ã€‚\n\n\n## :chart_with_upwards_trend: Star å†å²\n\n[![Star History Chart]][Star History Chart Link]\n\n\n\n<!-- Definitions -->\n\n[GitHub stars]: https://img.shields.io/github/stars/wechat-article/wechat-article-exporter?style=social&label=Star&style=plastic\n\n[GitHub forks]: https://img.shields.io/github/forks/wechat-article/wechat-article-exporter?style=social&label=Fork&style=plastic\n\n[GitHub License]: https://img.shields.io/github/license/wechat-article/wechat-article-exporter?label=License\n\n[Package Version]: https://img.shields.io/github/package-json/v/wechat-article/wechat-article-exporter\n\n\n[Deno Deploy]: https://deno.com/deploy\n\n[Cloudflare Workers]: https://workers.cloudflare.com\n\n[Wechat_Article]: https://github.com/1061700625/WeChat_Article\n\n[Star History Chart]: https://api.star-history.com/svg?repos=wechat-article/wechat-article-exporter&type=Timeline\n\n[Star History Chart Link]: https://star-history.com/#wechat-article/wechat-article-exporter&Timeline\n\n[åœ¨çº¿ç½‘ç«™]: https://down.mptext.top\n",
      "stars_today": 90
    },
    {
      "id": 1074785178,
      "name": "bentopdf",
      "full_name": "alam00000/bentopdf",
      "description": "A Privacy First PDF Toolkit",
      "html_url": "https://github.com/alam00000/bentopdf",
      "stars": 10839,
      "forks": 836,
      "language": "JavaScript",
      "topics": [
        "adobe-acrobat",
        "docker",
        "hacktoberfest",
        "javascript",
        "jpgtopdf",
        "pdf",
        "pdf-converter",
        "pdf-editor",
        "pdf-generation",
        "pdf-ocr",
        "pdf-tools",
        "pdf-viewer",
        "pdf-viewer-component",
        "pdffiller",
        "pdfjs",
        "privacy",
        "self-hosted",
        "self-hosting",
        "toolkit",
        "typescript"
      ],
      "created_at": "2025-10-12T13:30:08Z",
      "updated_at": "2026-01-28T01:19:12Z",
      "pushed_at": "2026-01-27T21:48:48Z",
      "open_issues": 111,
      "owner": {
        "login": "alam00000",
        "avatar_url": "https://avatars.githubusercontent.com/u/50314772?v=4"
      },
      "readme": "<p align=\"center\"><img src=\"public/images/favicon-no-bg.svg\" width=\"80\"></p>\n<h1 align=\"center\">BentoPDF</h1>\n<p align=\"center\">\n  <a href=\"https://www.digitalocean.com/?refcode=d93c189ef6d0&utm_campaign=Referral_Invite&utm_medium=Referral_Program&utm_source=badge\">\n    <img src=\"https://web-platforms.sfo2.cdn.digitaloceanspaces.com/WWW/Badge%203.svg\" alt=\"DigitalOcean Referral Badge\">\n  </a>\n</p>\n\n**BentoPDF** is a powerful, privacy-first, client-side PDF toolkit that is self hostable and allows you to manipulate, edit, merge, and process PDF files directly in your browser. No server-side processing is required, ensuring your files remain secure and private.\n\n![Docker Pulls](https://img.shields.io/docker/pulls/bentopdfteam/bentopdf) [![Ko-fi](https://img.shields.io/badge/Buy%20me%20a%20Coffee-yellow?logo=kofi&style=flat-square)](https://ko-fi.com/alio01) ![GitHub Stars](https://img.shields.io/github/stars/alam00000/bentopdf?style=social)\n[![Sponsor me on GitHub](https://img.shields.io/badge/Sponsor-%E2%9D%A4-ff69b4)](https://github.com/sponsors/alam00000)\n\n![BentoPDF Tools](public/images/bentopdf-tools.png)\n\n---\n\n## Table of Contents\n\n- [Join Us on Discord](#-join-us-on-discord)\n- [Documentation](#-documentation)\n- [Licensing](#-licensing)\n- [Stargazers over time](#-stargazers-over-time)\n- [Thank You to Our Sponsors](#-thank-you-to-our-sponsors)\n- [Why BentoPDF?](#-why-bentopdf)\n- [Features / Tools Supported](#ï¸-features--tools-supported)\n  - [Organize & Manage PDFs](#organize--manage-pdfs)\n  - [Edit & Modify PDFs](#edit--modify-pdfs)\n  - [Convert to PDF](#convert-to-pdf)\n  - [Convert from PDF](#convert-from-pdf)\n  - [Secure & Optimize PDFs](#secure--optimize-pdfs)\n- [Translations](#-translations)\n- [Getting Started](#-getting-started)\n  - [Prerequisites](#prerequisites)\n  - [Quick Start](#-quick-start)\n  - [Static Hosting](#static-hosting-using-netlify-vercel-and-github-pages)\n  - [Self-Hosting Locally](#-self-hosting-locally)\n  - [Docker Compose / Podman Compose](#-run-with-docker-compose--podman-compose-recommended)\n  - [Podman Quadlet](#-podman-quadlet-systemd-integration)\n  - [Simple Mode](#-simple-mode-for-internal-use)\n  - [Security Features](#-security-features)\n  - [Digital Signature CORS Proxy](#digital-signature-cors-proxy-required)\n  - [Version Management](#-version-management)\n  - [Development Setup](#-development-setup)\n- [Tech Stack & Background](#ï¸-tech-stack--background)\n- [Roadmap](#ï¸-roadmap)\n- [Contributing](#-contributing)\n- [Special Thanks](#special-thanks)\n\n---\n\n## ğŸ“¢ Join Us on Discord\n\n[![Discord](https://img.shields.io/badge/Discord-Join%20Server-7289da?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Bgq3Ay3f2w)\n\nHave questions, feature requests, or want to chat with the community? Join our Discord server!\n\n---\n\n## ğŸ“š Documentation\n\n[![Documentation](https://img.shields.io/badge/Docs-VitePress-646cff?style=for-the-badge&logo=vite&logoColor=white)](https://bentopdf.com/docs/)\n\nVisit our [Documentation](https://bentopdf.com/docs/) for:\n\n- **Getting Started** guide\n- **Tools Reference** (50+ tools)\n- **Self-Hosting** guides (Docker, Vercel, Netlify, Cloudflare, AWS, Hostinger, Nginx, Apache)\n- **Contributing** guide\n- **Commercial License** details\n\n---\n\n## ğŸ“œ Licensing\n\nBentoPDF is **dual-licensed** to fit your needs:\n\n| License        | Best For                                     | Price              |\n| -------------- | -------------------------------------------- | ------------------ |\n| **AGPL-3.0**   | Open-source projects with public source code | **Free**           |\n| **Commercial** | Proprietary / closed-source applications     | **$49** (lifetime) |\n\n<p align=\"center\">\n  <a href=\"https://buy.polar.sh/polar_cl_ThDfffbl733x7oAodcIryCzhlO57ZtcWPq6HJ1qMChd\">\n    <img src=\"https://img.shields.io/badge/ğŸš€_Get_Commercial_License-$49_Lifetime-6366f1?style=for-the-badge&labelColor=1f2937\" alt=\"Get Commercial License\">\n  </a>\n</p>\n\n> **One-time purchase** Â· **Unlimited devices & users** Â· **Lifetime updates** Â· **No AGPL obligations**\n\nğŸ“– For more details, see our [Licensing Page](https://bentopdf.com/licensing.html)\n\n### AGPL Components (Not Bundled)\n\nBentoPDF does **not** bundle AGPL-licensed processing libraries. The following components must be configured separately via **Advanced Settings** if you wish to use their features:\n\n| Component              | License  | Features Enabled                                                                                    |\n| ---------------------- | -------- | --------------------------------------------------------------------------------------------------- |\n| **PyMuPDF**            | AGPL-3.0 | PDF to Text/Markdown/SVG/DOCX, Extract Images/Tables, EPUB/MOBI/XPS conversion, Compression, Deskew |\n| **Ghostscript**        | AGPL-3.0 | PDF/A Conversion, Font to Outline                                                                   |\n| **CoherentPDF (CPDF)** | AGPL-3.0 | Merge, Split by Bookmarks, Table of Contents, PDF to/from JSON, Attachments                         |\n\n> **Why?** This separation ensures clear legal boundaries. Users who need these features can configure their own WASM sources or use our optional [WASM Proxy](cloudflare/WASM-PROXY.md) to load them from external URLs.\n\n**To enable these features:**\n\n1. Navigate to **Advanced Settings** in BentoPDF\n2. Configure the URL for each WASM module you need\n3. The modules will be loaded dynamically when required\n\n<hr>\n\n## â­ Stargazers over time\n\n[![Star History Chart](https://api.star-history.com/svg?repos=alam00000/bentopdf&type=Date)](https://star-history.com/#alam00000/bentopdf&Date)\n\n---\n\n## ğŸ’– Thank You to Our Sponsors\n\nWe're incredibly grateful to all our sponsors and supporters who help keep BentoPDF free and open source!\n\n[![Sponsor me on GitHub](https://img.shields.io/badge/Become%20a%20Sponsor-%E2%9D%A4-ff69b4?style=for-the-badge)](https://github.com/sponsors/alam00000)\n[![Buy me a Coffee](https://img.shields.io/badge/Buy%20me%20a%20Coffee-yellow?style=for-the-badge&logo=kofi)](https://ko-fi.com/alio01)\n\n<!-- sponsors -->\n<!-- sponsors -->\n\n---\n\n## âœ¨ Why BentoPDF?\n\n- **Privacy First**: All processing happens in your browser. Your files are never uploaded to a server, guaranteeing 100% privacy.\n- **No Limits**: Manipulate as many files as you want, as often you want. There are no restrictions or upload limits.\n- **High Performance**: Built with modern web technologies, BentoPDF is fast and efficient, handling even large PDF files with ease.\n- **Completely Free**: BentoPDF is a free and open-source tool for everyone.\n\n---\n\n## ğŸ› ï¸ Features / Tools Supported\n\nBentoPDF offers a comprehensive suite of tools to handle all your PDF needs.\n\n### Organize & Manage PDFs\n\n| Tool Name                    | Description                                                                                             |\n| :--------------------------- | :------------------------------------------------------------------------------------------------------ |\n| **Merge PDFs**               | Combine multiple PDF files into one. Preserves Bookmarks.                                               |\n| **Split PDFs**               | Extract specific pages or divide a document into smaller files.                                         |\n| **Organize Pages**           | Reorder, duplicate, or delete pages with a simple drag-and-drop interface.                              |\n| **Extract Pages**            | Save a specific range of pages as a new PDF.                                                            |\n| **Delete Pages**             | Remove unwanted pages from your document.                                                               |\n| **Rotate PDF**               | Rotate individual or all pages in a document.                                                           |\n| **Rotate by Custom Degrees** | Rotate pages by any custom angle.                                                                       |\n| **N-Up PDF**                 | Combine multiple pages onto a single page.                                                              |\n| **View PDF**                 | A powerful, integrated PDF viewer.                                                                      |\n| **Alternate & Mix Pages**    | Merge pages by alternating pages from each PDF. Preserves Bookmarks.                                    |\n| **Posterize PDF**            | Split a PDF into multiple smaller pages for print.                                                      |\n| **PDF Multi Tool**           | Merge, Split, Organize, Delete, Rotate, Add Blank Pages, Extract and Duplicate in an unified interface. |\n| **PDF Booklet**              | Rearrange pages for double-sided booklet printing. Fold and staple to create a booklet.                 |\n| **Add Attachments**          | Embed one or more files into your PDF.                                                                  |\n| **Extract Attachments**      | Extract all embedded files from PDF(s) as a ZIP.                                                        |\n| **Edit Attachments**         | View or remove attachments in your PDF.                                                                 |\n| **Divide Pages**             | Divide pages horizontally or vertically.                                                                |\n| **Combine to Single Page**   | Stitch all pages into one continuous scroll.                                                            |\n| **Add Blank Page**           | Insert an empty page anywhere in your PDF.                                                              |\n| **Reverse Pages**            | Flip the order of all pages in your document.                                                           |\n| **View Metadata**            | Inspect the hidden properties of your PDF.                                                              |\n| **PDFs to ZIP**              | Package multiple PDF files into a ZIP archive.                                                          |\n| **Compare PDFs**             | Compare two PDFs side by side.                                                                          |\n\n### Edit & Modify PDFs\n\n| Tool Name                 | Description                                                                                                                                                                                     |\n| :------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **PDF Editor**            | Annotate, highlight, redact, comment, add shapes/images, search, and view PDFs.                                                                                                                 |\n| **Create Fillable Forms** | Create professional fillable PDF forms with text fields, checkboxes, dropdowns, radio buttons, signatures, and more. Fully compliant with PDF standards for compatibility with all PDF viewers. |\n| **PDF Form Filler**       | Fill in forms directly in the browser. Also supports XFA forms.                                                                                                                                 |\n| **Add Page Numbers**      | Easily add page numbers with customizable formatting.                                                                                                                                           |\n| **Add Watermark**         | Add text or image watermarks to protect your documents.                                                                                                                                         |\n| **Header & Footer**       | Add customizable headers and footers.                                                                                                                                                           |\n| **Crop PDF**              | Crop specific pages or the entire document.                                                                                                                                                     |\n| **Deskew PDF**            | Automatically straighten tilted scanned pages using OpenCV.                                                                                                                                     |\n| **Font to Outline**       | Convert all fonts to vector outlines for consistent rendering across all devices.                                                                                                               |\n| **Invert Colors**         | Invert the colors of your PDF pages for better readability.                                                                                                                                     |\n| **Change Background**     | Modify the background color of your PDF.                                                                                                                                                        |\n| **Change Text Color**     | Change the color of text content within the PDF.                                                                                                                                                |\n| **Flatten PDF**           | Flatten form fields and annotations into static content.                                                                                                                                        |\n| **Remove Annotations**    | Remove comments, highlights, and other annotations.                                                                                                                                             |\n| **Remove Blank Pages**    | Auto detect and remove blank pages in a PDF.                                                                                                                                                    |\n| **Edit Bookmarks**        | Add, Edit, Create, Import and Export PDF Bookmarks.                                                                                                                                             |\n| **Add Stamps**            | Add image stamps to your PDF using the annotation toolbar.                                                                                                                                      |\n| **Table of Contents**     | Generate a table of contents page from PDF bookmarks.                                                                                                                                           |\n| **Redact Content**        | Permanently remove sensitive content from your PDFs.                                                                                                                                            |\n\n### Convert to PDF\n\n| Tool Name             | Description                                                                                            |\n| :-------------------- | :----------------------------------------------------------------------------------------------------- |\n| **Image to PDF**      | Convert JPG, PNG, BMP, GIF, TIFF, PNM, PGM, PBM, PPM, PAM, JXR, JPX, JP2, PSD, SVG, HEIC, WebP to PDF. |\n| **JPG to PDF**        | Convert JPG, JPEG, and JPEG2000 (JP2/JPX) images to PDF.                                               |\n| **PNG to PDF**        | Convert PNG images to PDF.                                                                             |\n| **WebP to PDF**       | Convert WebP images to PDF.                                                                            |\n| **SVG to PDF**        | Convert SVG images to PDF.                                                                             |\n| **BMP to PDF**        | Convert BMP images to PDF.                                                                             |\n| **HEIC to PDF**       | Convert HEIC images to PDF.                                                                            |\n| **TIFF to PDF**       | Convert TIFF images to PDF.                                                                            |\n| **PSD to PDF**        | Convert Adobe Photoshop (PSD) files to PDF.                                                            |\n| **Word to PDF**       | Convert Word documents (DOCX, DOC, ODT, RTF) to PDF.                                                   |\n| **Excel to PDF**      | Convert Excel spreadsheets (XLSX, XLS, ODS, CSV) to PDF.                                               |\n| **PowerPoint to PDF** | Convert PowerPoint presentations (PPTX, PPT, ODP) to PDF.                                              |\n| **ODT to PDF**        | Convert OpenDocument Text files to PDF.                                                                |\n| **ODS to PDF**        | Convert OpenDocument Spreadsheet (ODS) files to PDF.                                                   |\n| **ODP to PDF**        | Convert OpenDocument Presentation (ODP) files to PDF.                                                  |\n| **ODG to PDF**        | Convert OpenDocument Graphics (ODG) files to PDF.                                                      |\n| **RTF to PDF**        | Convert Rich Text Format documents to PDF.                                                             |\n| **CSV to PDF**        | Convert CSV spreadsheet files to PDF.                                                                  |\n| **Markdown to PDF**   | Write or paste Markdown and export it as a beautifully formatted PDF.                                  |\n| **Text to PDF**       | Convert plain text files into a PDF.                                                                   |\n| **JSON to PDF**       | Convert JSON files to PDF.                                                                             |\n| **XML to PDF**        | Convert XML documents to PDF.                                                                          |\n| **EPUB to PDF**       | Convert EPUB e-books to PDF.                                                                           |\n| **MOBI to PDF**       | Convert MOBI e-books to PDF.                                                                           |\n| **FB2 to PDF**        | Convert FictionBook (FB2) e-books to PDF.                                                              |\n| **CBZ to PDF**        | Convert comic book archives (CBZ/CBR) to PDF.                                                          |\n| **XPS to PDF**        | Convert XPS/OXPS documents to PDF.                                                                     |\n| **Email to PDF**      | Convert email files (EML, MSG) to PDF. Supports Outlook exports.                                       |\n| **Pages to PDF**      | Convert Apple Pages documents to PDF.                                                                  |\n| **WPD to PDF**        | Convert WordPerfect documents (WPD) to PDF.                                                            |\n| **WPS to PDF**        | Convert WPS Office documents to PDF.                                                                   |\n| **PUB to PDF**        | Convert Microsoft Publisher (PUB) files to PDF.                                                        |\n| **VSD to PDF**        | Convert Microsoft Visio (VSD, VSDX) files to PDF.                                                      |\n\n### Convert from PDF\n\n| Tool Name            | Description                                                                    |\n| :------------------- | :----------------------------------------------------------------------------- |\n| **PDF to Image**     | Convert PDF pages to JPG, PNG, WebP, BMP, or TIFF formats.                     |\n| **PDF to JPG**       | Convert each PDF page into a JPG image.                                        |\n| **PDF to PNG**       | Convert each PDF page into a PNG image.                                        |\n| **PDF to WebP**      | Convert each PDF page into a WebP image.                                       |\n| **PDF to BMP**       | Convert each PDF page into a BMP image.                                        |\n| **PDF to TIFF**      | Convert each PDF page into a TIFF image.                                       |\n| **PDF to SVG**       | Convert each page into a scalable vector graphic (SVG) for perfect quality.    |\n| **PDF to Greyscale** | Convert a color PDF into a black-and-white version.                            |\n| **PDF to Text**      | Extract text from PDF files and save as plain text (.txt).                     |\n| **PDF to JSON**      | Convert PDF files to JSON format.                                              |\n| **PDF to CSV**       | Extract tables from PDF and convert to CSV format.                             |\n| **PDF to Excel**     | Extract tables from PDF and convert to Excel (XLSX) format.                    |\n| **Extract Tables**   | Extract tables from PDF files and export as CSV, JSON, or Markdown.            |\n| **OCR PDF**          | Make scanned PDFs searchable and copyable using Optical Character Recognition. |\n\n### Secure & Optimize PDFs\n\n| Tool Name               | Description                                                                                                |\n| :---------------------- | :--------------------------------------------------------------------------------------------------------- |\n| **Compress PDF**        | Reduce file size while maintaining quality.                                                                |\n| **Repair PDF**          | Attempt to repair and recover data from a corrupted PDF.                                                   |\n| **Encrypt PDF**         | Add a password to protect your PDF from unauthorized access.                                               |\n| **Decrypt PDF**         | Remove password protection from a PDF (password required).                                                 |\n| **Change Permissions**  | Set or modify user permissions for printing, copying, and editing.                                         |\n| **Sign PDF**            | Draw, type, or upload your signature.                                                                      |\n| **Digital Signature**   | Add cryptographic digital signatures using X.509 certificates (PFX/PEM). Private key never leaves browser. |\n| **Validate Signature**  | Verify digital signatures, check certificate validity, and confirm document integrity.                     |\n| **Redact Content**      | Permanently remove sensitive content from your PDFs.                                                       |\n| **Edit Metadata**       | View and modify PDF metadata (author, title, keywords, etc.).                                              |\n| **Remove Metadata**     | Strip all metadata from your PDF for privacy.                                                              |\n| **Linearize PDF**       | Optimize PDF for fast web viewing.                                                                         |\n| **Sanitize PDF**        | Remove metadata, annotations, scripts, and more.                                                           |\n| **Fix Page Size**       | Standardize all pages to a uniform size.                                                                   |\n| **Page Dimensions**     | Analyze page size, orientation, and units.                                                                 |\n| **Remove Restrictions** | Remove password protection and security restrictions associated with digitally signed PDF files.           |\n\n---\n\n## ğŸŒ Translations\n\nBentoPDF is available in multiple languages:\n\n| Language            | Status                                                                                                                    |\n| ------------------- | ------------------------------------------------------------------------------------------------------------------------- |\n| English             | [![English](https://img.shields.io/badge/Complete-green?style=flat-square)](public/locales/en/common.json)                |\n| Chinese             | [![Chinese](https://img.shields.io/badge/Complete-green?style=flat-square)](public/locales/zh/common.json)                |\n| Traditional Chinese | [![Traditional Chinese](https://img.shields.io/badge/Complete-green?style=flat-square)](public/locales/zh-TW/common.json) |\n| French              | [![French](https://img.shields.io/badge/Complete-green?style=flat-square)](public/locales/fr/common.json)                 |\n| German              | [![German](https://img.shields.io/badge/Complete-green?style=flat-square)](public/locales/de/common.json)                 |\n| Indonesian          | [![Indonesian](https://img.shields.io/badge/Complete-green?style=flat-square)](public/locales/id/common.json)             |\n| Italian             | [![Italian](https://img.shields.io/badge/Complete-green?style=flat-square)](public/locales/it/common.json)                |\n| Portuguese          | [![Portuguese](https://img.shields.io/badge/Complete-green?style=flat-square)](public/locales/pt/common.json)             |\n| Turkish             | [![Turkish](https://img.shields.io/badge/Complete-green?style=flat-square)](public/locales/tr/common.json)                |\n| Vietnamese          | [![Vietnamese](https://img.shields.io/badge/Complete-green?style=flat-square)](public/locales/vi/common.json)             |\n\nWant to help translate BentoPDF into your language? Check out our [Translation Guide](TRANSLATION.md)!\n\n---\n\n## ğŸš€ Getting Started\n\nYou can run BentoPDF locally for development or personal use.\n\n### Prerequisites\n\n- [Node.js](https://nodejs.org/) (v18 or higher recommended)\n- [npm](https://www.npmjs.com/) (or yarn/pnpm)\n- [Docker](https://www.docker.com/) & [Docker Compose](https://docs.docker.com/compose/install/) (for containerized setup)\n\n### ğŸš€ Quick Start\n\nRun BentoPDF instantly from GitHub Container Registry (Recommended):\n\n```bash\ndocker run -p 3000:8080 ghcr.io/alam00000/bentopdf:latest\n```\n\nOpen your browser at: http://localhost:3000\n\n<details>\n<summary><b>Alternative: Using Docker Hub or Podman</b></summary>\n\n**Docker Hub:**\n\n```bash\ndocker run -p 3000:8080 bentopdfteam/bentopdf:latest\n```\n\n**Podman (GHCR):**\n\n```bash\npodman run -p 3000:8080 ghcr.io/alam00000/bentopdf:latest\n```\n\n**Podman (Docker Hub):**\n\n```bash\npodman run -p 3000:8080 docker.io/bentopdfteam/bentopdf:latest\n```\n\n> **Note:** All `docker` commands in this documentation work with Podman by replacing `docker` with `podman`.\n\n</details>\n\n### Static Hosting using Netlify, Vercel, and GitHub Pages\n\nIt is very straightforward to host your own instance of BentoPDF using a static web page hosting service. Plus, services such as Netlify, Vercel, and GitHub Pages all offer a free tier for getting started. See [Static Hosting](https://github.com/alam00000/bentopdf/blob/main/STATIC-HOSTING.md) for details.\n\n### ğŸ  Self-Hosting Locally\n\nSince BentoPDF is fully client-side, all processing happens in the user's browser and no server-side processing is required. This means you can host BentoPDF as simple static files on any web server or hosting platform.\n\n**Download from Releases (Recommended):**\n\nThe easiest way to self-host is to download the pre-built distribution file from our [GitHub releases](https://github.com/alam00000/bentopdf/releases). Each release includes a `dist-{version}.zip` file that contains all necessary files for self-hosting.\n\n1. Go to [BentoPDF Releases](https://github.com/alam00000/bentopdf/releases)\n2. Download the latest `dist-{version}.zip` file\n3. Extract the zip file\n4. Serve the extracted folder with your preferred web server\n\n**Serve the extracted folder (requires Node.js):**\n\n```bash\n# Navigate to the extracted folder\ncd dist-1.7.3  # Replace with your version\n\n# Start a local server\nnpx http-server -c-1\n```\n\nThe website will be accessible at: `http://localhost:8080/`\n\n> **Note:** The `-c-1` flag disables caching for development.\n\n**Build from Source (Advanced):**\n\nIf you prefer to build from source:\n\n```bash\n# Clone the repository\ngit clone https://github.com/alam00000/bentopdf.git\ncd bentopdf\n\n# Install dependencies\nnpm install\n\n# Build the project\nnpm run build\n\n# Package the distribution for hosting (optional)\nnpm run package\n\n# Preview the build locally\nnpm run preview\n\n# The website will be accessible at: http://localhost:4173/\n\n```\n\n**Compression Modes:**\n\nBentoPDF supports different compression modes for optimized builds:\n\n```bash\n# Gzip only (smallest Docker image size)\nnpm run build:gzip\ndocker build --build-arg COMPRESSION_MODE=g -t bentopdf:gzip .\n\n# Brotli only (best compression ratio)\nnpm run build:brotli\ndocker build --build-arg COMPRESSION_MODE=b -t bentopdf:brotli .\n\n# No compression (fastest build time)\nnpm run build:original\ndocker build --build-arg COMPRESSION_MODE=o -t bentopdf:original .\n\n# All formats (default, maximum browser compatibility)\nnpm run build:all\ndocker build --build-arg COMPRESSION_MODE=all -t bentopdf:all .\n```\n\n| Mode  | Files Kept  | Use Case                          |\n| ----- | ----------- | --------------------------------- |\n| `g`   | `.gz` only  | Standard nginx or minimal size    |\n| `b`   | `.br` only  | Modern CDN with Brotli support    |\n| `o`   | originals   | Development or custom compression |\n| `all` | all formats | Maximum compatibility (default)   |\n\n**CDN Optimization:**\n\nBentoPDF can use jsDelivr CDN to serve large WASM files (LibreOffice, Ghostscript, PyMuPDF) for improved performance and reduced bandwidth costs:\n\n```bash\n# Production build with CDN (Recommended)\nVITE_USE_CDN=true npm run build\n\n# Standard build with local files only\nnpm run build\n```\n\n**How it works:**\n\n- When `VITE_USE_CDN=true`: Browser loads WASM files from jsDelivr CDN (fast, global delivery)\n- Local files are **always included** as automatic fallback\n- If CDN fails then it falls back to local files\n\n**Subdirectory Hosting:**\n\nBentoPDF can also be hosted from a subdirectory (e.g., `example.com/tools/bentopdf/`):\n\n```bash\n\n# Example:\n# 1. Build the app with the specific BASE_URL. BASE_URL must have a trailing and leading slash. The BASE_URL can be any url of your choice. Here we are using /tools/bentopdf/ as an example.\n\nBASE_URL=/tools/bentopdf/ npm run build\n\n# 2. Create the nested directory structure inside serve-test (or any folder of your choice for local testing. In case of production, create the nested directory structure inside the root directory)\nmkdir -p serve-test/tools/bentopdf\n\n# 3. Copy all files from the 'dist' folder into that nested directory\ncp -r dist/* serve-test/tools/bentopdf/\n\n# 4. Serve the 'serve-test' folder\nnpx serve serve-test\n```\n\nThe website can be accessible at: `http://localhost:3000/tools/bentopdf/`\n\nThe `npm run package` command creates a `dist-{version}.zip` file that you can use for self-hosting.\n\n**Docker Subdirectory Deployment:**\n\nBentoPDF's Docker image also supports the `BASE_URL` build argument for subdirectory deployments:\n\n```bash\n# Build for subdirectory deployment\ndocker build --build-arg BASE_URL=/bentopdf/ -t bentopdf .\n\n# Run the container\ndocker run -p 3000:8080 bentopdf\n\n# The app will be accessible at http://localhost:3000/bentopdf/\n```\n\n**Combined with Simple Mode:**\n\n```bash\n# Build with both BASE_URL and SIMPLE_MODE\ndocker build \\\n  --build-arg BASE_URL=/tools/pdf/ \\\n  --build-arg SIMPLE_MODE=true \\\n  -t bentopdf-simple .\n\ndocker run -p 3000:8080 bentopdf-simple\n```\n\n> **Important**:\n>\n> - Always include trailing slashes in `BASE_URL` (e.g., `/bentopdf/` not `/bentopdf`)\n> - The default value is `/` for root deployment\n\n### ğŸš€ Run with Docker Compose / Podman Compose (Recommended)\n\nFor a more robust setup with auto-restart capabilities:\n\n1. **Download the repo and create a `docker-compose.yml` file or use the one given in repo**:\n\n```yaml\nservices:\n  bentopdf:\n    image: ghcr.io/alam00000/bentopdf:latest # Recommended\n    # image: bentopdfteam/bentopdf:latest     # Alternative: Docker Hub\n    container_name: bentopdf\n    ports:\n      - '3000:8080'\n    restart: unless-stopped\n```\n\n2. **Start the application**:\n\n```bash\n# Docker Compose\ndocker-compose up -d\n\n# Podman Compose\npodman-compose up -d\n```\n\nThe application will be available at `http://localhost:3000`.\n\n### ğŸ§ Podman Quadlet (Systemd Integration)\n\nFor Linux production deployments, you can run BentoPDF as a systemd service using [Podman Quadlet](https://docs.podman.io/en/latest/markdown/podman-systemd.unit.5.html).\n\nCreate `~/.config/containers/systemd/bentopdf.container`:\n\n```ini\n[Unit]\nDescription=BentoPDF - Privacy-first PDF toolkit\nAfter=network-online.target\n\n[Container]\nImage=ghcr.io/alam00000/bentopdf:latest\nContainerName=bentopdf\nPublishPort=3000:8080\nAutoUpdate=registry\n\n[Service]\nRestart=always\n\n[Install]\nWantedBy=default.target\n```\n\nThen enable and start:\n\n```bash\nsystemctl --user daemon-reload\nsystemctl --user enable --now bentopdf\n```\n\nFor detailed Quadlet configuration, see [Self-Hosting Docker Guide](https://bentopdf.com/docs/self-hosting/docker).\n\n### ğŸ¢ Simple Mode for Internal Use\n\nFor organizations that want a clean, distraction-free interface focused solely on PDF tools, BentoPDF supports a **Simple Mode** that hides all branding and marketing content.\n\n**What Simple Mode does:**\n\n- Hides navigation, hero section, features, FAQ, testimonials, and footer\n- Shows only the essential PDF tools\n- Updates page title to \"PDF Tools\"\n- Perfect for internal company tools and educational institutions\n\nFor more details, see [SIMPLE_MODE.md](SIMPLE_MODE.md).\n\n### ğŸ”’ Security Features\n\nBentoPDF runs as a non-root user using nginx-unprivileged for enhanced security:\n\n- **Non-Root Execution**: Container runs with minimal privileges using nginx-unprivileged\n- **Port 8080**: Uses high port number to avoid requiring root privileges\n- **Security Best Practices**: Follows Principle of Least Privilege\n\n#### Basic Usage\n\n```bash\ndocker build -t bentopdf .\ndocker run -p 8080:8080 bentopdf\n```\n\nFor detailed security configuration, see [SECURITY.md](SECURITY.md).\n\n### Digital Signature CORS Proxy (Required)\n\nThe **Digital Signature** tool uses a signing library that may need to fetch certificate chain data from certificate authority provider. Since many certificate servers don't include CORS headers, a proxy is required for this feature to work in the browser.\n\n**When is the proxy needed?**\n\n- Only when using the Digital Signature tool\n- Only if your certificate requires fetching issuer certificates from external URLs\n- Self-signed certificates typically don't need this\n\n**Deploying the CORS Proxy (Cloudflare Workers):**\n\n1. **Navigate to the cloudflare directory:**\n\n   ```bash\n   cd cloudflare\n   ```\n\n2. **Login to Cloudflare (if not already):**\n\n   ```bash\n   npx wrangler login\n   ```\n\n3. **Deploy the worker:**\n\n   ```bash\n   npx wrangler deploy\n   ```\n\n4. **Note your worker URL** (e.g., `https://bentopdf-cors-proxy.your-subdomain.workers.dev`)\n\n5. **Set the environment variable when building:**\n   ```bash\n   VITE_CORS_PROXY_URL=https://your-worker-url.workers.dev npm run build\n   ```\n\n#### Production Security Features\n\nThe CORS proxy includes several security measures:\n\n| Feature                 | Description                                                               |\n| ----------------------- | ------------------------------------------------------------------------- |\n| **URL Restrictions**    | Only allows certificate URLs (`.crt`, `.cer`, `.pem`, `/certs/`, `/ocsp`) |\n| **Private IP Blocking** | Blocks requests to localhost, 10.x, 192.168.x, 172.16-31.x                |\n| **File Size Limit**     | Rejects files larger than 10MB                                            |\n| **Rate Limiting**       | 60 requests per IP per minute (requires KV)                               |\n| **HMAC Signatures**     | Optional client-side signing (limited protection)                         |\n\n#### Enabling Rate Limiting (Recommended)\n\nRate limiting requires Cloudflare KV storage:\n\n```bash\ncd cloudflare\n\n# Create KV namespace\nnpx wrangler kv namespace create \"RATE_LIMIT_KV\"\n\n# Copy the returned ID and add to wrangler.toml:\n# [[kv_namespaces]]\n# binding = \"RATE_LIMIT_KV\"\n# id = \"YOUR_ID_HERE\"\n\n# Redeploy\nnpx wrangler deploy\n```\n\n**Free tier limits:** 100,000 reads/day, 1,000 writes/day (~300-500 signatures/day)\n\n#### HMAC Signature Verification (Optional)\n\n> **âš ï¸ Security Warning:** Client-side secrets can be extracted from bundled JavaScript. For production deployments with sensitive requirements, use your own backend server to proxy requests instead of embedding secrets in frontend code.\n\nBentoPDF uses client-side HMAC as a deterrent against casual abuse, but accepts this tradeoff due to its fully client-side architecture. To enable:\n\n```bash\n# Generate a secret\nopenssl rand -hex 32\n\n# Set on Cloudflare Worker\nnpx wrangler secret put PROXY_SECRET\n\n# Set in build environment\nVITE_CORS_PROXY_SECRET=your-secret npm run build\n```\n\n### ğŸ“¦ Version Management\n\nBentoPDF supports semantic versioning with multiple container tags available:\n\n**GitHub Container Registry (Recommended):**\n\n- **Latest**: `ghcr.io/alam00000/bentopdf:latest`\n- **Specific Version**: `ghcr.io/alam00000/bentopdf:1.0.0`\n- **Version with Prefix**: `ghcr.io/alam00000/bentopdf:v1.0.0`\n\n**Docker Hub:**\n\n- **Latest**: `bentopdfteam/bentopdf:latest`\n- **Specific Version**: `bentopdfteam/bentopdf:1.0.0`\n- **Version with Prefix**: `bentopdfteam/bentopdf:v1.0.0`\n\n#### Quick Release\n\n```bash\n# Release a patch version (0.0.1 â†’ 0.0.2)\nnpm run release\n\n# Release a minor version (0.0.1 â†’ 0.1.0)\nnpm run release:minor\n\n# Release a major version (0.0.1 â†’ 1.0.0)\nnpm run release:major\n```\n\nFor detailed release instructions, see [RELEASE.md](RELEASE.md).\n\n### ğŸš€ Development Setup\n\n#### Option 1: Run with npm\n\n1. **Clone the Repository**:\n\n   ```bash\n   git clone https://github.com/alam00000/bentopdf.git\n   cd bentopdf\n   ```\n\n2. **Install Dependencies**:\n\n   ```bash\n   npm install\n   ```\n\n3. **Run the Development Server**:\n   ```bash\n   npm run dev\n   ```\n   The application will be available at `http://localhost:5173`.\n\n#### Option 2: Build and Run with Docker Compose\n\n1. **Clone the Repository**:\n\n   ```bash\n   git clone https://github.com/alam00000/bentopdf.git\n   cd bentopdf\n   ```\n\n2. **Run with Docker Compose**:\n\n   ```bash\n   docker-compose -f docker-compose.dev.yml up -d\n   ```\n\n   The application will be available at `http://localhost:3000`.\n\n   > **Note:** After making any local changes to the code, rebuild the Docker image using:\n\n   ```bash\n   docker-compose -f docker-compose.dev.yml up --build -d\n   ```\n\n   This ensures your latest changes are applied inside the container.\n\n---\n\n## ğŸ› ï¸ Tech Stack & Background\n\nBentoPDF was originally built using **HTML**, **CSS**, and **vanilla JavaScript**. As the project grew, it was migrated to a modern stack for better maintainability and scalability:\n\n- **Vite**: A fast build tool for modern web development.\n- **TypeScript**: For type safety and an improved developer experience.\n- **Tailwind CSS**: For rapid and consistent UI development.\n\n> **Note:** Some parts of the codebase still use legacy structures from the original implementation. Contributors should expect gradual updates as testing and refactoring continue.\n\n---\n\n## ğŸ—ºï¸ Roadmap\n\n### Planned Features:\n\n- **HTML to PDF**: Convert HTML files or web pages into PDF documents.\n- **Markdown to PDF**: Enhanced support for converting `.md` files to PDF.\n- **Convert to PDF/A**: Convert PDFs to the PDF/A archival format.\n- **Edit PDF Content**: Directly edit text and other content within your PDF.\n- **PDF to Office**: Converts PDF files into editable Word, Excel, and PowerPoint formats.\n- **Office to PDF**: Converts Word, Excel, and PowerPoint documents into optimized PDFs.\n\nContributions and discussions on the roadmap are welcome! Join the conversation via [Discord](https://discord.gg/Bgq3Ay3f2w).\n\n---\n\n## ğŸ¤ Contributing\n\nWe welcome contributions from the community! Here's how you can get started:\n\n1.  **Fork the repository** and create your branch from `main`.\n2.  Follow the **Getting Started** steps to set up your local environment.\n3.  Make your changes and commit them with a clear message.\n4.  **Open a Pull Request** and describe the changes you've made.\n\nHave an idea for a new tool or an improvement? [Open an issue](https://github.com/alam00000/bentopdf/issues) to discuss it first.\n\n### ğŸ“– Contributing to Documentation\n\nOur documentation is built with [VitePress](https://vitepress.dev/). Here's how to contribute:\n\n```bash\n# Install dependencies\nnpm install\n\n# Start docs dev server\nnpm run docs:dev\n\n# Build docs for production\nnpm run docs:build\n\n# Preview the built docs\nnpm run docs:preview\n```\n\nDocumentation files are in the `docs/` folder:\n\n- `docs/index.md` - Home page\n- `docs/getting-started.md` - Getting started guide\n- `docs/tools/` - Tools reference\n- `docs/self-hosting/` - Self-hosting guides (Docker, Vercel, Netlify, Hostinger, etc.)\n- `docs/contributing.md` - Contributing guide\n- `docs/licensing.md` - Commercial license info\n\n---\n\n## Special Thanks\n\nBentoPDF wouldn't be possible without the amazing open-source tools and libraries that power it. We'd like to extend our heartfelt thanks to the creators and maintainers of:\n\n**Bundled Libraries:**\n\n- **[PDFLib.js](https://pdf-lib.js.org/)** â€“ For enabling powerful client-side PDF manipulation.\n- **[PDF.js](https://mozilla.github.io/pdf.js/)** â€“ For the robust PDF rendering engine in the browser.\n- **[PDFKit](https://pdfkit.org/)** â€“ For creating and editing PDFs with ease.\n- **[EmbedPDF](https://github.com/embedpdf/embed-pdf-viewer)** â€“ For seamless PDF editing in pure JS.\n- **[Cropper.js](https://fengyuanchen.github.io/cropperjs/)** â€“ For intuitive image cropping functionality.\n- **[Vite](https://vitejs.dev/)** â€“ For lightning-fast development and build tooling.\n- **[Tailwind CSS](https://tailwindcss.com/)** â€“ For rapid, flexible, and beautiful UI styling.\n- **[qpdf](https://github.com/qpdf/qpdf)** and **[qpdf-wasm](https://github.com/neslinesli93/qpdf-wasm)** â€“ For inspecting, repairing, and transforming PDF files.\n- **[LibreOffice](https://www.libreoffice.org/)** â€“ For powerful document conversion capabilities.\n\n**AGPL Libraries (Not Bundled - User Configured):**\n\n- **[CoherentPDF (cpdf)](https://www.coherentpdf.com/)** â€“ For content-preserving PDF operations. _(AGPL-3.0)_\n- **[PyMuPDF](https://github.com/pymupdf/PyMuPDF)** â€“ For high-performance PDF manipulation and data extraction. _(AGPL-3.0)_\n- **[Ghostscript (GhostPDL)](https://github.com/ArtifexSoftware/ghostpdl)** â€“ For PDF/A conversion and font outlining. _(AGPL-3.0)_\n\n> **Note:** AGPL-licensed libraries are not bundled with BentoPDF. Users can optionally configure these via Advanced Settings to enable additional features.\n\nYour work inspires and empowers developers everywhere. Thank you for making open-source amazing!\n",
      "stars_today": 87
    },
    {
      "id": 15634981,
      "name": "godot",
      "full_name": "godotengine/godot",
      "description": "Godot Engine â€“ Multi-platform 2D and 3D game engine",
      "html_url": "https://github.com/godotengine/godot",
      "stars": 105980,
      "forks": 24169,
      "language": "C++",
      "topics": [
        "game-development",
        "game-engine",
        "gamedev",
        "godot",
        "godotengine",
        "multi-platform",
        "open-source"
      ],
      "created_at": "2014-01-04T16:05:36Z",
      "updated_at": "2026-01-28T01:59:48Z",
      "pushed_at": "2026-01-27T21:28:22Z",
      "open_issues": 17313,
      "owner": {
        "login": "godotengine",
        "avatar_url": "https://avatars.githubusercontent.com/u/6318500?v=4"
      },
      "readme": "# Godot Engine\n\n<p align=\"center\">\n  <a href=\"https://godotengine.org\">\n    <img src=\"logo_outlined.svg\" width=\"400\" alt=\"Godot Engine logo\">\n  </a>\n</p>\n\n## 2D and 3D cross-platform game engine\n\n**[Godot Engine](https://godotengine.org) is a feature-packed, cross-platform\ngame engine to create 2D and 3D games from a unified interface.** It provides a\ncomprehensive set of [common tools](https://godotengine.org/features), so that\nusers can focus on making games without having to reinvent the wheel. Games can\nbe exported with one click to a number of platforms, including the major desktop\nplatforms (Linux, macOS, Windows), mobile platforms (Android, iOS), as well as\nWeb-based platforms and [consoles](https://godotengine.org/consoles).\n\n## Free, open source and community-driven\n\nGodot is completely free and open source under the very permissive [MIT license](https://godotengine.org/license).\nNo strings attached, no royalties, nothing. The users' games are theirs, down\nto the last line of engine code. Godot's development is fully independent and\ncommunity-driven, empowering users to help shape their engine to match their\nexpectations. It is supported by the [Godot Foundation](https://godot.foundation/)\nnot-for-profit.\n\nBefore being open sourced in [February 2014](https://github.com/godotengine/godot/commit/0b806ee0fc9097fa7bda7ac0109191c9c5e0a1ac),\nGodot had been developed by [Juan Linietsky](https://github.com/reduz) and\n[Ariel Manzur](https://github.com/punto-) for several years as an in-house\nengine, used to publish several work-for-hire titles.\n\n![Screenshot of a 3D scene in the Godot Engine editor](https://raw.githubusercontent.com/godotengine/godot-design/master/screenshots/editor_tps_demo_1920x1080.jpg)\n\n## Getting the engine\n\n### Binary downloads\n\nOfficial binaries for the Godot editor and the export templates can be found\n[on the Godot website](https://godotengine.org/download).\n\n### Compiling from source\n\n[See the official docs](https://docs.godotengine.org/en/latest/engine_details/development/compiling)\nfor compilation instructions for every supported platform.\n\n## Community and contributing\n\nGodot is not only an engine but an ever-growing community of users and engine\ndevelopers. The main community channels are listed [on the homepage](https://godotengine.org/community).\n\nThe best way to get in touch with the core engine developers is to join the\n[Godot Contributors Chat](https://chat.godotengine.org).\n\nTo get started contributing to the project, see the [contributing guide](CONTRIBUTING.md).\nThis document also includes guidelines for reporting bugs.\n\n## Documentation and demos\n\nThe official documentation is hosted on [Read the Docs](https://docs.godotengine.org).\nIt is maintained by the Godot community in its own [GitHub repository](https://github.com/godotengine/godot-docs).\n\nThe [class reference](https://docs.godotengine.org/en/latest/classes/)\nis also accessible from the Godot editor.\n\nWe also maintain official demos in their own [GitHub repository](https://github.com/godotengine/godot-demo-projects)\nas well as a list of [awesome Godot community resources](https://github.com/godotengine/awesome-godot).\n\nThere are also a number of other\n[learning resources](https://docs.godotengine.org/en/latest/community/tutorials.html)\nprovided by the community, such as text and video tutorials, demos, etc.\nConsult the [community channels](https://godotengine.org/community)\nfor more information.\n\n[![Code Triagers Badge](https://www.codetriage.com/godotengine/godot/badges/users.svg)](https://www.codetriage.com/godotengine/godot)\n[![Translate on Weblate](https://hosted.weblate.org/widgets/godot-engine/-/godot/svg-badge.svg)](https://hosted.weblate.org/engage/godot-engine/?utm_source=widget)\n",
      "stars_today": 78
    },
    {
      "id": 781685192,
      "name": "flowsurface",
      "full_name": "flowsurface-rs/flowsurface",
      "description": "A native desktop charting platform for crypto markets",
      "html_url": "https://github.com/flowsurface-rs/flowsurface",
      "stars": 1304,
      "forks": 217,
      "language": "Rust",
      "topics": [
        "cryptocurrency",
        "iced",
        "orderbook-tick-data"
      ],
      "created_at": "2024-04-03T21:03:33Z",
      "updated_at": "2026-01-28T01:32:04Z",
      "pushed_at": "2026-01-27T10:14:56Z",
      "open_issues": 10,
      "owner": {
        "login": "flowsurface-rs",
        "avatar_url": "https://avatars.githubusercontent.com/u/235447981?v=4"
      },
      "readme": "# Flowsurface\n\n[![Crates.io](https://img.shields.io/crates/v/flowsurface)](https://crates.io/crates/flowsurface)\n[![Lint](https://github.com/flowsurface-rs/flowsurface/actions/workflows/lint.yml/badge.svg)](https://github.com/flowsurface-rs/flowsurface/actions/workflows/lint.yml)\n[![Format](https://github.com/flowsurface-rs/flowsurface/actions/workflows/format.yml/badge.svg)](https://github.com/flowsurface-rs/flowsurface/actions/workflows/format.yml)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://github.com/flowsurface-rs/flowsurface/blob/main/LICENSE)\n[![Made with iced](https://iced.rs/badge.svg)](https://github.com/iced-rs/iced)\n\nAn experimental open-source desktop charting application. Supports Binance, Bybit, Hyperliquid and OKX\n\n<div align=\"center\">\n  <img\n    src=\"https://github.com/user-attachments/assets/baddc444-e079-48e5-82b2-4f97094eba07\"\n    alt=\"Flowsurface screenshot\"\n    style=\"max-width: 100%; height: auto;\"\n  />\n</div>\n\n### Key Features\n\n-   Multiple chart/panel types:\n    -   **Heatmap (Historical DOM):** Uses live trades and L2 orderbook to create a time-series heatmap chart. Supports customizable price grouping, different time aggregations, fixed or visible range volume profiles.\n    -   **Candlestick:** Traditional kline chart supporting both time-based and custom tick-based intervals.\n    -   **Footprint:** Price grouped and interval aggregated views for trades on top of a candlestick chart. Supports different clustering methods, configurable imbalance and naked-POC studies.\n    -   **Time & Sales:** Scrollable list of live trades.\n    -   **DOM (Depth of Market) / Ladder:** Displays current L2 orderbook alongside recent trade volumes on grouped price levels.\n    -   **Comparison:** Line graph for comparing multiple data sources, normalized by kline `close` prices on a percentage scale\n-   Real-time sound effects driven by trade streams\n-   Multi window/monitor support\n-   Pane linking for quickly switching tickers across multiple panes\n-   Persistent layouts and customizable themes with editable color palettes\n\n##### Market data is received directly from exchanges' public REST APIs and WebSockets\n\n#\n\n#### Historical Trades on Footprint Charts:\n\n-   By default, it captures and plots live trades in real time via WebSocket.\n-   For Binance tickers, you can optionally backfill the visible time range by enabling trade fetching in the settings:\n    -   [data.binance.vision](https://data.binance.vision/): Fast daily bulk downloads (no intraday).\n    -   REST API (e.g., `/fapi/v1/aggTrades`): Slower, paginated intraday fetching (subject to rate limits).\n    -   The Binance connector can use either or both methods to retrieve historical data as needed.\n-   Fetching trades for Bybit/Hyperliquid is not supported, as both lack a suitable REST API. OKX is WIP.\n\n## Installation\n\n### Method 1: Prebuilt Binaries\n\nStandalone executables are available for Windows, macOS, and Linux on the [Releases page](https://github.com/flowsurface-rs/flowsurface/releases).\n\n<details>\n<summary><strong>Having trouble running the file? (Permission/Security warnings)</strong></summary>\n \nSince these binaries are currently unsigned they might get flagged.\n\n-   **Windows**: If you see a \"Windows protected your PC\" pop-up, click **More info** -> **Run anyway**.\n-   **macOS**: If you see \"Developer cannot be verified\", control-click (right-click) the app and select **Open**, or go to _System Settings > Privacy & Security_ to allow it.\n</details>\n\n### Method 2: Build from Source\n\n#### Requirements\n\n-   [Rust toolchain](https://www.rust-lang.org/tools/install)\n-   [Git version control system](https://git-scm.com/)\n-   System dependencies:\n    -   **Linux**:\n        -   Debian/Ubuntu: `sudo apt install build-essential pkg-config libasound2-dev`\n        -   Arch: `sudo pacman -S base-devel alsa-lib`\n        -   Fedora: `sudo dnf install gcc make alsa-lib-devel`\n    -   **macOS**: Install Xcode Command Line Tools: `xcode-select --install`\n    -   **Windows**: No additional dependencies required\n\n#### Option A: `cargo install`\n\n```bash\n# Install latest globally\ncargo install --git https://github.com/flowsurface-rs/flowsurface flowsurface\n\n# Run\nflowsurface\n```\n\n#### Option B: Cloning the repo\n\n```bash\n# Clone the repository\ngit clone https://github.com/flowsurface-rs/flowsurface\n\ncd flowsurface\n\n# Build and run\ncargo build --release\ncargo run --release\n```\n\n### Credits and thanks to\n\n-   [Kraken Desktop](https://www.kraken.com/desktop) (formerly [Cryptowatch](https://blog.kraken.com/product/cryptowatch-to-sunset-kraken-pro-to-integrate-cryptowatch-features)), the main inspiration that sparked this project\n-   [Halloy](https://github.com/squidowl/halloy), an excellent open-source reference for the foundational code design and the project architecture\n-   And of course, [iced](https://github.com/iced-rs/iced), the GUI library that makes all of this possible\n",
      "stars_today": 70
    },
    {
      "id": 292065578,
      "name": "czkawka",
      "full_name": "qarmin/czkawka",
      "description": "Multi functional app to find duplicates, empty folders, similar images etc.",
      "html_url": "https://github.com/qarmin/czkawka",
      "stars": 28850,
      "forks": 948,
      "language": "Rust",
      "topics": [
        "cleaner",
        "duplicates",
        "gtk-rs",
        "multiplatform",
        "rust",
        "similar-images",
        "similar-music",
        "similar-videos"
      ],
      "created_at": "2020-09-01T17:37:29Z",
      "updated_at": "2026-01-28T01:58:48Z",
      "pushed_at": "2026-01-27T21:41:20Z",
      "open_issues": 426,
      "owner": {
        "login": "qarmin",
        "avatar_url": "https://avatars.githubusercontent.com/u/41945903?v=4"
      },
      "readme": "![krokiet_logo](https://github.com/user-attachments/assets/567a7a38-d754-4a79-86b5-3cc898dbbade)\n\n**Krokiet** ((IPA: [ËˆkrÉ”cÉ›t]), \"croquette\" in Polish) new generation GUI frontend, simple, multiplatform, fast and free app to remove unnecessary files from your computer.\n\n![czkawka_logo](https://user-images.githubusercontent.com/41945903/102616149-66490400-4137-11eb-9cd6-813b2b070834.png)\n\n**Czkawka** (_tchâ€¢kavâ€¢ka_ (IPA: [ËˆÊ§Ì‘kafka]), \"hiccup\" in Polish) older gtk4 GUI frontend, superseded by Krokiet, but still receiving bugfix updates.\n\n## Features\n\n- Written in memory-safe Rust - almost 100% unsafe code free\n- Amazingly fast - due to using more or less advanced algorithms and multithreading\n- Free, Open Source without ads\n- Multiplatform - works on Linux, Windows, macOS, FreeBSD and many more\n- Cache support - second and further scans should be much faster than the first one\n- CLI frontend - for easy automation\n- GUI frontend - uses Slint or GTK 4 frameworks\n- Core library - allows to reuse functionality in other apps\n- No spying - Czkawka does not have access to the Internet, nor does it collect any user information or statistics\n- Multilingual - support multiple languages like Polish, English or Italian\n- Multiple tools to use:\n    - Duplicates - Finds duplicates based on file name, size or hash\n    - Empty Folders - Finds empty folders with the help of an advanced algorithm\n    - Big Files - Finds the provided number of the biggest files in given location\n    - Empty Files - Looks for empty files across the drive\n    - Temporary Files - Finds temporary files\n    - Similar Images - Finds images which are not exactly the same (different resolution, watermarks)\n    - Similar Videos - Looks for visually similar videos\n    - Same Music - Searches for similar music by tags or by reading content and comparing it\n    - Invalid Symbolic Links - Shows symbolic links which point to non-existent files/directories\n    - Broken Files - Finds files that are invalid or corrupted\n    - Bad Extensions - Lists files whose content not match with their extension\n    - Exif Remover - Removes Exif metadata from various file types\n    - Video Optimizer - Crops from static parts and converts videos to more efficient formats\n    - Bad Names - Finds files with names that may be not wanted (e.g., containing special characters)\n\n![Krokiet](https://github.com/user-attachments/assets/720e98c3-598a-41aa-a04b-0c0c1d8a28e6)\n\n![Czkawka](https://github.com/user-attachments/assets/b0409515-1bec-4e13-8fac-7bdfa15f5848)\n\nChangelog about each version can be found in [CHANGELOG.md](Changelog.md).\n\nNew releases can be found in [Github releases](https://github.com/qarmin/czkawka/releases) and nightly builds also in [Nightly releases](https://github.com/qarmin/czkawka/releases/tag/Nightly)\n\n## Usage, installation, compilation, requirements, license\n\nEach tool uses different technologies, so you can find instructions for each of them in the appropriate file:\n\n- [Krokiet GUI (Slint frontend)](krokiet/README.md)</br>\n- [Czkawka GUI (GTK frontend)](czkawka_gui/README.md)</br>\n- [Czkawka CLI](czkawka_cli/README.md)</br>\n- [Czkawka Core](czkawka_core/README.md)</br>\n\n## Comparison to other tools\n\nBleachbit is a master at finding and removing temporary files, while Czkawka only finds the most basic ones. So these\ntwo apps shouldn't be compared directly or be considered as an alternative to one another.\n\nIn this comparison remember, that even if app have same features they may work different(e.g. one app may have more\noptions to choose than other).\n\n|                           |     Czkawka      |   Krokiet   | FSlint |     DupeGuru      |  Bleachbit  |\n|:-------------------------:|:----------------:|:-----------:|:------:|:-----------------:|:-----------:|\n|         Language          |       Rust       |    Rust     | Python |   Python/Obj-C    |   Python    |\n|  Framework base language  |        C         |    Rust     |   C    | C/C++/Obj-C/Swift |      C      |\n|         Framework         |      GTK 4       |    Slint    | PyGTK2 | Qt 5 (PyQt)/Cocoa |   PyGTK3    |\n|            OS             |   Lin,Mac,Win    | Lin,Mac,Win |  Lin   |    Lin,Mac,Win    | Lin,Mac,Win |\n|     Duplicate finder      |        âœ”         |      âœ”      |   âœ”    |         âœ”         |             |\n|        Empty files        |        âœ”         |      âœ”      |   âœ”    |                   |             |\n|       Empty folders       |        âœ”         |      âœ”      |   âœ”    |                   |             |\n|      Temporary files      |        âœ”         |      âœ”      |   âœ”    |                   |      âœ”      |\n|         Big files         |        âœ”         |      âœ”      |        |                   |             |\n|      Similar images       |        âœ”         |      âœ”      |        |         âœ”         |             |\n|      Similar videos       |        âœ”         |      âœ”      |        |                   |             |\n|  Music duplicates(tags)   |        âœ”         |      âœ”      |        |         âœ”         |             |\n| Music duplicates(content) |        âœ”         |      âœ”      |        |                   |             |\n|     Invalid symlinks      |        âœ”         |      âœ”      |   âœ”    |                   |             |\n|       Broken files        |        âœ”         |      âœ”      |        |                   |             |\n| Invalid names/extensions  |        âœ”         |      âœ”      |   âœ”    |                   |             |\n|       Exif cleaner        |                  |      âœ”      |        |                   |             |\n|      Video optimizer      |                  |      âœ”      |        |                   |             |\n|         Bad Names         |                  |      âœ”      |        |                   |             |\n|      Names conflict       |                  |             |   âœ”    |                   |             |\n|    Installed packages     |                  |             |   âœ”    |                   |             |\n|          Bad ID           |                  |             |   âœ”    |                   |             |\n|   Non stripped binaries   |                  |             |   âœ”    |                   |             |\n|   Redundant whitespace    |                  |             |   âœ”    |                   |             |\n|     Overwriting files     |                  |             |   âœ”    |                   |      âœ”      |\n|     Portable version      |        âœ”         |      âœ”      |        |                   |      âœ”      |\n|    Multiple languages     |        âœ”         |      âœ”      |   âœ”    |         âœ”         |      âœ”      |\n|       Cache support       |        âœ”         |      âœ”      |        |         âœ”         |             |\n|   In active development   | Yes<sup>**</sup> |     Yes     |   No   |  No<sup>*</sup>   |     Yes     |\n\n<p><sup>*</sup> Few small commits added recently and last version released in 2023</p> \n<p><sup>**</sup> Czkawka GTK is in maintenance mode receiving only bugfixes</p>\n\n## Other apps\n\nThere are many similar applications to Czkawka on the Internet, which do some things better and some things worse:\n\n### GUI\n\n- [DupeGuru](https://github.com/arsenetar/dupeguru) - Many options to customize; great photo compare tool\n- [FSlint](https://github.com/pixelb/fslint) - A little outdated, but still have some tools not available in Czkawka\n- [AntiDupl.NET](https://github.com/ermig1979/AntiDupl) - Shows a lot of metadata of compared images\n- [Video Duplicate Finder](https://github.com/0x90d/videoduplicatefinder) - Finds similar videos(surprising, isn't it), supports video thumbnails\n\n### CLI\n\nDue to limited time, the biggest emphasis is on the GUI version so if you are looking for really good and feature-packed\nconsole apps, then take a look at these:\n\n- [Fclones](https://github.com/pkolaczk/fclones) - One of the fastest tools to find duplicates; it is written also in\n  Rust\n- [Rmlint](https://github.com/sahib/rmlint) - Nice console interface and also is feature packed\n- [RdFind](https://github.com/pauldreik/rdfind) - Fast, but written in C++ Â¯\\\\\\_(ãƒ„)\\_/Â¯\n\n\n## Projects using Czkawka\n\nCzkawka exposes its common functionality through a crate called **`czkawka_core`**, which can be reused by other projects.\n\nIt is written in Rust and is used by all Czkawka frontends (`czkawka_gui`, `czkawka_cli`, `krokiet`).\n\nIt is also used by external projects, such as:\n\n- **Czkawka Tauri** - https://github.com/shixinhuang99/czkawka-tauri - A Tauri-based GUI frontend for Czkawka.\n- **page-dewarp** â€“ https://github.com/lmmx/page-dewarp - A library for dewarping document images using a cubic sheet model.\n\nBindings are also available for:\n\n- **Python** â€“ https://pypi.org/project/czkawka/\n\nSome projects work as wrappers around `czkawka_cli`. Without directly depending on `czkawka_core`, they allow simple scanning and retrieving results in JSON format:\n\n- **Schluckauf** â€“ https://github.com/fadykuzman/schluckauf\n\n## Thanks\n\nBig thanks to PÃ¡draig Brady, creator of fantastic FSlint, because without his work I wouldn't create this tool.\n\nThanks also to all the people who create patches for this program, create and fix translations, make it available on other systems, create videos,\narticles about it etc.\n\nAlso, I really appreciate work of people that create crates on which Czkawka is based and for that I try to report bugs\nto make it even better.\n\n## Officially Supported Projects\nOnly this repository, [prebuild-binaries](https://github.com/qarmin/czkawka/releases), projects on [crates.io](https://crates.io/crates/czkawka_gui) and [flathub](https://flathub.org/apps/com.github.qarmin.czkawka) are directly maintained by me.  \n\nCzkawka does not have an official website, so do not trust any sites that claim to be the official one.  \n\nIf you use packages from unofficial sources, make sure they are safe.\n\n## License\n\nThe entire code in this repository is licensed under the [MIT](https://mit-license.org/) license.\n\nAll images are licensed under the [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n\nThe Czkawka GTK GUI and CLI applications are licensed under the [MIT](https://mit-license.org/) license, while the Krokiet is licensed under the [GPL-3.0-only](https://www.gnu.org/licenses/gpl-3.0.en.html) license.\n\n## Donations\n\nIf you are using the app, I would appreciate a donation for its further development, which can be\ndone [here](https://github.com/sponsors/qarmin).\n\n",
      "stars_today": 69
    },
    {
      "id": 299354207,
      "name": "rustdesk",
      "full_name": "rustdesk/rustdesk",
      "description": "An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.",
      "html_url": "https://github.com/rustdesk/rustdesk",
      "stars": 106444,
      "forks": 15759,
      "language": "Rust",
      "topics": [
        "android",
        "anydesk",
        "dart",
        "flatpak",
        "flutter",
        "flutter-apps",
        "ios",
        "linux",
        "macos",
        "p2p",
        "rdp",
        "remote-control",
        "remote-desktop",
        "rust",
        "rust-lang",
        "teamviewer",
        "vnc",
        "wayland",
        "windows"
      ],
      "created_at": "2020-09-28T15:36:08Z",
      "updated_at": "2026-01-28T02:00:40Z",
      "pushed_at": "2026-01-27T08:38:38Z",
      "open_issues": 105,
      "owner": {
        "login": "rustdesk",
        "avatar_url": "https://avatars.githubusercontent.com/u/71636191?v=4"
      },
      "readme": "<p align=\"center\">\n  <img src=\"res/logo-header.svg\" alt=\"RustDesk - Your remote desktop\"><br>\n  <a href=\"#raw-steps-to-build\">Build</a> â€¢\n  <a href=\"#how-to-build-with-docker\">Docker</a> â€¢\n  <a href=\"#file-structure\">Structure</a> â€¢\n  <a href=\"#snapshot\">Snapshot</a><br>\n  [<a href=\"docs/README-UA.md\">Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°</a>] | [<a href=\"docs/README-CS.md\">Äesky</a>] | [<a href=\"docs/README-ZH.md\">ä¸­æ–‡</a>] | [<a href=\"docs/README-HU.md\">Magyar</a>] | [<a href=\"docs/README-ES.md\">EspaÃ±ol</a>] | [<a href=\"docs/README-FA.md\">ÙØ§Ø±Ø³ÛŒ</a>] | [<a href=\"docs/README-FR.md\">FranÃ§ais</a>] | [<a href=\"docs/README-DE.md\">Deutsch</a>] | [<a href=\"docs/README-PL.md\">Polski</a>] | [<a href=\"docs/README-ID.md\">Indonesian</a>] | [<a href=\"docs/README-FI.md\">Suomi</a>] | [<a href=\"docs/README-ML.md\">à´®à´²à´¯à´¾à´³à´‚</a>] | [<a href=\"docs/README-JP.md\">æ—¥æœ¬èª</a>] | [<a href=\"docs/README-NL.md\">Nederlands</a>] | [<a href=\"docs/README-IT.md\">Italiano</a>] | [<a href=\"docs/README-RU.md\">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a>] | [<a href=\"docs/README-PTBR.md\">PortuguÃªs (Brasil)</a>] | [<a href=\"docs/README-EO.md\">Esperanto</a>] | [<a href=\"docs/README-KR.md\">í•œêµ­ì–´</a>] | [<a href=\"docs/README-AR.md\">Ø§Ù„Ø¹Ø±Ø¨ÙŠ</a>] | [<a href=\"docs/README-VN.md\">Tiáº¿ng Viá»‡t</a>] | [<a href=\"docs/README-DA.md\">Dansk</a>] | [<a href=\"docs/README-GR.md\">Î•Î»Î»Î·Î½Î¹ÎºÎ¬</a>] | [<a href=\"docs/README-TR.md\">TÃ¼rkÃ§e</a>] | [<a href=\"docs/README-NO.md\">Norsk</a>] | [<a href=\"docs/README-RO.md\">RomÃ¢nÄƒ</a>]<br>\n  <b>We need your help to translate this README, <a href=\"https://github.com/rustdesk/rustdesk/tree/master/src/lang\">RustDesk UI</a> and <a href=\"https://github.com/rustdesk/doc.rustdesk.com\">RustDesk Doc</a> to your native language</b>\n</p>\n\n> [!Caution]\n> **Misuse Disclaimer:** <br>\n> The developers of RustDesk do not condone or support any unethical or illegal use of this software. Misuse, such as unauthorized access, control or invasion of privacy, is strictly against our guidelines. The authors are not responsible for any misuse of the application.\n\n\nChat with us: [Discord](https://discord.gg/nDceKgxnkV) | [Twitter](https://twitter.com/rustdesk) | [Reddit](https://www.reddit.com/r/rustdesk) | [YouTube](https://www.youtube.com/@rustdesk)\n\n[![RustDesk Server Pro](https://img.shields.io/badge/RustDesk%20Server%20Pro-Advanced%20Features-blue)](https://rustdesk.com/pricing.html)\n\nYet another remote desktop solution, written in Rust. Works out of the box with no configuration required. You have full control of your data, with no concerns about security. You can use our rendezvous/relay server, [set up your own](https://rustdesk.com/server), or [write your own rendezvous/relay server](https://github.com/rustdesk/rustdesk-server-demo).\n\n![image](https://user-images.githubusercontent.com/71636191/171661982-430285f0-2e12-4b1d-9957-4a58e375304d.png)\n\nRustDesk welcomes contribution from everyone. See [CONTRIBUTING.md](docs/CONTRIBUTING.md) for help getting started.\n\n[**FAQ**](https://github.com/rustdesk/rustdesk/wiki/FAQ)\n\n[**BINARY DOWNLOAD**](https://github.com/rustdesk/rustdesk/releases)\n\n[**NIGHTLY BUILD**](https://github.com/rustdesk/rustdesk/releases/tag/nightly)\n\n[<img src=\"https://f-droid.org/badge/get-it-on.png\"\n    alt=\"Get it on F-Droid\"\n    height=\"80\">](https://f-droid.org/en/packages/com.carriez.flutter_hbb)\n[<img src=\"https://flathub.org/api/badge?svg&locale=en\"\n    alt=\"Get it on Flathub\"\n    height=\"80\">](https://flathub.org/apps/com.rustdesk.RustDesk)\n\n## Dependencies\n\nDesktop versions use Flutter or Sciter (deprecated) for GUI, this tutorial is for Sciter only, since it is easier and more friendly to start. Check out our [CI](https://github.com/rustdesk/rustdesk/blob/master/.github/workflows/flutter-build.yml) for building Flutter version.\n\nPlease download Sciter dynamic library yourself.\n\n[Windows](https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.win/x64/sciter.dll) |\n[Linux](https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so) |\n[macOS](https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.osx/libsciter.dylib)\n\n## Raw Steps to build\n\n- Prepare your Rust development env and C++ build env\n\n- Install [vcpkg](https://github.com/microsoft/vcpkg), and set `VCPKG_ROOT` env variable correctly\n\n  - Windows: vcpkg install libvpx:x64-windows-static libyuv:x64-windows-static opus:x64-windows-static aom:x64-windows-static\n  - Linux/macOS: vcpkg install libvpx libyuv opus aom\n\n- run `cargo run`\n\n## [Build](https://rustdesk.com/docs/en/dev/build/)\n\n## How to Build on Linux\n\n### Ubuntu 18 (Debian 10)\n\n```sh\nsudo apt install -y zip g++ gcc git curl wget nasm yasm libgtk-3-dev clang libxcb-randr0-dev libxdo-dev \\\n        libxfixes-dev libxcb-shape0-dev libxcb-xfixes0-dev libasound2-dev libpulse-dev cmake make \\\n        libclang-dev ninja-build libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev libpam0g-dev\n```\n\n### openSUSE Tumbleweed\n\n```sh\nsudo zypper install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libXfixes-devel cmake alsa-lib-devel gstreamer-devel gstreamer-plugins-base-devel xdotool-devel pam-devel\n```\n\n### Fedora 28 (CentOS 8)\n\n```sh\nsudo yum -y install gcc-c++ git curl wget nasm yasm gcc gtk3-devel clang libxcb-devel libxdo-devel libXfixes-devel pulseaudio-libs-devel cmake alsa-lib-devel gstreamer1-devel gstreamer1-plugins-base-devel pam-devel\n```\n\n### Arch (Manjaro)\n\n```sh\nsudo pacman -Syu --needed unzip git cmake gcc curl wget yasm nasm zip make pkg-config clang gtk3 xdotool libxcb libxfixes alsa-lib pipewire\n```\n\n### Install vcpkg\n\n```sh\ngit clone https://github.com/microsoft/vcpkg\ncd vcpkg\ngit checkout 2023.04.15\ncd ..\nvcpkg/bootstrap-vcpkg.sh\nexport VCPKG_ROOT=$HOME/vcpkg\nvcpkg/vcpkg install libvpx libyuv opus aom\n```\n\n### Fix libvpx (For Fedora)\n\n```sh\ncd vcpkg/buildtrees/libvpx/src\ncd *\n./configure\nsed -i 's/CFLAGS+=-I/CFLAGS+=-fPIC -I/g' Makefile\nsed -i 's/CXXFLAGS+=-I/CXXFLAGS+=-fPIC -I/g' Makefile\nmake\ncp libvpx.a $HOME/vcpkg/installed/x64-linux/lib/\ncd\n```\n\n### Build\n\n```sh\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource $HOME/.cargo/env\ngit clone --recurse-submodules https://github.com/rustdesk/rustdesk\ncd rustdesk\nmkdir -p target/debug\nwget https://raw.githubusercontent.com/c-smile/sciter-sdk/master/bin.lnx/x64/libsciter-gtk.so\nmv libsciter-gtk.so target/debug\nVCPKG_ROOT=$HOME/vcpkg cargo run\n```\n\n## How to build with Docker\n\nBegin by cloning the repository and building the Docker container:\n\n```sh\ngit clone https://github.com/rustdesk/rustdesk\ncd rustdesk\ngit submodule update --init --recursive\ndocker build -t \"rustdesk-builder\" .\n```\n\nThen, each time you need to build the application, run the following command:\n\n```sh\ndocker run --rm -it -v $PWD:/home/user/rustdesk -v rustdesk-git-cache:/home/user/.cargo/git -v rustdesk-registry-cache:/home/user/.cargo/registry -e PUID=\"$(id -u)\" -e PGID=\"$(id -g)\" rustdesk-builder\n```\n\nNote that the first build may take longer before dependencies are cached, subsequent builds will be faster. Additionally, if you need to specify different arguments to the build command, you may do so at the end of the command in the `<OPTIONAL-ARGS>` position. For instance, if you wanted to build an optimized release version, you would run the command above followed by `--release`. The resulting executable will be available in the target folder on your system, and can be run with:\n\n```sh\ntarget/debug/rustdesk\n```\n\nOr, if you're running a release executable:\n\n```sh\ntarget/release/rustdesk\n```\n\nPlease ensure that you run these commands from the root of the RustDesk repository, or the application may not find the required resources. Also note that other cargo subcommands such as `install` or `run` are not currently supported via this method as they would install or run the program inside the container instead of the host.\n\n## File Structure\n\n- **[libs/hbb_common](https://github.com/rustdesk/rustdesk/tree/master/libs/hbb_common)**: video codec, config, tcp/udp wrapper, protobuf, fs functions for file transfer, and some other utility functions\n- **[libs/scrap](https://github.com/rustdesk/rustdesk/tree/master/libs/scrap)**: screen capture\n- **[libs/enigo](https://github.com/rustdesk/rustdesk/tree/master/libs/enigo)**: platform specific keyboard/mouse control\n- **[libs/clipboard](https://github.com/rustdesk/rustdesk/tree/master/libs/clipboard)**: file copy and paste implementation for Windows, Linux, macOS.\n- **[src/ui](https://github.com/rustdesk/rustdesk/tree/master/src/ui)**: obsolete Sciter UI (deprecated)\n- **[src/server](https://github.com/rustdesk/rustdesk/tree/master/src/server)**: audio/clipboard/input/video services, and network connections\n- **[src/client.rs](https://github.com/rustdesk/rustdesk/tree/master/src/client.rs)**: start a peer connection\n- **[src/rendezvous_mediator.rs](https://github.com/rustdesk/rustdesk/tree/master/src/rendezvous_mediator.rs)**: Communicate with [rustdesk-server](https://github.com/rustdesk/rustdesk-server), wait for remote direct (TCP hole punching) or relayed connection\n- **[src/platform](https://github.com/rustdesk/rustdesk/tree/master/src/platform)**: platform specific code\n- **[flutter](https://github.com/rustdesk/rustdesk/tree/master/flutter)**: Flutter code for desktop and mobile\n- **[flutter/web/js](https://github.com/rustdesk/rustdesk/tree/master/flutter/web/v1/js)**: JavaScript for Flutter web client\n\n## Screenshots\n\n![Connection Manager](https://github.com/rustdesk/rustdesk/assets/28412477/db82d4e7-c4bc-4823-8e6f-6af7eadf7651)\n\n![Connected to a Windows PC](https://github.com/rustdesk/rustdesk/assets/28412477/9baa91e9-3362-4d06-aa1a-7518edcbd7ea)\n\n![File Transfer](https://github.com/rustdesk/rustdesk/assets/28412477/39511ad3-aa9a-4f8c-8947-1cce286a46ad)\n\n![TCP Tunneling](https://github.com/rustdesk/rustdesk/assets/28412477/78e8708f-e87e-4570-8373-1360033ea6c5)\n\n",
      "stars_today": 67
    },
    {
      "id": 186733095,
      "name": "v2rayNG",
      "full_name": "2dust/v2rayNG",
      "description": "A V2Ray client for Android, support Xray core and v2fly core",
      "html_url": "https://github.com/2dust/v2rayNG",
      "stars": 50178,
      "forks": 6850,
      "language": "Kotlin",
      "topics": [
        "android",
        "proxy",
        "shadowsocks",
        "socks5",
        "trojan",
        "v2fly",
        "v2ray",
        "vless",
        "vmess",
        "vpn",
        "xray",
        "xtls"
      ],
      "created_at": "2019-05-15T02:15:31Z",
      "updated_at": "2026-01-28T02:08:33Z",
      "pushed_at": "2026-01-27T02:02:46Z",
      "open_issues": 6,
      "owner": {
        "login": "2dust",
        "avatar_url": "https://avatars.githubusercontent.com/u/31833384?v=4"
      },
      "readme": "# v2rayNG\n\nA V2Ray client for Android, support [Xray core](https://github.com/XTLS/Xray-core) and [v2fly core](https://github.com/v2fly/v2ray-core)\n\n[![API](https://img.shields.io/badge/API-24%2B-yellow.svg?style=flat)](https://developer.android.com/about/versions/lollipop)\n[![Kotlin Version](https://img.shields.io/badge/Kotlin-2.3.0-blue.svg)](https://kotlinlang.org)\n[![GitHub commit activity](https://img.shields.io/github/commit-activity/m/2dust/v2rayNG)](https://github.com/2dust/v2rayNG/commits/master)\n[![CodeFactor](https://www.codefactor.io/repository/github/2dust/v2rayng/badge)](https://www.codefactor.io/repository/github/2dust/v2rayng)\n[![GitHub Releases](https://img.shields.io/github/downloads/2dust/v2rayNG/latest/total?logo=github)](https://github.com/2dust/v2rayNG/releases)\n[![Chat on Telegram](https://img.shields.io/badge/Chat%20on-Telegram-brightgreen.svg)](https://t.me/v2rayn)\n\n### Telegram Channel\n[github_2dust](https://t.me/github_2dust)\n\n### Usage\n\n#### Geoip and Geosite\n- geoip.dat and geosite.dat files are in `Android/data/com.v2ray.ang/files/assets` (path may differ on some Android device)\n- download feature will get enhanced version in this [repo](https://github.com/Loyalsoldier/v2ray-rules-dat) (Note it need a working proxy)\n- latest official [domain list](https://github.com/Loyalsoldier/v2ray-rules-dat) and [ip list](https://github.com/Loyalsoldier/geoip) can be imported manually\n- possible to use third party dat file in the same folder, like [h2y](https://guide.v2fly.org/routing/sitedata.html#%E5%A4%96%E7%BD%AE%E7%9A%84%E5%9F%9F%E5%90%8D%E6%96%87%E4%BB%B6)\n\n### More in our [wiki](https://github.com/2dust/v2rayNG/wiki)\n\n### Development guide\n\nAndroid project under V2rayNG folder can be compiled directly in Android Studio, or using Gradle wrapper. But the v2ray core inside the aar is (probably) outdated.  \nThe aar can be compiled from the Golang project [AndroidLibV2rayLite](https://github.com/2dust/AndroidLibV2rayLite) or [AndroidLibXrayLite](https://github.com/2dust/AndroidLibXrayLite).\nFor a quick start, read guide for [Go Mobile](https://github.com/golang/go/wiki/Mobile) and [Makefiles for Go Developers](https://tutorialedge.net/golang/makefiles-for-go-developers/)\n\nv2rayNG can run on Android Emulators. For WSA, VPN permission need to be granted via\n`appops set [package name] ACTIVATE_VPN allow`\n",
      "stars_today": 64
    },
    {
      "id": 606220217,
      "name": "skip",
      "full_name": "skiptools/skip",
      "description": "Skip enables the creation of native SwiftUI apps for iOS and Android",
      "html_url": "https://github.com/skiptools/skip",
      "stars": 2667,
      "forks": 85,
      "language": "Swift",
      "topics": [
        "android",
        "ios",
        "swift"
      ],
      "created_at": "2023-02-24T21:55:33Z",
      "updated_at": "2026-01-28T01:42:23Z",
      "pushed_at": "2026-01-27T14:33:55Z",
      "open_issues": 95,
      "owner": {
        "login": "skiptools",
        "avatar_url": "https://avatars.githubusercontent.com/u/126294127?v=4"
      },
      "readme": "# Skip\n\n[![CI](https://github.com/skiptools/skip/actions/workflows/ci.yml/badge.svg)](https://github.com/skiptools/skip/actions/workflows/ci.yml)\n[![Slack](https://img.shields.io/badge/slack-chat-informational.svg?label=Slack&logo=slack)](https://www.skip.dev/slack)\n\nSkip is a technology for creating dual-platform apps in Swift that run on iOS and Android.\nRead the [documentation](https://skip.dev/docs/) to learn more about Skip.\n\nThis repository hosts the Skip Xcode and SwiftPM build plugin[^plugins]. It works works hand-in-hand with the [skipstone](https://github.com/skiptools/skipstone) tool, which is the binary distribution that powers both the `skip` CLI and the plugin commands. Most of the interesting code is in `skipstone`, but this is the package which Skip projects will directly depend on. For more information on how Skip packages are architected, see the [Framework Structure docs](https://skip.dev/docs/project-types/#framework_structure), or see one of the sample projects like [Hello Skip](https://github.com/skiptools/skipapp-hello).\n\n[^plugins]: Extend package manager functionality with build or command plugins. â€” [https://docs.swift.org/swiftpm/documentation/packagemanagerdocs/plugins/](https://docs.swift.org/swiftpm/documentation/packagemanagerdocs/plugins/)\n\nFor those who want to dive _right_ in without delay, the [Getting Started Guide](https://skip.dev/docs/gettingstarted/) can be summarized like so:\n\n```console\nbrew install skiptools/skip/skip\nskip checkup\nskip create\n```\n\nâ€¦and your Skip project will be created and opened in Xcode.\n\nThis repository also hosts the Skip forums for [support and discussions](http://community.skip.dev) as well as specific [issues and bug reports](https://github.com/skiptools/skip/issues).\n\n\n",
      "stars_today": 64
    },
    {
      "id": 61349723,
      "name": "just",
      "full_name": "casey/just",
      "description": "ğŸ¤– Just a command runner",
      "html_url": "https://github.com/casey/just",
      "stars": 30058,
      "forks": 651,
      "language": "Rust",
      "topics": [],
      "created_at": "2016-06-17T06:08:08Z",
      "updated_at": "2026-01-28T01:54:27Z",
      "pushed_at": "2026-01-26T20:57:05Z",
      "open_issues": 381,
      "owner": {
        "login": "casey",
        "avatar_url": "https://avatars.githubusercontent.com/u/1945?v=4"
      },
      "readme": "<div align=right>Table of Contentsâ†—ï¸</div>\n\n<h1 align=center><code>just</code></h1>\n\n<div align=center>\n  <a href=https://crates.io/crates/just>\n    <img src=https://img.shields.io/crates/v/just.svg alt=\"crates.io version\">\n  </a>\n  <a href=https://github.com/casey/just/actions/workflows/ci.yaml>\n    <img src=https://github.com/casey/just/actions/workflows/ci.yaml/badge.svg alt=\"build status\">\n  </a>\n  <a href=https://github.com/casey/just/releases>\n    <img src=https://img.shields.io/github/downloads/casey/just/total.svg alt=downloads>\n  </a>\n  <a href=https://discord.gg/ezYScXR>\n    <img src=https://img.shields.io/discord/695580069837406228?logo=discord alt=\"chat on discord\">\n  </a>\n  <a href=mailto:casey@rodarmor.com?subject=Thanks%20for%20Just!>\n    <img src=https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg alt=\"say thanks\">\n  </a>\n</div>\n<br>\n\n`just` is a handy way to save and run project-specific commands.\n\nThis readme is also available as a [book](https://just.systems/man/en/). The\nbook reflects the latest release, whereas the\n[readme on GitHub](https://github.com/casey/just/blob/master/README.md)\nreflects latest master.\n\n(ä¸­æ–‡æ–‡æ¡£åœ¨ [è¿™é‡Œ](https://github.com/casey/just/blob/master/README.ä¸­æ–‡.md),\nå¿«çœ‹è¿‡æ¥!)\n\nCommands, called recipes, are stored in a file called `justfile` with syntax\ninspired by `make`:\n\n![screenshot](https://raw.githubusercontent.com/casey/just/master/screenshot.png)\n\nYou can then run them with `just RECIPE`:\n\n```console\n$ just test-all\ncc *.c -o main\n./test --all\nYay, all your tests passed!\n```\n\n`just` has a ton of useful features, and many improvements over `make`:\n\n- `just` is a command runner, not a build system, so it avoids much of\n  [`make`'s complexity and idiosyncrasies](#what-are-the-idiosyncrasies-of-make-that-just-avoids).\n  No need for `.PHONY` recipes!\n\n- Linux, MacOS, Windows, and other reasonable unices are supported with no\n  additional dependencies. (Although if your system doesn't have an `sh`,\n  you'll need to [choose a different shell](#shell).)\n\n- Errors are specific and informative, and syntax errors are reported along\n  with their source context.\n\n- Recipes can accept [command line arguments](#recipe-parameters).\n\n- Wherever possible, errors are resolved statically. Unknown recipes and\n  circular dependencies are reported before anything runs.\n\n- `just` [loads `.env` files](#dotenv-settings), making it easy to populate\n  environment variables.\n\n- Recipes can be [listed from the command line](#listing-available-recipes).\n\n- Command line completion scripts are\n  [available for most popular shells](#shell-completion-scripts).\n\n- Recipes can be written in\n  [arbitrary languages](#shebang-recipes), like Python or NodeJS.\n\n- `just` can be invoked from any subdirectory, not just the directory that\n  contains the `justfile`.\n\n- And [much more](https://just.systems/man/en/)!\n\nIf you need help with `just` please feel free to open an issue or ping me on\n[Discord](https://discord.gg/ezYScXR). Feature requests and bug reports are\nalways welcome!\n\nInstallation\n------------\n\n### Prerequisites\n\n`just` should run on any system with a reasonable `sh`, including Linux, MacOS,\nand the BSDs.\n\n#### Windows\n\nOn Windows, `just` works with the `sh` provided by\n[Git for Windows](https://git-scm.com),\n[GitHub Desktop](https://desktop.github.com), or\n[Cygwin](http://www.cygwin.com). After installation, `sh` must be available in\nthe `PATH` of the shell you want to invoke `just` from.\n\nIf you'd rather not install `sh`, you can use the `shell` setting to use the\nshell of your choice.\n\nLike PowerShell:\n\n```just\n# use PowerShell instead of sh:\nset shell := [\"powershell.exe\", \"-c\"]\n\nhello:\n  Write-Host \"Hello, world!\"\n```\n\nâ€¦or `cmd.exe`:\n\n```just\n# use cmd.exe instead of sh:\nset shell := [\"cmd.exe\", \"/c\"]\n\nlist:\n  dir\n```\n\nYou can also set the shell using command-line arguments. For example, to use\nPowerShell, launch `just` with `--shell powershell.exe --shell-arg -c`.\n\n(PowerShell is installed by default on Windows 7 SP1 and Windows Server 2008 R2\nS1 and later, and `cmd.exe` is quite fiddly, so PowerShell is recommended for\nmost Windows users.)\n\n### Packages\n\n#### Cross-platform\n\n<table>\n  <thead>\n    <tr>\n      <th>Package Manager</th>\n      <th>Package</th>\n      <th>Command</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><a href=https://github.com/alexellis/arkade>arkade</a></td>\n      <td>just</td>\n      <td><code>arkade get just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://asdf-vm.com>asdf</a></td>\n      <td><a href=https://github.com/olofvndrhr/asdf-just>just</a></td>\n      <td>\n        <code>asdf plugin add just</code><br>\n        <code>asdf install just &lt;version&gt;</code>\n      </td>\n    </tr>\n    <tr>\n      <td><a href=https://www.rust-lang.org>Cargo</a></td>\n      <td><a href=https://crates.io/crates/just>just</a></td>\n      <td><code>cargo install just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://docs.conda.io/projects/conda/en/latest/index.html>Conda</a></td>\n      <td><a href=https://anaconda.org/conda-forge/just>just</a></td>\n      <td><code>conda install -c conda-forge just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://brew.sh>Homebrew</a></td>\n      <td><a href=https://formulae.brew.sh/formula/just>just</a></td>\n      <td><code>brew install just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://nixos.org/nix/>Nix</a></td>\n      <td><a href=https://github.com/NixOS/nixpkgs/blob/master/pkgs/by-name/ju/just/package.nix>just</a></td>\n      <td><code>nix-env -iA nixpkgs.just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://www.npmjs.com/>npm</a></td>\n      <td><a href=https://www.npmjs.com/package/rust-just>rust-just</a></td>\n      <td><code>npm install -g rust-just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://pipx.pypa.io/stable/>pipx</a></td>\n      <td><a href=https://pypi.org/project/rust-just/>rust-just</a></td>\n      <td><code>pipx install rust-just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://snapcraft.io>Snap</a></td>\n      <td><a href=https://snapcraft.io/just>just</a></td>\n      <td><code>snap install --edge --classic just</code></td>\n    </tr>\n  </tbody>\n</table>\n\n#### BSD\n\n<table>\n  <thead>\n    <tr>\n      <th>Operating System</th>\n      <th>Package Manager</th>\n      <th>Package</th>\n      <th>Command</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><a href=https://www.freebsd.org>FreeBSD</a></td>\n      <td><a href=https://www.freebsd.org/doc/handbook/pkgng-intro.html>pkg</a></td>\n      <td><a href=https://www.freshports.org/deskutils/just/>just</a></td>\n      <td><code>pkg install just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://www.openbsd.org>OpenBSD</a></td>\n      <td><a href=https://www.openbsd.org/faq/faq15.html>pkg_*</a></td>\n      <td><a href=https://cvsweb.openbsd.org/cgi-bin/cvsweb/ports/sysutils/just>just</a></td>\n      <td><code>pkg_add just</code></td>\n    </tr>\n  </tbody>\n</table>\n\n#### Linux\n\n<table>\n  <thead>\n    <tr>\n      <th>Operating System</th>\n      <th>Package Manager</th>\n      <th>Package</th>\n      <th>Command</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><a href=https://alpinelinux.org>Alpine</a></td>\n      <td><a href=https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management>apk-tools</a></td>\n      <td><a href=https://pkgs.alpinelinux.org/package/edge/community/x86_64/just>just</a></td>\n      <td><code>apk add just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://www.archlinux.org>Arch</a></td>\n      <td><a href=https://wiki.archlinux.org/title/Pacman>pacman</a></td>\n      <td><a href=https://archlinux.org/packages/extra/x86_64/just/>just</a></td>\n      <td><code>pacman -S just</code></td>\n    </tr>\n    <tr>\n      <td>\n        <a href=https://debian.org>Debian 13</a> and\n        <a href=https://ubuntu.com>Ubuntu 24.04</a> derivatives</td>\n      <td><a href=https://en.wikipedia.org/wiki/APT_(software)>apt</a></td>\n      <td><a href=https://packages.debian.org/trixie/just>just</a></td>\n      <td><code>apt install just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://getfedora.org>Fedora</a></td>\n      <td><a href=https://dnf.readthedocs.io/en/latest/>DNF</a></td>\n      <td><a href=https://src.fedoraproject.org/rpms/rust-just>just</a></td>\n      <td><code>dnf install just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://www.gentoo.org>Gentoo</a></td>\n      <td><a href=https://wiki.gentoo.org/wiki/Portage>Portage</a></td>\n      <td><a href=https://github.com/gentoo-mirror/guru/tree/master/dev-build/just>guru/dev-build/just</a></td>\n      <td>\n        <code>eselect repository enable guru</code><br>\n        <code>emerge --sync guru</code><br>\n        <code>emerge dev-build/just</code>\n      </td>\n    </tr>\n    <tr>\n      <td><a href=https://nixos.org/nixos/>NixOS</a></td>\n      <td><a href=https://nixos.org/nix/>Nix</a></td>\n      <td><a href=https://github.com/NixOS/nixpkgs/blob/master/pkgs/by-name/ju/just/package.nix>just</a></td>\n      <td><code>nix-env -iA nixos.just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://opensuse.org>openSUSE</a></td>\n      <td><a href=https://en.opensuse.org/Portal:Zypper>Zypper</a></td>\n      <td><a href=https://build.opensuse.org/package/show/Base:System/just>just</a></td>\n      <td><code>zypper in just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://getsol.us>Solus</a></td>\n      <td><a href=https://getsol.us/articles/package-management/basics/en>eopkg</a></td>\n      <td><a href=https://dev.getsol.us/source/just/>just</a></td>\n      <td><code>eopkg install just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://voidlinux.org>Void</a></td>\n      <td><a href=https://wiki.voidlinux.org/XBPS>XBPS</a></td>\n      <td><a href=https://github.com/void-linux/void-packages/blob/master/srcpkgs/just/template>just</a></td>\n      <td><code>xbps-install -S just</code></td>\n    </tr>\n  </tbody>\n</table>\n\n#### Windows\n\n<table>\n  <thead>\n    <tr>\n      <th>Package Manager</th>\n      <th>Package</th>\n      <th>Command</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><a href=https://chocolatey.org>Chocolatey</a></td>\n      <td><a href=https://github.com/michidk/just-choco>just</a></td>\n      <td><code>choco install just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://scoop.sh>Scoop</a></td>\n      <td><a href=https://github.com/ScoopInstaller/Main/blob/master/bucket/just.json>just</a></td>\n      <td><code>scoop install just</code></td>\n    </tr>\n    <tr>\n      <td><a href=https://learn.microsoft.com/en-us/windows/package-manager/>Windows Package Manager</a></td>\n      <td><a href=https://github.com/microsoft/winget-pkgs/tree/master/manifests/c/Casey/Just>Casey/Just</a></td>\n      <td><code>winget install --id Casey.Just --exact</code></td>\n    </tr>\n  </tbody>\n</table>\n\n#### macOS\n\n<table>\n  <thead>\n    <tr>\n      <th>Package Manager</th>\n      <th>Package</th>\n      <th>Command</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><a href=https://www.macports.org>MacPorts</a></td>\n      <td><a href=https://ports.macports.org/port/just/summary>just</a></td>\n      <td><code>port install just</code></td>\n    </tr>\n  </tbody>\n</table>\n\n![just package version table](https://repology.org/badge/vertical-allrepos/just.svg)\n\n### Pre-Built Binaries\n\nPre-built binaries for Linux, MacOS, and Windows can be found on\n[the releases page](https://github.com/casey/just/releases).\n\nYou can use the following command on Linux, MacOS, or Windows to download the\nlatest release, just replace `DEST` with the directory where you'd like to put\n`just`:\n\n```console\ncurl --proto '=https' --tlsv1.2 -sSf https://just.systems/install.sh | bash -s -- --to DEST\n```\n\nFor example, to install `just` to `~/bin`:\n\n```console\n# create ~/bin\nmkdir -p ~/bin\n\n# download and extract just to ~/bin/just\ncurl --proto '=https' --tlsv1.2 -sSf https://just.systems/install.sh | bash -s -- --to ~/bin\n\n# add `~/bin` to the paths that your shell searches for executables\n# this line should be added to your shells initialization file,\n# e.g. `~/.bashrc` or `~/.zshrc`\nexport PATH=\"$PATH:$HOME/bin\"\n\n# just should now be executable\njust --help\n```\n\nNote that `install.sh` may fail on GitHub Actions, or in other environments\nwhere many machines share IP addresses. `install.sh` calls GitHub APIs in order\nto determine the latest version of `just` to install, and those API calls are\nrate-limited on a per-IP basis. To make `install.sh` more reliable in such\ncircumstances, pass a specific tag to install with `--tag`.\n\nAnother way to avoid rate-limiting is to pass a GitHub authentication token to\n`install.sh` as an environment variable named `GITHUB_TOKEN`, allowing it to\nauthenticate its requests.\n\n[Releases](https://github.com/casey/just/releases) include a `SHA256SUM` file\nwhich can be used to verify the integrity of pre-built binary archives.\n\nTo verify a release, download the pre-built binary archive along with the\n`SHA256SUM` file and run:\n\n```sh\nshasum --algorithm 256 --ignore-missing --check SHA256SUMS\n```\n\n### GitHub Actions\n\n`just` can be installed on GitHub Actions in a few ways.\n\nUsing package managers pre-installed on GitHub Actions runners on MacOS with\n`brew install just`, and on Windows with `choco install just`.\n\nWith [extractions/setup-just](https://github.com/extractions/setup-just):\n\n```yaml\n- uses: extractions/setup-just@v3\n  with:\n    just-version: 1.5.0  # optional semver specification, otherwise latest\n```\n\nOr with [taiki-e/install-action](https://github.com/taiki-e/install-action):\n\n```yaml\n- uses: taiki-e/install-action@just\n```\n\n### Release RSS Feed\n\nAn [RSS feed](https://en.wikipedia.org/wiki/RSS) of `just` releases is available [here](https://github.com/casey/just/releases.atom).\n\n### Node.js Installation\n\n[just-install](https://npmjs.com/package/just-install) can be used to automate\ninstallation of `just` in Node.js applications.\n\n`just` is a great, more robust alternative to npm scripts. If you want to\ninclude `just` in the dependencies of a Node.js application, `just-install`\nwill install a local, platform-specific binary as part of the `npm install`\ncommand. This removes the need for every developer to install `just`\nindependently using one of the processes mentioned above. After installation,\nthe `just` command will work in npm scripts or with npx. It's great for teams\nwho want to make the set up process for their project as easy as possible.\n\nFor more information, see the\n[just-install README file](https://github.com/brombal/just-install#readme).\n\nBackwards Compatibility\n-----------------------\n\nWith the release of version 1.0, `just` features a strong commitment to\nbackwards compatibility and stability.\n\nFuture releases will not introduce backwards incompatible changes that make\nexisting `justfile`s stop working, or break working invocations of the\ncommand-line interface.\n\nThis does not, however, preclude fixing outright bugs, even if doing so might\nbreak `justfiles` that rely on their behavior.\n\nThere will never be a `just` 2.0. Any desirable backwards-incompatible changes\nwill be opt-in on a per-`justfile` basis, so users may migrate at their\nleisure.\n\nFeatures that aren't yet ready for stabilization are marked as unstable and may\nbe changed or removed at any time. Using unstable features produces an error by\ndefault, which can be suppressed with by passing the `--unstable` flag,\n`set unstable`, or setting the environment variable `JUST_UNSTABLE`, to any\nvalue other than `false`, `0`, or the empty string.\n\nEditor Support\n--------------\n\n`justfile` syntax is close enough to `make` that you may want to tell your\neditor to use `make` syntax highlighting for `just`.\n\n### Vim and Neovim\n\nVim version 9.1.1042 or better and Neovim version 0.11 or better support\nJustfile syntax highlighting out of the box, thanks to\n[pbnj](https://github.com/pbnj).\n\n#### `vim-just`\n\nThe [vim-just](https://github.com/NoahTheDuke/vim-just) plugin provides syntax\nhighlighting for `justfile`s.\n\nInstall it with your favorite package manager, like\n[Plug](https://github.com/junegunn/vim-plug):\n\n```vim\ncall plug#begin()\n\nPlug 'NoahTheDuke/vim-just'\n\ncall plug#end()\n```\n\nOr with Vim's built-in package support:\n\n```console\nmkdir -p ~/.vim/pack/vendor/start\ncd ~/.vim/pack/vendor/start\ngit clone https://github.com/NoahTheDuke/vim-just.git\n```\n\n#### `tree-sitter-just`\n\n[tree-sitter-just](https://github.com/IndianBoy42/tree-sitter-just) is an\n[Nvim Treesitter](https://github.com/nvim-treesitter/nvim-treesitter) plugin\nfor Neovim.\n\n#### Makefile Syntax Highlighting\n\nVim's built-in makefile syntax highlighting isn't perfect for `justfile`s, but\nit's better than nothing. You can put the following in `~/.vim/filetype.vim`:\n\n```vimscript\nif exists(\"did_load_filetypes\")\n  finish\nendif\n\naugroup filetypedetect\n  au BufNewFile,BufRead justfile setf make\naugroup END\n```\n\nOr add the following to an individual `justfile` to enable `make` mode on a\nper-file basis:\n\n```text\n# vim: set ft=make :\n```\n\n### Emacs\n\n[just-mode](https://github.com/leon-barrett/just-mode.el) provides syntax\nhighlighting and automatic indentation of `justfile`s. It is available on\n[MELPA](https://melpa.org/) as [just-mode](https://melpa.org/#/just-mode).\n\n[justl](https://github.com/psibi/justl.el) provides commands for executing and\nlisting recipes.\n\nYou can add the following to an individual `justfile` to enable `make` mode on\na per-file basis:\n\n```text\n# Local Variables:\n# mode: makefile\n# End:\n```\n\n### Visual Studio Code\n\nAn extension for VS Code is [available here](https://github.com/nefrob/vscode-just).\n\nUnmaintained VS Code extensions include\n[skellock/vscode-just](https://github.com/skellock/vscode-just) and\n[sclu1034/vscode-just](https://github.com/sclu1034/vscode-just).\n\n### JetBrains IDEs\n\nA plugin for JetBrains IDEs by [linux_china](https://github.com/linux-china) is\n[available here](https://plugins.jetbrains.com/plugin/18658-just).\n\n### Kakoune\n\nKakoune supports `justfile` syntax highlighting out of the box, thanks to\nTeddyDD.\n\n### Helix\n\n[Helix](https://helix-editor.com/) supports `justfile` syntax highlighting\nout-of-the-box since version 23.05.\n\n### Sublime Text\n\nThe [Just package](https://github.com/nk9/just_sublime) by\n[nk9](https://github.com/nk9) with `just` syntax and some other tools is\navailable on [PackageControl](https://packagecontrol.io/packages/Just).\n\n### Micro\n\n[Micro](https://micro-editor.github.io/) supports Justfile syntax highlighting\nout of the box, thanks to [tomodachi94](https://github.com/tomodachi94).\n\n### Zed\n\nThe [zed-just](https://github.com/jackTabsCode/zed-just/) extension by\n[jackTabsCode](https://github.com/jackTabsCode) is avilable on the\n[Zed extensions page](https://zed.dev/extensions?query=just).\n\n### Other Editors\n\nFeel free to send me the commands necessary to get syntax highlighting working\nin your editor of choice so that I may include them here.\n\n### Language Server Protocol\n\n[just-lsp](https://github.com/terror/just-lsp) provides a [language server\nprotocol](https://en.wikipedia.org/wiki/Language_Server_Protocol)\nimplementation, enabling features such as go-to-definition, inline diagnostics,\nand code completion.\n\n### Model Context Protocol\n\n[just-mcp](http://github.com/promptexecution/just-mcp) provides a\n[model context protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\nadapter to allow LLMs to query the contents of `justfiles` and run recipes.\n\nQuick Start\n-----------\n\nSee the installation section for how to install `just` on your computer. Try\nrunning `just --version` to make sure that it's installed correctly.\n\nFor an overview of the syntax, check out\n[this cheatsheet](https://cheatography.com/linux-china/cheat-sheets/justfile/).\n\nOnce `just` is installed and working, create a file named `justfile` in the\nroot of your project with the following contents:\n\n```just\nrecipe-name:\n  echo 'This is a recipe!'\n\n# this is a comment\nanother-recipe:\n  @echo 'This is another recipe.'\n```\n\nWhen you invoke `just` it looks for file `justfile` in the current directory\nand upwards, so you can invoke it from any subdirectory of your project.\n\nThe search for a `justfile` is case insensitive, so any case, like `Justfile`,\n`JUSTFILE`, or `JuStFiLe`, will work. `just` will also look for files with the\nname `.justfile`, in case you'd like to hide a `justfile`.\n\nRunning `just` with no arguments runs the first recipe in the `justfile`:\n\n```console\n$ just\necho 'This is a recipe!'\nThis is a recipe!\n```\n\nOne or more arguments specify the recipe(s) to run:\n\n```console\n$ just another-recipe\nThis is another recipe.\n```\n\n`just` prints each command to standard error before running it, which is why\n`echo 'This is a recipe!'` was printed. This is suppressed for lines starting\nwith `@`, which is why `echo 'This is another recipe.'` was not printed.\n\nRecipes stop running if a command fails. Here `cargo publish` will only run if\n`cargo test` succeeds:\n\n```just\npublish:\n  cargo test\n  # tests passed, time to publish!\n  cargo publish\n```\n\nRecipes can depend on other recipes. Here the `test` recipe depends on the\n`build` recipe, so `build` will run before `test`:\n\n```just\nbuild:\n  cc main.c foo.c bar.c -o main\n\ntest: build\n  ./test\n\nsloc:\n  @echo \"`wc -l *.c` lines of code\"\n```\n\n```console\n$ just test\ncc main.c foo.c bar.c -o main\n./test\ntestingâ€¦ all tests passed!\n```\n\nRecipes without dependencies will run in the order they're given on the command\nline:\n\n```console\n$ just build sloc\ncc main.c foo.c bar.c -o main\n1337 lines of code\n```\n\nDependencies will always run first, even if they are passed after a recipe that\ndepends on them:\n\n```console\n$ just test build\ncc main.c foo.c bar.c -o main\n./test\ntestingâ€¦ all tests passed!\n```\n\nRecipes may depend on recipes in submodules:\n\n```justfile\nmod foo\n\nbaz: foo::bar\n```\n\nExamples\n--------\n\nA variety of `justfile`s can be found in the\n[examples directory](https://github.com/casey/just/tree/master/examples) and on\n[GitHub](https://github.com/search?q=path%3A**%2Fjustfile&type=code).\n\nFeatures\n--------\n\n### The Default Recipe\n\nWhen `just` is invoked without a recipe, it runs the recipe with the\n`[default]` attribute, or the first recipe in the `justfile` if no recipe has\nthe `[default]` attribute.\n\nThis recipe might be the most frequently run command in the project, like\nrunning the tests:\n\n```just\ntest:\n  cargo test\n```\n\nYou can also use dependencies to run multiple recipes by default:\n\n```just\ndefault: lint build test\n\nbuild:\n  echo Buildingâ€¦\n\ntest:\n  echo Testingâ€¦\n\nlint:\n  echo Lintingâ€¦\n```\n\nIf no recipe makes sense as the default recipe, you can add a recipe to the\nbeginning of your `justfile` that lists the available recipes:\n\n```just\ndefault:\n  just --list\n```\n\n### Listing Available Recipes\n\nRecipes can be listed in alphabetical order with `just --list`:\n\n```console\n$ just --list\nAvailable recipes:\n    build\n    test\n    deploy\n    lint\n```\n\nRecipes in [submodules](#modules1190) can be listed with `just --list PATH`,\nwhere `PATH` is a space- or `::`-separated module path:\n\n```\n$ cat justfile\nmod foo\n$ cat foo.just\nmod bar\n$ cat bar.just\nbaz:\n$ just --list foo bar\nAvailable recipes:\n    baz\n$ just --list foo::bar\nAvailable recipes:\n    baz\n```\n\n`just --summary` is more concise:\n\n```console\n$ just --summary\nbuild test deploy lint\n```\n\nPass `--unsorted` to print recipes in the order they appear in the `justfile`:\n\n```just\ntest:\n  echo 'Testing!'\n\nbuild:\n  echo 'Building!'\n```\n\n```console\n$ just --list --unsorted\nAvailable recipes:\n    test\n    build\n```\n\n```console\n$ just --summary --unsorted\ntest build\n```\n\nIf you'd like `just` to default to listing the recipes in the `justfile`, you\ncan use this as your default recipe:\n\n```just\ndefault:\n  @just --list\n```\n\nNote that you may need to add `--justfile {{justfile()}}` to the line above.\nWithout it, if you executed `just -f /some/distant/justfile -d .` or\n`just -f ./non-standard-justfile`, the plain `just --list` inside the recipe\nwould not necessarily use the file you provided. It would try to find a\njustfile in your current path, maybe even resulting in a `No justfile found`\nerror.\n\nThe heading text can be customized with `--list-heading`:\n\n```console\n$ just --list --list-heading $'Cool stuffâ€¦\\n'\nCool stuffâ€¦\n    test\n    build\n```\n\nAnd the indentation can be customized with `--list-prefix`:\n\n```console\n$ just --list --list-prefix Â·Â·Â·Â·\nAvailable recipes:\nÂ·Â·Â·Â·test\nÂ·Â·Â·Â·build\n```\n\nThe argument to `--list-heading` replaces both the heading and the newline\nfollowing it, so it should contain a newline if non-empty. It works this way so\nyou can suppress the heading line entirely by passing the empty string:\n\n```console\n$ just --list --list-heading ''\n    test\n    build\n```\n\n### Invoking Multiple Recipes\n\nMultiple recipes may be invoked on the command line at once:\n\n```just\nbuild:\n  make web\n\nserve:\n  python3 -m http.server -d out 8000\n```\n\n```console\n$ just build serve\nmake web\npython3 -m http.server -d out 8000\n```\n\nKeep in mind that recipes with parameters will swallow arguments, even if they\nmatch the names of other recipes:\n\n```just\nbuild project:\n  make {{project}}\n\nserve:\n  python3 -m http.server -d out 8000\n```\n\n```console\n$ just build serve\nmake: *** No rule to make target `serve'.  Stop.\n```\n\nThe `--one` flag can be used to restrict command-line invocations to a single\nrecipe:\n\n```console\n$ just --one build serve\nerror: Expected 1 command-line recipe invocation but found 2.\n```\n\n### Working Directory\n\nBy default, recipes run with the working directory set to the directory that\ncontains the `justfile`.\n\nThe `[no-cd]` attribute can be used to make recipes run with the working\ndirectory set to directory in which `just` was invoked.\n\n```just\n@foo:\n  pwd\n\n[no-cd]\n@bar:\n  pwd\n```\n\n```console\n$ cd subdir\n$ just foo\n/\n$ just bar\n/subdir\n```\n\nYou can override the working directory for all recipes with\n`set working-directory := 'â€¦'`:\n\n```just\nset working-directory := 'bar'\n\n@foo:\n  pwd\n```\n\n```console\n$ pwd\n/home/bob\n$ just foo\n/home/bob/bar\n```\n\nYou can override the working directory for a specific recipe with the\n`working-directory` attribute<sup>1.38.0</sup>:\n\n```just\n[working-directory: 'bar']\n@foo:\n  pwd\n```\n\n```console\n$ pwd\n/home/bob\n$ just foo\n/home/bob/bar\n```\n\nThe argument to the `working-directory` setting or `working-directory`\nattribute may be absolute or relative. If it is relative it is interpreted\nrelative to the default working directory.\n\n### Aliases\n\nAliases allow recipes to be invoked on the command line with alternative names:\n\n```just\nalias b := build\n\nbuild:\n  echo 'Building!'\n```\n\n```console\n$ just b\necho 'Building!'\nBuilding!\n```\n\nThe target of an alias may be a recipe in a submodule:\n\n```justfile\nmod foo\n\nalias baz := foo::bar\n```\n\n### Settings\n\nSettings control interpretation and execution. Each setting may be specified at\nmost once, anywhere in the `justfile`.\n\nFor example:\n\n```just\nset shell := [\"zsh\", \"-cu\"]\n\nfoo:\n  # this line will be run as `zsh -cu 'ls **/*.txt'`\n  ls **/*.txt\n```\n\n#### Table of Settings\n\n| Name | Value | Default | Description |\n|------|-------|---------|-------------|\n| `allow-duplicate-recipes` | boolean | `false` | Allow recipes appearing later in a `justfile` to override earlier recipes with the same name. |\n| `allow-duplicate-variables` | boolean | `false` | Allow variables appearing later in a `justfile` to override earlier variables with the same name. |\n| `dotenv-filename` | string | - | Load a `.env` file with a custom name, if present. |\n| `dotenv-load` | boolean | `false` | Load a `.env` file, if present. |\n| `dotenv-override` | boolean | `false` | Override existing environment variables with values from the `.env` file. |\n| `dotenv-path` | string | - | Load a `.env` file from a custom path and error if not present. Overrides `dotenv-filename`. |\n| `dotenv-required` | boolean | `false` | Error if a `.env` file isn't found. |\n| `export` | boolean | `false` | Export all variables as environment variables. |\n| `fallback` | boolean | `false` | Search `justfile` in parent directory if the first recipe on the command line is not found. |\n| `ignore-comments` | boolean | `false` | Ignore recipe lines beginning with `#`. |\n| `positional-arguments` | boolean | `false` | Pass positional arguments. |\n| `quiet` | boolean | `false` | Disable echoing recipe lines before executing. |\n| `script-interpreter`<sup>1.33.0</sup> | `[COMMAND, ARGSâ€¦]` | `['sh', '-eu']` | Set command used to invoke recipes with empty `[script]` attribute. |\n| `shell` | `[COMMAND, ARGSâ€¦]` | - | Set command used to invoke recipes and evaluate backticks. |\n| `tempdir` | string | - | Create temporary directories in `tempdir` instead of the system default temporary directory. |\n| `unstable`<sup>1.31.0</sup> | boolean | `false` | Enable unstable features. |\n| `windows-powershell` | boolean | `false` | Use PowerShell on Windows as default shell. (Deprecated. Use `windows-shell` instead. |\n| `windows-shell` | `[COMMAND, ARGSâ€¦]` | - | Set the command used to invoke recipes and evaluate backticks. |\n| `working-directory`<sup>1.33.0</sup> | string | - | Set the working directory for recipes and backticks, relative to the default working directory. |\n\nBoolean settings can be written as:\n\n```justfile\nset NAME\n```\n\nWhich is equivalent to:\n\n```justfile\nset NAME := true\n```\n\nNon-boolean settings can be set to both strings and\nexpressions.<sup>1.46.0</sup>\n\nHowever, because settings affect the behavior of backticks and many functions,\nthose expressions may not contain backticks or function calls, directly or\ntransitively via reference.\n\n#### Allow Duplicate Recipes\n\nIf `allow-duplicate-recipes` is set to `true`, defining multiple recipes with\nthe same name is not an error and the last definition is used. Defaults to\n`false`.\n\n```just\nset allow-duplicate-recipes\n\n@foo:\n  echo foo\n\n@foo:\n  echo bar\n```\n\n```console\n$ just foo\nbar\n```\n\n#### Allow Duplicate Variables\n\nIf `allow-duplicate-variables` is set to `true`, defining multiple variables\nwith the same name is not an error and the last definition is used. Defaults to\n`false`.\n\n```just\nset allow-duplicate-variables\n\na := \"foo\"\na := \"bar\"\n\n@foo:\n  echo {{a}}\n```\n\n```console\n$ just foo\nbar\n```\n\n#### Dotenv Settings\n\nIf any of `dotenv-load`, `dotenv-filename`, `dotenv-override`, `dotenv-path`,\nor `dotenv-required` are set, `just` will try to load environment variables\nfrom a file.\n\nIf `dotenv-path` is set, `just` will look for a file at the given path, which\nmay be absolute, or relative to the working directory.\n\nThe command-line option `--dotenv-path`, short form `-E`, can be used to set or\noverride `dotenv-path` at runtime.\n\nIf `dotenv-filename` is set `just` will look for a file at the given path,\nrelative to the working directory and each of its ancestors.\n\nIf `dotenv-filename` is not set, but `dotenv-load` or `dotenv-required` are\nset, just will look for a file named `.env`, relative to the working directory\nand each of its ancestors.\n\n`dotenv-filename` and `dotenv-path` are similar, but `dotenv-path` is only\nchecked relative to the working directory, whereas `dotenv-filename` is checked\nrelative to the working directory and each of its ancestors.\n\nIt is not an error if an environment file is not found, unless\n`dotenv-required` is set.\n\nThe loaded variables are environment variables, not `just` variables, and so\nmust be accessed using `$VARIABLE_NAME` in recipes and backticks.\n\nIf `dotenv-override` is set, variables from the environment file will override\nexisting environment variables.\n\nFor example, if your `.env` file contains:\n\n```console\n# a comment, will be ignored\nDATABASE_ADDRESS=localhost:6379\nSERVER_PORT=1337\n```\n\nAnd your `justfile` contains:\n\n```just\nset dotenv-load\n\nserve:\n  @echo \"Starting server with database $DATABASE_ADDRESS on port $SERVER_PORTâ€¦\"\n  ./server --database $DATABASE_ADDRESS --port $SERVER_PORT\n```\n\n`just serve` will output:\n\n```console\n$ just serve\nStarting server with database localhost:6379 on port 1337â€¦\n./server --database $DATABASE_ADDRESS --port $SERVER_PORT\n```\n\n#### Export\n\nThe `export` setting causes all `just` variables to be exported as environment\nvariables. Defaults to `false`.\n\n```just\nset export\n\na := \"hello\"\n\n@foo b:\n  echo $a\n  echo $b\n```\n\n```console\n$ just foo goodbye\nhello\ngoodbye\n```\n\n#### Positional Arguments\n\nIf `positional-arguments` is `true`, recipe arguments will be passed as\npositional arguments to commands. For linewise recipes, argument `$0` will be\nthe name of the recipe.\n\nFor example, running this recipe:\n\n```just\nset positional-arguments\n\n@foo bar:\n  echo $0\n  echo $1\n```\n\nWill produce the following output:\n\n```console\n$ just foo hello\nfoo\nhello\n```\n\nWhen using an `sh`-compatible shell, such as `bash` or `zsh`, `$@` expands to\nthe positional arguments given to the recipe, starting from one. When used\nwithin double quotes as `\"$@\"`, arguments including whitespace will be passed\non as if they were double-quoted. That is, `\"$@\"` is equivalent to `\"$1\" \"$2\"`â€¦\nWhen there are no positional parameters, `\"$@\"` and `$@` expand to nothing\n(i.e., they are removed).\n\nThis example recipe will print arguments one by one on separate lines:\n\n```just\nset positional-arguments\n\n@test *args='':\n  bash -c 'while (( \"$#\" )); do echo - $1; shift; done' -- \"$@\"\n```\n\nRunning it with _two_ arguments:\n\n```console\n$ just test foo \"bar baz\"\n- foo\n- bar baz\n```\n\nPositional arguments may also be turned on on a per-recipe basis with the\n`[positional-arguments]` attribute<sup>1.29.0</sup>:\n\n```just\n[positional-arguments]\n@foo bar:\n  echo $0\n  echo $1\n```\n\nNote that PowerShell does not handle positional arguments in the same way as\nother shells, so turning on positional arguments will likely break recipes that\nuse PowerShell.\n\nIf using PowerShell 7.4 or better, the `-CommandWithArgs` flag will make\npositional arguments work as expected:\n\n```just\nset shell := ['pwsh.exe', '-CommandWithArgs']\nset positional-arguments\n\nprint-args a b c:\n  Write-Output @($args[1..($args.Count - 1)])\n```\n\n#### Shell\n\nThe `shell` setting controls the command used to invoke recipe lines and\nbackticks. Shebang recipes are unaffected. The default shell is `sh -cu`.\n\n```just\n# use python3 to execute recipe lines and backticks\nset shell := [\"python3\", \"-c\"]\n\n# use print to capture result of evaluation\nfoos := `print(\"foo\" * 4)`\n\nfoo:\n  print(\"Snake snake snake snake.\")\n  print(\"{{foos}}\")\n```\n\n`just` passes the command to be executed as an argument. Many shells will need\nan additional flag, often `-c`, to make them evaluate the first argument.\n\n##### Windows Shell\n\n`just` uses `sh` on Windows by default. To use a different shell on Windows,\nuse `windows-shell`:\n\n```just\nset windows-shell := [\"powershell.exe\", \"-NoLogo\", \"-Command\"]\n\nhello:\n  Write-Host \"Hello, world!\"\n```\n\nSee\n[powershell.just](https://github.com/casey/just/blob/master/examples/powershell.just)\nfor a justfile that uses PowerShell on all platforms.\n\n##### Windows PowerShell\n\n*`set windows-powershell` uses the legacy `powershell.exe` binary, and is no\nlonger recommended. See the `windows-shell` setting above for a more flexible\nway to control which shell is used on Windows.*\n\n`just` uses `sh` on Windows by default. To use `powershell.exe` instead, set\n`windows-powershell` to true.\n\n```just\nset windows-powershell := true\n\nhello:\n  Write-Host \"Hello, world!\"\n```\n\n##### Python 3\n\n```just\nset shell := [\"python3\", \"-c\"]\n```\n\n##### Bash\n\n```just\nset shell := [\"bash\", \"-uc\"]\n```\n\n##### Z Shell\n\n```just\nset shell := [\"zsh\", \"-uc\"]\n```\n\n##### Fish\n\n```just\nset shell := [\"fish\", \"-c\"]\n```\n\n##### Nushell\n\n```just\nset shell := [\"nu\", \"-c\"]\n```\n\nIf you want to change the default table mode to `light`:\n\n```just\nset shell := ['nu', '-m', 'light', '-c']\n```\n\n*[Nushell](https://github.com/nushell/nushell) was written in Rust, and **has\ncross-platform support for Windows / macOS and Linux**.*\n\n### Documentation Comments\n\nComments immediately preceding a recipe will appear in `just --list`:\n\n```just\n# build stuff\nbuild:\n  ./bin/build\n\n# test stuff\ntest:\n  ./bin/test\n```\n\n```console\n$ just --list\nAvailable recipes:\n    build # build stuff\n    test # test stuff\n```\n\nThe `[doc]` attribute can be used to set or suppress a recipe's doc comment:\n\n```just\n# This comment won't appear\n[doc('Build stuff')]\nbuild:\n  ./bin/build\n\n# This one won't either\n[doc]\ntest:\n  ./bin/test\n```\n\n```console\n$ just --list\nAvailable recipes:\n    build # Build stuff\n    test\n```\n\n### Expressions and Substitutions\n\nVarious operators and function calls are supported in expressions, which may be\nused in assignments, default recipe arguments, and inside recipe body `{{â€¦}}`\nsubstitutions.\n\n```just\ntmpdir  := `mktemp -d`\nversion := \"0.2.7\"\ntardir  := tmpdir / \"awesomesauce-\" + version\ntarball := tardir + \".tar.gz\"\nconfig  := quote(config_dir() / \".project-config\")\n\npublish:\n  rm -f {{tarball}}\n  mkdir {{tardir}}\n  cp README.md *.c {{ config }} {{tardir}}\n  tar zcvf {{tarball}} {{tardir}}\n  scp {{tarball}} me@server.com:release/\n  rm -rf {{tarball}} {{tardir}}\n```\n\n#### Concatenation\n\nThe `+` operator returns the left-hand argument concatenated with the\nright-hand argument:\n\n```just\nfoobar := 'foo' + 'bar'\n```\n\n#### Logical Operators\n\nThe logical operators `&&` and `||` can be used to coalesce string\nvalues<sup>1.37.0</sup>, similar to Python's `and` and `or`. These operators\nconsider the empty string `''` to be false, and all other strings to be true.\n\nThese operators are currently unstable.\n\nThe `&&` operator returns the empty string if the left-hand argument is the\nempty string, otherwise it returns the right-hand argument:\n\n```justfile\nfoo := '' && 'goodbye'      # ''\nbar := 'hello' && 'goodbye' # 'goodbye'\n```\n\nThe `||` operator returns the left-hand argument if it is non-empty, otherwise\nit returns the right-hand argument:\n\n```justfile\nfoo := '' || 'goodbye'      # 'goodbye'\nbar := 'hello' || 'goodbye' # 'hello'\n```\n\n#### Joining Paths\n\nThe `/` operator can be used to join two strings with a slash:\n\n```just\nfoo := \"a\" / \"b\"\n```\n\n```\n$ just --evaluate foo\na/b\n```\n\nNote that a `/` is added even if one is already present:\n\n```just\nfoo := \"a/\"\nbar := foo / \"b\"\n```\n\n```\n$ just --evaluate bar\na//b\n```\n\nAbsolute paths can also be constructed<sup>1.5.0</sup>:\n\n```just\nfoo := / \"b\"\n```\n\n```\n$ just --evaluate foo\n/b\n```\n\nThe `/` operator uses the `/` character, even on Windows. Thus, using the `/`\noperator should be avoided with paths that use universal naming convention\n(UNC), i.e., those that start with `\\?`, since forward slashes are not\nsupported with UNC paths.\n\n#### Escaping `{{`\n\nTo write a recipe containing `{{`, use `{{{{`:\n\n```just\nbraces:\n  echo 'I {{{{LOVE}} curly braces!'\n```\n\n(An unmatched `}}` is ignored, so it doesn't need to be escaped.)\n\nAnother option is to put all the text you'd like to escape inside of an\ninterpolation:\n\n```just\nbraces:\n  echo '{{'I {{LOVE}} curly braces!'}}'\n```\n\nYet another option is to use `{{ \"{{\" }}`:\n\n```just\nbraces:\n  echo 'I {{ \"{{\" }}LOVE}} curly braces!'\n```\n\n### Strings\n\n`'single'`, `\"double\"`, and `'''triple'''` quoted string literals are\nsupported. Unlike in recipe bodies, `{{â€¦}}` interpolations are not supported\ninside strings.\n\nDouble-quoted strings support escape sequences:\n\n```just\ncarriage-return   := \"\\r\"\ndouble-quote      := \"\\\"\"\nnewline           := \"\\n\"\nno-newline        := \"\\\n\"\nslash             := \"\\\\\"\ntab               := \"\\t\"\nunicode-codepoint := \"\\u{1F916}\"\n```\n\n```console\n$ just --evaluate\n\"arriage-return   := \"\ndouble-quote      := \"\"\"\nnewline           := \"\n\"\nno-newline        := \"\"\nslash             := \"\\\"\ntab               := \"     \"\nunicode-codepoint := \"ğŸ¤–\"\n```\n\nThe unicode character escape sequence `\\u{â€¦}`<sup>1.36.0</sup> accepts up to\nsix hex digits.\n\nStrings may contain line breaks:\n\n```just\nsingle := '\nhello\n'\n\ndouble := \"\ngoodbye\n\"\n```\n\nSingle-quoted strings do not recognize escape sequences:\n\n```just\nescapes := '\\t\\n\\r\\\"\\\\'\n```\n\n```console\n$ just --evaluate\nescapes := \"\\t\\n\\r\\\"\\\\\"\n```\n\nIndented versions of both single- and double-quoted strings, delimited by\ntriple single- or double-quotes, are supported. Indented string lines are\nstripped of a leading line break, and leading whitespace common to all\nnon-blank lines:\n\n```just\n# this string will evaluate to `foo\\nbar\\n`\nx := '''\n  foo\n  bar\n'''\n\n# this string will evaluate to `abc\\n  wuv\\nxyz\\n`\ny := \"\"\"\n  abc\n    wuv\n  xyz\n\"\"\"\n```\n\nSimilar to unindented strings, indented double-quoted strings process escape\nsequences, and indented single-quoted strings ignore escape sequences. Escape\nsequence processing takes place after unindentation. The unindentation\nalgorithm does not take escape-sequence produced whitespace or newlines into\naccount.\n\n#### Shell-expanded strings\n\nStrings prefixed with `x` are shell expanded<sup>1.27.0</sup>:\n\n```justfile\nfoobar := x'~/$FOO/${BAR}'\n```\n\n| Value | Replacement |\n|------|-------------|\n| `$VAR` | value of environment variable `VAR` |\n| `${VAR}` | value of environment variable `VAR` |\n| `${VAR:-DEFAULT}` | value of environment variable `VAR`, or `DEFAULT` if `VAR` is not set |\n| Leading `~` | path to current user's home directory |\n| Leading `~USER` | path to `USER`'s home directory |\n\nThis expansion is performed at compile time, so variables from `.env` files and\nexported `just` variables cannot be used. However, this allows shell expanded\nstrings to be used in places like settings and import paths, which cannot\ndepend on `just` variables and `.env` files.\n\n#### Format strings\n\nStrings prefixed with `f` are format strings<sup>1.44.0</sup>:\n\n```justfile\nname := \"world\"\nmessage := f'Hello, {{name}}!'\n```\n\nFormat strings may contain interpolations delimited with `{{â€¦}}` that contain\nexpressions. Format strings evaluate to the concatenated string fragments and\nevaluated expressions.\n\nUse `{{{{` to include a literal `{{` in a format string:\n\n```justfile\nfoo := f'I {{{{LOVE} curly braces!'\n```\n\n### Ignoring Errors\n\nNormally, if a command returns a non-zero exit status, execution will stop. To\ncontinue execution after a command, even if it fails, prefix the command with\n`-`:\n\n```just\nfoo:\n  -cat foo\n  echo 'Done!'\n```\n\n```console\n$ just foo\ncat foo\ncat: foo: No such file or directory\necho 'Done!'\nDone!\n```\n\n### Functions\n\n`just` provides many built-in functions for use in expressions, including\nrecipe body `{{â€¦}}` substitutions, assignments, and default parameter values.\n\nAll functions ending in `_directory` can be abbreviated to `_dir`. So\n`home_directory()` can also be written as `home_dir()`. In addition,\n`invocation_directory_native()` can be abbreviated to\n`invocation_dir_native()`.\n\n#### System Information\n\n- `arch()` â€” Instruction set architecture. Possible values are: `\"aarch64\"`,\n  `\"arm\"`, `\"asmjs\"`, `\"hexagon\"`, `\"mips\"`, `\"msp430\"`, `\"powerpc\"`,\n  `\"powerpc64\"`, `\"s390x\"`, `\"sparc\"`, `\"wasm32\"`, `\"x86\"`, `\"x86_64\"`, and\n  `\"xcore\"`.\n- `num_cpus()`<sup>1.15.0</sup> - Number of logical CPUs.\n- `os()` â€” Operating system. Possible values are: `\"android\"`, `\"bitrig\"`,\n  `\"dragonfly\"`, `\"emscripten\"`, `\"freebsd\"`, `\"haiku\"`, `\"ios\"`, `\"linux\"`,\n  `\"macos\"`, `\"netbsd\"`, `\"openbsd\"`, `\"solaris\"`, and `\"windows\"`.\n- `os_family()` â€” Operating system family; possible values are: `\"unix\"` and\n  `\"windows\"`.\n\nFor example:\n\n```just\nsystem-info:\n  @echo \"This is an {{arch()}} machine\".\n```\n\n```console\n$ just system-info\nThis is an x86_64 machine\n```\n\nThe `os_family()` function can be used to create cross-platform `justfile`s\nthat work on various operating systems. For an example, see\n[cross-platform.just](https://github.com/casey/just/blob/master/examples/cross-platform.just)\nfile.\n\n#### External Commands\n\n- `shell(command, args...)`<sup>1.27.0</sup> returns the standard output of shell script\n  `command` with zero or more positional arguments `args`. The shell used to\n  interpret `command` is the same shell that is used to evaluate recipe lines,\n  and can be changed with `set shell := [â€¦]`.\n\n  `command` is passed as the first argument, so if the command is `'echo $@'`,\n  the full command line, with the default shell command `sh -cu` and `args`\n  `'foo'` and `'bar'` will be:\n\n  ```\n  'sh' '-cu' 'echo $@' 'echo $@' 'foo' 'bar'\n  ```\n\n  This is so that `$@` works as expected, and `$1` refers to the first\n  argument. `$@` does not include the first positional argument, which is\n  expected to be the name of the program being run.\n\n```just\n# arguments can be variables or expressions\nfile := '/sys/class/power_supply/BAT0/status'\nbat0stat := shell('cat $1', file)\n\n# commands can be variables or expressions\ncommand := 'wc -l'\noutput := shell(command + ' \"$1\"', 'main.c')\n\n# arguments referenced by the shell command must be used\nempty := shell('echo', 'foo')\nfull := shell('echo $1', 'foo')\nerror := shell('echo $1')\n```\n\n```just\n# Using python as the shell. Since `python -c` sets `sys.argv[0]` to `'-c'`,\n# the first \"real\" positional argument will be `sys.argv[2]`.\nset shell := [\"python3\", \"-c\"]\nolleh := shell('import sys; print(sys.argv[2][::-1])', 'hello')\n```\n\n#### Environment Variables\n\n- `env(key)`<sup>1.15.0</sup> â€” Retrieves the environment variable with name `key`, aborting\n  if it is not present.\n\n```just\nhome_dir := env('HOME')\n\ntest:\n  echo \"{{home_dir}}\"\n```\n\n```console\n$ just\n/home/user1\n```\n\n- `env(key, default)`<sup>1.15.0</sup> â€” Retrieves the environment variable with\n  name `key`, returning `default` if it is not present.\n- `env_var(key)` â€” Deprecated alias for `env(key)`.\n- `env_var_or_default(key, default)` â€” Deprecated alias for `env(key, default)`.\n\nA default can be substituted for an empty environment variable value with the\n`||` operator, currently unstable:\n\n```just\nset unstable\n\nfoo := env('FOO', '') || 'DEFAULT_VALUE'\n```\n\n#### Executables\n\n- `require(name)`<sup>1.39.0</sup> â€” Search directories in the `PATH`\n  environment variable for the executable `name` and return its full path, or\n  halt with an error if no executable with `name` exists.\n\n  ```just\n  bash := require(\"bash\")\n\n  @test:\n      echo \"bash: '{{bash}}'\"\n  ```\n\n  ```console\n  $ just\n  bash: '/bin/bash'\n  ```\n\n- `which(name)`<sup>1.39.0</sup> â€” Search directories in the `PATH` environment\n  variable for the executable `name` and return its full path, or the empty\n  string if no executable with `name` exists. Currently unstable.\n\n\n  ```just\n  set unstable\n\n  bosh := which(\"bosh\")\n\n  @test:\n      echo \"bosh: '{{bosh}}'\"\n  ```\n\n  ```console\n  $ just\n  bosh: ''\n  ```\n\n#### Invocation Information\n\n- `is_dependency()` - Returns the string `true` if the current recipe is being\n  run as a dependency of another recipe, rather than being run directly,\n  otherwise returns the string `false`.\n\n#### Invocation Directory\n\n- `invocation_directory()` - Retrieves the absolute path to the current\n  directory when `just` was invoked, before  `just` changed it (chdir'd) prior\n  to executing commands. On Windows, `invocation_directory()` uses `cygpath` to\n  convert the invocation directory to a Cygwin-compatible `/`-separated path.\n  Use `invocation_directory_native()` to return the verbatim invocation\n  directory on all platforms.\n\nFor example, to call `rustfmt` on files just under the \"current directory\"\n(from the user/invoker's perspective), use the following rule:\n\n```just\nrustfmt:\n  find {{invocation_directory()}} -name \\*.rs -exec rustfmt {} \\;\n```\n\nAlternatively, if your command needs to be run from the current directory, you\ncould use (e.g.):\n\n```just\nbuild:\n  cd {{invocation_directory()}}; ./some_script_that_needs_to_be_run_from_here\n```\n\n- `invocation_directory_native()` - Retrieves the absolute path to the current\n  directory when `just` was invoked, before  `just` changed it (chdir'd) prior\n  to executing commands.\n\n#### Justfile and Justfile Directory\n\n- `justfile()` - Retrieves the path of the current `justfile`.\n\n- `justfile_directory()` - Retrieves the path of the parent directory of the\n  current `justfile`.\n\nFor example, to run a command relative to the location of the current\n`justfile`:\n\n```just\nscript:\n  {{justfile_directory()}}/scripts/some_script\n```\n\n#### Source and Source Directory\n\n- `source_file()`<sup>1.27.0</sup> - Retrieves the path of the current source file.\n\n- `source_directory()`<sup>1.27.0</sup> - Retrieves the path of the parent directory of the\n  current source file.\n\n`source_file()` and `source_directory()` behave the same as `justfile()` and\n`justfile_directory()` in the root `justfile`, but will return the path and\ndirectory, respectively, of the current `import` or `mod` source file when\ncalled from within an import or submodule.\n\n#### Just Executable\n\n- `just_executable()` - Absolute path to the `just` executable.\n\nFor example:\n\n```just\nexecutable:\n  @echo The executable is at: {{just_executable()}}\n```\n\n```console\n$ just\nThe executable is at: /bin/just\n```\n\n#### Just Process ID\n\n- `just_pid()` - Process ID of the `just` executable.\n\nFor example:\n\n```just\npid:\n  @echo The process ID is: {{ just_pid() }}\n```\n\n```console\n$ just\nThe process ID is: 420\n```\n\n#### String Manipulation\n\n- `append(suffix, s)`<sup>1.27.0</sup> Append `suffix` to whitespace-separated\n  strings in `s`. `append('/src', 'foo bar baz')` â†’ `'foo/src bar/src baz/src'`\n- `prepend(prefix, s)`<sup>1.27.0</sup> Prepend `prefix` to\n  whitespace-separated strings in `s`. `prepend('src/', 'foo bar baz')` â†’\n  `'src/foo src/bar src/baz'`\n- `encode_uri_component(s)`<sup>1.27.0</sup> - Percent-encode characters in `s`\n  except `[A-Za-z0-9_.!~*'()-]`, matching the behavior of the\n  [JavaScript `encodeURIComponent` function](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/encodeURIComponent).\n- `quote(s)` - Replace all single quotes with `'\\''` and prepend and append\n  single quotes to `s`. This is sufficient to escape special characters for\n  many shells, including most Bourne shell descendants.\n- `replace(s, from, to)` - Replace all occurrences of `from` in `s` to `to`.\n- `replace_regex(s, regex, replacement)` - Replace all occurrences of `regex`\n  in `s` to `replacement`. Regular expressions are provided by the\n  [Rust `regex` crate](https://docs.rs/regex/latest/regex/). See the\n  [syntax documentation](https://docs.rs/regex/latest/regex/#syntax) for usage\n  examples. Capture groups are supported. The `replacement` string uses\n  [Replacement string syntax](https://docs.rs/regex/latest/regex/struct.Regex.html#replacement-string-syntax).\n- `trim(s)` - Remove leading and trailing whitespace from `s`.\n- `trim_end(s)` - Remove trailing whitespace from `s`.\n- `trim_end_match(s, substring)` - Remove suffix of `s` matching `substring`.\n- `trim_end_matches(s, substring)` - Repeatedly remove suffixes of `s` matching\n  `substring`.\n- `trim_start(s)` - Remove leading whitespace from `s`.\n- `trim_start_match(s, substring)` - Remove prefix of `s` matching `substring`.\n- `trim_start_matches(s, substring)` - Repeatedly remove prefixes of `s`\n  matching `substring`.\n\n#### Case Conversion\n\n- `capitalize(s)`<sup>1.7.0</sup> - Convert first character of `s` to uppercase\n  and the rest to lowercase.\n- `kebabcase(s)`<sup>1.7.0</sup> - Convert `s` to `kebab-case`.\n- `lowercamelcase(s)`<sup>1.7.0</sup> - Convert `s` to `lowerCamelCase`.\n- `lowercase(s)` - Convert `s` to lowercase.\n- `shoutykebabcase(s)`<sup>1.7.0</sup> - Convert `s` to `SHOUTY-KEBAB-CASE`.\n- `shoutysnakecase(s)`<sup>1.7.0</sup> - Convert `s` to `SHOUTY_SNAKE_CASE`.\n- `snakecase(s)`<sup>1.7.0</sup> - Convert `s` to `snake_case`.\n- `titlecase(s)`<sup>1.7.0</sup> - Convert `s` to `Title Case`.\n- `uppercamelcase(s)`<sup>1.7.0</sup> - Convert `s` to `UpperCamelCase`.\n- `uppercase(s)` - Convert `s` to uppercase.\n\n#### Path Manipulation\n\n##### Fallible\n\n- `absolute_path(path)` - Absolute path to relative `path` in the working\n  directory. `absolute_path(\"./bar.txt\")` in directory `/foo` is\n  `/foo/bar.txt`.\n- `canonicalize(path)`<sup>1.24.0</sup> - Canonicalize `path` by resolving symlinks and removing\n  `.`, `..`, and extra `/`s where possible.\n- `extension(path)` - Extension of `path`. `extension(\"/foo/bar.txt\")` is\n  `txt`.\n- `file_name(path)` - File name of `path` with any leading directory components\n  removed. `file_name(\"/foo/bar.txt\")` is `bar.txt`.\n- `file_stem(path)` - File name of `path` without extension.\n  `file_stem(\"/foo/bar.txt\")` is `bar`.\n- `parent_directory(path)` - Parent directory of `path`.\n  `parent_directory(\"/foo/bar.txt\")` is `/foo`.\n- `without_extension(path)` - `path` without extension.\n  `without_extension(\"/foo/bar.txt\")` is `/foo/bar`.\n\nThese functions can fail, for example if a path does not have an extension,\nwhich will halt execution.\n\n##### Infallible\n\n- `clean(path)` - Simplify `path` by removing extra path separators,\n  intermediate `.` components, and `..` where possible. `clean(\"foo//bar\")` is\n  `foo/bar`, `clean(\"foo/..\")` is `.`, `clean(\"foo/./bar\")` is `foo/bar`.\n- `join(a, bâ€¦)` - *This function uses `/` on Unix and `\\` on Windows, which can\n  be lead to unwanted behavior. The `/` operator, e.g., `a / b`, which always\n  uses `/`, should be considered as a replacement unless `\\`s are specifically\n  desired on Windows.* Join path `a` with path `b`. `join(\"foo/bar\", \"baz\")` is\n  `foo/bar/baz`. Accepts two or more arguments.\n\n#### Filesystem Access\n\n- `path_exists(path)` - Returns `true` if the path points at an existing entity\n  and `false` otherwise. Traverses symbolic links, and returns `false` if the\n  path is inaccessible or points to a broken symlink.\n- `read(path)`<sup>1.39.0</sup> - Returns the content of file at `path` as\n  string.\n\n##### Error Reporting\n\n- `error(message)` - Abort execution and report error `message` to user.\n\n#### UUID and Hash Generation\n\n- `blake3(string)`<sup>1.25.0</sup> - Return [BLAKE3] hash of `string` as hexadecimal string.\n- `blake3_file(path)`<sup>1.25.0</sup> - Return [BLAKE3] hash of file at `path` as hexadecimal\n  string.\n- `sha256(string)` - Return the SHA-256 hash of `string` as hexadecimal string.\n- `sha256_file(path)` - Return SHA-256 hash of file at `path` as hexadecimal\n  string.\n- `uuid()` - Generate a random version 4 UUID.\n\n[BLAKE3]: https://github.com/BLAKE3-team/BLAKE3/\n\n#### Random\n\n- `choose(n, alphabet)`<sup>1.27.0</sup> - Generate a string of `n` randomly\n  selected characters from `alphabet`, which may not contain repeated\n  characters. For example, `choose('64', HEX)` will generate a random\n  64-character lowercase hex string.\n\n#### Datetime\n\n- `datetime(format)`<sup>1.30.0</sup> - Return local time with `format`.\n- `datetime_utc(format)`<sup>1.30.0</sup> - Return UTC time with `format`.\n\nThe arguments to `datetime` and `datetime_utc` are `strftime`-style format\nstrings, see the\n[`chrono` library docs](https://docs.rs/chrono/latest/chrono/format/strftime/index.html)\nfor details.\n\n#### Semantic Versions\n\n- `semver_matches(version, requirement)`<sup>1.16.0</sup> - Check whether a\n  [semantic `version`](https://semver.org), e.g., `\"0.1.0\"` matches a\n  `requirement`, e.g., `\">=0.1.0\"`, returning `\"true\"` if so and `\"false\"`\n  otherwise.\n\n#### Style\n\n- `style(name)`<sup>1.37.0</sup> - Return a named terminal display attribute\n  escape sequence used by `just`. Unlike terminal display attribute escape\n  sequence constants, which contain standard colors and styles, `style(name)`\n  returns an escape sequence used by `just` itself, and can be used to make\n  recipe output match `just`'s own output.\n\n  Recognized values for `name` are `'command'`, for echoed recipe lines,\n  `error`, and `warning`.\n\n  For example, to style an error message:\n\n  ```just\n  scary:\n    @echo '{{ style(\"error\") }}OH NO{{ NORMAL }}'\n  ```\n\n##### User Directories<sup>1.23.0</sup>\n\nThese functions return paths to user-specific directories for things like\nconfiguration, data, caches, executables, and the user's home directory.\n\nOn Unix, these functions follow the\n[XDG Base Directory Specification](https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html).\n\nOn MacOS and Windows, these functions return the system-specified user-specific\ndirectories. For example, `cache_directory()` returns `~/Library/Caches` on\nMacOS and `{FOLDERID_LocalAppData}` on Windows.\n\nSee the [`dirs`](https://docs.rs/dirs/latest/dirs/index.html) crate for more\ndetails.\n\n- `cache_directory()` - The user-specific cache directory.\n- `config_directory()` - The user-specific configuration directory.\n- `config_local_directory()` - The local user-specific configuration directory.\n- `data_directory()` - The user-specific data directory.\n- `data_local_directory()` - The local user-specific data directory.\n- `executable_directory()` - The user-specific executable directory.\n- `home_directory()` - The user's home directory.\n\nIf you would like to use XDG base directories on all platforms you can use the\n`env(â€¦)` function with the appropriate environment variable and fallback,\nalthough note that the XDG specification requires ignoring non-absolute paths,\nso for full compatibility with spec-compliant applications, you would need to\ndo:\n\n```just\nxdg_config_dir := if env('XDG_CONFIG_HOME', '') =~ '^/' {\n  env('XDG_CONFIG_HOME')\n} else {\n  home_directory() / '.config'\n}\n```\n\n### Constants\n\nA number of constants are predefined:\n\n| Name | Value | Value on Windows |\n|---|---|---|\n| `HEX`<sup>1.27.0</sup> | `\"0123456789abcdef\"` |  |\n| `HEXLOWER`<sup>1.27.0</sup> | `\"0123456789abcdef\"` |  |\n| `HEXUPPER`<sup>1.27.0</sup> | `\"0123456789ABCDEF\"` |  |\n| `PATH_SEP`<sup>1.41.0</sup> | `\"/\"` | `\"\\\"` |\n| `PATH_VAR_SEP`<sup>1.41.0</sup> | `\":\"` | `\";\"` |\n| `CLEAR`<sup>1.37.0</sup> | `\"\\ec\"` |  |\n| `NORMAL`<sup>1.37.0</sup> | `\"\\e[0m\"` |  |\n| `BOLD`<sup>1.37.0</sup> | `\"\\e[1m\"` |  |\n| `ITALIC`<sup>1.37.0</sup> | `\"\\e[3m\"` |  |\n| `UNDERLINE`<sup>1.37.0</sup> | `\"\\e[4m\"` |  |\n| `INVERT`<sup>1.37.0</sup> | `\"\\e[7m\"` |  |\n| `HIDE`<sup>1.37.0</sup> | `\"\\e[8m\"` |  |\n| `STRIKETHROUGH`<sup>1.37.0</sup> | `\"\\e[9m\"` |  |\n| `BLACK`<sup>1.37.0</sup> | `\"\\e[30m\"` |  |\n| `RED`<sup>1.37.0</sup> | `\"\\e[31m\"` |  |\n| `GREEN`<sup>1.37.0</sup> | `\"\\e[32m\"` |  |\n| `YELLOW`<sup>1.37.0</sup> | `\"\\e[33m\"` |  |\n| `BLUE`<sup>1.37.0</sup> | `\"\\e[34m\"` |  |\n| `MAGENTA`<sup>1.37.0</sup> | `\"\\e[35m\"` |  |\n| `CYAN`<sup>1.37.0</sup> | `\"\\e[36m\"` |  |\n| `WHITE`<sup>1.37.0</sup> | `\"\\e[37m\"` |  |\n| `BG_BLACK`<sup>1.37.0</sup> | `\"\\e[40m\"` |  |\n| `BG_RED`<sup>1.37.0</sup> | `\"\\e[41m\"` |  |\n| `BG_GREEN`<sup>1.37.0</sup> | `\"\\e[42m\"` |  |\n| `BG_YELLOW`<sup>1.37.0</sup> | `\"\\e[43m\"` |  |\n| `BG_BLUE`<sup>1.37.0</sup> | `\"\\e[44m\"` |  |\n| `BG_MAGENTA`<sup>1.37.0</sup> | `\"\\e[45m\"` |  |\n| `BG_CYAN`<sup>1.37.0</sup> | `\"\\e[46m\"` |  |\n| `BG_WHITE`<sup>1.37.0</sup> | `\"\\e[47m\"` |  |\n\n```just\n@foo:\n  echo {{HEX}}\n```\n\n```console\n$ just foo\n0123456789abcdef\n```\n\nConstants starting with `\\e` are\n[ANSI escape sequences](https://en.wikipedia.org/wiki/ANSI_escape_code).\n\n`CLEAR` clears the screen, similar to the `clear` command. The rest are of the\nform `\\e[Nm`, where `N` is an integer, and set terminal display attributes.\n\nTerminal display attribute escape sequences can be combined, for example text\nweight `BOLD`, text style `STRIKETHROUGH`, foreground color `CYAN`, and\nbackground color `BG_BLUE`. They should be followed by `NORMAL`, to reset the\nterminal back to normal.\n\nEscape sequences should be quoted, since `[` is treated as a special character\nby some shells.\n\n```just\n@foo:\n  echo '{{BOLD + STRIKETHROUGH + CYAN + BG_BLUE}}Hi!{{NORMAL}}'\n```\n\n### Attributes\n\nRecipes, `mod` statements, and aliases may be annotated with attributes that\nchange their behavior.\n\n| Name | Type | Description |\n|------|------|-------------|\n| `[arg(ARG, help=\"HELP\")]`<sup>1.46.0</sup> | recipe | Print help string `HELP` for `ARG` in usage messages. |\n| `[arg(ARG, long=\"LONG\")]`<sup>1.46.0</sup> | recipe | Require values of argument `ARG` to be passed as `--LONG` option. |\n| `[arg(ARG, short=\"S\")]`<sup>1.46.0</sup> | recipe | Require values of argument `ARG` to be passed as short `-S` option. |\n| `[arg(ARG, value=\"VALUE\")]`<sup>1.46.0</sup> | recipe | Makes option `ARG` a flag which does not take a value. |\n| `[arg(ARG, pattern=\"PATTERN\")]`<sup>1.45.0</sup> | recipe | Require values of argument `ARG` to match regular expression `PATTERN`. |\n| `[confirm]`<sup>1.17.0</sup> | recipe | Require confirmation prior to executing recipe. |\n| `[confirm(PROMPT)]`<sup>1.23.0</sup> | recipe | Require confirmation prior to executing recipe with a custom prompt. |\n| `[default]`<sup>1.43.0</sup> | recipe | Use recipe as module's default recipe. |\n| `[doc(DOC)]`<sup>1.27.0</sup> | module, recipe | Set recipe or module's [documentation comment](#documentation-comments) to `DOC`. |\n| `[extension(EXT)]`<sup>1.32.0</sup> | recipe | Set shebang recipe script's file extension to `EXT`. `EXT` should include a period if one is desired. |\n| `[group(NAME)]`<sup>1.27.0</sup> | module, recipe | Put recipe or module in in [group](#groups) `NAME`. |\n| `[linux]`<sup>1.8.0</sup> | recipe | Enable recipe on Linux. |\n| `[macos]`<sup>1.8.0</sup> | recipe | Enable recipe on MacOS. |\n| `[metadata(METADATA)]`<sup>1.42.0</sup> | recipe | Attach `METADATA` to recipe. |\n| `[no-cd]`<sup>1.9.0</sup> | recipe | Don't change directory before executing recipe. |\n| `[no-exit-message]`<sup>1.7.0</sup> | recipe | Don't print an error message if recipe fails. |\n| `[no-quiet]`<sup>1.23.0</sup> | recipe | Override globally quiet recipes and always echo out the recipe. |\n| `[openbsd]`<sup>1.38.0</sup> | recipe | Enable recipe on OpenBSD. |\n| `[parallel]`<sup>1.42.0</sup> | recipe | Run this recipe's dependencies in parallel. |\n| `[positional-arguments]`<sup>1.29.0</sup> | recipe | Turn on [positional arguments](#positional-arguments) for this recipe. |\n| `[private]`<sup>1.10.0</sup> | alias, recipe | Make recipe, alias, or variable private. See [Private Recipes](#private-recipes). |\n| `[script]`<sup>1.33.0</sup> | recipe | Execute recipe as script. See [script recipes](#script-recipes) for more details. |\n| `[script(COMMAND)]`<sup>1.32.0</sup> | recipe | Execute recipe as a script interpreted by `COMMAND`. See [script recipes](#script-recipes) for more details. |\n| `[unix]`<sup>1.8.0</sup> | recipe | Enable recipe on Unixes. (Includes MacOS). |\n| `[windows]`<sup>1.8.0</sup> | recipe | Enable recipe on Windows. |\n| `[working-directory(PATH)]`<sup>1.38.0</sup> | recipe | Set recipe working directory. `PATH` may be relative or absolute. If relative, it is interpreted relative to the default working directory. |\n\nA recipe can have multiple attributes, either on multiple lines:\n\n```just\n[no-cd]\n[private]\nfoo:\n    echo \"foo\"\n```\n\nOr separated by commas on a single line<sup>1.14.0</sup>:\n\n```just\n[no-cd, private]\nfoo:\n    echo \"foo\"\n```\n\nAttributes with a single argument may be written with a colon:\n\n```just\n[group: 'bar']\nfoo:\n```\n\n#### Enabling and Disabling Recipes<sup>1.8.0</sup>\n\nThe `[linux]`, `[macos]`, `[unix]`, and `[windows]` attributes are\nconfiguration attributes. By default, recipes are always enabled. A recipe with\none or more configuration attributes will only be enabled when one or more of\nthose configurations is active.\n\nThis can be used to write `justfile`s that behave differently depending on\nwhich operating system they run on. The `run` recipe in this `justfile` will\ncompile and run `main.c`, using a different C compiler and using the correct\noutput binary name for that compiler depending on the operating system:\n\n```just\n[unix]\nrun:\n  cc main.c\n  ./a.out\n\n[windows]\nrun:\n  cl main.c\n  main.exe\n```\n\n#### Disabling Changing Directory<sup>1.9.0</sup>\n\n`just` normally executes recipes with the current directory set to the\ndirectory that contains the `justfile`. This can be disabled using the\n`[no-cd]` attribute. This can be used to create recipes which use paths\nrelative to the invocation directory, or which operate on the current\ndirectory.\n\nFor example, this `commit` recipe:\n\n```just\n[no-cd]\ncommit file:\n  git add {{file}}\n  git commit\n```\n\nCan be used with paths that are relative to the current directory, because\n`[no-cd]` prevents `just` from changing the current directory when executing\n`commit`.\n\n#### Requiring Confirmation for Recipes<sup>1.17.0</sup>\n\n`just` normally executes all recipes unless there is an error. The `[confirm]`\nattribute allows recipes require confirmation in the terminal prior to running.\nThis can be overridden by passing `--yes` to `just`, which will automatically\nconfirm any recipes marked by this attribute.\n\nRecipes dependent on a recipe that requires confirmation will not be run if the\nrelied upon recipe is not confirmed, as well as recipes passed after any recipe\nthat requires confirmation.\n\n```just\n[confirm]\ndelete-all:\n  rm -rf *\n```\n\n#### Custom Confirmation Prompt<sup>1.23.0</sup>\n\nThe default confirmation prompt can be overridden with `[confirm(PROMPT)]`:\n\n```just\n[confirm(\"Are you sure you want to delete everything?\")]\ndelete-everything:\n  rm -rf *\n```\n\n### Groups\n\nRecipes and modules may be annotated with one or more group names:\n\n```just\n[group('lint')]\njs-lint:\n    echo 'Running JS linterâ€¦'\n\n[group('rust recipes')]\n[group('lint')]\nrust-lint:\n    echo 'Running Rust linterâ€¦'\n\n[group('lint')]\ncpp-lint:\n  echo 'Running C++ linterâ€¦'\n\n# not in any group\nemail-everyone:\n    echo 'Sending mass emailâ€¦'\n```\n\nRecipes are listed by group:\n\n```\n$ just --list\nAvailable recipes:\n    email-everyone # not in any group\n\n    [lint]\n    cpp-lint\n    js-lint\n    rust-lint\n\n    [rust recipes]\n    rust-lint\n```\n\n`just --list --unsorted` prints recipes in their justfile order within each group:\n\n```\n$ just --list --unsorted\nAvailable recipes:\n    (no group)\n    email-everyone # not in any group\n\n    [lint]\n    js-lint\n    rust-lint\n    cpp-lint\n\n    [rust recipes]\n    rust-lint\n```\n\nGroups can be listed with `--groups`:\n\n```\n$ just --groups\nRecipe groups:\n  lint\n  rust recipes\n```\n\nUse `just --groups --unsorted` to print groups in their justfile order.\n\n### Command Evaluation Using Backticks\n\nBackticks can be used to store the result of commands:\n\n```just\nlocalhost := `dumpinterfaces | cut -d: -f2 | sed 's/\\/.*//' | sed 's/ //g'`\n\nserve:\n  ./serve {{localhost}} 8080\n```\n\nIndented backticks, delimited by three backticks, are de-indented in the same\nmanner as indented strings:\n\n````just\n# This backtick evaluates the command `echo foo\\necho bar\\n`, which produces the value `foo\\nbar\\n`.\nstuff := ```\n    echo foo\n    echo bar\n  ```\n````\n\nSee the [Strings](#strings) section for details on unindenting.\n\nBackticks may not start with `#!`. This syntax is reserved for a future\nupgrade.\n\nThe [`shell(â€¦)` function](#external-commands) provides a more general mechanism\nto invoke external commands, including the ability to execute the contents of a\nvariable as a command, and to pass arguments to a command.\n\n### Conditional Expressions\n\n`if`/`else` expressions evaluate different branches depending on if two\nexpressions evaluate to the same value:\n\n```just\nfoo := if \"2\" == \"2\" { \"Good!\" } else { \"1984\" }\n\nbar:\n  @echo \"{{foo}}\"\n```\n\n```console\n$ just bar\nGood!\n```\n\nIt is also possible to test for inequality:\n\n```just\nfoo := if \"hello\" != \"goodbye\" { \"xyz\" } else { \"abc\" }\n\nbar:\n  @echo {{foo}}\n```\n\n```console\n$ just bar\nxyz\n```\n\nAnd match against regular expressions:\n\n```just\nfoo := if \"hello\" =~ 'hel+o' { \"match\" } else { \"mismatch\" }\n\nbar:\n  @echo {{foo}}\n```\n\n```console\n$ just bar\nmatch\n```\n\nRegular expressions are provided by the\n[regex crate](https://github.com/rust-lang/regex), whose syntax is documented on\n[docs.rs](https://docs.rs/regex/1.5.4/regex/#syntax). Since regular expressions\ncommonly use backslash escape sequences, consider using single-quoted string\nliterals, which will pass slashes to the regex parser unmolested.\n\nConditional expressions short-circuit, which means they only evaluate one of\ntheir branches. This can be used to make sure that backtick expressions don't\nrun when they shouldn't.\n\n```just\nfoo := if env_var(\"RELEASE\") == \"true\" { `get-something-from-release-database` } else { \"dummy-value\" }\n```\n\nConditionals can be used inside of recipes:\n\n```just\nbar foo:\n  echo {{ if foo == \"bar\" { \"hello\" } else { \"goodbye\" } }}\n```\n\nMultiple conditionals can be chained:\n\n```just\nfoo := if \"hello\" == \"goodbye\" {\n  \"xyz\"\n} else if \"a\" == \"a\" {\n  \"abc\"\n} else {\n  \"123\"\n}\n\nbar:\n  @echo {{foo}}\n```\n\n```console\n$ just bar\nabc\n```\n\n### Stopping execution with error\n\nExecution can be halted with the `error` function. For example:\n\n```just\nfoo := if \"hello\" == \"goodbye\" {\n  \"xyz\"\n} else if \"a\" == \"b\" {\n  \"abc\"\n} else {\n  error(\"123\")\n}\n```\n\nWhich produce the following error when run:\n\n```\nerror: Call to function `error` failed: 123\n   |\n16 |   error(\"123\")\n```\n\n### Setting Variables from the Command Line\n\nVariables can be overridden from the command line.\n\n```just\nos := \"linux\"\n\ntest: build\n  ./test --test {{os}}\n\nbuild:\n  ./build {{os}}\n```\n\n```console\n$ just\n./build linux\n./test --test linux\n```\n\nAny number of arguments of the form `NAME=VALUE` can be passed before recipes:\n\n```console\n$ just os=plan9\n./build plan9\n./test --test plan9\n```\n\nOr you can use the `--set` flag:\n\n```console\n$ just --set os bsd\n./build bsd\n./test --test bsd\n```\n\n### Getting and Setting Environment Variables\n\n#### Exporting `just` Variables\n\nAssignments prefixed with the `export` keyword will be exported to recipes as\nenvironment variables:\n\n```just\nexport RUST_BACKTRACE := \"1\"\n\ntest:\n  # will print a stack trace if it crashes\n  cargo test\n```\n\nParameters prefixed with a `$` will be exported as environment variables:\n\n```just\ntest $RUST_BACKTRACE=\"1\":\n  # will print a stack trace if it crashes\n  cargo test\n```\n\nExported variables and parameters are not exported to backticks in the same scope.\n\n```just\nexport WORLD := \"world\"\n# This backtick will fail with \"WORLD: unbound variable\"\nBAR := `echo hello $WORLD`\n```\n\n```just\n# Running `just a foo` will fail with \"A: unbound variable\"\na $A $B=`echo $A`:\n  echo $A $B\n```\n\nWhen [export](#export) is set, all `just` variables are exported as environment\nvariables.\n\n#### Unexporting Environment Variables<sup>1.29.0</sup>\n\nEnvironment variables can be unexported with the `unexport keyword`:\n\n```just\nunexport FOO\n\n@foo:\n  echo $FOO\n```\n\n```\n$ export FOO=bar\n$ just foo\nsh: FOO: unbound variable\n```\n\n#### Getting Environment Variables from the environment\n\nEnvironment variables from the environment are passed automatically to the\nrecipes.\n\n```just\nprint_home_folder:\n  echo \"HOME is: '${HOME}'\"\n```\n\n```console\n$ just\nHOME is '/home/myuser'\n```\n\n#### Setting `just` Variables from Environment Variables\n\nEnvironment variables can be propagated to `just` variables using the `env()` function.\nSee\n[environment-variables](#environment-variables).\n\n### Recipe Parameters\n\nRecipes may have parameters. Here recipe `build` has a parameter called\n`target`:\n\n```just\nbuild target:\n  @echo 'Building {{target}}â€¦'\n  cd {{target}} && make\n```\n\nTo pass arguments on the command line, put them after the recipe name:\n\n```console\n$ just build my-awesome-project\nBuilding my-awesome-projectâ€¦\ncd my-awesome-project && make\n```\n\nTo pass arguments to a dependency, put the dependency in parentheses along with\nthe arguments:\n\n```just\ndefault: (build \"main\")\n\nbuild target:\n  @echo 'Building {{target}}â€¦'\n  cd {{target}} && make\n```\n\nVariables can also be passed as arguments to dependencies:\n\n```just\ntarget := \"main\"\n\n_build version:\n  @echo 'Building {{version}}â€¦'\n  cd {{version}} && make\n\nbuild: (_build target)\n```\n\nA command's arguments can be passed to dependency by putting the dependency in\nparentheses along with the arguments:\n\n```just\nbuild target:\n  @echo \"Building {{target}}â€¦\"\n\npush target: (build target)\n  @echo 'Pushing {{target}}â€¦'\n```\n\nParameters may have default values:\n\n```just\ndefault := 'all'\n\ntest target tests=default:\n  @echo 'Testing {{target}}:{{tests}}â€¦'\n  ./test --tests {{tests}} {{target}}\n```\n\nParameters with default values may be omitted:\n\n```console\n$ just test server\nTesting server:allâ€¦\n./test --tests all server\n```\n\nOr supplied:\n\n```console\n$ just test server unit\nTesting server:unitâ€¦\n./test --tests unit server\n```\n\nDefault values may be arbitrary expressions, but expressions containing the\n`+`, `&&`, `||`, or `/` operators must be parenthesized:\n\n```just\narch := \"wasm\"\n\ntest triple=(arch + \"-unknown-unknown\") input=(arch / \"input.dat\"):\n  ./test {{triple}}\n```\n\nThe last parameter of a recipe may be variadic, indicated with either a `+` or\na `*` before the argument name:\n\n```just\nbackup +FILES:\n  scp {{FILES}} me@server.com:\n```\n\nVariadic parameters prefixed with `+` accept _one or more_ arguments and expand\nto a string containing those arguments separated by spaces:\n\n```console\n$ just backup FAQ.md GRAMMAR.md\nscp FAQ.md GRAMMAR.md me@server.com:\nFAQ.md                  100% 1831     1.8KB/s   00:00\nGRAMMAR.md              100% 1666     1.6KB/s   00:00\n```\n\nVariadic parameters prefixed with `*` accept _zero or more_ arguments and\nexpand to a string containing those arguments separated by spaces, or an empty\nstring if no arguments are present:\n\n```just\ncommit MESSAGE *FLAGS:\n  git commit {{FLAGS}} -m \"{{MESSAGE}}\"\n```\n\nVariadic parameters can be assigned default values. These are overridden by\narguments passed on the command line:\n\n```just\ntest +FLAGS='-q':\n  cargo test {{FLAGS}}\n```\n\n`{{â€¦}}` substitutions may need to be quoted if they contain spaces. For\nexample, if you have the following recipe:\n\n```just\nsearch QUERY:\n  lynx https://www.google.com/?q={{QUERY}}\n```\n\nAnd you type:\n\n```console\n$ just search \"cat toupee\"\n```\n\n`just` will run the command `lynx https://www.google.com/?q=cat toupee`, which\nwill get parsed by `sh` as `lynx`, `https://www.google.com/?q=cat`, and\n`toupee`, and not the intended `lynx` and `https://www.google.com/?q=cat toupee`.\n\nYou can fix this by adding quotes:\n\n```just\nsearch QUERY:\n  lynx 'https://www.google.com/?q={{QUERY}}'\n```\n\nParameters prefixed with a `$` will be exported as environment variables:\n\n```just\nfoo $bar:\n  echo $bar\n```\n\nParameters may be constrained to match regular expression patterns using the\n`[arg(\"name\", pattern=\"pattern\")]` attribute<sup>1.45.0</sup>:\n\n```just\n[arg('n', pattern='\\d+')]\ndouble n:\n  echo $(({{n}} * 2))\n```\n\nA leading `^` and trailing `$` are added to the pattern, so it must match the\nentire argument value.\n\nYou may constrain the pattern to a number of alternatives using the `|`\noperator:\n\n```just\n[arg('flag', pattern='--help|--version')]\ninfo flag:\n  just {{flag}}\n```\n\nRegular expressions are provided by the\n[Rust `regex` crate](https://docs.rs/regex/latest/regex/). See the\n[syntax documentation](https://docs.rs/regex/latest/regex/#syntax) for usage\nexamples.\n\nUsage information for a recipe may be printed with the `--usage`\nsubcommand<sup>1.46.0</sup>:\n\n```console\n$ just --usage foo\nUsage: just foo [OPTIONS] bar\n\nArguments:\n  bar\n```\n\nHelp strings may be added to arguments using the `[arg(ARG, help=HELP)]` attribute:\n\n```just\n[arg(\"bar\", help=\"hello\")]\nfoo bar:\n```\n\n```console\n$ just --usage foo\nUsage: just foo bar\n\nArguments:\n  bar hello\n```\n\n#### Recipe Flags and Options\n\nRecipe parameters are positional by default.\n\nIn this `justfile`:\n\n```just\n@foo bar:\n  echo bar={{bar}}\n```\n\nThe parameter `bar` is positional:\n\n```console\n$ just foo hello\nbar=hello\n```\n\nThe `[arg(ARG, long=OPTION)]`<sup>1.46.0</sup> attribute can be used to make a\nparameter a long option.\n\nIn this `justfile`:\n\n```just\n[arg(\"bar\", long=\"bar\")]\nfoo bar:\n```\n\nThe parameter `bar` is given with the `--bar` option:\n\n```console\n$ just foo --bar hello\nbar=hello\n```\n\nOptions may also be passed with `--name=value` syntax:\n\n```console\n$ just foo --bar=hello\nbar=hello\n```\n\nThe value of `long` can be omitted, in which case the option defaults to the\nname of the parameter:\n\n```just\n[arg(\"bar\", long)]\nfoo bar:\n```\n\nThe `[arg(ARG, short=OPTION)]`<sup>1.46.0</sup> attribute can be used to make a\nparameter a short option.\n\nIn this `justfile`:\n\n```just\n[arg(\"bar\", short=\"b\")]\nfoo bar:\n```\n\nThe parameter `bar` is given with the `-b` option:\n\n```console\n$ just foo -b hello\nbar=hello\n```\n\nIf a parameter has both a long and short option, it may be passed using either.\n\nVariadic `+` and `?` parameters cannot be options.\n\nThe `[arg(ARG, value=VALUE, â€¦)]`<sup>1.46.0</sup> attribute can be used with\n`long` or `short` to make a parameter a flag which does not take a value.\n\nIn this `justfile`:\n\n```just\n[arg(\"bar\", long=\"bar\", value=\"hello\")]\nfoo bar:\n```\n\nThe parameter `bar` is given with the `--bar` option, but does not take a\nvalue, and instead takes the value given in the `[arg]` attribute:\n\n```console\n$ just foo --bar\nbar=hello\n```\n\nThis is useful for unconditionally requiring a flag like `--force` on dangerous\ncommands.\n\nA flag is optional if its parameter has a default:\n\n```just\n[arg(\"bar\", long=\"bar\", value=\"hello\")]\nfoo bar=\"goodbye\":\n```\n\nCausing it to receive the default when not passed in the invocation:\n\n```console\n$ just foo\nbar=goodbye\n```\n\n### Dependencies\n\nDependencies run before recipes that depend on them:\n\n```just\na: b\n  @echo A\n\nb:\n  @echo B\n```\n\n```\n$ just a\nB\nA\n```\n\nIn a given invocation of `just`, a recipe with the same arguments will only run\nonce, regardless of how many times it appears in the command-line invocation,\nor how many times it appears as a dependency:\n\n```just\na:\n  @echo A\n\nb: a\n  @echo B\n\nc: a\n  @echo C\n```\n\n```\n$ just a a a a a\nA\n$ just b c\nA\nB\nC\n```\n\nMultiple recipes may depend on a recipe that performs some kind of setup, and\nwhen those recipes run, that setup will only be performed once:\n\n```just\nbuild:\n  cc main.c\n\ntest-foo: build\n  ./a.out --test foo\n\ntest-bar: build\n  ./a.out --test bar\n```\n\n```\n$ just test-foo test-bar\ncc main.c\n./a.out --test foo\n./a.out --test bar\n```\n\nRecipes in a given run are only skipped when they receive the same arguments:\n\n```just\nbuild:\n  cc main.c\n\ntest TEST: build\n  ./a.out --test {{TEST}}\n```\n\n```\n$ just test foo test bar\ncc main.c\n./a.out --test foo\n./a.out --test bar\n```\n\n#### Running Recipes at the End of a Recipe\n\nNormal dependencies of a recipes always run before a recipe starts. That is to\nsay, the dependee always runs before the depender. These dependencies are\ncalled \"prior dependencies\".\n\nA recipe can also have subsequent dependencies, which run immediately after the\nrecipe and are introduced with an `&&`:\n\n```just\na:\n  echo 'A!'\n\nb: a && c d\n  echo 'B!'\n\nc:\n  echo 'C!'\n\nd:\n  echo 'D!'\n```\n\nâ€¦running _b_ prints:\n\n```console\n$ just b\necho 'A!'\nA!\necho 'B!'\nB!\necho 'C!'\nC!\necho 'D!'\nD!\n```\n\n#### Running Recipes in the Middle of a Recipe\n\n`just` doesn't support running recipes in the middle of another recipe, but you\ncan call `just` recursively in the middle of a recipe. Given the following\n`justfile`:\n\n```just\na:\n  echo 'A!'\n\nb: a\n  echo 'B start!'\n  just c\n  echo 'B end!'\n\nc:\n  echo 'C!'\n```\n\nâ€¦running _b_ prints:\n\n```console\n$ just b\necho 'A!'\nA!\necho 'B start!'\nB start!\necho 'C!'\nC!\necho 'B end!'\nB end!\n```\n\nThis has limitations, since recipe `c` is run with an entirely new invocation\nof `just`: Assignments will be recalculated, dependencies might run twice, and\ncommand line arguments will not be propagated to the child `just` process.\n\n### Shebang Recipes\n\nRecipes that start with `#!` are called shebang recipes, and are executed by\nsaving the recipe body to a file and running it. This lets you write recipes in\ndifferent languages:\n\n```just\npolyglot: python js perl sh ruby nu\n\npython:\n  #!/usr/bin/env python3\n  print('Hello from python!')\n\njs:\n  #!/usr/bin/env node\n  console.log('Greetings from JavaScript!')\n\nperl:\n  #!/usr/bin/env perl\n  print \"Larry Wall says Hi!\\n\";\n\nsh:\n  #!/usr/bin/env sh\n  hello='Yo'\n  echo \"$hello from a shell script!\"\n\nnu:\n  #!/usr/bin/env nu\n  let hello = 'Hola'\n  echo $\"($hello) from a nushell script!\"\n\nruby:\n  #!/usr/bin/env ruby\n  puts \"Hello from ruby!\"\n```\n\n```console\n$ just polyglot\nHello from python!\nGreetings from JavaScript!\nLarry Wall says Hi!\nYo from a shell script!\nHola from a nushell script!\nHello from ruby!\n```\n\nOn Unix-like operating systems, including Linux and MacOS, shebang recipes are\nexecuted by saving the recipe body to a file in a temporary directory, marking\nthe file as executable, and executing it. The OS then parses the shebang line\ninto a command line and invokes it, including the path to the file. For\nexample, if a recipe starts with `#!/usr/bin/env bash`, the final command that\nthe OS runs will be something like `/usr/bin/env bash\n/tmp/PATH_TO_SAVED_RECIPE_BODY`.\n\nShebang line splitting is operating system dependent. When passing a command\nwith arguments, you may need to tell `env` to split them explicitly by using\nthe `-S` flag:\n\n```just\nrun:\n  #!/usr/bin/env -S bash -x\n  ls\n```\n\nWindows does not support shebang lines. On Windows, `just` splits the shebang\nline into a command and arguments, saves the recipe body to a file, and invokes\nthe split command and arguments, adding the path to the saved recipe body as\nthe final argument. For example, on Windows, if a recipe starts with `#! py`,\nthe final command the OS runs will be something like\n`py C:\\Temp\\PATH_TO_SAVED_RECIPE_BODY`.\n\n### Script Recipes\n\nRecipes with a `[script(COMMAND)]`<sup>1.32.0</sup> attribute are run as\nscripts interpreted by `COMMAND`. This avoids some of the issues with shebang\nrecipes, such as the use of `cygpath` on Windows, the need to use\n`/usr/bin/env`, inconsistencies in shebang line splitting across Unix OSs, and\nrequiring a temporary directory from which files can be executed.\n\nRecipes with an empty `[script]` attribute are executed with the value of `set\nscript-interpreter := [â€¦]`<sup>1.33.0</sup>, defaulting to `sh -eu`, and *not*\nthe value of `set shell`.\n\nThe body of the recipe is evaluated, written to disk in the temporary\ndirectory, and run by passing its path as an argument to `COMMAND`.\n\n### Script and Shebang Recipe Temporary Files\n\nBoth script and shebang recipes write the recipe body to a temporary file for\nexecution. Script recipes execute that file by passing it to a command, while\nshebang recipes execute the file directly. Shebang recipe execution will fail\nif the filesystem containing the temporary file is mounted with `noexec` or is\notherwise non-executable.\n\nThe directory that `just` writes temporary files to may be configured in a\nnumber of ways, from highest to lowest precedence:\n\n- Globally with the `--tempdir` command-line option or the `JUST_TEMPDIR`\n  environment variable<sup>1.41.0</sup>.\n\n- On a per-module basis with the `tempdir` setting.\n\n- Globally on Linux with the `XDG_RUNTIME_DIR` environment variable.\n\n- Falling back to the directory returned by\n  [std::env::temp_dir](https://doc.rust-lang.org/std/env/fn.temp_dir.html).\n\n### Python Recipes with `uv`\n\n[`uv`](https://github.com/astral-sh/uv) is an excellent cross-platform python\nproject manager, written in Rust.\n\nUsing the `[script]` attribute and `script-interpreter` setting, `just` can\neasily be configured to run Python recipes with `uv`:\n\n```just\nset unstable\n\nset script-interpreter := ['uv', 'run', '--script']\n\n[script]\nhello:\n  print(\"Hello from Python!\")\n\n[script]\ngoodbye:\n  # /// script\n  # requires-python = \">=3.11\"\n  # dependencies=[\"sh\"]\n  # ///\n  import sh\n  print(sh.echo(\"Goodbye from Python!\"), end='')\n```\n\nOf course, a shebang also works:\n\n```just\nhello:\n  #!/usr/bin/env -S uv run --script\n  print(\"Hello from Python!\")\n```\n\n\n### Safer Bash Shebang Recipes\n\nIf you're writing a `bash` shebang recipe, consider adding `set -euxo\npipefail`:\n\n```just\nfoo:\n  #!/usr/bin/env bash\n  set -euxo pipefail\n  hello='Yo'\n  echo \"$hello from Bash!\"\n```\n\nIt isn't strictly necessary, but `set -euxo pipefail` turns on a few useful\nfeatures that make `bash` shebang recipes behave more like normal, linewise\n`just` recipe:\n\n- `set -e` makes `bash` exit if a command fails.\n\n- `set -u` makes `bash` exit if a variable is undefined.\n\n- `set -x` makes `bash` print each script line before it's run.\n\n- `set -o pipefail` makes `bash` exit if a command in a pipeline fails. This is\n  `bash`-specific, so isn't turned on in normal linewise `just` recipes.\n\nTogether, these avoid a lot of shell scripting gotchas.\n\n#### Shebang Recipe Execution on Windows\n\nOn Windows, shebang interpreter paths containing a `/` are translated from\nUnix-style paths to Windows-style paths using `cygpath`, a utility that ships\nwith [Cygwin](http://www.cygwin.com).\n\nFor example, to execute this recipe on Windows:\n\n```just\necho:\n  #!/bin/sh\n  echo \"Hello!\"\n```\n\nThe interpreter path `/bin/sh` will be translated to a Windows-style path using\n`cygpath` before being executed.\n\nIf the interpreter path does not contain a `/` it will be executed without\nbeing translated. This is useful if `cygpath` is not available, or you wish to\npass a Windows-style path to the interpreter.\n\n### Setting Variables in a Recipe\n\nRecipe lines are interpreted by the shell, not `just`, so it's not possible to\nset `just` variables in the middle of a recipe:\n\n```justfile\nfoo:\n  x := \"hello\" # This doesn't work!\n  echo {{x}}\n```\n\nIt is possible to use shell variables, but there's another problem. Every\nrecipe line is run by a new shell instance, so variables set in one line won't\nbe set in the next:\n\n```just\nfoo:\n  x=hello && echo $x # This works!\n  y=bye\n  echo $y            # This doesn't, `y` is undefined here!\n```\n\nThe best way to work around this is to use a shebang recipe. Shebang recipe\nbodies are extracted and run as scripts, so a single shell instance will run\nthe whole thing:\n\n```just\nfoo:\n  #!/usr/bin/env bash\n  set -euxo pipefail\n  x=hello\n  echo $x\n```\n\n### Sharing Environment Variables Between Recipes\n\nEach line of each recipe is executed by a fresh shell, so it is not possible to\nshare environment variables between recipes.\n\n#### Using Python Virtual Environments\n\nSome tools, like [Python's venv](https://docs.python.org/3/library/venv.html),\nrequire loading environment variables in order to work, making them challenging\nto use with `just`. As a workaround, you can execute the virtual environment\nbinaries directly:\n\n```just\nvenv:\n  [ -d foo ] || python3 -m venv foo\n\nrun: venv\n  ./foo/bin/python3 main.py\n```\n\n### Changing the Working Directory in a Recipe\n\nEach recipe line is executed by a new shell, so if you change the working\ndirectory on one line, it won't have an effect on later lines:\n\n```just\nfoo:\n  pwd    # This `pwd` will print the same directoryâ€¦\n  cd bar\n  pwd    # â€¦as this `pwd`!\n```\n\nThere are a couple ways around this. One is to call `cd` on the same line as\nthe command you want to run:\n\n```just\nfoo:\n  cd bar && pwd\n```\n\nThe other is to use a shebang recipe. Shebang recipe bodies are extracted and\nrun as scripts, so a single shell instance will run the whole thing, and thus a\n`cd` on one line will affect later lines, just like a shell script:\n\n```just\nfoo:\n  #!/usr/bin/env bash\n  set -euxo pipefail\n  cd bar\n  pwd\n```\n\n### Indentation\n\nRecipe lines can be indented with spaces or tabs, but not a mix of both. All of\na recipe's lines must have the same type of indentation, but different recipes\nin the same `justfile` may use different indentation.\n\nEach recipe must be indented at least one level from the `recipe-name` but\nafter that may be further indented.\n\nHere's a justfile with a recipe indented with spaces, represented as `Â·`, and\ntabs, represented as `â†’`.\n\n```justfile\nset windows-shell := [\"pwsh\", \"-NoLogo\", \"-NoProfileLoadTime\", \"-Command\"]\n\nset ignore-comments\n\nlist-space directory:\nÂ·Â·#!pwsh\nÂ·Â·foreach ($item in $(Get-ChildItem {{directory}} )) {\nÂ·Â·Â·Â·echo $item.Name\nÂ·Â·}\nÂ·Â·echo \"\"\n\n# indentation nesting works even when newlines are escaped\nlist-tab directory:\nâ†’ @foreach ($item in $(Get-ChildItem {{directory}} )) { \\\nâ†’ â†’ echo $item.Name \\\nâ†’ }\nâ†’ @echo \"\"\n```\n\n```pwsh\nPS > just list-space ~\nDesktop\nDocuments\nDownloads\n\nPS > just list-tab ~\nDesktop\nDocuments\nDownloads\n```\n\n### Multi-Line Constructs\n\nRecipes without an initial shebang are evaluated and run line-by-line, which\nmeans that multi-line constructs probably won't do what you want.\n\nFor example, with the following `justfile`:\n\n```justfile\nconditional:\n  if true; then\n    echo 'True!'\n  fi\n```\n\nThe extra leading whitespace before the second line of the `conditional` recipe\nwill produce a parse error:\n\n```console\n$ just conditional\nerror: Recipe line has extra leading whitespace\n  |\n3 |         echo 'True!'\n  |     ^^^^^^^^^^^^^^^^\n```\n\nTo work around this, you can write conditionals on one line, escape newlines\nwith slashes, or add a shebang to your recipe. Some examples of multi-line\nconstructs are provided for reference.\n\n#### `if` statements\n\n```just\nconditional:\n  if true; then echo 'True!'; fi\n```\n\n```just\nconditional:\n  if true; then \\\n    echo 'True!'; \\\n  fi\n```\n\n```just\nconditional:\n  #!/usr/bin/env sh\n  if true; then\n    echo 'True!'\n  fi\n```\n\n#### `for` loops\n\n```just\nfor:\n  for file in `ls .`; do echo $file; done\n```\n\n```just\nfor:\n  for file in `ls .`; do \\\n    echo $file; \\\n  done\n```\n\n```just\nfor:\n  #!/usr/bin/env sh\n  for file in `ls .`; do\n    echo $file\n  done\n```\n\n#### `while` loops\n\n```just\nwhile:\n  while `server-is-dead`; do ping -c 1 server; done\n```\n\n```just\nwhile:\n  while `server-is-dead`; do \\\n    ping -c 1 server; \\\n  done\n```\n\n```just\nwhile:\n  #!/usr/bin/env sh\n  while `server-is-dead`; do\n    ping -c 1 server\n  done\n```\n\n#### Outside Recipe Bodies\n\nParenthesized expressions can span multiple lines:\n\n```just\nabc := ('a' +\n        'b'\n         + 'c')\n\nabc2 := (\n  'a' +\n  'b' +\n  'c'\n)\n\nfoo param=('foo'\n      + 'bar'\n    ):\n  echo {{param}}\n\nbar: (foo\n        'Foo'\n     )\n  echo 'Bar!'\n```\n\nLines ending with a backslash continue on to the next line as if the lines were\njoined by whitespace<sup>1.15.0</sup>:\n\n```just\na := 'foo' + \\\n     'bar'\n\nfoo param1 \\\n  param2='foo' \\\n  *varparam='': dep1 \\\n                (dep2 'foo')\n  echo {{param1}} {{param2}} {{varparam}}\n\ndep1: \\\n    # this comment is not part of the recipe body\n  echo 'dep1'\n\ndep2 \\\n  param:\n    echo 'Dependency with parameter {{param}}'\n```\n\nBackslash line continuations can also be used in interpolations. The line\nfollowing the backslash must be indented.\n\n```just\nrecipe:\n  echo '{{ \\\n  \"This interpolation \" + \\\n    \"has a lot of text.\" \\\n  }}'\n  echo 'back to recipe body'\n```\n\n### Command-line Options\n\n`just` supports a number of useful command-line options for listing, dumping,\nand debugging recipes and variables:\n\n```console\n$ just --list\nAvailable recipes:\n  js\n  perl\n  polyglot\n  python\n  ruby\n$ just --show perl\nperl:\n  #!/usr/bin/env perl\n  print \"Larry Wall says Hi!\\n\";\n$ just --show polyglot\npolyglot: python js perl sh ruby\n```\n\n#### Setting Command-line Options with Environment Variables\n\nSome command-line options can be set with environment variables\n\nFor example, unstable features can be enabled either with the `--unstable`\nflag:\n\n```console\n$ just --unstable\n```\n\nOr by setting the `JUST_UNSTABLE` environment variable:\n\n```console\n$ export JUST_UNSTABLE=1\n$ just\n```\n\nSince environment variables are inherited by child processes, command-line\noptions set with environment variables are inherited by recursive invocations\nof `just`, where as command line options set with arguments are not.\n\nConsult `just --help` for which options can be set with environment variables.\n\n### Private Recipes\n\nRecipes and aliases whose name starts with a `_` are omitted from `just --list`:\n\n```just\ntest: _test-helper\n  ./bin/test\n\n_test-helper:\n  ./bin/super-secret-test-helper-stuff\n```\n\n```console\n$ just --list\nAvailable recipes:\n    test\n```\n\nAnd from `just --summary`:\n\n```console\n$ just --summary\ntest\n```\n\nThe `[private]` attribute<sup>1.10.0</sup> may also be used to hide recipes or\naliases without needing to change the name:\n\n```just\n[private]\nfoo:\n\n[private]\nalias b := bar\n\nbar:\n```\n\n```console\n$ just --list\nAvailable recipes:\n    bar\n```\n\nThis is useful for helper recipes which are only meant to be used as\ndependencies of other recipes.\n\n### Quiet Recipes\n\nA recipe name may be prefixed with `@` to invert the meaning of `@` before each\nline:\n\n```just\n@quiet:\n  echo hello\n  echo goodbye\n  @# all done!\n```\n\nNow only the lines starting with `@` will be echoed:\n\n```console\n$ just quiet\nhello\ngoodbye\n# all done!\n```\n\nAll recipes in a Justfile can be made quiet with `set quiet`:\n\n```just\nset quiet\n\nfoo:\n  echo \"This is quiet\"\n\n@foo2:\n  echo \"This is also quiet\"\n```\n\nThe `[no-quiet]` attribute overrides this setting:\n\n```just\nset quiet\n\nfoo:\n  echo \"This is quiet\"\n\n[no-quiet]\nfoo2:\n  echo \"This is not quiet\"\n```\n\nShebang recipes are quiet by default:\n\n```just\nfoo:\n  #!/usr/bin/env bash\n  echo 'Foo!'\n```\n\n```console\n$ just foo\nFoo!\n```\n\nAdding `@` to a shebang recipe name makes `just` print the recipe before\nexecuting it:\n\n```just\n@bar:\n  #!/usr/bin/env bash\n  echo 'Bar!'\n```\n\n```console\n$ just bar\n#!/usr/bin/env bash\necho 'Bar!'\nBar!\n```\n\n`just` normally prints error messages when a recipe line fails. These error\nmessages can be suppressed using the `[no-exit-message]`<sup>1.7.0</sup>\nattribute. You may find this especially useful with a recipe that wraps a tool:\n\n```just\ngit *args:\n    @git {{args}}\n```\n\n```console\n$ just git status\nfatal: not a git repository (or any of the parent directories): .git\nerror: Recipe `git` failed on line 2 with exit code 128\n```\n\nAdd the attribute to suppress the exit error message when the tool exits with a\nnon-zero code:\n\n```just\n[no-exit-message]\ngit *args:\n    @git {{args}}\n```\n\n```console\n$ just git status\nfatal: not a git repository (or any of the parent directories): .git\n```\n\n### Selecting Recipes to Run With an Interactive Chooser\n\nThe `--choose` subcommand makes `just` invoke a chooser to select which recipes\nto run. Choosers should read lines containing recipe names from standard input\nand print one or more of those names separated by spaces to standard output.\n\nBecause there is currently no way to run a recipe that requires arguments with\n`--choose`, such recipes will not be given to the chooser. Private recipes and\naliases are also skipped.\n\nThe chooser can be overridden with the `--chooser` flag. If `--chooser` is not\ngiven, then `just` first checks if `$JUST_CHOOSER` is set. If it isn't, then\nthe chooser defaults to `fzf`, a popular fuzzy finder.\n\nArguments can be included in the chooser, i.e. `fzf --exact`.\n\nThe chooser is invoked in the same way as recipe lines. For example, if the\nchooser is `fzf`, it will be invoked with `sh -cu 'fzf'`, and if the shell, or\nthe shell arguments are overridden, the chooser invocation will respect those\noverrides.\n\nIf you'd like `just` to default to selecting recipes with a chooser, you can\nuse this as your default recipe:\n\n```just\ndefault:\n  @just --choose\n```\n\n### Invoking `justfile`s in Other Directories\n\nIf the first argument passed to `just` contains a `/`, then the following\noccurs:\n\n1.  The argument is split at the last `/`.\n\n2.  The part before the last `/` is treated as a directory. `just` will start\n    its search for the `justfile` there, instead of in the current directory.\n\n3.  The part after the last slash is treated as a normal argument, or ignored\n    if it is empty.\n\nThis may seem a little strange, but it's useful if you wish to run a command in\na `justfile` that is in a subdirectory.\n\nFor example, if you are in a directory which contains a subdirectory named\n`foo`, which contains a `justfile` with the recipe `build`, which is also the\ndefault recipe, the following are all equivalent:\n\n```console\n$ (cd foo && just build)\n$ just foo/build\n$ just foo/\n```\n\nAdditional recipes after the first are sought in the same `justfile`. For\nexample, the following are both equivalent:\n\n```console\n$ just foo/a b\n$ (cd foo && just a b)\n```\n\nAnd will both invoke recipes `a` and `b` in `foo/justfile`.\n\n### Imports\n\nOne `justfile` can include the contents of another using `import` statements.\n\nIf you have the following `justfile`:\n\n```justfile\nimport 'foo/bar.just'\n\na: b\n  @echo A\n```\n\nAnd the following text in `foo/bar.just`:\n\n```just\nb:\n  @echo B\n```\n\n`foo/bar.just` will be included in `justfile` and recipe `b` will be defined:\n\n```console\n$ just b\nB\n$ just a\nB\nA\n```\n\nThe `import` path can be absolute or relative to the location of the justfile\ncontaining it. A leading `~/` in the import path is replaced with the current\nusers home directory.\n\nJustfiles are insensitive to order, so included files can reference variables\nand recipes defined after the `import` statement.\n\nImported files can themselves contain `import`s, which are processed\nrecursively.\n\n`allow-duplicate-recipes` and `allow-duplicate-variables` allow duplicate\nrecipes and variables, respectively, to override each other, instead of\nproducing an error.\n\nWithin a module, later definitions override earlier definitions:\n\n```just\nset allow-duplicate-recipes\n\nfoo:\n\nfoo:\n  echo 'yes'\n```\n\nWhen `import`s are involved, things unfortunately get much more complicated and\nhard to explain.\n\nShallower definitions always override deeper definitions, so recipes at the top\nlevel will override recipes in imports, and recipes in an import will override\nrecipes in an import which itself imports those recipes.\n\nWhen two duplicate definitions are imported and are at the same depth, the one\nfrom the earlier import will override the one from the later import.\n\nThis is because `just` uses a stack when processing imports, pushing imports\nonto the stack in source-order, and always processing the top of the stack\nnext, so earlier imports are actually handled later by the compiler.\n\nThis is definitely a bug, but since `just` has very strong backwards\ncompatibility guarantees and we take enormous pains not to break anyone's\n`justfile`, we have created issue #2540 to discuss whether or not we can\nactually fix it.\n\nImports may be made optional by putting a `?` after the `import` keyword:\n\n```just\nimport? 'foo/bar.just'\n```\n\nImporting the same source file multiple times is not an error<sup>1.37.0</sup>.\nThis allows importing multiple justfiles, for example `foo.just` and\n`bar.just`, which both import a third justfile containing shared recipes, for\nexample `baz.just`, without the duplicate import of `baz.just` being an error:\n\n```justfile\n# justfile\nimport 'foo.just'\nimport 'bar.just'\n```\n\n```justfile\n# foo.just\nimport 'baz.just'\nfoo: baz\n```\n\n```justfile\n# bar.just\nimport 'baz.just'\nbar: baz\n```\n\n```just\n# baz\nbaz:\n```\n\n### Modules<sup>1.19.0</sup>\n\nA `justfile` can declare modules using `mod` statements.\n\n`mod` statements were stabilized in `just`<sup>1.31.0</sup>. In earlier\nversions, you'll need to use the `--unstable` flag, `set unstable`, or set the\n`JUST_UNSTABLE` environment variable to use them.\n\nIf you have the following `justfile`:\n\n```justfile\nmod bar\n\na:\n  @echo A\n```\n\nAnd the following text in `bar.just`:\n\n```just\nb:\n  @echo B\n```\n\n`bar.just` will be included in `justfile` as a submodule. Recipes, aliases, and\nvariables defined in one submodule cannot be used in another, and each module\nuses its own settings.\n\nRecipes in submodules can be invoked as subcommands:\n\n```console\n$ just bar b\nB\n```\n\nOr with path syntax:\n\n```console\n$ just bar::b\nB\n```\n\nIf a module is named `foo`, just will search for the module file in `foo.just`,\n`foo/mod.just`, `foo/justfile`, and `foo/.justfile`. In the latter two cases,\nthe module file may have any capitalization.\n\nModule statements may be of the form:\n\n```justfile\nmod foo 'PATH'\n```\n\nWhich loads the module's source file from `PATH`, instead of from the usual\nlocations. A leading `~/` in `PATH` is replaced with the current user's home\ndirectory. `PATH` may point to the module source file itself, or to a directory\ncontaining the module source file with the name `mod.just`, `justfile`, or\n`.justfile`. In the latter two cases, the module file may have any\ncapitalization.\n\nEnvironment files are only loaded for the root justfile, and loaded environment\nvariables are available in submodules. Settings in submodules that affect\nenvironment file loading are ignored.\n\nRecipes in submodules without the `[no-cd]` attribute run with the working\ndirectory set to the directory containing the submodule source file.\n\n`justfile()` and `justfile_directory()` always return the path to the root\njustfile and the directory that contains it, even when called from submodule\nrecipes.\n\nModules may be made optional by putting a `?` after the `mod` keyword:\n\n```just\nmod? foo\n```\n\nMissing source files for optional modules do not produce an error.\n\nOptional modules with no source file do not conflict, so you can have multiple\nmod statements with the same name, but with different source file paths, as\nlong as at most one source file exists:\n\n```just\nmod? foo 'bar.just'\nmod? foo 'baz.just'\n```\n\nModules may be given doc comments which appear in `--list`\noutput<sup>1.30.0</sup>:\n\n```justfile\n# foo is a great module!\nmod foo\n```\n\n```console\n$ just --list\nAvailable recipes:\n    foo ... # foo is a great module!\n```\n\nModules are still missing a lot of features, for example, the ability to refer\nto variables in other modules. See the [module improvement tracking\nissue](https://github.com/casey/just/issues/2252) for more information.\n\n### Hiding `justfile`s\n\n`just` looks for `justfile`s named `justfile` and `.justfile`, which can be\nused to keep a `justfile` hidden.\n\n### Just Scripts\n\nBy adding a shebang line to the top of a `justfile` and making it executable,\n`just` can be used as an interpreter for scripts:\n\n```console\n$ cat > script <<EOF\n#!/usr/bin/env just --justfile\n\nfoo:\n  echo foo\nEOF\n$ chmod +x script\n$ ./script foo\necho foo\nfoo\n```\n\nWhen a script with a shebang is executed, the system supplies the path to the\nscript as an argument to the command in the shebang. So, with a shebang of\n`#!/usr/bin/env just --justfile`, the command will be `/usr/bin/env just --justfile PATH_TO_SCRIPT`.\n\nWith the above shebang, `just` will change its working directory to the\nlocation of the script. If you'd rather leave the working directory unchanged,\nuse `#!/usr/bin/env just --working-directory . --justfile`.\n\nNote: Shebang line splitting is not consistent across operating systems. The\nprevious examples have only been tested on macOS. On Linux, you may need to\npass the `-S` flag to `env`:\n\n```just\n#!/usr/bin/env -S just --justfile\n\ndefault:\n  echo foo\n```\n\n### Formatting and dumping `justfile`s\n\nEach `justfile` has a canonical formatting with respect to whitespace and\nnewlines.\n\nYou can overwrite the current justfile with a canonically-formatted version\nusing the currently-unstable `--fmt` flag:\n\n```console\n$ cat justfile\n# A lot of blank lines\n\n\n\n\n\nsome-recipe:\n  echo \"foo\"\n$ just --fmt --unstable\n$ cat justfile\n# A lot of blank lines\n\nsome-recipe:\n    echo \"foo\"\n```\n\nInvoking `just --fmt --check --unstable` runs `--fmt` in check mode. Instead of\noverwriting the `justfile`, `just` will exit with an exit code of 0 if it is\nformatted correctly, and will exit with 1 and print a diff if it is not.\n\nYou can use the `--dump` command to output a formatted version of the\n`justfile` to stdout:\n\n```console\n$ just --dump > formatted-justfile\n```\n\nThe `--dump` command can be used with `--dump-format json` to print a JSON\nrepresentation of a `justfile`.\n\n### Fallback to parent `justfile`s\n\nIf a recipe is not found in a `justfile` and the `fallback` setting is set,\n`just` will look for `justfile`s in the parent directory and up, until it\nreaches the root directory. `just` will stop after it reaches a `justfile` in\nwhich the `fallback` setting is `false` or unset.\n\nAs an example, suppose the current directory contains this `justfile`:\n\n```just\nset fallback\nfoo:\n  echo foo\n```\n\nAnd the parent directory contains this `justfile`:\n\n```just\nbar:\n  echo bar\n```\n\n```console\n$ just bar\nTrying ../justfile\necho bar\nbar\n```\n\n### Avoiding Argument Splitting\n\nGiven this `justfile`:\n\n```just\nfoo argument:\n  touch {{argument}}\n```\n\nThe following command will create two files, `some` and `argument.txt`:\n\n```console\n$ just foo \"some argument.txt\"\n```\n\nThe user's shell will parse `\"some argument.txt\"` as a single argument, but\nwhen `just` replaces `touch {{argument}}` with `touch some argument.txt`, the\nquotes are not preserved, and `touch` will receive two arguments.\n\nThere are a few ways to avoid this: quoting, positional arguments, and exported\narguments.\n\n#### Quoting\n\nQuotes can be added around the `{{argument}}` interpolation:\n\n```just\nfoo argument:\n  touch '{{argument}}'\n```\n\nThis preserves `just`'s ability to catch variable name typos before running,\nfor example if you were to write `{{argument}}`, but will not do what you want\nif the value of `argument` contains single quotes.\n\n#### Positional Arguments\n\nThe `positional-arguments` setting causes all arguments to be passed as\npositional arguments, allowing them to be accessed with `$1`, `$2`, â€¦, and\n`$@`, which can be then double-quoted to avoid further splitting by the shell:\n\n```just\nset positional-arguments\n\nfoo argument:\n  touch \"$1\"\n```\n\nThis defeats `just`'s ability to catch typos, for example if you type `$2`\ninstead of `$1`, but works for all possible values of `argument`, including\nthose with double quotes.\n\n#### Exported Arguments\n\nAll arguments are exported when the `export` setting is set:\n\n```just\nset export\n\nfoo argument:\n  touch \"$argument\"\n```\n\nOr individual arguments may be exported by prefixing them with `$`:\n\n```just\nfoo $argument:\n  touch \"$argument\"\n```\n\nThis defeats `just`'s ability to catch typos, for example if you type\n`$argument`, but works for all possible values of `argument`, including those\nwith double quotes.\n\n### Configuring the Shell\n\nThere are a number of ways to configure the shell for linewise recipes, which\nare the default when a recipe does not start with a `#!` shebang. Their\nprecedence, from highest to lowest, is:\n\n1. The `--shell` and `--shell-arg` command line options. Passing either of\n   these will cause `just` to ignore any settings in the current justfile.\n2. `set windows-shell := [...]`\n3. `set windows-powershell` (deprecated)\n4. `set shell := [...]`\n\nSince `set windows-shell` has higher precedence than `set shell`, you can use\n`set windows-shell` to pick a shell on Windows, and `set shell` to pick a shell\nfor all other platforms.\n\n### Timestamps\n\n`just` can print timestamps before each recipe commands:\n\n```just\nrecipe:\n  echo one\n  sleep 2\n  echo two\n```\n\n```\n$ just --timestamp recipe\n[07:28:46] echo one\none\n[07:28:46] sleep 2\n[07:28:48] echo two\ntwo\n```\n\nBy default, timestamps are formatted as `HH:MM:SS`. The format can be changed\nwith `--timestamp-format`:\n\n```\n$ just --timestamp recipe --timestamp-format '%H:%M:%S%.3f %Z'\n[07:32:11:.349 UTC] echo one\none\n[07:32:11:.350 UTC] sleep 2\n[07:32:13:.352 UTC] echo two\ntwo\n```\n\nThe argument to `--timestamp-format` is a `strftime`-style format string, see\nthe\n[`chrono` library docs](https://docs.rs/chrono/latest/chrono/format/strftime/index.html)\nfor details.\n\n### Signal Handling\n\n[Signals](https://en.wikipedia.org/wiki/Signal_(IPC)) are messsages sent to\nrunning programs to trigger specific behavior. For example, `SIGINT` is sent to\nall processes in the terminal forground process group when `CTRL-C` is pressed.\n\n`just` tries to exit when requested by a signal, but it also tries to avoid\nleaving behind running child proccesses, two goals which are somewhat in\nconflict.\n\nIf `just` exits leaving behind child processes, the user will have no recourse\nbut to `ps aux | grep` for the children and manually `kill` them, a tedious\nendevour.\n\n#### Fatal Signals\n\n`SIGHUP`, `SIGINT`, and `SIGQUIT` are generated when the user closes the\nterminal, types `ctrl-c`, or types `ctrl-\\`, respectively, and are sent to all\nprocesses in the foreground process group.\n\n`SIGTERM` is the default signal sent by the `kill` command, and is delivered\nonly to its intended victim.\n\nWhen a child process is not running, `just` will exit immediately on receipt of\nany of the above signals.\n\nWhen a child process *is* running, `just` will wait until it terminates, to\navoid leaving it behind.\n\nAdditionally, on receipt of `SIGTERM`, `just` will forward `SIGTERM` to any\nrunning children<sup>1.41.0</sup>, since unlike other fatal signals, `SIGTERM`,\nwas likely sent to `just` alone.\n\nRegardless of whether a child process terminates successfully after `just`\nreceives a fatal signal, `just` halts execution.\n\n#### `SIGINFO`\n\n`SIGINFO` is sent to all processes in the foreground process group when the\nuser types `ctrl-t` on\n[BSD](https://en.wikipedia.org/wiki/Berkeley_Software_Distribution)-derived\noperating systems, including MacOS, but not Linux.\n\n`just` responds by printing a list of all child process IDs and\ncommands<sup>1.41.0</sup>.\n\n#### Windows\n\nOn Windows, `just` behaves as if it had received `SIGINT` when the user types\n`ctrl-c`. Other signals are unsupported.\n\nChangelog\n---------\n\nA changelog for the latest release is available in\n[CHANGELOG.md](https://raw.githubusercontent.com/casey/just/master/CHANGELOG.md).\nChangelogs for previous releases are available on\n[the releases page](https://github.com/casey/just/releases). `just --changelog`\ncan also be used to make a `just` binary print its changelog.\n\nMiscellanea\n-----------\n\n### Re-running recipes when files change\n\n[`watchexec`](https://github.com/mattgreen/watchexec) can re-run any command\nwhen files change.\n\nTo re-run the recipe `foo` when any file changes:\n\n```console\nwatchexec just foo\n```\n\nSee `watchexec --help` for more info, including how to specify which files\nshould be watched for changes.\n\n### Parallelism\n\nDependencies may be run in parallel with the `[parallel]` attribute.\n\nIn this `justfile`, `foo`, `bar`, and `baz` will execute in parallel when\n`main` is run:\n\n```just\n[parallel]\nmain: foo bar baz\n\nfoo:\n  sleep 1\n\nbar:\n  sleep 1\n\nbaz:\n  sleep 1\n```\n\nGNU `parallel` may be used to run recipe lines concurrently:\n\n```just\nparallel:\n  #!/usr/bin/env -S parallel --shebang --ungroup --jobs {{ num_cpus() }}\n  echo task 1 start; sleep 3; echo task 1 done\n  echo task 2 start; sleep 3; echo task 2 done\n  echo task 3 start; sleep 3; echo task 3 done\n  echo task 4 start; sleep 3; echo task 4 done\n```\n\n### Shell Alias\n\nFor lightning-fast command running, put `alias j=just` in your shell's\nconfiguration file.\n\nIn `bash`, the aliased command may not keep the shell completion functionality\ndescribed in the next section. Add the following line to your `.bashrc` to use\nthe same completion function as `just` for your aliased command:\n\n```console\ncomplete -F _just -o bashdefault -o default j\n```\n\n### Shell Completion Scripts\n\nShell completion scripts for Bash, Elvish, Fish, Nushell, PowerShell, and Zsh\nare available [release archives](https://github.com/casey/just/releases).\n\nThe `just` binary can also generate the same completion scripts at runtime\nusing `just --completions SHELL`:\n\n```console\n$ just --completions zsh > just.zsh\n```\n\nPlease refer to your shell's documentation for how to install them.\n\n*macOS Note:* Recent versions of macOS use zsh as the default shell. If you use\nHomebrew to install `just`, it will automatically install the most recent copy\nof the zsh completion script in the Homebrew zsh directory, which the built-in\nversion of zsh doesn't know about by default. It's best to use this copy of the\nscript if possible, since it will be updated whenever you update `just` via\nHomebrew. Also, many other Homebrew packages use the same location for\ncompletion scripts, and the built-in zsh doesn't know about those either. To\ntake advantage of `just` completion in zsh in this scenario, you can set\n`fpath` to the Homebrew location before calling `compinit`. Note also that Oh\nMy Zsh runs `compinit` by default. So your `.zshrc` file could look like this:\n\n```zsh\n# Init Homebrew, which adds environment variables\neval \"$(brew shellenv)\"\n\nfpath=($HOMEBREW_PREFIX/share/zsh/site-functions $fpath)\n\n# Then choose one of these options:\n# 1. If you're using Oh My Zsh, you can initialize it here\n# source $ZSH/oh-my-zsh.sh\n\n# 2. Otherwise, run compinit yourself\n# autoload -U compinit\n# compinit\n```\n\n### Man Page\n\n`just` can print its own man page with `just --man`. Man pages are written in\n[`roff`](https://en.wikipedia.org/wiki/Roff_%28software%29), a venerable markup\nlanguage and one of the first practical applications of Unix. If you have\n[`groff`](https://www.gnu.org/software/groff/) installed you can view the man\npage with  `just --man | groff -mandoc -Tascii | less`.\n\n### Grammar\n\nA non-normative grammar of `justfile`s can be found in\n[GRAMMAR.md](https://github.com/casey/just/blob/master/GRAMMAR.md).\n\n### just.sh\n\nBefore `just` was a fancy Rust program it was a tiny shell script that called\n`make`. You can find the old version in\n[contrib/just.sh](https://github.com/casey/just/blob/master/contrib/just.sh).\n\n### Global and User `justfile`s\n\nIf you want some recipes to be available everywhere, you have a few options.\n\n#### Global Justfile\n\n`just --global-justfile`, or `just -g` for short, searches the following paths,\nin-order, for a justfile:\n\n- `$XDG_CONFIG_HOME/just/justfile`\n- `$HOME/.config/just/justfile`\n- `$HOME/justfile`\n- `$HOME/.justfile`\n\nYou can put recipes that are used across many projects in a global justfile to\neasily invoke them from any directory.\n\n#### User justfile tips\n\nYou can also adopt some of the following workflows. These tips assume you've\ncreated a `justfile` at `~/.user.justfile`, but you can put this `justfile`\nat any convenient path on your system.\n\n##### Recipe Aliases\n\nIf you want to call the recipes in `~/.user.justfile` by name, and don't mind\ncreating an alias for every recipe, add the following to your shell's\ninitialization script:\n\n```console\nfor recipe in `just --justfile ~/.user.justfile --summary`; do\n  alias $recipe=\"just --justfile ~/.user.justfile --working-directory . $recipe\"\ndone\n```\n\nNow, if you have a recipe called `foo` in `~/.user.justfile`, you can just type\n`foo` at the command line to run it.\n\nIt took me way too long to realize that you could create recipe aliases like\nthis. Notwithstanding my tardiness, I am very pleased to bring you this major\nadvance in `justfile` technology.\n\n##### Forwarding Alias\n\nIf you'd rather not create aliases for every recipe, you can create a single alias:\n\n```console\nalias .j='just --justfile ~/.user.justfile --working-directory .'\n```\n\nNow, if you have a recipe called `foo` in `~/.user.justfile`, you can just type\n`.j foo` at the command line to run it.\n\nI'm pretty sure that nobody actually uses this feature, but it's there.\n\nÂ¯\\\\\\_(ãƒ„)\\_/Â¯\n\n##### Customization\n\nYou can customize the above aliases with additional options. For example, if\nyou'd prefer to have the recipes in your `justfile` run in your home directory,\ninstead of the current directory:\n\n```console\nalias .j='just --justfile ~/.user.justfile --working-directory ~'\n```\n\n### Node.js `package.json` Script Compatibility\n\nThe following export statement gives `just` recipes access to local Node module\nbinaries, and makes `just` recipe commands behave more like `script` entries in\nNode.js `package.json` files:\n\n```just\nexport PATH := \"./node_modules/.bin:\" + env_var('PATH')\n```\n\n### Paths on Windows\n\nOn Windows, all functions that return paths, except `invocation_directory()`\nwill return `\\`-separated paths. When not using PowerShell or `cmd.exe` these\npaths should be quoted to prevent the `\\`s from being interpreted as character\nescapes:\n\n```just\nls:\n    echo '{{absolute_path(\".\")}}'\n```\n\n`cygpath.exe` is an executable included in some distributions of Unix userlands\nfor Windows, including [Cygwin](https://www.cygwin.com/) and\n[Git](https://git-scm.com/downloads) for Windows.\n\n`just` uses `cygpath.exe` in two places:\n\nFor backwards compatibility, `invocation_directory()`, uses `cygpath.exe` to\nconvert the invocation directory into a unix-style `/`-separated path. Use\n`invocation_directory_native()` to get the native, Windows-style path. On unix,\n`invocation_directory()` and `invocation_directory_native()` both return the\nsame unix-style path.\n\n`cygpath.exe` is used also used to convert Unix-style shebang lines into\nWindows paths. As an alternative, the `[script]` attribute, currently unstable,\ncan be used, which does not depend on `cygpath.exe`.\n\nIf `cygpath.exe` is available, you can use it to convert between path styles:\n\n```just\nfoo_unix := '/hello/world'\nfoo_windows := shell('cygpath --windows $1', foo_unix)\n\nbar_windows := 'C:\\hello\\world'\nbar_unix := shell('cygpath --unix $1', bar_windows)\n```\n\n### Remote Justfiles\n\nIf you wish to include a `mod` or `import` source file in many `justfiles`\nwithout needing to duplicate it, you can use an optional `mod` or `import`,\nalong with a recipe to fetch the module source:\n\n```just\nimport? 'foo.just'\n\nfetch:\n  curl https://raw.githubusercontent.com/casey/just/master/justfile > foo.just\n```\n\nGiven the above `justfile`, after running `just fetch`, the recipes in\n`foo.just` will be available.\n\n### Printing Complex Strings\n\n`echo` can be used to print strings, but because it processes escape sequences,\nlike `\\n`, and different implementations of `echo` recognize different escape\nsequences, using `printf` is often a better choice.\n\n`printf` takes a C-style format string and any number of arguments, which are\ninterpolated into the format string.\n\nThis can be combined with indented, triple quoted strings to emulate shell\nheredocs.\n\nSubstitution complex strings into recipe bodies with `{â€¦}` can also lead to\ntrouble as it may be split by the shell into multiple arguments depending on\nthe presence of whitespace and quotes. Exporting complex strings as environment\nvariables and referring to them with `\"$NAME\"`, note the double quotes, can\nalso help.\n\nPutting all this together, to print a string verbatim to standard output, with\nall its various escape sequences and quotes undisturbed:\n\n```just\nexport FOO := '''\n  a complicated string with\n  some dis\\tur\\bi\\ng escape sequences\n  and \"quotes\" of 'different' kinds\n'''\n\nbar:\n  printf %s \"$FOO\"\n```\n\n### Alternatives and Prior Art\n\nThere is no shortage of command runners! Some more or less similar alternatives\nto `just` include:\n\n- [make](https://en.wikipedia.org/wiki/Make_(software)): The Unix build tool\n  that inspired `just`. There are a few different modern day descendents of the\n  original `make`, including\n  [FreeBSD Make](https://www.freebsd.org/cgi/man.cgi?make(1)) and\n  [GNU Make](https://www.gnu.org/software/make/).\n- [task](https://github.com/go-task/task): A YAML-based command runner written\n  in Go.\n- [maid](https://github.com/egoist/maid): A Markdown-based command runner\n  written in JavaScript.\n- [microsoft/just](https://github.com/microsoft/just): A JavaScript-based\n  command runner written in JavaScript.\n- [cargo-make](https://github.com/sagiegurari/cargo-make): A command runner for\n  Rust projects.\n- [mmake](https://github.com/tj/mmake): A wrapper around `make` with a number\n  of improvements, including remote includes.\n- [robo](https://github.com/tj/robo): A YAML-based command runner written in\n  Go.\n- [mask](https://github.com/jakedeichert/mask): A Markdown-based command runner\n  written in Rust.\n- [makesure](https://github.com/xonixx/makesure): A simple and portable command\n  runner written in AWK and shell.\n- [haku](https://github.com/VladimirMarkelov/haku): A make-like command runner\n  written in Rust.\n- [mise](https://mise.jdx.dev/): A development environment tool manager written\n  in Rust supporing tasks in TOML files and standalone scripts.\n\nContributing\n------------\n\n`just` welcomes your contributions! `just` is released under the maximally\npermissive\n[CC0](https://creativecommons.org/publicdomain/zero/1.0/legalcode.txt) public\ndomain dedication and fallback license, so your changes must also be released\nunder this license.\n\n### Getting Started\n\n`just` is written in Rust. Use\n[rustup](https://www.rust-lang.org/tools/install) to install a Rust toolchain.\n\n`just` is extensively tested. All new features must be covered by unit or\nintegration tests. Unit tests are under\n[src](https://github.com/casey/just/blob/master/src), live alongside the code\nbeing tested, and test code in isolation. Integration tests are in the [tests\ndirectory](https://github.com/casey/just/blob/master/tests) and test the `just`\nbinary from the outside by invoking `just` on a given `justfile` and set of\ncommand-line arguments, and checking the output.\n\nYou should write whichever type of tests are easiest to write for your feature\nwhile still providing good test coverage.\n\nUnit tests are useful for testing new Rust functions that are used internally\nand as an aid for development. A good example are the unit tests which cover\nthe\n[`unindent()` function](https://github.com/casey/just/blob/master/src/unindent.rs),\nused to unindent triple-quoted strings and backticks. `unindent()` has a bunch\nof tricky edge cases which are easy to exercise with unit tests that call\n`unindent()` directly.\n\nIntegration tests are useful for making sure that the final behavior of the\n`just` binary is correct. `unindent()` is also covered by integration tests\nwhich make sure that evaluating a triple-quoted string produces the correct\nunindented value. However, there are not integration tests for all possible\ncases. These are covered by faster, more concise unit tests that call\n`unindent()` directly.\n\nIntegration tests use the `Test` struct, a builder which allows for easily\ninvoking `just` with a given `justfile`, arguments, and environment variables,\nand checking the program's stdout, stderr, and exit code .\n\n### Contribution Workflow\n\n1. Make sure the feature is wanted. There should be an open issue about the\n   feature with a comment from [@casey](https://github.com/casey) saying that\n   it's a good idea or seems reasonable. If there isn't, open a new issue and\n   ask for feedback.\n\n   There are lots of good features which can't be merged, either because they\n   aren't backwards compatible, have an implementation which would\n   overcomplicate the codebase, or go against `just`'s design philosophy.\n\n2. Settle on the design of the feature. If the feature has multiple possible\n   implementations or syntaxes, make sure to nail down the details in the\n   issue.\n\n3. Clone `just` and start hacking. The best workflow is to have the code you're\n   working on in an editor alongside a job that re-runs tests whenever a file\n   changes. You can run such a job by installing\n   [cargo-watch](https://github.com/watchexec/cargo-watch) with `cargo install\n   cargo-watch` and running `just watch test`.\n\n4. Add a failing test for your feature. Most of the time this will be an\n   integration test which exercises the feature end-to-end. Look for an\n   appropriate file to put the test in in\n   [tests](https://github.com/casey/just/blob/master/tests), or add a new file\n   in [tests](https://github.com/casey/just/blob/master/tests) and add a `mod`\n   statement importing that file in\n   [tests/lib.rs](https://github.com/casey/just/blob/master/tests/lib.rs).\n\n5. Implement the feature.\n\n6. Run `just ci` to make sure that all tests, lints, and checks pass. Requires\n   [mdBook](https://github.com/rust-lang/mdBook) and\n   [mdbook-linkcheck](https://github.com/Michael-F-Bryan/mdbook-linkcheck).\n\n7. Open a PR with the new code that is editable by maintainers. PRs often\n   require rebasing and minor tweaks. If the PR is not editable by maintainers,\n   each rebase and tweak will require a round trip of code review. Your PR may\n   be summarily closed if it is not editable by maintainers.\n\n8. Incorporate feedback.\n\n9. Enjoy the sweet feeling of your PR getting merged!\n\nFeel free to open a draft PR at any time for discussion and feedback.\n\n### Hints\n\nHere are some hints to get you started with specific kinds of new features,\nwhich you can use in addition to the contribution workflow above.\n\n#### Adding a New Attribute\n\n1. Write a new integration test in\n   [tests/attributes.rs](https://github.com/casey/just/blob/master/tests/attributes.rs).\n\n2. Add a new variant to the\n   [`Attribute`](https://github.com/casey/just/blob/master/src/attribute.rs)\n   enum.\n\n3. Implement the functionality of the new attribute.\n\n4. Run `just ci` to make sure that all tests pass.\n\n### Janus\n\n[Janus](https://github.com/casey/janus) is a tool for checking whether a change\nto `just` breaks or changes the interpretation of existing `justfile`s. It\ncollects and analyzes public `justfile`s on GitHub.\n\nBefore merging a particularly large or gruesome change, Janus should be run to\nmake sure that nothing breaks. Don't worry about running Janus yourself, Casey\nwill happily run it for you on changes that need it.\n\n### Minimum Supported Rust Version\n\nThe minimum supported Rust version, or MSRV, is current stable Rust. It may\nbuild on older versions of Rust, but this is not guaranteed.\n\n### New Releases\n\nNew releases of `just` are made frequently so that users quickly get access to\nnew features.\n\nRelease commit messages use the following template:\n\n```\nRelease x.y.z\n\n- Bump version: x.y.z â†’ x.y.z\n- Update changelog\n- Update changelog contributor credits\n- Update dependencies\n- Update version references in readme\n```\n\nFrequently Asked Questions\n--------------------------\n\n### What are the idiosyncrasies of Make that Just avoids?\n\n`make` has some behaviors which are confusing, complicated, or make it\nunsuitable for use as a general command runner.\n\nOne example is that under some circumstances, `make` won't actually run the\ncommands in a recipe. For example, if you have a file called `test` and the\nfollowing makefile:\n\n```just\ntest:\n  ./test\n```\n\n`make` will refuse to run your tests:\n\n```console\n$ make test\nmake: `test' is up to date.\n```\n\n`make` assumes that the `test` recipe produces a file called `test`. Since this\nfile exists and the recipe has no other dependencies, `make` thinks that it\ndoesn't have anything to do and exits.\n\nTo be fair, this behavior is desirable when using `make` as a build system, but\nnot when using it as a command runner. You can disable this behavior for\nspecific targets using `make`'s built-in\n[`.PHONY` target name](https://www.gnu.org/software/make/manual/html_node/Phony-Targets.html),\nbut the syntax is verbose and can be hard to remember. The explicit list of\nphony targets, written separately from the recipe definitions, also introduces\nthe risk of accidentally defining a new non-phony target. In `just`, all\nrecipes are treated as if they were phony.\n\nOther examples of `make`'s idiosyncrasies include the difference between `=`\nand `:=` in assignments, the confusing error messages that are produced if you\nmess up your makefile, needing `$$` to use environment variables in recipes,\nand incompatibilities between different flavors of `make`.\n\n### What's the relationship between Just and Cargo build scripts?\n\n[`cargo` build scripts](http://doc.crates.io/build-script.html) have a pretty\nspecific use, which is to control how `cargo` builds your Rust project. This\nmight include adding flags to `rustc` invocations, building an external\ndependency, or running some kind of codegen step.\n\n`just`, on the other hand, is for all the other miscellaneous commands you\nmight run as part of development. Things like running tests in different\nconfigurations, linting your code, pushing build artifacts to a server,\nremoving temporary files, and the like.\n\nAlso, although `just` is written in Rust, it can be used regardless of the\nlanguage or build system your project uses.\n\nFurther Ramblings\n-----------------\n\nI personally find it very useful to write a `justfile` for almost every\nproject, big or small.\n\nOn a big project with multiple contributors, it's very useful to have a file\nwith all the commands needed to work on the project close at hand.\n\nThere are probably different commands to test, build, lint, deploy, and the\nlike, and having them all in one place is useful and cuts down on the time you\nhave to spend telling people which commands to run and how to type them.\n\nAnd, with an easy place to put commands, it's likely that you'll come up with\nother useful things which are part of the project's collective wisdom, but\nwhich aren't written down anywhere, like the arcane commands needed for some\npart of your revision control workflow, to install all your project's\ndependencies, or all the random flags you might need to pass to the build\nsystem.\n\nSome ideas for recipes:\n\n- Deploying/publishing the project\n\n- Building in release mode vs debug mode\n\n- Running in debug mode or with logging enabled\n\n- Complex git workflows\n\n- Updating dependencies\n\n- Running different sets of tests, for example fast tests vs slow tests, or\n  running them with verbose output\n\n- Any complex set of commands that you really should write down somewhere, if\n  only to be able to remember them\n\nEven for small, personal projects it's nice to be able to remember commands by\nname instead of ^Reverse searching your shell history, and it's a huge boon to\nbe able to go into an old project written in a random language with a\nmysterious build system and know that all the commands you need to do whatever\nyou need to do are in the `justfile`, and that if you type `just` something\nuseful (or at least interesting!) will probably happen.\n\nFor ideas for recipes, check out\n[this project's `justfile`](https://github.com/casey/just/blob/master/justfile),\nor some of the\n`justfile`s\n[out in the wild](https://github.com/search?q=path%3A**%2Fjustfile&type=code).\n\nAnyways, I think that's about it for this incredibly long-winded README.\n\nI hope you enjoy using `just` and find great success and satisfaction in all\nyour computational endeavors!\n\nğŸ˜¸\n\n[ğŸ”¼ Back to the top!](#just)\n",
      "stars_today": 63
    },
    {
      "id": 1019432584,
      "name": "claude-relay-service",
      "full_name": "Wei-Shaw/claude-relay-service",
      "description": "CRS-è‡ªå»ºClaude Codeé•œåƒï¼Œä¸€ç«™å¼å¼€æºä¸­è½¬æœåŠ¡ï¼Œè®© Claudeã€OpenAIã€Geminiã€Droid è®¢é˜…ç»Ÿä¸€æ¥å…¥ï¼Œæ”¯æŒæ‹¼è½¦å…±äº«ï¼Œæ›´é«˜æ•ˆåˆ†æ‘Šæˆæœ¬ï¼ŒåŸç”Ÿå·¥å…·æ— ç¼ä½¿ç”¨ã€‚",
      "html_url": "https://github.com/Wei-Shaw/claude-relay-service",
      "stars": 7586,
      "forks": 1254,
      "language": "JavaScript",
      "topics": [
        "claude",
        "claude-api",
        "claude-code",
        "claude-proxy",
        "codex-cli",
        "crs",
        "droid",
        "droid-cli",
        "droid2api",
        "gemini-cli"
      ],
      "created_at": "2025-07-14T10:11:32Z",
      "updated_at": "2026-01-28T02:14:25Z",
      "pushed_at": "2026-01-27T12:33:17Z",
      "open_issues": 185,
      "owner": {
        "login": "Wei-Shaw",
        "avatar_url": "https://avatars.githubusercontent.com/u/26101719?v=4"
      },
      "readme": "# Claude Relay Service\n\n> [!CAUTION]\n> **å®‰å…¨æ›´æ–°é€šçŸ¥**ï¼šv1.1.248 åŠä»¥ä¸‹ç‰ˆæœ¬å­˜åœ¨ä¸¥é‡çš„ç®¡ç†å‘˜è®¤è¯ç»•è¿‡æ¼æ´ï¼Œæ”»å‡»è€…å¯æœªæˆæƒè®¿é—®ç®¡ç†é¢æ¿ã€‚\n>\n> **è¯·ç«‹å³æ›´æ–°åˆ° v1.1.249+ ç‰ˆæœ¬**ï¼Œæˆ–è¿ç§»åˆ°æ–°ä¸€ä»£é¡¹ç›® **[CRS 2.0 (sub2api)](https://github.com/Wei-Shaw/sub2api)**\n\n<div align=\"center\">\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Node.js](https://img.shields.io/badge/Node.js-18+-green.svg)](https://nodejs.org/)\n[![Redis](https://img.shields.io/badge/Redis-6+-red.svg)](https://redis.io/)\n[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)\n[![Docker Build](https://github.com/Wei-Shaw/claude-relay-service/actions/workflows/auto-release-pipeline.yml/badge.svg)](https://github.com/Wei-Shaw/claude-relay-service/actions/workflows/auto-release-pipeline.yml)\n[![Docker Pulls](https://img.shields.io/docker/pulls/weishaw/claude-relay-service)](https://hub.docker.com/r/weishaw/claude-relay-service)\n\n**ğŸ” è‡ªè¡Œæ­å»ºClaude APIä¸­è½¬æœåŠ¡ï¼Œæ”¯æŒå¤šè´¦æˆ·ç®¡ç†**\n\n[English](README_EN.md) â€¢ [å¿«é€Ÿå¼€å§‹](https://pincc.ai/) â€¢ [æ¼”ç¤ºç«™ç‚¹](https://demo.pincc.ai/admin-next/login) â€¢ [å…¬å‘Šé¢‘é“](https://t.me/claude_relay_service)\n\n</div>\n\n---\n\n## ğŸ’ Claude/Codex æ‹¼è½¦æœåŠ¡æ¨è\n\n<div align=\"center\">\n\n| å¹³å° | æœåŠ¡ | ä»‹ç» |\n|:---|:---|:---|\n| **[pincc.ai](https://pincc.ai/)** | <small>âœ… Claude Code<br>âœ… Codex CLI</small> | æä¾›ç¨³å®šçš„ Codex CLI æ‹¼è½¦æœåŠ¡<br><br> **å…¨æ–°ä¸Šçº¿ 2API æ¸ é“**ï¼šæ¥å…¥CCçš„æ•ˆæœåª²ç¾å®˜æ–¹ Anthropic Console è´¦å·ï¼Œæš‚ä¸æ”¯æŒ Websearch å’Œ PDF è¯†åˆ«åŠŸèƒ½ï¼ˆWebsearch åæœŸä¼šæ”¯æŒï¼‰<br>ğŸ’° å•ä»·ï¼š0.8å…ƒ=1ç¾é‡‘é¢åº¦ |\n\n\n</div>\n\n---\n\n## âš ï¸ é‡è¦æé†’\n\n**ä½¿ç”¨æœ¬é¡¹ç›®å‰è¯·ä»”ç»†é˜…è¯»ï¼š**\n\nğŸš¨ **æœåŠ¡æ¡æ¬¾é£é™©**: ä½¿ç”¨æœ¬é¡¹ç›®å¯èƒ½è¿åAnthropicçš„æœåŠ¡æ¡æ¬¾ã€‚è¯·åœ¨ä½¿ç”¨å‰ä»”ç»†é˜…è¯»Anthropicçš„ç”¨æˆ·åè®®ï¼Œä½¿ç”¨æœ¬é¡¹ç›®çš„ä¸€åˆ‡é£é™©ç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…ã€‚\n\nğŸ“– **å…è´£å£°æ˜**: æœ¬é¡¹ç›®ä»…ä¾›æŠ€æœ¯å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ï¼Œä½œè€…ä¸å¯¹å› ä½¿ç”¨æœ¬é¡¹ç›®å¯¼è‡´çš„è´¦æˆ·å°ç¦ã€æœåŠ¡ä¸­æ–­æˆ–å…¶ä»–æŸå¤±æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚\n\n\n## ğŸ¤” è¿™ä¸ªé¡¹ç›®é€‚åˆä½ å—ï¼Ÿ\n\n- ğŸŒ **åœ°åŒºé™åˆ¶**: æ‰€åœ¨åœ°åŒºæ— æ³•ç›´æ¥è®¿é—®Claude CodeæœåŠ¡ï¼Ÿ\n- ğŸ”’ **éšç§æ‹…å¿§**: æ‹…å¿ƒç¬¬ä¸‰æ–¹é•œåƒæœåŠ¡ä¼šè®°å½•æˆ–æ³„éœ²ä½ çš„å¯¹è¯å†…å®¹ï¼Ÿ\n- ğŸ‘¥ **æˆæœ¬åˆ†æ‘Š**: æƒ³å’Œæœ‹å‹ä¸€èµ·åˆ†æ‘ŠClaude Code Maxè®¢é˜…è´¹ç”¨ï¼Ÿ\n- âš¡ **ç¨³å®šæ€§**: ç¬¬ä¸‰æ–¹é•œåƒç«™ç»å¸¸æ•…éšœä¸ç¨³å®šï¼Œå½±å“æ•ˆç‡ ï¼Ÿ\n\nå¦‚æœæœ‰ä»¥ä¸Šå›°æƒ‘ï¼Œé‚£è¿™ä¸ªé¡¹ç›®å¯èƒ½é€‚åˆä½ ã€‚\n\n### é€‚åˆçš„åœºæ™¯\n\nâœ… **æ‰¾æœ‹å‹æ‹¼è½¦**: ä¸‰äº”å¥½å‹ä¸€èµ·åˆ†æ‘ŠClaude Code Maxè®¢é˜…  \nâœ… **éšç§æ•æ„Ÿ**: ä¸æƒ³è®©ç¬¬ä¸‰æ–¹é•œåƒçœ‹åˆ°ä½ çš„å¯¹è¯å†…å®¹  \nâœ… **æŠ€æœ¯æŠ˜è…¾**: æœ‰åŸºæœ¬çš„æŠ€æœ¯åŸºç¡€ï¼Œæ„¿æ„è‡ªå·±æ­å»ºå’Œç»´æŠ¤  \nâœ… **ç¨³å®šéœ€æ±‚**: éœ€è¦é•¿æœŸç¨³å®šçš„Claudeè®¿é—®ï¼Œä¸æƒ³å—åˆ¶äºé•œåƒç«™  \nâœ… **åœ°åŒºå—é™**: æ— æ³•ç›´æ¥è®¿é—®Claudeå®˜æ–¹æœåŠ¡\n\n---\n\n## ğŸ’­ ä¸ºä»€ä¹ˆè¦è‡ªå·±æ­ï¼Ÿ\n\n### ç°æœ‰é•œåƒç«™å¯èƒ½çš„é—®é¢˜\n\n- ğŸ•µï¸ **éšç§é£é™©**: ä½ çš„å¯¹è¯å†…å®¹éƒ½è¢«äººå®¶çœ‹å¾—ä¸€æ¸…äºŒæ¥šï¼Œå•†ä¸šæœºå¯†ä»€ä¹ˆçš„å°±åˆ«æƒ³äº†\n- ğŸŒ **æ€§èƒ½ä¸ç¨³**: ç”¨çš„äººå¤šäº†å°±æ…¢ï¼Œé«˜å³°æœŸç»å¸¸å¡æ­»\n- ğŸ’° **ä»·æ ¼ä¸é€æ˜**: ä¸çŸ¥é“å®é™…æˆæœ¬\n\n### è‡ªå»ºçš„å¥½å¤„\n\n- ğŸ” **æ•°æ®å®‰å…¨**: æ‰€æœ‰æ¥å£è¯·æ±‚éƒ½åªç»è¿‡ä½ è‡ªå·±çš„æœåŠ¡å™¨ï¼Œç›´è¿Anthropic API\n- âš¡ **æ€§èƒ½å¯æ§**: å°±ä½ ä»¬å‡ ä¸ªäººç”¨ï¼ŒMax 200åˆ€å¥—é¤åŸºæœ¬ä¸Šå¯ä»¥çˆ½ç”¨Opus\n- ğŸ’° **æˆæœ¬é€æ˜**: ç”¨äº†å¤šå°‘tokenä¸€ç›®äº†ç„¶ï¼ŒæŒ‰å®˜æ–¹ä»·æ ¼æ¢ç®—äº†å…·ä½“è´¹ç”¨\n- ğŸ“Š **ç›‘æ§å®Œæ•´**: ä½¿ç”¨æƒ…å†µã€æˆæœ¬åˆ†æã€æ€§èƒ½ç›‘æ§å…¨éƒ½æœ‰\n\n---\n\n## ğŸš€ æ ¸å¿ƒåŠŸèƒ½\n\n### åŸºç¡€åŠŸèƒ½\n\n- âœ… **å¤šè´¦æˆ·ç®¡ç†**: å¯ä»¥æ·»åŠ å¤šä¸ªClaudeè´¦æˆ·è‡ªåŠ¨è½®æ¢\n- âœ… **è‡ªå®šä¹‰API Key**: ç»™æ¯ä¸ªäººåˆ†é…ç‹¬ç«‹çš„Key\n- âœ… **ä½¿ç”¨ç»Ÿè®¡**: è¯¦ç»†è®°å½•æ¯ä¸ªäººç”¨äº†å¤šå°‘token\n\n### é«˜çº§åŠŸèƒ½\n\n- ğŸ”„ **æ™ºèƒ½åˆ‡æ¢**: è´¦æˆ·å‡ºé—®é¢˜è‡ªåŠ¨æ¢ä¸‹ä¸€ä¸ª\n- ğŸš€ **æ€§èƒ½ä¼˜åŒ–**: è¿æ¥æ± ã€ç¼“å­˜ï¼Œå‡å°‘å»¶è¿Ÿ\n- ğŸ“Š **ç›‘æ§é¢æ¿**: Webç•Œé¢æŸ¥çœ‹æ‰€æœ‰æ•°æ®\n- ğŸ›¡ï¸ **å®‰å…¨æ§åˆ¶**: è®¿é—®é™åˆ¶ã€é€Ÿç‡æ§åˆ¶ã€å®¢æˆ·ç«¯é™åˆ¶\n- ğŸŒ **ä»£ç†æ”¯æŒ**: æ”¯æŒHTTP/SOCKS5ä»£ç†\n\n---\n\n## ğŸ“‹ éƒ¨ç½²è¦æ±‚\n\n### ç¡¬ä»¶è¦æ±‚ï¼ˆæœ€ä½é…ç½®ï¼‰\n\n- **CPU**: 1æ ¸å¿ƒå°±å¤Ÿäº†\n- **å†…å­˜**: 512MBï¼ˆå»ºè®®1GBï¼‰\n- **ç¡¬ç›˜**: 30GBå¯ç”¨ç©ºé—´\n- **ç½‘ç»œ**: èƒ½è®¿é—®åˆ°Anthropic APIï¼ˆå»ºè®®ä½¿ç”¨USåœ°åŒºçš„æœºå™¨ï¼‰\n- **å»ºè®®**: 2æ ¸4Gçš„åŸºæœ¬å¤Ÿäº†ï¼Œç½‘ç»œå°½é‡é€‰å›å›½çº¿è·¯å¿«ä¸€ç‚¹çš„ï¼ˆä¸ºäº†æé«˜é€Ÿåº¦ï¼Œå»ºè®®ä¸è¦å¼€ä»£ç†æˆ–è€…è®¾ç½®æœåŠ¡å™¨çš„IPç›´è¿ï¼‰\n- **ç»éªŒ**: é˜¿é‡Œäº‘ã€è…¾è®¯äº‘çš„æµ·å¤–ä¸»æœºç»æµ‹è¯•ä¼šè¢«Cloudflareæ‹¦æˆªï¼Œæ— æ³•ç›´æ¥è®¿é—®claude api\n\n### è½¯ä»¶è¦æ±‚\n\n- **Node.js** 18æˆ–æ›´é«˜ç‰ˆæœ¬\n- **Redis** 6æˆ–æ›´é«˜ç‰ˆæœ¬\n- **æ“ä½œç³»ç»Ÿ**: å»ºè®®Linux\n\n### è´¹ç”¨ä¼°ç®—\n\n- **æœåŠ¡å™¨**: è½»é‡äº‘æœåŠ¡å™¨ï¼Œä¸€ä¸ªæœˆ30-60å—\n- **Claudeè®¢é˜…**: çœ‹ä½ æ€ä¹ˆåˆ†æ‘Šäº†\n- **å…¶ä»–**: åŸŸåï¼ˆå¯é€‰ï¼‰\n\n---\n\n## ğŸš€ è„šæœ¬éƒ¨ç½²ï¼ˆæ¨èï¼‰\n\næ¨èä½¿ç”¨ç®¡ç†è„šæœ¬è¿›è¡Œä¸€é”®éƒ¨ç½²ï¼Œç®€å•å¿«æ·ï¼Œè‡ªåŠ¨å¤„ç†æ‰€æœ‰ä¾èµ–å’Œé…ç½®ã€‚\n\n### å¿«é€Ÿå®‰è£…\n\n```bash\ncurl -fsSL https://pincc.ai/manage.sh -o manage.sh && chmod +x manage.sh && ./manage.sh install\n```\n\n### è„šæœ¬åŠŸèƒ½\n\n- âœ… **ä¸€é”®å®‰è£…**: è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿç¯å¢ƒï¼Œå®‰è£… Node.js 18+ã€Redis ç­‰ä¾èµ–\n- âœ… **äº¤äº’å¼é…ç½®**: å‹å¥½çš„é…ç½®å‘å¯¼ï¼Œè®¾ç½®ç«¯å£ã€Redis è¿æ¥ç­‰\n- âœ… **è‡ªåŠ¨å¯åŠ¨**: å®‰è£…å®Œæˆåè‡ªåŠ¨å¯åŠ¨æœåŠ¡å¹¶æ˜¾ç¤ºè®¿é—®åœ°å€\n- âœ… **ä¾¿æ·ç®¡ç†**: é€šè¿‡ `crs` å‘½ä»¤éšæ—¶ç®¡ç†æœåŠ¡çŠ¶æ€\n\n### ç®¡ç†å‘½ä»¤\n\n```bash\ncrs install   # å®‰è£…æœåŠ¡\ncrs start     # å¯åŠ¨æœåŠ¡\ncrs stop      # åœæ­¢æœåŠ¡\ncrs restart   # é‡å¯æœåŠ¡\ncrs status    # æŸ¥çœ‹çŠ¶æ€\ncrs update    # æ›´æ–°æœåŠ¡\ncrs uninstall # å¸è½½æœåŠ¡\n```\n\n### å®‰è£…ç¤ºä¾‹\n\n```bash\n$ crs install\n\n# ä¼šä¾æ¬¡è¯¢é—®ï¼š\nå®‰è£…ç›®å½• (é»˜è®¤: ~/claude-relay-service):\næœåŠ¡ç«¯å£ (é»˜è®¤: 3000): 8080\nRedis åœ°å€ (é»˜è®¤: localhost):\nRedis ç«¯å£ (é»˜è®¤: 6379):\nRedis å¯†ç  (é»˜è®¤: æ— å¯†ç ):\n\n# å®‰è£…å®Œæˆåè‡ªåŠ¨å¯åŠ¨å¹¶æ˜¾ç¤ºï¼š\næœåŠ¡å·²æˆåŠŸå®‰è£…å¹¶å¯åŠ¨ï¼\n\nè®¿é—®åœ°å€ï¼š\n  æœ¬åœ° Web: http://localhost:8080/web\n  å…¬ç½‘ Web: http://YOUR_IP:8080/web\n\nç®¡ç†å‘˜è´¦å·ä¿¡æ¯å·²ä¿å­˜åˆ°: data/init.json\n```\n\n### ç³»ç»Ÿè¦æ±‚\n\n- æ”¯æŒç³»ç»Ÿ: Ubuntu/Debianã€CentOS/RedHatã€Arch Linuxã€macOS\n- è‡ªåŠ¨å®‰è£… Node.js 18+ å’Œ Redis\n- Redis ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä½ç½®ï¼Œæ•°æ®ç‹¬ç«‹äºåº”ç”¨\n\n---\n\n## ğŸ“¦ æ‰‹åŠ¨éƒ¨ç½²\n\n### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡\n\n**Ubuntu/Debianç”¨æˆ·ï¼š**\n\n```bash\n# å®‰è£…Node.js\ncurl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\nsudo apt-get install -y nodejs\n\n# å®‰è£…Redis\nsudo apt update\nsudo apt install redis-server\nsudo systemctl start redis-server\n```\n\n**CentOS/RHELç”¨æˆ·ï¼š**\n\n```bash\n# å®‰è£…Node.js\ncurl -fsSL https://rpm.nodesource.com/setup_18.x | sudo bash -\nsudo yum install -y nodejs\n\n# å®‰è£…Redis\nsudo yum install redis\nsudo systemctl start redis\n```\n\n### ç¬¬äºŒæ­¥ï¼šä¸‹è½½å’Œé…ç½®\n\n```bash\n# ä¸‹è½½é¡¹ç›®\ngit clone https://github.com/Wei-Shaw//claude-relay-service.git\ncd claude-relay-service\n\n# å®‰è£…ä¾èµ–\nnpm install\n\n# å¤åˆ¶é…ç½®æ–‡ä»¶ï¼ˆé‡è¦ï¼ï¼‰\ncp config/config.example.js config/config.js\ncp .env.example .env\n```\n\n### ç¬¬ä¸‰æ­¥ï¼šé…ç½®æ–‡ä»¶è®¾ç½®\n\n**ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š**\n\n```bash\n# è¿™ä¸¤ä¸ªå¯†é’¥éšä¾¿ç”Ÿæˆï¼Œä½†è¦è®°ä½\nJWT_SECRET=ä½ çš„è¶…çº§ç§˜å¯†å¯†é’¥\nENCRYPTION_KEY=32ä½çš„åŠ å¯†å¯†é’¥éšä¾¿å†™\n\n# Redisé…ç½®\nREDIS_HOST=localhost\nREDIS_PORT=6379\nREDIS_PASSWORD=\n\n```\n\n**ç¼–è¾‘ `config/config.js` æ–‡ä»¶ï¼š**\n\n```javascript\nmodule.exports = {\n  server: {\n    port: 3000, // æœåŠ¡ç«¯å£ï¼Œå¯ä»¥æ”¹\n    host: '0.0.0.0' // ä¸ç”¨æ”¹\n  },\n  redis: {\n    host: '127.0.0.1', // Redisåœ°å€\n    port: 6379 // Redisç«¯å£\n  }\n  // å…¶ä»–é…ç½®ä¿æŒé»˜è®¤å°±è¡Œ\n}\n```\n\n### ç¬¬å››æ­¥ï¼šå®‰è£…å‰ç«¯ä¾èµ–å¹¶æ„å»º\n\n```bash\n# å®‰è£…å‰ç«¯ä¾èµ–\nnpm run install:web\n\n# æ„å»ºå‰ç«¯ï¼ˆç”Ÿæˆ dist ç›®å½•ï¼‰\nnpm run build:web\n```\n\n### ç¬¬äº”æ­¥ï¼šå¯åŠ¨æœåŠ¡\n\n```bash\n# åˆå§‹åŒ–\nnpm run setup # ä¼šéšæœºç”Ÿæˆåå°è´¦å·å¯†ç ä¿¡æ¯ï¼Œå­˜å‚¨åœ¨ data/init.json\n# æˆ–è€…é€šè¿‡ç¯å¢ƒå˜é‡é¢„è®¾ç®¡ç†å‘˜å‡­æ®ï¼š\n# export ADMIN_USERNAME=cr_admin_custom\n# export ADMIN_PASSWORD=your-secure-password\n\n# å¯åŠ¨æœåŠ¡\nnpm run service:start:daemon   # åå°è¿è¡Œ\n\n# æŸ¥çœ‹çŠ¶æ€\nnpm run service:status\n```\n\n---\n\n## ğŸ³ Docker éƒ¨ç½²\n\n### Docker compose\n\n#### ç¬¬ä¸€æ­¥ï¼šä¸‹è½½æ„å»ºdocker-compose.ymlæ–‡ä»¶çš„è„šæœ¬å¹¶æ‰§è¡Œ\n```bash\ncurl -fsSL https://pincc.ai/crs-compose.sh -o crs-compose.sh && chmod +x crs-compose.sh && ./crs-compose.sh\n```\n\n#### ç¬¬äºŒæ­¥ï¼šå¯åŠ¨\n```bash\ndocker-compose up -d\n```\n\n### Docker Compose é…ç½®\n\ndocker-compose.yml å·²åŒ…å«ï¼š\n\n- âœ… è‡ªåŠ¨åˆå§‹åŒ–ç®¡ç†å‘˜è´¦å·\n- âœ… æ•°æ®æŒä¹…åŒ–ï¼ˆlogså’Œdataç›®å½•è‡ªåŠ¨æŒ‚è½½ï¼‰\n- âœ… Redisæ•°æ®åº“\n- âœ… å¥åº·æ£€æŸ¥\n- âœ… è‡ªåŠ¨é‡å¯\n\n### ç¯å¢ƒå˜é‡è¯´æ˜\n\n#### å¿…å¡«é¡¹\n\n- `JWT_SECRET`: JWTå¯†é’¥ï¼Œè‡³å°‘32ä¸ªå­—ç¬¦\n- `ENCRYPTION_KEY`: åŠ å¯†å¯†é’¥ï¼Œå¿…é¡»æ˜¯32ä¸ªå­—ç¬¦\n\n#### å¯é€‰é¡¹\n\n- `ADMIN_USERNAME`: ç®¡ç†å‘˜ç”¨æˆ·åï¼ˆä¸è®¾ç½®åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰\n- `ADMIN_PASSWORD`: ç®¡ç†å‘˜å¯†ç ï¼ˆä¸è®¾ç½®åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰\n- `LOG_LEVEL`: æ—¥å¿—çº§åˆ«ï¼ˆé»˜è®¤ï¼šinfoï¼‰\n- æ›´å¤šé…ç½®é¡¹è¯·å‚è€ƒ `.env.example` æ–‡ä»¶\n\n### ç®¡ç†å‘˜å‡­æ®è·å–æ–¹å¼\n\n1. **æŸ¥çœ‹å®¹å™¨æ—¥å¿—**\n\n   ```bash\n   docker logs claude-relay-service\n   ```\n\n2. **æŸ¥çœ‹æŒ‚è½½çš„æ–‡ä»¶**\n\n   ```bash\n   cat ./data/init.json\n   ```\n\n3. **ä½¿ç”¨ç¯å¢ƒå˜é‡é¢„è®¾**\n   ```bash\n   # åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®\n   ADMIN_USERNAME=cr_admin_custom\n   ADMIN_PASSWORD=your-secure-password\n   ```\n\n---\n\n## ğŸ® å¼€å§‹ä½¿ç”¨\n\n### 1. æ‰“å¼€ç®¡ç†ç•Œé¢\n\næµè§ˆå™¨è®¿é—®ï¼š`http://ä½ çš„æœåŠ¡å™¨IP:3000/web`\n\nç®¡ç†å‘˜è´¦å·ï¼š\n\n- è‡ªåŠ¨ç”Ÿæˆï¼šæŸ¥çœ‹ data/init.json\n- ç¯å¢ƒå˜é‡é¢„è®¾ï¼šé€šè¿‡ ADMIN_USERNAME å’Œ ADMIN_PASSWORD è®¾ç½®\n- Docker éƒ¨ç½²ï¼šæŸ¥çœ‹å®¹å™¨æ—¥å¿— `docker logs claude-relay-service`\n\n### 2. æ·»åŠ Claudeè´¦æˆ·\n\nè¿™ä¸€æ­¥æ¯”è¾ƒå…³é”®ï¼Œéœ€è¦OAuthæˆæƒï¼š\n\n1. ç‚¹å‡»ã€ŒClaudeè´¦æˆ·ã€æ ‡ç­¾\n2. å¦‚æœä½ æ‹…å¿ƒå¤šä¸ªè´¦å·å…±ç”¨1ä¸ªIPæ€•è¢«å°ç¦ï¼Œå¯ä»¥é€‰æ‹©è®¾ç½®é™æ€ä»£ç†IPï¼ˆå¯é€‰ï¼‰\n3. ç‚¹å‡»ã€Œæ·»åŠ è´¦æˆ·ã€\n4. ç‚¹å‡»ã€Œç”Ÿæˆæˆæƒé“¾æ¥ã€ï¼Œä¼šæ‰“å¼€ä¸€ä¸ªæ–°é¡µé¢\n5. åœ¨æ–°é¡µé¢å®ŒæˆClaudeç™»å½•å’Œæˆæƒ\n6. å¤åˆ¶è¿”å›çš„Authorization Code\n7. ç²˜è´´åˆ°é¡µé¢å®Œæˆæ·»åŠ \n\n**æ³¨æ„**: å¦‚æœä½ åœ¨å›½å†…ï¼Œè¿™ä¸€æ­¥å¯èƒ½éœ€è¦ç§‘å­¦ä¸Šç½‘ã€‚\n\n### 3. åˆ›å»ºAPI Key\n\nç»™æ¯ä¸ªä½¿ç”¨è€…åˆ†é…ä¸€ä¸ªKeyï¼š\n\n1. ç‚¹å‡»ã€ŒAPI Keysã€æ ‡ç­¾\n2. ç‚¹å‡»ã€Œåˆ›å»ºæ–°Keyã€\n3. ç»™Keyèµ·ä¸ªåå­—ï¼Œæ¯”å¦‚ã€Œå¼ ä¸‰çš„Keyã€\n4. è®¾ç½®ä½¿ç”¨é™åˆ¶ï¼ˆå¯é€‰ï¼‰ï¼š\n   - **é€Ÿç‡é™åˆ¶**: é™åˆ¶æ¯ä¸ªæ—¶é—´çª—å£çš„è¯·æ±‚æ¬¡æ•°å’ŒTokenä½¿ç”¨é‡\n   - **å¹¶å‘é™åˆ¶**: é™åˆ¶åŒæ—¶å¤„ç†çš„è¯·æ±‚æ•°\n   - **æ¨¡å‹é™åˆ¶**: é™åˆ¶å¯è®¿é—®çš„æ¨¡å‹åˆ—è¡¨\n   - **å®¢æˆ·ç«¯é™åˆ¶**: é™åˆ¶åªå…è®¸ç‰¹å®šå®¢æˆ·ç«¯ä½¿ç”¨ï¼ˆå¦‚ClaudeCodeã€Gemini-CLIç­‰ï¼‰\n5. ä¿å­˜ï¼Œè®°ä¸‹ç”Ÿæˆçš„Key\n\n### 4. å¼€å§‹ä½¿ç”¨ Claude Code å’Œ Gemini CLI\n\nç°åœ¨ä½ å¯ä»¥ç”¨è‡ªå·±çš„æœåŠ¡æ›¿æ¢å®˜æ–¹APIäº†ï¼š\n\n**Claude Code è®¾ç½®ç¯å¢ƒå˜é‡ï¼š**\n\n\n**ä½¿ç”¨æ ‡å‡† Claude è´¦å·æ± **\n\né»˜è®¤ä½¿ç”¨æ ‡å‡† Claude è´¦å·æ± ï¼š\n\n```bash\nexport ANTHROPIC_BASE_URL=\"http://127.0.0.1:3000/api/\" # æ ¹æ®å®é™…å¡«å†™ä½ æœåŠ¡å™¨çš„ipåœ°å€æˆ–è€…åŸŸå\nexport ANTHROPIC_AUTH_TOKEN=\"åå°åˆ›å»ºçš„APIå¯†é’¥\"\n```\n\n**ä½¿ç”¨ Antigravity è´¦æˆ·æ± **\n\né€‚ç”¨äºé€šè¿‡ Antigravity æ¸ é“ä½¿ç”¨ Claude æ¨¡å‹ï¼ˆå¦‚ `claude-opus-4-5` ç­‰ï¼‰ã€‚\n\n```bash\n# 1. è®¾ç½® Base URL ä¸º Antigravity ä¸“ç”¨è·¯å¾„\nexport ANTHROPIC_BASE_URL=\"http://127.0.0.1:3000/antigravity/api/\"\n\n# 2. è®¾ç½® API Keyï¼ˆåœ¨åå°åˆ›å»ºï¼Œæƒé™éœ€åŒ…å« 'all' æˆ– 'gemini'ï¼‰\nexport ANTHROPIC_AUTH_TOKEN=\"åå°åˆ›å»ºçš„APIå¯†é’¥\"\n\n# 3. æŒ‡å®šæ¨¡å‹åç§°ï¼ˆç›´æ¥ä½¿ç”¨çŸ­åï¼Œæ— éœ€å‰ç¼€ï¼ï¼‰\nexport ANTHROPIC_MODEL=\"claude-opus-4-5\"\n\n# 4. å¯åŠ¨\nclaude\n```\n\n**VSCode Claude æ’ä»¶é…ç½®ï¼š**\n\nå¦‚æœä½¿ç”¨ VSCode çš„ Claude æ’ä»¶ï¼Œéœ€è¦åœ¨ `~/.claude/config.json` æ–‡ä»¶ä¸­é…ç½®ï¼š\n\n```json\n{\n    \"primaryApiKey\": \"crs\"\n}\n```\n\nå¦‚æœè¯¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ‰‹åŠ¨åˆ›å»ºã€‚Windows ç”¨æˆ·è·¯å¾„ä¸º `C:\\Users\\ä½ çš„ç”¨æˆ·å\\.claude\\config.json`ã€‚\n\n> ğŸ’¡ **IntelliJ IDEA ç”¨æˆ·æ¨è**ï¼š[Claude Code Plus](https://github.com/touwaeriol/claude-code-plus) - å°† Claude Code ç›´æ¥é›†æˆåˆ° IDEï¼Œæ”¯æŒä»£ç ç†è§£ã€æ–‡ä»¶è¯»å†™ã€å‘½ä»¤æ‰§è¡Œã€‚æ’ä»¶å¸‚åœºæœç´¢ `Claude Code Plus` å³å¯å®‰è£…ã€‚\n\n**Gemini CLI è®¾ç½®ç¯å¢ƒå˜é‡ï¼š**\n\n**æ–¹å¼ä¸€ï¼ˆæ¨èï¼‰ï¼šé€šè¿‡ Gemini Assist API æ–¹å¼è®¿é—®**\n\n```bash\nCODE_ASSIST_ENDPOINT=\"http://127.0.0.1:3000/gemini\"  # æ ¹æ®å®é™…å¡«å†™ä½ æœåŠ¡å™¨çš„ipåœ°å€æˆ–è€…åŸŸå\nGOOGLE_CLOUD_ACCESS_TOKEN=\"åå°åˆ›å»ºçš„APIå¯†é’¥\"\nGOOGLE_GENAI_USE_GCA=\"true\"\nGEMINI_MODEL=\"gemini-2.5-pro\" # å¦‚æœä½ æœ‰gemini3æƒé™å¯ä»¥å¡«ï¼š gemini-3-pro-preview\n```\n\n> **è®¤è¯**ï¼šåªèƒ½é€‰ ```Login with Google``` è¿›è¡Œè®¤è¯ï¼Œå¦‚æœè·³ Googleè¯·åˆ é™¤ ```~/.gemini/settings.json``` åå†å°è¯•å¯åŠ¨```gemini```ã€‚  \n> **æ³¨æ„**ï¼šgemini-cli æ§åˆ¶å°ä¼šæç¤º `Failed to fetch user info: 401 Unauthorized`ï¼Œä½†ä½¿ç”¨ä¸å—ä»»ä½•å½±å“ã€‚  \n\n**æ–¹å¼äºŒï¼šé€šè¿‡ Gemini API æ–¹å¼è®¿é—®**\n\n\n```bash\nGOOGLE_GEMINI_BASE_URL=\"http://127.0.0.1:3000/gemini\"  # æ ¹æ®å®é™…å¡«å†™ä½ æœåŠ¡å™¨çš„ipåœ°å€æˆ–è€…åŸŸå\nGEMINI_API_KEY=\"åå°åˆ›å»ºçš„APIå¯†é’¥\"\nGEMINI_MODEL=\"gemini-2.5-pro\" # å¦‚æœä½ æœ‰gemini3æƒé™å¯ä»¥å¡«ï¼š gemini-3-pro-preview\n```\n\n> **è®¤è¯**ï¼šåªèƒ½é€‰ ```Use Gemini API Key``` è¿›è¡Œè®¤è¯ï¼Œå¦‚æœæç¤º ```Enter Gemini API Key``` è¯·ç›´æ¥ç•™ç©ºæŒ‰å›è½¦ã€‚å¦‚æœä¸€æ‰“å¼€å°±è·³ Googleè¯·åˆ é™¤ ```~/.gemini/settings.json``` åå†å°è¯•å¯åŠ¨```gemini```ã€‚\n\n> ğŸ’¡ **è¿›é˜¶ç”¨æ³•**ï¼šæƒ³åœ¨ Claude Code ä¸­ç›´æ¥ä½¿ç”¨ Gemini 3 æ¨¡å‹ï¼Ÿè¯·å‚è€ƒ [Claude Code è°ƒç”¨ Gemini 3 æ¨¡å‹æŒ‡å—](docs/claude-code-gemini3-guide/README.md)\n\n**ä½¿ç”¨ Claude Codeï¼š**\n\n```bash\nclaude\n```\n\n**ä½¿ç”¨ Gemini CLIï¼š**\n\n```bash\ngemini  # æˆ–å…¶ä»– Gemini CLI å‘½ä»¤\n```\n\n**Codex é…ç½®ï¼š**\n\nåœ¨ `~/.codex/config.toml` æ–‡ä»¶**å¼€å¤´**æ·»åŠ ä»¥ä¸‹é…ç½®ï¼š\n\n```toml\nmodel_provider = \"crs\"\nmodel = \"gpt-5.1-codex-max\"\nmodel_reasoning_effort = \"high\"\ndisable_response_storage = true\npreferred_auth_method = \"apikey\"\n\n[model_providers.crs]\nname = \"crs\"\nbase_url = \"http://127.0.0.1:3000/openai\"  # æ ¹æ®å®é™…å¡«å†™ä½ æœåŠ¡å™¨çš„ipåœ°å€æˆ–è€…åŸŸå\nwire_api = \"responses\"\nrequires_openai_auth = true\nenv_key = \"CRS_OAI_KEY\"\n```\n\nåœ¨ `~/.codex/auth.json` æ–‡ä»¶ä¸­é…ç½®APIå¯†é’¥ä¸º nullï¼š\n\n```json\n{\n    \"OPENAI_API_KEY\": null  \n}\n```\n\nç¯å¢ƒå˜é‡è®¾ç½®ï¼š\n\n```bash\nexport CRS_OAI_KEY=\"åå°åˆ›å»ºçš„APIå¯†é’¥\"\n```\n\n> âš ï¸ åœ¨é€šè¿‡ Nginx åå‘ä»£ç† CRS æœåŠ¡å¹¶ä½¿ç”¨ Codex CLI æ—¶ï¼Œéœ€è¦åœ¨ http å—ä¸­æ·»åŠ  underscores_in_headers on;ã€‚å› ä¸º Nginx é»˜è®¤ä¼šç§»é™¤å¸¦ä¸‹åˆ’çº¿çš„è¯·æ±‚å¤´ï¼ˆå¦‚ session_idï¼‰ï¼Œä¸€æ—¦è¯¥å¤´è¢«ä¸¢å¼ƒï¼Œå¤šè´¦å·ç¯å¢ƒä¸‹çš„ç²˜æ€§ä¼šè¯åŠŸèƒ½å°†å¤±æ•ˆã€‚\n\n**Droid CLI é…ç½®ï¼š**\n\nDroid CLI è¯»å– `~/.factory/config.json`ã€‚å¯ä»¥åœ¨è¯¥æ–‡ä»¶ä¸­æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹ä»¥æŒ‡å‘æœ¬æœåŠ¡çš„æ–°ç«¯ç‚¹ï¼š\n\n```json\n{\n  \"custom_models\": [\n    {\n      \"model_display_name\": \"Opus 4.5 [crs]\",\n      \"model\": \"claude-opus-4-5-20251101\",\n      \"base_url\": \"http://127.0.0.1:3000/droid/claude\",\n      \"api_key\": \"åå°åˆ›å»ºçš„APIå¯†é’¥\",\n      \"provider\": \"anthropic\",\n      \"max_tokens\": 64000\n    },\n    {\n      \"model_display_name\": \"GPT5-Codex [crs]\",\n      \"model\": \"gpt-5-codex\",\n      \"base_url\": \"http://127.0.0.1:3000/droid/openai\",\n      \"api_key\": \"åå°åˆ›å»ºçš„APIå¯†é’¥\",\n      \"provider\": \"openai\",\n      \"max_tokens\": 16384\n    },\n    {\n      \"model_display_name\": \"Gemini-3-Pro [crs]\",\n      \"model\": \"gemini-3-pro-preview\",\n      \"base_url\": \"http://127.0.0.1:3000/droid/comm/v1/\",\n      \"api_key\": \"åå°åˆ›å»ºçš„APIå¯†é’¥\",\n      \"provider\": \"generic-chat-completion-api\",\n      \"max_tokens\": 65535\n    },\n    {\n      \"model_display_name\": \"GLM-4.6 [crs]\",\n      \"model\": \"glm-4.6\",\n      \"base_url\": \"http://127.0.0.1:3000/droid/comm/v1/\",\n      \"api_key\": \"åå°åˆ›å»ºçš„APIå¯†é’¥\",\n      \"provider\": \"generic-chat-completion-api\",\n      \"max_tokens\": 202800\n    }\n  ]\n}\n```\n\n> ğŸ’¡ å°†ç¤ºä¾‹ä¸­çš„ `http://127.0.0.1:3000` æ›¿æ¢ä¸ºä½ çš„æœåŠ¡åŸŸåæˆ–å…¬ç½‘åœ°å€ï¼Œå¹¶å†™å…¥åå°ç”Ÿæˆçš„ API å¯†é’¥ï¼ˆcr_ å¼€å¤´ï¼‰ã€‚\n\n### 5. ç¬¬ä¸‰æ–¹å·¥å…·APIæ¥å…¥\n\næœ¬æœåŠ¡æ”¯æŒå¤šç§APIç«¯ç‚¹æ ¼å¼ï¼Œæ–¹ä¾¿æ¥å…¥ä¸åŒçš„ç¬¬ä¸‰æ–¹å·¥å…·ï¼ˆå¦‚Cherry Studioç­‰ï¼‰ã€‚\n\n#### Cherry Studio æ¥å…¥ç¤ºä¾‹\n\nCherry Studioæ”¯æŒå¤šç§AIæœåŠ¡çš„æ¥å…¥ï¼Œä¸‹é¢æ˜¯ä¸åŒè´¦å·ç±»å‹çš„è¯¦ç»†é…ç½®ï¼š\n\n**1. Claudeè´¦å·æ¥å…¥ï¼š**\n\n```\n# APIåœ°å€\nhttp://ä½ çš„æœåŠ¡å™¨:3000/claude\n\n# æ¨¡å‹IDç¤ºä¾‹\nclaude-sonnet-4-5-20250929 # Claude Sonnet 4.5\nclaude-opus-4-20250514     # Claude Opus 4\n```\n\né…ç½®æ­¥éª¤ï¼š\n- ä¾›åº”å•†ç±»å‹é€‰æ‹©\"Anthropic\"\n- APIåœ°å€å¡«å…¥ï¼š`http://ä½ çš„æœåŠ¡å™¨:3000/claude`\n- API Keyå¡«å…¥ï¼šåå°åˆ›å»ºçš„APIå¯†é’¥ï¼ˆcr_å¼€å¤´ï¼‰\n\n**2. Geminiè´¦å·æ¥å…¥ï¼š**\n\n```\n# APIåœ°å€\nhttp://ä½ çš„æœåŠ¡å™¨:3000/gemini\n\n# æ¨¡å‹IDç¤ºä¾‹\ngemini-2.5-pro             # Gemini 2.5 Pro\n```\n\né…ç½®æ­¥éª¤ï¼š\n- ä¾›åº”å•†ç±»å‹é€‰æ‹©\"Gemini\"\n- APIåœ°å€å¡«å…¥ï¼š`http://ä½ çš„æœåŠ¡å™¨:3000/gemini`\n- API Keyå¡«å…¥ï¼šåå°åˆ›å»ºçš„APIå¯†é’¥ï¼ˆcr_å¼€å¤´ï¼‰\n\n**3. Codexæ¥å…¥ï¼š**\n\n```\n# APIåœ°å€\nhttp://ä½ çš„æœåŠ¡å™¨:3000/openai\n\n# æ¨¡å‹IDï¼ˆå›ºå®šï¼‰\ngpt-5                      # Codexä½¿ç”¨å›ºå®šæ¨¡å‹ID\n```\n\né…ç½®æ­¥éª¤ï¼š\n- ä¾›åº”å•†ç±»å‹é€‰æ‹©\"Openai-Response\"\n- APIåœ°å€å¡«å…¥ï¼š`http://ä½ çš„æœåŠ¡å™¨:3000/openai`\n- API Keyå¡«å…¥ï¼šåå°åˆ›å»ºçš„APIå¯†é’¥ï¼ˆcr_å¼€å¤´ï¼‰\n- **é‡è¦**ï¼šCodexåªæ”¯æŒOpenai-Responseæ ‡å‡†\n\n\n**Cherry Studio åœ°å€æ ¼å¼é‡è¦è¯´æ˜ï¼š**\n\n- âœ… **æ¨èæ ¼å¼**ï¼š`http://ä½ çš„æœåŠ¡å™¨:3000/claude`ï¼ˆä¸åŠ ç»“å°¾ `/`ï¼Œè®© Cherry Studio è‡ªåŠ¨åŠ ä¸Š v1ï¼‰\n- âœ… **ç­‰æ•ˆæ ¼å¼**ï¼š`http://ä½ çš„æœåŠ¡å™¨:3000/claude/v1/`ï¼ˆæ‰‹åŠ¨æŒ‡å®š v1 å¹¶åŠ ç»“å°¾ `/`ï¼‰\n- ğŸ’¡ **è¯´æ˜**ï¼šè¿™ä¸¤ç§æ ¼å¼åœ¨ Cherry Studio ä¸­æ˜¯å®Œå…¨ç­‰æ•ˆçš„\n- âŒ **é”™è¯¯æ ¼å¼**ï¼š`http://ä½ çš„æœåŠ¡å™¨:3000/claude/`ï¼ˆå•ç‹¬çš„ `/` ç»“å°¾ä¼šè¢« Cherry Studio å¿½ç•¥ v1 ç‰ˆæœ¬ï¼‰\n\n#### å…¶ä»–ç¬¬ä¸‰æ–¹å·¥å…·æ¥å…¥\n\n**æ¥å…¥è¦ç‚¹ï¼š**\n\n- æ‰€æœ‰è´¦å·ç±»å‹éƒ½ä½¿ç”¨ç›¸åŒçš„APIå¯†é’¥ï¼ˆåœ¨åå°ç»Ÿä¸€åˆ›å»ºï¼‰\n- æ ¹æ®ä¸åŒçš„è·¯ç”±å‰ç¼€è‡ªåŠ¨è¯†åˆ«è´¦å·ç±»å‹\n- `/claude/` - ä½¿ç”¨Claudeè´¦å·æ± \n- `/antigravity/api/` - ä½¿ç”¨Antigravityè´¦å·æ± ï¼ˆæ¨èç”¨äºClaude Codeï¼‰\n- `/droid/claude/` - ä½¿ç”¨Droidç±»å‹Claudeè´¦å·æ± ï¼ˆåªå»ºè®®apiè°ƒç”¨æˆ–Droid Cliä¸­ä½¿ç”¨ï¼‰\n- `/gemini/` - ä½¿ç”¨Geminiè´¦å·æ± \n- `/openai/` - ä½¿ç”¨Codexè´¦å·ï¼ˆåªæ”¯æŒOpenai-Responseæ ¼å¼ï¼‰\n- `/droid/openai/` - ä½¿ç”¨Droidç±»å‹OpenAIå…¼å®¹è´¦å·æ± ï¼ˆåªå»ºè®®apiè°ƒç”¨æˆ–Droid Cliä¸­ä½¿ç”¨ï¼‰\n- æ”¯æŒæ‰€æœ‰æ ‡å‡†APIç«¯ç‚¹ï¼ˆmessagesã€modelsç­‰ï¼‰\n\n**é‡è¦è¯´æ˜ï¼š**\n\n- ç¡®ä¿åœ¨åå°å·²æ·»åŠ å¯¹åº”ç±»å‹çš„è´¦å·ï¼ˆClaude/Gemini/Codexï¼‰\n- APIå¯†é’¥å¯ä»¥é€šç”¨ï¼Œç³»ç»Ÿä¼šæ ¹æ®è·¯ç”±è‡ªåŠ¨é€‰æ‹©è´¦å·ç±»å‹\n- å»ºè®®ä¸ºä¸åŒç”¨æˆ·åˆ›å»ºä¸åŒçš„APIå¯†é’¥ä¾¿äºä½¿ç”¨ç»Ÿè®¡\n\n---\n\n## ğŸ”§ æ—¥å¸¸ç»´æŠ¤\n\n### æœåŠ¡ç®¡ç†\n\n```bash\n# æŸ¥çœ‹æœåŠ¡çŠ¶æ€\nnpm run service:status\n\n# æŸ¥çœ‹æ—¥å¿—\nnpm run service:logs\n\n# é‡å¯æœåŠ¡\nnpm run service:restart:daemon\n\n# åœæ­¢æœåŠ¡\nnpm run service:stop\n```\n\n### ç›‘æ§ä½¿ç”¨æƒ…å†µ\n\n- **Webç•Œé¢**: `http://ä½ çš„åŸŸå:3000/web` - æŸ¥çœ‹ä½¿ç”¨ç»Ÿè®¡\n- **å¥åº·æ£€æŸ¥**: `http://ä½ çš„åŸŸå:3000/health` - ç¡®è®¤æœåŠ¡æ­£å¸¸\n- **æ—¥å¿—æ–‡ä»¶**: `logs/` ç›®å½•ä¸‹çš„å„ç§æ—¥å¿—æ–‡ä»¶\n\n### å‡çº§æŒ‡å—\n\nå½“æœ‰æ–°ç‰ˆæœ¬å‘å¸ƒæ—¶ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å‡çº§æœåŠ¡ï¼š\n\n```bash\n# 1. è¿›å…¥é¡¹ç›®ç›®å½•\ncd claude-relay-service\n\n# 2. æ‹‰å–æœ€æ–°ä»£ç \ngit pull origin main\n\n# å¦‚æœé‡åˆ° package-lock.json å†²çªï¼Œä½¿ç”¨è¿œç¨‹ç‰ˆæœ¬\ngit checkout --theirs package-lock.json\ngit add package-lock.json\n\n# 3. å®‰è£…æ–°çš„ä¾èµ–ï¼ˆå¦‚æœæœ‰ï¼‰\nnpm install\n\n# 4. å®‰è£…å¹¶æ„å»ºå‰ç«¯\nnpm run install:web\nnpm run build:web\n\n# 5. é‡å¯æœåŠ¡\nnpm run service:restart:daemon\n\n# 6. æ£€æŸ¥æœåŠ¡çŠ¶æ€\nnpm run service:status\n```\n\n**æ³¨æ„äº‹é¡¹ï¼š**\n\n- å‡çº§å‰å»ºè®®å¤‡ä»½é‡è¦é…ç½®æ–‡ä»¶ï¼ˆ.env, config/config.jsï¼‰\n- æŸ¥çœ‹æ›´æ–°æ—¥å¿—äº†è§£æ˜¯å¦æœ‰ç ´åæ€§å˜æ›´\n- å¦‚æœæœ‰æ•°æ®åº“ç»“æ„å˜æ›´ï¼Œä¼šè‡ªåŠ¨è¿ç§»\n\n---\n\n## ğŸ”’ å®¢æˆ·ç«¯é™åˆ¶åŠŸèƒ½\n\n### åŠŸèƒ½è¯´æ˜\n\nå®¢æˆ·ç«¯é™åˆ¶åŠŸèƒ½å…è®¸ä½ æ§åˆ¶æ¯ä¸ªAPI Keyå¯ä»¥è¢«å“ªäº›å®¢æˆ·ç«¯ä½¿ç”¨ï¼Œé€šè¿‡User-Agentè¯†åˆ«å®¢æˆ·ç«¯ï¼Œæé«˜APIçš„å®‰å…¨æ€§ã€‚\n\n### ä½¿ç”¨æ–¹æ³•\n\n1. **åœ¨åˆ›å»ºæˆ–ç¼–è¾‘API Keyæ—¶å¯ç”¨å®¢æˆ·ç«¯é™åˆ¶**ï¼š\n   - å‹¾é€‰\"å¯ç”¨å®¢æˆ·ç«¯é™åˆ¶\"\n   - é€‰æ‹©å…è®¸çš„å®¢æˆ·ç«¯ï¼ˆæ”¯æŒå¤šé€‰ï¼‰\n\n2. **é¢„å®šä¹‰å®¢æˆ·ç«¯**ï¼š\n   - **ClaudeCode**: å®˜æ–¹Claude CLIï¼ˆåŒ¹é… `claude-cli/x.x.x (external, cli)` æ ¼å¼ï¼‰\n   - **Gemini-CLI**: Geminiå‘½ä»¤è¡Œå·¥å…·ï¼ˆåŒ¹é… `GeminiCLI/vx.x.x (platform; arch)` æ ¼å¼ï¼‰\n\n3. **è°ƒè¯•å’Œè¯Šæ–­**ï¼š\n   - ç³»ç»Ÿä¼šåœ¨æ—¥å¿—ä¸­è®°å½•æ‰€æœ‰è¯·æ±‚çš„User-Agent\n   - å®¢æˆ·ç«¯éªŒè¯å¤±è´¥æ—¶ä¼šè¿”å›403é”™è¯¯å¹¶è®°å½•è¯¦ç»†ä¿¡æ¯\n   - é€šè¿‡æ—¥å¿—å¯ä»¥æŸ¥çœ‹å®é™…çš„User-Agentæ ¼å¼ï¼Œæ–¹ä¾¿é…ç½®è‡ªå®šä¹‰å®¢æˆ·ç«¯\n\n\n### æ—¥å¿—ç¤ºä¾‹\n\nè®¤è¯æˆåŠŸæ—¶çš„æ—¥å¿—ï¼š\n\n```\nğŸ”“ Authenticated request from key: æµ‹è¯•Key (key-id) in 5ms\n   User-Agent: \"claude-cli/1.0.58 (external, cli)\"\n```\n\nå®¢æˆ·ç«¯é™åˆ¶æ£€æŸ¥æ—¥å¿—ï¼š\n\n```\nğŸ” Checking client restriction for key: key-id (æµ‹è¯•Key)\n   User-Agent: \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n   Allowed clients: claude_code, gemini_cli\nğŸš« Client restriction failed for key: key-id (æµ‹è¯•Key) from 127.0.0.1, User-Agent: Mozilla/5.0...\n```\n\n### å¸¸è§é—®é¢˜å¤„ç†\n\n**Redisè¿ä¸ä¸Šï¼Ÿ**\n\n```bash\n# æ£€æŸ¥Redisæ˜¯å¦å¯åŠ¨\nredis-cli ping\n\n# åº”è¯¥è¿”å› PONG\n```\n\n**OAuthæˆæƒå¤±è´¥ï¼Ÿ**\n\n- æ£€æŸ¥ä»£ç†è®¾ç½®æ˜¯å¦æ­£ç¡®\n- ç¡®ä¿èƒ½æ­£å¸¸è®¿é—® claude.ai\n- æ¸…é™¤æµè§ˆå™¨ç¼“å­˜é‡è¯•\n\n**APIè¯·æ±‚å¤±è´¥ï¼Ÿ**\n\n- æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®\n- æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶æ‰¾é”™è¯¯ä¿¡æ¯\n- ç¡®è®¤Claudeè´¦æˆ·çŠ¶æ€æ­£å¸¸\n\n---\n\n## ğŸ› ï¸ è¿›é˜¶\n\n### åå‘ä»£ç†éƒ¨ç½²æŒ‡å—\n\nåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå»ºè®®é€šè¿‡åå‘ä»£ç†è¿›è¡Œè¿æ¥ï¼Œä»¥ä¾¿ä½¿ç”¨è‡ªåŠ¨ HTTPSã€å®‰å…¨å¤´éƒ¨å’Œæ€§èƒ½ä¼˜åŒ–ã€‚ä¸‹é¢æä¾›ä¸¤ç§å¸¸ç”¨æ–¹æ¡ˆï¼š **Caddy** å’Œ **Nginx Proxy Manager (NPM)**ã€‚\n\n---\n\n## Caddy æ–¹æ¡ˆ\n\nCaddy æ˜¯ä¸€æ¬¾è‡ªåŠ¨ç®¡ç† HTTPS è¯ä¹¦çš„ Web æœåŠ¡å™¨ï¼Œé…ç½®ç®€å•ã€æ€§èƒ½ä¼˜ç§€ï¼Œå¾ˆé€‚åˆä¸éœ€è¦ Docker ç¯å¢ƒçš„éƒ¨ç½²æ–¹æ¡ˆã€‚\n\n**1. å®‰è£… Caddy**\n\n```bash\n# Ubuntu/Debian\nsudo apt install -y debian-keyring debian-archive-keyring apt-transport-https\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/gpg.key' | sudo gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/debian.deb.txt' | sudo tee /etc/apt/sources.list.d/caddy-stable.list\nsudo apt update\nsudo apt install caddy\n\n# CentOS/RHEL/Fedora\nsudo yum install yum-plugin-copr\nsudo yum copr enable @caddy/caddy\nsudo yum install caddy\n```\n\n**2. Caddy é…ç½®**\n\nç¼–è¾‘ `/etc/caddy/Caddyfile` ï¼š\n\n```caddy\nyour-domain.com {\n    # åå‘ä»£ç†åˆ°æœ¬åœ°æœåŠ¡\n    reverse_proxy 127.0.0.1:3000 {\n        # æ”¯æŒæµå¼å“åº”æˆ– SSE\n        flush_interval -1\n\n        # ä¼ é€’çœŸå® IP\n        header_up X-Real-IP {remote_host}\n        header_up X-Forwarded-For {remote_host}\n        header_up X-Forwarded-Proto {scheme}\n\n        # é•¿è¯»/å†™è¶…æ—¶é…ç½®\n        transport http {\n            read_timeout 300s\n            write_timeout 300s\n            dial_timeout 30s\n        }\n    }\n\n    # å®‰å…¨å¤´éƒ¨\n    header {\n        Strict-Transport-Security \"max-age=31536000; includeSubDomains\"\n        X-Frame-Options \"DENY\"\n        X-Content-Type-Options \"nosniff\"\n        -Server\n    }\n}\n```\n\n**3. å¯åŠ¨ Caddy**\n\n```bash\nsudo caddy validate --config /etc/caddy/Caddyfile\nsudo systemctl start caddy\nsudo systemctl enable caddy\nsudo systemctl status caddy\n```\n\n**4. æœåŠ¡é…ç½®**\n\nCaddy ä¼šè‡ªåŠ¨ç®¡ç† HTTPSï¼Œå› æ­¤å¯ä»¥å°†æœåŠ¡é™åˆ¶åœ¨æœ¬åœ°è¿›è¡Œç›‘å¬ï¼š\n\n```javascript\n// config/config.js\nmodule.exports = {\n  server: {\n    port: 3000,\n    host: '127.0.0.1' // åªç›‘å¬æœ¬åœ°\n  }\n}\n```\n\n**Caddy ç‰¹ç‚¹**\n\n* ğŸ”’ è‡ªåŠ¨ HTTPSï¼Œé›¶é…ç½®è¯ä¹¦ç®¡ç†\n* ğŸ›¡ï¸ å®‰å…¨é»˜è®¤é…ç½®ï¼Œå¯ç”¨ç°ä»£ TLS å¥—ä»¶\n* âš¡ HTTP/2 å’Œæµå¼ä¼ è¾“æ”¯æŒ\n* ğŸ”§ é…ç½®æ–‡ä»¶ç®€æ´ï¼Œæ˜“äºç»´æŠ¤\n\n---\n\n## Nginx Proxy Manager (NPM) æ–¹æ¡ˆ\n\nNginx Proxy Manager é€šè¿‡å›¾å½¢åŒ–ç•Œé¢ç®¡ç†åå‘ä»£ç†å’Œ HTTPS è¯ä¹¦ï¼Œä¸¦ä»¥ Docker å®¹å™¨éƒ¨ç½²ã€‚\n\n**1. åœ¨ NPM åˆ›å»ºæ–°çš„ Proxy Host**\n\nDetails é…ç½®å¦‚ä¸‹ï¼š\n\n| é¡¹ç›®                    | è®¾ç½®                      |\n| --------------------- | ----------------------- |\n| Domain Names          | relay.example.com       |\n| Scheme                | http                    |\n| Forward Hostname / IP | 192.168.0.1 (docker æœºå™¨ IP) |\n| Forward Port          | 3000                    |\n| Block Common Exploits | â˜‘ï¸                      |\n| Websockets Support    | âŒ **å…³é—­**                |\n| Cache Assets          | âŒ **å…³é—­**                |\n| Access List           | Publicly Accessible     |\n\n> æ³¨æ„ï¼š\n> - è¯·ç¡®ä¿ Claude Relay Service **ç›‘å¬ host ä¸º `0.0.0.0` ã€å®¹å™¨ IP æˆ–æœ¬æœº IP**ï¼Œä»¥ä¾¿ NPM å®ç°å†…ç½‘è¿æ¥ã€‚\n> - **Websockets Support å’Œ Cache Assets å¿…é¡»å…³é—­**ï¼Œå¦åˆ™ä¼šå¯¼è‡´ SSE / æµå¼å“åº”å¤±è´¥ã€‚\n\n**2. Custom locations**\n\nç„¡éœ€æ·»åŠ ä»»ä½•å†…å®¹ï¼Œä¿æŒä¸ºç©ºã€‚\n\n**3. SSL è®¾ç½®**\n\n* **SSL Certificate**: Request a new SSL Certificate (Let's Encrypt) æˆ–å·²æœ‰è¯ä¹¦\n* â˜‘ï¸ **Force SSL**\n* â˜‘ï¸ **HTTP/2 Support**\n* â˜‘ï¸ **HSTS Enabled**\n* â˜‘ï¸ **HSTS Subdomains**\n\n**4. Advanced é…ç½®**\n\nCustom Nginx Configuration ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š\n\n```nginx\n# ä¼ é€’çœŸå®ç”¨æˆ· IP\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Proto $scheme;\n\n# æ”¯æŒ WebSocket / SSE ç­‰æµå¼é€šä¿¡\nproxy_http_version 1.1;\nproxy_set_header Upgrade $http_upgrade;\nproxy_set_header Connection \"upgrade\";\nproxy_buffering off;\n\n# é•¿è¿æ¥ / è¶…æ—¶è®¾ç½®ï¼ˆé€‚åˆ AI èŠå¤©æµå¼ä¼ è¾“ï¼‰\nproxy_read_timeout 300s;\nproxy_send_timeout 300s;\nproxy_connect_timeout 30s;\n\n# ---- å®‰å…¨æ€§è®¾ç½® ----\n# ä¸¥æ ¼ HTTPS ç­–ç•¥ (HSTS)\nadd_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n\n# é˜»æŒ¡ç‚¹å‡»åŠ«æŒä¸å†…å®¹å—…æ¢\nadd_header X-Frame-Options \"DENY\" always;\nadd_header X-Content-Type-Options \"nosniff\" always;\n\n# Referrer / Permissions é™åˆ¶ç­–ç•¥\nadd_header Referrer-Policy \"no-referrer-when-downgrade\" always;\nadd_header Permissions-Policy \"camera=(), microphone=(), geolocation=()\" always;\n\n# éšè—æœåŠ¡å™¨ä¿¡æ¯ï¼ˆç­‰æ•ˆäº Caddy çš„ `-Server`ï¼‰\nproxy_hide_header Server;\n\n# ---- æ€§èƒ½å¾®è°ƒ ----\n# å…³é—­ä»£ç†ç«¯ç¼“å­˜ï¼Œç¡®ä¿å³æ—¶å“åº”ï¼ˆSSE / Streamingï¼‰\nproxy_cache_bypass $http_upgrade;\nproxy_no_cache $http_upgrade;\nproxy_request_buffering off;\n```\n\n**4. å¯åŠ¨å’ŒéªŒè¯**\n\n* ä¿å­˜åç­‰å¾… NPM è‡ªåŠ¨ç”³è¯· Let's Encrypt è¯ä¹¦ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚\n* Dashboard ä¸­æŸ¥çœ‹ Proxy Host çŠ¶æ€ï¼Œç¡®ä¿æ˜¾ç¤ºä¸º \"Online\"ã€‚\n* è®¿é—® `https://relay.example.com`ï¼Œå¦‚æœæ˜¾ç¤ºç»¿è‰²é”å›¾æ ‡å³è¡¨ç¤º HTTPS æ­£å¸¸ã€‚\n\n**NPM ç‰¹ç‚¹**\n\n* ğŸ”’ è‡ªåŠ¨ç”³è¯·å’Œç»­æœŸè¯ä¹¦\n* ğŸ”§ å›¾å½¢åŒ–ç•Œé¢ï¼Œæ–¹ä¾¿ç®¡ç†å¤šæœåŠ¡\n* âš¡ åŸç”Ÿæ”¯æŒ HTTP/2 / HTTPS\n* ğŸš€ é€‚åˆ Docker å®¹å™¨éƒ¨ç½²\n\n---\n\nä¸Šè¿°ä¸¤ç§æ–¹æ¡ˆå‡å¯ç”¨äºç”Ÿäº§éƒ¨ç½²ã€‚\n\n---\n\n## ğŸ’¡ ä½¿ç”¨å»ºè®®\n\n### è´¦æˆ·ç®¡ç†\n\n- **å®šæœŸæ£€æŸ¥**: æ¯å‘¨çœ‹çœ‹è´¦æˆ·çŠ¶æ€ï¼ŒåŠæ—¶å¤„ç†å¼‚å¸¸\n- **åˆç†åˆ†é…**: å¯ä»¥ç»™ä¸åŒçš„äººåˆ†é…ä¸åŒçš„apikeyï¼Œå¯ä»¥æ ¹æ®ä¸åŒçš„apikeyæ¥åˆ†æç”¨é‡\n\n### å®‰å…¨å»ºè®®\n\n- **ä½¿ç”¨HTTPS**: å¼ºçƒˆå»ºè®®ä½¿ç”¨Caddyåå‘ä»£ç†ï¼ˆè‡ªåŠ¨HTTPSï¼‰ï¼Œç¡®ä¿æ•°æ®ä¼ è¾“å®‰å…¨\n- **å®šæœŸå¤‡ä»½**: é‡è¦é…ç½®å’Œæ•°æ®è¦å¤‡ä»½\n- **ç›‘æ§æ—¥å¿—**: å®šæœŸæŸ¥çœ‹å¼‚å¸¸æ—¥å¿—\n- **æ›´æ–°å¯†é’¥**: å®šæœŸæ›´æ¢JWTå’ŒåŠ å¯†å¯†é’¥\n- **é˜²ç«å¢™è®¾ç½®**: åªå¼€æ”¾å¿…è¦çš„ç«¯å£ï¼ˆ80, 443ï¼‰ï¼Œéšè—ç›´æ¥æœåŠ¡ç«¯å£\n\n---\n\n## ğŸ†˜ é‡åˆ°é—®é¢˜æ€ä¹ˆåŠï¼Ÿ\n\n### è‡ªåŠ©æ’æŸ¥\n\n1. **æŸ¥çœ‹æ—¥å¿—**: `logs/` ç›®å½•ä¸‹çš„æ—¥å¿—æ–‡ä»¶\n2. **æ£€æŸ¥é…ç½®**: ç¡®è®¤é…ç½®æ–‡ä»¶è®¾ç½®æ­£ç¡®\n3. **æµ‹è¯•è¿é€šæ€§**: ç”¨ curl æµ‹è¯•APIæ˜¯å¦æ­£å¸¸\n4. **é‡å¯æœåŠ¡**: æœ‰æ—¶å€™é‡å¯ä¸€ä¸‹å°±å¥½äº†\n\n### å¯»æ±‚å¸®åŠ©\n\n- **GitHub Issues**: æäº¤è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯\n- **æŸ¥çœ‹æ–‡æ¡£**: ä»”ç»†é˜…è¯»é”™è¯¯ä¿¡æ¯å’Œæ–‡æ¡£\n- **ç¤¾åŒºè®¨è®º**: çœ‹çœ‹å…¶ä»–äººæ˜¯å¦é‡åˆ°ç±»ä¼¼é—®é¢˜\n\n---\n\n## â¤ï¸ èµåŠ©æ”¯æŒ\n\nå¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘èµåŠ©æ”¯æŒé¡¹ç›®çš„æŒç»­å¼€å‘ã€‚æ‚¨çš„æ”¯æŒæ˜¯æˆ‘ä»¬æœ€å¤§çš„åŠ¨åŠ›ï¼\n\n<div align=\"center\">\n\n<a href=\"https://afdian.com/a/claude-relay-service\" target=\"_blank\">\n  <img src=\"https://img.shields.io/badge/è¯·æˆ‘å–æ¯å’–å•¡-çˆ±å‘ç”µ-946ce6?style=for-the-badge&logo=buy-me-a-coffee&logoColor=white\" alt=\"Sponsor\">\n</a>\n\n<table>\n  <tr>\n    <td><img src=\"docs/sponsoring/wechat.jpg\" width=\"200\" alt=\"wechat\" /></td>\n    <td><img src=\"docs/sponsoring/alipay.jpg\" width=\"200\" alt=\"alipay\" /></td>\n  </tr>\n</table>\n\n</div>\n\n---\n\n## ğŸ“„ è®¸å¯è¯\n\næœ¬é¡¹ç›®é‡‡ç”¨ [MITè®¸å¯è¯](LICENSE)ã€‚\n\n---\n\n<div align=\"center\">\n\n**â­ è§‰å¾—æœ‰ç”¨çš„è¯ç»™ä¸ªStarå‘—ï¼Œè¿™æ˜¯å¯¹ä½œè€…æœ€å¤§çš„é¼“åŠ±ï¼**\n\n**ğŸ¤ æœ‰é—®é¢˜æ¬¢è¿æIssueï¼Œæœ‰æ”¹è¿›å»ºè®®æ¬¢è¿PR**\n\n</div>\n",
      "stars_today": 63
    },
    {
      "id": 476507964,
      "name": "artcraft",
      "full_name": "storytold/artcraft",
      "description": "ArtCraft is an intentional crafting engine for artists, designers, and filmmakers",
      "html_url": "https://github.com/storytold/artcraft",
      "stars": 436,
      "forks": 26,
      "language": "Rust",
      "topics": [
        "3d-graphics",
        "ai",
        "aivideo",
        "filmmaking",
        "imagegeneration"
      ],
      "created_at": "2022-03-31T23:19:12Z",
      "updated_at": "2026-01-28T01:55:45Z",
      "pushed_at": "2026-01-27T16:35:20Z",
      "open_issues": 53,
      "owner": {
        "login": "storytold",
        "avatar_url": "https://avatars.githubusercontent.com/u/76897702?v=4"
      },
      "readme": "<p align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/b4e24c27-d87d-4fd1-8599-dc0d0b8af48d\" width=\"100%\" autoplay=\"true\" loop controls>\n</p>\n\n<p align=\"center\">The IDE for artists.</p>\n<p align=\"center\">\n  <a href=\"https://discord.gg/artcraft\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1359579021108842617?style=for-the-badge&label=discord&color=ffffff&logo=discord&logoColor=ffffff\" /></a>\n  <a href=\"https://www.youtube.com/@OfficialArtCraftStudios\"><img alt=\"YouTube\" src=\"https://img.shields.io/youtube/channel/subscribers/UCdjY4VG0ntoGwFsKZO4sVWA?style=for-the-badge&logo=YouTube\" /></a>\n  <a href=\"https://x.com/intent/follow?screen_name=get_artcraft\"><img alt=\"X\" src=\"https://img.shields.io/twitter/follow/get_artcraft?style=for-the-badge&label=follow&logo=x&logoColor=ffffff&color=ffffff\" /></a>\n  <a href=\"https://www.linkedin.com/company/artcraft-ai\"><img alt=\"LinkedIn\" src=\"https://img.shields.io/badge/linkedin--0A66C2?style=for-the-badge\"></a>\n</p>\n\n---\n\nArtCraft\n========\nArtCraft is the IDE for interactive AI image and video creation.\nWe turn prompting into *crafting*, so your ideas become a form of tangible expression and computing.\nThis is Adobe Photoshop for everyone, and we're giving away the source code!\n\n## Show, Don't Tell: Advanced Crafting Features\n\nText-to-image is great, but artists *need control*. It's important to know what your image will look like before you generate it, and it's vitally important to achieve consistency and repeatability.\n\n| Feature                           | Demo + Explanation                                                                                                                                                                                                                                                      \n|-----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Image to Location**             | ![Video](https://github.com/user-attachments/assets/21f103e3-cc19-4882-a630-9caa1b76ae31) Placing virtual actors into physical environments establishes single-location consistency. You can film multiple shots within a room without having things disappear.         |\n| **3D Image Compositing**          | ![Video](https://github.com/user-attachments/assets/f93a616f-571d-474e-bcc0-53736de7303d) Use images (backdrops, foreground elements, props, etc.) in scenes with depth and blend them naturally together. Just a couple of images usually leads to great compositions. |\n| **2D Image Compositing**          | ![Video](https://github.com/user-attachments/assets/d6f99391-e496-4c62-9e37-29734ba5f899) Use images, background removal, layers, and simple drawing tools to precisely compose a scene.                                                                                |\n| **Image to 3D Mesh**              | ![Video](https://github.com/user-attachments/assets/600a405c-e360-48c1-9b42-6e657ae6243b) It's almost impossible to lay out complicated objects or block complicated scenes; turning images into 3D helps position elements exactingly and intentionally.               |\n| **Character Posing**              | ![Video](https://github.com/user-attachments/assets/52a8e983-7c8f-42d2-be8b-25296ab9ed57) You can dynamically pose your characters to achieve the precise character, scene, and camera blocking before calling \"action\".                                                |\n| **Scene Blocking w/ Kit Bashing** | ![Video](https://github.com/user-attachments/assets/eef025ac-0346-4a46-a023-d48e23629eb5) Use 3D asset kits to precisely block out your scene: get the correct angles, object positions, and rich depth layering you can't with text prompting.                         |\n| **Character Identity Transfer**   | ![Video](https://github.com/user-attachments/assets/629119ee-8c76-4a83-9827-8c6c995a3ec1) Use mannequins as simple 3D ControlNets for posing any character.                                                                                                             |\n| **Background Removal**            | ![Video](https://github.com/user-attachments/assets/90c65057-5531-404f-83af-b34e66e24ec1) Remove backgrounds from images to make them useful in 2D or 3D compositing. They can be props, layers, or backdrops.                                                          |\n| **Mixed Asset Crafting**          | ![Video](https://raw.githubusercontent.com/storytold/github-media/main/ship-editing.gif) You can use image cutouts, worlds, and simple 3D meshes all together to precisely and intentionally lay out your scenes.                                                       |\n| **Scene Blocking**                | (preview coming soon)                                                                                                                                                                                                                                                   |\n| **Canvas Editing**                | (preview coming soon)                                                                                                                                                                                                                                                   |\n| **Scene Relighting**              | (preview coming soon)                                                                                                                                                                                                                                                   |\n\nNote: all of the above videos were generated for free with Grok Video; the cost to build this README was negligible.\n\n## Quick and Easy Prompting\nWe haven't abandoned text-to-asset generation for quick prototyping and ideation. We support every popular workflow in a first class fashion.\n\n| Feature               | Demo + Explanation                                                                                                                                    \n|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Text to Image**     | ![Video](https://github.com/user-attachments/assets/9cc289cd-faf4-4eaf-aed2-21134cce127c) Text prompt over a dozen different image models.            |\n| **Image Editing**     | ![Video](https://github.com/user-attachments/assets/a06fa6ad-936c-42d0-8767-48fdbb8ff141) Edit with Nano Banana Pro and GPT Image 1.5.                |\n| **Image Editing**     | ![Video](https://github.com/user-attachments/assets/f036e08a-f3a6-417a-98ee-ec7f04b2b5ff) Use inpainting, drawing, masking, etc. to edit images.      |\n| **Image to Video**    | ![Video](https://github.com/user-attachments/assets/2bc6c592-511e-4fba-b40f-03c96699b7f7) Image to video with lots of different options and controls. |\n| **Image Inpainting**  | (preview coming soon)                                                                                                                                 |\n| **Image Ingredients** | (preview coming soon)                                                                                                                                 |\n\n## Models and Providers Supported within Artcraft\n\n| Provider   | Features                                                                                                                                                                |\n|------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Artcraft   | Nano Banana, Nano Banana Pro, GPT-Image-1 / 1.5, Seedream 4 / 4.5, Flux 1.1 / Kontext, Veo 2 / 3 / 3.1, Kling 1.6 / 2.1 / 2.5 / 2.6, Seedance, Sora 2 / Pro, Hunyuan 3d 2 / 3 |\n| Grok       | Grok Imagine, Grok Video                                                                                                                                                |\n| Midjourney | Image Gen (all versions)                                                                                                                                                |\n| Sora       | Sora 1, Sora 2, GPT-Image-1                                                                                                                                             |\n| WorldLabs  | Marble (Gaussian Splat World Generation)                                                                                                                                |\n\nWe're going to be adding the following providers soon: Kling (via Kling website accounts), Google (via API keys), \nRunway (via website account), Luma (via website account).\n\nWe're potentially interested in adding other aggregators for those who already have subscriptions and credits at \nthose providers, for example: OpenArt, FreePik, etc.\n\n## Downloads\n\n- [Visit our website for the stable Windows and MacOS releases](https://getartcraft.com/)\n- Or you can grab a [more recent Windows and MacOS build directly](https://github.com/storytold/artcraft/releases)\n- Linux requires building from source for now\n\n## Documentation\n\n- [developer documentation](./_docs)\n- [tools, scripts, misc](./script)\n- [license](./LICENSE.md)\n- [roadmap](./ROADMAP.md)\n\n",
      "stars_today": 62
    },
    {
      "id": 1008713177,
      "name": "qwen-code",
      "full_name": "QwenLM/qwen-code",
      "description": "An open-source AI agent that lives in your terminal.",
      "html_url": "https://github.com/QwenLM/qwen-code",
      "stars": 17827,
      "forks": 1557,
      "language": "TypeScript",
      "topics": [],
      "created_at": "2025-06-26T01:37:46Z",
      "updated_at": "2026-01-28T02:09:58Z",
      "pushed_at": "2026-01-28T01:42:22Z",
      "open_issues": 474,
      "owner": {
        "login": "QwenLM",
        "avatar_url": "https://avatars.githubusercontent.com/u/141221163?v=4"
      },
      "readme": "<div align=\"center\">\n\n[![npm version](https://img.shields.io/npm/v/@qwen-code/qwen-code.svg)](https://www.npmjs.com/package/@qwen-code/qwen-code)\n[![License](https://img.shields.io/github/license/QwenLM/qwen-code.svg)](./LICENSE)\n[![Node.js Version](https://img.shields.io/badge/node-%3E%3D20.0.0-brightgreen.svg)](https://nodejs.org/)\n[![Downloads](https://img.shields.io/npm/dm/@qwen-code/qwen-code.svg)](https://www.npmjs.com/package/@qwen-code/qwen-code)\n\n<a href=\"https://trendshift.io/repositories/15287\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/15287\" alt=\"QwenLM%2Fqwen-code | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n**An open-source AI agent that lives in your terminal.**\n\n<a href=\"https://qwenlm.github.io/qwen-code-docs/zh/users/overview\">ä¸­æ–‡</a> |\n<a href=\"https://qwenlm.github.io/qwen-code-docs/de/users/overview\">Deutsch</a> |\n<a href=\"https://qwenlm.github.io/qwen-code-docs/fr/users/overview\">franÃ§ais</a> |\n<a href=\"https://qwenlm.github.io/qwen-code-docs/ja/users/overview\">æ—¥æœ¬èª</a> |\n<a href=\"https://qwenlm.github.io/qwen-code-docs/ru/users/overview\">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a> |\n<a href=\"https://qwenlm.github.io/qwen-code-docs/pt-BR/users/overview\">PortuguÃªs (Brasil)</a>\n\n</div>\n\nQwen Code is an open-source AI agent for the terminal, optimized for [Qwen3-Coder](https://github.com/QwenLM/Qwen3-Coder). It helps you understand large codebases, automate tedious work, and ship faster.\n\n![](https://gw.alicdn.com/imgextra/i1/O1CN01D2DviS1wwtEtMwIzJ_!!6000000006373-2-tps-1600-900.png)\n\n## Why Qwen Code?\n\n- **OpenAI-compatible, OAuth free tier**: use an OpenAI-compatible API, or sign in with Qwen OAuth to get 2,000 free requests/day.\n- **Open-source, co-evolving**: both the framework and the Qwen3-Coder model are open-sourceâ€”and they ship and evolve together.\n- **Agentic workflow, feature-rich**: rich built-in tools (Skills, SubAgents, Plan Mode) for a full agentic workflow and a Claude Code-like experience.\n- **Terminal-first, IDE-friendly**: built for developers who live in the command line, with optional integration for VS Code, Zed, and JetBrains IDEs.\n\n## Installation\n\n#### Prerequisites\n\n```bash\n# Node.js 20+\ncurl -qL https://www.npmjs.com/install.sh | sh\n```\n\n#### NPM (recommended)\n\n```bash\nnpm install -g @qwen-code/qwen-code@latest\n```\n\n#### Homebrew (macOS, Linux)\n\n```bash\nbrew install qwen-code\n```\n\n## Quick Start\n\n```bash\n# Start Qwen Code (interactive)\nqwen\n\n# Then, in the session:\n/help\n/auth\n```\n\nOn first use, you'll be prompted to sign in. You can run `/auth` anytime to switch authentication methods.\n\nExample prompts:\n\n```text\nWhat does this project do?\nExplain the codebase structure.\nHelp me refactor this function.\nGenerate unit tests for this module.\n```\n\n<details>\n<summary>Click to watch a demo video</summary>\n\n<video src=\"https://cloud.video.taobao.com/vod/HLfyppnCHplRV9Qhz2xSqeazHeRzYtG-EYJnHAqtzkQ.mp4\" controls>\nYour browser does not support the video tag.\n</video>\n\n</details>\n\n## Authentication\n\nQwen Code supports two authentication methods:\n\n- **Qwen OAuth (recommended & free)**: sign in with your `qwen.ai` account in a browser.\n- **OpenAI-compatible API**: use `OPENAI_API_KEY` (and optionally a custom base URL / model).\n\n#### Qwen OAuth (recommended)\n\nStart `qwen`, then run:\n\n```bash\n/auth\n```\n\nChoose **Qwen OAuth** and complete the browser flow. Your credentials are cached locally so you usually won't need to log in again.\n\n#### OpenAI-compatible API (API key)\n\nEnvironment variables (recommended for CI / headless environments):\n\n```bash\nexport OPENAI_API_KEY=\"your-api-key-here\"\nexport OPENAI_BASE_URL=\"https://api.openai.com/v1\"  # optional\nexport OPENAI_MODEL=\"gpt-4o\"                        # optional\n```\n\nFor details (including `.qwen/.env` loading and security notes), see the [authentication guide](https://qwenlm.github.io/qwen-code-docs/en/users/configuration/auth/).\n\n## Usage\n\nAs an open-source terminal agent, you can use Qwen Code in four primary ways:\n\n1. Interactive mode (terminal UI)\n2. Headless mode (scripts, CI)\n3. IDE integration (VS Code, Zed)\n4. TypeScript SDK\n\n#### Interactive mode\n\n```bash\ncd your-project/\nqwen\n```\n\nRun `qwen` in your project folder to launch the interactive terminal UI. Use `@` to reference local files (for example `@src/main.ts`).\n\n#### Headless mode\n\n```bash\ncd your-project/\nqwen -p \"your question\"\n```\n\nUse `-p` to run Qwen Code without the interactive UIâ€”ideal for scripts, automation, and CI/CD. Learn more: [Headless mode](https://qwenlm.github.io/qwen-code-docs/en/users/features/headless).\n\n#### IDE integration\n\nUse Qwen Code inside your editor (VS Code, Zed, and JetBrains IDEs):\n\n- [Use in VS Code](https://qwenlm.github.io/qwen-code-docs/en/users/integration-vscode/)\n- [Use in Zed](https://qwenlm.github.io/qwen-code-docs/en/users/integration-zed/)\n- [Use in JetBrains IDEs](https://qwenlm.github.io/qwen-code-docs/en/users/integration-jetbrains/)\n\n#### TypeScript SDK\n\nBuild on top of Qwen Code with the TypeScript SDK:\n\n- [Use the Qwen Code SDK](./packages/sdk-typescript/README.md)\n\n## Commands & Shortcuts\n\n### Session Commands\n\n- `/help` - Display available commands\n- `/clear` - Clear conversation history\n- `/compress` - Compress history to save tokens\n- `/stats` - Show current session information\n- `/bug` - Submit a bug report\n- `/exit` or `/quit` - Exit Qwen Code\n\n### Keyboard Shortcuts\n\n- `Ctrl+C` - Cancel current operation\n- `Ctrl+D` - Exit (on empty line)\n- `Up/Down` - Navigate command history\n\n> Learn more about [Commands](https://qwenlm.github.io/qwen-code-docs/en/users/features/commands/)\n>\n> **Tip**: In YOLO mode (`--yolo`), vision switching happens automatically without prompts when images are detected. Learn more about [Approval Mode](https://qwenlm.github.io/qwen-code-docs/en/users/features/approval-mode/)\n\n## Configuration\n\nQwen Code can be configured via `settings.json`, environment variables, and CLI flags.\n\n- **User settings**: `~/.qwen/settings.json`\n- **Project settings**: `.qwen/settings.json`\n\nSee [settings](https://qwenlm.github.io/qwen-code-docs/en/users/configuration/settings/) for available options and precedence.\n\n## Benchmark Results\n\n### Terminal-Bench Performance\n\n| Agent     | Model              | Accuracy |\n| --------- | ------------------ | -------- |\n| Qwen Code | Qwen3-Coder-480A35 | 37.5%    |\n| Qwen Code | Qwen3-Coder-30BA3B | 31.3%    |\n\n## Ecosystem\n\nLooking for a graphical interface?\n\n- [**AionUi**](https://github.com/iOfficeAI/AionUi) A modern GUI for command-line AI tools including Qwen Code\n- [**Gemini CLI Desktop**](https://github.com/Piebald-AI/gemini-cli-desktop) A cross-platform desktop/web/mobile UI for Qwen Code\n\n## Troubleshooting\n\nIf you encounter issues, check the [troubleshooting guide](https://qwenlm.github.io/qwen-code-docs/en/users/support/troubleshooting/).\n\nTo report a bug from within the CLI, run `/bug` and include a short title and repro steps.\n\n## Connect with Us\n\n- Discord: https://discord.gg/ycKBjdNd\n- Dingtalk: https://qr.dingtalk.com/action/joingroup?code=v1,k1,+FX6Gf/ZDlTahTIRi8AEQhIaBlqykA0j+eBKKdhLeAE=&_dt_no_comment=1&origin=1\n\n## Acknowledgments\n\nThis project is based on [Google Gemini CLI](https://github.com/google-gemini/gemini-cli). We acknowledge and appreciate the excellent work of the Gemini CLI team. Our main contribution focuses on parser-level adaptations to better support Qwen-Coder models.\n",
      "stars_today": 60
    },
    {
      "id": 988488798,
      "name": "Peekaboo",
      "full_name": "steipete/Peekaboo",
      "description": "Peekaboo is a macOS CLI & optional MCP server that enables AI agents to capture screenshots of applications, or the entire system, with optional visual question answering through local or remote AI models.",
      "html_url": "https://github.com/steipete/Peekaboo",
      "stars": 1345,
      "forks": 74,
      "language": "Swift",
      "topics": [
        "ai",
        "macos",
        "mcp",
        "screenshots",
        "swift"
      ],
      "created_at": "2025-05-22T16:09:06Z",
      "updated_at": "2026-01-28T00:37:55Z",
      "pushed_at": "2026-01-18T14:08:51Z",
      "open_issues": 9,
      "owner": {
        "login": "steipete",
        "avatar_url": "https://avatars.githubusercontent.com/u/58493?v=4"
      },
      "readme": "# Peekaboo ğŸ«£ - Mac automation that sees the screen and does the clicks.\n\n![Peekaboo Banner](assets/peekaboo.png)\n\n[![npm package](https://img.shields.io/badge/npm_package-3.0.0--beta1-brightgreen?logo=npm&logoColor=white&style=flat-square)](https://www.npmjs.com/package/@steipete/peekaboo)\n[![License: MIT](https://img.shields.io/badge/License-MIT-ffd60a?style=flat-square)](https://opensource.org/licenses/MIT)\n[![macOS 15.0+ (Sequoia)](https://img.shields.io/badge/macOS-15.0%2B_(Sequoia)-0078d7?logo=apple&logoColor=white&style=flat-square)](https://www.apple.com/macos/)\n[![Swift 6.2](https://img.shields.io/badge/Swift-6.2-F05138?logo=swift&logoColor=white&style=flat-square)](https://swift.org/)\n[![node >=22](https://img.shields.io/badge/node-%3E%3D22.0.0-2ea44f?logo=node.js&logoColor=white&style=flat-square)](https://nodejs.org/)\n[![Download macOS](https://img.shields.io/badge/Download-macOS-000000?logo=apple&logoColor=white&style=flat-square)](https://github.com/steipete/peekaboo/releases/latest)\n[![Homebrew](https://img.shields.io/badge/Homebrew-steipete%2Ftap-b28f62?logo=homebrew&logoColor=white&style=flat-square)](https://github.com/steipete/homebrew-tap)\n[![Ask DeepWiki](https://img.shields.io/badge/Ask-DeepWiki-0088cc?style=flat-square)](https://deepwiki.com/steipete/peekaboo)\n\nPeekaboo brings high-fidelity screen capture, AI analysis, and complete GUI automation to macOS. Version 3 adds native agent flows and multi-screen automation across the CLI and MCP server.\n> Note: v3 is currently in beta (3.0.0-beta4) and has a few known issues; see the changelog for details.\n\n## What you get\n- Pixel-accurate captures (windows, screens, menu bar) with optional Retina 2x scaling.\n- Natural-language agent that chains Peekaboo tools (see, click, type, scroll, hotkey, menu, window, app, dock, space).\n- Menu and menubar discovery with structured JSON; no clicks required.\n- Multi-provider AI: GPT-5.1 family, Claude 4.x, Grok 4-fast (vision), Gemini 2.5, and local Ollama models.\n- MCP server for Claude Desktop and Cursor plus a native CLI; the same tools in both.\n- Configurable, testable workflows with reproducible sessions and strict typing.\n- Requires macOS Screen Recording + Accessibility permissions (see [docs/permissions.md](docs/permissions.md)).\n\n## Install\n- macOS app + CLI (Homebrew):\n  ```bash\n  brew install steipete/tap/peekaboo\n  ```\n- MCP server (Node 22+, no global install needed):\n  ```bash\n  npx -y @steipete/peekaboo\n  ```\n\n## Quick start\n```bash\n# Capture full screen at Retina scale and save to Desktop\npeekaboo image --mode screen --retina --path ~/Desktop/screen.png\n\n# Click a button by label (captures, resolves, and clicks in one go)\npeekaboo see --app Safari --json-output | jq -r '.data.snapshot_id' | read SNAPSHOT\npeekaboo click --on \"Reload this page\" --snapshot \"$SNAPSHOT\"\n\n# Run a natural-language automation\npeekaboo \"Open Notes and create a TODO list with three items\"\n\n# Run as an MCP server (Claude/Cursor)\nnpx -y @steipete/peekaboo\n\n# Minimal Claude Desktop config snippet (Developer â†’ Edit Config):\n# {\n#   \"mcpServers\": {\n#     \"peekaboo\": {\n#       \"command\": \"npx\",\n#       \"args\": [\"-y\", \"@steipete/peekaboo\"],\n#       \"env\": {\n#         \"PEEKABOO_AI_PROVIDERS\": \"openai/gpt-5.1,anthropic/claude-opus-4\"\n#       }\n#     }\n#   }\n# }\n```\n\n| Command | Key flags / subcommands | What it does |\n| --- | --- | --- |\n| [see](docs/commands/see.md) | `--app`, `--mode screen/window`, `--retina`, `--json-output` | Capture and annotate UI, return snapshot + element IDs |\n| [click](docs/commands/click.md) | `--on <id/query>`, `--snapshot`, `--wait`, coords | Click by element ID, label, or coordinates |\n| [type](docs/commands/type.md) | `--text`, `--clear`, `--delay-ms` | Enter text with pacing options |\n| [press](docs/commands/press.md) | key names, `--repeat` | Special keys and sequences |\n| [hotkey](docs/commands/hotkey.md) | combos like `cmd,shift,t` | Modifier combos (cmd/ctrl/alt/shift) |\n| [scroll](docs/commands/scroll.md) | `--on <id>`, `--direction up/down`, `--ticks` | Scroll views or elements |\n| [swipe](docs/commands/swipe.md) | `--from/--to`, `--duration`, `--steps` | Smooth gesture-style drags |\n| [drag](docs/commands/drag.md) | `--from/--to`, modifiers, Dock/Trash targets | Drag-and-drop between elements/coords |\n| [move](docs/commands/move.md) | `--to <id/coords>`, `--screen-index` | Position the cursor without clicking |\n| [window](docs/commands/window.md) | `list`, `move`, `resize`, `focus`, `set-bounds` | Move/resize/focus windows and Spaces |\n| [app](docs/commands/app.md) | `launch`, `quit`, `relaunch`, `switch`, `list` | Launch, quit, relaunch, switch apps |\n| [space](docs/commands/space.md) | `list`, `switch`, `move-window` | List or switch macOS Spaces |\n| [menu](docs/commands/menu.md) | `list`, `list-all`, `click`, `click-extra` | List/click app menus and extras |\n| [menubar](docs/commands/menubar.md) | `list`, `click` | Target status-bar items by name/index |\n| [dock](docs/commands/dock.md) | `launch`, `right-click`, `hide`, `show`, `list` | Interact with Dock items |\n| [dialog](docs/commands/dialog.md) | `list`, `click`, `input`, `file`, `dismiss` | Drive system dialogs (open/save/etc.) |\n| [image](docs/commands/image.md) | `--mode screen/window/menu`, `--retina`, `--analyze` | Screenshot screen/window/menu bar (+analyze) |\n| [list](docs/commands/list.md) | `apps`, `windows`, `screens`, `menubar`, `permissions` | Enumerate apps, windows, screens, permissions |\n| [tools](docs/commands/tools.md) | `--verbose`, `--json-output`, `--no-sort` | Inspect native Peekaboo tools |\n| [config](docs/commands/config.md) | `init`, `show`, `add`, `login`, `models` | Manage credentials/providers/settings |\n| [permissions](docs/commands/permissions.md) | `status`, `grant` | Check/grant required macOS permissions |\n| [run](docs/commands/run.md) | `.peekaboo.json`, `--output`, `--no-fail-fast` | Execute `.peekaboo.json` automation scripts |\n| [sleep](docs/commands/sleep.md) | `--duration` (ms) | Millisecond delays between steps |\n| [clean](docs/commands/clean.md) | `--all-snapshots`, `--older-than`, `--snapshot` | Prune snapshots and caches |\n| [agent](docs/commands/agent.md) | `--model`, `--dry-run`, `--resume`, `--max-steps`, audio | Natural-language multi-step automation |\n| [mcp](docs/commands/mcp.md) | `serve` (default) | Run Peekaboo as an MCP server |\n\n## Models and providers\n- OpenAI: GPT-5.1 (default) and GPT-4.1/4o vision\n- Anthropic: Claude 4.x\n- xAI: Grok 4-fast reasoning + vision\n- Google: Gemini 2.5 (pro/flash)\n- Local: Ollama (llama3.3, llava, etc.)\n\nSet providers via `PEEKABOO_AI_PROVIDERS` or `peekaboo config add`.\n\n## Learn more\n- Command reference: [docs/commands/](docs/commands/)\n- Architecture: [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)\n- Building from source: [docs/building.md](docs/building.md)\n- Testing guide: [docs/testing/tools.md](docs/testing/tools.md)\n- MCP setup: [docs/commands/mcp.md](docs/commands/mcp.md)\n- Permissions: [docs/permissions.md](docs/permissions.md)\n- Ollama/local models: [docs/ollama.md](docs/ollama.md)\n- Agent chat loop: [docs/agent-chat.md](docs/agent-chat.md)\n- Service API reference: [docs/service-api-reference.md](docs/service-api-reference.md)\n\n## Development basics\n- Requirements: macOS 15+, Xcode 16+/Swift 6.2. Node 22+ only if you run the pnpm docs/build helper scripts (core CLI/app/MCP are Swift-only).\n- Install deps: `pnpm install` then `pnpm run build:cli` or `pnpm run test:safe`.\n- Lint/format: `pnpm run lint && pnpm run format`.\n\n## License\nMIT\n",
      "stars_today": 57
    },
    {
      "id": 1034478597,
      "name": "open-lovable",
      "full_name": "firecrawl/open-lovable",
      "description": "ğŸ”¥ Clone and recreate any website as a modern React app in seconds",
      "html_url": "https://github.com/firecrawl/open-lovable",
      "stars": 23749,
      "forks": 4632,
      "language": "TypeScript",
      "topics": [],
      "created_at": "2025-08-08T13:04:02Z",
      "updated_at": "2026-01-28T02:11:08Z",
      "pushed_at": "2025-11-19T15:29:48Z",
      "open_issues": 120,
      "owner": {
        "login": "firecrawl",
        "avatar_url": "https://avatars.githubusercontent.com/u/135057108?v=4"
      },
      "readme": "# Open Lovable\n\nChat with AI to build React apps instantly. An example app made by the [Firecrawl](https://firecrawl.dev/?ref=open-lovable-github) team. For a complete cloud solution, check out [Lovable.dev](https://lovable.dev/) â¤ï¸.\n\n<img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExbmZtaHFleGRsMTNlaWNydGdianI4NGQ4dHhyZjB0d2VkcjRyeXBucCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ZFVLWMa6dVskQX0qu1/giphy.gif\" alt=\"Open Lovable Demo\" width=\"100%\"/>\n\n## Setup\n\n1. **Clone & Install**\n```bash\ngit clone https://github.com/firecrawl/open-lovable.git\ncd open-lovable\npnpm install  # or npm install / yarn install\n```\n\n2. **Add `.env.local`**\n\n```env\n# =================================================================\n# REQUIRED\n# =================================================================\nFIRECRAWL_API_KEY=your_firecrawl_api_key    # https://firecrawl.dev\n\n# =================================================================\n# AI PROVIDER - Choose your LLM\n# =================================================================\nGEMINI_API_KEY=your_gemini_api_key        # https://aistudio.google.com/app/apikey\nANTHROPIC_API_KEY=your_anthropic_api_key  # https://console.anthropic.com\nOPENAI_API_KEY=your_openai_api_key        # https://platform.openai.com\nGROQ_API_KEY=your_groq_api_key            # https://console.groq.com\n\n# =================================================================\n# FAST APPLY (Optional - for faster edits)\n# =================================================================\nMORPH_API_KEY=your_morphllm_api_key    # https://morphllm.com/dashboard\n\n# =================================================================\n# SANDBOX PROVIDER - Choose ONE: Vercel (default) or E2B\n# =================================================================\nSANDBOX_PROVIDER=vercel  # or 'e2b'\n\n# Option 1: Vercel Sandbox (default)\n# Choose one authentication method:\n\n# Method A: OIDC Token (recommended for development)\n# Run `vercel link` then `vercel env pull` to get VERCEL_OIDC_TOKEN automatically\nVERCEL_OIDC_TOKEN=auto_generated_by_vercel_env_pull\n\n# Method B: Personal Access Token (for production or when OIDC unavailable)\n# VERCEL_TEAM_ID=team_xxxxxxxxx      # Your Vercel team ID \n# VERCEL_PROJECT_ID=prj_xxxxxxxxx    # Your Vercel project ID\n# VERCEL_TOKEN=vercel_xxxxxxxxxxxx   # Personal access token from Vercel dashboard\n\n# Option 2: E2B Sandbox\n# E2B_API_KEY=your_e2b_api_key      # https://e2b.dev\n```\n\n3. **Run**\n```bash\npnpm dev  # or npm run dev / yarn dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000)\n\n## License\n\nMIT",
      "stars_today": 51
    },
    {
      "id": 942771284,
      "name": "github-mcp-server",
      "full_name": "github/github-mcp-server",
      "description": "GitHub's official MCP Server",
      "html_url": "https://github.com/github/github-mcp-server",
      "stars": 26384,
      "forks": 3470,
      "language": "Go",
      "topics": [
        "github",
        "mcp",
        "mcp-server"
      ],
      "created_at": "2025-03-04T16:42:04Z",
      "updated_at": "2026-01-28T01:46:18Z",
      "pushed_at": "2026-01-28T00:06:56Z",
      "open_issues": 256,
      "owner": {
        "login": "github",
        "avatar_url": "https://avatars.githubusercontent.com/u/9919?v=4"
      },
      "readme": "[![Go Report Card](https://goreportcard.com/badge/github.com/github/github-mcp-server)](https://goreportcard.com/report/github.com/github/github-mcp-server)\n\n# GitHub MCP Server\n\nThe GitHub MCP Server connects AI tools directly to GitHub's platform. This gives AI agents, assistants, and chatbots the ability to read repositories and code files, manage issues and PRs, analyze code, and automate workflows. All through natural language interactions.\n\n### Use Cases\n\n- Repository Management: Browse and query code, search files, analyze commits, and understand project structure across any repository you have access to.\n- Issue & PR Automation: Create, update, and manage issues and pull requests. Let AI help triage bugs, review code changes, and maintain project boards.\n- CI/CD & Workflow Intelligence: Monitor GitHub Actions workflow runs, analyze build failures, manage releases, and get insights into your development pipeline.\n- Code Analysis: Examine security findings, review Dependabot alerts, understand code patterns, and get comprehensive insights into your codebase.\n- Team Collaboration: Access discussions, manage notifications, analyze team activity, and streamline processes for your team.\n\nBuilt for developers who want to connect their AI tools to GitHub context and capabilities, from simple natural language queries to complex multi-step agent workflows.\n\n---\n\n## Remote GitHub MCP Server\n\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github&config=%7B%22type%22%3A%20%22http%22%2C%22url%22%3A%20%22https%3A%2F%2Fapi.githubcopilot.com%2Fmcp%2F%22%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Server-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github&config=%7B%22type%22%3A%20%22http%22%2C%22url%22%3A%20%22https%3A%2F%2Fapi.githubcopilot.com%2Fmcp%2F%22%7D&quality=insiders)\n\nThe remote GitHub MCP Server is hosted by GitHub and provides the easiest method for getting up and running. If your MCP host does not support remote MCP servers, don't worry! You can use the [local version of the GitHub MCP Server](https://github.com/github/github-mcp-server?tab=readme-ov-file#local-github-mcp-server) instead.\n\n### Prerequisites\n\n1. A compatible MCP host with remote server support (VS Code 1.101+, Claude Desktop, Cursor, Windsurf, etc.)\n2. Any applicable [policies enabled](https://github.com/github/github-mcp-server/blob/main/docs/policies-and-governance.md)\n\n### Install in VS Code\n\nFor quick installation, use one of the one-click install buttons above. Once you complete that flow, toggle Agent mode (located by the Copilot Chat text input) and the server will start. Make sure you're using [VS Code 1.101](https://code.visualstudio.com/updates/v1_101) or [later](https://code.visualstudio.com/updates) for remote MCP and OAuth support.\n\nAlternatively, to manually configure VS Code, choose the appropriate JSON block from the examples below and add it to your host configuration:\n\n<table>\n<tr><th>Using OAuth</th><th>Using a GitHub PAT</th></tr>\n<tr><th align=left colspan=2>VS Code (version 1.101 or greater)</th></tr>\n<tr valign=top>\n<td>\n\n```json\n{\n  \"servers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\"\n    }\n  }\n}\n```\n\n</td>\n<td>\n\n```json\n{\n  \"servers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${input:github_mcp_pat}\"\n      }\n    }\n  },\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"github_mcp_pat\",\n      \"description\": \"GitHub Personal Access Token\",\n      \"password\": true\n    }\n  ]\n}\n```\n\n</td>\n</tr>\n</table>\n\n### Install in other MCP hosts\n\n- **[Copilot CLI](/docs/installation-guides/install-copilot-cli.md)** - Installation guide for GitHub Copilot CLI\n- **[GitHub Copilot in other IDEs](/docs/installation-guides/install-other-copilot-ides.md)** - Installation for JetBrains, Visual Studio, Eclipse, and Xcode with GitHub Copilot\n- **[Claude Applications](/docs/installation-guides/install-claude.md)** - Installation guide for Claude Desktop and Claude Code CLI\n- **[Codex](/docs/installation-guides/install-codex.md)** - Installation guide for OpenAI Codex\n- **[Cursor](/docs/installation-guides/install-cursor.md)** - Installation guide for Cursor IDE\n- **[Windsurf](/docs/installation-guides/install-windsurf.md)** - Installation guide for Windsurf IDE\n- **[Rovo Dev CLI](/docs/installation-guides/install-rovo-dev-cli.md)** - Installation guide for Rovo Dev CLI\n\n> **Note:** Each MCP host application needs to configure a GitHub App or OAuth App to support remote access via OAuth. Any host application that supports remote MCP servers should support the remote GitHub server with PAT authentication. Configuration details and support levels vary by host. Make sure to refer to the host application's documentation for more info.\n\n### Configuration\n\n#### Toolset configuration\n\nSee [Remote Server Documentation](docs/remote-server.md) for full details on remote server configuration, toolsets, headers, and advanced usage. This file provides comprehensive instructions and examples for connecting, customizing, and installing the remote GitHub MCP Server in VS Code and other MCP hosts.\n\nWhen no toolsets are specified, [default toolsets](#default-toolset) are used.\n\n#### Insiders Mode\n\n> **Try new features early!** The remote server offers an insiders version with early access to new features and experimental tools.\n\n<table>\n<tr><th>Using URL Path</th><th>Using Header</th></tr>\n<tr valign=top>\n<td>\n\n```json\n{\n  \"servers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/insiders\"\n    }\n  }\n}\n```\n\n</td>\n<td>\n\n```json\n{\n  \"servers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\",\n      \"headers\": {\n        \"X-MCP-Insiders\": \"true\"\n      }\n    }\n  }\n}\n```\n\n</td>\n</tr>\n</table>\n\nSee [Remote Server Documentation](docs/remote-server.md#insiders-mode) for more details and examples.\n\n#### GitHub Enterprise\n\n##### GitHub Enterprise Cloud with data residency (ghe.com)\n\nGitHub Enterprise Cloud can also make use of the remote server.\n\nExample for `https://octocorp.ghe.com` with GitHub PAT token:\n\n```\n{\n    ...\n    \"proxima-github\": {\n      \"type\": \"http\",\n      \"url\": \"https://copilot-api.octocorp.ghe.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${input:github_mcp_pat}\"\n      }\n    },\n    ...\n}\n```\n\n> **Note:** When using OAuth with GitHub Enterprise with VS Code and GitHub Copilot, you also need to configure your VS Code settings to point to your GitHub Enterprise instance - see [Authenticate from VS Code](https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/configure-personal-settings/authenticate-to-ghecom)\n\n##### GitHub Enterprise Server\n\nGitHub Enterprise Server does not support remote server hosting. Please refer to [GitHub Enterprise Server and Enterprise Cloud with data residency (ghe.com)](#github-enterprise-server-and-enterprise-cloud-with-data-residency-ghecom) from the local server configuration.\n\n---\n\n## Local GitHub MCP Server\n\n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github&inputs=%5B%7B%22id%22%3A%22github_token%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22GitHub%20Personal%20Access%20Token%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22GITHUB_PERSONAL_ACCESS_TOKEN%22%2C%22ghcr.io%2Fgithub%2Fgithub-mcp-server%22%5D%2C%22env%22%3A%7B%22GITHUB_PERSONAL_ACCESS_TOKEN%22%3A%22%24%7Binput%3Agithub_token%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Server-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=github&inputs=%5B%7B%22id%22%3A%22github_token%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22GitHub%20Personal%20Access%20Token%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22GITHUB_PERSONAL_ACCESS_TOKEN%22%2C%22ghcr.io%2Fgithub%2Fgithub-mcp-server%22%5D%2C%22env%22%3A%7B%22GITHUB_PERSONAL_ACCESS_TOKEN%22%3A%22%24%7Binput%3Agithub_token%7D%22%7D%7D&quality=insiders)\n\n### Prerequisites\n\n1. To run the server in a container, you will need to have [Docker](https://www.docker.com/) installed.\n2. Once Docker is installed, you will also need to ensure Docker is running. The Docker image is available at `ghcr.io/github/github-mcp-server`. The image is public; if you get errors on pull, you may have an expired token and need to `docker logout ghcr.io`.\n3. Lastly you will need to [Create a GitHub Personal Access Token](https://github.com/settings/personal-access-tokens/new).\nThe MCP server can use many of the GitHub APIs, so enable the permissions that you feel comfortable granting your AI tools (to learn more about access tokens, please check out the [documentation](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)).\n\n<details><summary><b>Handling PATs Securely</b></summary>\n\n### Environment Variables (Recommended)\n\nTo keep your GitHub PAT secure and reusable across different MCP hosts:\n\n1. **Store your PAT in environment variables**\n\n   ```bash\n   export GITHUB_PAT=your_token_here\n   ```\n\n   Or create a `.env` file:\n\n   ```env\n   GITHUB_PAT=your_token_here\n   ```\n\n2. **Protect your `.env` file**\n\n   ```bash\n   # Add to .gitignore to prevent accidental commits\n   echo \".env\" >> .gitignore\n   ```\n\n3. **Reference the token in configurations**\n\n   ```bash\n   # CLI usage\n   claude mcp update github -e GITHUB_PERSONAL_ACCESS_TOKEN=$GITHUB_PAT\n\n   # In config files (where supported)\n   \"env\": {\n     \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"$GITHUB_PAT\"\n   }\n   ```\n\n> **Note**: Environment variable support varies by host app and IDE. Some applications (like Windsurf) require hardcoded tokens in config files.\n\n### Token Security Best Practices\n\n- **Minimum scopes**: Only grant necessary permissions\n  - `repo` - Repository operations\n  - `read:packages` - Docker image access\n  - `read:org` - Organization team access\n- **Separate tokens**: Use different PATs for different projects/environments\n- **Regular rotation**: Update tokens periodically\n- **Never commit**: Keep tokens out of version control\n- **File permissions**: Restrict access to config files containing tokens\n\n  ```bash\n  chmod 600 ~/.your-app/config.json\n  ```\n\n</details>\n\n### GitHub Enterprise Server and Enterprise Cloud with data residency (ghe.com)\n\nThe flag `--gh-host` and the environment variable `GITHUB_HOST` can be used to set\nthe hostname for GitHub Enterprise Server or GitHub Enterprise Cloud with data residency.\n\n- For GitHub Enterprise Server, prefix the hostname with the `https://` URI scheme, as it otherwise defaults to `http://`, which GitHub Enterprise Server does not support.\n- For GitHub Enterprise Cloud with data residency, use `https://YOURSUBDOMAIN.ghe.com` as the hostname.\n\n``` json\n\"github\": {\n    \"command\": \"docker\",\n    \"args\": [\n    \"run\",\n    \"-i\",\n    \"--rm\",\n    \"-e\",\n    \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n    \"-e\",\n    \"GITHUB_HOST\",\n    \"ghcr.io/github/github-mcp-server\"\n    ],\n    \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\",\n        \"GITHUB_HOST\": \"https://<your GHES or ghe.com domain name>\"\n    }\n}\n```\n\n## Installation\n\n### Install in GitHub Copilot on VS Code\n\nFor quick installation, use one of the one-click install buttons above. Once you complete that flow, toggle Agent mode (located by the Copilot Chat text input) and the server will start.\n\nMore about using MCP server tools in VS Code's [agent mode documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\nInstall in GitHub Copilot on other IDEs (JetBrains, Visual Studio, Eclipse, etc.)\n\nAdd the following JSON block to your IDE's MCP settings.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"github_token\",\n        \"description\": \"GitHub Personal Access Token\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"github\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"-e\",\n          \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n          \"ghcr.io/github/github-mcp-server\"\n        ],\n        \"env\": {\n          \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add a similar example (i.e. without the mcp key) to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with other host applications that accept the same format.\n\n<details>\n<summary><b>Example JSON block without the MCP key included</b></summary>\n<br>\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"github_token\",\n      \"description\": \"GitHub Personal Access Token\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"github\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-e\",\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n        \"ghcr.io/github/github-mcp-server\"\n      ],\n      \"env\": {\n        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n### Install in Other MCP Hosts\n\nFor other MCP host applications, please refer to our installation guides:\n\n- **[Copilot CLI](docs/installation-guides/install-copilot-cli.md)** - Installation guide for GitHub Copilot CLI\n- **[GitHub Copilot in other IDEs](/docs/installation-guides/install-other-copilot-ides.md)** - Installation for JetBrains, Visual Studio, Eclipse, and Xcode with GitHub Copilot\n- **[Claude Code & Claude Desktop](docs/installation-guides/install-claude.md)** - Installation guide for Claude Code and Claude Desktop\n- **[Cursor](docs/installation-guides/install-cursor.md)** - Installation guide for Cursor IDE\n- **[Google Gemini CLI](docs/installation-guides/install-gemini-cli.md)** - Installation guide for Google Gemini CLI\n- **[Windsurf](docs/installation-guides/install-windsurf.md)** - Installation guide for Windsurf IDE\n\nFor a complete overview of all installation options, see our **[Installation Guides Index](docs/installation-guides)**.\n\n> **Note:** Any host application that supports local MCP servers should be able to access the local GitHub MCP server. However, the specific configuration process, syntax and stability of the integration will vary by host application. While many may follow a similar format to the examples above, this is not guaranteed. Please refer to your host application's documentation for the correct MCP configuration syntax and setup process.\n\n### Build from source\n\nIf you don't have Docker, you can use `go build` to build the binary in the\n`cmd/github-mcp-server` directory, and use the `github-mcp-server stdio` command with the `GITHUB_PERSONAL_ACCESS_TOKEN` environment variable set to your token. To specify the output location of the build, use the `-o` flag. You should configure your server to use the built executable as its `command`. For example:\n\n```JSON\n{\n  \"mcp\": {\n    \"servers\": {\n      \"github\": {\n        \"command\": \"/path/to/github-mcp-server\",\n        \"args\": [\"stdio\"],\n        \"env\": {\n          \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"<YOUR_TOKEN>\"\n        }\n      }\n    }\n  }\n}\n```\n\n### CLI utilities\n\nThe `github-mcp-server` binary includes a few CLI subcommands that are helpful for debugging and exploring the server.\n\n- `github-mcp-server tool-search \"<query>\"` searches tools by name, description, and input parameter names. Use `--max-results` to return more matches.\nExample (color output requires a TTY; use `docker run -t` (or `-it`) when running in Docker):\n```bash\ndocker run -it --rm ghcr.io/github/github-mcp-server tool-search \"issue\" --max-results 5\ngithub-mcp-server tool-search \"issue\" --max-results 5\n```\n\n## Tool Configuration\n\nThe GitHub MCP Server supports enabling or disabling specific groups of functionalities via the `--toolsets` flag. This allows you to control which GitHub API capabilities are available to your AI tools. Enabling only the toolsets that you need can help the LLM with tool choice and reduce the context size.\n\n_Toolsets are not limited to Tools. Relevant MCP Resources and Prompts are also included where applicable._\n\nWhen no toolsets are specified, [default toolsets](#default-toolset) are used.\n\n> **Looking for examples?** See the [Server Configuration Guide](./docs/server-configuration.md) for common recipes like minimal setups, read-only mode, and combining tools with toolsets.\n\n#### Specifying Toolsets\n\nTo specify toolsets you want available to the LLM, you can pass an allow-list in two ways:\n\n1. **Using Command Line Argument**:\n\n   ```bash\n   github-mcp-server --toolsets repos,issues,pull_requests,actions,code_security\n   ```\n\n2. **Using Environment Variable**:\n\n   ```bash\n   GITHUB_TOOLSETS=\"repos,issues,pull_requests,actions,code_security\" ./github-mcp-server\n   ```\n\nThe environment variable `GITHUB_TOOLSETS` takes precedence over the command line argument if both are provided.\n\n#### Specifying Individual Tools\n\nYou can also configure specific tools using the `--tools` flag. Tools can be used independently or combined with toolsets and dynamic toolsets discovery for fine-grained control.\n\n1. **Using Command Line Argument**:\n\n   ```bash\n   github-mcp-server --tools get_file_contents,issue_read,create_pull_request\n   ```\n\n2. **Using Environment Variable**:\n\n   ```bash\n   GITHUB_TOOLS=\"get_file_contents,issue_read,create_pull_request\" ./github-mcp-server\n   ```\n\n3. **Combining with Toolsets** (additive):\n\n   ```bash\n   github-mcp-server --toolsets repos,issues --tools get_gist\n   ```\n\n   This registers all tools from `repos` and `issues` toolsets, plus `get_gist`.\n\n4. **Combining with Dynamic Toolsets** (additive):\n\n   ```bash\n   github-mcp-server --tools get_file_contents --dynamic-toolsets\n   ```\n\n   This registers `get_file_contents` plus the dynamic toolset tools (`enable_toolset`, `list_available_toolsets`, `get_toolset_tools`).\n\n**Important Notes:**\n\n- Tools, toolsets, and dynamic toolsets can all be used together\n- Read-only mode takes priority: write tools are skipped if `--read-only` is set, even if explicitly requested via `--tools`\n- Tool names must match exactly (e.g., `get_file_contents`, not `getFileContents`). Invalid tool names will cause the server to fail at startup with an error message\n- When tools are renamed, old names are preserved as aliases for backward compatibility. See [Deprecated Tool Aliases](docs/deprecated-tool-aliases.md) for details.\n\n### Using Toolsets With Docker\n\nWhen using Docker, you can pass the toolsets as environment variables:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=<your-token> \\\n  -e GITHUB_TOOLSETS=\"repos,issues,pull_requests,actions,code_security\" \\\n  ghcr.io/github/github-mcp-server\n```\n\n### Using Tools With Docker\n\nWhen using Docker, you can pass specific tools as environment variables. You can also combine tools with toolsets:\n\n```bash\n# Tools only\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=<your-token> \\\n  -e GITHUB_TOOLS=\"get_file_contents,issue_read,create_pull_request\" \\\n  ghcr.io/github/github-mcp-server\n\n# Tools combined with toolsets (additive)\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=<your-token> \\\n  -e GITHUB_TOOLSETS=\"repos,issues\" \\\n  -e GITHUB_TOOLS=\"get_gist\" \\\n  ghcr.io/github/github-mcp-server\n```\n\n### Special toolsets\n\n#### \"all\" toolset\n\nThe special toolset `all` can be provided to enable all available toolsets regardless of any other configuration:\n\n```bash\n./github-mcp-server --toolsets all\n```\n\nOr using the environment variable:\n\n```bash\nGITHUB_TOOLSETS=\"all\" ./github-mcp-server\n```\n\n#### \"default\" toolset\n\nThe default toolset `default` is the configuration that gets passed to the server if no toolsets are specified.\n\nThe default configuration is:\n\n- context\n- repos\n- issues\n- pull_requests\n- users\n\nTo keep the default configuration and add additional toolsets:\n\n```bash\nGITHUB_TOOLSETS=\"default,stargazers\" ./github-mcp-server\n```\n\n### Insiders Mode\n\nThe local GitHub MCP Server offers an insiders version with early access to new features and experimental tools.\n\n1. **Using Command Line Argument**:\n\n   ```bash\n   ./github-mcp-server --insiders\n   ```\n\n2. **Using Environment Variable**:\n\n   ```bash\n   GITHUB_INSIDERS=true ./github-mcp-server\n   ```\n\nWhen using Docker:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=<your-token> \\\n  -e GITHUB_INSIDERS=true \\\n  ghcr.io/github/github-mcp-server\n```\n\n### Available Toolsets\n\nThe following sets of tools are available:\n\n<!-- START AUTOMATED TOOLSETS -->\n|     | Toolset                 | Description                                                   |\n| --- | ----------------------- | ------------------------------------------------------------- |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/person-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/person-light.png\"><img src=\"pkg/octicons/icons/person-light.png\" width=\"20\" height=\"20\" alt=\"person\"></picture> | `context`               | **Strongly recommended**: Tools that provide context about the current user and GitHub context you are operating in |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/workflow-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/workflow-light.png\"><img src=\"pkg/octicons/icons/workflow-light.png\" width=\"20\" height=\"20\" alt=\"workflow\"></picture> | `actions` | GitHub Actions workflows and CI/CD operations |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/codescan-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/codescan-light.png\"><img src=\"pkg/octicons/icons/codescan-light.png\" width=\"20\" height=\"20\" alt=\"codescan\"></picture> | `code_security` | Code security related tools, such as GitHub Code Scanning |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/dependabot-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/dependabot-light.png\"><img src=\"pkg/octicons/icons/dependabot-light.png\" width=\"20\" height=\"20\" alt=\"dependabot\"></picture> | `dependabot` | Dependabot tools |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/comment-discussion-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/comment-discussion-light.png\"><img src=\"pkg/octicons/icons/comment-discussion-light.png\" width=\"20\" height=\"20\" alt=\"comment-discussion\"></picture> | `discussions` | GitHub Discussions related tools |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/logo-gist-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/logo-gist-light.png\"><img src=\"pkg/octicons/icons/logo-gist-light.png\" width=\"20\" height=\"20\" alt=\"logo-gist\"></picture> | `gists` | GitHub Gist related tools |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/git-branch-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/git-branch-light.png\"><img src=\"pkg/octicons/icons/git-branch-light.png\" width=\"20\" height=\"20\" alt=\"git-branch\"></picture> | `git` | GitHub Git API related tools for low-level Git operations |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/issue-opened-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/issue-opened-light.png\"><img src=\"pkg/octicons/icons/issue-opened-light.png\" width=\"20\" height=\"20\" alt=\"issue-opened\"></picture> | `issues` | GitHub Issues related tools |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/tag-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/tag-light.png\"><img src=\"pkg/octicons/icons/tag-light.png\" width=\"20\" height=\"20\" alt=\"tag\"></picture> | `labels` | GitHub Labels related tools |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/bell-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/bell-light.png\"><img src=\"pkg/octicons/icons/bell-light.png\" width=\"20\" height=\"20\" alt=\"bell\"></picture> | `notifications` | GitHub Notifications related tools |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/organization-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/organization-light.png\"><img src=\"pkg/octicons/icons/organization-light.png\" width=\"20\" height=\"20\" alt=\"organization\"></picture> | `orgs` | GitHub Organization related tools |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/project-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/project-light.png\"><img src=\"pkg/octicons/icons/project-light.png\" width=\"20\" height=\"20\" alt=\"project\"></picture> | `projects` | GitHub Projects related tools |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/git-pull-request-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/git-pull-request-light.png\"><img src=\"pkg/octicons/icons/git-pull-request-light.png\" width=\"20\" height=\"20\" alt=\"git-pull-request\"></picture> | `pull_requests` | GitHub Pull Request related tools |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/repo-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/repo-light.png\"><img src=\"pkg/octicons/icons/repo-light.png\" width=\"20\" height=\"20\" alt=\"repo\"></picture> | `repos` | GitHub Repository related tools |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/shield-lock-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/shield-lock-light.png\"><img src=\"pkg/octicons/icons/shield-lock-light.png\" width=\"20\" height=\"20\" alt=\"shield-lock\"></picture> | `secret_protection` | Secret protection related tools, such as GitHub Secret Scanning |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/shield-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/shield-light.png\"><img src=\"pkg/octicons/icons/shield-light.png\" width=\"20\" height=\"20\" alt=\"shield\"></picture> | `security_advisories` | Security advisories related tools |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/star-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/star-light.png\"><img src=\"pkg/octicons/icons/star-light.png\" width=\"20\" height=\"20\" alt=\"star\"></picture> | `stargazers` | GitHub Stargazers related tools |\n| <picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/people-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/people-light.png\"><img src=\"pkg/octicons/icons/people-light.png\" width=\"20\" height=\"20\" alt=\"people\"></picture> | `users` | GitHub User related tools |\n<!-- END AUTOMATED TOOLSETS -->\n\n### Additional Toolsets in Remote GitHub MCP Server\n\n| Toolset                 | Description                                                   |\n| ----------------------- | ------------------------------------------------------------- |\n| `copilot` | Copilot related tools (e.g. Copilot Coding Agent) |\n| `copilot_spaces` | Copilot Spaces related tools |\n| `github_support_docs_search` | Search docs to answer GitHub product and support questions |\n\n## Tools\n\n<!-- START AUTOMATED TOOLS -->\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/workflow-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/workflow-light.png\"><img src=\"pkg/octicons/icons/workflow-light.png\" width=\"20\" height=\"20\" alt=\"workflow\"></picture> Actions</summary>\n\n- **actions_get** - Get details of GitHub Actions resources (workflows, workflow runs, jobs, and artifacts)\n  - **Required OAuth Scopes**: `repo`\n  - `method`: The method to execute (string, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `resource_id`: The unique identifier of the resource. This will vary based on the \"method\" provided, so ensure you provide the correct ID:\n    - Provide a workflow ID or workflow file name (e.g. ci.yaml) for 'get_workflow' method.\n    - Provide a workflow run ID for 'get_workflow_run', 'get_workflow_run_usage', and 'get_workflow_run_logs_url' methods.\n    - Provide an artifact ID for 'download_workflow_run_artifact' method.\n    - Provide a job ID for 'get_workflow_job' method.\n     (string, required)\n\n- **actions_list** - List GitHub Actions workflows in a repository\n  - **Required OAuth Scopes**: `repo`\n  - `method`: The action to perform (string, required)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (default: 1) (number, optional)\n  - `per_page`: Results per page for pagination (default: 30, max: 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `resource_id`: The unique identifier of the resource. This will vary based on the \"method\" provided, so ensure you provide the correct ID:\n    - Do not provide any resource ID for 'list_workflows' method.\n    - Provide a workflow ID or workflow file name (e.g. ci.yaml) for 'list_workflow_runs' method, or omit to list all workflow runs in the repository.\n    - Provide a workflow run ID for 'list_workflow_jobs' and 'list_workflow_run_artifacts' methods.\n     (string, optional)\n  - `workflow_jobs_filter`: Filters for workflow jobs. **ONLY** used when method is 'list_workflow_jobs' (object, optional)\n  - `workflow_runs_filter`: Filters for workflow runs. **ONLY** used when method is 'list_workflow_runs' (object, optional)\n\n- **actions_run_trigger** - Trigger GitHub Actions workflow actions\n  - **Required OAuth Scopes**: `repo`\n  - `inputs`: Inputs the workflow accepts. Only used for 'run_workflow' method. (object, optional)\n  - `method`: The method to execute (string, required)\n  - `owner`: Repository owner (string, required)\n  - `ref`: The git reference for the workflow. The reference can be a branch or tag name. Required for 'run_workflow' method. (string, optional)\n  - `repo`: Repository name (string, required)\n  - `run_id`: The ID of the workflow run. Required for all methods except 'run_workflow'. (number, optional)\n  - `workflow_id`: The workflow ID (numeric) or workflow file name (e.g., main.yml, ci.yaml). Required for 'run_workflow' method. (string, optional)\n\n- **get_job_logs** - Get GitHub Actions workflow job logs\n  - **Required OAuth Scopes**: `repo`\n  - `failed_only`: When true, gets logs for all failed jobs in the workflow run specified by run_id. Requires run_id to be provided. (boolean, optional)\n  - `job_id`: The unique identifier of the workflow job. Required when getting logs for a single job. (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `return_content`: Returns actual log content instead of URLs (boolean, optional)\n  - `run_id`: The unique identifier of the workflow run. Required when failed_only is true to get logs for all failed jobs in the run. (number, optional)\n  - `tail_lines`: Number of lines to return from the end of the log (number, optional)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/codescan-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/codescan-light.png\"><img src=\"pkg/octicons/icons/codescan-light.png\" width=\"20\" height=\"20\" alt=\"codescan\"></picture> Code Security</summary>\n\n- **get_code_scanning_alert** - Get code scanning alert\n  - **Required OAuth Scopes**: `security_events`\n  - **Accepted OAuth Scopes**: `repo`, `security_events`\n  - `alertNumber`: The number of the alert. (number, required)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **list_code_scanning_alerts** - List code scanning alerts\n  - **Required OAuth Scopes**: `security_events`\n  - **Accepted OAuth Scopes**: `repo`, `security_events`\n  - `owner`: The owner of the repository. (string, required)\n  - `ref`: The Git reference for the results you want to list. (string, optional)\n  - `repo`: The name of the repository. (string, required)\n  - `severity`: Filter code scanning alerts by severity (string, optional)\n  - `state`: Filter code scanning alerts by state. Defaults to open (string, optional)\n  - `tool_name`: The name of the tool used for code scanning. (string, optional)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/person-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/person-light.png\"><img src=\"pkg/octicons/icons/person-light.png\" width=\"20\" height=\"20\" alt=\"person\"></picture> Context</summary>\n\n- **get_me** - Get my user profile\n  - No parameters required\n\n- **get_team_members** - Get team members\n  - **Required OAuth Scopes**: `read:org`\n  - **Accepted OAuth Scopes**: `admin:org`, `read:org`, `write:org`\n  - `org`: Organization login (owner) that contains the team. (string, required)\n  - `team_slug`: Team slug (string, required)\n\n- **get_teams** - Get teams\n  - **Required OAuth Scopes**: `read:org`\n  - **Accepted OAuth Scopes**: `admin:org`, `read:org`, `write:org`\n  - `user`: Username to get teams for. If not provided, uses the authenticated user. (string, optional)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/dependabot-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/dependabot-light.png\"><img src=\"pkg/octicons/icons/dependabot-light.png\" width=\"20\" height=\"20\" alt=\"dependabot\"></picture> Dependabot</summary>\n\n- **get_dependabot_alert** - Get dependabot alert\n  - **Required OAuth Scopes**: `security_events`\n  - **Accepted OAuth Scopes**: `repo`, `security_events`\n  - `alertNumber`: The number of the alert. (number, required)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **list_dependabot_alerts** - List dependabot alerts\n  - **Required OAuth Scopes**: `security_events`\n  - **Accepted OAuth Scopes**: `repo`, `security_events`\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n  - `severity`: Filter dependabot alerts by severity (string, optional)\n  - `state`: Filter dependabot alerts by state. Defaults to open (string, optional)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/comment-discussion-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/comment-discussion-light.png\"><img src=\"pkg/octicons/icons/comment-discussion-light.png\" width=\"20\" height=\"20\" alt=\"comment-discussion\"></picture> Discussions</summary>\n\n- **get_discussion** - Get discussion\n  - **Required OAuth Scopes**: `repo`\n  - `discussionNumber`: Discussion Number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_discussion_comments** - Get discussion comments\n  - **Required OAuth Scopes**: `repo`\n  - `after`: Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs. (string, optional)\n  - `discussionNumber`: Discussion Number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_discussion_categories** - List discussion categories\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name. If not provided, discussion categories will be queried at the organisation level. (string, optional)\n\n- **list_discussions** - List discussions\n  - **Required OAuth Scopes**: `repo`\n  - `after`: Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs. (string, optional)\n  - `category`: Optional filter by discussion category ID. If provided, only discussions with this category are listed. (string, optional)\n  - `direction`: Order direction. (string, optional)\n  - `orderBy`: Order discussions by field. If provided, the 'direction' also needs to be provided. (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name. If not provided, discussions will be queried at the organisation level. (string, optional)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/logo-gist-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/logo-gist-light.png\"><img src=\"pkg/octicons/icons/logo-gist-light.png\" width=\"20\" height=\"20\" alt=\"logo-gist\"></picture> Gists</summary>\n\n- **create_gist** - Create Gist\n  - **Required OAuth Scopes**: `gist`\n  - `content`: Content for simple single-file gist creation (string, required)\n  - `description`: Description of the gist (string, optional)\n  - `filename`: Filename for simple single-file gist creation (string, required)\n  - `public`: Whether the gist is public (boolean, optional)\n\n- **get_gist** - Get Gist Content\n  - `gist_id`: The ID of the gist (string, required)\n\n- **list_gists** - List Gists\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `since`: Only gists updated after this time (ISO 8601 timestamp) (string, optional)\n  - `username`: GitHub username (omit for authenticated user's gists) (string, optional)\n\n- **update_gist** - Update Gist\n  - **Required OAuth Scopes**: `gist`\n  - `content`: Content for the file (string, required)\n  - `description`: Updated description of the gist (string, optional)\n  - `filename`: Filename to update or create (string, required)\n  - `gist_id`: ID of the gist to update (string, required)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/git-branch-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/git-branch-light.png\"><img src=\"pkg/octicons/icons/git-branch-light.png\" width=\"20\" height=\"20\" alt=\"git-branch\"></picture> Git</summary>\n\n- **get_repository_tree** - Get repository tree\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path_filter`: Optional path prefix to filter the tree results (e.g., 'src/' to only show files in the src directory) (string, optional)\n  - `recursive`: Setting this parameter to true returns the objects or subtrees referenced by the tree. Default is false (boolean, optional)\n  - `repo`: Repository name (string, required)\n  - `tree_sha`: The SHA1 value or ref (branch or tag) name of the tree. Defaults to the repository's default branch (string, optional)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/issue-opened-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/issue-opened-light.png\"><img src=\"pkg/octicons/icons/issue-opened-light.png\" width=\"20\" height=\"20\" alt=\"issue-opened\"></picture> Issues</summary>\n\n- **add_issue_comment** - Add comment to issue\n  - **Required OAuth Scopes**: `repo`\n  - `body`: Comment content (string, required)\n  - `issue_number`: Issue number to comment on (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **assign_copilot_to_issue** - Assign Copilot to issue\n  - **Required OAuth Scopes**: `repo`\n  - `base_ref`: Git reference (e.g., branch) that the agent will start its work from. If not specified, defaults to the repository's default branch (string, optional)\n  - `custom_instructions`: Optional custom instructions to guide the agent beyond the issue body. Use this to provide additional context, constraints, or guidance that is not captured in the issue description (string, optional)\n  - `issue_number`: Issue number (number, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_label** - Get a specific label from a repository.\n  - **Required OAuth Scopes**: `repo`\n  - `name`: Label name. (string, required)\n  - `owner`: Repository owner (username or organization name) (string, required)\n  - `repo`: Repository name (string, required)\n\n- **issue_read** - Get issue details\n  - **Required OAuth Scopes**: `repo`\n  - `issue_number`: The number of the issue (number, required)\n  - `method`: The read operation to perform on a single issue.\n    Options are:\n    1. get - Get details of a specific issue.\n    2. get_comments - Get issue comments.\n    3. get_sub_issues - Get sub-issues of the issue.\n    4. get_labels - Get labels assigned to the issue.\n     (string, required)\n  - `owner`: The owner of the repository (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: The name of the repository (string, required)\n\n- **issue_write** - Create or update issue.\n  - **Required OAuth Scopes**: `repo`\n  - `assignees`: Usernames to assign to this issue (string[], optional)\n  - `body`: Issue body content (string, optional)\n  - `duplicate_of`: Issue number that this issue is a duplicate of. Only used when state_reason is 'duplicate'. (number, optional)\n  - `issue_number`: Issue number to update (number, optional)\n  - `labels`: Labels to apply to this issue (string[], optional)\n  - `method`: Write operation to perform on a single issue.\n    Options are:\n    - 'create' - creates a new issue.\n    - 'update' - updates an existing issue.\n     (string, required)\n  - `milestone`: Milestone number (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `state`: New state (string, optional)\n  - `state_reason`: Reason for the state change. Ignored unless state is changed. (string, optional)\n  - `title`: Issue title (string, optional)\n  - `type`: Type of this issue. Only use if the repository has issue types configured. Use list_issue_types tool to get valid type values for the organization. If the repository doesn't support issue types, omit this parameter. (string, optional)\n\n- **list_issue_types** - List available issue types\n  - **Required OAuth Scopes**: `read:org`\n  - **Accepted OAuth Scopes**: `admin:org`, `read:org`, `write:org`\n  - `owner`: The organization owner of the repository (string, required)\n\n- **list_issues** - List issues\n  - **Required OAuth Scopes**: `repo`\n  - `after`: Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs. (string, optional)\n  - `direction`: Order direction. If provided, the 'orderBy' also needs to be provided. (string, optional)\n  - `labels`: Filter by labels (string[], optional)\n  - `orderBy`: Order issues by field. If provided, the 'direction' also needs to be provided. (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `since`: Filter by date (ISO 8601 timestamp) (string, optional)\n  - `state`: Filter by state, by default both open and closed issues are returned when not provided (string, optional)\n\n- **search_issues** - Search issues\n  - **Required OAuth Scopes**: `repo`\n  - `order`: Sort order (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only issues for this repository are listed. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Search query using GitHub issues search syntax (string, required)\n  - `repo`: Optional repository name. If provided with owner, only issues for this repository are listed. (string, optional)\n  - `sort`: Sort field by number of matches of categories, defaults to best match (string, optional)\n\n- **sub_issue_write** - Change sub-issue\n  - **Required OAuth Scopes**: `repo`\n  - `after_id`: The ID of the sub-issue to be prioritized after (either after_id OR before_id should be specified) (number, optional)\n  - `before_id`: The ID of the sub-issue to be prioritized before (either after_id OR before_id should be specified) (number, optional)\n  - `issue_number`: The number of the parent issue (number, required)\n  - `method`: The action to perform on a single sub-issue\n    Options are:\n    - 'add' - add a sub-issue to a parent issue in a GitHub repository.\n    - 'remove' - remove a sub-issue from a parent issue in a GitHub repository.\n    - 'reprioritize' - change the order of sub-issues within a parent issue in a GitHub repository. Use either 'after_id' or 'before_id' to specify the new position.\n    \t\t\t\t (string, required)\n  - `owner`: Repository owner (string, required)\n  - `replace_parent`: When true, replaces the sub-issue's current parent issue. Use with 'add' method only. (boolean, optional)\n  - `repo`: Repository name (string, required)\n  - `sub_issue_id`: The ID of the sub-issue to add. ID is not the same as issue number (number, required)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/tag-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/tag-light.png\"><img src=\"pkg/octicons/icons/tag-light.png\" width=\"20\" height=\"20\" alt=\"tag\"></picture> Labels</summary>\n\n- **get_label** - Get a specific label from a repository.\n  - **Required OAuth Scopes**: `repo`\n  - `name`: Label name. (string, required)\n  - `owner`: Repository owner (username or organization name) (string, required)\n  - `repo`: Repository name (string, required)\n\n- **label_write** - Write operations on repository labels.\n  - **Required OAuth Scopes**: `repo`\n  - `color`: Label color as 6-character hex code without '#' prefix (e.g., 'f29513'). Required for 'create', optional for 'update'. (string, optional)\n  - `description`: Label description text. Optional for 'create' and 'update'. (string, optional)\n  - `method`: Operation to perform: 'create', 'update', or 'delete' (string, required)\n  - `name`: Label name - required for all operations (string, required)\n  - `new_name`: New name for the label (used only with 'update' method to rename) (string, optional)\n  - `owner`: Repository owner (username or organization name) (string, required)\n  - `repo`: Repository name (string, required)\n\n- **list_label** - List labels from a repository\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (username or organization name) - required for all operations (string, required)\n  - `repo`: Repository name - required for all operations (string, required)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/bell-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/bell-light.png\"><img src=\"pkg/octicons/icons/bell-light.png\" width=\"20\" height=\"20\" alt=\"bell\"></picture> Notifications</summary>\n\n- **dismiss_notification** - Dismiss notification\n  - **Required OAuth Scopes**: `notifications`\n  - `state`: The new state of the notification (read/done) (string, required)\n  - `threadID`: The ID of the notification thread (string, required)\n\n- **get_notification_details** - Get notification details\n  - **Required OAuth Scopes**: `notifications`\n  - `notificationID`: The ID of the notification (string, required)\n\n- **list_notifications** - List notifications\n  - **Required OAuth Scopes**: `notifications`\n  - `before`: Only show notifications updated before the given time (ISO 8601 format) (string, optional)\n  - `filter`: Filter notifications to, use default unless specified. Read notifications are ones that have already been acknowledged by the user. Participating notifications are those that the user is directly involved in, such as issues or pull requests they have commented on or created. (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only notifications for this repository are listed. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Optional repository name. If provided with owner, only notifications for this repository are listed. (string, optional)\n  - `since`: Only show notifications updated after the given time (ISO 8601 format) (string, optional)\n\n- **manage_notification_subscription** - Manage notification subscription\n  - **Required OAuth Scopes**: `notifications`\n  - `action`: Action to perform: ignore, watch, or delete the notification subscription. (string, required)\n  - `notificationID`: The ID of the notification thread. (string, required)\n\n- **manage_repository_notification_subscription** - Manage repository notification subscription\n  - **Required OAuth Scopes**: `notifications`\n  - `action`: Action to perform: ignore, watch, or delete the repository notification subscription. (string, required)\n  - `owner`: The account owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **mark_all_notifications_read** - Mark all notifications as read\n  - **Required OAuth Scopes**: `notifications`\n  - `lastReadAt`: Describes the last point that notifications were checked (optional). Default: Now (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only notifications for this repository are marked as read. (string, optional)\n  - `repo`: Optional repository name. If provided with owner, only notifications for this repository are marked as read. (string, optional)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/organization-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/organization-light.png\"><img src=\"pkg/octicons/icons/organization-light.png\" width=\"20\" height=\"20\" alt=\"organization\"></picture> Organizations</summary>\n\n- **search_orgs** - Search organizations\n  - **Required OAuth Scopes**: `read:org`\n  - **Accepted OAuth Scopes**: `admin:org`, `read:org`, `write:org`\n  - `order`: Sort order (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Organization search query. Examples: 'microsoft', 'location:california', 'created:>=2025-01-01'. Search is automatically scoped to type:org. (string, required)\n  - `sort`: Sort field by category (string, optional)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/project-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/project-light.png\"><img src=\"pkg/octicons/icons/project-light.png\" width=\"20\" height=\"20\" alt=\"project\"></picture> Projects</summary>\n\n- **projects_get** - Get details of GitHub Projects resources\n  - **Required OAuth Scopes**: `read:project`\n  - **Accepted OAuth Scopes**: `project`, `read:project`\n  - `field_id`: The field's ID. Required for 'get_project_field' method. (number, optional)\n  - `fields`: Specific list of field IDs to include in the response when getting a project item (e.g. [\"102589\", \"985201\", \"169875\"]). If not provided, only the title field is included. Only used for 'get_project_item' method. (string[], optional)\n  - `item_id`: The item's ID. Required for 'get_project_item' method. (number, optional)\n  - `method`: The method to execute (string, required)\n  - `owner`: The owner (user or organization login). The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (user or org). If not provided, will be automatically detected. (string, optional)\n  - `project_number`: The project's number. (number, required)\n\n- **projects_list** - List GitHub Projects resources\n  - **Required OAuth Scopes**: `read:project`\n  - **Accepted OAuth Scopes**: `project`, `read:project`\n  - `after`: Forward pagination cursor from previous pageInfo.nextCursor. (string, optional)\n  - `before`: Backward pagination cursor from previous pageInfo.prevCursor (rare). (string, optional)\n  - `fields`: Field IDs to include when listing project items (e.g. [\"102589\", \"985201\"]). CRITICAL: Always provide to get field values. Without this, only titles returned. Only used for 'list_project_items' method. (string[], optional)\n  - `method`: The action to perform (string, required)\n  - `owner`: The owner (user or organization login). The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (user or org). If not provided, will automatically try both. (string, optional)\n  - `per_page`: Results per page (max 50) (number, optional)\n  - `project_number`: The project's number. Required for 'list_project_fields' and 'list_project_items' methods. (number, optional)\n  - `query`: Filter/query string. For list_projects: filter by title text and state (e.g. \"roadmap is:open\"). For list_project_items: advanced filtering using GitHub's project filtering syntax. (string, optional)\n\n- **projects_write** - Modify GitHub Project items\n  - **Required OAuth Scopes**: `project`\n  - `issue_number`: The issue number (use when item_type is 'issue' for 'add_project_item' method). Provide either issue_number or pull_request_number. (number, optional)\n  - `item_id`: The project item ID. Required for 'update_project_item' and 'delete_project_item' methods. (number, optional)\n  - `item_owner`: The owner (user or organization) of the repository containing the issue or pull request. Required for 'add_project_item' method. (string, optional)\n  - `item_repo`: The name of the repository containing the issue or pull request. Required for 'add_project_item' method. (string, optional)\n  - `item_type`: The item's type, either issue or pull_request. Required for 'add_project_item' method. (string, optional)\n  - `method`: The method to execute (string, required)\n  - `owner`: The project owner (user or organization login). The name is not case sensitive. (string, required)\n  - `owner_type`: Owner type (user or org). If not provided, will be automatically detected. (string, optional)\n  - `project_number`: The project's number. (number, required)\n  - `pull_request_number`: The pull request number (use when item_type is 'pull_request' for 'add_project_item' method). Provide either issue_number or pull_request_number. (number, optional)\n  - `updated_field`: Object consisting of the ID of the project field to update and the new value for the field. To clear the field, set value to null. Example: {\"id\": 123456, \"value\": \"New Value\"}. Required for 'update_project_item' method. (object, optional)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/git-pull-request-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/git-pull-request-light.png\"><img src=\"pkg/octicons/icons/git-pull-request-light.png\" width=\"20\" height=\"20\" alt=\"git-pull-request\"></picture> Pull Requests</summary>\n\n- **add_comment_to_pending_review** - Add review comment to the requester's latest pending pull request review\n  - **Required OAuth Scopes**: `repo`\n  - `body`: The text of the review comment (string, required)\n  - `line`: The line of the blob in the pull request diff that the comment applies to. For multi-line comments, the last line of the range (number, optional)\n  - `owner`: Repository owner (string, required)\n  - `path`: The relative path to the file that necessitates a comment (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n  - `side`: The side of the diff to comment on. LEFT indicates the previous state, RIGHT indicates the new state (string, optional)\n  - `startLine`: For multi-line comments, the first line of the range that the comment applies to (number, optional)\n  - `startSide`: For multi-line comments, the starting side of the diff that the comment applies to. LEFT indicates the previous state, RIGHT indicates the new state (string, optional)\n  - `subjectType`: The level at which the comment is targeted (string, required)\n\n- **create_pull_request** - Open new pull request\n  - **Required OAuth Scopes**: `repo`\n  - `base`: Branch to merge into (string, required)\n  - `body`: PR description (string, optional)\n  - `draft`: Create as draft PR (boolean, optional)\n  - `head`: Branch containing changes (string, required)\n  - `maintainer_can_modify`: Allow maintainer edits (boolean, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `title`: PR title (string, required)\n\n- **list_pull_requests** - List pull requests\n  - **Required OAuth Scopes**: `repo`\n  - `base`: Filter by base branch (string, optional)\n  - `direction`: Sort direction (string, optional)\n  - `head`: Filter by head user/org and branch (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `sort`: Sort by (string, optional)\n  - `state`: Filter by state (string, optional)\n\n- **merge_pull_request** - Merge pull request\n  - **Required OAuth Scopes**: `repo`\n  - `commit_message`: Extra detail for merge commit (string, optional)\n  - `commit_title`: Title for merge commit (string, optional)\n  - `merge_method`: Merge method (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **pull_request_read** - Get details for a single pull request\n  - **Required OAuth Scopes**: `repo`\n  - `method`: Action to specify what pull request data needs to be retrieved from GitHub. \n    Possible options: \n     1. get - Get details of a specific pull request.\n     2. get_diff - Get the diff of a pull request.\n     3. get_status - Get status of a head commit in a pull request. This reflects status of builds and checks.\n     4. get_files - Get the list of files changed in a pull request. Use with pagination parameters to control the number of results returned.\n     5. get_review_comments - Get review threads on a pull request. Each thread contains logically grouped review comments made on the same code location during pull request reviews. Returns threads with metadata (isResolved, isOutdated, isCollapsed) and their associated comments. Use cursor-based pagination (perPage, after) to control results.\n     6. get_reviews - Get the reviews on a pull request. When asked for review comments, use get_review_comments method.\n     7. get_comments - Get comments on a pull request. Use this if user doesn't specifically want review comments. Use with pagination parameters to control the number of results returned.\n     (string, required)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **pull_request_review_write** - Write operations (create, submit, delete) on pull request reviews.\n  - **Required OAuth Scopes**: `repo`\n  - `body`: Review comment text (string, optional)\n  - `commitID`: SHA of commit to review (string, optional)\n  - `event`: Review action to perform. (string, optional)\n  - `method`: The write operation to perform on pull request review. (string, required)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **request_copilot_review** - Request Copilot review\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n- **search_pull_requests** - Search pull requests\n  - **Required OAuth Scopes**: `repo`\n  - `order`: Sort order (string, optional)\n  - `owner`: Optional repository owner. If provided with repo, only pull requests for this repository are listed. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Search query using GitHub pull request search syntax (string, required)\n  - `repo`: Optional repository name. If provided with owner, only pull requests for this repository are listed. (string, optional)\n  - `sort`: Sort field by number of matches of categories, defaults to best match (string, optional)\n\n- **update_pull_request** - Edit pull request\n  - **Required OAuth Scopes**: `repo`\n  - `base`: New base branch name (string, optional)\n  - `body`: New description (string, optional)\n  - `draft`: Mark pull request as draft (true) or ready for review (false) (boolean, optional)\n  - `maintainer_can_modify`: Allow maintainer edits (boolean, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number to update (number, required)\n  - `repo`: Repository name (string, required)\n  - `reviewers`: GitHub usernames to request reviews from (string[], optional)\n  - `state`: New state (string, optional)\n  - `title`: New title (string, optional)\n\n- **update_pull_request_branch** - Update pull request branch\n  - **Required OAuth Scopes**: `repo`\n  - `expectedHeadSha`: The expected SHA of the pull request's HEAD ref (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `pullNumber`: Pull request number (number, required)\n  - `repo`: Repository name (string, required)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/repo-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/repo-light.png\"><img src=\"pkg/octicons/icons/repo-light.png\" width=\"20\" height=\"20\" alt=\"repo\"></picture> Repositories</summary>\n\n- **create_branch** - Create branch\n  - **Required OAuth Scopes**: `repo`\n  - `branch`: Name for new branch (string, required)\n  - `from_branch`: Source branch (defaults to repo default) (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **create_or_update_file** - Create or update file\n  - **Required OAuth Scopes**: `repo`\n  - `branch`: Branch to create/update the file in (string, required)\n  - `content`: Content of the file (string, required)\n  - `message`: Commit message (string, required)\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path`: Path where to create/update the file (string, required)\n  - `repo`: Repository name (string, required)\n  - `sha`: The blob SHA of the file being replaced. (string, optional)\n\n- **create_repository** - Create repository\n  - **Required OAuth Scopes**: `repo`\n  - `autoInit`: Initialize with README (boolean, optional)\n  - `description`: Repository description (string, optional)\n  - `name`: Repository name (string, required)\n  - `organization`: Organization to create the repository in (omit to create in your personal account) (string, optional)\n  - `private`: Whether repo should be private (boolean, optional)\n\n- **delete_file** - Delete file\n  - **Required OAuth Scopes**: `repo`\n  - `branch`: Branch to delete the file from (string, required)\n  - `message`: Commit message (string, required)\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path`: Path to the file to delete (string, required)\n  - `repo`: Repository name (string, required)\n\n- **fork_repository** - Fork repository\n  - **Required OAuth Scopes**: `repo`\n  - `organization`: Organization to fork to (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_commit** - Get commit details\n  - **Required OAuth Scopes**: `repo`\n  - `include_diff`: Whether to include file diffs and stats in the response. Default is true. (boolean, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `sha`: Commit SHA, branch name, or tag name (string, required)\n\n- **get_file_contents** - Get file or directory contents\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (username or organization) (string, required)\n  - `path`: Path to file/directory (string, optional)\n  - `ref`: Accepts optional git refs such as `refs/tags/{tag}`, `refs/heads/{branch}` or `refs/pull/{pr_number}/head` (string, optional)\n  - `repo`: Repository name (string, required)\n  - `sha`: Accepts optional commit SHA. If specified, it will be used instead of ref (string, optional)\n\n- **get_latest_release** - Get latest release\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **get_release_by_tag** - Get a release by tag name\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `tag`: Tag name (e.g., 'v1.0.0') (string, required)\n\n- **get_tag** - Get tag details\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n  - `tag`: Tag name (string, required)\n\n- **list_branches** - List branches\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_commits** - List commits\n  - **Required OAuth Scopes**: `repo`\n  - `author`: Author username or email address to filter commits by (string, optional)\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n  - `sha`: Commit SHA, branch or tag name to list commits of. If not provided, uses the default branch of the repository. If a commit SHA is provided, will list commits up to that SHA. (string, optional)\n\n- **list_releases** - List releases\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **list_tags** - List tags\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (string, required)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `repo`: Repository name (string, required)\n\n- **push_files** - Push files to repository\n  - **Required OAuth Scopes**: `repo`\n  - `branch`: Branch to push to (string, required)\n  - `files`: Array of file objects to push, each object with path (string) and content (string) (object[], required)\n  - `message`: Commit message (string, required)\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **search_code** - Search code\n  - **Required OAuth Scopes**: `repo`\n  - `order`: Sort order for results (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Search query using GitHub's powerful code search syntax. Examples: 'content:Skill language:Java org:github', 'NOT is:archived language:Python OR language:go', 'repo:github/github-mcp-server'. Supports exact matching, language filters, path filters, and more. (string, required)\n  - `sort`: Sort field ('indexed' only) (string, optional)\n\n- **search_repositories** - Search repositories\n  - **Required OAuth Scopes**: `repo`\n  - `minimal_output`: Return minimal repository information (default: true). When false, returns full GitHub API repository objects. (boolean, optional)\n  - `order`: Sort order (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: Repository search query. Examples: 'machine learning in:name stars:>1000 language:python', 'topic:react', 'user:facebook'. Supports advanced search syntax for precise filtering. (string, required)\n  - `sort`: Sort repositories by field, defaults to best match (string, optional)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/shield-lock-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/shield-lock-light.png\"><img src=\"pkg/octicons/icons/shield-lock-light.png\" width=\"20\" height=\"20\" alt=\"shield-lock\"></picture> Secret Protection</summary>\n\n- **get_secret_scanning_alert** - Get secret scanning alert\n  - **Required OAuth Scopes**: `security_events`\n  - **Accepted OAuth Scopes**: `repo`, `security_events`\n  - `alertNumber`: The number of the alert. (number, required)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n\n- **list_secret_scanning_alerts** - List secret scanning alerts\n  - **Required OAuth Scopes**: `security_events`\n  - **Accepted OAuth Scopes**: `repo`, `security_events`\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n  - `resolution`: Filter by resolution (string, optional)\n  - `secret_type`: A comma-separated list of secret types to return. All default secret patterns are returned. To return generic patterns, pass the token name(s) in the parameter. (string, optional)\n  - `state`: Filter by state (string, optional)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/shield-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/shield-light.png\"><img src=\"pkg/octicons/icons/shield-light.png\" width=\"20\" height=\"20\" alt=\"shield\"></picture> Security Advisories</summary>\n\n- **get_global_security_advisory** - Get a global security advisory\n  - **Required OAuth Scopes**: `security_events`\n  - **Accepted OAuth Scopes**: `repo`, `security_events`\n  - `ghsaId`: GitHub Security Advisory ID (format: GHSA-xxxx-xxxx-xxxx). (string, required)\n\n- **list_global_security_advisories** - List global security advisories\n  - **Required OAuth Scopes**: `security_events`\n  - **Accepted OAuth Scopes**: `repo`, `security_events`\n  - `affects`: Filter advisories by affected package or version (e.g. \"package1,package2@1.0.0\"). (string, optional)\n  - `cveId`: Filter by CVE ID. (string, optional)\n  - `cwes`: Filter by Common Weakness Enumeration IDs (e.g. [\"79\", \"284\", \"22\"]). (string[], optional)\n  - `ecosystem`: Filter by package ecosystem. (string, optional)\n  - `ghsaId`: Filter by GitHub Security Advisory ID (format: GHSA-xxxx-xxxx-xxxx). (string, optional)\n  - `isWithdrawn`: Whether to only return withdrawn advisories. (boolean, optional)\n  - `modified`: Filter by publish or update date or date range (ISO 8601 date or range). (string, optional)\n  - `published`: Filter by publish date or date range (ISO 8601 date or range). (string, optional)\n  - `severity`: Filter by severity. (string, optional)\n  - `type`: Advisory type. (string, optional)\n  - `updated`: Filter by update date or date range (ISO 8601 date or range). (string, optional)\n\n- **list_org_repository_security_advisories** - List org repository security advisories\n  - **Required OAuth Scopes**: `security_events`\n  - **Accepted OAuth Scopes**: `repo`, `security_events`\n  - `direction`: Sort direction. (string, optional)\n  - `org`: The organization login. (string, required)\n  - `sort`: Sort field. (string, optional)\n  - `state`: Filter by advisory state. (string, optional)\n\n- **list_repository_security_advisories** - List repository security advisories\n  - **Required OAuth Scopes**: `security_events`\n  - **Accepted OAuth Scopes**: `repo`, `security_events`\n  - `direction`: Sort direction. (string, optional)\n  - `owner`: The owner of the repository. (string, required)\n  - `repo`: The name of the repository. (string, required)\n  - `sort`: Sort field. (string, optional)\n  - `state`: Filter by advisory state. (string, optional)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/star-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/star-light.png\"><img src=\"pkg/octicons/icons/star-light.png\" width=\"20\" height=\"20\" alt=\"star\"></picture> Stargazers</summary>\n\n- **list_starred_repositories** - List starred repositories\n  - **Required OAuth Scopes**: `repo`\n  - `direction`: The direction to sort the results by. (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `sort`: How to sort the results. Can be either 'created' (when the repository was starred) or 'updated' (when the repository was last pushed to). (string, optional)\n  - `username`: Username to list starred repositories for. Defaults to the authenticated user. (string, optional)\n\n- **star_repository** - Star repository\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n- **unstar_repository** - Unstar repository\n  - **Required OAuth Scopes**: `repo`\n  - `owner`: Repository owner (string, required)\n  - `repo`: Repository name (string, required)\n\n</details>\n\n<details>\n\n<summary><picture><source media=\"(prefers-color-scheme: dark)\" srcset=\"pkg/octicons/icons/people-dark.png\"><source media=\"(prefers-color-scheme: light)\" srcset=\"pkg/octicons/icons/people-light.png\"><img src=\"pkg/octicons/icons/people-light.png\" width=\"20\" height=\"20\" alt=\"people\"></picture> Users</summary>\n\n- **search_users** - Search users\n  - **Required OAuth Scopes**: `repo`\n  - `order`: Sort order (string, optional)\n  - `page`: Page number for pagination (min 1) (number, optional)\n  - `perPage`: Results per page for pagination (min 1, max 100) (number, optional)\n  - `query`: User search query. Examples: 'john smith', 'location:seattle', 'followers:>100'. Search is automatically scoped to type:user. (string, required)\n  - `sort`: Sort users by number of followers or repositories, or when the person joined GitHub. (string, optional)\n\n</details>\n<!-- END AUTOMATED TOOLS -->\n\n### Additional Tools in Remote GitHub MCP Server\n\n<details>\n\n<summary>Copilot</summary>\n\n- **create_pull_request_with_copilot** - Perform task with GitHub Copilot coding agent\n  - `owner`: Repository owner. You can guess the owner, but confirm it with the user before proceeding. (string, required)\n  - `repo`: Repository name. You can guess the repository name, but confirm it with the user before proceeding. (string, required)\n  - `problem_statement`: Detailed description of the task to be performed (e.g., 'Implement a feature that does X', 'Fix bug Y', etc.) (string, required)\n  - `title`: Title for the pull request that will be created (string, required)\n  - `base_ref`: Git reference (e.g., branch) that the agent will start its work from. If not specified, defaults to the repository's default branch (string, optional)\n\n</details>\n\n<details>\n\n<summary>Copilot Spaces</summary>\n\n- **get_copilot_space** - Get Copilot Space\n  - `owner`: The owner of the space. (string, required)\n  - `name`: The name of the space. (string, required)\n\n- **list_copilot_spaces** - List Copilot Spaces\n\n</details>\n\n<details>\n\n<summary>GitHub Support Docs Search</summary>\n\n- **github_support_docs_search** - Retrieve documentation relevant to answer GitHub product and support questions. Support topics include: GitHub Actions Workflows, Authentication, GitHub Support Inquiries, Pull Request Practices, Repository Maintenance, GitHub Pages, GitHub Packages, GitHub Discussions, Copilot Spaces\n  - `query`: Input from the user about the question they need answered. This is the latest raw unedited user message. You should ALWAYS leave the user message as it is, you should never modify it. (string, required)\n\n</details>\n\n## Dynamic Tool Discovery\n\n**Note**: This feature is currently in beta and is not available in the Remote GitHub MCP Server. Please test it out and let us know if you encounter any issues.\n\nInstead of starting with all tools enabled, you can turn on dynamic toolset discovery. Dynamic toolsets allow the MCP host to list and enable toolsets in response to a user prompt. This should help to avoid situations where the model gets confused by the sheer number of tools available.\n\n### Using Dynamic Tool Discovery\n\nWhen using the binary, you can pass the `--dynamic-toolsets` flag.\n\n```bash\n./github-mcp-server --dynamic-toolsets\n```\n\nWhen using Docker, you can pass the toolsets as environment variables:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=<your-token> \\\n  -e GITHUB_DYNAMIC_TOOLSETS=1 \\\n  ghcr.io/github/github-mcp-server\n```\n\n## Read-Only Mode\n\nTo run the server in read-only mode, you can use the `--read-only` flag. This will only offer read-only tools, preventing any modifications to repositories, issues, pull requests, etc.\n\n```bash\n./github-mcp-server --read-only\n```\n\nWhen using Docker, you can pass the read-only mode as an environment variable:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=<your-token> \\\n  -e GITHUB_READ_ONLY=1 \\\n  ghcr.io/github/github-mcp-server\n```\n\n## Lockdown Mode\n\nLockdown mode limits the content that the server will surface from public repositories. When enabled, the server checks whether the author of each item has push access to the repository. Private repositories are unaffected, and collaborators keep full access to their own content.\n\n```bash\n./github-mcp-server --lockdown-mode\n```\n\nWhen running with Docker, set the corresponding environment variable:\n\n```bash\ndocker run -i --rm \\\n  -e GITHUB_PERSONAL_ACCESS_TOKEN=<your-token> \\\n  -e GITHUB_LOCKDOWN_MODE=1 \\\n  ghcr.io/github/github-mcp-server\n```\n\nThe behavior of lockdown mode depends on the tool invoked.\n\nFollowing tools will return an error when the author lacks the push access:\n\n- `issue_read:get`\n- `pull_request_read:get`\n\nFollowing tools will filter out content from users lacking the push access:\n\n- `issue_read:get_comments`\n- `issue_read:get_sub_issues`\n- `pull_request_read:get_comments`\n- `pull_request_read:get_review_comments`\n- `pull_request_read:get_reviews`\n\n## i18n / Overriding Descriptions\n\nThe descriptions of the tools can be overridden by creating a\n`github-mcp-server-config.json` file in the same directory as the binary.\n\nThe file should contain a JSON object with the tool names as keys and the new\ndescriptions as values. For example:\n\n```json\n{\n  \"TOOL_ADD_ISSUE_COMMENT_DESCRIPTION\": \"an alternative description\",\n  \"TOOL_CREATE_BRANCH_DESCRIPTION\": \"Create a new branch in a GitHub repository\"\n}\n```\n\nYou can create an export of the current translations by running the binary with\nthe `--export-translations` flag.\n\nThis flag will preserve any translations/overrides you have made, while adding\nany new translations that have been added to the binary since the last time you\nexported.\n\n```sh\n./github-mcp-server --export-translations\ncat github-mcp-server-config.json\n```\n\nYou can also use ENV vars to override the descriptions. The environment\nvariable names are the same as the keys in the JSON file, prefixed with\n`GITHUB_MCP_` and all uppercase.\n\nFor example, to override the `TOOL_ADD_ISSUE_COMMENT_DESCRIPTION` tool, you can\nset the following environment variable:\n\n```sh\nexport GITHUB_MCP_TOOL_ADD_ISSUE_COMMENT_DESCRIPTION=\"an alternative description\"\n```\n\n## Library Usage\n\nThe exported Go API of this module should currently be considered unstable, and subject to breaking changes. In the future, we may offer stability; please file an issue if there is a use case where this would be valuable.\n\n## License\n\nThis project is licensed under the terms of the MIT open source license. Please refer to [MIT](./LICENSE) for the full terms.\n",
      "stars_today": 47
    },
    {
      "id": 48378947,
      "name": "frp",
      "full_name": "fatedier/frp",
      "description": "A fast reverse proxy to help you expose a local server behind a NAT or firewall to the internet.",
      "html_url": "https://github.com/fatedier/frp",
      "stars": 103926,
      "forks": 14834,
      "language": "Go",
      "topics": [
        "expose",
        "firewall",
        "frp",
        "go",
        "http-proxy",
        "nat",
        "p2p",
        "proxy",
        "reverse-proxy",
        "tunnel"
      ],
      "created_at": "2015-12-21T15:24:59Z",
      "updated_at": "2026-01-28T01:17:34Z",
      "pushed_at": "2026-01-26T18:56:57Z",
      "open_issues": 45,
      "owner": {
        "login": "fatedier",
        "avatar_url": "https://avatars.githubusercontent.com/u/7346661?v=4"
      },
      "readme": "# frp\n\n[![Build Status](https://circleci.com/gh/fatedier/frp.svg?style=shield)](https://circleci.com/gh/fatedier/frp)\n[![GitHub release](https://img.shields.io/github/tag/fatedier/frp.svg?label=release)](https://github.com/fatedier/frp/releases)\n[![Go Report Card](https://goreportcard.com/badge/github.com/fatedier/frp)](https://goreportcard.com/report/github.com/fatedier/frp)\n[![GitHub Releases Stats](https://img.shields.io/github/downloads/fatedier/frp/total.svg?logo=github)](https://somsubhra.github.io/github-release-stats/?username=fatedier&repository=frp)\n\n[README](README.md) | [ä¸­æ–‡æ–‡æ¡£](README_zh.md)\n\n## Sponsors\n\nfrp is an open source project with its ongoing development made possible entirely by the support of our awesome sponsors. If you'd like to join them, please consider [sponsoring frp's development](https://github.com/sponsors/fatedier).\n\n<h3 align=\"center\">Gold Sponsors</h3>\n<!--gold sponsors start-->\n<p align=\"center\">\n  <a href=\"https://requestly.com/?utm_source=github&utm_medium=partnered&utm_campaign=frp\" target=\"_blank\">\n    <img width=\"480px\" src=\"https://github.com/user-attachments/assets/24670320-997d-4d62-9bca-955c59fe883d\">\n    <br>\n    <b>Requestly - Free & Open-Source alternative to Postman</b>\n    <br>\n    <sub>All-in-one platform to Test, Mock and Intercept APIs.</sub>\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://jb.gg/frp\" target=\"_blank\">\n    <img width=\"420px\" src=\"https://raw.githubusercontent.com/fatedier/frp/dev/doc/pic/sponsor_jetbrains.jpg\">\n\t<br>\n\t<b>The complete IDE crafted for professional Go developers</b>\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/beclab/Olares\" target=\"_blank\">\n    <img width=\"420px\" src=\"https://raw.githubusercontent.com/fatedier/frp/dev/doc/pic/sponsor_olares.jpeg\">\n\t<br>\n\t<b>The sovereign cloud that puts you in control</b>\n\t<br>\n\t<sub>An open source, self-hosted alternative to public clouds, built for data ownership and privacy</sub>\n  </a>\n</p>\n<div align=\"center\">\n\n## Recall.ai - API for meeting recordings\n\nIf you're looking for a meeting recording API, consider checking out [Recall.ai](https://www.recall.ai/?utm_source=github&utm_medium=sponsorship&utm_campaign=fatedier-frp),\n\nan API that records Zoom, Google Meet, Microsoft Teams, in-person meetings, and more.\n\n</div>\n<!--gold sponsors end-->\n\n## What is frp?\n\nfrp is a fast reverse proxy that allows you to expose a local server located behind a NAT or firewall to the Internet. It currently supports **TCP** and **UDP**, as well as **HTTP** and **HTTPS** protocols, enabling requests to be forwarded to internal services via domain name.\n\nfrp also offers a P2P connect mode.\n\n## Table of Contents\n\n<!-- vim-markdown-toc GFM -->\n\n* [Development Status](#development-status)\n    * [About V2](#about-v2)\n* [Architecture](#architecture)\n* [Example Usage](#example-usage)\n    * [Access your computer in a LAN network via SSH](#access-your-computer-in-a-lan-network-via-ssh)\n    * [Multiple SSH services sharing the same port](#multiple-ssh-services-sharing-the-same-port)\n    * [Accessing Internal Web Services with Custom Domains in LAN](#accessing-internal-web-services-with-custom-domains-in-lan)\n    * [Forward DNS query requests](#forward-dns-query-requests)\n    * [Forward Unix Domain Socket](#forward-unix-domain-socket)\n    * [Expose a simple HTTP file server](#expose-a-simple-http-file-server)\n    * [Enable HTTPS for a local HTTP(S) service](#enable-https-for-a-local-https-service)\n    * [Expose your service privately](#expose-your-service-privately)\n    * [P2P Mode](#p2p-mode)\n* [Features](#features)\n    * [Configuration Files](#configuration-files)\n    * [Using Environment Variables](#using-environment-variables)\n    * [Split Configures Into Different Files](#split-configures-into-different-files)\n    * [Server Dashboard](#server-dashboard)\n    * [Client Admin UI](#client-admin-ui)\n    * [Monitor](#monitor)\n        * [Prometheus](#prometheus)\n    * [Authenticating the Client](#authenticating-the-client)\n        * [Token Authentication](#token-authentication)\n        * [OIDC Authentication](#oidc-authentication)\n    * [Encryption and Compression](#encryption-and-compression)\n        * [TLS](#tls)\n    * [Hot-Reloading frpc configuration](#hot-reloading-frpc-configuration)\n    * [Get proxy status from client](#get-proxy-status-from-client)\n    * [Only allowing certain ports on the server](#only-allowing-certain-ports-on-the-server)\n    * [Port Reuse](#port-reuse)\n    * [Bandwidth Limit](#bandwidth-limit)\n        * [For Each Proxy](#for-each-proxy)\n    * [TCP Stream Multiplexing](#tcp-stream-multiplexing)\n    * [Support KCP Protocol](#support-kcp-protocol)\n    * [Support QUIC Protocol](#support-quic-protocol)\n    * [Connection Pooling](#connection-pooling)\n    * [Load balancing](#load-balancing)\n    * [Service Health Check](#service-health-check)\n    * [Rewriting the HTTP Host Header](#rewriting-the-http-host-header)\n    * [Setting other HTTP Headers](#setting-other-http-headers)\n    * [Get Real IP](#get-real-ip)\n        * [HTTP X-Forwarded-For](#http-x-forwarded-for)\n        * [Proxy Protocol](#proxy-protocol)\n    * [Require HTTP Basic Auth (Password) for Web Services](#require-http-basic-auth-password-for-web-services)\n    * [Custom Subdomain Names](#custom-subdomain-names)\n    * [URL Routing](#url-routing)\n    * [TCP Port Multiplexing](#tcp-port-multiplexing)\n    * [Connecting to frps via PROXY](#connecting-to-frps-via-proxy)\n    * [Port range mapping](#port-range-mapping)\n    * [Client Plugins](#client-plugins)\n    * [Server Manage Plugins](#server-manage-plugins)\n    * [SSH Tunnel Gateway](#ssh-tunnel-gateway)\n    * [Virtual Network (VirtualNet)](#virtual-network-virtualnet)\n* [Feature Gates](#feature-gates)\n    * [Available Feature Gates](#available-feature-gates)\n    * [Enabling Feature Gates](#enabling-feature-gates)\n    * [Feature Lifecycle](#feature-lifecycle)\n* [Related Projects](#related-projects)\n* [Contributing](#contributing)\n* [Donation](#donation)\n    * [GitHub Sponsors](#github-sponsors)\n    * [PayPal](#paypal)\n\n<!-- vim-markdown-toc -->\n\n## Development Status\n\nfrp is currently under development. You can try the latest release version in the `master` branch, or use the `dev` branch to access the version currently in development.\n\nWe are currently working on version 2 and attempting to perform some code refactoring and improvements. However, please note that it will not be compatible with version 1.\n\nWe will transition from version 0 to version 1 at the appropriate time and will only accept bug fixes and improvements, rather than big feature requests.\n\n### About V2\n\nThe complexity and difficulty of the v2 version are much higher than anticipated. I can only work on its development during fragmented time periods, and the constant interruptions disrupt productivity significantly. Given this situation, we will continue to optimize and iterate on the current version until we have more free time to proceed with the major version overhaul.\n\nThe concept behind v2 is based on my years of experience and reflection in the cloud-native domain, particularly in K8s and ServiceMesh. Its core is a modernized four-layer and seven-layer proxy, similar to envoy. This proxy itself is highly scalable, not only capable of implementing the functionality of intranet penetration but also applicable to various other domains. Building upon this highly scalable core, we aim to implement all the capabilities of frp v1 while also addressing the functionalities that were previously unachievable or difficult to implement in an elegant manner. Furthermore, we will maintain efficient development and iteration capabilities.\n\nIn addition, I envision frp itself becoming a highly extensible system and platform, similar to how we can provide a range of extension capabilities based on K8s. In K8s, we can customize development according to enterprise needs, utilizing features such as CRD, controller mode, webhook, CSI, and CNI. In frp v1, we introduced the concept of server plugins, which implemented some basic extensibility. However, it relies on a simple HTTP protocol and requires users to start independent processes and manage them on their own. This approach is far from flexible and convenient, and real-world demands vary greatly. It is unrealistic to expect a non-profit open-source project maintained by a few individuals to meet everyone's needs.\n\nFinally, we acknowledge that the current design of modules such as configuration management, permission verification, certificate management, and API management is not modern enough. While we may carry out some optimizations in the v1 version, ensuring compatibility remains a challenging issue that requires a considerable amount of effort to address.\n\nWe sincerely appreciate your support for frp.\n\n## Architecture\n\n![architecture](/doc/pic/architecture.png)\n\n## Example Usage\n\nTo begin, download the latest program for your operating system and architecture from the [Release](https://github.com/fatedier/frp/releases) page.\n\nNext, place the `frps` binary and server configuration file on Server A, which has a public IP address.\n\nFinally, place the `frpc` binary and client configuration file on Server B, which is located on a LAN that cannot be directly accessed from the public internet.\n\nSome antiviruses improperly mark frpc as malware and delete it. This is due to frp being a networking tool capable of creating reverse proxies. Antiviruses sometimes flag reverse proxies due to their ability to bypass firewall port restrictions. If you are using antivirus, then you may need to whitelist/exclude frpc in your antivirus settings to avoid accidental quarantine/deletion. See [issue 3637](https://github.com/fatedier/frp/issues/3637) for more details.\n\n### Access your computer in a LAN network via SSH\n\n1. Modify `frps.toml` on server A by setting the `bindPort` for frp clients to connect to:\n\n  ```toml\n  # frps.toml\n  bindPort = 7000\n  ```\n\n2. Start `frps` on server A:\n\n  `./frps -c ./frps.toml`\n\n3. Modify `frpc.toml` on server B and set the `serverAddr` field to the public IP address of your frps server:\n\n  ```toml\n  # frpc.toml\n  serverAddr = \"x.x.x.x\"\n  serverPort = 7000\n\n  [[proxies]]\n  name = \"ssh\"\n  type = \"tcp\"\n  localIP = \"127.0.0.1\"\n  localPort = 22\n  remotePort = 6000\n  ```\n\nNote that the `localPort` (listened on the client) and `remotePort` (exposed on the server) are used for traffic going in and out of the frp system, while the `serverPort` is used for communication between frps and frpc.\n\n4. Start `frpc` on server B:\n\n  `./frpc -c ./frpc.toml`\n\n5. To access server B from another machine through server A via SSH (assuming the username is `test`), use the following command:\n\n  `ssh -oPort=6000 test@x.x.x.x`\n\n### Multiple SSH services sharing the same port\n\nThis example implements multiple SSH services exposed through the same port using a proxy of type tcpmux. Similarly, as long as the client supports the HTTP Connect proxy connection method, port reuse can be achieved in this way.\n\n1. Deploy frps on a machine with a public IP and modify the frps.toml file. Here is a simplified configuration:\n\n  ```toml\n  bindPort = 7000\n  tcpmuxHTTPConnectPort = 5002\n  ```\n\n2. Deploy frpc on the internal machine A with the following configuration:\n\n  ```toml\n  serverAddr = \"x.x.x.x\"\n  serverPort = 7000\n\n  [[proxies]]\n  name = \"ssh1\"\n  type = \"tcpmux\"\n  multiplexer = \"httpconnect\"\n  customDomains = [\"machine-a.example.com\"]\n  localIP = \"127.0.0.1\"\n  localPort = 22\n  ```\n\n3. Deploy another frpc on the internal machine B with the following configuration:\n\n  ```toml\n  serverAddr = \"x.x.x.x\"\n  serverPort = 7000\n\n  [[proxies]]\n  name = \"ssh2\"\n  type = \"tcpmux\"\n  multiplexer = \"httpconnect\"\n  customDomains = [\"machine-b.example.com\"]\n  localIP = \"127.0.0.1\"\n  localPort = 22\n  ```\n\n4. To access internal machine A using SSH ProxyCommand, assuming the username is \"test\":\n\n  `ssh -o 'proxycommand socat - PROXY:x.x.x.x:%h:%p,proxyport=5002' test@machine-a.example.com`\n\n5. To access internal machine B, the only difference is the domain name, assuming the username is \"test\":\n\n  `ssh -o 'proxycommand socat - PROXY:x.x.x.x:%h:%p,proxyport=5002' test@machine-b.example.com`\n\n### Accessing Internal Web Services with Custom Domains in LAN\n\nSometimes we need to expose a local web service behind a NAT network to others for testing purposes with our own domain name.\n\nUnfortunately, we cannot resolve a domain name to a local IP. However, we can use frp to expose an HTTP(S) service.\n\n1. Modify `frps.toml` and set the HTTP port for vhost to 8080:\n\n  ```toml\n  # frps.toml\n  bindPort = 7000\n  vhostHTTPPort = 8080\n  ```\n\n  If you want to configure an https proxy, you need to set up the `vhostHTTPSPort`.\n\n2. Start `frps`:\n\n  `./frps -c ./frps.toml`\n\n3. Modify `frpc.toml` and set `serverAddr` to the IP address of the remote frps server. Specify the `localPort` of your web service:\n\n  ```toml\n  # frpc.toml\n  serverAddr = \"x.x.x.x\"\n  serverPort = 7000\n\n  [[proxies]]\n  name = \"web\"\n  type = \"http\"\n  localPort = 80\n  customDomains = [\"www.example.com\"]\n  ```\n\n4. Start `frpc`:\n\n  `./frpc -c ./frpc.toml`\n\n5. Map the A record of `www.example.com` to either the public IP of the remote frps server or a CNAME record pointing to your original domain.\n\n6. Visit your local web service using url `http://www.example.com:8080`.\n\n### Forward DNS query requests\n\n1. Modify `frps.toml`:\n\n  ```toml\n  # frps.toml\n  bindPort = 7000\n  ```\n\n2. Start `frps`:\n\n  `./frps -c ./frps.toml`\n\n3. Modify `frpc.toml` and set `serverAddr` to the IP address of the remote frps server. Forward DNS query requests to the Google Public DNS server `8.8.8.8:53`:\n\n  ```toml\n  # frpc.toml\n  serverAddr = \"x.x.x.x\"\n  serverPort = 7000\n\n  [[proxies]]\n  name = \"dns\"\n  type = \"udp\"\n  localIP = \"8.8.8.8\"\n  localPort = 53\n  remotePort = 6000\n  ```\n\n4. Start frpc:\n\n  `./frpc -c ./frpc.toml`\n\n5. Test DNS resolution using the `dig` command:\n\n  `dig @x.x.x.x -p 6000 www.google.com`\n\n### Forward Unix Domain Socket\n\nExpose a Unix domain socket (e.g. the Docker daemon socket) as TCP.\n\nConfigure `frps` as above.\n\n1. Start `frpc` with the following configuration:\n\n  ```toml\n  # frpc.toml\n  serverAddr = \"x.x.x.x\"\n  serverPort = 7000\n\n  [[proxies]]\n  name = \"unix_domain_socket\"\n  type = \"tcp\"\n  remotePort = 6000\n  [proxies.plugin]\n  type = \"unix_domain_socket\"\n  unixPath = \"/var/run/docker.sock\"\n  ```\n\n2. Test the configuration by getting the docker version using `curl`:\n\n  `curl http://x.x.x.x:6000/version`\n\n### Expose a simple HTTP file server\n\nExpose a simple HTTP file server to access files stored in the LAN from the public Internet.\n\nConfigure `frps` as described above, then:\n\n1. Start `frpc` with the following configuration:\n\n  ```toml\n  # frpc.toml\n  serverAddr = \"x.x.x.x\"\n  serverPort = 7000\n\n  [[proxies]]\n  name = \"test_static_file\"\n  type = \"tcp\"\n  remotePort = 6000\n  [proxies.plugin]\n  type = \"static_file\"\n  localPath = \"/tmp/files\"\n  stripPrefix = \"static\"\n  httpUser = \"abc\"\n  httpPassword = \"abc\"\n  ```\n\n2. Visit `http://x.x.x.x:6000/static/` from your browser and specify correct username and password to view files in `/tmp/files` on the `frpc` machine.\n\n### Enable HTTPS for a local HTTP(S) service\n\nYou may substitute `https2https` for the plugin, and point the `localAddr` to a HTTPS endpoint.\n\n1. Start `frpc` with the following configuration:\n\n  ```toml\n  # frpc.toml\n  serverAddr = \"x.x.x.x\"\n  serverPort = 7000\n\n  [[proxies]]\n  name = \"test_https2http\"\n  type = \"https\"\n  customDomains = [\"test.example.com\"]\n\n  [proxies.plugin]\n  type = \"https2http\"\n  localAddr = \"127.0.0.1:80\"\n  crtPath = \"./server.crt\"\n  keyPath = \"./server.key\"\n  hostHeaderRewrite = \"127.0.0.1\"\n  requestHeaders.set.x-from-where = \"frp\"\n  ```\n\n2. Visit `https://test.example.com`.\n\n### Expose your service privately\n\nTo mitigate risks associated with exposing certain services directly to the public network, STCP (Secret TCP) mode requires a preshared key to be used for access to the service from other clients.\n\nConfigure `frps` same as above.\n\n1. Start `frpc` on machine B with the following config. This example is for exposing the SSH service (port 22), and note the `secretKey` field for the preshared key, and that the `remotePort` field is removed here:\n\n  ```toml\n  # frpc.toml\n  serverAddr = \"x.x.x.x\"\n  serverPort = 7000\n\n  [[proxies]]\n  name = \"secret_ssh\"\n  type = \"stcp\"\n  secretKey = \"abcdefg\"\n  localIP = \"127.0.0.1\"\n  localPort = 22\n  ```\n\n2. Start another `frpc` (typically on another machine C) with the following config to access the SSH service with a security key (`secretKey` field):\n\n  ```toml\n  # frpc.toml\n  serverAddr = \"x.x.x.x\"\n  serverPort = 7000\n\n  [[visitors]]\n  name = \"secret_ssh_visitor\"\n  type = \"stcp\"\n  serverName = \"secret_ssh\"\n  secretKey = \"abcdefg\"\n  bindAddr = \"127.0.0.1\"\n  bindPort = 6000\n  ```\n\n3. On machine C, connect to SSH on machine B, using this command:\n\n  `ssh -oPort=6000 127.0.0.1`\n\n### P2P Mode\n\n**xtcp** is designed to transmit large amounts of data directly between clients. A frps server is still needed, as P2P here only refers to the actual data transmission.\n\nNote that it may not work with all types of NAT devices. You might want to fallback to stcp if xtcp doesn't work.\n\n1. Start `frpc` on machine B, and expose the SSH port. Note that the `remotePort` field is removed:\n\n  ```toml\n  # frpc.toml\n  serverAddr = \"x.x.x.x\"\n  serverPort = 7000\n  # set up a new stun server if the default one is not available.\n  # natHoleStunServer = \"xxx\"\n\n  [[proxies]]\n  name = \"p2p_ssh\"\n  type = \"xtcp\"\n  secretKey = \"abcdefg\"\n  localIP = \"127.0.0.1\"\n  localPort = 22\n  ```\n\n2. Start another `frpc` (typically on another machine C) with the configuration to connect to SSH using P2P mode:\n\n  ```toml\n  # frpc.toml\n  serverAddr = \"x.x.x.x\"\n  serverPort = 7000\n  # set up a new stun server if the default one is not available.\n  # natHoleStunServer = \"xxx\"\n\n  [[visitors]]\n  name = \"p2p_ssh_visitor\"\n  type = \"xtcp\"\n  serverName = \"p2p_ssh\"\n  secretKey = \"abcdefg\"\n  bindAddr = \"127.0.0.1\"\n  bindPort = 6000\n  # when automatic tunnel persistence is required, set it to true\n  keepTunnelOpen = false\n  ```\n\n3. On machine C, connect to SSH on machine B, using this command:\n\n  `ssh -oPort=6000 127.0.0.1`\n\n## Features\n\n### Configuration Files\n\nSince v0.52.0, we support TOML, YAML, and JSON for configuration. Please note that INI is deprecated and will be removed in future releases. New features will only be available in TOML, YAML, or JSON. Users wanting these new features should switch their configuration format accordingly.\n\nRead the full example configuration files to find out even more features not described here.\n\nExamples use TOML format, but you can still use YAML or JSON.\n\nThese configuration files is for reference only. Please do not use this configuration directly to run the program as it may have various issues.\n\n[Full configuration file for frps (Server)](./conf/frps_full_example.toml)\n\n[Full configuration file for frpc (Client)](./conf/frpc_full_example.toml)\n\n### Using Environment Variables\n\nEnvironment variables can be referenced in the configuration file, using Go's standard format:\n\n```toml\n# frpc.toml\nserverAddr = \"{{ .Envs.FRP_SERVER_ADDR }}\"\nserverPort = 7000\n\n[[proxies]]\nname = \"ssh\"\ntype = \"tcp\"\nlocalIP = \"127.0.0.1\"\nlocalPort = 22\nremotePort = {{ .Envs.FRP_SSH_REMOTE_PORT }}\n```\n\nWith the config above, variables can be passed into `frpc` program like this:\n\n```\nexport FRP_SERVER_ADDR=x.x.x.x\nexport FRP_SSH_REMOTE_PORT=6000\n./frpc -c ./frpc.toml\n```\n\n`frpc` will render configuration file template using OS environment variables. Remember to prefix your reference with `.Envs`.\n\n### Split Configures Into Different Files\n\nYou can split multiple proxy configs into different files and include them in the main file.\n\n```toml\n# frpc.toml\nserverAddr = \"x.x.x.x\"\nserverPort = 7000\nincludes = [\"./confd/*.toml\"]\n```\n\n```toml\n# ./confd/test.toml\n\n[[proxies]]\nname = \"ssh\"\ntype = \"tcp\"\nlocalIP = \"127.0.0.1\"\nlocalPort = 22\nremotePort = 6000\n```\n\n### Server Dashboard\n\nCheck frp's status and proxies' statistics information by Dashboard.\n\nConfigure a port for dashboard to enable this feature:\n\n```toml\n# The default value is 127.0.0.1. Change it to 0.0.0.0 when you want to access it from a public network.\nwebServer.addr = \"0.0.0.0\"\nwebServer.port = 7500\n# dashboard's username and password are both optional\nwebServer.user = \"admin\"\nwebServer.password = \"admin\"\n```\n\nThen visit `http://[serverAddr]:7500` to see the dashboard, with username and password both being `admin`.\n\nAdditionally, you can use HTTPS port by using your domains wildcard or normal SSL certificate:\n\n```toml\nwebServer.port = 7500\n# dashboard's username and password are both optional\nwebServer.user = \"admin\"\nwebServer.password = \"admin\"\nwebServer.tls.certFile = \"server.crt\"\nwebServer.tls.keyFile = \"server.key\"\n```\n\nThen visit `https://[serverAddr]:7500` to see the dashboard in secure HTTPS connection, with username and password both being `admin`.\n\n![dashboard](/doc/pic/dashboard.png)\n\n### Client Admin UI\n\nThe Client Admin UI helps you check and manage frpc's configuration.\n\nConfigure an address for admin UI to enable this feature:\n\n```toml\nwebServer.addr = \"127.0.0.1\"\nwebServer.port = 7400\nwebServer.user = \"admin\"\nwebServer.password = \"admin\"\n```\n\nThen visit `http://127.0.0.1:7400` to see admin UI, with username and password both being `admin`.\n\n### Monitor\n\nWhen web server is enabled, frps will save monitor data in cache for 7 days. It will be cleared after process restart.\n\nPrometheus is also supported.\n\n#### Prometheus\n\nEnable dashboard first, then configure `enablePrometheus = true` in `frps.toml`.\n\n`http://{dashboard_addr}/metrics` will provide prometheus monitor data.\n\n### Authenticating the Client\n\nThere are 2 authentication methods to authenticate frpc with frps. \n\nYou can decide which one to use by configuring `auth.method` in `frpc.toml` and `frps.toml`, the default one is token.\n\nConfiguring `auth.additionalScopes = [\"HeartBeats\"]` will use the configured authentication method to add and validate authentication on every heartbeat between frpc and frps.\n\nConfiguring `auth.additionalScopes = [\"NewWorkConns\"]` will do the same for every new work connection between frpc and frps.\n\n#### Token Authentication\n\nWhen specifying `auth.method = \"token\"` in `frpc.toml` and `frps.toml` - token based authentication will be used.\n\nMake sure to specify the same `auth.token` in `frps.toml` and `frpc.toml` for frpc to pass frps validation\n\n##### Token Source\n\nfrp supports reading authentication tokens from external sources using the `tokenSource` configuration. Currently, file-based token source is supported.\n\n**File-based token source:**\n\n```toml\n# frpc.toml\nauth.method = \"token\"\nauth.tokenSource.type = \"file\"\nauth.tokenSource.file.path = \"/path/to/token/file\"\n```\n\nThe token will be read from the specified file at startup. This is useful for scenarios where tokens are managed by external systems or need to be kept separate from configuration files for security reasons.\n\n#### OIDC Authentication\n\nWhen specifying `auth.method = \"oidc\"` in `frpc.toml` and `frps.toml` - OIDC based authentication will be used.\n\nOIDC stands for OpenID Connect, and the flow used is called [Client Credentials Grant](https://tools.ietf.org/html/rfc6749#section-4.4).\n\nTo use this authentication type - configure `frpc.toml` and `frps.toml` as follows:\n\n```toml\n# frps.toml\nauth.method = \"oidc\"\nauth.oidc.issuer = \"https://example-oidc-issuer.com/\"\nauth.oidc.audience = \"https://oidc-audience.com/.default\"\n```\n\n```toml\n# frpc.toml\nauth.method = \"oidc\"\nauth.oidc.clientID = \"98692467-37de-409a-9fac-bb2585826f18\" # Replace with OIDC client ID\nauth.oidc.clientSecret = \"oidc_secret\"\nauth.oidc.audience = \"https://oidc-audience.com/.default\"\nauth.oidc.tokenEndpointURL = \"https://example-oidc-endpoint.com/oauth2/v2.0/token\"\n```\n\n### Encryption and Compression\n\nThe features are off by default. You can turn on encryption and/or compression:\n\n```toml\n# frpc.toml\n\n[[proxies]]\nname = \"ssh\"\ntype = \"tcp\"\nlocalPort = 22\nremotePort = 6000\ntransport.useEncryption = true\ntransport.useCompression = true\n```\n\n#### TLS\n\nSince v0.50.0, the default value of `transport.tls.enable` and `transport.tls.disableCustomTLSFirstByte` has been changed to true, and tls is enabled by default.\n\nFor port multiplexing, frp sends a first byte `0x17` to dial a TLS connection. This only takes effect when you set `transport.tls.disableCustomTLSFirstByte` to false.\n\nTo **enforce** `frps` to only accept TLS connections - configure `transport.tls.force = true` in `frps.toml`. **This is optional.**\n\n**`frpc` TLS settings:**\n\n```toml\ntransport.tls.enable = true\ntransport.tls.certFile = \"certificate.crt\"\ntransport.tls.keyFile = \"certificate.key\"\ntransport.tls.trustedCaFile = \"ca.crt\"\n```\n\n**`frps` TLS settings:**\n\n```toml\ntransport.tls.force = true\ntransport.tls.certFile = \"certificate.crt\"\ntransport.tls.keyFile = \"certificate.key\"\ntransport.tls.trustedCaFile = \"ca.crt\"\n```\n\nYou will need **a root CA cert** and **at least one SSL/TLS certificate**. It **can** be self-signed or regular (such as Let's Encrypt or another SSL/TLS certificate provider).\n\nIf you using `frp` via IP address and not hostname, make sure to set the appropriate IP address in the Subject Alternative Name (SAN) area when generating SSL/TLS Certificates.\n\nGiven an example:\n\n* Prepare openssl config file. It exists at `/etc/pki/tls/openssl.cnf` in Linux System and `/System/Library/OpenSSL/openssl.cnf` in MacOS, and you can copy it to current path, like `cp /etc/pki/tls/openssl.cnf ./my-openssl.cnf`. If not, you can build it by yourself, like:\n```\ncat > my-openssl.cnf << EOF\n[ ca ]\ndefault_ca = CA_default\n[ CA_default ]\nx509_extensions = usr_cert\n[ req ]\ndefault_bits        = 2048\ndefault_md          = sha256\ndefault_keyfile     = privkey.pem\ndistinguished_name  = req_distinguished_name\nattributes          = req_attributes\nx509_extensions     = v3_ca\nstring_mask         = utf8only\n[ req_distinguished_name ]\n[ req_attributes ]\n[ usr_cert ]\nbasicConstraints       = CA:FALSE\nnsComment              = \"OpenSSL Generated Certificate\"\nsubjectKeyIdentifier   = hash\nauthorityKeyIdentifier = keyid,issuer\n[ v3_ca ]\nsubjectKeyIdentifier   = hash\nauthorityKeyIdentifier = keyid:always,issuer\nbasicConstraints       = CA:true\nEOF\n```\n\n* build ca certificates:\n```\nopenssl genrsa -out ca.key 2048\nopenssl req -x509 -new -nodes -key ca.key -subj \"/CN=example.ca.com\" -days 5000 -out ca.crt\n```\n\n* build frps certificates:\n```\nopenssl genrsa -out server.key 2048\n\nopenssl req -new -sha256 -key server.key \\\n    -subj \"/C=XX/ST=DEFAULT/L=DEFAULT/O=DEFAULT/CN=server.com\" \\\n    -reqexts SAN \\\n    -config <(cat my-openssl.cnf <(printf \"\\n[SAN]\\nsubjectAltName=DNS:localhost,IP:127.0.0.1,DNS:example.server.com\")) \\\n    -out server.csr\n\nopenssl x509 -req -days 365 -sha256 \\\n\t-in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n\t-extfile <(printf \"subjectAltName=DNS:localhost,IP:127.0.0.1,DNS:example.server.com\") \\\n\t-out server.crt\n```\n\n* build frpc certificatesï¼š\n```\nopenssl genrsa -out client.key 2048\nopenssl req -new -sha256 -key client.key \\\n    -subj \"/C=XX/ST=DEFAULT/L=DEFAULT/O=DEFAULT/CN=client.com\" \\\n    -reqexts SAN \\\n    -config <(cat my-openssl.cnf <(printf \"\\n[SAN]\\nsubjectAltName=DNS:client.com,DNS:example.client.com\")) \\\n    -out client.csr\n\nopenssl x509 -req -days 365 -sha256 \\\n    -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n\t-extfile <(printf \"subjectAltName=DNS:client.com,DNS:example.client.com\") \\\n\t-out client.crt\n```\n\n### Hot-Reloading frpc configuration\n\nThe `webServer` fields are required for enabling HTTP API:\n\n```toml\n# frpc.toml\nwebServer.addr = \"127.0.0.1\"\nwebServer.port = 7400\n```\n\nThen run command `frpc reload -c ./frpc.toml` and wait for about 10 seconds to let `frpc` create or update or remove proxies.\n\n**Note that global client parameters won't be modified except 'start'.**\n\nYou can run command `frpc verify -c ./frpc.toml` before reloading to check if there are config errors.\n\n### Get proxy status from client\n\nUse `frpc status -c ./frpc.toml` to get status of all proxies. The `webServer` fields are required for enabling HTTP API.\n\n### Only allowing certain ports on the server\n\n`allowPorts` in `frps.toml` is used to avoid abuse of ports:\n\n```toml\n# frps.toml\nallowPorts = [\n  { start = 2000, end = 3000 },\n  { single = 3001 },\n  { single = 3003 },\n  { start = 4000, end = 50000 }\n]\n```\n\n### Port Reuse\n\n`vhostHTTPPort` and `vhostHTTPSPort` in frps can use same port with `bindPort`. frps will detect the connection's protocol and handle it correspondingly.\n\nWhat you need to pay attention to is that if you want to configure `vhostHTTPSPort` and `bindPort` to the same port, you need to first set `transport.tls.disableCustomTLSFirstByte` to false.\n\nWe would like to try to allow multiple proxies bind a same remote port with different protocols in the future.\n\n### Bandwidth Limit\n\n#### For Each Proxy\n\n```toml\n# frpc.toml\n\n[[proxies]]\nname = \"ssh\"\ntype = \"tcp\"\nlocalPort = 22\nremotePort = 6000\ntransport.bandwidthLimit = \"1MB\"\n```\n\nSet `transport.bandwidthLimit` in each proxy's configure to enable this feature. Supported units are `MB` and `KB`.\n\nSet `transport.bandwidthLimitMode` to `client` or `server` to limit bandwidth on the client or server side. Default is `client`.\n\n### TCP Stream Multiplexing\n\nfrp supports tcp stream multiplexing since v0.10.0 like HTTP2 Multiplexing, in which case all logic connections to the same frpc are multiplexed into the same TCP connection.\n\nYou can disable this feature by modify `frps.toml` and `frpc.toml`:\n\n```toml\n# frps.toml and frpc.toml, must be same\ntransport.tcpMux = false\n```\n\n### Support KCP Protocol\n\nKCP is a fast and reliable protocol that can achieve the transmission effect of a reduction of the average latency by 30% to 40% and reduction of the maximum delay by a factor of three, at the cost of 10% to 20% more bandwidth wasted than TCP.\n\nKCP mode uses UDP as the underlying transport. Using KCP in frp:\n\n1. Enable KCP in frps:\n\n  ```toml\n  # frps.toml\n  bindPort = 7000\n  # Specify a UDP port for KCP.\n  kcpBindPort = 7000\n  ```\n\n  The `kcpBindPort` number can be the same number as `bindPort`, since `bindPort` field specifies a TCP port.\n\n2. Configure `frpc.toml` to use KCP to connect to frps:\n\n  ```toml\n  # frpc.toml\n  serverAddr = \"x.x.x.x\"\n  # Same as the 'kcpBindPort' in frps.toml\n  serverPort = 7000\n  transport.protocol = \"kcp\"\n  ```\n\n### Support QUIC Protocol\n\nQUIC is a new multiplexed transport built on top of UDP.\n\nUsing QUIC in frp:\n\n1. Enable QUIC in frps:\n\n  ```toml\n  # frps.toml\n  bindPort = 7000\n  # Specify a UDP port for QUIC.\n  quicBindPort = 7000\n  ```\n\n  The `quicBindPort` number can be the same number as `bindPort`, since `bindPort` field specifies a TCP port.\n\n2. Configure `frpc.toml` to use QUIC to connect to frps:\n\n  ```toml\n  # frpc.toml\n  serverAddr = \"x.x.x.x\"\n  # Same as the 'quicBindPort' in frps.toml\n  serverPort = 7000\n  transport.protocol = \"quic\"\n  ```\n\n### Connection Pooling\n\nBy default, frps creates a new frpc connection to the backend service upon a user request. With connection pooling, frps keeps a certain number of pre-established connections, reducing the time needed to establish a connection.\n\nThis feature is suitable for a large number of short connections.\n\n1. Configure the limit of pool count each proxy can use in `frps.toml`:\n\n  ```toml\n  # frps.toml\n  transport.maxPoolCount = 5\n  ```\n\n2. Enable and specify the number of connection pool:\n\n  ```toml\n  # frpc.toml\n  transport.poolCount = 1\n  ```\n\n### Load balancing\n\nLoad balancing is supported by `group`.\n\nThis feature is only available for types `tcp`, `http`, `tcpmux` now.\n\n```toml\n# frpc.toml\n\n[[proxies]]\nname = \"test1\"\ntype = \"tcp\"\nlocalPort = 8080\nremotePort = 80\nloadBalancer.group = \"web\"\nloadBalancer.groupKey = \"123\"\n\n[[proxies]]\nname = \"test2\"\ntype = \"tcp\"\nlocalPort = 8081\nremotePort = 80\nloadBalancer.group = \"web\"\nloadBalancer.groupKey = \"123\"\n```\n\n`loadBalancer.groupKey` is used for authentication.\n\nConnections to port 80 will be dispatched to proxies in the same group randomly.\n\nFor type `tcp`, `remotePort` in the same group should be the same.\n\nFor type `http`, `customDomains`, `subdomain`, `locations` should be the same.\n\n### Service Health Check\n\nHealth check feature can help you achieve high availability with load balancing.\n\nAdd `healthCheck.type = \"tcp\"` or `healthCheck.type = \"http\"` to enable health check.\n\nWith health check type **tcp**, the service port will be pinged (TCPing):\n\n```toml\n# frpc.toml\n\n[[proxies]]\nname = \"test1\"\ntype = \"tcp\"\nlocalPort = 22\nremotePort = 6000\n# Enable TCP health check\nhealthCheck.type = \"tcp\"\n# TCPing timeout seconds\nhealthCheck.timeoutSeconds = 3\n# If health check failed 3 times in a row, the proxy will be removed from frps\nhealthCheck.maxFailed = 3\n# A health check every 10 seconds\nhealthCheck.intervalSeconds = 10\n```\n\nWith health check type **http**, an HTTP request will be sent to the service and an HTTP 2xx OK response is expected:\n\n```toml\n# frpc.toml\n\n[[proxies]]\nname = \"web\"\ntype = \"http\"\nlocalIP = \"127.0.0.1\"\nlocalPort = 80\ncustomDomains = [\"test.example.com\"]\n# Enable HTTP health check\nhealthCheck.type = \"http\"\n# frpc will send a GET request to '/status'\n# and expect an HTTP 2xx OK response\nhealthCheck.path = \"/status\"\nhealthCheck.timeoutSeconds = 3\nhealthCheck.maxFailed = 3\nhealthCheck.intervalSeconds = 10\n```\n\n### Rewriting the HTTP Host Header\n\nBy default frp does not modify the tunneled HTTP requests at all as it's a byte-for-byte copy.\n\nHowever, speaking of web servers and HTTP requests, your web server might rely on the `Host` HTTP header to determine the website to be accessed. frp can rewrite the `Host` header when forwarding the HTTP requests, with the `hostHeaderRewrite` field:\n\n```toml\n# frpc.toml\n\n[[proxies]]\nname = \"web\"\ntype = \"http\"\nlocalPort = 80\ncustomDomains = [\"test.example.com\"]\nhostHeaderRewrite = \"dev.example.com\"\n```\n\nThe HTTP request will have the `Host` header rewritten to `Host: dev.example.com` when it reaches the actual web server, although the request from the browser probably has `Host: test.example.com`.\n\n### Setting other HTTP Headers\n\nSimilar to `Host`, You can override other HTTP request and response headers with proxy type `http`.\n\n```toml\n# frpc.toml\n\n[[proxies]]\nname = \"web\"\ntype = \"http\"\nlocalPort = 80\ncustomDomains = [\"test.example.com\"]\nhostHeaderRewrite = \"dev.example.com\"\nrequestHeaders.set.x-from-where = \"frp\"\nresponseHeaders.set.foo = \"bar\"\n```\n\nIn this example, it will set header `x-from-where: frp` in the HTTP request and `foo: bar` in the HTTP response.\n\n### Get Real IP\n\n#### HTTP X-Forwarded-For\n\nThis feature is for `http` proxies or proxies with the `https2http` and `https2https` plugins enabled.\n\nYou can get user's real IP from HTTP request headers `X-Forwarded-For`.\n\n#### Proxy Protocol\n\nfrp supports Proxy Protocol to send user's real IP to local services.\n\nHere is an example for https service:\n\n```toml\n# frpc.toml\n\n[[proxies]]\nname = \"web\"\ntype = \"https\"\nlocalPort = 443\ncustomDomains = [\"test.example.com\"]\n\n# now v1 and v2 are supported\ntransport.proxyProtocolVersion = \"v2\"\n```\n\nYou can enable Proxy Protocol support in nginx to expose user's real IP in HTTP header `X-Real-IP`, and then read `X-Real-IP` header in your web service for the real IP.\n\n### Require HTTP Basic Auth (Password) for Web Services\n\nAnyone who can guess your tunnel URL can access your local web server unless you protect it with a password.\n\nThis enforces HTTP Basic Auth on all requests with the username and password specified in frpc's configure file.\n\nIt can only be enabled when proxy type is http.\n\n```toml\n# frpc.toml\n\n[[proxies]]\nname = \"web\"\ntype = \"http\"\nlocalPort = 80\ncustomDomains = [\"test.example.com\"]\nhttpUser = \"abc\"\nhttpPassword = \"abc\"\n```\n\nVisit `http://test.example.com` in the browser and now you are prompted to enter the username and password.\n\n### Custom Subdomain Names\n\nIt is convenient to use `subdomain` configure for http and https types when many people share one frps server.\n\n```toml\n# frps.toml\nsubDomainHost = \"frps.com\"\n```\n\nResolve `*.frps.com` to the frps server's IP. This is usually called a Wildcard DNS record.\n\n```toml\n# frpc.toml\n\n[[proxies]]\nname = \"web\"\ntype = \"http\"\nlocalPort = 80\nsubdomain = \"test\"\n```\n\nNow you can visit your web service on `test.frps.com`.\n\nNote that if `subdomainHost` is not empty, `customDomains` should not be the subdomain of `subdomainHost`.\n\n### URL Routing\n\nfrp supports forwarding HTTP requests to different backend web services by url routing.\n\n`locations` specifies the prefix of URL used for routing. frps first searches for the most specific prefix location given by literal strings regardless of the listed order.\n\n```toml\n# frpc.toml\n\n[[proxies]]\nname = \"web01\"\ntype = \"http\"\nlocalPort = 80\ncustomDomains = [\"web.example.com\"]\nlocations = [\"/\"]\n\n[[proxies]]\nname = \"web02\"\ntype = \"http\"\nlocalPort = 81\ncustomDomains = [\"web.example.com\"]\nlocations = [\"/news\", \"/about\"]\n```\n\nHTTP requests with URL prefix `/news` or `/about` will be forwarded to **web02** and other requests to **web01**.\n\n### TCP Port Multiplexing\n\nfrp supports receiving TCP sockets directed to different proxies on a single port on frps, similar to `vhostHTTPPort` and `vhostHTTPSPort`.\n\nThe only supported TCP port multiplexing method available at the moment is `httpconnect` - HTTP CONNECT tunnel.\n\nWhen setting `tcpmuxHTTPConnectPort` to anything other than 0 in frps, frps will listen on this port for HTTP CONNECT requests.\n\nThe host of the HTTP CONNECT request will be used to match the proxy in frps. Proxy hosts can be configured in frpc by configuring `customDomains` and / or `subdomain` under `tcpmux` proxies, when `multiplexer = \"httpconnect\"`.\n\nFor example:\n\n```toml\n# frps.toml\nbindPort = 7000\ntcpmuxHTTPConnectPort = 1337\n```\n\n```toml\n# frpc.toml\nserverAddr = \"x.x.x.x\"\nserverPort = 7000\n\n[[proxies]]\nname = \"proxy1\"\ntype = \"tcpmux\"\nmultiplexer = \"httpconnect\"\ncustomDomains = [\"test1\"]\nlocalPort = 80\n\n[[proxies]]\nname = \"proxy2\"\ntype = \"tcpmux\"\nmultiplexer = \"httpconnect\"\ncustomDomains = [\"test2\"]\nlocalPort = 8080\n```\n\nIn the above configuration - frps can be contacted on port 1337 with a HTTP CONNECT header such as:\n\n```\nCONNECT test1 HTTP/1.1\\r\\n\\r\\n\n```\nand the connection will be routed to `proxy1`.\n\n### Connecting to frps via PROXY\n\nfrpc can connect to frps through proxy if you set OS environment variable `HTTP_PROXY`, or if `transport.proxyURL` is set in frpc.toml file.\n\nIt only works when protocol is tcp.\n\n```toml\n# frpc.toml\nserverAddr = \"x.x.x.x\"\nserverPort = 7000\ntransport.proxyURL = \"http://user:pwd@192.168.1.128:8080\"\n```\n\n### Port range mapping\n\n*Added in v0.56.0*\n\nWe can use the range syntax of Go template combined with the built-in `parseNumberRangePair` function to achieve port range mapping.\n\nThe following example, when run, will create 8 proxies named `test-6000, test-6001 ... test-6007`, each mapping the remote port to the local port.\n\n```\n{{- range $_, $v := parseNumberRangePair \"6000-6006,6007\" \"6000-6006,6007\" }}\n[[proxies]]\nname = \"tcp-{{ $v.First }}\"\ntype = \"tcp\"\nlocalPort = {{ $v.First }}\nremotePort = {{ $v.Second }}\n{{- end }}\n```\n\n### Client Plugins\n\nfrpc only forwards requests to local TCP or UDP ports by default.\n\nPlugins are used for providing rich features. There are built-in plugins such as `unix_domain_socket`, `http_proxy`, `socks5`, `static_file`, `http2https`, `https2http`, `https2https` and you can see [example usage](#example-usage).\n\nUsing plugin **http_proxy**:\n\n```toml\n# frpc.toml\n\n[[proxies]]\nname = \"http_proxy\"\ntype = \"tcp\"\nremotePort = 6000\n[proxies.plugin]\ntype = \"http_proxy\"\nhttpUser = \"abc\"\nhttpPassword = \"abc\"\n```\n\n`httpUser` and `httpPassword` are configuration parameters used in `http_proxy` plugin.\n\n### Server Manage Plugins\n\nRead the [document](/doc/server_plugin.md).\n\nFind more plugins in [gofrp/plugin](https://github.com/gofrp/plugin).\n\n### SSH Tunnel Gateway\n\n*added in v0.53.0*\n\nfrp supports listening to an SSH port on the frps side and achieves TCP protocol proxying through the SSH -R protocol, without relying on frpc.\n\n```toml\n# frps.toml\nsshTunnelGateway.bindPort = 2200\n```\n\nWhen running `./frps -c frps.toml`, a private key file named `.autogen_ssh_key` will be automatically created in the current working directory. This generated private key file will be used by the SSH server in frps.\n\nExecuting the command\n\n```bash\nssh -R :80:127.0.0.1:8080 v0@{frp address} -p 2200 tcp --proxy_name \"test-tcp\" --remote_port 9090\n```\n\nsets up a proxy on frps that forwards the local 8080 service to the port 9090.\n\n```bash\nfrp (via SSH) (Ctrl+C to quit)\n\nUser:\nProxyName: test-tcp\nType: tcp\nRemoteAddress: :9090\n```\n\nThis is equivalent to:\n\n```bash\nfrpc tcp --proxy_name \"test-tcp\" --local_ip 127.0.0.1 --local_port 8080 --remote_port 9090\n```\n\nPlease refer to this [document](/doc/ssh_tunnel_gateway.md) for more information.\n\n### Virtual Network (VirtualNet)\n\n*Alpha feature added in v0.62.0*\n\nThe VirtualNet feature enables frp to create and manage virtual network connections between clients and visitors through a TUN interface. This allows for IP-level routing between machines, extending frp beyond simple port forwarding to support full network connectivity.\n\nFor detailed information about configuration and usage, please refer to the [VirtualNet documentation](/doc/virtual_net.md).\n\n## Feature Gates\n\nfrp supports feature gates to enable or disable experimental features. This allows users to try out new features before they're considered stable.\n\n### Available Feature Gates\n\n| Name | Stage | Default | Description |\n|------|-------|---------|-------------|\n| VirtualNet | ALPHA | false | Virtual network capabilities for frp |\n\n### Enabling Feature Gates\n\nTo enable an experimental feature, add the feature gate to your configuration:\n\n```toml\nfeatureGates = { VirtualNet = true }\n```\n\n### Feature Lifecycle\n\nFeatures typically go through three stages:\n1. **ALPHA**: Disabled by default, may be unstable\n2. **BETA**: May be enabled by default, more stable but still evolving\n3. **GA (Generally Available)**: Enabled by default, ready for production use\n\n## Related Projects\n\n* [gofrp/plugin](https://github.com/gofrp/plugin) - A repository for frp plugins that contains a variety of plugins implemented based on the frp extension mechanism, meeting the customization needs of different scenarios.\n* [gofrp/tiny-frpc](https://github.com/gofrp/tiny-frpc) - A lightweight version of the frp client (around 3.5MB at minimum) implemented using the ssh protocol, supporting some of the most commonly used features, suitable for devices with limited resources.\n\n## Contributing\n\nInterested in getting involved? We would like to help you!\n\n* Take a look at our [issues list](https://github.com/fatedier/frp/issues) and consider sending a Pull Request to **dev branch**.\n* If you want to add a new feature, please create an issue first to describe the new feature, as well as the implementation approach. Once a proposal is accepted, create an implementation of the new features and submit it as a pull request.\n* Sorry for my poor English. Improvements for this document are welcome, even some typo fixes.\n* If you have great ideas, send an email to fatedier@gmail.com.\n\n**Note: We prefer you to give your advise in [issues](https://github.com/fatedier/frp/issues), so others with a same question can search it quickly and we don't need to answer them repeatedly.**\n\n## Donation\n\nIf frp helps you a lot, you can support us by:\n\n### GitHub Sponsors\n\nSupport us by [Github Sponsors](https://github.com/sponsors/fatedier).\n\nYou can have your company's logo placed on README file of this project.\n\n### PayPal\n\nDonate money by [PayPal](https://www.paypal.me/fatedier) to my account **fatedier@gmail.com**.\n",
      "stars_today": 46
    },
    {
      "id": 469321930,
      "name": "komodo",
      "full_name": "moghtech/komodo",
      "description": "ğŸ¦ a tool to build and deploy software on many servers ğŸ¦",
      "html_url": "https://github.com/moghtech/komodo",
      "stars": 9924,
      "forks": 269,
      "language": "Rust",
      "topics": [],
      "created_at": "2022-03-13T09:14:50Z",
      "updated_at": "2026-01-28T01:19:42Z",
      "pushed_at": "2026-01-27T23:59:00Z",
      "open_issues": 373,
      "owner": {
        "login": "moghtech",
        "avatar_url": "https://avatars.githubusercontent.com/u/93411308?v=4"
      },
      "readme": "# Komodo ğŸ¦\n\nA tool to build and deploy software across many servers. \n\nğŸ¦ [See the docs](https://komo.do)\n\nğŸ¦ [Try the Demo](https://demo.komo.do) - Login: `demo` : `demo`\n\nğŸ¦ [See the Build Server](https://build.komo.do)  - Login: `komodo` : `komodo`\n\nğŸ¦ [Join the Discord](https://discord.gg/DRqE8Fvg5c)\n\n## About\n\nThe Komodo dragon is the largest living member of the [*Monitor* family of lizards](https://en.wikipedia.org/wiki/Monitor_lizard).\n\nThere is no limit to the number of servers you can connect, and there will never be. There is no limit to what API you can use for automation, and there never will be. No \"business edition\" here.\n\n## Disclaimer\n\nWarning. This is open source software (GPL-V3), and while we make a best effort to ensure releases are stable and bug-free,\nthere are no warranties. Use at your own risk.\n\n## Links\n\n- [periphery setup](https://github.com/moghtech/komodo/blob/main/scripts/readme.md)\n- [roadmap](https://github.com/moghtech/komodo/blob/main/roadmap.md)\n\n## Screenshots\n\n### Light Theme\n\n![Dashboard](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Dashboard.png)\n![Stack](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Stack.png)\n![Compose](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Compose.png)\n![Env](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Env.png)\n![Sync](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Sync.png)\n![Update](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Update.png)\n![Stats](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Stats.png)\n![Export](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Light-Export.png)\n\n### Dark Theme\n\n![Dashboard](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Dashboard.png)\n![Stack](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Stack.png)\n![Compose](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Compose.png)\n![Env](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Env.png)\n![Sync](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Sync.png)\n![Update](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Update.png)\n![Stats](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Stats.png)\n![Export](https://raw.githubusercontent.com/moghtech/komodo/main/screenshots/Dark-Export.png)\n",
      "stars_today": 45
    },
    {
      "id": 850267588,
      "name": "xiaozhi-esp32",
      "full_name": "78/xiaozhi-esp32",
      "description": "An MCP-based chatbot | ä¸€ä¸ªåŸºäºMCPçš„èŠå¤©æœºå™¨äºº",
      "html_url": "https://github.com/78/xiaozhi-esp32",
      "stars": 23585,
      "forks": 4986,
      "language": "C++",
      "topics": [
        "chatbot",
        "esp32",
        "mcp"
      ],
      "created_at": "2024-08-31T10:08:16Z",
      "updated_at": "2026-01-28T01:06:31Z",
      "pushed_at": "2026-01-27T18:57:38Z",
      "open_issues": 498,
      "owner": {
        "login": "78",
        "avatar_url": "https://avatars.githubusercontent.com/u/4488133?v=4"
      },
      "readme": "# An MCP-based Chatbot\n\n(English | [ä¸­æ–‡](README_zh.md) | [æ—¥æœ¬èª](README_ja.md))\n\n## Introduction\n\nğŸ‘‰ [Human: Give AI a camera vs AI: Instantly finds out the owner hasn't washed hair for three daysã€bilibiliã€‘](https://www.bilibili.com/video/BV1bpjgzKEhd/)\n\nğŸ‘‰ [Handcraft your AI girlfriend, beginner's guideã€bilibiliã€‘](https://www.bilibili.com/video/BV1XnmFYLEJN/)\n\nAs a voice interaction entry, the XiaoZhi AI chatbot leverages the AI capabilities of large models like Qwen / DeepSeek, and achieves multi-terminal control via the MCP protocol.\n\n<img src=\"docs/mcp-based-graph.jpg\" alt=\"Control everything via MCP\" width=\"320\">\n\n## Version Notes\n\nThe current v2 version is incompatible with the v1 partition table, so it is not possible to upgrade from v1 to v2 via OTA. For partition table details, see [partitions/v2/README.md](partitions/v2/README.md).\n\nAll hardware running v1 can be upgraded to v2 by manually flashing the firmware.\n\nThe stable version of v1 is 1.9.2. You can switch to v1 by running `git checkout v1`. The v1 branch will be maintained until February 2026.\n\n### Features Implemented\n\n- Wi-Fi / ML307 Cat.1 4G\n- Offline voice wake-up [ESP-SR](https://github.com/espressif/esp-sr)\n- Supports two communication protocols ([Websocket](docs/websocket.md) or MQTT+UDP)\n- Uses OPUS audio codec\n- Voice interaction based on streaming ASR + LLM + TTS architecture\n- Speaker recognition, identifies the current speaker [3D Speaker](https://github.com/modelscope/3D-Speaker)\n- OLED / LCD display, supports emoji display\n- Battery display and power management\n- Multi-language support (Chinese, English, Japanese)\n- Supports ESP32-C3, ESP32-S3, ESP32-P4 chip platforms\n- Device-side MCP for device control (Speaker, LED, Servo, GPIO, etc.)\n- Cloud-side MCP to extend large model capabilities (smart home control, PC desktop operation, knowledge search, email, etc.)\n- Customizable wake words, fonts, emojis, and chat backgrounds with online web-based editing ([Custom Assets Generator](https://github.com/78/xiaozhi-assets-generator))\n\n## Hardware\n\n### Breadboard DIY Practice\n\nSee the Feishu document tutorial:\n\nğŸ‘‰ [\"XiaoZhi AI Chatbot Encyclopedia\"](https://ccnphfhqs21z.feishu.cn/wiki/F5krwD16viZoF0kKkvDcrZNYnhb?from=from_copylink)\n\nBreadboard demo:\n\n![Breadboard Demo](docs/v1/wiring2.jpg)\n\n### Supports 70+ Open Source Hardware (Partial List)\n\n- <a href=\"https://oshwhub.com/li-chuang-kai-fa-ban/li-chuang-shi-zhan-pai-esp32-s3-kai-fa-ban\" target=\"_blank\" title=\"LiChuang ESP32-S3 Development Board\">LiChuang ESP32-S3 Development Board</a>\n- <a href=\"https://github.com/espressif/esp-box\" target=\"_blank\" title=\"Espressif ESP32-S3-BOX3\">Espressif ESP32-S3-BOX3</a>\n- <a href=\"https://docs.m5stack.com/zh_CN/core/CoreS3\" target=\"_blank\" title=\"M5Stack CoreS3\">M5Stack CoreS3</a>\n- <a href=\"https://docs.m5stack.com/en/atom/Atomic%20Echo%20Base\" target=\"_blank\" title=\"AtomS3R + Echo Base\">M5Stack AtomS3R + Echo Base</a>\n- <a href=\"https://gf.bilibili.com/item/detail/1108782064\" target=\"_blank\" title=\"Magic Button 2.4\">Magic Button 2.4</a>\n- <a href=\"https://www.waveshare.net/shop/ESP32-S3-Touch-AMOLED-1.8.htm\" target=\"_blank\" title=\"Waveshare ESP32-S3-Touch-AMOLED-1.8\">Waveshare ESP32-S3-Touch-AMOLED-1.8</a>\n- <a href=\"https://github.com/Xinyuan-LilyGO/T-Circle-S3\" target=\"_blank\" title=\"LILYGO T-Circle-S3\">LILYGO T-Circle-S3</a>\n- <a href=\"https://oshwhub.com/tenclass01/xmini_c3\" target=\"_blank\" title=\"XiaGe Mini C3\">XiaGe Mini C3</a>\n- <a href=\"https://oshwhub.com/movecall/cuican-ai-pendant-lights-up-y\" target=\"_blank\" title=\"Movecall CuiCan ESP32S3\">CuiCan AI Pendant</a>\n- <a href=\"https://github.com/WMnologo/xingzhi-ai\" target=\"_blank\" title=\"WMnologo-Xingzhi-1.54\">WMnologo-Xingzhi-1.54TFT</a>\n- <a href=\"https://www.seeedstudio.com/SenseCAP-Watcher-W1-A-p-5979.html\" target=\"_blank\" title=\"SenseCAP Watcher\">SenseCAP Watcher</a>\n- <a href=\"https://www.bilibili.com/video/BV1BHJtz6E2S/\" target=\"_blank\" title=\"ESP-HI Low Cost Robot Dog\">ESP-HI Low Cost Robot Dog</a>\n\n<div style=\"display: flex; justify-content: space-between;\">\n  <a href=\"docs/v1/lichuang-s3.jpg\" target=\"_blank\" title=\"LiChuang ESP32-S3 Development Board\">\n    <img src=\"docs/v1/lichuang-s3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/espbox3.jpg\" target=\"_blank\" title=\"Espressif ESP32-S3-BOX3\">\n    <img src=\"docs/v1/espbox3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/m5cores3.jpg\" target=\"_blank\" title=\"M5Stack CoreS3\">\n    <img src=\"docs/v1/m5cores3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/atoms3r.jpg\" target=\"_blank\" title=\"AtomS3R + Echo Base\">\n    <img src=\"docs/v1/atoms3r.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/magiclick.jpg\" target=\"_blank\" title=\"Magic Button 2.4\">\n    <img src=\"docs/v1/magiclick.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/waveshare.jpg\" target=\"_blank\" title=\"Waveshare ESP32-S3-Touch-AMOLED-1.8\">\n    <img src=\"docs/v1/waveshare.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/lilygo-t-circle-s3.jpg\" target=\"_blank\" title=\"LILYGO T-Circle-S3\">\n    <img src=\"docs/v1/lilygo-t-circle-s3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/xmini-c3.jpg\" target=\"_blank\" title=\"XiaGe Mini C3\">\n    <img src=\"docs/v1/xmini-c3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/movecall-cuican-esp32s3.jpg\" target=\"_blank\" title=\"CuiCan\">\n    <img src=\"docs/v1/movecall-cuican-esp32s3.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/wmnologo_xingzhi_1.54.jpg\" target=\"_blank\" title=\"WMnologo-Xingzhi-1.54\">\n    <img src=\"docs/v1/wmnologo_xingzhi_1.54.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/sensecap_watcher.jpg\" target=\"_blank\" title=\"SenseCAP Watcher\">\n    <img src=\"docs/v1/sensecap_watcher.jpg\" width=\"240\" />\n  </a>\n  <a href=\"docs/v1/esp-hi.jpg\" target=\"_blank\" title=\"ESP-HI Low Cost Robot Dog\">\n    <img src=\"docs/v1/esp-hi.jpg\" width=\"240\" />\n  </a>\n</div>\n\n## Software\n\n### Firmware Flashing\n\nFor beginners, it is recommended to use the firmware that can be flashed without setting up a development environment.\n\nThe firmware connects to the official [xiaozhi.me](https://xiaozhi.me) server by default. Personal users can register an account to use the Qwen real-time model for free.\n\nğŸ‘‰ [Beginner's Firmware Flashing Guide](https://ccnphfhqs21z.feishu.cn/wiki/Zpz4wXBtdimBrLk25WdcXzxcnNS)\n\n### Development Environment\n\n- Cursor or VSCode\n- Install ESP-IDF plugin, select SDK version 5.4 or above\n- Linux is better than Windows for faster compilation and fewer driver issues\n- This project uses Google C++ code style, please ensure compliance when submitting code\n\n### Developer Documentation\n\n- [Custom Board Guide](docs/custom-board.md) - Learn how to create custom boards for XiaoZhi AI\n- [MCP Protocol IoT Control Usage](docs/mcp-usage.md) - Learn how to control IoT devices via MCP protocol\n- [MCP Protocol Interaction Flow](docs/mcp-protocol.md) - Device-side MCP protocol implementation\n- [MQTT + UDP Hybrid Communication Protocol Document](docs/mqtt-udp.md)\n- [A detailed WebSocket communication protocol document](docs/websocket.md)\n\n## Large Model Configuration\n\nIf you already have a XiaoZhi AI chatbot device and have connected to the official server, you can log in to the [xiaozhi.me](https://xiaozhi.me) console for configuration.\n\nğŸ‘‰ [Backend Operation Video Tutorial (Old Interface)](https://www.bilibili.com/video/BV1jUCUY2EKM/)\n\n## Related Open Source Projects\n\nFor server deployment on personal computers, refer to the following open-source projects:\n\n- [xinnan-tech/xiaozhi-esp32-server](https://github.com/xinnan-tech/xiaozhi-esp32-server) Python server\n- [joey-zhou/xiaozhi-esp32-server-java](https://github.com/joey-zhou/xiaozhi-esp32-server-java) Java server\n- [AnimeAIChat/xiaozhi-server-go](https://github.com/AnimeAIChat/xiaozhi-server-go) Golang server\n- [hackers365/xiaozhi-esp32-server-golang](https://github.com/hackers365/xiaozhi-esp32-server-golang) Golang server\n\nOther client projects using the XiaoZhi communication protocol:\n\n- [huangjunsen0406/py-xiaozhi](https://github.com/huangjunsen0406/py-xiaozhi) Python client\n- [TOM88812/xiaozhi-android-client](https://github.com/TOM88812/xiaozhi-android-client) Android client\n- [100askTeam/xiaozhi-linux](http://github.com/100askTeam/xiaozhi-linux) Linux client by 100ask\n- [78/xiaozhi-sf32](https://github.com/78/xiaozhi-sf32) Bluetooth chip firmware by Sichuan\n- [QuecPython/solution-xiaozhiAI](https://github.com/QuecPython/solution-xiaozhiAI) QuecPython firmware by Quectel\n\nCustom Assets Tools:\n\n- [78/xiaozhi-assets-generator](https://github.com/78/xiaozhi-assets-generator) Custom Assets Generator (Wake words, fonts, emojis, backgrounds)\n\n## About the Project\n\nThis is an open-source ESP32 project, released under the MIT license, allowing anyone to use it for free, including for commercial purposes.\n\nWe hope this project helps everyone understand AI hardware development and apply rapidly evolving large language models to real hardware devices.\n\nIf you have any ideas or suggestions, please feel free to raise Issues or join our [Discord](https://discord.gg/x3S4jgXHk3) or QQ group: 994694848\n\n## Star History\n\n<a href=\"https://star-history.com/#78/xiaozhi-esp32&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=78/xiaozhi-esp32&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=78/xiaozhi-esp32&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=78/xiaozhi-esp32&type=Date\" />\n </picture>\n</a> \n",
      "stars_today": 42
    },
    {
      "id": 766341786,
      "name": "puter",
      "full_name": "HeyPuter/puter",
      "description": "ğŸŒ The Internet Computer! Free, Open-Source, and Self-Hostable.",
      "html_url": "https://github.com/HeyPuter/puter",
      "stars": 39036,
      "forks": 3436,
      "language": "JavaScript",
      "topics": [
        "cloud",
        "cloud-os",
        "cloud-storage",
        "desktop",
        "desktop-environment",
        "dropbox",
        "good-first-issue",
        "gui",
        "javascript",
        "nas",
        "open-source",
        "operating-system",
        "os",
        "osjs",
        "puter",
        "remote-desktop",
        "storage",
        "web-desktop",
        "web-os",
        "webtop"
      ],
      "created_at": "2024-03-03T01:44:32Z",
      "updated_at": "2026-01-28T01:45:37Z",
      "pushed_at": "2026-01-28T01:38:51Z",
      "open_issues": 191,
      "owner": {
        "login": "HeyPuter",
        "avatar_url": "https://avatars.githubusercontent.com/u/103972607?v=4"
      },
      "readme": "<h3 align=\"center\"><img width=\"80\" alt=\"Puter.com, The Personal Cloud Computer: All your files, apps, and games in one place accessible from anywhere at any time.\" src=\"https://assets.puter.site/puter-logo.png\"></h3>\n\n<h3 align=\"center\">The Internet OS! Free, Open-Source, and Self-Hostable.</h3>\n\n<p align=\"center\">\n    <a href=\"https://puter.com/?ref=github.com\"><strong>Â« LIVE DEMO Â»</strong></a>\n    <br />\n    <br />\n    <a href=\"https://puter.com/?ref=github.com\">Puter.com</a>\n    Â·\n    <a href=\"https://puter.com/app/app-center\">App Store</a>\n    Â·\n    <a href=\"https://developer.puter.com\" target=\"_blank\">Developers</a>\n    Â·\n    <a href=\"https://github.com/heyputer/puter-cli\" target=\"_blank\">CLI</a>\n    Â·\n    <a href=\"https://discord.com/invite/PQcx7Teh8u\">Discord</a>\n    Â·\n    <a href=\"https://reddit.com/r/puter\">Reddit</a>\n    Â·\n    <a href=\"https://twitter.com/HeyPuter\">X</a>\n</p>\n\n<h3 align=\"center\"><img width=\"800\" style=\"border-radius:5px;\" alt=\"screenshot\" src=\"https://assets.puter.site/puter.com-screenshot-3.webp\"></h3>\n\n<br/>\n\n## Puter\n\nPuter is an advanced, open-source internet operating system designed to be feature-rich, fast, and highly extensible. Puter can be used as:\n\n- A privacy-first personal cloud to keep all your files, apps, and games in one secure place, accessible from anywhere at any time.\n- A platform for building and publishing websites, web apps, and games.\n- An alternative to Dropbox, Google Drive, OneDrive, etc. with a fresh interface and powerful features.\n- A remote desktop environment for servers and workstations.\n- A friendly, open-source project and community to learn about web development, cloud computing, distributed systems, and much more!\n\n<br/>\n\n## Getting Started\n\n### ğŸ’» Local Development\n\n```bash\ngit clone https://github.com/HeyPuter/puter\ncd puter\nnpm install\nnpm start\n```\n**â†’** This should launch Puter at \n<font color=\"red\"> http://puter.localhost:4100 (or the next available port). </font>\n\n\n\nIf this does not work, see [First Run Issues](./doc/self-hosters/first-run-issues.md) for\ntroubleshooting steps.\n\n<br/>\n\n### ğŸ³ Docker\n\n```bash\nmkdir puter && cd puter && mkdir -p puter/config puter/data && sudo chown -R 1000:1000 puter && docker run --rm -p 4100:4100 -v `pwd`/puter/config:/etc/puter -v `pwd`/puter/data:/var/puter  ghcr.io/heyputer/puter\n```\n**â†’** This should launch Puter at \n<font color=\"red\"> http://puter.localhost:4100 (or the next available port). </font>\n\n<br/>\n\n### ğŸ™ Docker Compose\n\n#### Linux/macOS\n\n```bash\nmkdir -p puter/config puter/data\nsudo chown -R 1000:1000 puter\nwget https://raw.githubusercontent.com/HeyPuter/puter/main/docker-compose.yml\ndocker compose up\n```\n**â†’** This should be available at \n<font color=\"red\"> http://puter.localhost:4100 (or the next available port). </font>\n\n<br/>\n\n#### Windows\n\n```powershell\nmkdir -p puter\ncd puter\nNew-Item -Path \"puter\\config\" -ItemType Directory -Force\nNew-Item -Path \"puter\\data\" -ItemType Directory -Force\nInvoke-WebRequest -Uri \"https://raw.githubusercontent.com/HeyPuter/puter/main/docker-compose.yml\" -OutFile \"docker-compose.yml\"\ndocker compose up\n```\n**â†’** This should launch Puter at \n<font color=\"red\"> http://puter.localhost:4100 (or the next available port). </font>\n\n<br/>\n\n### ğŸš€ Self-Hosting\n\nFor detailed guides on self-hosting Puter, including configuration options and best practices, see our [Self-Hosting Documentation](https://github.com/HeyPuter/puter/blob/main/doc/self-hosters/instructions.md).\n\n<br/>\n\n### â˜ï¸ Puter.com\n\nPuter is available as a hosted service at [**puter.com**](https://puter.com).\n\n<br/>\n\n## System Requirements\n\n- **Operating Systems:** Linux, macOS, Windows\n- **RAM:** 2GB minimum (4GB recommended)\n- **Disk Space:** 1GB free space\n- **Node.js:** Version 24+\n- **npm:** Latest stable version\n\n<br/>\n\n## Support\n\nConnect with the maintainers and community through these channels:\n\n- Bug report or feature request? Please [open an issue](https://github.com/HeyPuter/puter/issues/new/choose).\n- Discord: [discord.com/invite/PQcx7Teh8u](https://discord.com/invite/PQcx7Teh8u)\n- X (Twitter): [x.com/HeyPuter](https://x.com/HeyPuter)\n- Reddit: [reddit.com/r/puter/](https://www.reddit.com/r/puter/)\n- Mastodon: [mastodon.social/@puter](https://mastodon.social/@puter)\n- Security issues? [security@puter.com](mailto:security@puter.com)\n- Email maintainers at [hi@puter.com](mailto:hi@puter.com)\n\nWe are always happy to help you with any questions you may have. Don't hesitate to ask!\n\n<br/>\n\n## License\n\nThis repository, including all its contents, sub-projects, modules, and components, is licensed under [AGPL-3.0](https://github.com/HeyPuter/puter/blob/main/LICENSE.txt) unless explicitly stated otherwise. Third-party libraries included in this repository may be subject to their own licenses.\n\n<br/>\n\n## Translations\n\n- [Arabic / Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.ar.md)\n- [Armenian / Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.hy.md)\n- [Bengali / à¦¬à¦¾à¦‚à¦²à¦¾](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.bn.md)\n- [Chinese / ä¸­æ–‡](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.zh.md)\n- [Danish / Dansk](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.da.md)\n- [English](https://github.com/HeyPuter/puter/blob/main/README.md)\n- [Farsi / ÙØ§Ø±Ø³ÛŒ](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.fa.md)\n- [Finnish / Suomi](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.fi.md)\n- [French / FranÃ§ais](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.fr.md)\n- [German /  Deutsch](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.de.md)\n- [Hebrew/ ×¢×‘×¨×™×ª](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.he.md)\n- [Hindi / à¤¹à¤¿à¤‚à¤¦à¥€](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.hi.md)\n- [Hungarian / Magyar](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.hu.md)\n- [Indonesian / Bahasa Indonesia](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.id.md)\n- [Italian / Italiano](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.it.md)\n- [Japanese / æ—¥æœ¬èª](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.jp.md)\n- [Korean / í•œêµ­ì–´](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.ko.md)\n- [Malay / Bahasa Malaysia](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.my.md)\n- [Malayalam / à´®à´²à´¯à´¾à´³à´‚](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.ml.md)\n- [Polish / Polski](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.pl.md)\n- [Portuguese / PortuguÃªs](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.pt.md)\n- [Punjabi / à¨ªà©°à¨œà¨¾à¨¬à©€](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.pa.md)\n- [Romanian / RomÃ¢nÄƒ](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.ro.md)\n- [Russian / Ğ ÑƒÑÑĞºĞ¸Ğ¹](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.ru.md)\n- [Spanish / EspaÃ±ol](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.es.md)\n- [Swedish / Svenska](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.sv.md)\n- [Tamil / à®¤à®®à®¿à®´à¯](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.ta.md)\n- [Telugu / à°¤à±†à°²à±à°—à±](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.te.md)\n- [Thai / à¹„à¸—à¸¢](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.th.md)\n- [Turkish / TÃ¼rkÃ§e](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.tr.md)\n- [Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.ua.md)\n- [Urdu / Ø§Ø±Ø¯Ùˆ](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.ur.md)\n- [Vietnamese / Tiáº¿ng Viá»‡t](https://github.com/HeyPuter/puter/blob/main/doc/i18n/README.vi.md)\n",
      "stars_today": 41
    },
    {
      "id": 3390243,
      "name": "servo",
      "full_name": "servo/servo",
      "description": "Servo aims to empower developers with a lightweight, high-performance alternative for embedding web technologies in applications.",
      "html_url": "https://github.com/servo/servo",
      "stars": 35250,
      "forks": 3455,
      "language": "Rust",
      "topics": [
        "browser",
        "rust",
        "servo",
        "web",
        "webbrowser",
        "webengine",
        "webplatform"
      ],
      "created_at": "2012-02-08T19:07:25Z",
      "updated_at": "2026-01-28T01:32:15Z",
      "pushed_at": "2026-01-28T00:42:15Z",
      "open_issues": 3058,
      "owner": {
        "login": "servo",
        "avatar_url": "https://avatars.githubusercontent.com/u/2566135?v=4"
      },
      "readme": "# The Servo Parallel Browser Engine Project\n\nServo is a prototype web browser engine written in the\n[Rust](https://github.com/rust-lang/rust) language. It is currently developed on\n64-bit macOS, 64-bit Linux, 64-bit Windows, 64-bit OpenHarmony, and Android.\n\nServo welcomes contribution from everyone. Check out:\n\n- The [Servo Book](https://book.servo.org) for documentation\n- [servo.org](https://servo.org/) for news and guides\n\nCoordination of Servo development happens:\n- Here in the Github Issues\n- On the [Servo Zulip](https://servo.zulipchat.com/)\n- In video calls advertised in the [Servo Project](https://github.com/servo/project/issues) repo.\n\n## Getting started\n\nFor more detailed build instructions, see the Servo Book under [Getting the Code] and [Building Servo].\n\n[Getting the Code]: https://book.servo.org/building/getting-the-code.html\n[Building Servo]: https://book.servo.org/building/building.html\n\n### macOS\n\n- Download and install [Xcode](https://developer.apple.com/xcode/) and [`brew`](https://brew.sh/).\n- Install `uv`: `curl -LsSf https://astral.sh/uv/install.sh | sh` \n- Install `rustup`: `curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh`\n- Restart your shell to make sure `cargo` is available\n- Install the other dependencies: `./mach bootstrap`\n- Build servoshell: `./mach build`\n\n### Linux\n\n- Install `curl`:\n  - Arch: `sudo pacman -S --needed curl`\n  - Debian, Ubuntu: `sudo apt install curl`\n  - Fedora: `sudo dnf install curl`\n  - Gentoo: `sudo emerge net-misc/curl`\n- Install `uv`: `curl -LsSf https://astral.sh/uv/install.sh | sh` \n- Install `rustup`: `curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh`\n- Restart your shell to make sure `cargo` is available\n- Install the other dependencies: `./mach bootstrap`\n- Build servoshell: `./mach build`\n\n### Windows\n\n- Download [`uv`](https://docs.astral.sh/uv/getting-started/installation/#standalone-installer), [`choco`](https://chocolatey.org/install#individual), and [`rustup`](https://win.rustup.rs/)\n  - Be sure to select *Quick install via the Visual Studio Community installer*\n- In the Visual Studio Installer, ensure the following components are installed:\n  - **Windows 10/11 SDK (anything >= 10.0.19041.0)** (`Microsoft.VisualStudio.Component.Windows{10, 11}SDK.{>=19041}`)\n  - **MSVC v143 - VS 2022 C++ x64/x86 build tools (Latest)** (`Microsoft.VisualStudio.Component.VC.Tools.x86.x64`)\n  - **C++ ATL for latest v143 build tools (x86 & x64)** (`Microsoft.VisualStudio.Component.VC.ATL`)\n- Restart your shell to make sure `cargo` is available\n- Install the other dependencies: `.\\mach bootstrap`\n- Build servoshell: `.\\mach build`\n\n### Android\n\n- Ensure that the following environment variables are set:\n  - `ANDROID_SDK_ROOT`\n  - `ANDROID_NDK_ROOT`: `$ANDROID_SDK_ROOT/ndk/28.2.13676358/`\n `ANDROID_SDK_ROOT` can be any directory (such as `~/android-sdk`).\n  All of the Android build dependencies will be installed there.\n- Install the latest version of the [Android command-line\n  tools](https://developer.android.com/studio#command-tools) to\n  `$ANDROID_SDK_ROOT/cmdline-tools/latest`.\n- Run the following command to install the necessary components:\n  ```shell\n  sudo $ANDROID_SDK_ROOT/cmdline-tools/latest/bin/sdkmanager --install \\\n   \"build-tools;34.0.0\" \\\n   \"emulator\" \\\n   \"ndk;28.2.13676358\" \\\n   \"platform-tools\" \\\n   \"platforms;android-33\" \\\n   \"system-images;android-33;google_apis;x86_64\"\n  ```\n- Follow the instructions above for the platform you are building on\n\n### OpenHarmony\n\n- Follow the instructions above for the platform you are building on to prepare the environment.\n- Depending on the target distribution (e.g. `HarmonyOS NEXT` vs pure `OpenHarmony`) the build configuration will differ slightly.\n- Ensure that the following environment variables are set\n  - `DEVECO_SDK_HOME` (Required when targeting `HarmonyOS NEXT`)\n  - `OHOS_BASE_SDK_HOME` (Required when targeting `OpenHarmony`)\n  - `OHOS_SDK_NATIVE` (e.g. `${DEVECO_SDK_HOME}/default/openharmony/native` or `${OHOS_BASE_SDK_HOME}/${API_VERSION}/native`)\n  - `SERVO_OHOS_SIGNING_CONFIG`: Path to json file containing a valid signing configuration for the demo app.\n- Review the detailed instructions at [Building for OpenHarmony].\n- The target distribution can be modified by passing `--flavor=<default|harmonyos>` to `mach <build|package|install>`.\n",
      "stars_today": 41
    },
    {
      "id": 317319347,
      "name": "social-analyzer",
      "full_name": "qeeqbox/social-analyzer",
      "description": "API, CLI, and Web App for analyzing and finding a person's profile in 1000 social media \\ websites",
      "html_url": "https://github.com/qeeqbox/social-analyzer",
      "stars": 20639,
      "forks": 1919,
      "language": "JavaScript",
      "topics": [
        "analysis",
        "analyzer",
        "cli",
        "information-gathering",
        "javascript",
        "nodejs",
        "nodejs-cli",
        "osint",
        "pentest",
        "pentesting",
        "person-profile",
        "profile",
        "python",
        "reconnaissance",
        "security-tools",
        "social-analyzer",
        "social-media",
        "sosint",
        "username"
      ],
      "created_at": "2020-11-30T19:04:26Z",
      "updated_at": "2026-01-27T23:28:50Z",
      "pushed_at": "2026-01-12T21:50:25Z",
      "open_issues": 33,
      "owner": {
        "login": "qeeqbox",
        "avatar_url": "https://avatars.githubusercontent.com/u/54420737?v=4"
      },
      "readme": "<p align=\"center\"> <img src=\"https://raw.githubusercontent.com/qeeqbox/social-analyzer/main/readme/socialanalyzerlogo_.png\"></p>\n\nSocial Analyzer - API, CLI, and Web App for analyzing & finding a person's profile across +1000 social media \\ websites. It includes different analysis and detection modules, and you can choose which modules to use during the investigation process.\n\nThe detection modules utilize a rating mechanism based on different detection techniques, which produces a rate value that starts from 0 to 100 (No-Maybe-Yes). This module is intended to have fewer false positives.\n\nThe analysis and public extracted information from this OSINT tool could help investigate profiles related to suspicious or malicious activities such as cyberbullying, cyber grooming, cyberstalking, and spreading misinformation.\n\n`This project is currently used by some law enforcement agencies in countries where resources are limited - The detection database is different than the one shared here..`\n\n## SoÂ·cial MeÂ·diÂ·a\nWebsites and applications that enable users to create and share content or to participate in social networking - Oxford Dictionary\n\n## Structure\n<img src=\"https://raw.githubusercontent.com/qeeqbox/social-analyzer/main/readme/structure.png\">\n\n\n## APP (Preferred!)\nStandard localhost WEB APP url: http://0.0.0.0:9005/app.html\n\n<img src=\"https://raw.githubusercontent.com/qeeqbox/social-analyzer/main/readme/intro_fast.gif\" style=\"max-width:768px\"/>\n\n## CLI \n<img src=\"https://raw.githubusercontent.com/qeeqbox/social-analyzer/main/readme/cli.gif\" style=\"max-width:768px\"/>\n\n## Features\n- String & name analysis (Permutations and Combinations)\n- Find a profile using multiple techniques (HTTPS library & Webdriver)\n- Multi profile search (Used for correlation - any combination separated with \",\" )\n- Multilayers detections (OCR, normal, advanced & special)\n- Visualized profile information using Ixora (Metadata & Patterns)\n- Metadata & Patterns extraction (Added from Qeeqbox OSINT project)\n- Force-directed Graph for Metadata (Needs ExtractPatterns)\n- Search by top ranking or by country (Alexa Ranking)\n- Search by type (adult, music, etc.. - automated websites stats)\n- Profiles stats and static info (Category country)\n- Cross Metadata stats (Added from Qeeqbox OSINT project)\n- Auto-flirtation to unnecessary output (Enable javascript etc..)\n- Search engine lookup (Google API - optional)\n- Custom search queries (Google API & DuckDuckGo API - optional)\n- Profile screenshot, title, info, and website description\n- Find name origins, name similarity & common words by language\n- Find possible profile\\person age (Limited analysis)\n- Custom user-agent, proxy, timeout & implicit wait\n- Python CLI & NodeJS CLI (limited to FindUserProfilesFast option)\n- Screenshots of detected profile (The latest version of Chrome must be installed)\n- Grid option for faster checking (limited to docker-compose)\n- Dump logs to folder or terminal (prettified)\n- Adjust finding\\getting profile workers (default 15)\n- Re-checking option for failed profiles\n- Filter profiles by good, maybe, and bad\n- Save the analysis as a JSON file\n- Simplified web interface and CLI\n- And, more!!\n\n## Special Detections\n- Facebook (Phone number, name, or profile name)\n- Gmail (example@gmail.com)\n- Google (example@example.com)\n\n## Install & Run\n### Linux (As Node WebApp)\n```bash\nsudo apt-get update\n#Depedning on your Linux distro, you may or may not need these 2 lines\nsudo DEBIAN_FRONTEND=noninteractive apt-get install -y software-properties-common\nsudo add-apt-repository ppa:mozillateam/ppa -y\nsudo apt-get install -y firefox-esr tesseract-ocr git nodejs npm\ngit clone https://github.com/qeeqbox/social-analyzer.git\ncd social-analyzer\nnpm update\nnpm install\nnpm start\n```\n\n### Linux (As Node CLI)\n```bash\nsudo apt-get update\n#Depedning on your Linux distro, you may or may not need these 2 lines\nsudo DEBIAN_FRONTEND=noninteractive apt-get install -y software-properties-common\nsudo add-apt-repository ppa:mozillateam/ppa -y\nsudo apt-get install -y firefox-esr tesseract-ocr git nodejs npm\ngit clone https://github.com/qeeqbox/social-analyzer.git\ncd social-analyzer\nnpm install\nnodejs app.js --username \"johndoe\"\n#or\nnodejs app.js --username \"johndoe,janedoe\" --metadata\n#or\nnodejs app.js --username \"johndoe,janedoe\" --metadata --top 100\n#or\nnodejs app.js --username \"johndoe\" --type \"adult\"\n```\n\n### Linux (As python package)\n```bash\nsudo apt-get update\nsudo apt-get install python3 python3-pip\npip3 install social-analyzer\npython3 -m social-analyzer --username \"johndoe\"\n#or\npython3 -m social-analyzer --username \"johndoe\" --metadata\n#or\npython3 -m social-analyzer --username \"johndoe\" --metadata --top 100\n#or\npython3 -m social-analyzer --username \"johndoe\" --type \"adult\"\n#or\npython3 -m social-analyzer --username \"johndoe\" --websites \"car\" --logs --screenshots\n```\n\n### Linux (As python script)\n```bash\nsudo apt-get update\nsudo apt-get install git python3 python3-pip\ngit clone https://github.com/qeeqbox/social-analyzer\ncd social-analyzer\npip3 install -r requirements.txt\npython3 app.py --username \"janedoe\"\n#or\npython3 app.py --username \"johndoe\" --metadata\n#or\npython3 app.py --username \"johndoe\" --metadata --top 100\n#or\npython3 app.py --username \"johndoe\" --type \"adult\"\n#or\npython3 app.py --username \"johndoe\" --websites \"car\" --logs --screenshots\n```\n\n### Importing as object (python)\n```python\n\n#E.g. #1\nfrom importlib import import_module\nSocialAnalyzer = import_module(\"social-analyzer\").SocialAnalyzer()\nresults = SocialAnalyzer.run_as_object(username=\"johndoe\",silent=True)\nprint(results)\n\n#E.g. #2\nfrom importlib import import_module\nSocialAnalyzer = import_module(\"social-analyzer\").SocialAnalyzer()\nresults = SocialAnalyzer.run_as_object(username=\"johndoe,janedoe\",silent=True,output=\"json\",filter=\"good\",metadata=False,timeout=10, profiles=\"detected\")\nprint(results)\n```\n\n### Linux, Windows, MacOS, Raspberry pi..\n- check this [wiki](https://github.com/qeeqbox/social-analyzer/wiki/install) for all possible installation methods\n- check this [wiki](https://github.com/qeeqbox/social-analyzer/wiki/integration) for integrating social-analyzer with your OSINT tools, feeds, etc...\n\n## social-analyzer --h\n```\nRequired Arguments:\n  --username   E.g. johndoe, john_doe or johndoe9999\n\nOptional Arguments:\n  --websites    A website or websites separated by space E.g. youtube, tiktokor tumblr\n  --mode        Analysis mode E.g.fast -> FindUserProfilesFast, slow -> FindUserProfilesSlow or special -> FindUserProfilesSpecial\n  --output      Show the output in the following format: json -> json outputfor integration or pretty -> prettify the output\n  --options     Show the following when a profile is found: link, rate, titleor text\n  --method      find -> show detected profiles, get -> show all profiles regardless detected or not, all -> combine find & get\n  --filter      Filter detected profiles by good, maybe or bad, you can do combine them with comma (good,bad) or use all\n  --profiles    Filter profiles by detected, unknown or failed, you can do combine them with comma (detected,failed) or use all\n  --countries   select websites by country or countries separated by space as: us br ru\n  --type        Select websites by type (Adult, Music etc)\n  --top         select top websites as 10, 50 etc...[--websites is not needed]\n  --extract     Extract profiles, urls & patterns if possible\n  --metadata    Extract metadata if possible (pypi QeeqBox OSINT)\n  --trim        Trim long strings\n  --gui         Reserved for a gui (Not implemented)\n  --cli         Reserved for a cli (Not needed)\n\nListing websites & detections:\n  --list        List all available websites\n\nSetting:\n  --headers     Headers as dict\n  --logs_dir    Change logs directory\n  --timeout     Change timeout between each request\n  --silent      Disable output to screen\n```\n\n## Open Shell\n[![Open in Cloud Shell](https://img.shields.io/static/v1?label=%3E_&message=Open%20in%20Cloud%20Shell&color=3267d6&style=flat-square)](https://ssh.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https://github.com/qeeqbox/social-analyzer&tutorial=README.md) [![Open in repl.it Shell](https://img.shields.io/static/v1?label=%3E_&message=Open%20in%20repl.it%20Shell&color=606c74&style=flat-square)](https://repl.it/github/qeeqbox/social-analyzer)\n\n## Resources\n- DuckDuckGo API, Google API, NodeJS, bootstrap, selectize, jQuery, Wikipedia, font-awesome, selenium-webdriver & tesseract.js\n- Let me know if I missed a reference or resource!\n\n## Disclaimer\\Notes\n- Download this project from GitHub and treat it as a security project\n- If you want your website to be excluded from this project list, please reach out to me\n- This tool is meant to be used locally, not as a service (It does not have any Access Control)\n- For issues related to modules that end with -private or under the private group ![](https://raw.githubusercontent.com/qeeqbox/social-analyzer/main/readme/modules.png), reach out directly to me (do not open an issue on GitHub)\n\n## Other Projects\n[![](https://github.com/qeeqbox/.github/blob/main/data/analyzer.png)](https://github.com/qeeqbox/analyzer) [![](https://github.com/qeeqbox/.github/blob/main/data/chameleon.png)](https://github.com/qeeqbox/chameleon) [![](https://github.com/qeeqbox/.github/blob/main/data/honeypots.png)](https://github.com/qeeqbox/honeypots) [![](https://github.com/qeeqbox/.github/blob/main/data/osint.png)](https://github.com/qeeqbox/osint) [![](https://github.com/qeeqbox/.github/blob/main/data/url-sandbox.png)](https://github.com/qeeqbox/url-sandbox) [![](https://github.com/qeeqbox/.github/blob/main/data/mitre-visualizer.png)](https://github.com/qeeqbox/mitre-visualizer) [![](https://github.com/qeeqbox/.github/blob/main/data/woodpecker.png)](https://github.com/qeeqbox/woodpecker) [![](https://github.com/qeeqbox/.github/blob/main/data/docker-images.png)](https://github.com/qeeqbox/docker-images) [![](https://github.com/qeeqbox/.github/blob/main/data/seahorse.png)](https://github.com/qeeqbox/seahorse) [![](https://github.com/qeeqbox/.github/blob/main/data/rhino.png)](https://github.com/qeeqbox/rhino) [![](https://github.com/qeeqbox/.github/blob/main/data/raven.png)](https://github.com/qeeqbox/raven) [![](https://github.com/qeeqbox/.github/blob/main/data/image-analyzer.png)](https://github.com/qeeqbox/image-analyzer)\n",
      "stars_today": 40
    },
    {
      "id": 1016509116,
      "name": "short-video-factory",
      "full_name": "YILS-LIN/short-video-factory",
      "description": "ä¸€é”®ç”Ÿæˆäº§å“è¥é”€ä¸æ³›å†…å®¹çŸ­è§†é¢‘ï¼ŒAIæ‰¹é‡è‡ªåŠ¨å‰ªè¾‘ï¼Œé«˜é¢œå€¼è·¨å¹³å°æ¡Œé¢ç«¯å·¥å…· One click generation of product marketing and general content short videos, AI batch automatic cliping, beautiful cross platform desktop tool",
      "html_url": "https://github.com/YILS-LIN/short-video-factory",
      "stars": 3281,
      "forks": 474,
      "language": "TypeScript",
      "topics": [
        "ai",
        "automatic",
        "automation",
        "clipping",
        "cross-platform",
        "desktop",
        "linux",
        "mac",
        "marketing",
        "pc",
        "product",
        "shortvideo",
        "tiktok",
        "windows"
      ],
      "created_at": "2025-07-09T05:55:44Z",
      "updated_at": "2026-01-28T01:20:27Z",
      "pushed_at": "2026-01-22T03:54:17Z",
      "open_issues": 16,
      "owner": {
        "login": "YILS-LIN",
        "avatar_url": "https://avatars.githubusercontent.com/u/62397373?v=4"
      },
      "readme": "<a id=\"readme-top\"></a>\n\n<!-- é¡¹ç›®æµ·æŠ¥ -->\n<div align=\"center\">\n  <img src=\"images/ScreenShot.png\" alt=\"Poster\" width=\"100%\">\n</div>\n\n---\n\n<!-- é¡¹ç›®LOGO -->\n<br />\n<div align=\"center\">\n  <a href=\"https://github.com/YILS-LIN/short-video-factory\">\n    <img src=\"public/icon.png\" alt=\"Logo\" height=\"100\">\n  </a>\n\n<h3 align=\"center\">AI Short Video Factory - çŸ­è§†é¢‘å·¥å‚</h3>\n\n  <p align=\"center\">\n    ğŸš€ ä¸€é”®ç”Ÿæˆäº§å“è¥é”€ä¸æ³›å†…å®¹çŸ­è§†é¢‘ï¼ŒAIæ‰¹é‡è‡ªåŠ¨å‰ªè¾‘ï¼Œé«˜é¢œå€¼è·¨å¹³å°æ¡Œé¢ç«¯å·¥å…·\n  </p>\n\n  <!-- é¡¹ç›®å¾½ç«  -->\n\n[![è´¡çŒ®è€…][contributors-shield]][contributors-url]\n[![åˆ†æ”¯][forks-shield]][forks-url]\n[![æ˜Ÿæ ‡][stars-shield]][stars-url]\n[![é—®é¢˜][issues-shield]][issues-url]\n[![æœ€æ–°ç‰ˆæœ¬][release-shield]][release-url]\n\n<!-- ![å‘å¸ƒæ—¥æœŸ][release-date-shield] -->\n\n[![è®¸å¯è¯][license-shield]][license-url]\n\n  <p align=\"center\">\n    <a href=\"https://github.com/YILS-LIN/short-video-factory/issues/new?labels=bug&template=bug-report---.md\">æŠ¥å‘ŠBug</a>\n    &middot;\n    <a href=\"https://github.com/YILS-LIN/short-video-factory/issues/new?labels=enhancement&template=feature-request---.md\">è¯·æ±‚åŠŸèƒ½</a>\n  </p>\n</div>\n\n<!-- å…³äºé¡¹ç›® -->\n\n## ğŸ“– å…³äºé¡¹ç›®\n\nçŸ­è§†é¢‘å·¥å‚æ˜¯ä¸€ä¸ªå¼€æºçš„æ¡Œé¢ç«¯åº”ç”¨ï¼Œæ—¨åœ¨é€šè¿‡AIæŠ€æœ¯ç®€åŒ–çŸ­è§†é¢‘çš„åˆ¶ä½œæµç¨‹ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•çš„æç¤ºè¯æ–‡æœ¬+è§†é¢‘åˆ†é•œç´ æï¼Œå¿«é€Ÿä¸”è‡ªåŠ¨çš„å‰ªè¾‘å‡ºé«˜è´¨é‡çš„äº§å“è¥é”€å’Œæ³›å†…å®¹çŸ­è§†é¢‘ã€‚è¯¥é¡¹ç›®é›†æˆäº†AIé©±åŠ¨çš„æ–‡æ¡ˆç”Ÿæˆã€è¯­éŸ³åˆæˆã€è§†é¢‘å‰ªè¾‘ã€å­—å¹•ç‰¹æ•ˆç­‰åŠŸèƒ½ï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›å¼€ç®±å³ç”¨çš„çŸ­è§†é¢‘åˆ¶ä½œä½“éªŒã€‚\n\n### æ ¸å¿ƒåŠŸèƒ½\n\n- ğŸ¤– **AIé©±åŠ¨**ï¼šé›†æˆäº†æœ€æ–°çš„AIæŠ€æœ¯ï¼Œæå‡è§†é¢‘åˆ¶ä½œæ•ˆç‡å’Œè´¨é‡\n- ğŸ“ **æ–‡æ¡ˆç”Ÿæˆ**ï¼šåŸºäºæç¤ºè¯ç”Ÿæˆé«˜è´¨é‡çš„çŸ­è§†é¢‘æ–‡æ¡ˆ\n- ğŸ¥ **è‡ªåŠ¨å‰ªè¾‘**ï¼šæ”¯æŒå¤šç§è§†é¢‘æ ¼å¼ï¼Œè‡ªåŠ¨åŒ–æ‰¹é‡å¤„ç†è§†é¢‘å‰ªè¾‘ä»»åŠ¡\n- ğŸ™ï¸ **è¯­éŸ³åˆæˆ**ï¼šå°†ç”Ÿæˆçš„æ–‡æ¡ˆè½¬æ¢ä¸ºè‡ªç„¶æµç•…çš„è¯­éŸ³\n- ğŸ¬ **å­—å¹•ç‰¹æ•ˆ**ï¼šè‡ªåŠ¨æ·»åŠ å­—å¹•å’Œç‰¹æ•ˆï¼Œæå‡è§†é¢‘è´¨é‡\n- ğŸ“¦ **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒæ‰¹é‡ä»»åŠ¡ï¼ŒæŒ‰é¢„è®¾è‡ªåŠ¨æŒç»­åˆæˆè§†é¢‘\n- ğŸŒ **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šç§è¯­è¨€ï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·éœ€æ±‚\n- ğŸ“¦ **å¼€ç®±å³ç”¨**ï¼šæ— éœ€å¤æ‚é…ç½®ï¼Œç”¨æˆ·å¯ä»¥å¿«é€Ÿä¸Šæ‰‹\n- ğŸ“ˆ **æŒç»­æ›´æ–°**ï¼šå®šæœŸå‘å¸ƒæ–°ç‰ˆæœ¬ï¼Œä¿®å¤bugå¹¶æ·»åŠ æ–°åŠŸèƒ½\n- ğŸ”’ **å®‰å…¨å¯é **ï¼šå®Œå…¨æœ¬åœ°æœ¬åœ°åŒ–è¿è¡Œï¼Œç¡®ä¿ç”¨æˆ·æ•°æ®å®‰å…¨\n- ğŸ¨ **ç”¨æˆ·å‹å¥½**ï¼šç®€æ´ç›´è§‚çš„ç”¨æˆ·ç•Œé¢ï¼Œæ˜“äºæ“ä½œ\n- ğŸ’» **å¤šå¹³å°æ”¯æŒ**ï¼šæ”¯æŒWindowsã€macOSå’ŒLinuxç­‰å¤šä¸ªæ“ä½œç³»ç»Ÿ\n\n<p align=\"right\">(<a href=\"#readme-top\">è¿”å›é¡¶éƒ¨</a>)</p>\n\n## ğŸš€ å¼€å§‹ä½¿ç”¨\n\nå‰å¾€ [Github Release](https://github.com/YILS-LIN/short-video-factory/releases) ä¸‹è½½æœ€æ–°ç‰ˆæœ¬\n\nå‰å¾€ [å®˜æ–¹æ–‡æ¡£](https://short-video-factory.yils.blog) æŸ¥çœ‹ä½¿ç”¨æ‰‹å†Œ\n\n<p align=\"right\">(<a href=\"#readme-top\">è¿”å›é¡¶éƒ¨</a>)</p>\n\n## ğŸ—ºï¸ è·¯çº¿å›¾\n\n**å–œæ¬¢å¯ä»¥ç‚¹ä¸ª Star æ”¯æŒä¸€ä¸‹å“¦ï¼**\n\nä¸‹é¢æ˜¯å·²å®ç°å’Œè®¡åˆ’ä¸­çš„åŠŸèƒ½ï¼š\n\n- [x] æ–‡æ¡ˆç”Ÿæˆï¼Œå…¼å®¹é€šç”¨çš„ OpenAI æ¥å£æ ¼å¼\n- [x] è¯­éŸ³åˆæˆï¼Œæ”¯æŒEdgeTTS\n- [x] è§†é¢‘å‰ªè¾‘ï¼Œæ–‡æ¡ˆã€è§†é¢‘ã€éŸ³é¢‘ã€å­—å¹•åˆæˆï¼Œè‡ªåŠ¨æ··å‰ª\n- [x] æ‰¹é‡å¤„ç†ï¼Œæ”¯æŒä¸€ä¸ªæ‰¹é‡ä»»åŠ¡ï¼ŒæŒ‰é¢„è®¾è‡ªåŠ¨æŒç»­åˆæˆè§†é¢‘\n- [x] å¤šè¯­è¨€æ”¯æŒï¼Œèƒ½å¤Ÿæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šç§è¯­è¨€\n- [x] å®Œå–„çš„ä½¿ç”¨æ‰‹å†Œ\n- [ ] æ›´å…¨é¢çš„å‚æ•°è°ƒæ•´\n- [ ] æ›´å¤šçš„è¯­éŸ³åˆæˆAPI\n- [ ] å­—å¹•ç‰¹æ•ˆï¼Œæ”¯æŒå¤šç§å­—å¹•æ ·å¼å’Œç‰¹æ•ˆ\n\næŸ¥çœ‹[å¼€æ”¾é—®é¢˜](https://github.com/YILS-LIN/short-video-factory/issues)ä»¥è·å–æè®®åŠŸèƒ½ï¼ˆå’Œå·²çŸ¥é—®é¢˜ï¼‰çš„å®Œæ•´åˆ—è¡¨ã€‚\n\n<p align=\"right\">(<a href=\"#readme-top\">è¿”å›é¡¶éƒ¨</a>)</p>\n\n## ğŸï¸ ç¤ºä¾‹è§†é¢‘\n\n<table>\n<thead>\n<tr>\n<th align=\"center\"><g-emoji class=\"g-emoji\" alias=\"arrow_forward\">â–¶ï¸</g-emoji> ã€Šäº§å“è¥é”€çŸ­è§†é¢‘ã€‹</th>\n<th align=\"center\"><g-emoji class=\"g-emoji\" alias=\"arrow_forward\">â–¶ï¸</g-emoji> ã€Šæš–å¿ƒæ²»æ„ˆç³»è¯­å½•ã€‹</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\"><video src=\"https://github.com/user-attachments/assets/165a8f96-861b-4cf3-946c-444b9692cef8\"></video></td>\n<td align=\"center\"><video src=\"https://github.com/user-attachments/assets/12694618-e0fe-4848-8a7e-98b3f3a7aece\"></video></td>\n</tr>\n</tbody>\n</table>\n\næ³¨ï¼šç´ ææ¥æºäºç½‘ç»œï¼Œä»…ç”¨äºå±•ç¤ºå‰ªè¾‘æ•ˆæœ\n\n<p align=\"right\">(<a href=\"#readme-top\">è¿”å›é¡¶éƒ¨</a>)</p>\n\n<!-- è´¡çŒ® -->\n\n## ğŸ¤ è´¡çŒ®\n\nè´¡çŒ®è®©å¼€æºç¤¾åŒºæˆä¸ºäº†ä¸€ä¸ªå­¦ä¹ ã€å¯å‘å’Œåˆ›é€ çš„ç»ä½³åœºæ‰€ã€‚**éå¸¸æ„Ÿè°¢**æ‚¨æ‰€åšçš„ä»»ä½•è´¡çŒ®ã€‚\n\nå¦‚æœæ‚¨æœ‰å¯ä»¥æ”¹å–„æ­¤é¡¹ç›®çš„å»ºè®®ï¼Œè¯·forkæœ¬é¡¹ç›®ä»“åº“å¹¶åˆ›å»ºä¸€ä¸ªpull requestã€‚æ‚¨ä¹Ÿå¯ä»¥ç®€å•åœ°åˆ›å»ºä¸€ä¸ªå¸¦æœ‰\"enhancement\"æ ‡ç­¾çš„issueã€‚\nä¸è¦å¿˜è®°ç»™é¡¹ç›®ç‚¹ä¸ªStarï¼å†æ¬¡æ„Ÿè°¢ï¼\n\n1. Forkæ­¤é¡¹ç›®\n2. åˆ›å»ºæ‚¨çš„åŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)\n3. æäº¤æ‚¨çš„æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)\n4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)\n5. å¼€å¯ä¸€ä¸ªPull Request\n\n### ä¸»è¦è´¡çŒ®è€…ï¼š\n\n<a href=\"https://github.com/YILS-LIN/short-video-factory/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=YILS-LIN/short-video-factory\" alt=\"contrib.rocks image\" />\n</a>\n\n<p align=\"right\">(<a href=\"#readme-top\">è¿”å›é¡¶éƒ¨</a>)</p>\n\n## ğŸ’– é¸£è°¢\n\n- [rany2/edge-tts](https://github.com/rany2/edge-tts)\n- [duyquangnvx/edge-tts](https://github.com/duyquangnvx/edge-tts)\n\n<p align=\"right\">(<a href=\"#readme-top\">è¿”å›é¡¶éƒ¨</a>)</p>\n\n<!-- è®¸å¯è¯ -->\n\n## ğŸ— è®¸å¯è¯\n\n[![è®¸å¯è¯][license-shield]][license-url]\n\nCopyright Â© 2025 YILS.\n\n<p align=\"right\">(<a href=\"#readme-top\">è¿”å›é¡¶éƒ¨</a>)</p>\n\n## ğŸ± æèµ \n\nå¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿è¯·ä½œè€…å–æ¯å’–å•¡(æˆ–è€…å•¤é…’) ğŸº\n\nä½ çš„ **Star â­** å’Œ **æèµ ** æ˜¯æˆ‘æŒç»­æ›´æ–°çš„æœ€å¤§åŠ¨åŠ›ï¼\n\n<div align=\"left\">\n  <img src=\"https://github.com/user-attachments/assets/6b832dd3-38ea-4927-9c3b-97549c77a1f0\" alt=\"YILSçš„å¾®ä¿¡èµèµç \" width=\"400\">\n</div>\n\nğŸ‘‰ åœ¨æ­¤å¤„æŸ¥çœ‹æèµ è€…åå•ï¼š[åƒå¤ç•™å - æèµ è€…ç•™è¨€æ¿](https://short-video-factory.yils.blog/donate/list.html)\n\n<p align=\"right\">(<a href=\"#readme-top\">è¿”å›é¡¶éƒ¨</a>)</p>\n\n<!-- æ˜Ÿæ ‡å†å² -->\n\n## â­ æ˜Ÿæ ‡å†å²\n\n<div align=\"center\">\n  <a href=\"https://star-history.com/#YILS-LIN/short-video-factory\">\n    <img src=\"https://api.star-history.com/svg?repos=YILS-LIN/short-video-factory&type=Date\" alt=\"Star History Chart\" width=\"800\">\n  </a>\n</div>\n\n<p align=\"right\">(<a href=\"#readme-top\">è¿”å›é¡¶éƒ¨</a>)</p>\n\n<!-- MARKDOWNé“¾æ¥å’Œå›¾ç‰‡ -->\n\n[contributors-shield]: https://img.shields.io/github/contributors/YILS-LIN/short-video-factory.svg?color=c4f042&labelColor=black&style=flat-square\n[contributors-url]: https://github.com/YILS-LIN/short-video-factory/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/YILS-LIN/short-video-factory.svg?color=8ae8ff&labelColor=black&style=flat-square\n[forks-url]: https://github.com/YILS-LIN/short-video-factory/network/members\n[stars-shield]: https://img.shields.io/github/stars/YILS-LIN/short-video-factory.svg?color=ffcb47&labelColor=black&style=flat-square\n[stars-url]: https://github.com/YILS-LIN/short-video-factory/stargazers\n[issues-shield]: https://img.shields.io/github/issues/YILS-LIN/short-video-factory.svg?labelColor=black&style=flat-square\n[issues-url]: https://github.com/YILS-LIN/short-video-factory/issues\n[release-shield]: https://img.shields.io/github/v/release/YILS-LIN/short-video-factory?labelColor=black&style=flat-square\n[release-url]: https://github.com/YILS-LIN/short-video-factory/releases\n[release-date-shield]: https://img.shields.io/github/release-date/YILS-LIN/short-video-factory?color=9cf&style=flat-round\n[license-shield]: https://img.shields.io/github/license/YILS-LIN/short-video-factory.svg?labelColor=black&style=flat-square\n[license-url]: https://github.com/YILS-LIN/short-video-factory/blob/main/LICENSE\n",
      "stars_today": 40
    },
    {
      "id": 676934005,
      "name": "niri",
      "full_name": "YaLTeR/niri",
      "description": "A scrollable-tiling Wayland compositor.",
      "html_url": "https://github.com/YaLTeR/niri",
      "stars": 17797,
      "forks": 659,
      "language": "Rust",
      "topics": [
        "rust",
        "smithay",
        "tiling-window-manager",
        "wayland",
        "wayland-compositor"
      ],
      "created_at": "2023-08-10T10:53:14Z",
      "updated_at": "2026-01-28T01:21:21Z",
      "pushed_at": "2026-01-27T17:56:03Z",
      "open_issues": 460,
      "owner": {
        "login": "YaLTeR",
        "avatar_url": "https://avatars.githubusercontent.com/u/1794388?v=4"
      },
      "readme": "<h1 align=\"center\"><img alt=\"niri\" src=\"https://github.com/user-attachments/assets/07d05cd0-d5dc-4a28-9a35-51bae8f119a0\"></h1>\n<p align=\"center\">A scrollable-tiling Wayland compositor.</p>\n<p align=\"center\">\n    <a href=\"https://matrix.to/#/#niri:matrix.org\"><img alt=\"Matrix\" src=\"https://img.shields.io/badge/matrix-%23niri-blue?logo=matrix\"></a>\n    <a href=\"https://github.com/YaLTeR/niri/blob/main/LICENSE\"><img alt=\"GitHub License\" src=\"https://img.shields.io/github/license/YaLTeR/niri\"></a>\n    <a href=\"https://github.com/YaLTeR/niri/releases\"><img alt=\"GitHub Release\" src=\"https://img.shields.io/github/v/release/YaLTeR/niri?logo=github\"></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://yalter.github.io/niri/Getting-Started.html\">Getting Started</a> | <a href=\"https://yalter.github.io/niri/Configuration%3A-Introduction.html\">Configuration</a> | <a href=\"https://github.com/YaLTeR/niri/discussions/325\">Setup&nbsp;Showcase</a>\n</p>\n\n![niri with a few windows open](https://github.com/user-attachments/assets/535e6530-2f44-4b84-a883-1240a3eee6e9)\n\n## About\n\nWindows are arranged in columns on an infinite strip going to the right.\nOpening a new window never causes existing windows to resize.\n\nEvery monitor has its own separate window strip.\nWindows can never \"overflow\" onto an adjacent monitor.\n\nWorkspaces are dynamic and arranged vertically.\nEvery monitor has an independent set of workspaces, and there's always one empty workspace present all the way down.\n\nThe workspace arrangement is preserved across disconnecting and connecting monitors where it makes sense.\nWhen a monitor disconnects, its workspaces will move to another monitor, but upon reconnection they will move back to the original monitor.\n\n## Features\n\n- Built from the ground up for scrollable tiling\n- [Dynamic workspaces](https://yalter.github.io/niri/Workspaces.html) like in GNOME\n- An [Overview](https://github.com/user-attachments/assets/379a5d1f-acdb-4c11-b36c-e85fd91f0995) that zooms out workspaces and windows\n- Built-in screenshot UI\n- Monitor and window screencasting through xdg-desktop-portal-gnome\n    - You can [block out](https://yalter.github.io/niri/Configuration%3A-Window-Rules.html#block-out-from) sensitive windows from screencasts\n    - [Dynamic cast target](https://yalter.github.io/niri/Screencasting.html#dynamic-screencast-target) that can change what it shows on the go\n- [Touchpad](https://github.com/YaLTeR/niri/assets/1794388/946a910e-9bec-4cd1-a923-4a9421707515) and [mouse](https://github.com/YaLTeR/niri/assets/1794388/8464e65d-4bf2-44fa-8c8e-5883355bd000) gestures\n- Group windows into [tabs](https://yalter.github.io/niri/Tabs.html)\n- Configurable layout: gaps, borders, struts, window sizes\n- [Gradient borders](https://yalter.github.io/niri/Configuration%3A-Layout.html#gradients) with Oklab and Oklch support\n- [Animations](https://github.com/YaLTeR/niri/assets/1794388/ce178da2-af9e-4c51-876f-8709c241d95e) with support for [custom shaders](https://github.com/YaLTeR/niri/assets/1794388/27a238d6-0a22-4692-b794-30dc7a626fad)\n- Live-reloading config\n- Works with [screen readers](https://yalter.github.io/niri/Accessibility.html)\n\n## Video Demo\n\nhttps://github.com/YaLTeR/niri/assets/1794388/bce834b0-f205-434e-a027-b373495f9729\n\nAlso check out this video from Brodie Robertson that showcases a lot of the niri functionality: [Niri Is My New Favorite Wayland Compositor](https://youtu.be/DeYx2exm04M)\n\n## Status\n\nNiri is stable for day-to-day use and does most things expected of a Wayland compositor.\nMany people are daily-driving niri, and are happy to help in our [Matrix channel].\n\nGive it a try!\nFollow the instructions on the [Getting Started](https://yalter.github.io/niri/Getting-Started.html) page.\nHave your [waybar]s and [fuzzel]s ready: niri is not a complete desktop environment.\nAlso check out [awesome-niri], a list of niri-related links and projects.\n\nHere are some points you may have questions about:\n\n- **Multi-monitor**: yes, a core part of the design from the very start. Mixed DPI works.\n- **Fractional scaling**: yes, plus all niri UI stays pixel-perfect.\n- **NVIDIA**: seems to work fine.\n- **Floating windows**: yes, starting from niri 25.01.\n- **Input devices**: niri supports tablets, touchpads, and touchscreens.\nYou can map the tablet to a specific monitor, or use [OpenTabletDriver].\nWe have touchpad gestures, but no touchscreen gestures yet.\n- **Wlr protocols**: yes, we have most of the important ones like layer-shell, gamma-control, screencopy.\nYou can check on [wayland.app](https://wayland.app) at the bottom of each protocol's page.\n- **Performance**: while I run niri on beefy machines, I try to stay conscious of performance.\nI've seen someone use it fine on an EeeÂ PCÂ 900 fromÂ 2008, of all things.\n- **Xwayland**: [integrated](https://yalter.github.io/niri/Xwayland.html#using-xwayland-satellite) via xwayland-satellite starting from niri 25.08.\n\n## Media\n\n[niri: Making a Wayland compositor in Rust](https://youtu.be/Kmz8ODolnDg?list=PLRdS-n5seLRqrmWDQY4KDqtRMfIwU0U3T) Â· *December 2024*\n\nMy talk from the 2024 Moscow RustCon about niri, and how I do randomized property testing and profiling, and measure input latency.\nThe talk is in Russian, but I prepared full English subtitles that you can find in YouTube's subtitle language selector.\n\n[An interview with Ivan, the developer behind Niri](https://www.trommelspeicher.de/podcast/special_the_developer_behind_niri) Â· *June 2025*\n\nAn interview by a German tech podcast Das Triumvirat (in English).\nWe talk about niri development and history, and my experience building and maintaining niri.\n\n[A tour of the niri scrolling-tiling Wayland compositor](https://lwn.net/Articles/1025866/) Â· *July 2025*\n\nAn LWN article with a nice overview and introduction to niri.\n\n## Contributing\n\nIf you'd like to help with niri, there are plenty of both coding- and non-coding-related ways to do so.\nSee [CONTRIBUTING.md](https://github.com/YaLTeR/niri/blob/main/CONTRIBUTING.md) for an overview.\n\n## Inspiration\n\nNiri is heavily inspired by [PaperWM] which implements scrollable tiling on top of GNOME Shell.\n\nOne of the reasons that prompted me to try writing my own compositor is being able to properly separate the monitors.\nBeing a GNOME Shell extension, PaperWM has to work against Shell's global window coordinate space to prevent windows from overflowing.\n\n## Tile Scrollably Elsewhere\n\nHere are some other projects which implement a similar workflow:\n\n- [PaperWM]: scrollable tiling on top of GNOME Shell.\n- [karousel]: scrollable tiling on top of KDE.\n- [scroll](https://github.com/dawsers/scroll) and [papersway]: scrollable tiling on top of sway/i3.\n- [hyprscrolling] and [hyprslidr]: scrollable tiling on top of Hyprland.\n- [PaperWM.spoon]: scrollable tiling on top of macOS.\n\n## Contact\n\nOur main communication channel is a Matrix chat, feel free to join and ask a question: https://matrix.to/#/#niri:matrix.org\n\nWe also have a community Discord server: https://discord.gg/vT8Sfjy7sx\n\n[PaperWM]: https://github.com/paperwm/PaperWM\n[waybar]: https://github.com/Alexays/Waybar\n[fuzzel]: https://codeberg.org/dnkl/fuzzel\n[awesome-niri]: https://github.com/Vortriz/awesome-niri\n[karousel]: https://github.com/peterfajdiga/karousel\n[papersway]: https://spwhitton.name/tech/code/papersway/\n[hyprscrolling]: https://github.com/hyprwm/hyprland-plugins/tree/main/hyprscrolling\n[hyprslidr]: https://gitlab.com/magus/hyprslidr\n[PaperWM.spoon]: https://github.com/mogenson/PaperWM.spoon\n[Matrix channel]: https://matrix.to/#/#niri:matrix.org\n[OpenTabletDriver]: https://opentabletdriver.net/\n",
      "stars_today": 38
    },
    {
      "id": 20989983,
      "name": "pencil",
      "full_name": "evolus/pencil",
      "description": "The Pencil Project's unique mission is to build a free and opensource tool for making diagrams and GUI prototyping that everyone can use. ",
      "html_url": "https://github.com/evolus/pencil",
      "stars": 9452,
      "forks": 747,
      "language": "JavaScript",
      "topics": [
        "drawing",
        "electron",
        "javascript",
        "mockup",
        "nodejs",
        "pencil",
        "prototyping",
        "sketching",
        "ui-design",
        "wireframe"
      ],
      "created_at": "2014-06-19T06:23:06Z",
      "updated_at": "2026-01-28T02:10:37Z",
      "pushed_at": "2024-09-10T07:20:03Z",
      "open_issues": 531,
      "owner": {
        "login": "evolus",
        "avatar_url": "https://avatars.githubusercontent.com/u/19740693?v=4"
      },
      "readme": "\n![Image of Yaktocat](screenshot.png)\n\n\nAbout The Next Version\n==========\n\nPencil V3 is a rewrite of Pencil that aims to fix major performance and scalability issues of the application. The new version is under heavy development and we are expecting GA builds in June. The following list summarizes important changes in the new version:\n\n  * Pencil V3 uses Electron instead of Mozilla XULRunner as the runtime. We expect that by moving into this Nodejs-based environment, the Pencil source code can be much easier for all fellow developers to play with. The fact that Mozilla XULRunner is outdated is also a reason for the movement.\n  * A new, zip-based file format was introduced to support large documents and better embedding of external bitmaps/resources.\n  * A new mechanism for page management that dramatically reduces memory usage for large documents.\n  * Document pages can now be structured into a tree-like model.\n  * Custom fonts can now be embedded into Pencil .epz documents\n  * Printing and PDF export will be improved to support all platforms and with many options\n  * New UI approach\n\n\nSetting up\n==========\n\nPencil uses [Atom Electron](http://electron.atom.io/) as the runtime, follow the these steps to setup the environment:\n\n  * Clone this repository\n  * Make sure you are using *nodejs 5+* (Pencil development only needs *npm* for package management. Node runtime is _not_ required.)\n  * Checkout this _development_ branch:\n  \n    ```bash\n    $ git checkout development\n    ```\n    \n  * Install dependencies:\n\n      ```bash\n      $ npm install\n      ```\n      \n  * Start Pencil using the prebuilt version of Electron\n\n    ```bash\n      $ npm start\n    ```\n\n\nSupported Platforms\n==================\n\nOS X\n----\n\nOnly OSX 64bit 10.9 and later are supported.\n\nWindows\n------\n\nWindows 7 and later are supported, older operating systems are not supported (and do not work).\n\nLinux\n-----\n\n* Tested\n    * Ubuntu 12.04 and later\n    * Fedora 21+\n    * Debian 8+\n* Packages\n    * **Arch Linux** - [pencil](https://aur.archlinux.org/packages/pencil/) on the [Arch User Repository](https://aur.archlinux.org/).\n    * **Gentoo Linux** - [media-gfx/evoluspencil](https://packages.gentoo.org/packages/media-gfx/evoluspencil).\n",
      "stars_today": 38
    },
    {
      "id": 566578350,
      "name": "N64Recomp",
      "full_name": "N64Recomp/N64Recomp",
      "description": "Tool to statically recompile N64 games into native executables",
      "html_url": "https://github.com/N64Recomp/N64Recomp",
      "stars": 7551,
      "forks": 413,
      "language": "C++",
      "topics": [],
      "created_at": "2022-11-16T00:57:41Z",
      "updated_at": "2026-01-27T23:09:14Z",
      "pushed_at": "2026-01-17T05:58:46Z",
      "open_issues": 47,
      "owner": {
        "login": "N64Recomp",
        "avatar_url": "https://avatars.githubusercontent.com/u/169956306?v=4"
      },
      "readme": "# N64: Recompiled\nN64: Recompiled is a tool to statically recompile N64 binaries into C code that can be compiled for any platform. This can be used for ports or tools as well as for simulating behaviors significantly faster than interpreters or dynamic recompilation can. More widely, it can be used in any context where you want to run some part of an N64 binary in a standalone environment.\n\nThis is not the first project that uses static recompilation on game console binaries. A well known example is [jamulator](https://github.com/andrewrk/jamulator), which targets NES binaries. Additionally, this is not even the first project to apply static recompilation to N64-related projects: the [IDO static recompilation](https://github.com/decompals/ido-static-recomp) recompiles the SGI IRIX IDO compiler on modern systems to faciliate matching decompilation of N64 games. This project works similarly to the IDO static recomp project in some ways, and that project was my main inspiration for making this.\n\n## Table of Contents\n* [How it Works](#how-it-works)\n* [Overlays](#overlays)\n* [How to Use](#how-to-use)\n* [Single File Output Mode](#single-file-output-mode-for-patches)\n* [RSP Microcode Support](#rsp-microcode-support)\n* [Planned Features](#planned-features)\n* [Building](#building)\n\n## How it Works\nThe recompiler works by accepting a list of symbols and metadata alongside the binary with the goal of splitting the input binary into functions that are each individually recompiled into a C function, named according to the metadata.\n\nInstructions are processed one-by-one and corresponding C code is emitted as each one gets processed. This translation is very literal in order to keep complexity low. For example, the instruction `addiu $r4, $r4, 0x20`, which adds `0x20` to the 32-bit value in the low bytes of register `$r4` and stores the sign extended 64-bit result in `$r4`, gets recompiled into `ctx->r4 = ADD32(ctx->r4, 0X20);` The `jal` (jump-and-link) instruction is recompiled directly into a function call, and `j` or `b` instructions (unconditional jumps and branches) that can be identified as tail-call optimizations are also recompiled into function calls as well. Branch delay slots are handled by duplicating instructions as necessary. There are other specific behaviors for certain instructions, such as the recompiler attempting to turn a `jr` instruction into a switch-case statement if it can tell that it's being used with a jump table. The recompiler has mostly been tested on binaries built with old MIPS compilers (e.g. mips gcc 2.7.2 and IDO) as well as modern clang targeting mips. Modern mips gcc may trip up the recompiler due to certain optimizations it can do, but those cases can probably be avoided by setting specific compilation flags.\n\nEvery output function created by the recompiler is currently emitted into its own file. An option may be provided in the future to group functions together into output files, which should help improve build times of the recompiler output by reducing file I/O in the build process.\n\nRecompiler output can be compiled with any C compiler (tested with msvc, gcc and clang). The output is expected to be used with a runtime that can provide the necessary functionality and macro implementations to run it. A runtime is provided in [N64ModernRuntime](https://github.com/N64Recomp/N64ModernRuntime) which can be seen in action in the [Zelda 64: Recompiled](https://github.com/Zelda64Recomp/Zelda64Recomp) project.\n\n## Overlays\nStatically linked and relocatable overlays can both be handled by this tool. In both cases, the tool emits function lookups for jump-and-link-register (i.e. function pointers or virtual functions) which the provided runtime can implement using any sort of lookup table. For example, the instruction `jalr $25` would get recompiled as `LOOKUP_FUNC(ctx->r25)(rdram, ctx);` The runtime can then maintain a list of which program sections are loaded and at what address they are at in order to determine which function to run whenever a lookup is triggered during runtime.\n\nFor relocatable overlays, the tool will modify supported instructions possessing relocation data (`lui`, `addiu`, load and store instructions) by emitting an extra macro that enables the runtime to relocate the instruction's immediate value field. For example, the instruction `lui $24, 0x80C0` in a section beginning at address `0x80BFA100` with a relocation against a symbol with an address of `0x80BFA730` will get recompiled as `ctx->r24 = S32(RELOC_HI16(1754, 0X630) << 16);`, where 1754 is the index of this section. The runtime can then implement the RELOC_HI16 and RELOC_LO16 macros in order to handle modifying the immediate based on the current loaded address of the section.\n\nSupport for relocations for TLB mapping is coming in the future, which will add the ability to provide a list of MIPS32 relocations so that the runtime can relocate them on load. Combining this with the functionality used for relocatable overlays should allow running most TLB mapped code without incurring a performance penalty on every RAM access.\n\n## How to Use\nThe recompiler is configured by providing a toml file in order to configure the recompiler behavior, which is the first argument provided to the recompiler. The toml is where you specify input and output file paths, as well as optionally stub out specific functions, skip recompilation of specific functions, and patch single instructions in the target binary. There is also planned functionality to be able to emit hooks in the recompiler output by adding them to the toml (the `[[patches.func]]` and `[[patches.hook]]` sections of the linked toml below), but this is currently unimplemented. Documentation on every option that the recompiler provides is not currently available, but an example toml can be found in the Zelda 64: Recompiled project [here](https://github.com/Mr-Wiseguy/Zelda64Recomp/blob/dev/us.rev1.toml).\n\nCurrently, the only way to provide the required metadata is by passing an elf file to this tool. The easiest way to get such an elf is to set up a disassembly or decompilation of the target binary, but there will be support for providing the metadata via a custom format to bypass the need to do so in the future.\n\n## Single File Output Mode (for Patches)\nThis tool can also be configured to recompile in \"single file output\" mode via an option in the configuration toml. This will emit all of the functions in the provided elf into a single output file. The purpose of this mode is to be able to compile patched versions of functions from the target binary.\n\nThis mode can be combined with the functionality provided by almost all linkers (ld, lld, MSVC's link.exe, etc.) to replace functions from the original recompiler output with modified versions. Those linkers only look for symbols in a static library if they weren't already found in a previous input file, so providing the recompiled patches to the linker before providing the original recompiler output will result in the patches taking priority over functions with the same names from the original recompiler output.\n\nThis saves a tremendous amount of time while iterating on patches for the target binary, as you can bypass rerunning the recompiler on the target binary as well as compiling the original recompiler output. An example of using this single file output mode for that purpose can be found in the Zelda 64: Recompiled project [here](https://github.com/Mr-Wiseguy/Zelda64Recomp/blob/dev/patches.toml), with the corresponding Makefile that gets used to build the elf for those patches [here](https://github.com/Mr-Wiseguy/Zelda64Recomp/blob/dev/patches/Makefile).\n\n## RSP Microcode Support\nRSP microcode can also be recompiled with this tool. Currently there is no support for recompiling RSP overlays, but it may be added in the future if desired. Documentation on how to use this functionality will be coming soon.\n\n## Planned Features\n* Custom metadata format to provide symbol names, relocations, and any other necessary data in order to operate without an elf\n* Emitting multiple functions per output file to speed up compilation\n* Support for recording MIPS32 relocations to allow runtimes to relocate them for TLB mapping\n* Ability to recompile into a dynamic language (such as Lua) to be able to load code at runtime for mod support\n\n## Building\nThis project can be built with CMake 3.20 or above and a C++ compiler that supports C++20. This repo uses git submodules, so be sure to clone recursively (`git clone --recurse-submodules`) or initialize submodules recursively after cloning (`git submodule update --init --recursive`). From there, building is identical to any other cmake project, e.g. run `cmake` in the target build folder and point it at the root of this repo, then run `cmake --build .` from that target folder.\n\n## Libraries Used\n* [rabbitizer](https://github.com/Decompollaborate/rabbitizer) for instruction decoding/analysis\n* [ELFIO](https://github.com/serge1/ELFIO) for elf parsing\n* [toml11](https://github.com/ToruNiina/toml11) for toml parsing\n* [fmtlib](https://github.com/fmtlib/fmt)\n",
      "stars_today": 38
    },
    {
      "id": 1047497737,
      "name": "aws-doctor",
      "full_name": "elC0mpa/aws-doctor",
      "description": "Diagnose AWS costs, detect idle resources, and optimize cloud spending directly from your terminal. ğŸ©º â˜ï¸",
      "html_url": "https://github.com/elC0mpa/aws-doctor",
      "stars": 254,
      "forks": 11,
      "language": "Go",
      "topics": [
        "aws",
        "aws-billing",
        "aws-cli",
        "aws-cost-explorer",
        "aws-doctor",
        "aws-trusted-advisor",
        "cli",
        "go",
        "golang"
      ],
      "created_at": "2025-08-30T14:57:45Z",
      "updated_at": "2026-01-28T01:17:38Z",
      "pushed_at": "2026-01-27T04:45:49Z",
      "open_issues": 6,
      "owner": {
        "login": "elC0mpa",
        "avatar_url": "https://avatars.githubusercontent.com/u/53531665?v=4"
      },
      "readme": "# aws-doctor\n\n[![CI](https://github.com/elC0mpa/aws-doctor/actions/workflows/ci.yml/badge.svg)](https://github.com/elC0mpa/aws-doctor/actions/workflows/ci.yml)\n[![Go Report Card](https://goreportcard.com/badge/github.com/elC0mpa/aws-doctor)](https://goreportcard.com/report/github.com/elC0mpa/aws-doctor)\n[![Go Reference](https://pkg.go.dev/badge/github.com/elC0mpa/aws-doctor.svg)](https://pkg.go.dev/github.com/elC0mpa/aws-doctor)\n[![Go Version](https://img.shields.io/github/go-mod/go-version/elC0mpa/aws-doctor)](https://github.com/elC0mpa/aws-doctor/blob/main/go.mod)\n[![License](https://img.shields.io/github/license/elC0mpa/aws-doctor)](https://github.com/elC0mpa/aws-doctor/blob/main/LICENSE)\n[![Maintained](https://img.shields.io/badge/Maintained-yes-green.svg)](https://github.com/elC0mpa/aws-doctor/commits/main)\n\nA terminal-based tool that acts as a comprehensive health check for your AWS accounts. Built with Golang, **aws-doctor** diagnoses cost anomalies, detects idle resources, and provides a proactive analysis of your cloud infrastructureâ€”effectively giving you the insights of AWS Trusted Advisor without the need for a Business or Enterprise support plan.\n\n![](https://github.com/elC0mpa/aws-cost-billing/blob/main/assets/logo.webp)\n\n## Demo\n\n### Basic usage\n\n![](https://github.com/elC0mpa/aws-cost-billing/blob/main/demo/basic.gif)\n\n### Trend\n\n![](https://github.com/elC0mpa/aws-cost-billing/blob/main/demo/trend.gif)\n\n### Waste\n\n![](https://github.com/elC0mpa/aws-cost-billing/blob/main/demo/waste.gif)\n\n## Features\n\n- **ğŸ“‰ Cost Comparison:** Compares costs between the current and previous month for the exact same period (e.g., comparing Jan 1â€“15 vs Feb 1â€“15) to give a fair assessment of spending velocity.\n- **ğŸ¥ Waste Detection (The \"Checkup\"):** Scans your account for \"zombie\" resources and inefficiencies that are silently inflating your bill.\n- **ğŸ“Š Trend Analysis:** Visualizes cost history over the last 6 months to spot long-term anomalies.\n\n## Motivation\n\nAs a Cloud Architect, I often need to check AWS costs and billing information. While the AWS Console provides raw data, it lacks the immediate context I need to answer the question: _\"Are we spending efficiently?\"_\n\nI created **aws-doctor** to fill that gap. It doesn't just show you the bill; it acts as a diagnostic tool that helps you understand **where** the money is going and **what** can be cleaned up. It automates the routine checks I used to perform manually, serving as a free, open-source alternative to the paid recommendations found in AWS Trusted Advisor.\n\n## Installation\n\n### Quick Install (macOS/Linux)\n\n```bash\ncurl -sSfL https://raw.githubusercontent.com/elC0mpa/aws-doctor/main/install.sh | sh\n```\n\n### Using Go\n\n```bash\ngo install github.com/elC0mpa/aws-doctor@latest\n```\n\n### Download Binary\n\nDownload the latest release for your platform from the [Releases page](https://github.com/elC0mpa/aws-doctor/releases).\n\nAvailable platforms:\n\n- macOS (Intel & Apple Silicon)\n- Linux (amd64 & arm64)\n- Windows (amd64)\n\n> [!TIP]\n> Once installed, you can keep **aws-doctor** up to date by running `aws-doctor --update`.\n\n## Flags\n\n- `--profile`: Specify the AWS profile to use (default is \"\").\n- `--region`: Specify the AWS region to use. If not provided, uses `AWS_REGION` or `AWS_DEFAULT_REGION` environment variables, or the region from `~/.aws/config`.\n- `--trend`: Shows a trend analysis for the last 6 months.\n- `--output`: Output format: `table` (default) or `json`.\n- `--waste`: Makes an analysis of possible money waste you have in your AWS Account.\n  - [x] Unused EBS Volumes (not attached to any instance).\n  - [x] EBS Volumes attached to stopped EC2 instances.\n  - [x] Unassociated Elastic IPs.\n  - [x] EC2 reserved instance that are scheduled to expire in the next 30 days or have expired in the preceding 30 days.\n  - [x] EC2 instance stopped for more than 30 days.\n  - [x] Load Balancers with no attached target groups.\n  - [x] Unused AMIs (not associated with any running or stopped instance and created more than 90 days ago).\n  - [x] Orphaned EBS Snapshots (source volume deleted and not used by any AMI).\n  - [x] Stale EBS Snapshots (created more than 90 days ago, source volume exists and not used by any AMI).\n  - [ ] Inactive VPC interface endpoints.\n  - [ ] Inactive NAT Gateways.\n  - [ ] Idle Load Balancers.\n  - [ ] RDS Idle DB Instances.\n- `--version`: Display version information.\n- `--update`: Updates the tool to the latest version.\n\n## Roadmap\n\n- [x] Add monthly trend analysis\n- [x] Add waste / wastage analysis logic\n- [x] Export reports to JSON format\n- [ ] Export reports to CSV and PDF formats (medical records for your cloud)\n- [ ] Distribute the CLI via Fedora, Ubuntu, and macOS repositories\n",
      "stars_today": 36
    },
    {
      "id": 643647588,
      "name": "AeroSpace",
      "full_name": "nikitabobko/AeroSpace",
      "description": "AeroSpace is an i3-like tiling window manager for macOS",
      "html_url": "https://github.com/nikitabobko/AeroSpace",
      "stars": 18446,
      "forks": 397,
      "language": "Swift",
      "topics": [
        "apple",
        "i3",
        "i3wm",
        "mac",
        "macos",
        "swift",
        "tiling",
        "tiling-window-manager",
        "window-manager"
      ],
      "created_at": "2023-05-21T20:20:17Z",
      "updated_at": "2026-01-28T02:00:12Z",
      "pushed_at": "2026-01-27T17:12:39Z",
      "open_issues": 182,
      "owner": {
        "login": "nikitabobko",
        "avatar_url": "https://avatars.githubusercontent.com/u/20517828?v=4"
      },
      "readme": "# AeroSpace Beta [![Build](https://github.com/nikitabobko/AeroSpace/actions/workflows/build.yml/badge.svg?branch=main)](https://github.com/nikitabobko/AeroSpace/actions/workflows/build.yml)\n\n<img src=\"./resources/Assets.xcassets/AppIcon.appiconset/icon.png\" width=\"40%\" align=\"right\">\n\nAeroSpace is an i3-like tiling window manager for macOS\n\nVideos:\n- [YouTube 91 sec Demo](https://www.youtube.com/watch?v=UOl7ErqWbrk)\n- [YouTube Guide by Josean Martinez](https://www.youtube.com/watch?v=-FoWClVHG5g)\n\nDocs:\n- [AeroSpace Guide](https://nikitabobko.github.io/AeroSpace/guide)\n- [AeroSpace Commands](https://nikitabobko.github.io/AeroSpace/commands)\n- [AeroSpace Goodies](https://nikitabobko.github.io/AeroSpace/goodies)\n\n## Key features\n\n- Tiling window manager based on a [tree paradigm](https://nikitabobko.github.io/AeroSpace/guide#tree)\n- [i3](https://i3wm.org/) inspired\n- Fast workspaces switching without animations and without the necessity to disable SIP\n- AeroSpace employs its [own emulation of virtual workspaces](https://nikitabobko.github.io/AeroSpace/guide#emulation-of-virtual-workspaces) instead of relying on native macOS Spaces due to [their considerable limitations](https://nikitabobko.github.io/AeroSpace/guide#emulation-of-virtual-workspaces)\n- Plain text configuration (dotfiles friendly). See: [default-config.toml](https://nikitabobko.github.io/AeroSpace/guide#default-config)\n- CLI first (manpages and shell completion included)\n- Doesn't require disabling SIP (System Integrity Protection)\n- [Proper multi-monitor support](https://nikitabobko.github.io/AeroSpace/guide#multiple-monitors) (i3-like paradigm)\n\n## Installation\n\nInstall via [Homebrew](https://brew.sh/) to get autoupdates (Preferred)\n\n```\nbrew install --cask nikitabobko/tap/aerospace\n```\n\nIn multi-monitor setup please make sure that monitors [are properly arranged](https://nikitabobko.github.io/AeroSpace/guide#proper-monitor-arrangement).\n\nOther installation options: https://nikitabobko.github.io/AeroSpace/guide#installation\n\n> [!NOTE]\n> By using AeroSpace, you acknowledge that it's not [notarized](https://developer.apple.com/documentation/security/notarizing_macos_software_before_distribution).\n>\n> Notarization is a \"security\" feature by Apple.\n> You send binaries to Apple, and they either approve them or not.\n> In reality, notarization is about building binaries the way Apple likes it.\n>\n> I don't have anything against notarization as a concept.\n> I specifically don't like the way Apple does notarization.\n> I don't have time to deal with Apple.\n>\n> [Homebrew installation script](https://github.com/nikitabobko/homebrew-tap/blob/main/Casks/aerospace.rb) is configured to\n> automatically delete `com.apple.quarantine` attribute, that's why the app should work out of the box, without any warnings that\n> \"Apple cannot check AeroSpace for malicious software\"\n\n## Community, discussions, issues\n\nAeroSpace project doesn't accept Issues directly - we ask you to create a [Discussion](https://github.com/nikitabobko/AeroSpace/discussions) first.\nPlease read [CONTRIBUTING.md](./CONTRIBUTING.md) for more details.\n\nCommunity discussions happen at GitHub Discussions.\nThere you can discuss bugs, propose new features, ask your questions, show off your setup, or just chat.\n\nThere are 7 channels:\n-   [#all](https://github.com/nikitabobko/AeroSpace/discussions).\n    [RSS](https://github.com/nikitabobko/AeroSpace/discussions.atom?discussions_q=sort%3Adate_created).\n    Feed with all discussions.\n-   [#announcements](https://github.com/nikitabobko/AeroSpace/discussions/categories/announcements).\n    [RSS](https://github.com/nikitabobko/AeroSpace/discussions/categories/announcements.atom?discussions_q=category%3Aannouncements+sort%3Adate_created).\n    Only maintainers can post here.\n    Highly moderated traffic.\n-   [#announcements-releases](https://github.com/nikitabobko/AeroSpace/discussions/categories/announcements-releases).\n    [RSS](https://github.com/nikitabobko/AeroSpace/discussions/categories/announcements-releases.atom?discussions_q=category%3Aannouncements-releases+sort%3Adate_created).\n    Announcements about non-patch releases.\n    Only maintainers can post here.\n-   [#feature-ideas](https://github.com/nikitabobko/AeroSpace/discussions/categories/feature-ideas).\n    [RSS](https://github.com/nikitabobko/AeroSpace/discussions/categories/feature-ideas.atom?discussions_q=category%3Afeature-ideas+sort%3Adate_created).\n-   [#general](https://github.com/nikitabobko/AeroSpace/discussions/categories/general).\n    [RSS](https://github.com/nikitabobko/AeroSpace/discussions/categories/general.atom?discussions_q=sort%3Adate_created+category%3Ageneral).\n-   [#potential-bugs](https://github.com/nikitabobko/AeroSpace/discussions/categories/potential-bugs).\n    [RSS](https://github.com/nikitabobko/AeroSpace/discussions/categories/potential-bugs.atom?discussions_q=category%3Apotential-bugs+sort%3Adate_created).\n    If you think that you have encountered a bug, you can discuss your bugs here.\n-   [#questions-and-answers](https://github.com/nikitabobko/AeroSpace/discussions/categories/questions-and-answers).\n    [RSS](https://github.com/nikitabobko/AeroSpace/discussions/categories/questions-and-answers.atom?discussions_q=category%3Aquestions-and-answers+sort%3Adate_created).\n    Everyone is welcome to ask questions.\n    Everyone is encouraged to answer other people's questions.\n\n## Project status\n\nPublic Beta. AeroSpace can be used as a daily driver, but expect breaking changes until 1.0 is reached.\n\nWhat stops us from 1.0 release:\n- [x] https://github.com/nikitabobko/AeroSpace/issues/131 Performance. Implement thread-per-application to circumvent macOS blocking AX API.\n- [ ] https://github.com/nikitabobko/AeroSpace/issues/1215 _Big refactoring_. Rewrite mutable double-linked core tree data structure to immutable single-linked persistent tree.\n  Important for: stability and potential performance\n  - [ ] https://github.com/nikitabobko/AeroSpace/issues/1216 The big refactoring will help us to fix stability issue that windows may randomly jump to the focused workspace\n  - [ ] https://github.com/nikitabobko/AeroSpace/issues/68 The big refactoring will help us to support macOS native tabs\n- [ ] https://github.com/nikitabobko/AeroSpace/issues/278 Implement shell-like combinators.\n  Ignore a lot of crazy fuss in the issue,\n  We are most probably going with the minimal approach to only introduce common shell-combinators: `||`, `&&`, `;` and `eval` command to send multiple commands in one go.\n- [ ] https://github.com/nikitabobko/AeroSpace/issues/1012 Investigate a possibility to use `CGEvent.tapCreate` API for global hotkeys\n  - [ ] https://github.com/nikitabobko/AeroSpace/issues/28 Maybe it will allow to distinguish left and right modifiers. Maybe not\n\nBig and important issues which will go after 1.0 release:\n- [ ] https://github.com/nikitabobko/AeroSpace/issues/2 sticky windows\n- [ ] https://github.com/nikitabobko/AeroSpace/issues/260 Dynamic TWM\n\n## Development\n\nA notes on how to setup the project, build it, how to run the tests, etc. can be found here: [dev-docs/development.md](./dev-docs/development.md)\n\n## Project values\n\n**Values**\n- AeroSpace is targeted at advanced users and developers\n- Keyboard centric\n- Breaking changes (configuration files, CLI, behavior) are avoided as much as possible, but it must not let the software stagnate.\n  Thus breaking changes can happen, but with careful considerations and helpful message.\n  [Semver](https://semver.org/) major version is bumped in case of a breaking change (It's all guaranteed once AeroSpace reaches 1.0 version, until then breaking changes just happen)\n- AeroSpace doesn't use GUI, unless necessarily\n  - AeroSpace will never provide a GUI for configuration.\n    For advanced users, it's easier to edit a configuration file in text editor rather than navigating through checkboxes in GUI.\n  - Status menu icon is ok, because visual feedback is needed\n- Provide _practical_ features. Fancy appearance features are not _practical_ (e.g. window borders, transparency, animations, etc.)\n- \"dark magic\" (aka \"private APIs\", \"code injections\", etc.) must be avoided as much as possible\n  - Right now, AeroSpace uses only a single private API to get window ID of accessibility object `_AXUIElementGetWindow`.\n    Everything else is [macOS public accessibility API](https://developer.apple.com/documentation/applicationservices/axuielement_h).\n  - AeroSpace will never require you to disable SIP (System Integrity Protection).\n  - The goal is to make AeroSpace easily maintainable, and resistant to macOS updates.\n\n**Non Values**\n- Play nicely with existing macOS features.\n  If limitations are imposed then AeroSpace won't play nicely with existing macOS features\n  (For example, AeroSpace doesn't acknowledge the existence of macOS Spaces, and it uses [emulation of its own workspaces](https://nikitabobko.github.io/AeroSpace/guide#emulation-of-virtual-workspaces))\n- Ricing.\n  AeroSpace provides only a very minimal support for ricing - gaps and a few callbacks for integrations with bars.\n  The current maintainer doesn't care about ricing.\n  Ricing issues are not a priority, and they are mostly ignored.\n  The ricing stance can change only with the appearance of more maintainers.\n\n## macOS compatibility table\n\n|                                                                                | macOS 13 (Ventura) | macOS 14 (Sonoma) | macOS 15 (Sequoia) | macOS 26 (Tahoe) |\n| ------------------------------------------------------------------------------ | ------------------ | ----------------- | ------------------ | ---------------- |\n| AeroSpace binary runs on ...                                                   | +                  | +                 | +                  | +                |\n| AeroSpace debug build from sources is supported on ...                         |                    | +                 | +                  | +                |\n| AeroSpace release build from sources is supported on ... (Requires Xcode 26+)  |                    |                   | +                  | +                |\n\n## Sponsorship\n\nAeroSpace is developed and maintained in my free time.\nIf you find it useful, [consider sponsoring](https://github.com/sponsors/nikitabobko#sponsors).\n\n## People who have write access\n\nIn alphabetical order:\n\n- [@mobile-ar](https://github.com/mobile-ar/)\n- [@nikitabobko](https://github.com/nikitabobko/)\n\n## Tip of the day\n\n```bash\ndefaults write -g NSWindowShouldDragOnGesture -bool true\n```\n\nNow, you can move windows by holding `ctrl`+`cmd` and dragging any part of the window (not necessarily the window title)\n\nSource: [reddit](https://www.reddit.com/r/MacOS/comments/k6hiwk/keyboard_modifier_to_simplify_click_drag_of/)\n\n## Related projects\n\n- [Amethyst](https://github.com/ianyh/Amethyst)\n- [yabai](https://github.com/koekeishiya/yabai)\n",
      "stars_today": 34
    },
    {
      "id": 447363864,
      "name": "Baileys",
      "full_name": "WhiskeySockets/Baileys",
      "description": "Socket-based TS/JavaScript API for WhatsApp Web",
      "html_url": "https://github.com/WhiskeySockets/Baileys",
      "stars": 7869,
      "forks": 2628,
      "language": "JavaScript",
      "topics": [
        "bun",
        "deno",
        "nodejs",
        "reverse-engineering",
        "typescript",
        "websockets",
        "whatsapp-web",
        "ws"
      ],
      "created_at": "2022-01-12T20:38:14Z",
      "updated_at": "2026-01-27T23:56:59Z",
      "pushed_at": "2026-01-26T22:38:53Z",
      "open_issues": 235,
      "owner": {
        "login": "WhiskeySockets",
        "avatar_url": "https://avatars.githubusercontent.com/u/131354555?v=4"
      },
      "readme": "<h1><img alt=\"Baileys logo\" src=\"https://raw.githubusercontent.com/WhiskeySockets/Baileys/refs/heads/master/Media/logo.png\" height=\"75\"/></h1>\n\n\n> [!CAUTION]\n> NOTICE OF BREAKING CHANGE.\n>\n> As of 7.0.0, multiple breaking changes were introduced into the library.\n>\n> Please check out https://whiskey.so/migrate-latest for more information.\n\nBaileys is a WebSockets-based TypeScript library for interacting with the WhatsApp Web API.\n\nJoin the WhiskeySockets community via the link: https://whiskey.so/discord\n\n# Usage & Guide\n\n> [!IMPORTANT]\n> The new guide is a work in progress. Expect missing pages/content. [Report missing or incorrect content.](https://github.com/WhiskeySockets/baileys.wiki-site/issues/new)\n>\n> **You can still access the old guide here:** [README.md](https://github.com/WhiskeySockets/Baileys/tree/master/README.md), or the [NPM homepage](https://npmjs.com/package/baileys).\n\nThe new guide is posted at https://baileys.wiki .\n\n# Get Support\n\nIf you'd like business to enterprise-level support from Rajeh, the current maintainer of Baileys, you can book a video chat. Book a 1 hour time slot by contacting him on Discord or pre-ordering [here](https://purpshell.dev/book). The earlier you pre-order the better, as his time slots usually fill up very quickly. He offers immense value per hour and will answer all your questions before the time runs out.\n\nIf you are a business, we encourage you to contribute back to the high development costs of the project and to feed the maintainers who dump tens of hours a week on this. You can do so by booking meetings or sponsoring below. All support, even in bona fide / contribution hours, is welcome by businesses of all sizes. This is not condoning or endorsing businesses to use the library. See the Disclaimer below.\n\n# Sponsor\n\nIf you'd like to financially support this project, you can do so by supporting the current maintainer [here](https://purpshell.dev/sponsor).\n\n# Disclaimer\n> [!CAUTION]\n> This project is not affiliated, associated, authorized, endorsed by, or in any way officially connected with WhatsApp or any of its subsidiaries or its affiliates.\n> The official WhatsApp website can be found at whatsapp.com. \"WhatsApp\" as well as related names, marks, emblems and images are registered trademarks of their respective owners.\n>\n> The maintainers of Baileys do not in any way condone the use of this application in practices that violate the Terms of Service of WhatsApp. The maintainers of this application call upon the personal responsibility of its users to use this application in a fair way, as it is intended to be used.\n> Use at your own discretion. Do not spam people with this. We discourage any stalkerware, bulk or automated messaging usage.\n\n# License\nCopyright (c) 2025 Rajeh Taher/WhiskeySockets\n\nLicensed under the MIT License:\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nThus, the maintainers of the project can't be held liable for any potential misuse of this project.\n",
      "stars_today": 30
    },
    {
      "id": 926616051,
      "name": "mangowc",
      "full_name": "DreamMaoMao/mangowc",
      "description": "wayland compositor base wlroots and scenefx(dwm but wayland)",
      "html_url": "https://github.com/DreamMaoMao/mangowc",
      "stars": 1839,
      "forks": 95,
      "language": "C",
      "topics": [
        "compositor",
        "wayland",
        "wayland-compositor",
        "wlroots"
      ],
      "created_at": "2025-02-03T15:16:36Z",
      "updated_at": "2026-01-28T01:40:46Z",
      "pushed_at": "2026-01-28T01:44:26Z",
      "open_issues": 100,
      "owner": {
        "login": "DreamMaoMao",
        "avatar_url": "https://avatars.githubusercontent.com/u/30348075?v=4"
      },
      "readme": "# Mango Wayland Compositor\n<div>\n  <img src=\"https://github.com/DreamMaoMao/mangowc/blob/main/assets/mango-transparency-256.png\" alt=\"MangoWC Logo\" width=\"120\"/>\n</div>\n\nThis project's development is based on [dwl](https://codeberg.org/dwl/dwl/).\n\n\n1. **Lightweight & Fast Build**\n\n   - _Mango_ is as lightweight as _dwl_, and can be built completely within a few seconds. Despite this, _Mango_ does not compromise on functionality.\n\n2. **Feature Highlights**\n   - In addition to basic WM functionality, Mango provides:\n     - Excellent xwayland support.\n     - Base tags not workspaces (supports separate window layouts for each tag)\n     - Smooth and customizable complete animations (window open/move/close, tag enter/leave,layer open/close/move)\n     - Excellent input method support (text input v2/v3)\n     - Flexible window layouts with easy switching (scroller, master-stack, monocle,center-master, etc.)\n     - Rich window states (swallow, minimize, maximize, unglobal, global, fakefullscreen, overlay, etc.)\n     - Simple yet powerful external configuration(support shortcuts hot-reload)\n     - Sway-like scratchpad and named scratchpad\n     - Ipc support(get/send message from/to compositor by external program)\n     - Hycov-like overview\n     - Window effects from scenefx (blur, shadow, corner radius, opacity)\n\nMaster-Stack Layout\n\nhttps://github.com/user-attachments/assets/a9d4776e-b50b-48fb-94ce-651d8a749b8a\n\nScroller Layout\n\nhttps://github.com/user-attachments/assets/c9bf9415-fad1-4400-bcdc-3ad2d76de85a\n\nLayer animation\n\nhttps://github.com/user-attachments/assets/014c893f-115c-4ae9-8342-f9ae3e9a0df0\n\n\n# Our discord\n[mangowc](https://discord.gg/CPjbDxesh5)\n\n# Supported layouts\n\n- tile\n- scroller\n- monocle\n- grid\n- deck\n- center_tile\n- vertical_tile\n- vertical_grid\n- vertical_scroller\n\n# Installation\n\n## Dependencies\n\n- glibc\n- wayland\n- wayland-protocols\n- libinput\n- libdrm\n- libxkbcommon\n- pixman\n- git\n- meson\n- ninja\n- libdisplay-info\n- libliftoff\n- hwdata\n- seatd\n- pcre2\n- xorg-xwayland\n- libxcb\n\n## Arch Linux\nThe package is in the Arch User Repository and is availble for manual download [here](https://aur.archlinux.org/packages/mangowc-git) or through a AUR helper like yay:\n```bash\nyay -S mangowc-git\n\n```\n\n## Gentoo Linux\nThe package is in the community-maintained repository called GURU.\nFirst, add GURU repository:\n\n```bash\nemerge --ask --verbose eselect-repository\neselect repository enable guru\nemerge --sync guru\n```\n\nThen, add `gui-libs/scenefx` and `gui-wm/mangowc` to the `package.accept_keywords`.\n\nFinally, install the package:\n\n```bash\nemerge --ask --verbose gui-wm/mangowc\n```\n\n## Fedora Linux\nThe package is in the third-party Terra repository.\nFirst, add the [Terra Repository](https://terra.fyralabs.com/).\n\nThen, install the package:\n\n```bash\ndnf install mangowc\n```\n\n## Other\n\n```bash\ngit clone -b 0.19.2 https://gitlab.freedesktop.org/wlroots/wlroots.git\ncd wlroots\nmeson build -Dprefix=/usr\nsudo ninja -C build install\n\ngit clone -b 0.4.1 https://github.com/wlrfx/scenefx.git\ncd scenefx\nmeson build -Dprefix=/usr\nsudo ninja -C build install\n\ngit clone https://github.com/DreamMaoMao/mangowc.git\ncd mangowc\nmeson build -Dprefix=/usr\nsudo ninja -C build install\n```\n\n## Suggested Tools\n\n### Hybrid component\n- [dms-shell](https://github.com/AvengeMedia/DankMaterialShell)\n\n### Independent component\n- Application launcher (rofi, bemenu, wmenu, fuzzel)\n- Terminal emulator (foot, wezterm, alacritty, kitty, ghostty)\n- Status bar (waybar, eww, quickshell, ags), waybar is preferred\n- Wallpaper setup (swww, swaybg)\n- Notification daemon (swaync, dunst,mako)\n- Desktop portal (xdg-desktop-portal, xdg-desktop-portal-wlr, xdg-desktop-portal-gtk)\n- Clipboard (wl-clipboard, wl-clip-persist, cliphist)\n- Gamma control/night light (wlsunset, gammastep)\n- Miscellaneous (xfce-polkit, wlogout)\n\n## Some Common Default Keybindings\n\n- alt+return: open foot terminal\n- alt+space: open rofi launcher\n- alt+q: kill client\n- alt+left/right/up/down: focus direction\n- super+m: quit mango\n\n## My Dotfiles\n\n### Daily\n- Dependencies\n\n```bash\nyay -S rofi foot xdg-desktop-portal-wlr swaybg waybar wl-clip-persist cliphist wl-clipboard wlsunset xfce-polkit swaync pamixer wlr-dpms sway-audio-idle-inhibit-git swayidle dimland-git brightnessctl swayosd wlr-randr grim slurp satty swaylock-effects-git wlogout sox\n```\n\n### Dms\n- Dependencies\n```bash\nyay -S foot xdg-desktop-portal-wlr swaybg wl-clip-persist cliphist wl-clipboard sway-audio-idle-inhibit-git brightnessctl grim slurp satty matugen-bin dms-shell-git\n\n```\n- use my dms config\n\n```bash\ngit clone -b dms https://github.com/DreamMaoMao/mango-config.git ~/.config/mango\n```\n- use my daily config\n\n```bash\ngit clone https://github.com/DreamMaoMao/mango-config.git ~/.config/mango\n```\n\n\n## Config Documentation\n\nRefer to the repo wiki [wiki](https://github.com/DreamMaoMao/mango/wiki/)\n\nor the website docs [docs](https://mangowc.vercel.app/docs)\n\n# NixOS + Home-manager\n\nThe repo contains a flake that provides a NixOS module and a home-manager module for mango.\nUse the NixOS module to install mango with other necessary components of a working Wayland environment.\nUse the home-manager module to declare configuration and autostart for mango.\n\nHere's an example of using the modules in a flake:\n\n```nix\n{\n  inputs = {\n    nixpkgs.url = \"github:NixOS/nixpkgs/nixos-unstable\";\n    home-manager = {\n      url = \"github:nix-community/home-manager\";\n      inputs.nixpkgs.follows = \"nixpkgs\";\n    };\n    flake-parts.url = \"github:hercules-ci/flake-parts\";\n    mango = {\n      url = \"github:DreamMaoMao/mango\";\n      inputs.nixpkgs.follows = \"nixpkgs\";\n    };\n  };\n  outputs =\n    inputs@{ self, flake-parts, ... }:\n    flake-parts.lib.mkFlake { inherit inputs; } {\n      debug = true;\n      systems = [ \"x86_64-linux\" ];\n      flake = {\n        nixosConfigurations = {\n          hostname = inputs.nixpkgs.lib.nixosSystem {\n            system = \"x86_64-linux\";\n            modules = [\n              inputs.home-manager.nixosModules.home-manager\n\n              # Add mango nixos module\n              inputs.mango.nixosModules.mango\n              {\n                programs.mango.enable = true;\n              }\n              {\n                home-manager = {\n                  useGlobalPkgs = true;\n                  useUserPackages = true;\n                  backupFileExtension = \"backup\";\n                  users.\"username\".imports =\n                    [\n                      (\n                        { ... }:\n                        {\n                          wayland.windowManager.mango = {\n                            enable = true;\n                            settings = ''\n                              # see config.conf\n                            '';\n                            autostart_sh = ''\n                              # see autostart.sh\n                              # Note: here no need to add shebang\n                            '';\n                          };\n                        }\n                      )\n                    ]\n                    ++ [\n                      # Add mango hm module\n                      inputs.mango.hmModules.mango\n                    ];\n                };\n              }\n            ];\n          };\n        };\n      };\n    };\n}\n```\n\n# Packaging mango\n\nTo package mango for other distributions, you can check the reference setup for:\n\n- [nix](https://github.com/DreamMaoMao/mangowc/blob/main/nix/default.nix)\n- [arch](https://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=mangowc-git).\n- [gentoo](https://data.gpo.zugaina.org/guru/gui-wm/mangowc)\n\nYou might need to package `scenefx` for your distribution, check availability [here](https://github.com/wlrfx/scenefx.git).\n\nIf you encounter build errors when packaging `mango`, feel free to create an issue and ask a question, but\nRead The Friendly Manual on packaging software in your distribution first.\n\n# Thanks to These Reference Repositories\n\n- https://gitlab.freedesktop.org/wlroots/wlroots - Implementation of Wayland protocol\n\n- https://github.com/dqrk0jeste/owl - Basal window animation\n\n- https://codeberg.org/dwl/dwl - Basal dwl feature\n\n- https://github.com/swaywm/sway - Sample of Wayland protocol\n\n- https://github.com/wlrfx/scenefx - Make it simple to add window effect.\n\n\n# Sponsor\nAt present, I can only accept sponsorship through an encrypted connection.\nIf you find this project helpful to you, you can offer sponsorship in the following ways.\n\n<img width=\"650\" height=\"870\" alt=\"image\" src=\"https://github.com/user-attachments/assets/8c860317-90d2-4071-971d-f1a92b674469\" />\n\n\nThanks to the following friends for their sponsorship of this project\n\n[@tonybanters](https://github.com/tonybanters)\n",
      "stars_today": 30
    },
    {
      "id": 490436263,
      "name": "DPaint-js",
      "full_name": "steffest/DPaint-js",
      "description": "Webbased image editor, modeled after the legendary Deluxe Paint with a focus on retro Amiga file formats: read and write Amiga icon files and IFF ILBM images",
      "html_url": "https://github.com/steffest/DPaint-js",
      "stars": 901,
      "forks": 51,
      "language": "JavaScript",
      "topics": [
        "amiga",
        "javascript"
      ],
      "created_at": "2022-05-09T20:33:55Z",
      "updated_at": "2026-01-27T22:38:14Z",
      "pushed_at": "2026-01-26T11:22:55Z",
      "open_issues": 25,
      "owner": {
        "login": "steffest",
        "avatar_url": "https://avatars.githubusercontent.com/u/763047?v=4"
      },
      "readme": "# DPaint.js\nWebbased image editor modeled after the legendary [Deluxe Paint](https://en.wikipedia.org/wiki/Deluxe_Paint) with a focus on retro Amiga file formats.\nNext to modern image formats, DPaint.js can read and write Amiga icon files and IFF ILBM images.\n\n![DPaint.js Logo](./_img/dpaint-logo.png?raw=true)\n\nOnline version available at https://www.stef.be/dpaint/\n\n![DPaint.js UI](./_img/ui.png?raw=true)\n\n## Main Features\n - Fully Featured image editor with a.o.\n   - Layers\n   - Selections\n   - Masking\n   - Transformation tools\n   - Effects and filters\n   - Multiple undo/redo\n   - Copy/Paste from any other image program or image source\n   - Customizable dither tools\n   - Color Cycling\n - Heavy focus on colour reduction with fine-grained dithering options\n - Amiga focus\n   - Read/write/convert Amiga icon files (all formats)\n   - Reads IFF ILBM images (all formats including HAM and 24-bit)\n   - Writes IFF ILBM images (up to 256 colors)\n   - Read and write directly from Amiga Disk Files (ADF)\n   - Embedded Amiga Emulator to preview your work in the real Deluxe Paint.\n   - Limit the palette to 12 bit for Amiga OCS/ECS mode, or 9 bit for Atari ST mode.\n - Deluxe Paint Legacy\n   - Supports PBM files as used by the PC version of Deluxe Paint (Thanks to [Michael Smith](https://github.com/michaelshmitty))\n   - Supports Deluxe Paint Atari ST compression modes (Thanks to [Nicolas Ramz](https://github.com/warpdesign))\n## Free and Open\nIt runs in your browser, works on any system and works fine on touch-screen devices like iPads.  \nIt is written in 100% plain JavaScript and has no dependencies.  \nIt's 100% free, no ads, no tracking, no accounts, no nothing.  \nAll processing is done in your browser, no data is sent to any server.  \n\nThe only part that is not included in this repository is the Amiga Emulator Files.\n(The emulator is based on the [Scripted Amiga Emulator](https://github.com/naTmeg/ScriptedAmigaEmulator))\n\n## Building\nDPaint.js doesn't need building.  \nIt also has zero dependencies so there's no need to install anything.  \nDPaint.js is written using ES6 modules and runs out of the box in modern browsers.  \nJust serve \"index.html\" from a webserver and you're good to go.  \n\nThere's an optional build step to create a compact version of DPaint.js if you like.  \nI'm using [Parcel.js](https://parceljs.org/) for this.  \nFor convenience, I've included a \"package.json\" file.  \nopen a terminal and run `npm install` to install Parcel.js and its dependencies.\nThen run `npm run build` to create a compact version of DPaint.js in the \"dist\" folder.\n\n## Documentation\nDocumentation can be found at https://www.stef.be/dpaint/docs/\n\n## Running offline\nDpaint.js is a web application, not an app that you install on your computer.\nThat being said: DPaint.js has no online dependencies and runs fine offline if you want.\nOne caveat: you have to serve the index.html file from a webserver, not just open it in your browser.  \nA quick way to do this is - for example - using the [Spark](https://github.com/rif/spark/releases) app.  \n[Download the binary](https://github.com/rif/spark/releases) for your platform, drop the Spark executable in the folder where you downloaded the Dpaint.js source files and run it.\nIf you then point your browser to http://localhost:8080/ it should work.  \n\nIf you are using Chrome, you can also \"install\" dpaint.js as app.  \n![image](https://github.com/user-attachments/assets/fa4a1e8b-4e45-4fe1-9d77-8b1e3364e867)  \nIt will then show up your Chrome apps and work offline.  \n\n\n## Contributing\nCurrent version is still alpha.  \nI'm sure there are bugs and missing features.  \nBug reports and pull requests are welcome.\n\n### Missing Features\nPlanned for the next release, already in the works:\n   - <strike>Color Cycling</strike> (done)\n   - <strike>Animation support (GIf and Amiga ANIM files)</strike> (done)\n   - <strike>Shading/transparency tools that stay within the palette.</strike> (done)\n\nPlanned for a future release if there's a need for it.\n  - Support for non-square pixel modes such as HiRes and Interlaced\n  - PSD import and export\n  - SpriteSheet support\n  - Write HAM,SHAM and Dynamic HiRes images\n  - Commodore 64 graphics modes\n      \n## Browser Quirks\nPlease note that the **Brave** browser is using \"[farbling](https://brave.com/privacy-updates/4-fingerprinting-defenses-2.0/#2-fingerprinting-protections-20-farbling-for-great-good)\" that introduces random image noise in certain conditions.\nThey claim this is to protect your privacy.\nAlthough I totally understand the sentiment, In my opinion a browser should not actively alter the content of a webpage or intentionally break functionality.  \nBut hey, who am I to speak, it's a free world.\nJust be aware that if you are using Brave, you will run into issues, so please \"lower your shields\" for this app in Brave or use another browser.\n\n## Color Cycling\nDpaint.js supports Color-Cycling - a long lost art of \"animating\" a static image by only rotating some colors in the palette.\nSee an example here:  \n\n\nhttps://github.com/user-attachments/assets/427bbecc-d38f-4120-ba0d-83c7f1249722\n\n\n\n[Open the layered source file of the above image directly in Dpaint.js](https://www.dpaint.app/?file=gallery/2026/the-vision-layered.json&play=true)\n\n\n\n",
      "stars_today": 30
    },
    {
      "id": 181042062,
      "name": "UTM",
      "full_name": "utmapp/UTM",
      "description": "Virtual machines for iOS and macOS",
      "html_url": "https://github.com/utmapp/UTM",
      "stars": 32504,
      "forks": 1631,
      "language": "Swift",
      "topics": [
        "apple",
        "emulation",
        "ios",
        "jailbreak",
        "macos",
        "qemu",
        "utm",
        "virtual-machines",
        "vm"
      ],
      "created_at": "2019-04-12T16:09:24Z",
      "updated_at": "2026-01-28T02:07:00Z",
      "pushed_at": "2026-01-27T15:58:27Z",
      "open_issues": 977,
      "owner": {
        "login": "utmapp",
        "avatar_url": "https://avatars.githubusercontent.com/u/49966544?v=4"
      },
      "readme": "#  UTM\n[![Build](https://github.com/utmapp/UTM/workflows/Build/badge.svg?branch=main&event=push)][1]\n\n> It is possible to invent a single machine which can be used to compute any computable sequence.\n\n-- <cite>Alan Turing, 1936</cite>\n\nUTM is a full featured system emulator and virtual machine host for iOS and macOS. It is based off of QEMU. In short, it allows you to run Windows, Linux, and more on your Mac, iPhone, and iPad. More information at https://getutm.app/ and https://mac.getutm.app/\n\n<p align=\"center\">\n  <img width=\"450px\" alt=\"UTM running on an iPhone\" src=\"screen.png\">\n  <br>\n  <img width=\"450px\" alt=\"UTM running on a MacBook\" src=\"screenmac.png\">\n</p>\n\n## Features\n\n* Full system emulation (MMU, devices, etc) using QEMU\n* 30+ processors supported including x86_64, ARM64, and RISC-V\n* VGA graphics mode using SPICE and QXL\n* Text terminal mode\n* USB devices\n* JIT based acceleration using QEMU TCG\n* Frontend designed from scratch for macOS 11 and iOS 11+ using the latest and greatest APIs\n* Create, manage, run VMs directly from your device\n\n## Additional macOS Features\n\n* Hardware accelerated virtualization using Hypervisor.framework and QEMU\n* Boot macOS guests with Virtualization.framework on macOS 12+\n\n## UTM SE\n\nUTM/QEMU requires dynamic code generation (JIT) for maximum performance. JIT on iOS devices require either a jailbroken device, or one of the various workarounds found for specific versions of iOS (see \"Install\" for more details).\n\nUTM SE (\"slow edition\") uses a [threaded interpreter][3] which performs better than a traditional interpreter but still slower than JIT. This technique is similar to what [iSH][4] does for dynamic execution. As a result, UTM SE does not require jailbreaking or any JIT workarounds and can be sideloaded as a regular app.\n\nTo optimize for size and build times, only the following architectures are included in UTM SE: ARM, PPC, RISC-V, and x86 (all with both 32-bit and 64-bit variants).\n\n## Install\n\nUTM (SE) for iOS: https://getutm.app/install/\n\nUTM is also available for macOS: https://mac.getutm.app/\n\n## Development\n\n### [macOS Development](Documentation/MacDevelopment.md)\n\n### [iOS Development](Documentation/iOSDevelopment.md)\n\n## Related\n\n* [iSH][4]: emulates a usermode Linux terminal interface for running x86 Linux applications on iOS\n* [a-shell][5]: packages common Unix commands and utilities built natively for iOS and accessible through a terminal interface\n\n## License\n\nUTM is distributed under the permissive Apache 2.0 license. However, it uses several (L)GPL components. Most are dynamically linked but the gstreamer plugins are statically linked and parts of the code are taken from qemu. Please be aware of this if you intend on redistributing this application.\n\nSome icons made by [Freepik](https://www.freepik.com) from [www.flaticon.com](https://www.flaticon.com/).\n\nAdditionally, UTM frontend depends on the following MIT/BSD License components:\n\n* [IQKeyboardManager](https://github.com/hackiftekhar/IQKeyboardManager)\n* [SwiftTerm](https://github.com/migueldeicaza/SwiftTerm)\n* [ZIP Foundation](https://github.com/weichsel/ZIPFoundation)\n* [InAppSettingsKit](https://github.com/futuretap/InAppSettingsKit)\n\nContinuous integration hosting is provided by [MacStadium](https://www.macstadium.com/opensource)\n\n[<img src=\"https://uploads-ssl.webflow.com/5ac3c046c82724970fc60918/5c019d917bba312af7553b49_MacStadium-developerlogo.png\" alt=\"MacStadium logo\" width=\"250\">](https://www.macstadium.com)\n\n  [1]: https://github.com/utmapp/UTM/actions?query=event%3Arelease+workflow%3ABuild\n  [2]: screen.png\n  [3]: https://github.com/ktemkin/qemu/blob/with_tcti/tcg/aarch64-tcti/README.md\n  [4]: https://github.com/ish-app/ish\n  [5]: https://github.com/holzschu/a-shell\n",
      "stars_today": 29
    },
    {
      "id": 674832198,
      "name": "Ice",
      "full_name": "jordanbaird/Ice",
      "description": "Powerful menu bar manager for macOS",
      "html_url": "https://github.com/jordanbaird/Ice",
      "stars": 25636,
      "forks": 566,
      "language": "Swift",
      "topics": [
        "macos",
        "macos-app",
        "menu-bar",
        "menubar",
        "menubar-app",
        "status-bar",
        "statusbar",
        "swift",
        "swiftui",
        "utility"
      ],
      "created_at": "2023-08-04T22:51:56Z",
      "updated_at": "2026-01-28T01:59:17Z",
      "pushed_at": "2025-09-20T20:28:17Z",
      "open_issues": 339,
      "owner": {
        "login": "jordanbaird",
        "avatar_url": "https://avatars.githubusercontent.com/u/90936861?v=4"
      },
      "readme": "<div align=\"center\">\n    <img src=\"Ice/Assets.xcassets/AppIcon.appiconset/icon_256x256.png\" width=200 height=200>\n    <h1>Ice</h1>\n</div>\n\nIce is a powerful menu bar management tool. While its primary function is hiding and showing menu bar items, it aims to cover a wide variety of additional features to make it one of the most versatile menu bar tools available.\n\n![Banner](https://github.com/user-attachments/assets/4423085c-4e4b-4f3d-ad0f-90a217c03470)\n\n[![Download](https://img.shields.io/badge/download-latest-brightgreen?style=flat-square)](https://github.com/jordanbaird/Ice/releases/latest)\n![Platform](https://img.shields.io/badge/platform-macOS-blue?style=flat-square)\n![Requirements](https://img.shields.io/badge/requirements-macOS%2014%2B-fa4e49?style=flat-square)\n[![Sponsor](https://img.shields.io/badge/Sponsor%20%E2%9D%A4%EF%B8%8F-8A2BE2?style=flat-square)](https://github.com/sponsors/jordanbaird)\n[![Website](https://img.shields.io/badge/Website-015FBA?style=flat-square)](https://icemenubar.app)\n[![License](https://img.shields.io/github/license/jordanbaird/Ice?style=flat-square)](LICENSE)\n\n> [!NOTE]\n> Ice is currently in active development. Some features have not yet been implemented. Download the latest release [here](https://github.com/jordanbaird/Ice/releases/latest) and see the roadmap below for upcoming features.\n\n<a href=\"https://www.buymeacoffee.com/jordanbaird\" target=\"_blank\">\n    <img src=\"https://cdn.buymeacoffee.com/buttons/v2/default-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\">\n</a>\n\n## Install\n\n### Manual Installation\n\nDownload the \"Ice.zip\" file from the [latest release](https://github.com/jordanbaird/Ice/releases/latest) and move the unzipped app into your `Applications` folder.\n\n### Homebrew\n\nInstall Ice using the following command:\n\n```sh\nbrew install --cask jordanbaird-ice\n```\n\n## Features/Roadmap\n\n### Menu bar item management\n\n- [x] Hide menu bar items\n- [x] \"Always-hidden\" menu bar section\n- [x] Show hidden menu bar items when hovering over the menu bar\n- [x] Show hidden menu bar items when an empty area in the menu bar is clicked\n- [x] Show hidden menu bar items by scrolling or swiping in the menu bar\n- [x] Automatically rehide menu bar items\n- [x] Hide application menus when they overlap with shown menu bar items\n- [x] Drag and drop interface to arrange individual menu bar items\n- [x] Display hidden menu bar items in a separate bar (e.g. for MacBooks with the notch)\n- [x] Search menu bar items\n- [x] Menu bar item spacing (BETA)\n- [ ] Profiles for menu bar layout\n- [ ] Individual spacer items\n- [ ] Menu bar item groups\n- [ ] Show menu bar items when trigger conditions are met\n\n### Menu bar appearance\n\n- [x] Menu bar tint (solid and gradient)\n- [x] Menu bar shadow\n- [x] Menu bar border\n- [x] Custom menu bar shapes (rounded and/or split)\n- [ ] Remove background behind menu bar\n- [ ] Rounded screen corners\n- [ ] Different settings for light/dark mode\n\n### Hotkeys\n\n- [x] Toggle individual menu bar sections\n- [x] Show the search panel\n- [x] Enable/disable the Ice Bar\n- [x] Show/hide section divider icons\n- [x] Toggle application menus\n- [ ] Enable/disable auto rehide\n- [ ] Temporarily show individual menu bar items\n\n### Other\n\n- [x] Launch at login\n- [x] Automatic updates\n- [ ] Menu bar widgets\n\n## Why does Ice only support macOS 14 and later?\n\nIce uses a number of system APIs that are available starting in macOS 14. As such, there are no plans to support earlier versions of macOS.\n\n## Gallery\n\n#### Show hidden menu bar items below the menu bar\n\n![Ice Bar](https://github.com/user-attachments/assets/f1429589-6186-4e1b-8aef-592219d49b9b)\n\n#### Drag-and-drop interface to arrange menu bar items\n\n![Menu Bar Layout](https://github.com/user-attachments/assets/095442ba-f2d0-4bb4-9632-91e26ef8d45b)\n\n#### Customize the menu bar's appearance\n\n![Menu Bar Appearance](https://github.com/user-attachments/assets/8c22c185-c3d2-49bb-971e-e1fc17df04b3)\n\n#### Menu bar item search\n\n![Menu Bar Item Search](https://github.com/user-attachments/assets/d1a7df3a-4989-4077-a0b1-8e7d5a1ba5b8)\n\n#### Custom menu bar item spacing\n\n![Menu Bar Item Spacing](https://github.com/user-attachments/assets/b196aa7e-184a-4d4c-b040-502f4aae40a6)\n\n## License\n\nIce is available under the [GPL-3.0 license](LICENSE).\n",
      "stars_today": 29
    },
    {
      "id": 100060912,
      "name": "terminal",
      "full_name": "microsoft/terminal",
      "description": "The new Windows Terminal and the original Windows console host, all in the same place!",
      "html_url": "https://github.com/microsoft/terminal",
      "stars": 101548,
      "forks": 9038,
      "language": "C++",
      "topics": [
        "cmd",
        "command-line",
        "console",
        "contributions-welcome",
        "good-first-issue",
        "hacktoberfest",
        "terminal",
        "windows",
        "windows-console",
        "windows-terminal",
        "wsl"
      ],
      "created_at": "2017-08-11T18:38:22Z",
      "updated_at": "2026-01-28T00:36:57Z",
      "pushed_at": "2026-01-28T02:12:30Z",
      "open_issues": 1695,
      "owner": {
        "login": "microsoft",
        "avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4"
      },
      "readme": "![terminal-logos](https://github.com/microsoft/terminal/assets/91625426/333ddc76-8ab2-4eb4-a8c0-4d7b953b1179)\n\n[![Terminal Build Status](https://dev.azure.com/shine-oss/terminal/_apis/build/status%2FTerminal%20CI?branchName=main)](https://dev.azure.com/shine-oss/terminal/_build/latest?definitionId=1&branchName=main)\n\n# Welcome to the Windows Terminal, Console and Command-Line repo\n\n<details>\n  <summary><strong>Table of Contents</strong></summary>\n\n- [Installing and running Windows Terminal](#installing-and-running-windows-terminal)\n  - [Microsoft Store \\[Recommended\\]](#microsoft-store-recommended)\n  - [Other install methods](#other-install-methods)\n    - [Via GitHub](#via-github)\n    - [Via Windows Package Manager CLI (aka winget)](#via-windows-package-manager-cli-aka-winget)\n    - [Via Chocolatey (unofficial)](#via-chocolatey-unofficial)\n    - [Via Scoop (unofficial)](#via-scoop-unofficial)\n- [Installing Windows Terminal Canary](#installing-windows-terminal-canary)\n- [Windows Terminal Roadmap](#windows-terminal-roadmap)\n- [Terminal \\& Console Overview](#terminal--console-overview)\n  - [Windows Terminal](#windows-terminal)\n  - [The Windows Console Host](#the-windows-console-host)\n  - [Shared Components](#shared-components)\n  - [Creating the new Windows Terminal](#creating-the-new-windows-terminal)\n- [Resources](#resources)\n- [FAQ](#faq)\n  - [I built and ran the new Terminal, but it looks just like the old console](#i-built-and-ran-the-new-terminal-but-it-looks-just-like-the-old-console)\n- [Documentation](#documentation)\n- [Contributing](#contributing)\n- [Communicating with the Team](#communicating-with-the-team)\n- [Developer Guidance](#developer-guidance)\n- [Prerequisites](#prerequisites)\n- [Building the Code](#building-the-code)\n  - [Building in PowerShell](#building-in-powershell)\n  - [Building in Cmd](#building-in-cmd)\n- [Running \\& Debugging](#running--debugging)\n  - [Coding Guidance](#coding-guidance)\n- [Code of Conduct](#code-of-conduct)\n\n</details>\n\n<br />\n\nThis repository contains the source code for:\n\n* [Windows Terminal](https://aka.ms/terminal)\n* [Windows Terminal Preview](https://aka.ms/terminal-preview)\n* The Windows console host (`conhost.exe`)\n* Components shared between the two projects\n* [ColorTool](./src/tools/ColorTool)\n* [Sample projects](./samples)\n  that show how to consume the Windows Console APIs\n\nRelated repositories include:\n\n* [Windows Terminal Documentation](https://docs.microsoft.com/windows/terminal)\n  ([Repo: Contribute to the docs](https://github.com/MicrosoftDocs/terminal))\n* [Console API Documentation](https://github.com/MicrosoftDocs/Console-Docs)\n* [Cascadia Code Font](https://github.com/Microsoft/Cascadia-Code)\n\n## Installing and running Windows Terminal\n\n> [!NOTE]\n> Windows Terminal requires Windows 10 2004 (build 19041) or later\n\n### Microsoft Store [Recommended]\n\nInstall the [Windows Terminal from the Microsoft Store][store-install-link].\nThis allows you to always be on the latest version when we release new builds\nwith automatic upgrades.\n\nThis is our preferred method.\n\n### Other install methods\n\n#### Via GitHub\n\nFor users who are unable to install Windows Terminal from the Microsoft Store,\nreleased builds can be manually downloaded from this repository's [Releases\npage](https://github.com/microsoft/terminal/releases).\n\nDownload the `Microsoft.WindowsTerminal_<versionNumber>.msixbundle` file from\nthe **Assets** section. To install the app, you can simply double-click on the\n`.msixbundle` file, and the app installer should automatically run. If that\nfails for any reason, you can try the following command at a PowerShell prompt:\n\n```powershell\n# NOTE: If you are using PowerShell 7+, please run\n# Import-Module Appx -UseWindowsPowerShell\n# before using Add-AppxPackage.\n\nAdd-AppxPackage Microsoft.WindowsTerminal_<versionNumber>.msixbundle\n```\n\n> [!NOTE]\n> If you install Terminal manually:\n>\n> * You may need to install the [VC++ v14 Desktop Framework Package](https://docs.microsoft.com/troubleshoot/cpp/c-runtime-packages-desktop-bridge#how-to-install-and-update-desktop-framework-packages).\n>   This should only be necessary on older builds of Windows 10 and only if you get an error about missing framework packages.\n> * Terminal will not auto-update when new builds are released so you will need\n>   to regularly install the latest Terminal release to receive all the latest\n>   fixes and improvements!\n\n#### Via Windows Package Manager CLI (aka winget)\n\n[winget](https://github.com/microsoft/winget-cli) users can download and install\nthe latest Terminal release by installing the `Microsoft.WindowsTerminal`\npackage:\n\n```powershell\nwinget install --id Microsoft.WindowsTerminal -e\n```\n\n> [!NOTE]\n> Dependency support is available in WinGet version [1.6.2631 or later](https://github.com/microsoft/winget-cli/releases). To install the Terminal stable release 1.18 or later, please make sure you have the updated version of the WinGet client.\n\n#### Via Chocolatey (unofficial)\n\n[Chocolatey](https://chocolatey.org) users can download and install the latest\nTerminal release by installing the `microsoft-windows-terminal` package:\n\n```powershell\nchoco install microsoft-windows-terminal\n```\n\nTo upgrade Windows Terminal using Chocolatey, run the following:\n\n```powershell\nchoco upgrade microsoft-windows-terminal\n```\n\nIf you have any issues when installing/upgrading the package please go to the\n[Windows Terminal package\npage](https://chocolatey.org/packages/microsoft-windows-terminal) and follow the\n[Chocolatey triage process](https://chocolatey.org/docs/package-triage-process)\n\n#### Via Scoop (unofficial)\n\n[Scoop](https://scoop.sh) users can download and install the latest Terminal\nrelease by installing the `windows-terminal` package:\n\n```powershell\nscoop bucket add extras\nscoop install windows-terminal\n```\n\nTo update Windows Terminal using Scoop, run the following:\n\n```powershell\nscoop update windows-terminal\n```\n\nIf you have any issues when installing/updating the package, please search for\nor report the same on the [issues\npage](https://github.com/lukesampson/scoop-extras/issues) of Scoop Extras bucket\nrepository.\n\n---\n\n## Installing Windows Terminal Canary\nWindows Terminal Canary is a nightly build of Windows Terminal. This build has the latest code from our `main` branch, giving you an opportunity to try features before they make it to Windows Terminal Preview.\n\nWindows Terminal Canary is our least stable offering, so you may discover bugs before we have had a chance to find them.\n\nWindows Terminal Canary is available as an App Installer distribution and a Portable ZIP distribution.\n\nThe App Installer distribution supports automatic updates. Due to platform limitations, this installer only works on Windows 11.\n\nThe Portable ZIP distribution is a portable application. It will not automatically update and will not automatically check for updates. This portable ZIP distribution works on Windows 10 (19041+) and Windows 11.\n\n| Distribution  | Architecture    | Link                                                 |\n|---------------|:---------------:|------------------------------------------------------|\n| App Installer | x64, arm64, x86 | [Download](https://aka.ms/terminal-canary-installer) |\n| Portable ZIP  | x64             | [Download](https://aka.ms/terminal-canary-zip-x64)   |\n| Portable ZIP  | ARM64           | [Download](https://aka.ms/terminal-canary-zip-arm64) |\n| Portable ZIP  | x86             | [Download](https://aka.ms/terminal-canary-zip-x86)   |\n\n_Learn more about the [types of Windows Terminal distributions](https://learn.microsoft.com/windows/terminal/distributions)._\n\n---\n\n## Windows Terminal Roadmap\n\nThe plan for the Windows Terminal [is described here](/doc/roadmap-2023.md) and\nwill be updated as the project proceeds.\n\n## Terminal & Console Overview\n\nPlease take a few minutes to review the overview below before diving into the\ncode:\n\n### Windows Terminal\n\nWindows Terminal is a new, modern, feature-rich, productive terminal application\nfor command-line users. It includes many of the features most frequently\nrequested by the Windows command-line community including support for tabs, rich\ntext, globalization, configurability, theming & styling, and more.\n\nThe Terminal will also need to meet our goals and measures to ensure it remains\nfast and efficient, and doesn't consume vast amounts of memory or power.\n\n### The Windows Console Host\n\nThe Windows Console host, `conhost.exe`, is Windows' original command-line user\nexperience. It also hosts Windows' command-line infrastructure and the Windows\nConsole API server, input engine, rendering engine, user preferences, etc. The\nconsole host code in this repository is the actual source from which the\n`conhost.exe` in Windows itself is built.\n\nSince taking ownership of the Windows command-line in 2014, the team added\nseveral new features to the Console, including background transparency,\nline-based selection, support for [ANSI / Virtual Terminal\nsequences](https://en.wikipedia.org/wiki/ANSI_escape_code), [24-bit\ncolor](https://devblogs.microsoft.com/commandline/24-bit-color-in-the-windows-console/),\na [Pseudoconsole\n(\"ConPTY\")](https://devblogs.microsoft.com/commandline/windows-command-line-introducing-the-windows-pseudo-console-conpty/),\nand more.\n\nHowever, because Windows Console's primary goal is to maintain backward\ncompatibility, we have been unable to add many of the features the community\n(and the team) have been wanting for the last several years including tabs,\nunicode text, and emoji.\n\nThese limitations led us to create the new Windows Terminal.\n\n> You can read more about the evolution of the command-line in general, and the\n> Windows command-line specifically in [this accompanying series of blog\n> posts](https://devblogs.microsoft.com/commandline/windows-command-line-backgrounder/)\n> on the Command-Line team's blog.\n\n### Shared Components\n\nWhile overhauling Windows Console, we modernized its codebase considerably,\ncleanly separating logical entities into modules and classes, introduced some\nkey extensibility points, replaced several old, home-grown collections and\ncontainers with safer, more efficient [STL\ncontainers](https://docs.microsoft.com/en-us/cpp/standard-library/stl-containers?view=vs-2022),\nand made the code simpler and safer by using Microsoft's [Windows Implementation\nLibraries - WIL](https://github.com/Microsoft/wil).\n\nThis overhaul resulted in several of Console's key components being available\nfor re-use in any terminal implementation on Windows. These components include a\nnew DirectWrite-based text layout and rendering engine, a text buffer capable of\nstoring both UTF-16 and UTF-8, a VT parser/emitter, and more.\n\n### Creating the new Windows Terminal\n\nWhen we started planning the new Windows Terminal application, we explored and\nevaluated several approaches and technology stacks. We ultimately decided that\nour goals would be best met by continuing our investment in our C++ codebase,\nwhich would allow us to reuse several of the aforementioned modernized\ncomponents in both the existing Console and the new Terminal. Further, we\nrealized that this would allow us to build much of the Terminal's core itself as\na reusable UI control that others can incorporate into their own applications.\n\nThe result of this work is contained within this repo and delivered as the\nWindows Terminal application you can download from the Microsoft Store, or\n[directly from this repo's\nreleases](https://github.com/microsoft/terminal/releases).\n\n---\n\n## Resources\n\nFor more information about Windows Terminal, you may find some of these\nresources useful and interesting:\n\n* [Command-Line Blog](https://devblogs.microsoft.com/commandline)\n* [Command-Line Backgrounder Blog\n  Series](https://devblogs.microsoft.com/commandline/windows-command-line-backgrounder/)\n* Windows Terminal Launch: [Terminal \"Sizzle\n  Video\"](https://www.youtube.com/watch?v=8gw0rXPMMPE&list=PLEHMQNlPj-Jzh9DkNpqipDGCZZuOwrQwR&index=2&t=0s)\n* Windows Terminal Launch: [Build 2019\n  Session](https://www.youtube.com/watch?v=KMudkRcwjCw)\n* Run As Radio: [Show 645 - Windows Terminal with Richard\n  Turner](https://www.runasradio.com/Shows/Show/645)\n* Azure Devops Podcast: [Episode 54 - Kayla Cinnamon and Rich Turner on DevOps\n  on the Windows\n  Terminal](http://azuredevopspodcast.clear-measure.com/kayla-cinnamon-and-rich-turner-on-devops-on-the-windows-terminal-team-episode-54)\n* Microsoft Ignite 2019 Session: [The Modern Windows Command Line: Windows\n  Terminal -\n  BRK3321](https://myignite.techcommunity.microsoft.com/sessions/81329?source=sessions)\n\n---\n\n## FAQ\n\n### I built and ran the new Terminal, but it looks just like the old console\n\nCause: You're launching the incorrect solution in Visual Studio.\n\nSolution: Make sure you're building & deploying the `CascadiaPackage` project in\nVisual Studio.\n\n> [!NOTE]\n> `OpenConsole.exe` is just a locally-built `conhost.exe`, the classic\n> Windows Console that hosts Windows' command-line infrastructure. OpenConsole\n> is used by Windows Terminal to connect to and communicate with command-line\n> applications (via\n> [ConPty](https://devblogs.microsoft.com/commandline/windows-command-line-introducing-the-windows-pseudo-console-conpty/)).\n\n---\n\n## Documentation\n\nAll project documentation is located at [aka.ms/terminal-docs](https://aka.ms/terminal-docs). If you would like\nto contribute to the documentation, please submit a pull request on the [Windows\nTerminal Documentation repo](https://github.com/MicrosoftDocs/terminal).\n\n---\n\n## Contributing\n\nWe are excited to work alongside you, our amazing community, to build and\nenhance Windows Terminal\\!\n\n***BEFORE you start work on a feature/fix***, please read & follow our\n[Contributor's\nGuide](./CONTRIBUTING.md) to\nhelp avoid any wasted or duplicate effort.\n\n## Communicating with the Team\n\nThe easiest way to communicate with the team is via GitHub issues.\n\nPlease file new issues, feature requests and suggestions, but **DO search for\nsimilar open/closed preexisting issues before creating a new issue.**\n\nIf you would like to ask a question that you feel doesn't warrant an issue\n(yet), please reach out to us via Twitter:\n\n* Christopher Nguyen, Product Manager:\n  [@nguyen_dows](https://twitter.com/nguyen_dows)\n* Dustin Howett, Engineering Lead: [@dhowett](https://twitter.com/DHowett)\n* Mike Griese, Senior Developer: [@zadjii@mastodon.social](https://mastodon.social/@zadjii)\n* Carlos Zamora, Developer: [@cazamor_msft](https://twitter.com/cazamor_msft)\n* Pankaj Bhojwani, Developer\n* Leonard Hecker, Developer: [@LeonardHecker](https://twitter.com/LeonardHecker)\n\n## Developer Guidance\n\n## Prerequisites\n\nYou can configure your environment to build Terminal in one of two ways:\n\n### Using WinGet configuration file\n\nAfter cloning the repository, you can use a [WinGet configuration file](https://learn.microsoft.com/en-us/windows/package-manager/configuration/#use-a-winget-configuration-file-to-configure-your-machine)\nto set up your environment. The [default configuration file](.config/configuration.winget) installs Visual Studio 2022 Community & rest of the required tools. There are two other variants of the configuration file available in the [.config](.config) directory for Enterprise & Professional editions of Visual Studio 2022. To run the default configuration file, you can either double-click the file from explorer or run the following command:\n\n```powershell\nwinget configure .config\\configuration.winget\n```\n\n### Manual configuration\n\n* You must be running Windows 10 2004 (build >= 10.0.19041.0) or later to run\n  Windows Terminal\n* You must [enable Developer Mode in the Windows Settings\n  app](https://docs.microsoft.com/en-us/windows/uwp/get-started/enable-your-device-for-development)\n  to locally install and run Windows Terminal\n* You must have [PowerShell 7 or later](https://github.com/PowerShell/PowerShell/releases/latest) installed\n* You must have the [Windows 11 (10.0.22621.0)\n  SDK](https://developer.microsoft.com/en-us/windows/downloads/windows-sdk/)\n  installed\n* You must have at least [VS\n  2022](https://visualstudio.microsoft.com/downloads/) installed\n* You must install the following Workloads via the VS Installer. Note: Opening\n  the solution in VS 2022 will [prompt you to install missing components\n  automatically](https://devblogs.microsoft.com/setup/configure-visual-studio-across-your-organization-with-vsconfig/):\n  * Desktop Development with C++\n  * Universal Windows Platform Development\n  * **The following Individual Components**\n    * C++ (v143) Universal Windows Platform Tools\n* You must install the [.NET Framework Targeting Pack](https://docs.microsoft.com/dotnet/framework/install/guide-for-developers#to-install-the-net-framework-developer-pack-or-targeting-pack) to build test projects\n\n## Building the Code\n\nOpenConsole.slnx may be built from within Visual Studio or from the command-line\nusing a set of convenience scripts & tools in the **/tools** directory:\n\n### Building in PowerShell\n\n```powershell\nImport-Module .\\tools\\OpenConsole.psm1\nSet-MsBuildDevEnvironment\nInvoke-OpenConsoleBuild\n```\n\n### Building in Cmd\n\n```shell\n.\\tools\\razzle.cmd\nbcz\n```\n\n## Running & Debugging\n\nTo debug the Windows Terminal in VS, right click on `CascadiaPackage` (in the\nSolution Explorer) and go to properties. In the Debug menu, change \"Application\nprocess\" and \"Background task process\" to \"Native Only\".\n\nYou should then be able to build & debug the Terminal project by hitting\n<kbd>F5</kbd>. Make sure to select either the \"x64\" or the \"x86\" platform - the\nTerminal doesn't build for \"Any Cpu\" (because the Terminal is a C++ application,\nnot a C# one).\n\n> ğŸ‘‰ You will _not_ be able to launch the Terminal directly by running the\n> WindowsTerminal.exe. For more details on why, see\n> [#926](https://github.com/microsoft/terminal/issues/926),\n> [#4043](https://github.com/microsoft/terminal/issues/4043)\n\n### Coding Guidance\n\nPlease review these brief docs below about our coding practices.\n\n> ğŸ‘‰ If you find something missing from these docs, feel free to contribute to\n> any of our documentation files anywhere in the repository (or write some new\n> ones!)\n\nThis is a work in progress as we learn what we'll need to provide people in\norder to be effective contributors to our project.\n\n* [Coding Style](./doc/STYLE.md)\n* [Code Organization](./doc/ORGANIZATION.md)\n* [Exceptions in our legacy codebase](./doc/EXCEPTIONS.md)\n* [Helpful smart pointers and macros for interfacing with Windows in WIL](./doc/WIL.md)\n\n---\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of\nConduct][conduct-code]. For more information see the [Code of Conduct\nFAQ][conduct-FAQ] or contact [opencode@microsoft.com][conduct-email] with any\nadditional questions or comments.\n\n[conduct-code]: https://opensource.microsoft.com/codeofconduct/\n[conduct-FAQ]: https://opensource.microsoft.com/codeofconduct/faq/\n[conduct-email]: mailto:opencode@microsoft.com\n[store-install-link]: https://aka.ms/terminal\n",
      "stars_today": 28
    },
    {
      "id": 22067521,
      "name": "imgui",
      "full_name": "ocornut/imgui",
      "description": "Dear ImGui: Bloat-free Graphical User interface for C++ with minimal dependencies",
      "html_url": "https://github.com/ocornut/imgui",
      "stars": 70989,
      "forks": 11498,
      "language": "C++",
      "topics": [
        "api",
        "cplusplus",
        "framework",
        "game-development",
        "game-engine",
        "gamedev",
        "gui",
        "imgui",
        "immediate-gui",
        "library",
        "multi-platform",
        "native",
        "toolkit",
        "tools",
        "ui"
      ],
      "created_at": "2014-07-21T14:29:47Z",
      "updated_at": "2026-01-28T02:03:45Z",
      "pushed_at": "2026-01-26T18:22:07Z",
      "open_issues": 1201,
      "owner": {
        "login": "ocornut",
        "avatar_url": "https://avatars.githubusercontent.com/u/8225057?v=4"
      },
      "readme": "Dear ImGui\n=====\n\n<center><b><i>\"Give someone state and they'll have a bug one day, but teach them how to represent state in two separate locations that have to be kept in sync and they'll have bugs for a lifetime.\"</i></b></center> <a href=\"https://twitter.com/rygorous/status/1507178315886444544\">-ryg</a>\n\n----\n\n[![Build Status](https://github.com/ocornut/imgui/workflows/build/badge.svg)](https://github.com/ocornut/imgui/actions?workflow=build) [![Static Analysis Status](https://github.com/ocornut/imgui/workflows/static-analysis/badge.svg)](https://github.com/ocornut/imgui/actions?workflow=static-analysis) [![Tests Status](https://github.com/ocornut/imgui_test_engine/workflows/tests/badge.svg)](https://github.com/ocornut/imgui_test_engine/actions?workflow=tests)\n\n<sub>(This library is available under a free and permissive license, but needs financial support to sustain its continued improvements. In addition to maintenance and stability there are many desirable features yet to be added. If your company is using Dear ImGui, please consider reaching out.)</sub>\n\nBusinesses: support continued development and maintenance via invoiced sponsoring/support contracts:\n<br>&nbsp;&nbsp;_E-mail: contact @ dearimgui dot com_\n<br>Individuals: support continued development and maintenance [here](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=WGHNC6MBFLZ2S). Also see [Funding](https://github.com/ocornut/imgui/wiki/Funding) page.\n\n| [The Pitch](#the-pitch) - [Usage](#usage) - [How it works](#how-it-works) - [Releases & Changelogs](#releases--changelogs) - [Demo](#demo) - [Getting Started & Integration](#getting-started--integration) |\n:----------------------------------------------------------: |\n| [Gallery](#gallery) - [Support, FAQ](#support-frequently-asked-questions-faq) -  [How to help](#how-to-help) - **[Funding & Sponsors](https://github.com/ocornut/imgui/wiki/Funding)** - [Credits](#credits) - [License](#license) |\n| [Wiki](https://github.com/ocornut/imgui/wiki) - [Extensions](https://github.com/ocornut/imgui/wiki/Useful-Extensions) - [Language bindings & framework backends](https://github.com/ocornut/imgui/wiki/Bindings) - [Software using Dear ImGui](https://github.com/ocornut/imgui/wiki/Software-using-dear-imgui) - [User quotes](https://github.com/ocornut/imgui/wiki/Quotes) |\n\n### The Pitch\n\nDear ImGui is a **bloat-free graphical user interface library for C++**. It outputs optimized vertex buffers that you can render anytime in your 3D-pipeline-enabled application. It is fast, portable, renderer agnostic, and self-contained (no external dependencies).\n\nDear ImGui is designed to **enable fast iterations** and to **empower programmers** to create **content creation tools and visualization / debug tools** (as opposed to UI for the average end-user). It favors simplicity and productivity toward this goal and lacks certain features commonly found in more high-level libraries. Among other things, full internationalization (right-to-left text, bidirectional text, text shaping etc.) and accessibility features are not supported.\n\nDear ImGui is particularly suited to integration in game engines (for tooling), real-time 3D applications, fullscreen applications, embedded applications, or any applications on console platforms where operating system features are non-standard.\n\n - Minimize state synchronization.\n - Minimize UI-related state storage on user side.\n - Minimize setup and maintenance.\n - Easy to use to create dynamic UI which are the reflection of a dynamic data set.\n - Easy to use to create code-driven and data-driven tools.\n - Easy to use to create ad hoc short-lived tools and long-lived, more elaborate tools.\n - Easy to hack and improve.\n - Portable, minimize dependencies, run on target (consoles, phones, etc.).\n - Efficient runtime and memory consumption.\n - Battle-tested, used by [many major actors in the game industry](https://github.com/ocornut/imgui/wiki/Software-using-dear-imgui).\n\n### Usage\n\n**The core of Dear ImGui is self-contained within a few platform-agnostic files** which you can easily compile in your application/engine. They are all the files in the root folder of the repository (`imgui*.cpp`, `imgui*.h`). **No specific build process is required**: you can add all files into your existing project.\n\n**Backends for a variety of graphics API and rendering platforms** are provided in the [backends/](https://github.com/ocornut/imgui/tree/master/backends) folder, along with example applications in the [examples/](https://github.com/ocornut/imgui/tree/master/examples) folder. You may also create your own backend. Anywhere where you can render textured triangles, you can render Dear ImGui.\n\nC++20 users wishing to use a module may the use [stripe2933/imgui-module](https://github.com/stripe2933/imgui-module) third-party extension.\n\nSee the [Getting Started & Integration](#getting-started--integration) section of this document for more details.\n\nAfter Dear ImGui is set up in your application, you can use it from \\_anywhere\\_ in your program loop:\n```cpp\nImGui::Text(\"Hello, world %d\", 123);\nif (ImGui::Button(\"Save\"))\n    MySaveFunction();\nImGui::InputText(\"string\", buf, IM_COUNTOF(buf));\nImGui::SliderFloat(\"float\", &f, 0.0f, 1.0f);\n```\n![sample code output (dark, segoeui font, freetype)](https://user-images.githubusercontent.com/8225057/191050833-b7ecf528-bfae-4a9f-ac1b-f3d83437a2f4.png)\n![sample code output (light, segoeui font, freetype)](https://user-images.githubusercontent.com/8225057/191050838-8742efd4-504d-4334-a9a2-e756d15bc2ab.png)\n\n```cpp\n// Create a window called \"My First Tool\", with a menu bar.\nImGui::Begin(\"My First Tool\", &my_tool_active, ImGuiWindowFlags_MenuBar);\nif (ImGui::BeginMenuBar())\n{\n    if (ImGui::BeginMenu(\"File\"))\n    {\n        if (ImGui::MenuItem(\"Open..\", \"Ctrl+O\")) { /* Do stuff */ }\n        if (ImGui::MenuItem(\"Save\", \"Ctrl+S\"))   { /* Do stuff */ }\n        if (ImGui::MenuItem(\"Close\", \"Ctrl+W\"))  { my_tool_active = false; }\n        ImGui::EndMenu();\n    }\n    ImGui::EndMenuBar();\n}\n\n// Edit a color stored as 4 floats\nImGui::ColorEdit4(\"Color\", my_color);\n\n// Generate samples and plot them\nfloat samples[100];\nfor (int n = 0; n < 100; n++)\n    samples[n] = sinf(n * 0.2f + ImGui::GetTime() * 1.5f);\nImGui::PlotLines(\"Samples\", samples, 100);\n\n// Display contents in a scrolling region\nImGui::TextColored(ImVec4(1,1,0,1), \"Important Stuff\");\nImGui::BeginChild(\"Scrolling\");\nfor (int n = 0; n < 50; n++)\n    ImGui::Text(\"%04d: Some text\", n);\nImGui::EndChild();\nImGui::End();\n```\n![my_first_tool_v188](https://user-images.githubusercontent.com/8225057/191055698-690a5651-458f-4856-b5a9-e8cc95c543e2.gif)\n\nDear ImGui allows you to **create elaborate tools** as well as very short-lived ones. On the extreme side of short-livedness: using the Edit&Continue (hot code reload) feature of modern compilers you can add a few widgets to tweak variables while your application is running, and remove the code a minute later! Dear ImGui is not just for tweaking values. You can use it to trace a running algorithm by just emitting text commands. You can use it along with your own reflection data to browse your dataset live. You can use it to expose the internals of a subsystem in your engine, to create a logger, an inspection tool, a profiler, a debugger, an entire game-making editor/framework, etc.\n\n### How it works\n\nThe IMGUI paradigm through its API tries to minimize superfluous state duplication, state synchronization, and state retention from the user's point of view. It is less error-prone (less code and fewer bugs) than traditional retained-mode interfaces, and lends itself to creating dynamic user interfaces. Check out the Wiki's [About the IMGUI paradigm](https://github.com/ocornut/imgui/wiki#about-the-imgui-paradigm) section for more details.\n\nDear ImGui outputs vertex buffers and command lists that you can easily render in your application. The number of draw calls and state changes required to render them is fairly small. Because Dear ImGui doesn't know or touch graphics state directly, you can call its functions  anywhere in your code (e.g. in the middle of a running algorithm, or in the middle of your own rendering process). Refer to the sample applications in the examples/ folder for instructions on how to integrate Dear ImGui with your existing codebase.\n\n_A common misunderstanding is to mistake immediate mode GUI for immediate mode rendering, which usually implies hammering your driver/GPU with a bunch of inefficient draw calls and state changes as the GUI functions are called. This is NOT what Dear ImGui does. Dear ImGui outputs vertex buffers and a small list of draw calls batches. It never touches your GPU directly. The draw call batches are decently optimal and you can render them later, in your app or even remotely._\n\n### Releases & Changelogs\n\nSee [Releases](https://github.com/ocornut/imgui/releases) page for decorated Changelogs.\nReading the changelogs is a good way to keep up to date with the things Dear ImGui has to offer, and maybe will give you ideas of some features that you've been ignoring until now!\n\n### Demo\n\nCalling the `ImGui::ShowDemoWindow()` function will create a demo window showcasing a variety of features and examples. The code is always available for reference in `imgui_demo.cpp`. \n- [Web version of the demo](https://pthom.github.io/imgui_manual_online/manual/imgui_manual.html) courtesy of [@pthom](https://github.com/pthom).\n- [Screenshot of the demo](https://raw.githubusercontent.com/wiki/ocornut/imgui/web/v167/v167-misc.png).\n\nYou should be able to build the examples from sources. If you don't, let us know! If you want to have a quick look at some Dear ImGui features, you can download Windows binaries of the demo app here:\n- [imgui-demo-binaries-20250625.zip](https://www.dearimgui.com/binaries/imgui-demo-binaries-20250625.zip) (Windows, 1.92.0, built 2025/06/25, master) or [older binaries](https://www.dearimgui.com/binaries).\n\n### Getting Started & Integration\n\nSee the [Getting Started](https://github.com/ocornut/imgui/wiki/Getting-Started) guide for details.\n\nOn most platforms and when using C++, **you should be able to use a combination of the [imgui_impl_xxxx](https://github.com/ocornut/imgui/tree/master/backends) backends without modification** (e.g. `imgui_impl_win32.cpp` + `imgui_impl_dx11.cpp`). If your engine supports multiple platforms, consider using more imgui_impl_xxxx files instead of rewriting them: this will be less work for you, and you can get Dear ImGui running immediately. You can _later_ decide to rewrite a custom backend using your custom engine functions if you wish so.\n\nIntegrating Dear ImGui within your custom engine is a matter of mainly 1) wiring mouse/keyboard/gamepad inputs 2) uploading a texture to your GPU/render engine 3) providing a render function that can create/update textures and render textured triangles. This is exactly what backends are doing.\n- The [examples/](https://github.com/ocornut/imgui/tree/master/examples) folder is populated with applications setting up a window and using standard backends.\n- The [Getting Started](https://github.com/ocornut/imgui/wiki/Getting-Started) guide has instructions to integrate imgui into an existing application using standard backends. It should in theory take you less than an hour to integrate Dear ImGui into your existing codebase where support libraries are linked. Less if you read carefully.\n- The [Backends](https://github.com/ocornut/imgui/blob/master/docs/BACKENDS.md) guide explains what backends are doing, and has instructions to implement a custom backend. You can also refer to the source code of our ~20 backends to understand how they work.\n- Generally, **make sure to spend time reading the [FAQ](https://www.dearimgui.com/faq), comments, and the examples applications!**\n\nOfficially maintained backends (in repository):\n- Renderers: DirectX9, DirectX10, DirectX11, DirectX12, Metal, OpenGL/ES/ES2, SDL_GPU, SDL_Renderer2/3, Vulkan, WebGPU.\n- Platforms: GLFW, SDL2/SDL3, Win32, Glut, OSX, Android.\n- Frameworks: Allegro5, Emscripten.\n\n[Third-party backends/bindings](https://github.com/ocornut/imgui/wiki/Bindings) wiki page:\n- Languages: C, C# and: Beef, ChaiScript, CovScript, Crystal, D, Go, Haskell, Haxe/hxcpp, Java, JavaScript, Julia, Kotlin, Lobster, Lua, Nim, Odin, Pascal, PureBasic, Python, ReaScript, Ruby, Rust, Swift, Zig...\n- Frameworks: AGS/Adventure Game Studio, Amethyst, Blender, bsf, Cinder, Cocos2d-x, Defold, Diligent Engine, Ebiten, Flexium, GML/Game Maker Studio, GLEQ, Godot, GTK3, Irrlicht Engine, JUCE, LÃ–VE+LUA, Mach Engine, Magnum, Marmalade, Monogame, NanoRT, nCine, Nim Game Lib, Nintendo 3DS/Switch/WiiU (homebrew), Ogre, openFrameworks, OSG/OpenSceneGraph, Orx, Photoshop, px_render, Qt/QtDirect3D, raylib, SFML, Sokol, Unity, Unreal Engine 4/5, UWP, vtk, VulkanHpp, VulkanSceneGraph, Win32 GDI, WxWidgets.\n- Many bindings are auto-generated (by good old [cimgui](https://github.com/cimgui/cimgui) or our newer [dear_bindings](https://github.com/dearimgui/dear_bindings)), you can use their metadata output to generate bindings for other languages.\n\n[Useful Extensions/Widgets](https://github.com/ocornut/imgui/wiki/Useful-Extensions) wiki page:\n- Automation/testing, Text editors, node editors, timeline editors, plotting, software renderers, remote network access, memory editors, gizmos, etc. Notable and well supported extensions include [ImPlot](https://github.com/epezent/implot) and [Dear ImGui Test Engine](https://github.com/ocornut/imgui_test_engine).\n\nAlso see [Wiki](https://github.com/ocornut/imgui/wiki) for more links and ideas.\n\n### Gallery\n\nExamples projects using Dear ImGui: [Tracy](https://github.com/wolfpld/tracy) (profiler), [ImHex](https://github.com/WerWolv/ImHex) (hex editor/data analysis), [RemedyBG](https://remedybg.itch.io/remedybg) (debugger) and [hundreds of others](https://github.com/ocornut/imgui/wiki/Software-using-Dear-ImGui).\n\nFor more user-submitted screenshots of projects using Dear ImGui, check out the [Gallery Threads](https://github.com/ocornut/imgui/issues?q=label%3Agallery)!\n\nFor a list of third-party widgets and extensions, check out the [Useful Extensions/Widgets](https://github.com/ocornut/imgui/wiki/Useful-Extensions) wiki page.\n\n|  |  |\n|--|--|\n| Custom engine [erhe](https://github.com/tksuoran/erhe) (docking branch)<BR>[![erhe](https://user-images.githubusercontent.com/8225057/190203358-6988b846-0686-480e-8663-1311fbd18abd.jpg)](https://user-images.githubusercontent.com/994606/147875067-a848991e-2ad2-4fd3-bf71-4aeb8a547bcf.png) | Custom engine for [Wonder Boy: The Dragon's Trap](http://www.TheDragonsTrap.com) (2017)<BR>[![the dragon's trap](https://user-images.githubusercontent.com/8225057/190203379-57fcb80e-4aec-4fec-959e-17ddd3cd71e5.jpg)](https://cloud.githubusercontent.com/assets/8225057/20628927/33e14cac-b329-11e6-80f6-9524e93b048a.png) |\n| Custom engine (untitled)<BR>[![editor white](https://user-images.githubusercontent.com/8225057/190203393-c5ac9f22-b900-4d1e-bfeb-6027c63e3d92.jpg)](https://raw.githubusercontent.com/wiki/ocornut/imgui/web/v160/editor_white.png) | Tracy Profiler ([github](https://github.com/wolfpld/tracy))<BR>[![tracy profiler](https://user-images.githubusercontent.com/8225057/190203401-7b595f6e-607c-44d3-97ea-4c2673244dfb.jpg)](https://raw.githubusercontent.com/wiki/ocornut/imgui/web/v176/tracy_profiler.png) |\n\n### Support, Frequently Asked Questions (FAQ)\n\nSee: [Frequently Asked Questions (FAQ)](https://github.com/ocornut/imgui/blob/master/docs/FAQ.md) where common questions are answered.\n\nSee: [Getting Started](https://github.com/ocornut/imgui/wiki/Getting-Started) and [Wiki](https://github.com/ocornut/imgui/wiki) for many links, references, articles.\n\nSee: [Articles about the IMGUI paradigm](https://github.com/ocornut/imgui/wiki#about-the-imgui-paradigm) to read/learn about the Immediate Mode GUI paradigm.\n\nSee: [Upcoming Changes](https://github.com/ocornut/imgui/wiki/Upcoming-Changes).\n\nSee: [Dear ImGui Test Engine + Test Suite](https://github.com/ocornut/imgui_test_engine) for Automation & Testing.\n\nFor the purposes of getting search engines to crawl the wiki, here's a link to the [Crawlable Wiki](https://github-wiki-see.page/m/ocornut/imgui/wiki) (not for humans, [here's why](https://github-wiki-see.page/)).\n\nGetting started? For first-time users having issues compiling/linking/running or issues loading fonts, please use [GitHub Discussions](https://github.com/ocornut/imgui/discussions). For ANY other questions, bug reports, requests, feedback, please post on [GitHub Issues](https://github.com/ocornut/imgui/issues). Please read and fill the New Issue template carefully.\n\nPrivate support is available for paying business customers (E-mail: _contact @ dearimgui dot com_).\n\n**Which version should I get?**\n\nWe occasionally tag [Releases](https://github.com/ocornut/imgui/releases) (with nice releases notes) but it is generally safe and recommended to sync to latest `master` or `docking` branch. The library is fairly stable and regressions tend to be fixed fast when reported. Advanced users may want to use the `docking` branch with [Multi-Viewport](https://github.com/ocornut/imgui/wiki/Multi-Viewports) and [Docking](https://github.com/ocornut/imgui/wiki/Docking) features. This branch is kept in sync with master regularly.\n\n**Who uses Dear ImGui?**\n\nSee the [Quotes](https://github.com/ocornut/imgui/wiki/Quotes), [Funding & Sponsors](https://github.com/ocornut/imgui/wiki/Funding), and [Software using Dear ImGui](https://github.com/ocornut/imgui/wiki/Software-using-dear-imgui) Wiki pages for an idea of who is using Dear ImGui. Please add your game/software if you can! Also, see the [Gallery Threads](https://github.com/ocornut/imgui/issues?q=label%3Agallery)!\n\nHow to help\n-----------\n\n**How can I help?**\n\n- See [GitHub Forum/Issues](https://github.com/ocornut/imgui/issues).\n- You may help with development and submit pull requests! Please understand that by submitting a PR you are also submitting a request for the maintainer to review your code and then take over its maintenance forever. PR should be crafted both in the interest of the end-users and also to ease the maintainer into understanding and accepting it.\n- See [Help wanted](https://github.com/ocornut/imgui/wiki/Help-Wanted) on the [Wiki](https://github.com/ocornut/imgui/wiki/) for some more ideas.\n- Be a [Funding Supporter](https://github.com/ocornut/imgui/wiki/Funding)! Have your company financially support this project via invoiced sponsors/maintenance or by buying a license for [Dear ImGui Test Engine](https://github.com/ocornut/imgui_test_engine) (please reach out: contact AT dearimgui DOT com).\n\nSponsors\n--------\n\nOngoing Dear ImGui development is and has been financially supported by users and private sponsors.\n<BR>Please see the **[detailed list of current and past Dear ImGui funding supporters and sponsors](https://github.com/ocornut/imgui/wiki/Funding)** for details.\n<BR>From November 2014 to December 2019, ongoing development has also been financially supported by its users on Patreon and through individual donations.\n\n**THANK YOU to all past and present supporters for helping to keep this project alive and thriving!**\n\nDear ImGui is using software and services provided free of charge for open source projects:\n- [PVS-Studio](https://pvs-studio.com/en/pvs-studio/?utm_source=website&utm_medium=github&utm_campaign=open_source) for static analysis (supports C/C++/C#/Java).\n- [GitHub actions](https://github.com/features/actions) for continuous integration systems.\n- [OpenCppCoverage](https://github.com/OpenCppCoverage/OpenCppCoverage) for code coverage analysis.\n\nCredits\n-------\n\nDeveloped by [Omar Cornut](https://www.miracleworld.net) and every direct or indirect [contributors](https://github.com/ocornut/imgui/graphs/contributors) to the GitHub. The early version of this library was developed with the support of [Media Molecule](https://www.mediamolecule.com) and first used internally on the game [Tearaway](https://youtu.be/w0oxBviRGlU) (PS Vita).\n\nRecurring contributors include Rokas Kupstys [@rokups](https://github.com/rokups) (2020-2022): a good portion of work on automation system and regression tests now available in [Dear ImGui Test Engine](https://github.com/ocornut/imgui_test_engine).\n\nMaintenance/support contracts, sponsoring invoices and other B2B transactions are hosted and handled by [Disco Hello](https://www.discohello.com).\n\nOmar: \"I first discovered the IMGUI paradigm at [Q-Games](https://www.q-games.com) where Atman Binstock had dropped his own simple implementation in the codebase, which I spent quite some time improving and thinking about. It turned out that Atman was exposed to the concept directly by working with Casey. When I moved to Media Molecule I rewrote a new library trying to overcome the flaws and limitations of the first one I've worked with. It became this library and since then I have spent an unreasonable amount of time iterating and improving it.\"\n\nEmbeds [ProggyClean.ttf, ProggyVector.ttf](https://www.proggyfonts.net) fonts by Tristan Grimmer (MIT license).\n<br>Embeds [stb_textedit.h, stb_truetype.h, stb_rect_pack.h](https://github.com/nothings/stb/) by Sean Barrett (public domain).\n\nInspiration, feedback, and testing for early versions: Casey Muratori, Atman Binstock, Mikko Mononen, Emmanuel Briney, Stefan Kamoda, Anton Mikhailov, Matt Willis. Special thanks to Alex Evans, Patrick Doane, Marco Koegler for kindly helping. Also thank you to everyone posting feedback, questions and patches on GitHub.\n\nLicense\n-------\n\nDear ImGui is licensed under the MIT License, see [LICENSE.txt](https://github.com/ocornut/imgui/blob/master/LICENSE.txt) for more information.\n",
      "stars_today": 27
    },
    {
      "id": 138754790,
      "name": "duckdb",
      "full_name": "duckdb/duckdb",
      "description": "DuckDB is an analytical in-process SQL database management system",
      "html_url": "https://github.com/duckdb/duckdb",
      "stars": 35709,
      "forks": 2877,
      "language": "C++",
      "topics": [
        "analytics",
        "database",
        "embedded-database",
        "olap",
        "sql"
      ],
      "created_at": "2018-06-26T15:04:45Z",
      "updated_at": "2026-01-28T01:00:17Z",
      "pushed_at": "2026-01-27T20:04:12Z",
      "open_issues": 615,
      "owner": {
        "login": "duckdb",
        "avatar_url": "https://avatars.githubusercontent.com/u/82039556?v=4"
      },
      "readme": "<div align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"logo/DuckDB_Logo-horizontal.svg\">\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"logo/DuckDB_Logo-horizontal-dark-mode.svg\">\n    <img alt=\"DuckDB logo\" src=\"logo/DuckDB_Logo-horizontal.svg\" height=\"100\">\n  </picture>\n</div>\n<br>\n\n<p align=\"center\">\n  <a href=\"https://github.com/duckdb/duckdb/actions\"><img src=\"https://github.com/duckdb/duckdb/actions/workflows/Main.yml/badge.svg?branch=main\" alt=\"Github Actions Badge\"></a>\n  <a href=\"https://discord.gg/tcvwpjfnZx\"><img src=\"https://shields.io/discord/909674491309850675\" alt=\"discord\" /></a>\n  <a href=\"https://github.com/duckdb/duckdb/releases/\"><img src=\"https://img.shields.io/github/v/release/duckdb/duckdb?color=brightgreen&display_name=tag&logo=duckdb&logoColor=white\" alt=\"Latest Release\"></a>\n</p>\n\n## DuckDB\n\nDuckDB is a high-performance analytical database system. It is designed to be fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect with support far beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions, collations, complex types (arrays, structs, maps), and [several extensions designed to make SQL easier to use](https://duckdb.org/docs/stable/sql/dialect/friendly_sql.html).\n\nDuckDB is available as a [standalone CLI application](https://duckdb.org/docs/stable/clients/cli/overview) and has clients for [Python](https://duckdb.org/docs/stable/clients/python/overview), [R](https://duckdb.org/docs/stable/clients/r), [Java](https://duckdb.org/docs/stable/clients/java), [Wasm](https://duckdb.org/docs/stable/clients/wasm/overview), etc., with deep integrations with packages such as [pandas](https://duckdb.org/docs/guides/python/sql_on_pandas) and [dplyr](https://duckdb.org/docs/stable/clients/r#duckplyr-dplyr-api).\n\nFor more information on using DuckDB, please refer to the [DuckDB documentation](https://duckdb.org/docs/stable/).\n\n## Installation\n\nIf you want to install DuckDB, please see [our installation page](https://duckdb.org/docs/installation/) for instructions.\n\n## Data Import\n\nFor CSV files and Parquet files, data import is as simple as referencing the file in the FROM clause:\n\n```sql\nSELECT * FROM 'myfile.csv';\nSELECT * FROM 'myfile.parquet';\n```\n\nRefer to our [Data Import](https://duckdb.org/docs/stable/data/overview) section for more information.\n\n## SQL Reference\n\nThe documentation contains a [SQL introduction and reference](https://duckdb.org/docs/stable/sql/introduction).\n\n## Development\n\nFor development, DuckDB requires [CMake](https://cmake.org), Python 3 and a `C++11` compliant compiler. In the root directory, run `make` to compile the sources. For development, use `make debug` to build a non-optimized debug version. You should run `make unit` and `make allunit` to verify that your version works properly after making changes. To test performance, you can run `BUILD_BENCHMARK=1 BUILD_TPCH=1 make` and then perform several standard benchmarks from the root directory by executing `./build/release/benchmark/benchmark_runner`. The details of benchmarks are in our [Benchmark Guide](benchmark/README.md).\n\nPlease also refer to our [Build Guide](https://duckdb.org/docs/stable/dev/building/overview) and [Contribution Guide](CONTRIBUTING.md).\n\n## Support\n\nSee the [Support Options](https://duckdblabs.com/support/) page and the dedicated [`endoflife.date`](https://endoflife.date/duckdb) page.\n",
      "stars_today": 25
    },
    {
      "id": 292014229,
      "name": "zellij",
      "full_name": "zellij-org/zellij",
      "description": "A terminal workspace with batteries included",
      "html_url": "https://github.com/zellij-org/zellij",
      "stars": 28481,
      "forks": 931,
      "language": "Rust",
      "topics": [
        "multiplexer",
        "terminal",
        "workspace"
      ],
      "created_at": "2020-09-01T14:04:28Z",
      "updated_at": "2026-01-28T01:39:45Z",
      "pushed_at": "2026-01-27T14:12:52Z",
      "open_issues": 1452,
      "owner": {
        "login": "zellij-org",
        "avatar_url": "https://avatars.githubusercontent.com/u/73778475?v=4"
      },
      "readme": "<h1 align=\"center\">\n  <br>\n  <img src=\"https://raw.githubusercontent.com/zellij-org/zellij/main/assets/logo.png\" alt=\"logo\" width=\"200\">\n  <br>\n  Zellij\n  <br>\n  <br>\n</h1>\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/CrUAFH3\"><img alt=\"Discord Chat\" src=\"https://img.shields.io/discord/771367133715628073?color=5865F2&label=discord&style=flat-square\"></a>\n  <a href=\"https://matrix.to/#/#zellij_general:matrix.org\"><img alt=\"Matrix Chat\" src=\"https://img.shields.io/matrix/zellij_general:matrix.org?color=1d7e64&label=matrix%20chat&style=flat-square&logo=matrix\"></a>\n  <a href=\"https://zellij.dev/documentation/\"><img alt=\"Zellij documentation\" src=\"https://img.shields.io/badge/zellij-documentation-fc0060?style=flat-square\"></a>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/zellij-org/zellij/main/assets/demo.gif\" alt=\"demo\">\n</p>\n\n<h4 align=\"center\">\n  [<a href=\"https://zellij.dev/documentation/installation\">Installation</a>]\n  [<a href=\"https://zellij.dev/screencasts/\">Screencasts & Tutorials</a>]\n  [<a href=\"https://zellij.dev/documentation/configuration\">Configuration</a>]\n  [<a href=\"https://zellij.dev/documentation/layouts\">Layouts</a>]\n  [<a href=\"https://zellij.dev/documentation/faq\">FAQ</a>]\n</h4>\n\n# What is this?\n\n[Zellij](#origin-of-the-name) is a workspace aimed at developers, ops-oriented people and anyone who loves the terminal. Similar programs are sometimes called \"Terminal Multiplexers\".\n\nZellij is designed around the philosophy that one must not sacrifice simplicity for power, taking pride in its great experience out of the box as well as the advanced features it places at its users' fingertips.\n\nZellij is geared toward beginner and power users alike - allowing deep customizability, personal automation through [layouts](https://zellij.dev/documentation/layouts.html), true multiplayer collaboration, unique UX features such as floating and stacked panes, and a [plugin system](https://zellij.dev/documentation/plugins.html) allowing one to create plugins in any language that compiles to WebAssembly.\n\nZellij includes a built-in [web-client](https://zellij.dev/tutorials/web-client/), making a terminal optional.\n\nYou can get started by [installing](https://zellij.dev/documentation/installation.html) Zellij and checking out the [Screencasts & Tutorials](https://zellij.dev/screencasts/).\n\nFor more details about our future plans, read about upcoming features in our [roadmap](#roadmap).\n\n## How do I install it?\n\nThe easiest way to install Zellij is through a [package for your OS](./docs/THIRD_PARTY_INSTALL.md).\n\nIf one is not available for your OS, you could download a prebuilt binary from the [latest release](https://github.com/zellij-org/zellij/releases/latest) and place it in your `$PATH`. If you'd like, we could [automatically choose one for you](#try-zellij-without-installing).\n\nYou can also install (compile) with `cargo`:\n\n```\ncargo install --locked zellij\n```\n\n#### Try Zellij without installing\n\nbash/zsh:\n```bash\nbash <(curl -L https://zellij.dev/launch)\n```\nfish/xonsh:\n```bash\nbash -c 'bash <(curl -L https://zellij.dev/launch)'\n```\n\n#### Installing from `main`\nInstalling Zellij from the `main` branch is not recommended. This branch represents pre-release code, is constantly being worked on and may contain broken or unusable features. In addition, using it may corrupt the cache for future versions, forcing users to clear it before they can use the officially released version.\n\nThat being said - no-one will stop you from using it (and bug reports involving new features are greatly appreciated), but please consider using the latest release instead as detailed at the top of this section.\n\n## How do I start a development environment?\n\n* Clone the project\n* In the project folder, for debug builds run: `cargo xtask run`\n* To run all tests: `cargo xtask test`\n\nFor more build commands, see [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Configuration\nFor configuring Zellij, please see the [Configuration Documentation](https://zellij.dev/documentation/configuration.html).\n\n## About issues in this repository\nIssues in this repository, whether open or closed, do not necessarily indicate a problem or a bug in the software. They only indicate that the reporter wanted to communicate their experiences or thoughts to the maintainers. The Zellij maintainers do their best to go over and reply to all issue reports, but unfortunately cannot promise these will always be dealt with or even read. Your understanding is appreciated.\n\n## Roadmap\nPresented here is the project roadmap, divided into three main sections.\n\nThese are issues that are either being actively worked on or are planned for the near future.\n\n***If you'll click on the image, you'll be led to an SVG version of it on the website where you can directly click on every issue***\n\n[![roadmap](https://github.com/user-attachments/assets/bb55d213-4a68-4c84-ae72-7db5c9bf94fb)](https://zellij.dev/roadmap)\n\n## Origin of the Name\n[From Wikipedia, the free encyclopedia](https://en.wikipedia.org/wiki/Zellij)\n\nZellij (Arabic: Ø§Ù„Ø²Ù„ÙŠØ¬, romanized: zillÄ«j; also spelled zillij or zellige) is a style of mosaic tilework made from individually hand-chiseled tile pieces. The pieces were typically of different colours and fitted together to form various patterns on the basis of tessellations, most notably elaborate Islamic geometric motifs such as radiating star patterns composed of various polygons. This form of Islamic art is one of the main characteristics of architecture in the western Islamic world. It is found in the architecture of Morocco, the architecture of Algeria, early Islamic sites in Tunisia, and in the historic monuments of al-Andalus (in the Iberian Peninsula).\n\n## License\n\nMIT\n\n## Sponsored by\n<a href=\"https://terminaltrove.com/\"><img src=\"https://avatars.githubusercontent.com/u/121595180?s=200&v=4\" width=\"80px\"></a>\n",
      "stars_today": 25
    },
    {
      "id": 898304583,
      "name": "eino",
      "full_name": "cloudwego/eino",
      "description": "The ultimate LLM/AI application development framework in Golang.",
      "html_url": "https://github.com/cloudwego/eino",
      "stars": 9390,
      "forks": 729,
      "language": "Go",
      "topics": [
        "ai",
        "ai-application",
        "ai-framework",
        "langchain",
        "langchain-for-go",
        "langchaingo",
        "llm-application"
      ],
      "created_at": "2024-12-04T06:47:27Z",
      "updated_at": "2026-01-28T01:48:00Z",
      "pushed_at": "2026-01-27T12:30:22Z",
      "open_issues": 101,
      "owner": {
        "login": "cloudwego",
        "avatar_url": "https://avatars.githubusercontent.com/u/79236453?v=4"
      },
      "readme": "# Eino\n\n![coverage](https://raw.githubusercontent.com/cloudwego/eino/badges/.badges/main/coverage.svg)\n[![Release](https://img.shields.io/github/v/release/cloudwego/eino)](https://github.com/cloudwego/eino/releases)\n[![WebSite](https://img.shields.io/website?up_message=cloudwego&url=https%3A%2F%2Fwww.cloudwego.io%2F)](https://www.cloudwego.io/)\n[![License](https://img.shields.io/github/license/cloudwego/eino)](https://github.com/cloudwego/eino/blob/main/LICENSE)\n[![Go Report Card](https://goreportcard.com/badge/github.com/cloudwego/eino)](https://goreportcard.com/report/github.com/cloudwego/eino)\n[![OpenIssue](https://img.shields.io/github/issues/cloudwego/eino)](https://github.com/cloudwego/kitex/eino)\n[![ClosedIssue](https://img.shields.io/github/issues-closed/cloudwego/eino)](https://github.com/cloudwego/eino/issues?q=is%3Aissue+is%3Aclosed)\n![Stars](https://img.shields.io/github/stars/cloudwego/eino)\n![Forks](https://img.shields.io/github/forks/cloudwego/eino)\n\nEnglish | [ä¸­æ–‡](README.zh_CN.md)\n\n# Overview\n\n**Eino['aino]** (pronounced similarly to \"I know\") aims to be the ultimate LLM application development framework in Golang. Drawing inspirations from many excellent LLM application development frameworks in the open-source community such as LangChain & Google ADK, etc., as well as learning from cutting-edge research and real world applications, Eino offers an LLM application development framework that emphasizes on simplicity, scalability, reliability and effectiveness that better aligns with Golang programming conventions.\n\nWhat Eino provides are:\n- a carefully curated list of **component** abstractions and implementations that can be easily reused and combined to build LLM applications\n- a powerful **composition** framework that does the heavy lifting of strong type checking, stream processing, concurrency management, aspect injection, option assignment, etc. for the user.\n- an **Agent Development Kit (ADK)** that provides high-level abstractions for building AI agents with multi-agent orchestration, human-in-the-loop interrupts, and prebuilt agent patterns.\n- a set of meticulously designed **API** that obsesses on simplicity and clarity.\n- an ever-growing collection of best practices in the form of bundled **flows** and **examples**.\n- a useful set of tools that covers the entire development cycle, from visualized development and debugging to online tracing and evaluation.\n\nWith the above arsenal, Eino can standardize, simplify, and improve efficiency at different stages of the AI application development cycle:\n![](.github/static/img/eino/eino_concept.jpeg)\n\n# A quick walkthrough\n\nUse a component directly:\n```Go\nmodel, _ := openai.NewChatModel(ctx, config) // create an invokable LLM instance\nmessage, _ := model.Generate(ctx, []*Message{\n    SystemMessage(\"you are a helpful assistant.\"),\n    UserMessage(\"what does the future AI App look like?\")})\n```\n\nOf course, you can do that, Eino provides lots of useful components to use out of the box. But you can do more by using orchestration, for three reasons:\n- orchestration encapsulates common patterns of LLM application.\n- orchestration solves the difficult problem of processing stream response by the LLM.\n- orchestration handles type safety, concurrency management, aspect injection and option assignment for you.\n\nEino provides three set of APIs for orchestration\n\n| API      | Characteristics and usage                                             |\n| -------- |-----------------------------------------------------------------------|\n| Chain    | Simple chained directed graph that can only go forward.               |\n| Graph    | Cyclic or Acyclic directed graph. Powerful and flexible.              |\n| Workflow | Acyclic graph that supports data mapping at struct field level. |\n\nLet's create a simple chain: a ChatTemplate followed by a ChatModel.\n\n![](.github/static/img/eino/simple_chain.png)\n\n```Go\nchain, _ := NewChain[map[string]any, *Message]().\n           AppendChatTemplate(prompt).\n           AppendChatModel(model).\n           Compile(ctx)\n\nchain.Invoke(ctx, map[string]any{\"query\": \"what's your name?\"})\n```\n\nNow let's create a graph that uses a ChatModel to generate answer or tool calls, then uses a ToolsNode to execute those tools if needed.\n\n![](.github/static/img/eino/tool_call_graph.png)\n\n```Go\ngraph := NewGraph[map[string]any, *schema.Message]()\n\n_ = graph.AddChatTemplateNode(\"node_template\", chatTpl)\n_ = graph.AddChatModelNode(\"node_model\", chatModel)\n_ = graph.AddToolsNode(\"node_tools\", toolsNode)\n_ = graph.AddLambdaNode(\"node_converter\", takeOne)\n\n_ = graph.AddEdge(START, \"node_template\")\n_ = graph.AddEdge(\"node_template\", \"node_model\")\n_ = graph.AddBranch(\"node_model\", branch)\n_ = graph.AddEdge(\"node_tools\", \"node_converter\")\n_ = graph.AddEdge(\"node_converter\", END)\n\ncompiledGraph, err := graph.Compile(ctx)\nif err != nil {\nreturn err\n}\nout, err := compiledGraph.Invoke(ctx, map[string]any{\"query\":\"Beijing's weather this weekend\"})\n```\n\nNow let's create a workflow that flexibly maps input & output at the field level:\n\n![](.github/static/img/eino/simple_workflow.png)\n\n```Go\ntype Input1 struct {\n    Input string\n}\n\ntype Output1 struct {\n    Output string\n}\n\ntype Input2 struct {\n    Role schema.RoleType\n}\n\ntype Output2 struct {\n    Output string\n}\n\ntype Input3 struct {\n    Query string\n    MetaData string\n}\n\nvar (\n    ctx context.Context\n    m model.BaseChatModel\n    lambda1 func(context.Context, Input1) (Output1, error)\n    lambda2 func(context.Context, Input2) (Output2, error)\n    lambda3 func(context.Context, Input3) (*schema.Message, error)\n)\n\nwf := NewWorkflow[[]*schema.Message, *schema.Message]()\nwf.AddChatModelNode(\"model\", m).AddInput(START)\nwf.AddLambdaNode(\"lambda1\", InvokableLambda(lambda1)).\n    AddInput(\"model\", MapFields(\"Content\", \"Input\"))\nwf.AddLambdaNode(\"lambda2\", InvokableLambda(lambda2)).\n    AddInput(\"model\", MapFields(\"Role\", \"Role\"))\nwf.AddLambdaNode(\"lambda3\", InvokableLambda(lambda3)).\n    AddInput(\"lambda1\", MapFields(\"Output\", \"Query\")).\n    AddInput(\"lambda2\", MapFields(\"Output\", \"MetaData\"))\nwf.End().AddInput(\"lambda3\")\nrunnable, err := wf.Compile(ctx)\nif err != nil {\n    return err\n}\nour, err := runnable.Invoke(ctx, []*schema.Message{\n    schema.UserMessage(\"kick start this workflow!\"),\n})\n```\n\nEino's **graph orchestration** provides the following benefits out of the box:\n- Type checking: it makes sure the two nodes' input and output types match at compile time.\n- Stream processing: concatenates message stream before passing to chatModel and toolsNode if needed, and copies the stream into callback handlers.\n- Concurrency management: the shared state can be safely read and written because the StatePreHandler is concurrency safe.\n- Aspect injection: injects callback aspects before and after the execution of ChatModel if the specified ChatModel implementation hasn't injected itself.\n- Option assignment: call options are assigned either globally, to specific component type or to specific node.\n\nFor example, you could easily extend the compiled graph with callbacks:\n```Go\nhandler := NewHandlerBuilder().\n  OnStartFn(\n    func(ctx context.Context, info *RunInfo, input CallbackInput) context.Context) {\n        log.Infof(\"onStart, runInfo: %v, input: %v\", info, input)\n    }).\n  OnEndFn(\n    func(ctx context.Context, info *RunInfo, output CallbackOutput) context.Context) {\n        log.Infof(\"onEnd, runInfo: %v, out: %v\", info, output)\n    }).\n  Build()\n  \ncompiledGraph.Invoke(ctx, input, WithCallbacks(handler))\n```\n\nor you could easily assign options to different nodes:\n```Go\n// assign to All nodes\ncompiledGraph.Invoke(ctx, input, WithCallbacks(handler))\n\n// assign only to ChatModel nodes\ncompiledGraph.Invoke(ctx, input, WithChatModelOption(WithTemperature(0.5))\n\n// assign only to node_1\ncompiledGraph.Invoke(ctx, input, WithCallbacks(handler).DesignateNode(\"node_1\"))\n```\n\nNow let's create a 'ReAct' agent: A ChatModel binds to Tools. It receives input Messages and decides independently whether to call the Tool or output the final result. The execution result of the Tool will again become the input Message for the ChatModel and serve as the context for the next round of independent judgment.\n\n![](.github/static/img/eino/react.png)\n\nEino's **Agent Development Kit (ADK)** provides `ChatModelAgent` that implements this pattern out of the box:\n\n```Go\nagent, _ := adk.NewChatModelAgent(ctx, &adk.ChatModelAgentConfig{\n    Name:        \"assistant\",\n    Description: \"A helpful assistant that can use tools\",\n    Model:       chatModel,\n    ToolsConfig: adk.ToolsConfig{\n        ToolsNodeConfig: compose.ToolsNodeConfig{\n            Tools: []tool.BaseTool{weatherTool, calculatorTool},\n        },\n    },\n})\n\nrunner := adk.NewRunner(ctx, adk.RunnerConfig{Agent: agent})\niter := runner.Query(ctx, \"What's the weather in Beijing this weekend?\")\nfor {\n    event, ok := iter.Next()\n    if !ok {\n        break\n    }\n    // process agent events (model outputs, tool calls, etc.)\n}\n```\n\nThe ADK handles the ReAct loop internally, emitting events for each step of the agent's reasoning process.\n\nBeyond the basic ReAct pattern, ADK provides powerful capabilities for building production-ready agent systems:\n\n**Multi-Agent with Context Management**: Agents can transfer control to sub-agents or be wrapped as tools. The framework automatically manages conversation context across agent boundaries:\n\n```Go\n// Set up agent hierarchy - mainAgent can now transfer to sub-agents\nmainAgentWithSubs, _ := adk.SetSubAgents(ctx, mainAgent, []adk.Agent{researchAgent, codeAgent})\n```\n\nWhen `mainAgent` transfers to `researchAgent`, the conversation history is automatically rewritten to provide appropriate context for the sub-agent.\n\nAgents can also be wrapped as tools, allowing one agent to invoke another as part of its tool-calling workflow:\n\n```Go\n// Wrap an agent as a tool that can be called by other agents\nresearchTool := adk.NewAgentTool(ctx, researchAgent)\n```\n\n**Interrupt Anywhere, Resume Directly**: Any agent can pause execution for human approval or external input, and resume exactly where it left off:\n\n```Go\n// Inside a tool or agent, trigger an interrupt\nreturn adk.Interrupt(ctx, \"Please confirm this action\")\n\n// Later, resume from checkpoint\niter, _ := runner.Resume(ctx, checkpointID)\n```\n\n**Prebuilt Agent Patterns**: Ready-to-use implementations for common architectures:\n\n```Go\n// Deep Agent: battle-tested pattern for complex task orchestration with \n// built-in task management, sub-agent delegation, and progress tracking\ndeepAgent, _ := deep.New(ctx, &deep.Config{\n    Name:        \"deep_agent\",\n    Description: \"An agent that breaks down and executes complex tasks\",\n    ChatModel:   chatModel,\n    SubAgents:   []adk.Agent{researchAgent, codeAgent},\n    ToolsConfig: adk.ToolsConfig{...},\n})\n\n// Supervisor pattern: one agent coordinates multiple specialists\nsupervisorAgent, _ := supervisor.New(ctx, &supervisor.Config{\n    Supervisor: coordinatorAgent,\n    SubAgents:  []adk.Agent{writerAgent, reviewerAgent},\n})\n\n// Sequential execution: agents run one after another\nseqAgent, _ := adk.NewSequentialAgent(ctx, &adk.SequentialAgentConfig{\n    SubAgents: []adk.Agent{plannerAgent, executorAgent, summarizerAgent},\n})\n```\n\n**Extensible Middleware System**: Add capabilities to agents without modifying their core logic:\n\n```Go\nfsMiddleware, _ := filesystem.NewMiddleware(ctx, &filesystem.Config{\n    Backend: myFileSystem,\n})\n\nagent, _ := adk.NewChatModelAgent(ctx, &adk.ChatModelAgentConfig{\n    // ...\n    Middlewares: []adk.AgentMiddleware{fsMiddleware},\n})\n```\n\n# Key Features\n\n## Rich Components\n\n- Encapsulates common building blocks into **component abstractions**, each have multiple **component implementations** that are ready to be used out of the box.\n    - component abstractions such as ChatModel, Tool, ChatTemplate, Retriever, Document Loader, Lambda, etc.\n    - each component type has an interface of its own: defined Input & Output Type, defined Option type, and streaming paradigms that make sense.\n    - implementations are transparent. Abstractions are all you care about when orchestrating components together.\n\n- Implementations can be nested and captures complex business logic.\n    - ReAct Agent, MultiQueryRetriever, Host MultiAgent, etc. They consist of multiple components and non-trivial business logic.\n    - They are still transparent from the outside. A MultiQueryRetriever can be used anywhere that accepts a Retriever.\n\n## Powerful Orchestration\n\n- Data flows from Retriever / Document Loaders / ChatTemplate to ChatModel, then flows to Tools and parsed as Final Answer. This directed, controlled flow of data through multiple components can be implemented through **graph orchestration**.\n- Component instances are graph nodes, and edges are data flow channels.\n- Graph orchestration is powerful and flexible enough to implement complex business logic:\n  - type checking, stream processing, concurrency management, aspect injection and option assignment are handled by the framework.\n  - branch out execution at runtime, read and write global state, or do field level data mapping using workflow(currently in alpha stage).\n  - **Aspects (Callbacks)** handle cross-cutting concerns such as logging, tracing, and metrics. Five aspects are supported: OnStart, OnEnd, OnError, OnStartWithStreamInput, OnEndWithStreamOutput. Custom callback handlers can be added during graph run via options.\n\n## Agent Development Kit (ADK)\n\nWhile graph orchestration gives you fine-grained control, the **ADK** package provides higher-level abstractions optimized for building AI agents:\n\n- **ChatModelAgent**: A ReAct-style agent that handles tool calling, conversation state, and the reasoning loop automatically.\n- **Multi-Agent with Context Engineering**: Build hierarchical agent systems where conversation history is automatically managed across agent transfers and agent-as-tool invocations, enabling seamless context sharing between specialized agents.\n- **Workflow Agents**: Compose agents using `SequentialAgent`, `ParallelAgent`, and `LoopAgent` for complex execution flows.\n- **Human-in-the-Loop**: `Interrupt` and `Resume` mechanisms with checkpoint persistence for workflows requiring human approval or input.\n- **Prebuilt Patterns**: Ready-to-use implementations including Deep Agent (task orchestration), Supervisor (hierarchical coordination), and Plan-Execute-Replan.\n- **Agent Middlewares**: Extensible middleware system for adding tools (filesystem operations) and managing context (token reduction).\n\n## Complete Stream Processing\n\n- Stream processing is important because ChatModel outputs chunks of messages in real time as it generates them. It's especially important with orchestration because more components need to handle streaming data.\n- Eino automatically **concatenates** stream chunks for downstream nodes that only accepts non-stream input, such as ToolsNode.\n- Eino automatically **boxes** non stream into stream when stream is needed during graph execution.  \n- Eino automatically **merges** multiple streams as they converge into a single downward node.\n- Eino automatically **copies** stream as they fan out to different downward node, or is passed to callback handlers.\n- Orchestration elements such as **branch** and **state handlers** are also stream aware.\n- With these streaming processing abilities, the streaming paradigms of components themselves become transparent to the user. \n- A compiled Graph can run with 4 different streaming paradigms:\n\n| Streaming Paradigm | Explanation                                                                 |\n| ------------------ | --------------------------------------------------------------------------- |\n| Invoke             | Accepts non-stream type I and returns non-stream type O                     |\n| Stream             | Accepts non-stream type I and returns stream type StreamReader[O]           |\n| Collect            | Accepts stream type StreamReader[I] and returns non-stream type O           |\n| Transform          | Accepts stream type StreamReader[I] and returns stream type StreamReader[O] |\n\n# Eino Framework Structure\n\n![](.github/static/img/eino/eino_framework.jpeg)\n\nThe Eino framework consists of several parts:\n\n- Eino(this repo): Contains Eino's type definitions, streaming mechanism, component abstractions, orchestration capabilities, agent implementations, aspect mechanisms, etc.\n\n- [EinoExt](https://github.com/cloudwego/eino-ext): Component implementations, callback handlers implementations, component usage examples, and various tools such as evaluators, prompt optimizers.\n\n- [Eino Devops](https://github.com/cloudwego/eino-ext/tree/main/devops): visualized developing, visualized debugging\n  etc.\n\n- [EinoExamples](https://github.com/cloudwego/eino-examples) is the repo containing example applications and best practices for Eino.\n\n## Detailed Documentation\n\nFor learning and using Eino, we provide a comprehensive Eino User Manual to help you quickly understand the concepts in Eino and master the skills of developing AI applications based on Eino. Start exploring through the [Eino User Manual](https://www.cloudwego.io/zh/docs/eino/) now!\n\nFor a quick introduction to building AI applications with Eino, we recommend starting with [Eino: Quick Start](https://www.cloudwego.io/zh/docs/eino/quick_start/)\n\n## Dependencies\n- Go 1.18 and above.\n\n## Code Style\n\nThis repo uses `golangci-lint` to enforce basic code conventions. You can check locally with:\n\n```bash\ngolangci-lint run ./...\n```\n\nRules enforced include:\n- Exported functions, interfaces, packages, etc. should have proper GoDoc comments.\n- Code should be formatted with `gofmt -s`.\n- Import order should follow `goimports` (std -> third party -> local).\n\n## Security\n\nIf you discover a potential security issue in this project, or think you may\nhave discovered a security issue, we ask that you notify Bytedance Security via our [security center](https://security.bytedance.com/src) or [vulnerability reporting email](sec@bytedance.com).\n\nPlease do **not** create a public GitHub issue.\n\n## Contact US\n- How to become a member: [COMMUNITY MEMBERSHIP](https://github.com/cloudwego/community/blob/main/COMMUNITY_MEMBERSHIP.md)\n- Issues: [Issues](https://github.com/cloudwego/eino/issues)\n- Lark: Scan the QR code below with [Register Feishu](https://www.feishu.cn/en/) to join our CloudWeGo/eino user group.\n\n&ensp;&ensp;&ensp; <img src=\".github/static/img/eino/lark_group_zh.png\" alt=\"LarkGroup\" width=\"200\"/>\n\n## License\n\nThis project is licensed under the [Apache-2.0 License](LICENSE-APACHE).\n",
      "stars_today": 25
    },
    {
      "id": 20580498,
      "name": "kubernetes",
      "full_name": "kubernetes/kubernetes",
      "description": "Production-Grade Container Scheduling and Management",
      "html_url": "https://github.com/kubernetes/kubernetes",
      "stars": 120112,
      "forks": 42305,
      "language": "Go",
      "topics": [
        "cncf",
        "containers",
        "go",
        "kubernetes"
      ],
      "created_at": "2014-06-06T22:56:04Z",
      "updated_at": "2026-01-28T01:40:33Z",
      "pushed_at": "2026-01-28T00:07:49Z",
      "open_issues": 2615,
      "owner": {
        "login": "kubernetes",
        "avatar_url": "https://avatars.githubusercontent.com/u/13629408?v=4"
      },
      "readme": "# Kubernetes (K8s)\n\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/569/badge)](https://bestpractices.coreinfrastructure.org/projects/569) [![Go Report Card](https://goreportcard.com/badge/github.com/kubernetes/kubernetes)](https://goreportcard.com/report/github.com/kubernetes/kubernetes) ![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/kubernetes/kubernetes?sort=semver)\n\n<img src=\"https://github.com/kubernetes/kubernetes/raw/master/logo/logo.png\" width=\"100\">\n\n----\n\nKubernetes, also known as K8s, is an open source system for managing [containerized applications]\nacross multiple hosts. It provides basic mechanisms for the deployment, maintenance,\nand scaling of applications.\n\nKubernetes builds upon a decade and a half of experience at Google running\nproduction workloads at scale using a system called [Borg],\ncombined with best-of-breed ideas and practices from the community.\n\nKubernetes is hosted by the Cloud Native Computing Foundation ([CNCF]).\nIf your company wants to help shape the evolution of\ntechnologies that are container-packaged, dynamically scheduled,\nand microservices-oriented, consider joining the CNCF.\nFor details about who's involved and how Kubernetes plays a role,\nread the CNCF [announcement].\n\n----\n\n## To start using K8s\n\nSee our documentation on [kubernetes.io].\n\nTake a free course on [Scalable Microservices with Kubernetes].\n\nTo use Kubernetes code as a library in other applications, see the [list of published components](https://git.k8s.io/kubernetes/staging/README.md).\nUse of the `k8s.io/kubernetes` module or `k8s.io/kubernetes/...` packages as libraries is not supported.\n\n## To start developing K8s\n\nThe [community repository] hosts all information about\nbuilding Kubernetes from source, how to contribute code\nand documentation, who to contact about what, etc.\n\nIf you want to build Kubernetes right away there are two options:\n\n##### You have a working [Go environment].\n\n```\ngit clone https://github.com/kubernetes/kubernetes\ncd kubernetes\nmake\n```\n\n##### You have a working [Docker environment].\n\n```\ngit clone https://github.com/kubernetes/kubernetes\ncd kubernetes\nmake quick-release\n```\n\nFor the full story, head over to the [developer's documentation].\n\n## Support\n\nIf you need support, start with the [troubleshooting guide],\nand work your way through the process that we've outlined.\n\nThat said, if you have questions, reach out to us\n[one way or another][communication].\n\n[announcement]: https://cncf.io/news/announcement/2015/07/new-cloud-native-computing-foundation-drive-alignment-among-container\n[Borg]: https://research.google.com/pubs/pub43438.html?authuser=1\n[CNCF]: https://www.cncf.io/about\n[communication]: https://git.k8s.io/community/communication\n[community repository]: https://git.k8s.io/community\n[containerized applications]: https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/\n[developer's documentation]: https://git.k8s.io/community/contributors/devel#readme\n[Docker environment]: https://docs.docker.com/engine\n[Go environment]: https://go.dev/doc/install\n[kubernetes.io]: https://kubernetes.io\n[Scalable Microservices with Kubernetes]: https://www.udacity.com/course/scalable-microservices-with-kubernetes--ud615\n[troubleshooting guide]: https://kubernetes.io/docs/tasks/debug/\n\n## Community Meetings \n\nThe [Calendar](https://www.kubernetes.dev/resources/calendar/) has the list of all the meetings in the Kubernetes community in a single location.\n\n## Adopters\n\nThe [User Case Studies](https://kubernetes.io/case-studies/) website has real-world use cases of organizations across industries that are deploying/migrating to Kubernetes.\n\n## Governance \n\nKubernetes project is governed by a framework of principles, values, policies and processes to help our community and constituents towards our shared goals.\n\nThe [Kubernetes Community](https://github.com/kubernetes/community/blob/master/governance.md) is the launching point for learning about how we organize ourselves.\n\nThe [Kubernetes Steering community repo](https://github.com/kubernetes/steering) is used by the Kubernetes Steering Committee, which oversees governance of the Kubernetes project.\n\n## Roadmap \n\nThe [Kubernetes Enhancements repo](https://github.com/kubernetes/enhancements) provides information about Kubernetes releases, as well as feature tracking and backlogs.\n",
      "stars_today": 24
    },
    {
      "id": 1014515436,
      "name": "ESP32-Bus-Pirate",
      "full_name": "geo-tp/ESP32-Bus-Pirate",
      "description": "A Hardware Hacking Tool with Web-Based CLI That Speaks Every Protocol ",
      "html_url": "https://github.com/geo-tp/ESP32-Bus-Pirate",
      "stars": 2447,
      "forks": 187,
      "language": "C",
      "topics": [
        "arduino",
        "bluetooth",
        "can-bus",
        "debugging",
        "eeprom",
        "esp32",
        "gpio",
        "hardware-hacking",
        "i2c",
        "iot",
        "jtag",
        "protocol",
        "pwm",
        "rfid",
        "serial-communication",
        "spi",
        "subghz",
        "uart",
        "wifi"
      ],
      "created_at": "2025-07-05T21:59:46Z",
      "updated_at": "2026-01-28T00:44:12Z",
      "pushed_at": "2026-01-28T00:44:09Z",
      "open_issues": 24,
      "owner": {
        "login": "geo-tp",
        "avatar_url": "https://avatars.githubusercontent.com/u/72315006?v=4"
      },
      "readme": "# ESP32 Bus Pirate\n\n![Logo banner of the ESP32 Bus Pirate firmware](images/logo_protocols_banner_small.png)\n\n\n**ESP32 Bus Pirate** is an open-source firmware that turns your device into a multi-protocol hacker's tool, inspired by the [legendary Bus Pirate](https://buspirate.com/).\n\nIt supports sniffing, sending, scripting, and interacting with various digital protocols (I2C, UART, 1-Wire, SPI, etc.) via a serial terminal or web-based CLI. It also communicates with radio protocols like Bluetooth, Wi-Fi, Sub-GHz and RFID.\n\nUse the [ESP32 Bus Pirate Web Flasher](https://geo-tp.github.io/ESP32-Bus-Pirate/webflasher/) to install the firmware in one click. See the [Wiki](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki) for step-by-step guides on every mode and command. Check [ESP32 Bus Pirate Scripts](https://github.com/geo-tp/ESP32-Bus-Pirate-Scripts) for a collection of scripts.\n\n![Demo showing the different mode of the ESP32 Bus Pirate firmware](images/help.gif)\n![Demo showing the LittleFS file system of the ESP32 Bus Pirate firmware](images/littlefs.gif)\n\n## Features\n\n- Interactive command-line interface (CLI) via **USB Serial or WiFi Web**.\n- **Modes for:**\n   - [HiZ](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/01-HiZ) (default)\n   - [I2C](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/05-I2C) (scan, glitch, slave mode, dump, eeprom)\n   - [SPI](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/06-SPI) (eeprom, flash, sdcard, slave mode)\n   - [UART](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/03-UART) / [Half-Duplex UART](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/04-HDUART) (bridge, read, write)\n   - [1WIRE](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/02-1WIRE) (ibutton, eeprom)\n   - [2WIRE](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/07-2WIRE) (sniff, smartcard) / [3WIRE](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/08-3WIRE) (eeprom)\n   - [DIO](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/09-DIO) (Digital I/O, read, pullup, set, pwm)\n   - [Infrared](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/11-INFRARED) (device-b-gone, universal remote)\n   - [USB](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/12-USB) (HID, mouse, keyboard, gamepad, storage)\n   - [Bluetooth](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/13-BLUETOOTH) (BLE HID, scan, spoofing, sniffing)\n   - [Wi-Fi](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/14-WIFI) / [Ethernet](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/18-ETHERNET) (sniff, deauth, nmap, netcat)\n   - [JTAG](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/15-JTAG) (scan pinout, SWD)\n   - [LED](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/10-LED) (animations, set LEDs)\n   - [I2S](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/16-I2S) (test speakers, mic, play sound)\n   - [CAN](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/17-CAN) (sniff, send and receive frames)\n   - [SUBGHZ](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/19-SUBGHZ) (sniff, scan, replay)\n   - [RFID](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/20-RFID) (read, write, clone)\n   - [RF24](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/21-RF24) (scan, sniff)\n\n\n- **Protocol sniffers** for I2C, SPI, 1Wire, 2wire, CAN, Wi-Fi, Bluetooth, SubGhz.\n- Baudrate **auto-detection**, AT commands and various tools for UART.\n- Registers manipulation, **EEPROM dump tools**, identify devices for I2C.\n- Read all sort of **EEPROM, Flash** and various others tools for SPI.\n- Scripting using **Bus Pirate-style bytecode** instructions or **Python**.\n- Device-B-Gone command with more than **80 supported INFRARED protocols**.\n- Direct I/O management, **PWM, servo**, pulse.\n- Analyze radio signals and frequencies **on every bands**.\n- Near than **50 addressable LEDs protocols** supported.\n- **Ethernet and WiFi** are supported to access networks.\n- Import and export data with the **LittleFS over HTTP.**\n\n## Supported Devices\n\n\n| Device               |                                     | Description                       |\n|-----------------------|------------------------------------------|---------------------------------------------------|\n| **ESP32 S3 Dev Kit**  | ![Photo of the ESP32 S3 Dev Kit](/images/s3-devkit_s.jpg)     | More than 20 available GPIO, 1 button |\n| **M5 Cardputer**      | ![Photo of the M5 Cardputer](/images/cardputer_s.png)            | 2 GPIO (Grove), screen, keyboard, mic, speaker, IR TX, SD card, battery, [standalone mode](#standalone-mode-for-the-cardputer)            |\n| **M5 Cardputer ADV**  | ![Photo of the M5 Cardputer ADV](/images/cardputer-adv_s.jpg)    | 12 GPIO (Grove, Header), screen, keyboard, mic, speaker, IR TX, SD card, IMU, battery, [standalone mode](#standalone-mode-for-the-cardputer)                  |\n| **M5 Stick C Plus 2** | ![Photo of the M5 Stick C Plus 2](/images/m5stick_s.jpg)      | 5 GPIO (Grove, Header), screen, mic, buzzer, IR TX, IMU, 3 buttons, battery                   |\n| **M5 Stick S3** | ![Photo of the M5 Stick S3](/images/m5sticks3_s.jpg)      | 13 GPIO (Grove, Header), screen, mic, speaker, IR TX, IR RX, IMU, 3 buttons, battery                 |\n| **M5 StampS3**        | ![Photo of the M5 StampS3](/images/stamps3_s.jpg)             | 9 GPIO (exposed pins), 1 button                       |\n| **M5 AtomS3 Lite**    | ![Photo of the M5 Atom S3 Lite](/images/atom_s.jpg)            | 8 GPIO (Grove, Header), IR TX, 1 buttton                  |\n| **LILYGO T-Embed**    | ![Photo of the LILYGO T-Embed](/images/tembed_s.jpg)          | 9 GPIO (Grove, Header), screen, encoder, speaker, mic, SD card                                         |\n| **LILYGO T-Embed CC1101** | ![Photo of the LILYGO T-Embed CC1101](/images/tembedcc1101_s.jpg) | 4 GPIO (2x Qwiic), screen, encoder, speaker, mic, SD Card, CC1101, PN532, IR TX, IR RX , battery                                 |\n| **Seeed Studio Xiao S3** | ![Photo of the Seeed Studio Xiao ESP32-S3](/images/xiaos3_s.jpg)        | 9 GPIO (exposed pins), 1 button        \n\n- **Other ESP32-S3-based Boards**\n\n  - All boards based on the **ESP32-S3 can be supported**, provided they have at least **8 MB of flash.**\n\n  - You can **flash the s3 dev-kit firmware onto any ESP32-S3 board.**\n\n  - Keep in mind that the **default pin mapping in the firmware may not match** your specific board.\n\n## Getting Started\n\n[![Banner of the ESP32 Bus Pirate web flasher](images/flasher.jpg)](https://geo-tp.github.io/ESP32-Bus-Pirate/webflasher/)\n\n1. ğŸ”§ Flash the firmware  \n   - Use the [ESP32 Bus Pirate Web Flasher](https://geo-tp.github.io/ESP32-Bus-Pirate/webflasher/) to burn the firmware directly from a web browser.\n   - You can also burn it on [M5Burner](https://docs.m5stack.com/en/download), in the M5stick, AtomS3, M5StampS3 or Cardputer category.\n\n2. ğŸ”Œ Connect via Serial or Web\n   - Serial: any terminal app (see [Connect via Serial](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Serial))\n   - Web: configure Wi-Fi and access the CLI via browser (see [Wi-Fi Connection](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/00-Terminal))\n\n3. ğŸ§ª Use commands like:\n   ```bash\n   mode\n   help\n   scan\n   sniff\n   ...\n    ```\n\n## Wiki\n\n[![Banner of the ESP32 Bus Pirate Wiki page](images/bus_pirate_wiki.png)](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/)\n\nğŸ“š Visit the **[Wiki](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki)** for detailed documentation on every mode and command.\n\nIncludes:\n- [Terminal mode](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/00-Terminal) - About serial and web terminal.\n- [Mode overviews](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki) - Browse supported modes.\n- [Instruction syntax](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Instructions) - Master the instructions.\n- [Serial setup](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Serial) - Serial access via USB.\n\nThe wiki is the best place to learn how everything works.\n\n## Scripting\n\n[![Banner of the ESP32 Bus Pirate Scripts page](images/bus_pirate_scripts.png)](https://github.com/geo-tp/ESP32-Bus-Pirate-Scripts/)\n\nğŸ› ï¸ You can [automate interactions with the ESP32 Bus Pirate](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Python) using **Python scripts over serial.**\n\n**Examples and ready-to-use scripts** are available in the repository: [ESP32 Bus Pirate Scripts](https://github.com/geo-tp/ESP32-Bus-Pirate-Scripts).\n\n**Including:** Logging data in a file, eeprom and flash dump, interracting with GPIOs, LED animation...\n   \n## ESP32 Bus Pirate on M5 Devices\n![A photo of the ESP32 Bus Pirate firmware running on M5 Stack devices](images/m5buspirate_s.jpg)\n\n## ESP32 Bus Pirate on T-Embed\n![A photo of the ESP32 Bus Pirate firmware running on Lilygo device](images/tembedbuspirate_s.jpg)\n\n## Command-Line Interfaces\n\nThe ESP32 Bus Pirate firmware provides three command-line interface (CLI) modes:\n\n| Interface         | Advantages                                                                 | Ideal for...                          |\n|------------------|-----------------------------------------------------------------------------|----------------------------------------|\n| **Web Interface** | - Accessible from any browser<br>- PC, tablets, mobiles<br>- Works over Wi-Fi<br>- No cables needed | Quick tests, demos, headless setups   |\n| **Serial Interface** | - Faster performance<br>- Instant responsiveness<br>- Handles large data smoothly | Intensive sessions, frequent interactions |\n| **Standalone** | - Only for the Cardputer<br>- On device keyboard<br>- On device screen | Portable sessions, Quick tests |\n\n\nAll interfaces share the same command structure and can be used interchangeably ([more details](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/00-Terminal)).\n\n## Mobile Web Interface over WiFi\n![An iPhone screenshot showing the Bus Pirate firmware web interface](images/presentation_mobile.png)\n\n## Standalone Mode for the Cardputer\n![A Cardputer running the ESP32 Bus pirate in standalone mode](images/standalonemode_s.png)\n\n## Using the ESP32 Bus Pirate to speak UART over WiFi\n![A demo Using the ESP32 Bus pirate firmware with UART](images/demo2.gif)\n\n## Contribute\nSee [How To Contribute](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Contribute) section, which outlines a **simple way to add a new command** to any mode.\n\n## Visuals Assets\n\n#### [![Small logo of the ESP32 Bus Pirate firmware](images/logo_square_small.png)](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Visual-Assets)\n\nSee [images, logo, presentations, photo, video, illustrations](https://github.com/geo-tp/ESP32-Bus-Pirate/wiki/99-Visual-Assets). These visuals can be **freely used in blog posts, documentation, videos, or articles** to help explain and promote the firmware.\n\n\n## Warning\n> âš ï¸ **Voltage Warning**: Devices should only operate at **3.3V** or **5V**.  \n> - Do **not** connect peripherals using other voltage levels â€” doing so may **damage your ESP32**.\n\n> âš ï¸ **Usage Warning**: This firmware is provided for **educational, diagnostic, and interoperability testing purposes only**.\n> - Do not use it to interfere with, probe, or manipulate devices without proper authorization.\n> - Avoid any unauthorized RF transmissions (e.g., sub-GHz) that could violate local regulations or disrupt networks and communications.\n> - The authors are not responsible for any misuse of this software or hardware, including legal consequences resulting from unauthorized access or signal emission.\n> - Always stay within the bounds of your countryâ€™s laws and responsible disclosure policies.\n\n\n",
      "stars_today": 24
    },
    {
      "id": 506113888,
      "name": "elide",
      "full_name": "elide-dev/elide",
      "description": "fast, all-in-one, AI-native, multi-lang, runtime",
      "html_url": "https://github.com/elide-dev/elide",
      "stars": 453,
      "forks": 39,
      "language": "Kotlin",
      "topics": [
        "graalvm",
        "javascript",
        "jvm",
        "kotlin",
        "nodejs",
        "python",
        "runtime",
        "typescript",
        "wintertc"
      ],
      "created_at": "2022-06-22T05:48:04Z",
      "updated_at": "2026-01-28T02:01:41Z",
      "pushed_at": "2026-01-26T21:26:54Z",
      "open_issues": 132,
      "owner": {
        "login": "elide-dev",
        "avatar_url": "https://avatars.githubusercontent.com/u/101684957?v=4"
      },
      "readme": "<p align=\"center\">\n  <a href=\"https://github.com/elide-dev\">\n    <img src=\"https://static.elide.dev/assets/org-profile/creative/elide-banner-purple.png\" alt=\"Elide\" />\n  </a>\n</p>\n\n<p align=\"center\">\n<b>Elide is a fast batteries-included runtime, combining support for Kotlin, JavaScript, TypeScript, and Python.</b>\n<br />\n<br />\n<i>elide: verb. to omit (a sound or syllable) when speaking. to join together; to merge.</i>\n<br />\n<br />\n</p>\n\n<hr />\n\n<p align=\"center\">\n  <a href=\"https://github.com/elide-dev/elide/actions/workflows/build.ci.yml\"><img src=\"https://github.com/elide-dev/elide/actions/workflows/on.push.yml/badge.svg\" /></a>\n  <a href=\"https://codecov.io/gh/elide-dev/elide\"><img src=\"https://codecov.io/gh/elide-dev/elide/branch/main/graph/badge.svg?token=FXxhJlpKG3\" /></a>\n  <a href=\"https://bestpractices.coreinfrastructure.org/projects/7690\"><img src=\"https://bestpractices.coreinfrastructure.org/projects/7690/badge\" /></a>\n  <a href=\"https://github.com/elide-dev/elide\"><img src=\"https://img.shields.io/badge/Contributor%20Covenant-v1.4-ff69b4.svg\" alt=\"Code of Conduct\" /></a>\n  <br />\n  <a href=\"https://elide.dev/discord\"><img src=\"https://img.shields.io/discord/1119121740161884252?b1&logo=discord&logoColor=white&label=Discord\" /></a>\n  <a href=\"https://262.ecma-international.org/13.0/\"><img src=\"https://img.shields.io/badge/-ECMA2024-blue.svg?logo=javascript&logoColor=white\" /></a>\n  <a href=\"https://typescriptlang.org\"><img src=\"https://img.shields.io/badge/-TypeScript-blue.svg?logo=typescript&logoColor=white\" /></a>\n  <img alt=\"Python 3.11.x\" src=\"https://img.shields.io/badge/Python%203.11.x-green?style=flat&logo=python&logoColor=white&color=blue\">\n  <a href=\"https://pkl-lang.org\"><img src=\"https://img.shields.io/badge/-Pkl-blue.svg?logo=apple&logoColor=white\" /></a>\n  <a href=\"https://kotlinlang.org\"><img src=\"https://img.shields.io/badge/-Kotlin-blue.svg?logo=kotlin&logoColor=white\" /></a>\n</p>\n\n<p align=\"center\">\nLatest: <code>1.0.0-beta10</code>\n</p>\n<p align=\"center\">\n  Learn more at <a href=\"https://elide.dev\">elide.dev</a> | <a href=\"https://docs.elide.dev\">Docs, Guides, and Samples</a>\n</p>\n\n<hr />\n\n> [!IMPORTANT]\n> Careful! Elide is in beta.\n\n## Usage\n\nElide is like Node or Python. Use it to run things:\n```shell\n> elide ./my-code.{ts,js,py,kts,kt}\n```\n\nYou can use Node APIs. You can even mix languages:\n```typescript\n// sample.mts\n\n// use node apis\nimport { readFileSync } from \"node:fs\"\n\n// interoperate across languages \nimport sample from \"./sample.py\"\n\n// this is typescript - no build step needed first, like deno or bun\nconst x: number = 42;\n\nconsole.log(sample.greeting() + ` The answer is ${x}`);\n```\n```python\n# sample.py\n\ndef greeting(name = \"Elide\"):\n  return f\"Hello, {name}!\"\n```\n\n```shell\n> elide ./sample.mts\nHello, Elide! The answer is 42\n```\n\n### Kotlin as a first-class citizen\n\nElide can run Kotlin with no prior build step, can build Java code identically to `javac`, and can build Kotlin code identically to `kotlinc`.\n\n![elide-projects](./project/gifs/init-build-test.gif)\n\n- KotlinX is supported out of the box with no need to install dependencies\n- Build Kotlin to JVM bytecode, run tests, and install from Maven, all without verbose configuration\n\n### Pkl as a manifest format\n\nElide uses [Apple's Pkl](https://pkl-lang.org) as a dialect for project manifests. This is like Elide's equivalent of `package.json` or `pom.xml`. Here's an example:\n\n```pkl\namends \"elide:project.pkl\"\n\nname = \"elide-test-ktjvm\"\ndescription = \"Example project using Elide with Kotlin/JVM.\"\n\ndependencies {\n  maven {\n    packages {\n      // Guava\n      \"com.google.guava:guava:33.4.8-jre\"\n    }\n  }\n}\n```\n\nThis is the manifest used above :point_up: in the _Kotlin as a first-class citizen_ sample.\n\n> [!NOTE]\n> See the full sources for the `ktjvm` sample [here](https://github.com/elide-dev/elide/tree/main/packages/cli/src/projects/ktjvm)\n\nRead more about Elide's [feature highlights](https://elide.dev)\n\n### Support for End-User Binaries + Containers\n\nElide has early support for building _your_ apps into native binaries, too! You can even wrap these in containers,\nwithout the need for Docker.\n\nAdding to the _Kotlin as a first-class-citizen_ example above:\n```pkl\nartifacts {\n  // Build a JAR from our Kotlin code.\n  [\"jar\"] = build.jar()\n\n  // Build a native image from our JAR and classpath.\n  [\"native\"] = build.nativeImage(\"jar\")\n\n  // Wrap the native image in a container image.\n  [\"container\"] = (build.containerImage(\"native\")) {\n    // Set this property to a remote image. This is the target image.\n    image = \"ghcr.io/elide-dev/samples/containers\"\n  }\n}\n```\n\nNow, `elide build` produces a JAR, a native image, and a container image, and then pushes it directly up to the registry\nlisted in the config:\n\n![elide-containers](./project/gifs/containers.gif)\n\n> [!NOTE]\n> See the full sources for the `containers` sample [here](https://github.com/elide-dev/elide/tree/main/packages/cli/src/projects/containers)\n\n## Installation\n\nYou can install Elide in several ways:\n\n### Script Install (Linux amd64 or macOS arm64)\n\n```shell\ncurl -sSL --tlsv1.2 elide.sh | bash -s -\n```\n\n### Homebrew (macOS)\n```shell\nbrew tap elide-dev/elide\nbrew install elide\n```\n\nAfter installation, you can run `elide --help` or `elide info` to see more information.\n\n> [!NOTE]\n> If you need a binary for a different architecture, please file an issue.\n\n### Using Elide via Docker\n\nWe provide a container image, hosted on GitHub:\n\n```\ndocker run --rm -it ghcr.io/elide-dev/elide\n```\n\n### Using Elide in GitHub Actions\n\nWe provide a [setup action](https://github.com/marketplace/actions/setup-elide):\n\n```yaml\n- name: \"Setup: Elide\"\n  uses: elide-dev/setup-elide@v3\n  with:\n    # any tag from the `elide-dev/elide` repo; omit for latest\n    version: 1.0.0-beta10\n```\n\n### Using Elide from Gradle\n\nWe provide an experimental [Gradle plugin](https://github.com/elide-dev/gradle) which can:\n\n- Accelerate `javac` compilations by up to 20x (drop-in!) with identical inputs and outputs\n- Accelerate downloading of Maven dependencies\n\nThe plugin documentation explains how it works. By native-image compiling tools like `javac`, JIT warmup is skipped, potentially yielding significant performance gains for projects under 10,000 classes.\n\n[Installation in Gradle](https://github.com/elide-dev/gradle)\n```kotlin\nplugins {\n  alias(elideRuntime.plugins.elide)\n}\n```\n\n### Using Elide via GitHub Codespaces\n\nWe provide a [GitHub Codespace](https://github.com/features/codespaces) with Elide pre-installed. You can click below to try it out, right from your browser:\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/elide-dev/elide?devcontainer_path=.devcontainer%2Fdevcontainer.json)\n\n## Contributing\n\nIssue reports and pull requests are welcome! See our [contribution guidelines](CONTRIBUTING.md) or join our [discord community](https://elide.dev/discord) and let us know which features you would like to see implemented, or simply participate in the discussions to help shape the future of the project.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=elide-dev/elide&type=Date)](https://star-history.com/#elide-dev/elide)\n\n[1]: https://kotlinlang.org/\n[2]: https://graalvm.org/\n[3]: https://micronaut.io/\n[4]: https://reactjs.org/\n[5]: https://developers.google.com/protocol-buffers\n[6]: https://grpc.io/\n[7]: https://developers.google.com/closure\n[8]: https://bazel.build/\n[9]: https://gradle.org/\n[10]: https://developers.google.com/speed/pagespeed/module\n[11]: https://github.com/sgammon/elide/tree/master\n[12]: https://github.com/sgammon/elide\n[13]: https://buf.build\n[14]: https://esbuild.github.io/\n",
      "stars_today": 24
    },
    {
      "id": 724854743,
      "name": "mlx",
      "full_name": "ml-explore/mlx",
      "description": "MLX: An array framework for Apple silicon",
      "html_url": "https://github.com/ml-explore/mlx",
      "stars": 23657,
      "forks": 1478,
      "language": "C++",
      "topics": [
        "mlx"
      ],
      "created_at": "2023-11-28T23:33:45Z",
      "updated_at": "2026-01-28T01:43:26Z",
      "pushed_at": "2026-01-28T01:43:22Z",
      "open_issues": 148,
      "owner": {
        "login": "ml-explore",
        "avatar_url": "https://avatars.githubusercontent.com/u/102832242?v=4"
      },
      "readme": "# MLX\n\n[**Quickstart**](#quickstart) | [**Installation**](#installation) |\n[**Documentation**](https://ml-explore.github.io/mlx/build/html/index.html) |\n[**Examples**](#examples)\n\n[![CircleCI](https://circleci.com/gh/ml-explore/mlx.svg?style=svg)](https://circleci.com/gh/ml-explore/mlx)\n\nMLX is an array framework for machine learning on Apple silicon,\nbrought to you by Apple machine learning research.\n\nSome key features of MLX include:\n\n- **Familiar APIs**: MLX has a Python API that closely follows NumPy. MLX\n   also has fully featured C++, [C](https://github.com/ml-explore/mlx-c), and\n   [Swift](https://github.com/ml-explore/mlx-swift/) APIs, which closely mirror\n   the Python API. MLX has higher-level packages like `mlx.nn` and\n   `mlx.optimizers` with APIs that closely follow PyTorch to simplify building\n   more complex models.\n\n- **Composable function transformations**: MLX supports composable function\n  transformations for automatic differentiation, automatic vectorization,\n  and computation graph optimization.\n\n- **Lazy computation**: Computations in MLX are lazy. Arrays are only\n  materialized when needed.\n\n- **Dynamic graph construction**: Computation graphs in MLX are constructed\n  dynamically. Changing the shapes of function arguments does not trigger\n  slow compilations, and debugging is simple and intuitive.\n\n- **Multi-device**: Operations can run on any of the supported devices\n  (currently the CPU and the GPU).\n\n- **Unified memory**: A notable difference from MLX and other frameworks\n  is the *unified memory model*. Arrays in MLX live in shared memory.\n  Operations on MLX arrays can be performed on any of the supported\n  device types without transferring data.\n\nMLX is designed by machine learning researchers for machine learning\nresearchers. The framework is intended to be user-friendly, but still efficient\nto train and deploy models. The design of the framework itself is also\nconceptually simple. We intend to make it easy for researchers to extend and\nimprove MLX with the goal of quickly exploring new ideas.\n\nThe design of MLX is inspired by frameworks like\n[NumPy](https://numpy.org/doc/stable/index.html),\n[PyTorch](https://pytorch.org/), [Jax](https://github.com/google/jax), and\n[ArrayFire](https://arrayfire.org/).\n\n## Examples\n\nThe [MLX examples repo](https://github.com/ml-explore/mlx-examples) has a\nvariety of examples, including:\n\n- [Transformer language model](https://github.com/ml-explore/mlx-examples/tree/main/transformer_lm) training.\n- Large-scale text generation with\n  [LLaMA](https://github.com/ml-explore/mlx-examples/tree/main/llms/llama) and\n  finetuning with [LoRA](https://github.com/ml-explore/mlx-examples/tree/main/lora).\n- Generating images with [Stable Diffusion](https://github.com/ml-explore/mlx-examples/tree/main/stable_diffusion).\n- Speech recognition with [OpenAI's Whisper](https://github.com/ml-explore/mlx-examples/tree/main/whisper).\n\n## Quickstart\n\nSee the [quick start\nguide](https://ml-explore.github.io/mlx/build/html/usage/quick_start.html)\nin the documentation.\n\n## Installation\n\nMLX is available on [PyPI](https://pypi.org/project/mlx/). To install MLX on\nmacOS, run:\n\n```bash\npip install mlx\n```\n\nTo install the CUDA backend on Linux, run:\n\n```bash\npip install mlx[cuda]\n```\n\nTo install a CPU-only Linux package, run:\n\n```bash\npip install mlx[cpu]\n```\n\nCheckout the\n[documentation](https://ml-explore.github.io/mlx/build/html/install.html#)\nfor more information on building the C++ and Python APIs from source.\n\n## Contributing\n\nCheck out the [contribution guidelines](https://github.com/ml-explore/mlx/tree/main/CONTRIBUTING.md) for more information\non contributing to MLX. See the\n[docs](https://ml-explore.github.io/mlx/build/html/install.html) for more\ninformation on building from source, and running tests.\n\nWe are grateful for all of [our\ncontributors](https://github.com/ml-explore/mlx/tree/main/ACKNOWLEDGMENTS.md#Individual-Contributors). If you contribute\nto MLX and wish to be acknowledged, please add your name to the list in your\npull request.\n\n## Citing MLX\n\nThe MLX software suite was initially developed with equal contribution by Awni\nHannun, Jagrit Digani, Angelos Katharopoulos, and Ronan Collobert. If you find\nMLX useful in your research and wish to cite it, please use the following\nBibTex entry:\n\n```text\n@software{mlx2023,\n  author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},\n  title = {{MLX}: Efficient and flexible machine learning on Apple silicon},\n  url = {https://github.com/ml-explore},\n  version = {0.0},\n  year = {2023},\n}\n```\n",
      "stars_today": 23
    },
    {
      "id": 476427476,
      "name": "Maestro",
      "full_name": "mobile-dev-inc/Maestro",
      "description": "Painless E2E Automation for Mobile and Web",
      "html_url": "https://github.com/mobile-dev-inc/Maestro",
      "stars": 10204,
      "forks": 578,
      "language": "Kotlin",
      "topics": [
        "android",
        "blackbox-testing",
        "ios",
        "ui-automation"
      ],
      "created_at": "2022-03-31T18:17:40Z",
      "updated_at": "2026-01-27T21:03:09Z",
      "pushed_at": "2026-01-27T12:02:03Z",
      "open_issues": 489,
      "owner": {
        "login": "mobile-dev-inc",
        "avatar_url": "https://avatars.githubusercontent.com/u/65870663?v=4"
      },
      "readme": "> [!TIP]\n> Great things happen when testers connect â€” [Join the Maestro Community](https://maestrodev.typeform.com/to/FelIEe8A)\n\n\n<p align=\"center\">\n  <a href=\"https://www.maestro.dev\">\n    <img width=\"1200\" alt=\"Maestro logo\" src=\"https://github.com/mobile-dev-inc/Maestro/blob/main/assets/banne_logo.png\" />\n  </a>\n</p>\n\n\n<p align=\"center\">\n  <strong>Maestro</strong> is an open-source framework that makes UI and end-to-end testing for Android, iOS, and web apps simple and fast.<br/>\n  Write your first test in under five minutes using YAML flows and run them on any emulator, simulator, or browser.\n</p>\n\n<img src=\"https://user-images.githubusercontent.com/847683/187275009-ddbdf963-ce1d-4e07-ac08-b10f145e8894.gif\" />\n\n---\n\n## Table of Contents\n\n- [Why Maestro?](#why-maestro)\n- [Getting Started](#getting-started)\n- [Resources & Community](#resources--community)\n- [Contributing](#contributing)\n- [Maestro Studio â€“ Test IDE](#maestro-studio--test-ide)\n- [Maestro Cloud â€“ Parallel Execution & Scalability](#maestro-cloud--parallel-execution--scalability)\n\n\n---\n\n## Why Maestro?\n\nMaestro is built on learnings from its predecessors (Appium, Espresso, UIAutomator, XCTest, Selenium, Playwright) and allows you to easily define and test your Flows.\n\nBy combining a human-readable YAML syntax with an interpreted execution engine, it lets you write, run, and scale cross-platform end-to-end tests for mobile and web with ease.\n\n- **Cross-platform coverage** â€“ test Android, iOS, and web apps (React Native, Flutter, hybrid) on emulators, simulators, or real devices.  \n- **Human-readable YAML flows** â€“ express interactions as commands like `launchApp`, `tapOn`, and `assertVisible`.  \n- **Resilience & smart waiting** â€“ built-in flakiness tolerance and automatic waiting handle dynamic UIs without manual `sleep()` calls.  \n- **Fast iteration & simple install** â€“ flows are interpreted (no compilation) and installation is a single script.\n\n**Simple Example:**\n```\n# flow_contacts_android.yaml\n\nappId: com.android.contacts\n---\n- launchApp\n- tapOn: \"Create new contact\"\n- tapOn: \"First Name\"\n- inputText: \"John\"\n- tapOn: \"Last Name\"\n- inputText: \"Snow\"\n- tapOn: \"Save\"\n```\n\n---\n## Getting Started\n\nMaestro requires Java 17 or higher to be installed on your system. You can verify your Java version by running:\n\n```\njava -version\n```\n\nInstalling the CLI:\n\nRun the following command to install Maestro on macOS, Linux or Windows (WSL):\n\n```\ncurl -fsSL \"https://get.maestro.mobile.dev\" | bash\n```\n\nThe links below will guide you through the next steps.\n\n- [Installing Maestro](https://docs.maestro.dev/getting-started/installing-maestro) (includes regular Windows installation)\n- [Build and install your app](https://docs.maestro.dev/getting-started/build-and-install-your-app)\n- [Run a sample flow](https://docs.maestro.dev/getting-started/run-a-sample-flow)\n- [Writing your first flow](https://docs.maestro.dev/getting-started/writing-your-first-flow)\n\n\n---\n\n## Resources & Community\n\n- ğŸ’¬ [Join the Slack Community](https://maestrodev.typeform.com/to/FelIEe8A)\n- ğŸ“˜ [Documentation](https://docs.maestro.dev)  \n- ğŸ“° [Blog](https://maestro.dev/blog?utm_source=github-readme) \n- ğŸ¦ [Follow us on X](https://twitter.com/maestro__dev)\n\n---\n\n## Contributing\n\nMaestro is open-source under the Apache 2.0 license â€” contributions are welcome!\n\n- Check [good first issues](https://github.com/mobile-dev-inc/maestro/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n- Read the [Contribution Guide](https://github.com/mobile-dev-inc/Maestro/blob/main/CONTRIBUTING.md) \n- Fork, create a branch, and open a Pull Request.\n\nIf you find Maestro useful, â­ star the repository to support the project.\n\n---\n\n## Maestro Studio â€“ Test IDE\n\n**Maestro Studio Desktop** is a lightweight IDE that lets you design and execute tests visually â€” no terminal needed. \nIt is also free, even though Studio is not an open-source project. So you won't find the Maestro Studio code here.\n\n- **Simple setup** â€“ just download the native app for macOS, Windows, or Linux.  \n- **Visual flow builder & inspector** â€“ record interactions, inspect elements, and build flows visually.  \n- **AI assistance** â€“ use MaestroGPT to generate commands and answer questions while authoring tests.\n\n[Download Maestro Studio](https://maestro.dev/?utm_source=github-readme#maestro-studio)\n\n---\n\n## Maestro Cloud â€“ Parallel Execution & Scalability\n\nWhen your test suite grows, run hundreds of tests in parallel on dedicated infrastructure, cutting execution times by up to 90%. Includes built-in notifications, deterministic environments, and complete debugging tools.\n\nPricing for Maestro Cloud is completely transparent and can be found on the [pricing page](https://maestro.dev/pricing?utm_source=github-readme).\n\nğŸ‘‰ [Start your free 7-day trial](https://maestro.dev/cloud?utm_source=github-readme)\n\n\n\n```\n  Built with â¤ï¸ by Maestro.dev\n```\n\n\n",
      "stars_today": 22
    },
    {
      "id": 884278258,
      "name": "InstallerX-Revived",
      "full_name": "wxxsfxyzm/InstallerX-Revived",
      "description": "More Expressive InstallerX !",
      "html_url": "https://github.com/wxxsfxyzm/InstallerX-Revived",
      "stars": 3368,
      "forks": 108,
      "language": "Kotlin",
      "topics": [
        "android",
        "apk",
        "apks",
        "dhizuku",
        "installer",
        "miuix",
        "root",
        "shizuku"
      ],
      "created_at": "2024-11-06T13:19:18Z",
      "updated_at": "2026-01-28T00:14:38Z",
      "pushed_at": "2026-01-27T13:18:29Z",
      "open_issues": 15,
      "owner": {
        "login": "wxxsfxyzm",
        "avatar_url": "https://avatars.githubusercontent.com/u/65166044?v=4"
      },
      "readme": "# InstallerX Revived (Community Edition)\n\n**English** | [ç®€ä½“ä¸­æ–‡](README_CN.md) | [EspaÃ±ol](README_ES.md) | [æ—¥æœ¬èª](README_JA.md) | [Deutsch](README_DE.md)\n\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)[![Latest Release](https://img.shields.io/github/v/release/wxxsfxyzm/InstallerX?label=Stable)](https://github.com/wxxsfxyzm/InstallerX/releases/latest)[![Prerelease](https://img.shields.io/github/v/release/wxxsfxyzm/InstallerX?include_prereleases&label=Beta)](https://github.com/wxxsfxyzm/InstallerX/releases)[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?logo=telegram&logoColor=white)](https://t.me/installerx_revived)\n\n- This is a community-maintained fork after the [original project](https://github.com/iamr0s/InstallerX) was archived by the author\n- Provides limited open-source updates and support\n- Strictly follows GNU GPLv3 - all modifications are open source\n- We welcome community contributions!\n\n## Introduction\n\n> A modern and functional Android app installer. (You know some birds are not meant to be caged, their feathers are just too bright.)\n\nLooking for a better app installer? Try **InstallerX**!\n\nMany customized Chinese ROMs come with subpar default installers. You can replace them with **InstallerX Revived**.\n\nCompared to stock installers, **InstallerX Revived** offers more installation features:\n- Rich installation types: APK, APKS, APKM, XAPK, APKs inside ZIP, and batch APKs.\n- Dialog-based installation\n- Notification-based installation (Live Activity API supported)\n- Automatic installation\n- Installer declaration\n- Setting install flags (can inherit Profile settings)\n- Install for specific user / all users\n- Dex2oat after successful installation\n- Block the installation of specific app's packageName or by sharedUID\n- Auto-delete APK after installation\n- No shell commands, native API calls only\n\n## Supported Versions\n\n- **Full support:** Android SDK 34 - 36.1 (Android 14 - 16)\n- **Limited support:** Android SDK 26 - 33 (Android 8.0 - 13) (please report issues)\n\n## Key Changes and Features\n\n- **UI Options:** Switchable between a new UI design based on Material 3 Expressive and Miuix which is like HyperOS.\n- **More Customization:** More customizable interface settings.\n- **Bug fixes:** Resolved APK deletion issues from the original project on certain systems.\n- **Performance:** Optimized parsing speed, improved parsing of various package types.\n- **Multilingual support:** More languages supported. Contributions for more languages are welcome!\n- **Dialog optimization:** Improved installation dialog display.\n- **System Icons:** Support for displaying system icon packs during installation. Allows switching between APK icons and system icon packs through a toggle.\n- **Version Comparison:** Support for displaying version number comparison in single-line or multi-line format.\n- **SDK Information:** Installation dialogs show targetSDK and minSDK in single-line or multi-line format.\n- **Session Install Confirmation**: With the help of [InxLocker](https://github.com/Chimioo/InxLocker), confirming installations from store apps (Aurora Store, F-Droid, etc.) is now supported.\n- **Bypass Interceptions:** Shizuku/Root can bypass custom OS chain-start restrictions when opening an App after installation.\n    - Currently only works for dialog installation.\n    - Dhizuku lacks sufficient permissions, so a customizable countdown option was added to reserve time for the app opening action.\n- **Extended Menu:** For dialog installation (can be enabled in settings):\n    - Displays permissions requested by the application.\n    - InstallFlags configuration (can inherit global Profile settings).\n      - **Important:** Setting InstallFlags **does not guarantee** they will always work. Some options might pose security risks, depending on the system.\n- **Preset Sources:** Support for pre-configuring installation source package names in settings, allowing quick selection in profiles and the dialog installation menu.\n- **Install from ZIP:** Support for installing APK files inside ZIP archives (dialog installation only).\n    - Supports unlimited quantity and multiple ZIP files.\n    - Supports APK files in nested directories within the ZIP, **not limited to the root directory**.\n    - Supports automatic handling of multiple versions of the same package:\n        - Deduplication\n        - Smart selection of the best package to install.\n- **Batch Installation:** Support for installing multiple APKs at once (multi-select and share to InstallerX).\n    - Dialog installation only.\n    - No quantity limit.\n    - APK files only.\n    - Supports automatic handling of multiple versions of the same package (deduplication and smart selection).\n- **APKS/APKM/XAPK Files:** Support for automatic selection of the best split.\n    - Supports both notification and dialog installation.\n        - Clicking \"Install\" in the notification selects the best option and proceeds with installation.\n        - In the dialog, the best option is selected by default, but can be chosen manually.\n    - The split selection interface shows user-friendly descriptions.\n- **Architecture Support:** Allows installing armeabi-v7a packages on arm64-v8a only systems (actual functionality depends on the system providing runtime translation).\n- **Downgrade with or without Data:** Support for performing app downgrades with or without data preservation on some OEM Android 15 systems.\n    - This feature only supports Android 15. On Android 14 or below, try the `Allow downgrade` option in the install options.\n    - The feature is available in the smart suggestions of the dialog installation. To use it, first enable the `Show smart suggestions` option.\n    - **Use this feature with extreme caution on system apps!** Loss of data from a system app could render the device unusable.\n    - Not compatible with OneUI 7.0, RealmeUI, and some ColorOS versions (AOSP has fixed). If you only see the downgrade option *without* data preservation, it means your system does not support downgrade *with* data.\n- **Blacklist:** Support for configuring a list of banned package names for installation in the settings.\n    - Support blacklist by packageName / sharedUID with exemptions\n    - `Allow once` in smart suggestions\n- **DexOpt:** After successful installation, the app can automatically perform dex2oat on the installed applications according to the configured Profile settings.\n    - Does not support Dhizuku\n- **Signature Verificationï¼š** Verify the signature of the installed app and apk to install, and give a warning if they do not match.\n- **Select Target User:** Support installing apps to a specific user.\n    - Dynamically obtain current user details.\n    - Does not support Dhizuku\n    - Can be overridden by `Install For All Users` install option\n- **Declare as Uninstaller:** Accept Uninstall intent on certain OS, custom OS may not be supported.\n- [Experimental] **Directly Install From Download Link:** The online version supports directly sharing the download link of an APK file to InstallerX for installation. Currently, the APK is not kept locally, but an option to retain the installation package will be added in the future.\n\n## FAQ\n\n> [!NOTE]\n> Please read the FAQ before providing feedback.\n> When providing feedback, please specify your phone brand, system version, software version, and operation in detail.\n\n- **Dhizuku not working properly?**\n    - Support for **official Dhizuku** is limited. Tested on AVDs with SDK â‰¥34. Operation on SDK <34 is not guaranteed.\n    - When using `OwnDroid`, the `Auto delete after installation` function might not work correctly.\n    - On Chinese ROMs, occasional errors are usually due to the system restricting Dhizuku's background operation. It is recommended to restart the Dhizuku app first.\n    - Dhizuku has limited permissions. Many operations are not possible (like bypassing system intent interceptors or specifying the installation source). Using Shizuku is recommended if possible.\n\n- **Unable to lock InstallerX as default installer?**\n    - Some Systems have very strict policy on Package Installers. You must use a LSPosed module to intercept the intent and forward it to the installer in this case.\n    - Works best with: [Chimioo/InxLocker](https://github.com/Chimioo/InxLocker)\n    - Other lockers working as LSPosed are no longer recommended\n\n- An error occurred in the resolution phase: `No Content Provider` or `reading provider` reported `Permission Denial`?\n    - You have enabled Hide app list or similar functions, please configure the whitelist.\n\n- **HyperOS shows \"Installing system apps requires declaring a valid installer\" error**\n    - It's a system security restriction. You must declare an installer that is a system app (recommended: `com.android.fileexplorer` or `com.android.vending` for HyperOS; app store for Vivo).\n    - Works with Shizuku/Root. **Dhizuku is not supported**.\n    - New feature: InstallerX automatically detects HyperOS and adds a default configuration (`com.miui.packageinstaller`). You can change it in the settings if needed.\n\n- **HyperOS reinstalls the default installer / locking fails**\n    - Try enabling `Auto Lock Installer` in settings.\n    - On some HyperOS versions, locking failure is expected.\n    - HyperOS intercepts USB installation requests (ADB/Shizuku) with a dialog. If the user rejects the installation of a new app, the system will revoke the installer setting and force the default one. If this happens, lock InstallerX again.\n\n- **Notification progress bar freezes**\n    - Some custom OS has very strict background app controls. Set \"No background restrictions\" for the app if you encounter this.\n    - The app is optimized: it ends all background services and closes 1 second after completing the installation task (when the user clicks \"Done\" or clears the notification). You can enable the foreground service notification to monitor.\n\n- **Problems on Oppo/Vivo/Lenovo/... systems?**\n    - We do not have devices from these brands for testing. You can discuss it in [Discussions](https://github.com/wxxsfxyzm/InstallerX-Revived/discussions), or report through our [Telegram Channel](https://t.me/installerx_revived).\n    - To lock the installer on Oppo/Vivo, use the lock tool.\n    - To install apps through shizuku on Honor devices, disable `Monitor ADB install` in developer settings.\n\n## About Releases\n\n> [!WARNING]\n> Development versions may be unstable and features may change/be removed without any notice.\n> Switching build channels may require data wipe/reinstallation.\n\n- **`dev` branch:** Contains features under development. If you want to test them, look for the corresponding CI builds in Github Actions.\n- **`main` branch:** When stable changes are merged from `dev`, the CI/CD system automatically builds and publishes a new alpha version.\n- **Stable releases:** Manually published when finishing a development/testing phase. CI/CD automatically publishes them as a release.\n- **About network permission:** As features have expanded, some network-related functions have been introduced. However, many users prefer the installer to remain purely local without requiring network access. Therefore, two versions will be released: **online** and **offline**. Both versions share the same package name, version code, and signature, so they can't be installed side by side (but can be replaced directly). Please download according to your needs.\n  - **Online version**: Supports sharing direct download links to InstallerX for installation. More network-related utilities may be added in the future, but network permission will **never** be used for non-installation purposes. Safe to use.\n  - **Offline version**: Requests no network permissions at all. When attempting to use online features, you will receive a clear error message. This version remains a purely local installer.\n\n## About Localization\n\nHelp us translate this project! You can contribute at: https://hosted.weblate.org/engage/installerx-revived/\n\n### Localization Status\n\n[![Localization Status](https://hosted.weblate.org/widget/installerx-revived/strings/multi-auto.svg)](https://hosted.weblate.org/engage/installerx-revived/)\n\n## License\n\nCopyright Â© [iamr0s](https://github.com/iamr0s) and [contributors](https://github.com/wxxsfxyzm/InstallerX-Revived/graphs/contributors)\n\nInstallerX is currently released under [**GNU General Public License v3 (GPL-3)**](http://www.gnu.org/licenses/gpl-3.0), though this commitment may change in the future. Maintainers reserve the right to modify license terms or the open-source status of the project.\n\nIf you base your development on InstallerX, you must comply with the terms of the open-source license of the specific version of the source code you use as a base, regardless of future changes made to the main project.\n\n## Acknowledgements\n\nThis project uses code from, or is based on the implementation of, the following projects:\n\n- [iamr0s/InstallerX](https://github.com/iamr0s/InstallerX)\n- [tiann/KernelSU](https://github.com/tiann/KernelSU)\n- [RikkaApps/Shizuku](https://github.com/RikkaApps/Shizuku)\n- [zacharee/InstallWithOptions](https://github.com/zacharee/InstallWithOptions)\n- [vvb2060/PackageInstaller](https://github.com/vvb2060/PackageInstaller)\n- [compose-miuix-ui/miuix](https://github.com/compose-miuix-ui/miuix)\n",
      "stars_today": 21
    },
    {
      "id": 1062297179,
      "name": "agentscope-java",
      "full_name": "agentscope-ai/agentscope-java",
      "description": "AgentScope Java: Agent-Oriented Programming for Building LLM Applications",
      "html_url": "https://github.com/agentscope-ai/agentscope-java",
      "stars": 1122,
      "forks": 242,
      "language": "Java",
      "topics": [
        "agent",
        "agentic",
        "agentic-ai",
        "ai",
        "llm"
      ],
      "created_at": "2025-09-23T04:37:43Z",
      "updated_at": "2026-01-28T00:41:41Z",
      "pushed_at": "2026-01-27T02:09:48Z",
      "open_issues": 124,
      "owner": {
        "login": "agentscope-ai",
        "avatar_url": "https://avatars.githubusercontent.com/u/211762292?v=4"
      },
      "readme": "<p align=\"center\">\n  <img\n    src=\"https://img.alicdn.com/imgextra/i1/O1CN01nTg6w21NqT5qFKH1u_!!6000000001621-55-tps-550-550.svg\"\n    alt=\"AgentScope Logo\"\n    width=\"200\"\n  />\n</p>\n\n<h3 align=\"center\">Build Production-Ready AI Agents in Java</h3>\n\n<p align=\"center\">\n  <a href=\"https://java.agentscope.io/\">ğŸ“– Documentation</a>\n  &nbsp;|&nbsp;\n  <a href=\"README_zh.md\">ä¸­æ–‡</a>\n  &nbsp;|&nbsp;\n  <a href=\"https://discord.gg/eYMpfnkG8h\">Discord</a>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/license-Apache--2.0-blue\" alt=\"License\" />\n  <img src=\"https://img.shields.io/badge/JDK-17%2B-orange\" alt=\"JDK 17+\" />\n  <img src=\"https://img.shields.io/maven-central/v/io.agentscope/agentscope?color=green\" alt=\"Maven Central\" />\n  <a href=\"https://deepwiki.com/agentscope-ai/agentscope-java\"><img src=\"https://deepwiki.com/badge.svg\" alt=\"Ask DeepWiki\"></a>\n</p>\n\n---\n\nAgentScope Java is an agent-oriented programming framework for building LLM-powered applications. It provides everything you need to create intelligent agents: ReAct reasoning, tool calling, memory management, multi-agent collaboration, and more.\n\n## Highlights\n\n### ğŸ¯ Smart Agents, Full Control\n\nAgentScope adopts the ReAct (Reasoning-Acting) paradigm, enabling agents to autonomously plan and execute complex tasks. Unlike rigid workflow-based approaches, ReAct agents dynamically decide which tools to use and when, adapting to changing requirements in real-time.\n\nHowever, autonomy without control is a liability in production. AgentScope provides comprehensive runtime intervention mechanisms:\n\n- **Safe Interruption** - Pause agent execution at any point while preserving full context and tool state, enabling seamless resumption without data loss\n- **Graceful Cancellation** - Terminate long-running or unresponsive tool calls without corrupting agent state, allowing immediate recovery and redirection\n- **Human-in-the-Loop** - Inject corrections, additional context, or guidance at any reasoning step through the Hook system, maintaining human oversight over critical decisions\n\n### ğŸ› ï¸ Built-in Tools\n\nAgentScope includes production-ready tools that address common challenges in agent development:\n\n- **PlanNotebook** - A structured task management system that decomposes complex objectives into ordered, trackable steps. Agents can create, modify, pause, and resume multiple concurrent plans, ensuring systematic execution of multi-step workflows.\n\n- **Structured Output** - A self-correcting output parser that guarantees type-safe responses. When LLM output deviates from the expected format, the system automatically detects errors and guides the model to produce valid output, mapping results directly to Java POJOs without manual parsing.\n\n- **Long-term Memory** - Persistent memory storage with semantic search capabilities across sessions. Supports automatic management, agent-controlled recording, or hybrid modes. Enables multi-tenant isolation for enterprise deployments where agents serve multiple users independently.\n\n- **RAG (Retrieval-Augmented Generation)** - Seamless integration with enterprise knowledge bases. Supports both self-hosted embedding-based retrieval and managed services like Alibaba Cloud Bailian, grounding agent responses in authoritative data sources.\n\n### ğŸ”Œ Seamless Integration\n\nAgentScope is designed to integrate with existing enterprise infrastructure without requiring extensive modifications:\n\n- **MCP Protocol** - Integrate with any MCP-compatible server to instantly extend agent capabilities. Connect to the growing ecosystem of MCP tools and servicesâ€”from file systems and databases to web browsers and code interpretersâ€”without writing custom integration code.\n\n- **A2A Protocol** - Enable distributed multi-agent collaboration through standard service discovery. Register agent capabilities to Nacos or similar registries, allowing agents to discover and invoke each other as naturally as calling microservices.\n\n### ğŸš€ Production Grade\n\nBuilt for enterprise deployment requirements:\n\n- **High Performance** - Reactive architecture based on Project Reactor ensures non-blocking execution. GraalVM native image compilation achieves 200ms cold start times, making AgentScope suitable for serverless and auto-scaling environments.\n\n- **Security Sandbox** - AgentScope Runtime provides isolated execution environments for untrusted tool code. Includes pre-built sandboxes for GUI automation, file system operations, and mobile device interaction, preventing unauthorized access to system resources.\n\n- **Observability** - Native integration with OpenTelemetry for distributed tracing across the entire agent execution pipeline. AgentScope Studio provides visual debugging, real-time monitoring, and comprehensive logging for development and production environments.\n\n## Quick Start\n\n**Requirements:** JDK 17+\n\n```xml\n<dependency>\n    <groupId>io.agentscope</groupId>\n    <artifactId>agentscope</artifactId>\n    <version>1.0.8</version>\n</dependency>\n```\n\n```java\nReActAgent agent = ReActAgent.builder()\n    .name(\"Assistant\")\n    .sysPrompt(\"You are a helpful AI assistant.\")\n    .model(DashScopeChatModel.builder()\n        .apiKey(System.getenv(\"DASHSCOPE_API_KEY\"))\n        .modelName(\"qwen-max\")\n        .build())\n    .build();\n\nMsg response = agent.call(Msg.builder()\n        .textContent(\"Hello!\")\n        .build()).block();\nSystem.out.println(response.getTextContent());\n```\n\nFor more examples, see the [documentation](https://java.agentscope.io/).\n\n## Contributing\n\nWe welcome contributions! Please see [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.\n\n## Community\n\n| [Discord](https://discord.gg/eYMpfnkG8h)                     | DingTalk | WeChat |\n|--------------------------------------------------------------|----------| ---------|\n| <img src=\"./docs/imgs/discord.png\" width=\"100\" height=\"100\"> | <img src=\"./docs/imgs/dingtalk_qr_code.jpg\" width=\"100\" height=\"100\"> | <img src=\"./docs/imgs/wechat.png\" width=\"100\" height=\"100\"> |\n\n## License\n\nApache License 2.0 - see [LICENSE](./LICENSE) for details.\n\n## Publications\n\nIf you find AgentScope helpful, please cite our papers:\n\n- [AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications](https://arxiv.org/abs/2508.16279)\n- [AgentScope: A Flexible yet Robust Multi-Agent Platform](https://arxiv.org/abs/2402.14034)\n\n## Contributors\n\n<a href=\"https://github.com/agentscope-ai/agentscope-java/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=agentscope-ai/agentscope-java&max=999&columns=12&anon=1\" />\n</a>\n",
      "stars_today": 21
    },
    {
      "id": 45821540,
      "name": "openwrt",
      "full_name": "openwrt/openwrt",
      "description": "This repository is a mirror of https://git.openwrt.org/openwrt/openwrt.git It is for reference only and is not active for check-ins.  We will continue to accept Pull Requests here. They will be merged via staging trees then into openwrt.git.",
      "html_url": "https://github.com/openwrt/openwrt",
      "stars": 25327,
      "forks": 11988,
      "language": "C",
      "topics": [],
      "created_at": "2015-11-09T07:13:55Z",
      "updated_at": "2026-01-28T02:00:46Z",
      "pushed_at": "2026-01-27T23:53:10Z",
      "open_issues": 3798,
      "owner": {
        "login": "openwrt",
        "avatar_url": "https://avatars.githubusercontent.com/u/2528830?v=4"
      },
      "readme": "![OpenWrt logo](include/logo.png)\n\nOpenWrt Project is a Linux operating system targeting embedded devices. Instead\nof trying to create a single, static firmware, OpenWrt provides a fully\nwritable filesystem with package management. This frees you from the\napplication selection and configuration provided by the vendor and allows you\nto customize the device through the use of packages to suit any application.\nFor developers, OpenWrt is the framework to build an application without having\nto build a complete firmware around it; for users this means the ability for\nfull customization, to use the device in ways never envisioned.\n\nSunshine!\n\n## Download\n\nBuilt firmware images are available for many architectures and come with a\npackage selection to be used as WiFi home router. To quickly find a factory\nimage usable to migrate from a vendor stock firmware to OpenWrt, try the\n*Firmware Selector*.\n\n* [OpenWrt Firmware Selector](https://firmware-selector.openwrt.org/)\n\nIf your device is supported, please follow the **Info** link to see install\ninstructions or consult the support resources listed below.\n\n## \n\nAn advanced user may require additional or specific package. (Toolchain, SDK, ...) For everything else than simple firmware download, try the wiki download page:\n\n* [OpenWrt Wiki Download](https://openwrt.org/downloads)\n\n## Development\n\nTo build your own firmware you need a GNU/Linux, BSD or macOS system (case\nsensitive filesystem required). Cygwin is unsupported because of the lack of a\ncase sensitive file system.\n\n### Requirements\n\nYou need the following tools to compile OpenWrt, the package names vary between\ndistributions. A complete list with distribution specific packages is found in\nthe [Build System Setup](https://openwrt.org/docs/guide-developer/build-system/install-buildsystem)\ndocumentation.\n\n```\nbinutils bzip2 diff find flex gawk gcc-6+ getopt grep install libc-dev libz-dev\nmake4.1+ perl python3.7+ rsync subversion unzip which\n```\n\n### Quickstart\n\n1. Run `./scripts/feeds update -a` to obtain all the latest package definitions\n   defined in feeds.conf / feeds.conf.default\n\n2. Run `./scripts/feeds install -a` to install symlinks for all obtained\n   packages into package/feeds/\n\n3. Run `make menuconfig` to select your preferred configuration for the\n   toolchain, target system & firmware packages.\n\n4. Run `make` to build your firmware. This will download all sources, build the\n   cross-compile toolchain and then cross-compile the GNU/Linux kernel & all chosen\n   applications for your target system.\n\n### Related Repositories\n\nThe main repository uses multiple sub-repositories to manage packages of\ndifferent categories. All packages are installed via the OpenWrt package\nmanager called `opkg`. If you're looking to develop the web interface or port\npackages to OpenWrt, please find the fitting repository below.\n\n* [LuCI Web Interface](https://github.com/openwrt/luci): Modern and modular\n  interface to control the device via a web browser.\n\n* [OpenWrt Packages](https://github.com/openwrt/packages): Community repository\n  of ported packages.\n\n* [OpenWrt Routing](https://github.com/openwrt/routing): Packages specifically\n  focused on (mesh) routing.\n\n* [OpenWrt Video](https://github.com/openwrt/video): Packages specifically\n  focused on display servers and clients (Xorg and Wayland).\n\n## Support Information\n\nFor a list of supported devices see the [OpenWrt Hardware Database](https://openwrt.org/supported_devices)\n\n### Documentation\n\n* [Quick Start Guide](https://openwrt.org/docs/guide-quick-start/start)\n* [User Guide](https://openwrt.org/docs/guide-user/start)\n* [Developer Documentation](https://openwrt.org/docs/guide-developer/start)\n* [Technical Reference](https://openwrt.org/docs/techref/start)\n\n### Support Community\n\n* [Forum](https://forum.openwrt.org): For usage, projects, discussions and hardware advise.\n* [Support Chat](https://webchat.oftc.net/#openwrt): Channel `#openwrt` on **oftc.net**.\n\n### Developer Community\n\n* [Bug Reports](https://bugs.openwrt.org): Report bugs in OpenWrt\n* [Dev Mailing List](https://lists.openwrt.org/mailman/listinfo/openwrt-devel): Send patches\n* [Dev Chat](https://webchat.oftc.net/#openwrt-devel): Channel `#openwrt-devel` on **oftc.net**.\n\n## License\n\nOpenWrt is licensed under GPL-2.0\n",
      "stars_today": 20
    },
    {
      "id": 7270538,
      "name": "anki",
      "full_name": "ankitects/anki",
      "description": "Anki is a smart spaced repetition flashcard program",
      "html_url": "https://github.com/ankitects/anki",
      "stars": 25949,
      "forks": 2703,
      "language": "Rust",
      "topics": [],
      "created_at": "2012-12-21T08:03:31Z",
      "updated_at": "2026-01-27T23:16:30Z",
      "pushed_at": "2026-01-25T13:15:30Z",
      "open_issues": 330,
      "owner": {
        "login": "ankitects",
        "avatar_url": "https://avatars.githubusercontent.com/u/42564322?v=4"
      },
      "readme": "# AnkiÂ®\n\n[![Build status](https://badge.buildkite.com/c9edf020a4aec976f9835e54751cc5409d843adbb66d043bd3.svg?branch=main)](https://buildkite.com/ankitects/anki-ci)\n\nThis repo contains the source code for the computer version of\n[Anki](https://apps.ankiweb.net).\n\n# About\n\nAnki is a spaced repetition program. Please see the [website](https://apps.ankiweb.net) to learn more.\n\n# Getting Started\n\n### Anki Betas\n\nIf you'd like to try development builds of Anki but don't feel comfortable\nbuilding the code, please see [Anki betas](https://betas.ankiweb.net/)\n\n### Developing\n\nFor more information on building and developing, please see [Development](./docs/development.md).\n\n### Contributing\n\nWant to contribute to Anki? Check out the [Contribution Guidelines](./docs/contributing.md).\n\n### Anki Contributors\n\n[CONTRIBUTORS](./CONTRIBUTORS)\n\n# License\n\nAnki's license: [LICENSE](./LICENSE)\n",
      "stars_today": 20
    },
    {
      "id": 283211901,
      "name": "SmartTube",
      "full_name": "yuliskov/SmartTube",
      "description": "Browse media content with your own rules on Android TV",
      "html_url": "https://github.com/yuliskov/SmartTube",
      "stars": 27794,
      "forks": 1491,
      "language": "Java",
      "topics": [
        "android",
        "android-tv",
        "android-tv-box",
        "java",
        "kotlin",
        "retrofit2",
        "rxjava-android"
      ],
      "created_at": "2020-07-28T12:53:52Z",
      "updated_at": "2026-01-28T01:32:01Z",
      "pushed_at": "2026-01-27T23:01:01Z",
      "open_issues": 2989,
      "owner": {
        "login": "yuliskov",
        "avatar_url": "https://avatars.githubusercontent.com/u/5897464?v=4"
      },
      "readme": "# Important announcement about the app\r\n\r\nMy development environment was infected by unknown malicious software, as a result of which a few builds may have been affected. Once the issue was detected, I secured everything with a full disk wipe, restored a clean setup, and now all builds are scanned with VirusTotal. The F-Droid version will also be verified before release.\r\n\r\nPublic keys may have been compromised, which is why I am sharing this issue. You can download the new version and the new public key below, and instructions for restoring backups are provided.\r\n\r\nNo extra actions are required since the app uses **one-time connection codes**. These codes have very limited permissions (for example, they cannot change your password). Still, you can revoke them if you want full peace of mind.\r\n\r\n# How to revoke access:\r\n\r\n1. Open [myaccount.google.com/security](https://myaccount.google.com/security)\r\n2. Find **â€œYour connections to third-party apps & servicesâ€**\r\n3. Tap **â€œSee all connectionâ€** and locate **YouTube TV** or **Google Drive**\r\n4. Select the app â†’ **â€œRemove accessâ€**\r\n\r\nPlease keep built-in security features enabled to stay protected.\r\n\r\n# SmartTube\r\n  \r\n<!-- <img width=\"100\" src=\"https://github.com/yuliskov/SmartTube/blob/master/smarttubetv/src/ststable/res/mipmap-nodpi/app_icon.png\" alt=\"logo\"/> -->\r\n\r\n![The app screenshot](./images/browse_home.png)\r\n\r\nSmartTube is a free and open-source advanced media player for Android TVs and TV boxes. It allows you to play content from various public sources.\r\n\r\n### âœ… Features\r\n- No ads  \r\n- SponsorBlock integration  \r\n- Adjustable playback speed  \r\n- 8K resolution support  \r\n- 60fps playback  \r\n- HDR compatibility  \r\n- View live chat  \r\n- Customizable buttons  \r\n- Does not require Google Services  \r\n- Helpful international community\r\n\r\n### âŒ Limitations\r\n- Not supported on phones and tablets  \r\n- Comment functionality is unstable  \r\n- Voice search and casting performance may be inferior to official apps, depending on your device  \r\n\r\nGive it a try!\r\n\r\n**Do you have any question?** Ctrl+F or âŒ˜F this readme first!\r\n\r\n[**Installation**](#installation) | [**Official Site**](https://smarttubeapp.github.io) | [**Donation**](#donation) | [**FAQ**](#faq) | [Support / Chat](#support) | [Build](#build) | [Translate the app](https://jtbrinkmann.de/tools/android-strings.xml-translator.html) | [Changelog](https://t.me/s/SmartTubeNewsEN) | [Liability](#liability)\r\n\r\n\r\n## Device support\r\n\r\n![Device support image](images/new/upload_f6ab10668ea7793f099108f2b2230406.png)\r\n* **Supported:** all Android TVs and TV boxes (incl. FireTV, NVIDIA Shield & Google Chromecast with Google TV), even older ones with Android 4.3 (Kitkat).\r\n* **Not supported:** Smartphones, non-Android platforms like Samsung Tizen, LG webOS, iOS, etc.\r\n\r\n\r\n## Installation\r\n\r\n> [video of the installation](images/new/zPV0imF.mp4) (note: download url changed to `kutt.it/stn_beta` or `kutt.it/stn_stable`)\r\n\r\n**Do not** download SmartTube from any **app store**, APK websites or blogs; these were uploaded by other people and may contain malware or ads. SmartTube is not officially published on any app store. Sadly, the Google PlayStore does not allow ad-free Youtube apps using unofficial APIs.\r\n\r\nThere is a **beta release** (recommended) and a **stable release**. Beta gets new features and bugfixes faster than the stable release.\r\n\r\nYou can use either of the following methods to install the app:\r\n\r\n- (**Easiest**) Install [Downloader by AFTVnews](https://www.aftvnews.com/downloader/) on your Android TV, open it and enter `kutt.it/stn_beta` or `kutt.it/stn_stable`, then read, understand and confirm the security prompts. (<small>You can also enter [**79015**](https://aftv.news/79015) (for beta) or [**28544**](https://aftv.news/28544) (for stable), but this requires an extra step to install the AFTVnews Downloader browser addon if you haven't already.</small>)\r\n- Install a file transfer app on your Android TV, download the APK on your phone or computer and transfer it to your TV (e.g. [_Send Files to TV_](https://sendfilestotv.app/) from the Google Play Store / Amazon AppStore)\r\n- Download the APK onto a USB stick, put the USB stick into your TV and use a file manager app from the Google Play Store / Amazon AppStore (e.g. [_FX File Explorer_](https://play.google.com/store/apps/details?id=nextapp.fx) or [_X-plore_](https://play.google.com/store/apps/details?id=com.lonelycatgames.Xplore)). Android's preinstalled file manager does not work! Do **not** get the ad-infested _FileCommander_.\r\n- If you are an advanced user, you can install it using ADB. [guide](https://fossbytes.com/side-load-apps-android-tv/#h-how-to-sideload-apps-on-your-android-tv-using-adb) | [alternative guide](https://www.aftvnews.com/sideload/)\r\n\r\n**Troubleshooting:** See device specific notes below. If installation fails, either your **disk space is full** or the APK file didn't download correctly; clear up space and try downloading again. If the app installed, but crashes when opening, make sure to install it to internal memory, not to an SD card / external storage.\r\n\r\n**The app has a built-in updater** with changelog. You can also find all releases and the **changelog** on the [Telegram channel @SmartTubeNewsEN](https://t.me/s/SmartTubeNewsEN) (readable without account) or on [Github](https://github.com/yuliskov/SmartTube/releases/).\r\n\r\n> latest [**beta download**](https://github.com/yuliskov/SmartTube/releases/download/latest/smarttube_beta.apk)\r\n>\r\n> latest [stable download](https://github.com/yuliskov/SmartTube/releases/download/latest/smarttube_stable.apk)\r\n\r\n\r\n### Installation (Chromecast with Google TV)\r\n\r\nOn **Chromecast with Google TV**, installation of apps is blocked by default, so an extra step is required:\r\n\r\n> **4.1. Enable Developer Options**\r\n>\r\n> On your Chromecast, open the side menu and go to _Settings > System > About_. Scroll down to the _Android TV OS build_ section and click that repeatedly. A toast message will appear, explaining that you are a few steps away from being a developer. Continue clicking until you trigger it.\r\n>\r\n>\r\n> **4.2. Turn on the \"unknown sources\" setting**\r\n>\r\n> Go back to the main _Settings_ page and select _Apps > Security & Restrictions > Unknown sources_. Turn on the toggle for \\[_Downloader by AFTVnews_ or\\] whichever file browser you decided to use [...].\r\n>\r\n> [[source & picture guide](https://www.androidpolice.com/2021/02/07/how-to-sideload-any-apk-on-the-chromecast-with-android-tv/#install-the-apk)]\r\n\r\nAfter this, you can follow the [general installation guide](#installation) above.\r\n\r\n\r\n### Installation (Xiaomi devices with Chinese firmware)\r\n\r\nXiaomi's **Chinese firmware** might block the installation **of the beta version**. The international firmware is not affected. Solutions:\r\n1. Use SmartTube's **stable version** instead (**recommended**)\r\n2. Use the international firmware for your device\r\n3. (if your device is from 2020 or before) You can do a factory reset and then install SmartTube beta before doing any system updates. You can then safely update your system, SmartTube should continue working.\r\n\r\n\r\n### Updating\r\n\r\nThe app has a built-in updater. You only need to follow the installation procedure **once**. A few seconds after launching SmartTube, it will notify you if there is any update and also show a changelog. You can disable automatic update checks or manually update in the settings under \"about\".\r\n\r\nIf the installation fails, either your **disk space is full** or the update didn't download correctly; clear up space and try updating again (_Settings > About > Check for updates_).\r\n\r\n\r\n## Compatibility\r\n\r\nSmartTube requires Android 4.3 or above. It does not work on non-Android devices (incl. LG or Samsung TVs). On unsupported TVs, you can use a TV stick or TV box. Though this app technically runs on smartphones and tablets, it is not optimized for such and offers no official support!\r\n\r\nIt has been successfully tested on TVs, TV boxes and TV sticks that are based on Android, including:\r\n\r\n- Android TVs & Google TVs (e.g. Philips, Sony)\r\n- Chromecast with Google TV & TVs with _Chromecast built-in_\r\n- Amazon FireTV stick (all generations)\r\n- NVIDIA Shield\r\n- TV boxes running Android (many cheap chinese no-name boxes)\r\n- Xiaomi Mi Box\r\n\r\n\r\n## Features\r\n\r\n### Adblocking\r\n\r\nSmartTube does not show any ad banners, preroll ads or ad intermissions. It not just tries to prevent them, it is literally programmed to be completely **unable** to display any ads, so YouTube cannot slip anything in. This also means you cannot allow ads or whitelist channels. Some YouTube channels include sponsored messages in their videos, these can also be skipped, see [SponsorBlock](#SponsorBlock) below.\r\n\r\n\r\n### SponsorBlock\r\n\r\nSmartTube includes a SponsorBlock integration. From the [SponsorBlock website](https://sponsor.ajay.app/):\r\n\r\n> SponsorBlock is an open-source crowdsourced browser extension and open API for **skipping sponsor segments** in YouTube videos. [...] the extension automatically skips sponsors **it knows about** using a privacy preserving query system. It also supports skipping **other categories**, such as intros, outros and reminders to subscribe [and non-music parts in music videos].\r\n\r\nYou can select which categories you want to skip in the settings. Unlike the browser addon, in SmartTube you cannot submit new segments (TVs and TV remotes aren't great devices for such precise operations). Note that SponsorBlock is a free and voluntary project based on user submissions, so don't expect it to 100% work every time. Sometimes, sponsor segments are not yet submitted to the database, sometimes the SponsorBlock servers are offline/overloaded.\r\n\r\n\r\n### Casting\r\n\r\nTo cast videos from your phone (or other devices), you must link that device to your TV. Unlike the original YouTube app, SmartTube does not automatically show up when you are in the same wifi network. How to link your smartphone and TV:\r\n\r\n1. open SmartTube and go to settings\r\n2. go to \"Remote control\" (2nd option)\r\n3. open your YouTube app on your phone, go to settings > General > watch on TV\r\n4. click on _connect using TV-code_ and enter the code from your TV\r\n\r\n[**Screenshot guide**](https://t.me/SmartTubeEN/8514)\r\n\r\nDue to technical limitations, you need to open the app on the TV before casting; SmartTube cannot automatically wake up the TV.\r\n\r\n\r\n### Picture-in-Picture (PiP)\r\n\r\nSmartTube supports playing videos in PiP mode. This needs to be enabled under _Settings > General > Background playback > Picture in picture_. The video will go into PiP mode when you press home while playing a video, and also when you press _back_ if enabled in _Settings > General > Background playback (activation)_.\r\n\r\n\r\n### Adjust Speed\r\n\r\nYou can adjust the playback speed pressing the speed-indicator icon (gauge) in the top row of the player. This is remembered across videos. Some speeds may case frame drops, this is a known issue.\r\n\r\n\r\n### Voice Search\r\n\r\nTo enable global voice search, an additional app must be installed alongside SmartTube. This _bridge app_ can intercept the System's attempts to open the original YouTube app and open SmartTube instead. For this to work, you must uninstall the original YouTube app. We know this sucks, but you can always reinstall it if you change your mind. The _bridge app_ will not show up in your launcher and you cannot launch it directly; it is only used internally by the system's voice search. On some devices, you need to explicitly say \"Youtube\" when searching (e.g. say \"youtube cute cats\" instead of just \"cute cats\").\r\n\r\n**On Amazon Fire TV**: \r\n\r\n1. Uninstall the original YouTube app (no root required)\r\n2. Download and install the Amazon Bridge SmartTube app: https://kutt.it/stn_bridge_amazon (e.g. via _Downloader by AFTVnews_)\r\n\r\n\r\n**On Google Chromecast with Google TV**: \r\n\r\n1. Uninstall the original YouTube app (no root required)\r\n2. Download and install the ATV Bridge SmartTube app: https://kutt.it/stn_bridge_atv (e.g. via _Downloader by AFTVnews_)\r\n\r\n\r\n**On all other Android devices**, sadly root is required to enable this:\r\n\r\n1. Root your device (search for a guide for your specific device)\r\n2. Uninstall the official YouTube app using root (`adb shell pm uninstall com.google.android.youtube.tv`)\r\n3. Download and install the ATV Bridge SmartTube app: https://kutt.it/stn_bridge_atv (e.g. via _Downloader by AFTVnews_)\r\n\r\n\r\n## Donation\r\n\r\nIf you want to support my developments you are welcome to buy me a cup of coffee :)\r\n\r\n- [**Patreon (Visa, Mastercard, PayPal)**](https://www.patreon.com/smarttube)  \r\n- **PayPal**: firsth<!-- abc@def -->ash@gmai<!-- @abc.com -->l.com  \r\n<!-- > [**Buy me a coffee**](https://www.buymeacoffee.com/stube) -->  \r\n- **BTC**: 1JAT5VVWarVBkpVbNDn8UA8HXNdrukuBSx  \r\n- **LTC**: ltc1qgc24eq9jl9cq78qnd5jpqhemkajg9vudwyd8pw  \r\n- **ETH**: 0xe455E21a085ae195a097cd4F456051A9916A5064  \r\n- **ETC**: 0x209eCd33Fa61fA92167595eB3Aea92EE1905c815  \r\n- **TRX**: TJNPY794aSGZf3WGHTna2VCWm2G5Yua7E8  \r\n- **USDT (TRC20)**: TJNPY794aSGZf3WGHTna2VCWm2G5Yua7E8  \r\n- **USDT (BEP20)**: 0x64B28da787BE6ac5889D276A5638d4f077840eC5  \r\n- **USDT (ERC20)**: 0xe455e21a085ae195a097cd4f456051a9916a5064   \r\n- **TON**: UQAc9zgnnzwS8yb5wxAu5CB0RddmjPBjWI-n46oQ7XfCQrgI  \r\n- **XMR**: 48QsMjqfkeW54vkgKyRnjodtYxdmLk6HXfTWPSZoaFPEDpoHDwFUciGCe1QC9VAeGrgGw4PKNAksX9RW7myFqYJQDN5cHGT    \r\n\r\n\r\n## Support\r\n\r\n**Please check the [FAQ](#faq) first!** Also at least have a short look at the recent chat history.\r\n\r\nYou can report in our Telegram group or via [issue tracker on Github](https://github.com/yuliskov/SmartTube/issues) (account required).\r\n\r\n- **Telegram group (international)**: [@SmartTubeEN](http://t.me/SmartTubeEN)  \r\n- **Discord group (international)**: [SmartTube Official](https://discord.gg/Wt8HDDej5z)  \r\n- **Telegram group (RU/UA)**: [@SmartTubeUA](http://t.me/SmartTubeUA)  \r\n- **Email**: firsth<!-- abc@def -->ash@gmai<!-- @abc.com -->l.com\r\n\r\nThe international group is in **English only**. But don't worry if your English is not perfect, we have a friendly international community.\r\n\r\n\r\n## Team\r\n\r\nSmartTube is developed single-handedly; there is no larger team or company behind this. This is an open source, hobby project. Several others have helped with translations, some of which can be seen on [Github](https://github.com/yuliskov/SmartTube/graphs/contributors), some have sent their translations directly to Yurii. There are also helpful people in the support chat.\r\n\r\n\r\n## Build\r\n    \r\n**NOTE: OpenJDK 14 or older (!) is required. Newer JDK could cause app crash!**  \r\nTo build and install debug version, run these commands:\r\n\r\n```\r\ngit clone https://github.com/yuliskov/SmartTube.git\r\ncd SmartTube\r\ngit submodule update --init\r\nadb connect <device_ip_address>\r\ngradlew clean installStstableDebug\r\n```\r\n\r\n\r\n## Video codecs\r\n\r\nVideo codecs are the algorithms used for video compression.\r\n\r\n\r\n### Which codec to choose / overview\r\n\r\n|          | recommendation                               | hardware support             | compression, bitrate\\*              | quality |\r\n|:--------:|:---------------------------------------------|:---------------------------- |:----------------------:|:-------:|\r\n| **AV01** aka. AV1 | best choice, **if your device supports** | first devices started coming in **2020**   | **best** <small>(e.g. 1.6 Mbps)</small>   | same   |\r\n| **VP9**  | **best choice on most devices**         | most devices **since 2015** | **better** <small>(e.g. 2.1 Mbps)</small> | same   |\r\n| AVC  | only for old or slow hardware | **all** devices             | good <small>(e.g. 2.7 Mbps)</small>   | same   |\r\n\r\n\r\n<small>\\* Examples taken from the video-only track at 1080p @ 25fps for this video: [Dua Lipa - New Rules (Official Music Video)](https://youtube.com/watch?v=k2qgadSvNyU)</small>\r\n\r\nAt the same resolution, a **lower bitrate is better!** YouTube explicitly targets the **same quality** regardless of the codec. Older codecs have a higher bitrate only because they are less efficient. On Youtube, you **do not** get better quality by simply choosing a higher bitrate. Newer codecs have a better compression = lower bitrate = use less bandwidth = save the environment. This is a feature, not a bug. You should use the newest codec that works smoothly on your device, not the least efficient one. AVC usually has the highest bitrate. This is bad, not good.\r\n\r\n\r\n### Which quality to choose?\r\n\r\nCurrently, there is no automatic mode based on your bandwidth. But you can configure a default video preset yourself under settings \\> video player \\> video presets. The first option (\"none\") will remember your last selection within the video player. Any other preset is used initially for each video; if the selected profile is not available, the next best available option is used. You can still override the profile on each video individually within the player.\r\n\r\nTo decide the optional resolution / video quality for you, you need to consider a few limiting factors:\r\n- Your bandwidth (choose only up to the bitrate that your bandwidth can handle; you can do a speedtest using [fast.com](https://fast.com) by Netflix)\r\n- Your TV's display resolution (the quality **might slightly** improve, if you select the next higher resolution, e.g. 1080p on a 720p display; but don't expect a big difference)\r\n- Your TV's capabilities (e.g. HDR, 60fps)\r\n\r\nGenerally 60fps is an improvement, but if you personally don't notice (or mind) the difference, you can save bandwidth (and the environment) by not choosing 60fps.\r\n\r\n\r\n### HDR\r\n\r\nHDR works only **if your hardware supports it**. It's a complicated mess:\r\n- Your TV must support it\r\n- If you use a TV box, that TV box **and** your TV cable **and** the TV must support HDR\r\n- Yes, there truly are different HDMI cable versions with different HDR-support, it's complicated\r\n- some devices (like the **NVIDIA Shield**) generally support HDR, but **not** the specific HDR format that is used on YouTube :cry:\r\n \r\nIf HDR videos look looked dim or washed out, then check [this article](https://www.wired.com/story/hdr-too-dark-how-to-fix-it/). **If HDR is not working**, it's probably not this app's fault. You might need to search on the web for \"HDR\" and your device name for any help.\r\n\r\n## Liability\r\n\r\nWe take no responsibility for the use of our tool, or external instances\r\nprovided by third parties. We strongly recommend you abide by the valid\r\nofficial regulations in your country. Furthermore, we refuse liability\r\nfor any inappropriate use of Invidious, such as illegal downloading.\r\nThis tool is provided to you in the spirit of free, open software.\r\n\r\nYou may view the LICENSE in which this software is provided to you [here](./LICENSE.md).\r\n\r\n>   16. Limitation of Liability.\r\n>\r\n> IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\r\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\r\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\r\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\r\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\r\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\r\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\r\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\r\nSUCH DAMAGES.\r\n\r\n## FAQ\r\n\r\n### Q: Videos buffer a LOT\r\n\r\nA: Try to switch to encrypted DNS like NextDNS. You can set-up such DNS either automatically or manually. To automatic set-up you can use the [Intra apk at fdroid](https://f-droid.org/en/packages/app.intra/) and the [\"AutoStart - No root\"](https://play.google.com/store/apps/details?id=com.autostart) apk to make it autolaunch after every TV restart. For manual set-up [use this guide](https://www.reddit.com/r/MiBox/s/7esEVGtAAa).\r\n\r\n### Q: There is no result for the search that I say (Android 11)\r\n\r\nA: They're some reports that the latest update for \"Google app for Android TV\" could cause this bug. Deleting the update should fix the problem.\r\n\r\n\r\n### Q: AV01 does not play / Why is VP9 slow on my device?\r\n\r\nA: Because AV01 is very new, **most** TVs and TV boxes **do not** offer hardware support and **cannot** play AV01 **at all**.\r\n\r\nIf your device has hardware support for a codec, videos using that codec should play smoothly. High resolutions might also be slow in VP9 on cheap TV boxes that don't officially support 4k. Your device probably can play VP9 videos even without hardware support, however this requires a powerful CPU to run smoothly. Fixing AV01 without hardware support is technically possible, but currently not planned and probably not efficient enough.\r\n\r\n\r\n### Q: Can you make SmartTube look like the original app?\r\n\r\nA: Compared to SmartTube's UI, Stock Youtube and YT Kids are far ahead. However, we'd need someone who's skilled and willing to dedicate enough time and energy into making it. And into maintaining it longterm (incl. new features, bug fixes). All of this for free. If you are / got someone like that, please help.\r\nNot to mention that SmartTube follows Google's official template & recommendations for Android TV apps. It's Google's fault that the template is somewhat ugly. ğŸ˜‚\r\n\r\n\r\n### Q: Can the search page be improved?\r\n\r\nA: It can be, but it takes someone to do it, similar to the above FAQ-entry. SmartTube is following Google's officially recommended design/template for TV apps and is using the official, preinstalled Android TV keyboard. Sadly, Google did a really bad job regarding the search page and keyboard. Maybe a future SmartTube update can add an embedded keyboard, similar to the original YouTube or other major Android TV apps. Maybe it can improve the looks to be as good or better than in the official YouTube app. But for now, it is the way it is due to lack of time and due to Google's official recommendations being bad.\r\n\r\n\r\n### Q: Can I install this on a Samsung Tizen TV / LG webOS TV / Roku / iOS / toaster?\r\n\r\nA: No, this only works on **Android** devices. If you look at an Android TV's product page, it usually says clearly that it's based on Android. The app **cannot** easily be ported over to other plattforms and we have no plans to even try. **Please do not ask**. Instead, you can connect a separate TV stick or TV box to your TV.\r\n\r\n\r\n### Q: Can I install this on a smartphone? / Can you add portrait mode? / Scrolling doesn't work.\r\n\r\nA: **Big No**. This app is **not** for smartphones, we offer **zero support** for that.\r\n\r\nYou **can cast** videos **from** your smartphone to a TV / TV box running SmartTube, though. Just use the official YouTube app or [ReVanced](https://github.com/ReVanced), see [the casting section](#casting) for more information.\r\n\r\n**There will not be a phone version.** You can use [ReVanced](https://github.com/ReVanced), [Pure Tuber](https://play.google.com/store/apps/details?id=free.tube.premium.advanced.tuber), [NewPipe](https://newpipe.schabi.org), or [NewPipe x SponsorBlock](https://github.com/polymorphicshade/NewPipe#newpipe-x-sponsorblock) instead. Please go to their respective support chats for help.\r\n\r\n\r\n### Q: Can I install this on a tablet / car screen / smartphone with docking station?\r\n\r\nYes... maybe.. Requirements:\r\n\r\n- It is an Android device\r\n- It has a large screen\r\n- It has a TV remote, controller, or keyboard\r\n  **Touch input is not supported.** Mouse/touchpad scrolling neither. You cannot properly use SmartTube with only touch or mouse input.\r\n\r\nSome users reported great success (incl. on a [car entertainment system](https://t.me/SmartTubeEN/6060)). **Please share your success stories with us.**\r\n\r\n\r\n### Q: I get \"unknown codec\" / \"can't download video\" errors\r\n\r\nA: please wait 5 seconds for the video to play. If that doesn't help, press the play button. Some users reported, that this issue only appears when they have a USB audio device attached or if their disk storage is full.\r\n\r\n\r\n### Q: I get \"the video profile is not supported\"\r\n\r\nA. Press the \"HQ\"-button in the bottom-left, select _video formats_ and select anything other than AV01. AV01 is **not supported** on most devices (apparently including yours), so select VP9 instead. See [the section on video codecs](#Video-codecs) for more information.\r\n\r\n\r\n### Q: I get \"video unavailable\" when watching unlisted videos / my own videos\r\n\r\nA: Right, that's currently a bug.\r\n\r\n\r\n### Q: It doesn't show up on my casting list\r\n\r\nA: Please read the [Casting](#casting) section.\r\n\r\n### Q: I get an error saying \"Sign in to confirm you're not a bot\"\r\n\r\nA: Your IP address range might be temporaily/permanently blocked by YouTube from watching videos if you not signed in to your account.\r\n\r\n### Q: The video is buffering a lot\r\n\r\nA: The issue might not be specific to SmartTube, as other unofficial YouTube apps also report this issue. It seems uncommon nowadays, but was very present in the 2nd quarter of 2021. Some users or devices seem to be more affected then others. The official YouTube app & website are apparently only rarely affected. The root cause of the issue is currently unclear, but it appears to be a server-side thing on YouTube's end. Possibly, YouTube is discriminating 3rd party apps.\r\n\r\nFor now, try to see if it helps to:\r\n\r\n- Reduce the resolution (or chance it back)\r\n- Change the video format to AVC\r\n- Increase the buffer in the settings\r\n- Hit the back button and try playing the video again\r\n\r\n\r\n### Q: The debug information says my display is 1080p, but I have a 4k/UHD display!\r\n\r\nA: Do not worry, **the debug information is incorrect.** SmartTube works fine even above 1080p and you should be able to see that, when you play a video in 4k or UHD.\r\nAlso do not worry if it says \"720p\" and you have a 1080p display.\r\n\r\n\r\n### Q: Why does it not autoselect highest quality?\r\n\r\nA: **It does** (by default). If you set a _video profile_ under settings, that acts as a maximum for automatic selection. Check if you configured a video profile, you can unset it by choosing \"none\".\r\n\r\n**Please do not confuse quality with bitrate**. See [the section on video codecs](#Video-codecs) for more information.\r\n\r\n\r\n### Q: Can I set a (maximum) resolution by default?\r\n\r\nA: SmartTube automatically select the highest available quality for your video, up to a maximum resolution that you can set in the settings under \"video profile\". If available, SmartTube will pick the selected video profile, or otherwise the next best one available will be used. You can still always change the video profile while watching videos.\r\n\r\n\r\n### Q: Can it set the resolution to \"auto\", depending on my available bandwidth?\r\n\r\nA: This is planned, but not available yet (sorry ğŸ™‡â€â™€ï¸). However, you can set a maximum resolution to something that should work for your bandwidth. See above for details.\r\n\r\n\r\n### Q: Why does it skip video segments?\r\n\r\nA: SmartTube has a feature called **SponsorBlock**. You can select categories should be skipped, if any. See the [SponsorBlock section](#sponsorblock) for more details.\r\n\r\n\r\n### Q: How to start the next video automatically / stop after every video?\r\n\r\nA: You can switch between different autoplay-modes using the loop-button ğŸ”\r\n\r\n[![screenshot showing the loop-button](images/new/V3GHGvWprmdE1w.jpg)](https://t.me/SmartTubeEN/24953)\r\n\r\n\r\n### Q: How to remove recommended videos (e.g. news) that are unrelated to me?\r\n\r\nA: Recommended videos are defined by YouTube and not by the app, we cannot change the algorithm. They are based on your country, which you can change in the settings. If you are logged in, they are based on your watch history, user profile data, and whatever else Google might use. If you are not logged in, you are like in \"incognito mode\", so your watch history does not influence your recommendations. Maybe a future version will add optional user profiling without logging in.\r\n\r\n\r\n### Q: Does HDR work?\r\n\r\nA: Yes, HDR works **if your hardware** supports it. The **NVIDIA Shield** does not. See [the section on HDR](#HDR) for more information.\r\n\r\n\r\n### Q: Why do some updates say \"don't update if satisfied with the current version\" in the changelog?\r\n\r\nA: These updates change a lot of code, trying to fix bugs that only affect a few users/devices. Only the affected users should update. For anyone else, there is nothing to gain from updating; however there is the chance of causing new bugs. Do not worry if you updated anyways.\r\n\r\n\r\n### Q: When playing at other speeds, frames are skipped!\r\n\r\nA: We currently cannot fix this, sorry.\r\n\r\n\r\n### Q: What is AFR?\r\n\r\nA: \"Auto Frame Rate\". It adjusts the refresh rate of your TV to match the content you're watching. It can slightly improve the smoothness, but the difference is very small; most people barely notice it. It does not work well on every hardware. If you don't know what it does and don't want to test it out yourself, you can safely keep it off.\r\n\r\n**Recommendation:** You can turn it on to see if it works on your device; if it causes issues (or if you don't care to test), turn it **off**.\r\n\r\n\r\n### Q: Should I choose high or low buffer?\r\n\r\nA: The higher your buffer, the more of a video will be preloaded ahead of your current position. A low buffer might minimally reduce your bandwidth usage, if you often close videos before they end. A high buffer can smooth out network issues and prevent the video from pausing to buffer. A higher buffer increases RAM usage, however this shouldn't be an issue.\r\n\r\n**Recommendation: high**.\r\n\r\n\r\n### Q: Can I retain the buffer when seeking back?\r\n\r\nA: No, when you seek back (e.g. jump back 5 seconds), SmartTube will have to rebuffer. This might be improved in a future update.\r\n\r\n\r\n### Q: My device freezes when watching YouTube\r\n\r\nA: That's a firmware or Android issue. If you are using a custom rom, maybe that rom is buggy. Because this issue is nearly impossible for the developer to debug, we cannot help you, sorry. You can try the usual workarounds: rebooting, clearing cache, reinstalling the app, or factory resetting the device.\r\n\r\n\r\n### Q: Can I download videos?\r\n\r\nA: Not with SmartTube\r\n\r\n\r\n### Q: Can updates be installed automatically?\r\n\r\nA: No, this is technically not possible. Only the preinstalled app manager (usually Google PlayStore, Amazon AppStore, etc) has the required permission. All other apps, incl. SmartTube can only show open installation prompt. A workaround using root would be possible, but hasn't been implemented yet.\r\n\r\n\r\n### Q: Can I whitelist ads on some channels?\r\n\r\nA: No, this is not possible. SmartTube does not have any code to display ads. Adding this functionality would actually take time and effort, which is instead spent on adding useful features and fixing bugs.\r\n\r\n\r\n\r\n",
      "stars_today": 20
    },
    {
      "id": 656264456,
      "name": "langchain4j",
      "full_name": "langchain4j/langchain4j",
      "description": "LangChain4j is an open-source Java library that simplifies the integration of LLMs into Java applications through a unified API, providing access to popular LLMs and vector databases. It makes implementing RAG, tool calling (including support for MCP), and agents easy. LangChain4j integrates seamlessly with various enterprise Java frameworks.",
      "html_url": "https://github.com/langchain4j/langchain4j",
      "stars": 10541,
      "forks": 1932,
      "language": "Java",
      "topics": [
        "anthropic",
        "chatgpt",
        "chroma",
        "embeddings",
        "gemini",
        "gpt",
        "huggingface",
        "java",
        "langchain",
        "llama",
        "llm",
        "llms",
        "milvus",
        "ollama",
        "onnx",
        "openai",
        "openai-api",
        "pgvector",
        "pinecone",
        "vector-database"
      ],
      "created_at": "2023-06-20T15:30:29Z",
      "updated_at": "2026-01-28T01:50:26Z",
      "pushed_at": "2026-01-27T16:19:33Z",
      "open_issues": 684,
      "owner": {
        "login": "langchain4j",
        "avatar_url": "https://avatars.githubusercontent.com/u/132277850?v=4"
      },
      "readme": "# LangChain for Java: Supercharge your Java application with the power of LLMs\n\n[![Build Status](https://img.shields.io/github/actions/workflow/status/langchain4j/langchain4j/main.yaml?branch=main&style=for-the-badge&label=CI%20BUILD&logo=github)](https://github.com/langchain4j/langchain4j/actions/workflows/main.yaml)\n[![Nightly Build](https://img.shields.io/github/actions/workflow/status/langchain4j/langchain4j/nightly_jdk17.yaml?branch=main&style=for-the-badge&label=NIGHTLY%20BUILD&logo=github)](https://github.com/langchain4j/langchain4j/actions/workflows/nightly_jdk17.yaml)\n[![CODACY](https://img.shields.io/badge/Codacy-Dashboard-blue?style=for-the-badge&logo=codacy)](https://app.codacy.com/gh/langchain4j/langchain4j/dashboard)\n\n[![Discord](https://img.shields.io/discord/1156626270772269217?logo=discord&style=for-the-badge)](https://discord.gg/JzTFvyjG6R)\n[![BlueSky](https://img.shields.io/badge/@langchain4j-follow-blue?logo=bluesky&style=for-the-badge)](https://bsky.app/profile/langchain4j.dev)\n[![X](https://img.shields.io/badge/@langchain4j-follow-blue?logo=x&style=for-the-badge)](https://x.com/langchain4j)\n[![Maven Version](https://img.shields.io/maven-central/v/dev.langchain4j/langchain4j?logo=apachemaven&style=for-the-badge)](https://search.maven.org/#search|gav|1|g:\"dev.langchain4j\"%20AND%20a:\"langchain4j\")\n\n\n## Introduction\n\nWelcome!\n\nThe goal of LangChain4j is to simplify integrating LLMs into Java applications.\n\nHere's how:\n1. **Unified APIs:**\n   LLM providers (like OpenAI or Google Vertex AI) and embedding (vector) stores (such as Pinecone or Milvus)\n   use proprietary APIs. LangChain4j offers a unified API to avoid the need for learning and implementing specific APIs for each of them.\n   To experiment with different LLMs or embedding stores, you can easily switch between them without the need to rewrite your code.\n   LangChain4j currently supports [20+ popular LLM providers](https://docs.langchain4j.dev/integrations/language-models/)\n   and [30+ embedding stores](https://docs.langchain4j.dev/integrations/embedding-stores/).\n2. **Comprehensive Toolbox:**\n   Since early 2023, the community has been building numerous LLM-powered applications,\n   identifying common abstractions, patterns, and techniques. LangChain4j has refined these into practical code.\n   Our toolbox includes tools ranging from low-level prompt templating, chat memory management, and function calling\n   to high-level patterns like Agents and RAG.\n   For each abstraction, we provide an interface along with multiple ready-to-use implementations based on common techniques.\n   Whether you're building a chatbot or developing a RAG with a complete pipeline from data ingestion to retrieval,\n   LangChain4j offers a wide variety of options.\n3. **Numerous Examples:**\n   These [examples](https://github.com/langchain4j/langchain4j-examples) showcase how to begin creating various LLM-powered applications,\n   providing inspiration and enabling you to start building quickly.\n\nLangChain4j began development in early 2023 amid the ChatGPT hype.\nWe noticed a lack of Java counterparts to the numerous Python and JavaScript LLM libraries and frameworks,\nand we had to fix that!\nAlthough \"LangChain\" is in our name, the project is a fusion of ideas and concepts from LangChain, Haystack,\nLlamaIndex, and the broader community, spiced up with a touch of our own innovation.\n\nWe actively monitor community developments, aiming to quickly incorporate new techniques and integrations,\nensuring you stay up-to-date.\nThe library is under active development. While some features are still being worked on,\nthe core functionality is in place, allowing you to start building LLM-powered apps now!\n\n\n## Documentation\nDocumentation can be found [here](https://docs.langchain4j.dev).\n\nThe documentation chatbot (experimental) can be found [here](https://chat.langchain4j.dev/).\n\n\n## Getting Started\nGetting started guide can be found [here](https://docs.langchain4j.dev/get-started).\n\n\n## Code Examples\nPlease see examples of how LangChain4j can be used in [langchain4j-examples](https://github.com/langchain4j/langchain4j-examples) repo:\n- [Examples in plain Java](https://github.com/langchain4j/langchain4j-examples/tree/main/other-examples/src/main/java)\n- [Examples with Quarkus](https://github.com/quarkiverse/quarkus-langchain4j/tree/main/samples) (uses [quarkus-langchain4j](https://github.com/quarkiverse/quarkus-langchain4j) dependency)\n- [Example with Spring Boot](https://github.com/langchain4j/langchain4j-examples/tree/main/spring-boot-example/src/main/java/dev/langchain4j/example)\n- [Examples with Helidon](https://github.com/helidon-io/helidon-examples/tree/helidon-4.x/examples/integrations/langchain4j) (uses [io.helidon.integrations.langchain4j](https://mvnrepository.com/artifact/io.helidon.integrations.langchain4j) dependency)\n- [Examples with Micronaut](https://github.com/micronaut-projects/micronaut-langchain4j/tree/0.3.x/doc-examples/example-openai-java) (uses [micronaut-langchain4j](https://micronaut-projects.github.io/micronaut-langchain4j/latest/guide/) dependency)\n\n## Useful Materials\nUseful materials can be found [here](https://docs.langchain4j.dev/useful-materials).\n\n\n## Get Help\nPlease use [Discord](https://discord.gg/JzTFvyjG6R) or [GitHub discussions](https://github.com/langchain4j/langchain4j/discussions)\nto get help.\n\n\n## Request Features\nPlease let us know what features you need by [opening an issue](https://github.com/langchain4j/langchain4j/issues/new/choose).\n\n\n## Contribute\nContribution guidelines can be found [here](https://github.com/langchain4j/langchain4j/blob/main/CONTRIBUTING.md).\n",
      "stars_today": 20
    },
    {
      "id": 20904437,
      "name": "gin",
      "full_name": "gin-gonic/gin",
      "description": "Gin is a high-performance HTTP web framework written in Go. It provides a Martini-like API but with significantly better performanceâ€”up to 40 times fasterâ€”thanks to httprouter. Gin is designed for building REST APIs, web applications, and microservices.",
      "html_url": "https://github.com/gin-gonic/gin",
      "stars": 87817,
      "forks": 8548,
      "language": "Go",
      "topics": [
        "framework",
        "gin",
        "go",
        "middleware",
        "performance",
        "router",
        "server"
      ],
      "created_at": "2014-06-16T23:57:25Z",
      "updated_at": "2026-01-28T01:54:25Z",
      "pushed_at": "2026-01-27T02:10:00Z",
      "open_issues": 881,
      "owner": {
        "login": "gin-gonic",
        "avatar_url": "https://avatars.githubusercontent.com/u/7894478?v=4"
      },
      "readme": "# Gin Web Framework\n\n<img align=\"right\" width=\"159px\" src=\"https://raw.githubusercontent.com/gin-gonic/logo/master/color.png\">\n\n[![Build Status](https://github.com/gin-gonic/gin/actions/workflows/gin.yml/badge.svg?branch=master)](https://github.com/gin-gonic/gin/actions/workflows/gin.yml)\n[![Trivy Security Scan](https://github.com/gin-gonic/gin/actions/workflows/trivy-scan.yml/badge.svg)](https://github.com/gin-gonic/gin/actions/workflows/trivy-scan.yml)\n[![codecov](https://codecov.io/gh/gin-gonic/gin/branch/master/graph/badge.svg)](https://codecov.io/gh/gin-gonic/gin)\n[![Go Report Card](https://goreportcard.com/badge/github.com/gin-gonic/gin)](https://goreportcard.com/report/github.com/gin-gonic/gin)\n[![Go Reference](https://pkg.go.dev/badge/github.com/gin-gonic/gin?status.svg)](https://pkg.go.dev/github.com/gin-gonic/gin?tab=doc)\n[![Sourcegraph](https://sourcegraph.com/github.com/gin-gonic/gin/-/badge.svg)](https://sourcegraph.com/github.com/gin-gonic/gin?badge)\n[![Open Source Helpers](https://www.codetriage.com/gin-gonic/gin/badges/users.svg)](https://www.codetriage.com/gin-gonic/gin)\n[![Release](https://img.shields.io/github/release/gin-gonic/gin.svg?style=flat-square)](https://github.com/gin-gonic/gin/releases)\n\n## ğŸ“° [Announcing Gin 1.11.0!](https://gin-gonic.com/en/blog/news/gin-1-11-0-release-announcement/)\n\nRead about the latest features and improvements in Gin 1.11.0 on our official blog.\n\n---\n\nGin is a high-performance HTTP web framework written in [Go](https://go.dev/). It provides a Martini-like API but with significantly better performanceâ€”up to 40 times fasterâ€”thanks to [httprouter](https://github.com/julienschmidt/httprouter). Gin is designed for building REST APIs, web applications, and microservices where speed and developer productivity are essential.\n\n**Why choose Gin?**\n\nGin combines the simplicity of Express.js-style routing with Go's performance characteristics, making it ideal for:\n\n- Building high-throughput REST APIs\n- Developing microservices that need to handle many concurrent requests\n- Creating web applications that require fast response times\n- Prototyping web services quickly with minimal boilerplate\n\n**Gin's key features:**\n\n- **Zero allocation router** - Extremely memory-efficient routing with no heap allocations\n- **High performance** - Benchmarks show superior speed compared to other Go web frameworks\n- **Middleware support** - Extensible middleware system for authentication, logging, CORS, etc.\n- **Crash-free** - Built-in recovery middleware prevents panics from crashing your server\n- **JSON validation** - Automatic request/response JSON binding and validation\n- **Route grouping** - Organize related routes and apply common middleware\n- **Error management** - Centralized error handling and logging\n- **Built-in rendering** - Support for JSON, XML, HTML templates, and more\n- **Extensible** - Large ecosystem of community middleware and plugins\n\n## Getting Started\n\n### Prerequisites\n\n- **Go version**: Gin requires [Go](https://go.dev/) version [1.24](https://go.dev/doc/devel/release#go1.24.0) or above\n- **Basic Go knowledge**: Familiarity with Go syntax and package management is helpful\n\n### Installation\n\nWith [Go's module support](https://go.dev/wiki/Modules#how-to-use-modules), simply import Gin in your code and Go will automatically fetch it during build:\n\n```go\nimport \"github.com/gin-gonic/gin\"\n```\n\n### Your First Gin Application\n\nHere's a complete example that demonstrates Gin's simplicity:\n\n```go\npackage main\n\nimport (\n  \"log\"\n  \"net/http\"\n\n  \"github.com/gin-gonic/gin\"\n)\n\nfunc main() {\n  // Create a Gin router with default middleware (logger and recovery)\n  r := gin.Default()\n\n  // Define a simple GET endpoint\n  r.GET(\"/ping\", func(c *gin.Context) {\n    // Return JSON response\n    c.JSON(http.StatusOK, gin.H{\n      \"message\": \"pong\",\n    })\n  })\n\n  // Start server on port 8080 (default)\n  // Server will listen on 0.0.0.0:8080 (localhost:8080 on Windows)\n  if err := r.Run(); err != nil {\n    log.Fatalf(\"failed to run server: %v\", err)\n  }\n}\n```\n\n**Running the application:**\n\n1. Save the code above as `main.go`\n2. Run the application:\n\n   ```sh\n   go run main.go\n   ```\n\n3. Open your browser and visit [`http://localhost:8080/ping`](http://localhost:8080/ping)\n4. You should see: `{\"message\":\"pong\"}`\n\n**What this example demonstrates:**\n\n- Creating a Gin router with default middleware\n- Defining HTTP endpoints with simple handler functions\n- Returning JSON responses\n- Starting an HTTP server\n\n### Next Steps\n\nAfter running your first Gin application, explore these resources to learn more:\n\n#### ğŸ“š Learning Resources\n\n- **[Gin Quick Start Guide](docs/doc.md)** - Comprehensive tutorial with API examples and build configurations\n- **[Example Repository](https://github.com/gin-gonic/examples)** - Ready-to-run examples demonstrating various Gin use cases:\n  - REST API development\n  - Authentication & middleware\n  - File uploads and downloads\n  - WebSocket connections\n  - Template rendering\n\n## ğŸ“– Documentation\n\n### API Reference\n\n- **[Go.dev API Documentation](https://pkg.go.dev/github.com/gin-gonic/gin)** - Complete API reference with examples\n\n### User Guides\n\nThe comprehensive documentation is available on [gin-gonic.com](https://gin-gonic.com) in multiple languages:\n\n- [English](https://gin-gonic.com/en/docs/) | [ç®€ä½“ä¸­æ–‡](https://gin-gonic.com/zh-cn/docs/) | [ç¹é«”ä¸­æ–‡](https://gin-gonic.com/zh-tw/docs/)\n- [æ—¥æœ¬èª](https://gin-gonic.com/ja/docs/) | [í•œêµ­ì–´](https://gin-gonic.com/ko-kr/docs/) | [EspaÃ±ol](https://gin-gonic.com/es/docs/)\n- [Turkish](https://gin-gonic.com/tr/docs/) | [Persian](https://gin-gonic.com/fa/docs/) | [PortuguÃªs](https://gin-gonic.com/pt/docs/)\n- [Russian](https://gin-gonic.com/ru/docs/) | [Indonesian](https://gin-gonic.com/id/docs/)\n\n### Official Tutorials\n\n- [Go.dev Tutorial: Developing a RESTful API with Go and Gin](https://go.dev/doc/tutorial/web-service-gin)\n\n## âš¡ Performance Benchmarks\n\nGin demonstrates exceptional performance compared to other Go web frameworks. It uses a custom version of [HttpRouter](https://github.com/julienschmidt/httprouter) for maximum efficiency. [View detailed benchmarks â†’](/BENCHMARKS.md)\n\n**Gin vs. Other Go Frameworks** (GitHub API routing benchmark):\n\n| Benchmark name                 |       (1) |             (2) |          (3) |             (4) |\n| ------------------------------ | --------: | --------------: | -----------: | --------------: |\n| BenchmarkGin_GithubAll         | **43550** | **27364 ns/op** |   **0 B/op** | **0 allocs/op** |\n| BenchmarkAce_GithubAll         |     40543 |     29670 ns/op |       0 B/op |     0 allocs/op |\n| BenchmarkAero_GithubAll        |     57632 |     20648 ns/op |       0 B/op |     0 allocs/op |\n| BenchmarkBear_GithubAll        |      9234 |    216179 ns/op |   86448 B/op |   943 allocs/op |\n| BenchmarkBeego_GithubAll       |      7407 |    243496 ns/op |   71456 B/op |   609 allocs/op |\n| BenchmarkBone_GithubAll        |       420 |   2922835 ns/op |  720160 B/op |  8620 allocs/op |\n| BenchmarkChi_GithubAll         |      7620 |    238331 ns/op |   87696 B/op |   609 allocs/op |\n| BenchmarkDenco_GithubAll       |     18355 |     64494 ns/op |   20224 B/op |   167 allocs/op |\n| BenchmarkEcho_GithubAll        |     31251 |     38479 ns/op |       0 B/op |     0 allocs/op |\n| BenchmarkGocraftWeb_GithubAll  |      4117 |    300062 ns/op |  131656 B/op |  1686 allocs/op |\n| BenchmarkGoji_GithubAll        |      3274 |    416158 ns/op |   56112 B/op |   334 allocs/op |\n| BenchmarkGojiv2_GithubAll      |      1402 |    870518 ns/op |  352720 B/op |  4321 allocs/op |\n| BenchmarkGoJsonRest_GithubAll  |      2976 |    401507 ns/op |  134371 B/op |  2737 allocs/op |\n| BenchmarkGoRestful_GithubAll   |       410 |   2913158 ns/op |  910144 B/op |  2938 allocs/op |\n| BenchmarkGorillaMux_GithubAll  |       346 |   3384987 ns/op |  251650 B/op |  1994 allocs/op |\n| BenchmarkGowwwRouter_GithubAll |     10000 |    143025 ns/op |   72144 B/op |   501 allocs/op |\n| BenchmarkHttpRouter_GithubAll  |     55938 |     21360 ns/op |       0 B/op |     0 allocs/op |\n| BenchmarkHttpTreeMux_GithubAll |     10000 |    153944 ns/op |   65856 B/op |   671 allocs/op |\n| BenchmarkKocha_GithubAll       |     10000 |    106315 ns/op |   23304 B/op |   843 allocs/op |\n| BenchmarkLARS_GithubAll        |     47779 |     25084 ns/op |       0 B/op |     0 allocs/op |\n| BenchmarkMacaron_GithubAll     |      3266 |    371907 ns/op |  149409 B/op |  1624 allocs/op |\n| BenchmarkMartini_GithubAll     |       331 |   3444706 ns/op |  226551 B/op |  2325 allocs/op |\n| BenchmarkPat_GithubAll         |       273 |   4381818 ns/op | 1483152 B/op | 26963 allocs/op |\n| BenchmarkPossum_GithubAll      |     10000 |    164367 ns/op |   84448 B/op |   609 allocs/op |\n| BenchmarkR2router_GithubAll    |     10000 |    160220 ns/op |   77328 B/op |   979 allocs/op |\n| BenchmarkRivet_GithubAll       |     14625 |     82453 ns/op |   16272 B/op |   167 allocs/op |\n| BenchmarkTango_GithubAll       |      6255 |    279611 ns/op |   63826 B/op |  1618 allocs/op |\n| BenchmarkTigerTonic_GithubAll  |      2008 |    687874 ns/op |  193856 B/op |  4474 allocs/op |\n| BenchmarkTraffic_GithubAll     |       355 |   3478508 ns/op |  820744 B/op | 14114 allocs/op |\n| BenchmarkVulcan_GithubAll      |      6885 |    193333 ns/op |   19894 B/op |   609 allocs/op |\n\n- (1): Total Repetitions achieved in constant time, higher means more confident result\n- (2): Single Repetition Duration (ns/op), lower is better\n- (3): Heap Memory (B/op), lower is better\n- (4): Average Allocations per Repetition (allocs/op), lower is better\n\n## ğŸ”Œ Middleware Ecosystem\n\nGin has a rich ecosystem of middleware for common web development needs. Explore community-contributed middleware:\n\n- **[gin-contrib](https://github.com/gin-contrib)** - Official middleware collection including:\n  - Authentication (JWT, Basic Auth, Sessions)\n  - CORS, Rate limiting, Compression\n  - Logging, Metrics, Tracing\n  - Static file serving, Template engines\n- **[gin-gonic/contrib](https://github.com/gin-gonic/contrib)** - Additional community middleware\n\n## ğŸ¢ Production Usage\n\nGin powers many high-traffic applications and services in production:\n\n- **[gorush](https://github.com/appleboy/gorush)** - High-performance push notification server\n- **[fnproject](https://github.com/fnproject/fn)** - Container-native, serverless platform\n- **[photoprism](https://github.com/photoprism/photoprism)** - AI-powered personal photo management\n- **[lura](https://github.com/luraproject/lura)** - Ultra-performant API Gateway framework\n- **[picfit](https://github.com/thoas/picfit)** - Real-time image processing server\n- **[dkron](https://github.com/distribworks/dkron)** - Distributed job scheduling system\n\n## ğŸ¤ Contributing\n\nGin is the work of hundreds of contributors from around the world. We welcome and appreciate your contributions!\n\n### How to Contribute\n\n- ğŸ› **Report bugs** - Help us identify and fix issues\n- ğŸ’¡ **Suggest features** - Share your ideas for improvements\n- ğŸ“ **Improve documentation** - Help make our docs clearer\n- ğŸ”§ **Submit code** - Fix bugs or implement new features\n- ğŸ§ª **Write tests** - Improve our test coverage\n\n### Getting Started with Contributing\n\n1. Check out our [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines\n2. Join our community discussions and ask questions\n\n**All contributions are valued and help make Gin better for everyone!**\n",
      "stars_today": 19
    },
    {
      "id": 6838921,
      "name": "prometheus",
      "full_name": "prometheus/prometheus",
      "description": "The Prometheus monitoring system and time series database.",
      "html_url": "https://github.com/prometheus/prometheus",
      "stars": 62414,
      "forks": 10122,
      "language": "Go",
      "topics": [
        "alerting",
        "graphing",
        "hacktoberfest",
        "metrics",
        "monitoring",
        "prometheus",
        "time-series"
      ],
      "created_at": "2012-11-24T11:14:12Z",
      "updated_at": "2026-01-27T23:59:50Z",
      "pushed_at": "2026-01-27T20:32:50Z",
      "open_issues": 753,
      "owner": {
        "login": "prometheus",
        "avatar_url": "https://avatars.githubusercontent.com/u/3380462?v=4"
      },
      "readme": "<h1 align=\"center\" style=\"border-bottom: none\">\n    <a href=\"https://prometheus.io\" target=\"_blank\"><img alt=\"Prometheus\" src=\"/documentation/images/prometheus-logo.svg\"></a><br>Prometheus\n</h1>\n\n<p align=\"center\">Visit <a href=\"https://prometheus.io\" target=\"_blank\">prometheus.io</a> for the full documentation,\nexamples and guides.</p>\n\n<div align=\"center\">\n\n[![CI](https://github.com/prometheus/prometheus/actions/workflows/ci.yml/badge.svg)](https://github.com/prometheus/prometheus/actions/workflows/ci.yml)\n[![Docker Repository on Quay](https://quay.io/repository/prometheus/prometheus/status)][quay]\n[![Docker Pulls](https://img.shields.io/docker/pulls/prom/prometheus.svg?maxAge=604800)][hub]\n[![Go Report Card](https://goreportcard.com/badge/github.com/prometheus/prometheus)](https://goreportcard.com/report/github.com/prometheus/prometheus)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/486/badge)](https://bestpractices.coreinfrastructure.org/projects/486)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/prometheus/prometheus/badge)](https://securityscorecards.dev/viewer/?uri=github.com/prometheus/prometheus)\n[![CLOMonitor](https://img.shields.io/endpoint?url=https://clomonitor.io/api/projects/cncf/prometheus/badge)](https://clomonitor.io/projects/cncf/prometheus)\n[![Gitpod ready-to-code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/prometheus/prometheus)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/prometheus.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:prometheus)\n\n</div>\n\nPrometheus, a [Cloud Native Computing Foundation](https://cncf.io/) project, is a systems and service monitoring system. It collects metrics\nfrom configured targets at given intervals, evaluates rule expressions,\ndisplays the results, and can trigger alerts when specified conditions are observed.\n\nThe features that distinguish Prometheus from other metrics and monitoring systems are:\n\n* A **multi-dimensional** data model (time series defined by metric name and set of key/value dimensions)\n* PromQL, a **powerful and flexible query language** to leverage this dimensionality\n* No dependency on distributed storage; **single server nodes are autonomous**\n* An HTTP **pull model** for time series collection\n* **Pushing time series** is supported via an intermediary gateway for batch jobs\n* Targets are discovered via **service discovery** or **static configuration**\n* Multiple modes of **graphing and dashboarding support**\n* Support for hierarchical and horizontal **federation**\n\n## Architecture overview\n\n![Architecture overview](documentation/images/architecture.svg)\n\n## Install\n\nThere are various ways of installing Prometheus.\n\n### Precompiled binaries\n\nPrecompiled binaries for released versions are available in the\n[*download* section](https://prometheus.io/download/)\non [prometheus.io](https://prometheus.io). Using the latest production release binary\nis the recommended way of installing Prometheus.\nSee the [Installing](https://prometheus.io/docs/introduction/install/)\nchapter in the documentation for all the details.\n\n### Docker images\n\nDocker images are available on [Quay.io](https://quay.io/repository/prometheus/prometheus) or [Docker Hub](https://hub.docker.com/r/prom/prometheus/).\n\nYou can launch a Prometheus container for trying it out with\n\n```bash\ndocker run --name prometheus -d -p 127.0.0.1:9090:9090 prom/prometheus\n```\n\nPrometheus will now be reachable at <http://localhost:9090/>.\n\n### Building from source\n\nTo build Prometheus from source code, You need:\n\n* Go: Version specified in [go.mod](./go.mod) or greater.\n* NodeJS: Version specified in [.nvmrc](./web/ui/.nvmrc) or greater.\n* npm: Version 10 or greater (check with `npm --version` and [here](https://www.npmjs.com/)).\n\nStart by cloning the repository:\n\n```bash\ngit clone https://github.com/prometheus/prometheus.git\ncd prometheus\n```\n\nYou can use the `go` tool to build and install the `prometheus`\nand `promtool` binaries into your `GOPATH`:\n\n```bash\ngo install github.com/prometheus/prometheus/cmd/...\nprometheus --config.file=your_config.yml\n```\n\n*However*, when using `go install` to build Prometheus, Prometheus will expect to be able to\nread its web assets from local filesystem directories under `web/ui/static`. In order for\nthese assets to be found, you will have to run Prometheus from the root of the cloned\nrepository. Note also that this directory does not include the React UI unless it has been\nbuilt explicitly using `make assets` or `make build`.\n\nAn example of the above configuration file can be found [here.](https://github.com/prometheus/prometheus/blob/main/documentation/examples/prometheus.yml)\n\nYou can also build using `make build`, which will compile in the web assets so that\nPrometheus can be run from anywhere:\n\n```bash\nmake build\n./prometheus --config.file=your_config.yml\n```\n\nThe Makefile provides several targets:\n\n* *build*: build the `prometheus` and `promtool` binaries (includes building and compiling in web assets)\n* *test*: run the tests\n* *test-short*: run the short tests\n* *format*: format the source code\n* *vet*: check the source code for common errors\n* *assets*: build the React UI\n\n### Service discovery plugins\n\nPrometheus is bundled with many service discovery plugins. You can customize\nwhich service discoveries are included in your build using Go build tags.\n\nTo exclude service discoveries when building with `make build`, add the desired\ntags to the `.promu.yml` file under `build.tags.all`:\n\n```yaml\nbuild:\n    tags:\n        all:\n            - netgo\n            - builtinassets\n            - remove_all_sd           # Exclude all optional SDs\n            - enable_kubernetes_sd    # Re-enable only kubernetes\n```\n\nThen run `make build` as usual. Alternatively, when using `go build` directly:\n\n```bash\ngo build -tags \"remove_all_sd,enable_kubernetes_sd\" ./cmd/prometheus\n```\n\nAvailable build tags:\n* `remove_all_sd` - Exclude all optional service discoveries (keeps file_sd, static_sd, and http_sd)\n* `enable_<name>_sd` - Re-enable a specific SD when using `remove_all_sd`\n\nIf you add out-of-tree plugins, which we do not endorse at the moment,\nadditional steps might be needed to adjust the `go.mod` and `go.sum` files. As\nalways, be extra careful when loading third party code.\n\n### Building the Docker image\n\nYou can build a docker image locally with the following commands:\n\n```bash\nmake promu\npromu crossbuild -p linux/amd64\nmake npm_licenses\nmake common-docker-amd64\n```\n\nThe `make docker` target is intended only for use in our CI system and will not\nproduce a fully working image when run locally.\n\n## Using Prometheus as a Go Library\n\n### Remote Write\n\nWe are publishing our Remote Write protobuf independently at\n[buf.build](https://buf.build/prometheus/prometheus/assets).\n\nYou can use that as a library:\n\n```shell\ngo get buf.build/gen/go/prometheus/prometheus/protocolbuffers/go@latest\n```\n\nThis is experimental.\n\n### Prometheus code base\n\nIn order to comply with [go mod](https://go.dev/ref/mod#versions) rules,\nPrometheus release number do not exactly match Go module releases.\n\nFor the\nPrometheus v3.y.z releases, we are publishing equivalent v0.3y.z tags. The y in v0.3y.z is always padded to two digits, with a leading zero if needed.\n\nTherefore, a user that would want to use Prometheus v3.0.0 as a library could do:\n\n```shell\ngo get github.com/prometheus/prometheus@v0.300.0\n```\n\nFor the\nPrometheus v2.y.z releases, we published the equivalent v0.y.z tags.\n\nTherefore, a user that would want to use Prometheus v2.35.0 as a library could do:\n\n```shell\ngo get github.com/prometheus/prometheus@v0.35.0\n```\n\nThis solution makes it clear that we might break our internal Go APIs between\nminor user-facing releases, as [breaking changes are allowed in major version\nzero](https://semver.org/#spec-item-4).\n\n## React UI Development\n\nFor more information on building, running, and developing on the React-based UI, see the React app's [README.md](web/ui/README.md).\n\n## More information\n\n* Godoc documentation is available via [pkg.go.dev](https://pkg.go.dev/github.com/prometheus/prometheus). Due to peculiarities of Go Modules, v3.y.z will be displayed as v0.3y.z (the y in v0.3y.z is always padded to two digits, with a leading zero if needed), while v2.y.z will be displayed as v0.y.z.\n* See the [Community page](https://prometheus.io/community) for how to reach the Prometheus developers and users on various communication channels.\n\n## Contributing\n\nRefer to [CONTRIBUTING.md](https://github.com/prometheus/prometheus/blob/main/CONTRIBUTING.md)\n\n## License\n\nApache License 2.0, see [LICENSE](https://github.com/prometheus/prometheus/blob/main/LICENSE).\n\n[hub]: https://hub.docker.com/r/prom/prometheus/\n[quay]: https://quay.io/repository/prometheus/prometheus\n",
      "stars_today": 19
    },
    {
      "id": 908589694,
      "name": "meeting-minutes",
      "full_name": "Zackriya-Solutions/meeting-minutes",
      "description": "Privacy first, AI meeting assistant with 4x faster Parakeet/Whisper live transcription, speaker diarization, and Ollama summarization built on Rust. 100% local processing. no cloud required. Meetily (Meetly Ai - https://meetily.ai) is the #1 Self-hosted,  Open-source Ai meeting note taker for macOS & Windows.  ",
      "html_url": "https://github.com/Zackriya-Solutions/meeting-minutes",
      "stars": 9465,
      "forks": 821,
      "language": "Rust",
      "topics": [
        "ai",
        "ai-meeting-assistant",
        "llm",
        "local-ai",
        "mac",
        "meeting-minutes",
        "meeting-notes",
        "offline-first",
        "ollama",
        "parakeet",
        "privacy-focused",
        "privacy-tools",
        "rust",
        "self-hosted",
        "speech-to-text",
        "transcription",
        "whisper",
        "whisper-cpp",
        "windows"
      ],
      "created_at": "2024-12-26T12:52:14Z",
      "updated_at": "2026-01-28T01:38:11Z",
      "pushed_at": "2026-01-23T09:38:27Z",
      "open_issues": 121,
      "owner": {
        "login": "Zackriya-Solutions",
        "avatar_url": "https://avatars.githubusercontent.com/u/82556810?v=4"
      },
      "readme": "<div align=\"center\" style=\"border-bottom: none\">\n    <h1>\n        <img src=\"docs/Meetily-6.png\" style=\"border-radius: 10px;\" />\n        <br>\n        Privacy-First AI Meeting Assistant\n    </h1>\n    <a href=\"https://trendshift.io/repositories/13272\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/13272\" alt=\"Zackriya-Solutions%2Fmeeting-minutes | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n    <br>\n    <br>\n    <a href=\"https://github.com/Zackriya-Solutions/meeting-minutes/releases/\"><img src=\"https://img.shields.io/badge/Pre_Release-Link-brightgreen\" alt=\"Pre-Release\"></a>\n    <a href=\"https://github.com/Zackriya-Solutions/meeting-minutes/releases\"><img alt=\"GitHub Repo stars\" src=\"https://img.shields.io/github/stars/zackriya-solutions/meeting-minutes?style=flat\">\n</a>\n <a href=\"https://github.com/Zackriya-Solutions/meeting-minutes/releases\"> <img alt=\"GitHub Downloads (all assets, all releases)\" src=\"https://img.shields.io/github/downloads/zackriya-solutions/meeting-minutes/total?style=plastic\"> </a>\n    <a href=\"https://github.com/Zackriya-Solutions/meeting-minutes/releases\"><img src=\"https://img.shields.io/badge/License-MIT-blue\" alt=\"License\"></a>\n    <a href=\"https://github.com/Zackriya-Solutions/meeting-minutes/releases\"><img src=\"https://img.shields.io/badge/Supported_OS-macOS,_Windows-white\" alt=\"Supported OS\"></a>\n    <a href=\"https://github.com/Zackriya-Solutions/meeting-minutes/releases\"><img alt=\"GitHub Tag\" src=\"https://img.shields.io/github/v/tag/zackriya-solutions/meeting-minutes?include_prereleases&color=yellow\">\n</a>\n    <br>\n    <h3>\n    <br>\n    Open Source â€¢ Privacy-First â€¢ Enterprise-Ready\n    </h3>\n    <p align=\"center\">\n    Get latest <a href=\"https://www.zackriya.com/meetily-subscribe/\"><b>Product updates</b></a> <br><br>\n    <a href=\"https://meetily.ai\"><b>Website</b></a> â€¢\n    <a href=\"https://www.linkedin.com/company/106363062/\"><b>LinkedIn</b></a> â€¢\n    <a href=\"https://discord.gg/crRymMQBFH\"><b>Meetily Discord</b></a> â€¢\n    <a href=\"https://discord.com/invite/vCFJvN4BwJ\"><b>Privacy-First AI</b></a> â€¢\n    <a href=\"https://www.reddit.com/r/meetily/\"><b>Reddit</b></a>\n</p>\n    <p align=\"center\">\n\nA privacy-first AI meeting assistant that captures, transcribes, and summarizes meetings entirely on your infrastructure. Built by expert AI engineers passionate about data sovereignty and open source solutions. Perfect for enterprises that need advanced meeting intelligence without compromising on privacy, compliance, or control.\n\n</p>\n\n<p align=\"center\">\n    <img src=\"docs/meetily_demo.gif\" width=\"650\" alt=\"Meetily Demo\" />\n    <br>\n    <a href=\"https://youtu.be/6FnhSC_eSz8\">View full Demo Video</a>\n</p>\n\n</div>\n\n---\n\n> **ğŸ‰ New: Meetily PRO Available** - Looking for enhanced accuracy and advanced features? Check out our professional-grade solution with custom summary templates, advanced exports (PDF, DOCX), auto-meeting detection, built-in GDPR compliance, and many more. **This Community Edition remains forever free & open source**. [Learn more about PRO â†’](https://meetily.ai/pro/)\n\n---\n\n<details>\n<summary>Table of Contents</summary>\n\n- [Introduction](#introduction)\n- [Why Meetily?](#why-meetily)\n- [Features](#features)\n- [Installation](#installation)\n- [Key Features in Action](#key-features-in-action)\n- [System Architecture](#system-architecture)\n- [For Developers](#for-developers)\n- [Meetily PRO](#meetily-pro)\n- [Contributing](#contributing)\n- [License](#license)\n\n</details>\n\n## Introduction\n\nMeetily is a privacy-first AI meeting assistant that runs entirely on your local machine. It captures your meetings, transcribes them in real-time, and generates summaries, all without sending any data to the cloud. This makes it the perfect solution for professionals and enterprises who need to maintain complete control over their sensitive information.\n\n## Why Meetily?\n\nWhile there are many meeting transcription tools available, this solution stands out by offering:\n\n- **Privacy First:** All processing happens locally on your device.\n- **Cost-Effective:** Uses open-source AI models instead of expensive APIs.\n- **Flexible:** Works offline and supports multiple meeting platforms.\n- **Customizable:** Self-host and modify for your specific needs.\n\n<details>\n<summary>The Privacy Problem</summary>\n\nMeeting AI tools create significant privacy and compliance risks across all sectors:\n\n- **$4.4M average cost per data breach** (IBM 2024)\n- **â‚¬5.88 billion in GDPR fines** issued by 2025\n- **400+ unlawful recording cases** filed in California this year\n\nWhether you're a defense consultant, enterprise executive, legal professional, or healthcare provider, your sensitive discussions shouldn't live on servers you don't control. Cloud meeting tools promise convenience but deliver privacy nightmares with unclear data storage practices and potential unauthorized access.\n\n**Meetily solves this:** Complete data sovereignty on your infrastructure, zero vendor lock-in, and full control over your sensitive conversations.\n\n</details>\n\n## Features\n\n- **Local First:** All processing is done on your machine. No data ever leaves your computer.\n- **Real-time Transcription:** Get a live transcript of your meeting as it happens.\n- **AI-Powered Summaries:** Generate summaries of your meetings using powerful language models.\n- **Multi-Platform:** Works on macOS, Windows, and Linux.\n- **Open Source:** Meetily is open source and free to use.\n- **Flexible AI Provider Support:** Choose from Ollama (local), Claude, Groq, OpenRouter, or use your own OpenAI-compatible endpoint.\n\n## Installation\n\n### ğŸªŸ **Windows**\n\n1. Download the latest `x64-setup.exe` from [Releases](https://github.com/Zackriya-Solutions/meeting-minutes/releases/latest)\n2. Right-click the downloaded file â†’ **Properties** â†’ Check **Unblock** â†’ Click **OK**\n3. Run the installer (if Windows shows a security warning: Click **More info** â†’ **Run anyway**)\n\n### ğŸ **macOS**\n\n1. Download `meetily_0.2.0_aarch64.dmg` from [Releases](https://github.com/Zackriya-Solutions/meeting-minutes/releases/latest)\n2. Open the downloaded `.dmg` file\n3. Drag **Meetily** to your Applications folder\n4. Open **Meetily** from Applications folder\n\n### ğŸ§ **Linux**\n\nBuild from source following our detailed guides:\n\n- [Building on Linux](docs/building_in_linux.md)\n- [General Build Instructions](docs/BUILDING.md)\n\n**Quick start:**\n\n```bash\ngit clone https://github.com/Zackriya-Solutions/meeting-minutes\ncd meeting-minutes/frontend\npnpm install\n./build-gpu.sh\n```\n\n## Key Features in Action\n\n### ğŸ¯ Local Transcription\n\nTranscribe meetings entirely on your device using **Whisper** or **Parakeet** models. No cloud required.\n\n<p align=\"center\">\n    <img src=\"docs/home.png\" width=\"650\" style=\"border-radius: 10px;\" alt=\"Meetily Demo\" />\n</p>\n\n### ğŸ¤– AI-Powered Summaries\n\nGenerate meeting summaries with your choice of AI provider. **Ollama** (local) is recommended, with support for Claude, Groq, OpenRouter, and OpenAI.\n\n<p align=\"center\">\n    <img src=\"docs/summary.png\" width=\"650\" style=\"border-radius: 10px;\" alt=\"Summary generation\" />\n</p>\n\n<p align=\"center\">\n    <img src=\"docs/editor1.png\" width=\"650\" style=\"border-radius: 10px;\" alt=\"Editor Summary generation\" />\n</p>\n\n### ğŸ”’ Privacy-First Design\n\nAll data stays on your machine. Transcription models, recordings, and transcripts are stored locally.\n\n<p align=\"center\">\n    <img src=\"docs/settings.png\" width=\"650\" style=\"border-radius: 10px;\" alt=\"Local Transcription and storage\" />\n</p>\n\n### ğŸŒ Custom OpenAI Endpoint Support\n\nUse your own OpenAI-compatible endpoint for AI summaries. Perfect for organizations with custom AI infrastructure or preferred providers.\n\n<p align=\"center\">\n    <img src=\"docs/custom.png\" width=\"650\" style=\"border-radius: 10px;\" alt=\"Custom OpenAI Endpoint Configuration\" />\n</p>\n\n### ğŸ™ï¸ Professional Audio Mixing\n\nCapture microphone and system audio simultaneously with intelligent ducking and clipping prevention.\n\n<p align=\"center\">\n    <img src=\"docs/audio.png\" width=\"650\" style=\"border-radius: 10px;\" alt=\"Device selection\" />\n</p>\n\n### âš¡ GPU Acceleration\n\nBuilt-in support for hardware acceleration across platforms:\n\n- **macOS**: Apple Silicon (Metal) + CoreML\n- **Windows/Linux**: NVIDIA (CUDA), AMD/Intel (Vulkan)\n\nAutomatically enabled at build time - no configuration needed.\n\n## System Architecture\n\nMeetily is a single, self-contained application built with [Tauri](https://tauri.app/). It uses a Rust-based backend to handle all the core logic, and a Next.js frontend for the user interface.\n\nFor more details, see the [Architecture documentation](docs/architecture.md).\n\n## For Developers\n\nIf you want to contribute to Meetily or build it from source, you'll need to have Rust and Node.js installed. For detailed build instructions, please see the [Building from Source guide](docs/BUILDING.md).\n\n## Meetily Pro\n\n<p align=\"center\">\n    <img src=\"docs/pv2.1.png\" width=\"650\" style=\"border-radius: 10px;\" alt=\"Upcoming version\" />\n</p>\n\n**Meetily PRO** is a professional-grade solution with enhanced accuracy and advanced features for serious users and teams. Built on a different codebase with superior transcription models and enterprise-ready capabilities.\n\n### Key Advantages Over Community Edition:\n\n- **Enhanced Accuracy**: Superior transcription models for professional-grade accuracy\n- **Custom Summary Templates**: Tailor summaries to your specific workflow and needs\n- **Advanced Export Options**: PDF, DOCX, and Markdown exports with formatting\n- **Auto-detect and Join Meetings**: Automatic meeting detection and joining\n- **Speaker Identification**: Distinguish between speakers automatically *(Coming Soon)*\n- **Chat with Meetings**: AI-powered meeting insights and queries *(Coming Soon)*\n- **Calendar Integration**: Seamless integration with your calendar *(Coming Soon)*\n- **Self-Hosted Deployment**: Deploy on your own infrastructure for teams\n- **GDPR Compliance Built-In**: Privacy by design architecture with complete audit trails\n- **Priority Support**: Dedicated support for PRO users\n\n### Who is PRO for?\n\n- **Professionals** who need the highest accuracy for critical meetings\n- **Teams and organizations** (2-100 users) requiring self-hosted deployment\n- **Power users** who need advanced export formats and custom workflows\n- **Compliance-focused organizations** requiring GDPR readiness\n\n> **Note:** Meetily Community Edition remains **free & open source forever** with local transcription, AI summaries, and core features. PRO is a separate professional solution for users who need enhanced accuracy and advanced capabilities.\n\nFor organizations needing 100+ users or managed compliance solutions, explore [Meetily Enterprise](https://meetily.ai/enterprise/).\n\n**Learn more about pricing and features:** [https://meetily.ai/pro/](https://meetily.ai/pro/)\n\n## Contributing\n\nWe welcome contributions from the community! If you have any questions or suggestions, please open an issue or submit a pull request. Please follow the established project structure and guidelines. For more details, refer to the [CONTRIBUTING.md](CONTRIBUTING.md) file.\n\nThanks for all the contributions. Our community is what makes this project possible.\n\n## License\n\nMIT License - Feel free to use this project for your own purposes.\n\n## Acknowledgments\n\n- We borrowed some code from [Whisper.cpp](https://github.com/ggerganov/whisper.cpp).\n- We borrowed some code from [Screenpipe](https://github.com/mediar-ai/screenpipe).\n- We borrowed some code from [transcribe-rs](https://crates.io/crates/transcribe-rs).\n- Thanks to **NVIDIA** for developing the **Parakeet** model.\n- Thanks to [istupakov](https://huggingface.co/istupakov/parakeet-tdt-0.6b-v3-onnx) for providing the **ONNX conversion** of the Parakeet model.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Zackriya-Solutions/meeting-minutes&type=Date)](https://star-history.com/#Zackriya-Solutions/meeting-minutes&Date)\n",
      "stars_today": 19
    },
    {
      "id": 8859474,
      "name": "jadx",
      "full_name": "skylot/jadx",
      "description": "Dex to Java decompiler",
      "html_url": "https://github.com/skylot/jadx",
      "stars": 47053,
      "forks": 5412,
      "language": "Java",
      "topics": [
        "android",
        "decompiler",
        "dex",
        "java"
      ],
      "created_at": "2013-03-18T17:08:21Z",
      "updated_at": "2026-01-28T01:51:38Z",
      "pushed_at": "2026-01-26T20:31:52Z",
      "open_issues": 415,
      "owner": {
        "login": "skylot",
        "avatar_url": "https://avatars.githubusercontent.com/u/118523?v=4"
      },
      "readme": "<img src=\"https://raw.githubusercontent.com/skylot/jadx/master/jadx-gui/src/main/resources/logos/jadx-logo.png\" width=\"64\" align=\"left\" />\n\n## JADX\n\n![Build status](https://img.shields.io/github/actions/workflow/status/skylot/jadx/build-artifacts.yml)\n![GitHub contributors](https://img.shields.io/github/contributors/skylot/jadx)\n![GitHub all releases](https://img.shields.io/github/downloads/skylot/jadx/total)\n![GitHub release (latest by SemVer)](https://img.shields.io/github/downloads/skylot/jadx/latest/total)\n![Latest release](https://img.shields.io/github/release/skylot/jadx.svg)\n[![Maven Central](https://img.shields.io/maven-central/v/io.github.skylot/jadx-core)](https://search.maven.org/search?q=g:io.github.skylot%20AND%20jadx)\n![Java 11+](https://img.shields.io/badge/Java-11%2B-blue)\n[![License](http://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)\n\n**jadx** - Dex to Java decompiler\n\nCommand line and GUI tools for producing Java source code from Android Dex and Apk files\n\n> [!WARNING]\n> Please note that in most cases **jadx** can't decompile all 100% of the code, so errors will occur.<br />\n> Check [Troubleshooting guide](https://github.com/skylot/jadx/wiki/Troubleshooting-Q&A#decompilation-issues) for workarounds.\n\n**Main features:**\n- decompile Dalvik bytecode to Java code from APK, dex, aar, aab and zip files\n- decode `AndroidManifest.xml` and other resources from `resources.arsc`\n- deobfuscator included\n\n**jadx-gui features:**\n- view decompiled code with highlighted syntax\n- jump to declaration\n- find usage\n- full text search\n- smali debugger, check [wiki page](https://github.com/skylot/jadx/wiki/Smali-debugger) for setup and usage\n\nJadx-gui key bindings can be found [here](https://github.com/skylot/jadx/wiki/JADX-GUI-Key-bindings)\n\nSee these features in action here: [jadx-gui features overview](https://github.com/skylot/jadx/wiki/jadx-gui-features-overview)\n\n<img src=\"https://user-images.githubusercontent.com/118523/142730720-839f017e-38db-423e-b53f-39f5f0a0316f.png\" width=\"700\"/>\n\n### Download\n- release\n  from [github: ![Latest release](https://img.shields.io/github/release/skylot/jadx.svg)](https://github.com/skylot/jadx/releases/latest)\n- latest [unstable build ![GitHub commits since tagged version (branch)](https://img.shields.io/github/commits-since/skylot/jadx/latest/master)](https://nightly.link/skylot/jadx/workflows/build-artifacts/master)\n\nAfter download unpack zip file go to `bin` directory and run:\n- `jadx` - command line version\n- `jadx-gui` - UI version\n\nOn Windows run `.bat` files with double-click\\\n**Note:** ensure you have installed Java 11 or later 64-bit version.\nFor Windows, you can download it from [oracle.com](https://www.oracle.com/java/technologies/downloads/#jdk17-windows) (select x64 Installer).\n\n### Install\n- Arch Linux\n  [![Arch Linux package](https://img.shields.io/archlinux/v/extra/any/jadx)](https://archlinux.org/packages/extra/any/jadx/)\n  [![AUR Version](https://img.shields.io/aur/version/jadx-git)](https://aur.archlinux.org/packages/jadx-git)\n  ```bash\n  sudo pacman -S jadx\n  ```\n- macOS\n  [![homebrew version](https://img.shields.io/homebrew/v/jadx)](https://formulae.brew.sh/formula/jadx)\n  ```bash\n  brew install jadx\n  ```\n- Flathub\n  [![Flathub Version](https://img.shields.io/flathub/v/com.github.skylot.jadx)](https://flathub.org/apps/com.github.skylot.jadx)\n  ```bash\n  flatpak install flathub com.github.skylot.jadx\n  ```\n\n### Use jadx as a library\nYou can use jadx in your java projects, check details on [wiki page](https://github.com/skylot/jadx/wiki/Use-jadx-as-a-library)\n\n### Build from source\nJDK 11 or higher must be installed:\n```\ngit clone https://github.com/skylot/jadx.git\ncd jadx\n./gradlew dist\n```\n\n(on Windows, use `gradlew.bat` instead of `./gradlew`)\n\nScripts for run jadx will be placed in `build/jadx/bin`\nand also packed to `build/jadx-<version>.zip`\n\n### Usage\n```\njadx[-gui] [command] [options] <input files> (.apk, .dex, .jar, .class, .smali, .zip, .aar, .arsc, .aab, .xapk, .apkm, .jadx.kts)\ncommands (use '<command> --help' for command options):\n  plugins\t  - manage jadx plugins\n\noptions:\n  -d, --output-dir                              - output directory\n  -ds, --output-dir-src                         - output directory for sources\n  -dr, --output-dir-res                         - output directory for resources\n  -r, --no-res                                  - do not decode resources\n  -s, --no-src                                  - do not decompile source code\n  -j, --threads-count                           - processing threads count, default: 16\n  --single-class                                - decompile a single class, full name, raw or alias\n  --single-class-output                         - file or dir for write if decompile a single class\n  --output-format                               - can be 'java' or 'json', default: java\n  -e, --export-gradle                           - save as gradle project (set '--export-gradle-type' to 'auto')\n  --export-gradle-type                          - Gradle project template for export:\n                                                   'auto' - detect automatically\n                                                   'android-app' - Android Application (apk)\n                                                   'android-library' - Android Library (aar)\n                                                   'simple-java' - simple Java\n  -m, --decompilation-mode                      - code output mode:\n                                                   'auto' - trying best options (default)\n                                                   'restructure' - restore code structure (normal java code)\n                                                   'simple' - simplified instructions (linear, with goto's)\n                                                   'fallback' - raw instructions without modifications\n  --show-bad-code                               - show inconsistent code (incorrectly decompiled)\n  --no-xml-pretty-print                         - do not prettify XML\n  --no-imports                                  - disable use of imports, always write entire package name\n  --no-debug-info                               - disable debug info parsing and processing\n  --add-debug-lines                             - add comments with debug line numbers if available\n  --no-inline-anonymous                         - disable anonymous classes inline\n  --no-inline-methods                           - disable methods inline\n  --no-move-inner-classes                       - disable move inner classes into parent\n  --no-inline-kotlin-lambda                     - disable inline for Kotlin lambdas\n  --no-finally                                  - don't extract finally block\n  --no-restore-switch-over-string               - don't restore switch over string\n  --no-replace-consts                           - don't replace constant value with matching constant field\n  --escape-unicode                              - escape non latin characters in strings (with \\u)\n  --respect-bytecode-access-modifiers           - don't change original access modifiers\n  --mappings-path                               - deobfuscation mappings file or directory. Allowed formats: Tiny and Tiny v2 (both '.tiny'), Enigma (.mapping) or Enigma directory\n  --mappings-mode                               - set mode for handling the deobfuscation mapping file:\n                                                   'read' - just read, user can always save manually (default)\n                                                   'read-and-autosave-every-change' - read and autosave after every change\n                                                   'read-and-autosave-before-closing' - read and autosave before exiting the app or closing the project\n                                                   'ignore' - don't read or save (can be used to skip loading mapping files referenced in the project file)\n  --deobf                                       - activate deobfuscation\n  --deobf-min                                   - min length of name, renamed if shorter, default: 3\n  --deobf-max                                   - max length of name, renamed if longer, default: 64\n  --deobf-whitelist                             - space separated list of classes (full name) and packages (ends with '.*') to exclude from deobfuscation, default: android.support.v4.* android.support.v7.* android.support.v4.os.* android.support.annotation.Px androidx.core.os.* androidx.annotation.Px\n  --deobf-cfg-file                              - deobfuscation mappings file used for JADX auto-generated names (in the JOBF file format), default: same dir and name as input file with '.jobf' extension\n  --deobf-cfg-file-mode                         - set mode for handling the JADX auto-generated names' deobfuscation map file:\n                                                   'read' - read if found, don't save (default)\n                                                   'read-or-save' - read if found, save otherwise (don't overwrite)\n                                                   'overwrite' - don't read, always save\n                                                   'ignore' - don't read and don't save\n  --deobf-res-name-source                       - better name source for resources:\n                                                   'auto' - automatically select best name (default)\n                                                   'resources' - use resources names\n                                                   'code' - use R class fields names\n  --use-source-name-as-class-name-alias         - use source name as class name alias:\n                                                   'always' - always use source name if it's available\n                                                   'if-better' - use source name if it seems better than the current one\n                                                   'never' - never use source name, even if it's available\n  --source-name-repeat-limit                    - allow using source name if it appears less than a limit number, default: 10\n  --use-kotlin-methods-for-var-names            - use kotlin intrinsic methods to rename variables, values: disable, apply, apply-and-hide, default: apply\n  --use-headers-for-detect-resource-extensions  - Use headers for detect resource extensions if resource obfuscated\n  --rename-flags                                - fix options (comma-separated list of):\n                                                   'case' - fix case sensitivity issues (according to --fs-case-sensitive option),\n                                                   'valid' - rename java identifiers to make them valid,\n                                                   'printable' - remove non-printable chars from identifiers,\n                                                  or single 'none' - to disable all renames\n                                                  or single 'all' - to enable all (default)\n  --integer-format                              - how integers are displayed:\n                                                   'auto' - automatically select (default)\n                                                   'decimal' - use decimal\n                                                   'hexadecimal' - use hexadecimal\n  --type-update-limit                           - type update limit count (per one instruction), default: 10\n  --fs-case-sensitive                           - treat filesystem as case sensitive, false by default\n  --cfg                                         - save methods control flow graph to dot file\n  --raw-cfg                                     - save methods control flow graph (use raw instructions)\n  -f, --fallback                                - set '--decompilation-mode' to 'fallback' (deprecated)\n  --use-dx                                      - use dx/d8 to convert java bytecode\n  --comments-level                              - set code comments level, values: error, warn, info, debug, user-only, none, default: info\n  --log-level                                   - set log level, values: quiet, progress, error, warn, info, debug, default: progress\n  -v, --verbose                                 - verbose output (set --log-level to DEBUG)\n  -q, --quiet                                   - turn off output (set --log-level to QUIET)\n  --disable-plugins                             - comma separated list of plugin ids to disable\n  --config <config-ref>                         - load configuration from file, <config-ref> can be:\n                                                   path to '.json' file\n                                                   short name - uses file with this name from config directory\n                                                   'none' - to disable config loading\n  --save-config <config-ref>                    - save current options into configuration file and exit, <config-ref> can be:\n                                                   empty - for default config\n                                                   path to '.json' file\n                                                   short name - file will be saved in config directory\n  --print-files                                 - print files and directories used by jadx (config, cache, temp)\n  --version                                     - print jadx version\n  -h, --help                                    - print this help\n\nPlugin options (-P<name>=<value>):\n  dex-input: Load .dex and .apk files\n    - dex-input.verify-checksum                 - verify dex file checksum before load, values: [yes, no], default: yes\n  java-convert: Convert .class, .jar and .aar files to dex\n    - java-convert.mode                         - convert mode, values: [dx, d8, both], default: both\n    - java-convert.d8-desugar                   - use desugar in d8, values: [yes, no], default: no\n  kotlin-metadata: Use kotlin.Metadata annotation for code generation\n    - kotlin-metadata.class-alias               - rename class alias, values: [yes, no], default: yes\n    - kotlin-metadata.method-args               - rename function arguments, values: [yes, no], default: yes\n    - kotlin-metadata.fields                    - rename fields, values: [yes, no], default: yes\n    - kotlin-metadata.companion                 - rename companion object, values: [yes, no], default: yes\n    - kotlin-metadata.data-class                - add data class modifier, values: [yes, no], default: yes\n    - kotlin-metadata.to-string                 - rename fields using toString, values: [yes, no], default: yes\n    - kotlin-metadata.getters                   - rename simple getters to field names, values: [yes, no], default: yes\n  kotlin-smap: Use kotlin.SourceDebugExtension annotation for rename class alias\n    - kotlin-smap.class-alias-source-dbg        - rename class alias from SourceDebugExtension, values: [yes, no], default: no\n  rename-mappings: various mappings support\n    - rename-mappings.format                    - mapping format, values: [AUTO, TINY_FILE, TINY_2_FILE, ENIGMA_FILE, ENIGMA_DIR, PROGUARD_FILE, SRG_FILE, XSRG_FILE, JAM_FILE, CSRG_FILE, TSRG_FILE, TSRG_2_FILE, INTELLIJ_MIGRATION_MAP_FILE, RECAF_SIMPLE_FILE, JOBF_FILE], default: AUTO\n    - rename-mappings.invert                    - invert mapping on load, values: [yes, no], default: no\n  smali-input: Load .smali files\n    - smali-input.api-level                     - Android API level, default: 27\n\nEnvironment variables:\n  JADX_DISABLE_XML_SECURITY - set to 'true' to disable all security checks for XML files\n  JADX_DISABLE_ZIP_SECURITY - set to 'true' to disable all security checks for zip files\n  JADX_ZIP_MAX_ENTRIES_COUNT - maximum allowed number of entries in zip files (default: 100 000)\n  JADX_CONFIG_DIR - custom config directory, using system by default\n  JADX_CACHE_DIR - custom cache directory, using system by default\n  JADX_TMP_DIR - custom temp directory, using system by default\n\nExamples:\n  jadx -d out classes.dex\n  jadx --rename-flags \"none\" classes.dex\n  jadx --rename-flags \"valid, printable\" classes.dex\n  jadx --log-level ERROR app.apk\n  jadx -Pdex-input.verify-checksum=no app.apk\n```\nThese options also work in jadx-gui running from command line and override options from preferences' dialog\n\nUsage for `plugins` command\n```\nusage: plugins [options]\noptions:\n  -i, --install <locationId>      - install plugin with locationId\n  -j, --install-jar <path-to.jar> - install plugin from jar file\n  -l, --list                      - list installed plugins\n  -a, --available                 - list available plugins from jadx-plugins-list (aka marketplace)\n  -u, --update                    - update installed plugins\n  --uninstall <pluginId>          - uninstall plugin with pluginId\n  --disable <pluginId>            - disable plugin with pluginId\n  --enable <pluginId>             - enable plugin with pluginId\n  --list-all                      - list all plugins including bundled and dropins\n  --list-versions <locationId>    - fetch latest versions of plugin from locationId (will download all artefacts, limited to 10)\n  -h, --help                      - print this help\n```\n\n\n### Troubleshooting\nPlease check wiki page [Troubleshooting Q&A](https://github.com/skylot/jadx/wiki/Troubleshooting-Q&A)\n\n### Contributing\nTo support this project you can:\n  - Post thoughts about new features/optimizations that important to you\n  - Submit decompilation issues, please read before proceed: [Open issue](CONTRIBUTING.md#Open-Issue)\n  - Open pull request, please follow these rules: [Pull Request Process](CONTRIBUTING.md#Pull-Request-Process)\n\n---------------------------------------\n*Licensed under the Apache 2.0 License*\n",
      "stars_today": 18
    },
    {
      "id": 12574344,
      "name": "cobra",
      "full_name": "spf13/cobra",
      "description": "A Commander for modern Go CLI interactions",
      "html_url": "https://github.com/spf13/cobra",
      "stars": 43007,
      "forks": 3046,
      "language": "Go",
      "topics": [
        "cli",
        "cli-app",
        "cobra",
        "cobra-generator",
        "cobra-library",
        "command",
        "command-cobra",
        "command-line",
        "commandline",
        "go",
        "golang",
        "golang-application",
        "golang-library",
        "posix",
        "posix-compliant-flags",
        "subcommands"
      ],
      "created_at": "2013-09-03T20:40:26Z",
      "updated_at": "2026-01-28T01:48:08Z",
      "pushed_at": "2025-12-10T02:18:50Z",
      "open_issues": 344,
      "owner": {
        "login": "spf13",
        "avatar_url": "https://avatars.githubusercontent.com/u/173412?v=4"
      },
      "readme": "<div align=\"center\">\n<a href=\"https://cobra.dev\">\n<img width=\"512\" height=\"535\" alt=\"cobra-logo\" src=\"https://github.com/user-attachments/assets/c8bf9aad-b5ae-41d3-8899-d83baec10af8\" />\n</a>\n</div>\n\nCobra is a library for creating powerful modern CLI applications.\n\n<a href=\"https://cobra.dev\">Visit Cobra.dev for extensive documentation</a> \n\n\nCobra is used in many Go projects such as [Kubernetes](https://kubernetes.io/),\n[Hugo](https://gohugo.io), and [GitHub CLI](https://github.com/cli/cli) to\nname a few. [This list](site/content/projects_using_cobra.md) contains a more extensive list of projects using Cobra.\n\n[![](https://img.shields.io/github/actions/workflow/status/spf13/cobra/test.yml?branch=main&longCache=true&label=Test&logo=github%20actions&logoColor=fff)](https://github.com/spf13/cobra/actions?query=workflow%3ATest)\n[![Go Reference](https://pkg.go.dev/badge/github.com/spf13/cobra.svg)](https://pkg.go.dev/github.com/spf13/cobra)\n[![Go Report Card](https://goreportcard.com/badge/github.com/spf13/cobra)](https://goreportcard.com/report/github.com/spf13/cobra)\n[![Slack](https://img.shields.io/badge/Slack-cobra-brightgreen)](https://gophers.slack.com/archives/CD3LP1199)\n<hr>\n<div align=\"center\" markdown=\"1\">\n   <sup>Supported by:</sup>\n   <br>\n   <br>\n   <a href=\"https://www.warp.dev/cobra\">\n      <img alt=\"Warp sponsorship\" width=\"400\" src=\"https://github.com/user-attachments/assets/ab8dd143-b0fd-4904-bdc5-dd7ecac94eae\">\n   </a>\n\n### [Warp, the AI terminal for devs](https://www.warp.dev/cobra)\n[Try Cobra in Warp today](https://www.warp.dev/cobra)<br>\n\n</div>\n<hr>\n\n# Overview\n\nCobra is a library providing a simple interface to create powerful modern CLI\ninterfaces similar to git & go tools.\n\nCobra provides:\n* Easy subcommand-based CLIs: `app server`, `app fetch`, etc.\n* Fully POSIX-compliant flags (including short & long versions)\n* Nested subcommands\n* Global, local and cascading flags\n* Intelligent suggestions (`app srver`... did you mean `app server`?)\n* Automatic help generation for commands and flags\n* Grouping help for subcommands\n* Automatic help flag recognition of `-h`, `--help`, etc.\n* Automatically generated shell autocomplete for your application (bash, zsh, fish, powershell)\n* Automatically generated man pages for your application\n* Command aliases so you can change things without breaking them\n* The flexibility to define your own help, usage, etc.\n* Optional seamless integration with [viper](https://github.com/spf13/viper) for 12-factor apps\n\n# Concepts\n\nCobra is built on a structure of commands, arguments & flags.\n\n**Commands** represent actions, **Args** are things and **Flags** are modifiers for those actions.\n\nThe best applications read like sentences when used, and as a result, users\nintuitively know how to interact with them.\n\nThe pattern to follow is\n`APPNAME VERB NOUN --ADJECTIVE`\n    or\n`APPNAME COMMAND ARG --FLAG`.\n\nA few good real world examples may better illustrate this point.\n\nIn the following example, 'server' is a command, and 'port' is a flag:\n\n    hugo server --port=1313\n\nIn this command we are telling Git to clone the url bare.\n\n    git clone URL --bare\n\n## Commands\n\nCommand is the central point of the application. Each interaction that\nthe application supports will be contained in a Command. A command can\nhave children commands and optionally run an action.\n\nIn the example above, 'server' is the command.\n\n[More about cobra.Command](https://pkg.go.dev/github.com/spf13/cobra#Command)\n\n## Flags\n\nA flag is a way to modify the behavior of a command. Cobra supports\nfully POSIX-compliant flags as well as the Go [flag package](https://golang.org/pkg/flag/).\nA Cobra command can define flags that persist through to children commands\nand flags that are only available to that command.\n\nIn the example above, 'port' is the flag.\n\nFlag functionality is provided by the [pflag\nlibrary](https://github.com/spf13/pflag), a fork of the flag standard library\nwhich maintains the same interface while adding POSIX compliance.\n\n# Installing\nUsing Cobra is easy. First, use `go get` to install the latest version\nof the library.\n\n```\ngo get -u github.com/spf13/cobra@latest\n```\n\nNext, include Cobra in your application:\n\n```go\nimport \"github.com/spf13/cobra\"\n```\n\n# Usage\n`cobra-cli` is a command line program to generate cobra applications and command files.\nIt will bootstrap your application scaffolding to rapidly\ndevelop a Cobra-based application. It is the easiest way to incorporate Cobra into your application.\n\nIt can be installed by running:\n\n```\ngo install github.com/spf13/cobra-cli@latest\n```\n\nFor complete details on using the Cobra-CLI generator, please read [The Cobra Generator README](https://github.com/spf13/cobra-cli/blob/main/README.md)\n\nFor complete details on using the Cobra library, please read [The Cobra User Guide](site/content/user_guide.md).\n\n# License\n\nCobra is released under the Apache 2.0 license. See [LICENSE.txt](LICENSE.txt)\n",
      "stars_today": 17
    },
    {
      "id": 6296790,
      "name": "spring-boot",
      "full_name": "spring-projects/spring-boot",
      "description": "Spring Boot helps you to create Spring-powered, production-grade applications and services with absolute minimum fuss.",
      "html_url": "https://github.com/spring-projects/spring-boot",
      "stars": 79778,
      "forks": 41832,
      "language": "Java",
      "topics": [
        "framework",
        "java",
        "spring",
        "spring-boot"
      ],
      "created_at": "2012-10-19T15:02:57Z",
      "updated_at": "2026-01-28T00:28:27Z",
      "pushed_at": "2026-01-27T12:43:59Z",
      "open_issues": 480,
      "owner": {
        "login": "spring-projects",
        "avatar_url": "https://avatars.githubusercontent.com/u/317776?v=4"
      },
      "readme": "= Spring Boot image:https://github.com/spring-projects/spring-boot/actions/workflows/build-and-deploy-snapshot.yml/badge.svg?branch=main[\"Build Status\", link=\"https://github.com/spring-projects/spring-boot/actions/workflows/build-and-deploy-snapshot.yml?query=branch%3Amain\"] image:https://img.shields.io/badge/Revved%20up%20by-Develocity-06A0CE?logo=Gradle&labelColor=02303A[\"Revved up by Develocity\", link=\"https://ge.spring.io/scans?&search.rootProjectNames=Spring%20Boot%20Build&search.rootProjectNames=spring-boot-build\"]\n\n:docs: https://docs.spring.io/spring-boot\n:github: https://github.com/spring-projects/spring-boot\n\nSpring Boot helps you to create Spring-powered, production-grade applications and services with absolute minimum fuss.\nIt takes an opinionated view of the Spring platform so that new and existing users can quickly get to the bits they need.\n\nYou can use Spring Boot to create stand-alone Java applications that can be started using `java -jar` or more traditional WAR deployments.\nWe also provide a command-line tool that runs Spring scripts.\n\nOur primary goals are:\n\n* Provide a radically faster and widely accessible getting started experience for all Spring development.\n* Be opinionated, but get out of the way quickly as requirements start to diverge from the defaults.\n* Provide a range of non-functional features common to large classes of projects (for example, embedded servers, security, metrics, health checks, externalized configuration).\n* Absolutely no code generation and no requirement for XML configuration.\n\n\n\n== Installation and Getting Started\n\nThe {docs}[reference documentation] includes detailed {docs}/installing.html[installation instructions] as well as a comprehensive {docs}/tutorial/first-application/index.html[``getting started``] guide.\n\nHere is a quick teaser of a complete Spring Boot application in Java:\n\n[source,java]\n----\nimport org.springframework.boot.*;\nimport org.springframework.boot.autoconfigure.*;\nimport org.springframework.web.bind.annotation.*;\n\n@RestController\n@SpringBootApplication\npublic class Example {\n\n\t@RequestMapping(\"/\")\n\tString home() {\n\t\treturn \"Hello World!\";\n\t}\n\n\tpublic static void main(String[] args) {\n\t\tSpringApplication.run(Example.class, args);\n\t}\n\n}\n----\n\n\n\n== Getting Help\n\nAre you having trouble with Spring Boot? We want to help!\n\n* Check the {docs}/[reference documentation], especially the {docs}/how-to/index.html[How-to's] -- they provide solutions to the most common questions.\n* Learn the Spring basics -- Spring Boot builds on many other Spring projects; check the https://spring.io[spring.io] website for a wealth of reference documentation.\n  If you are new to Spring, try one of the https://spring.io/guides[guides].\n* If you are upgrading, read the {github}/wiki[release notes] for upgrade instructions and \"new and noteworthy\" features.\n* Ask a question -- we monitor https://stackoverflow.com[stackoverflow.com] for questions tagged with https://stackoverflow.com/tags/spring-boot[`spring-boot`].\n* Report bugs with Spring Boot at {github}/issues[github.com/spring-projects/spring-boot/issues].\n\n\n\n== Contributing\n\nWe welcome contributions of all kinds!\nPlease read our link:CONTRIBUTING.adoc[contribution guidelines] before submitting a pull request.\n\n\n\n== Reporting Issues\n\nSpring Boot uses GitHub's integrated issue tracking system to record bugs and feature requests.\nIf you want to raise an issue, please follow the recommendations below:\n\n* Before you log a bug, please search the {github}/issues[issue tracker] to see if someone has already reported the problem.\n* If the issue doesn't already exist, {github}/issues/new[create a new issue].\n* Please provide as much information as possible with the issue report.\nWe like to know the Spring Boot version, operating system, and JVM version you're using.\n* If you need to paste code or include a stack trace, use Markdown.\n+++```+++ escapes before and after your text.\n* If possible, try to create a test case or project that replicates the problem and attach it to the issue.\n\n\n\n== Building from Source\n\nYou don't need to build from source to use Spring Boot.\nIf you want to try out the latest and greatest, Spring Boot can be built and published to your local Maven cache using the https://docs.gradle.org/current/userguide/gradle_wrapper.html[Gradle wrapper].\nYou also need JDK 25.\n\n[source,shell]\n----\n$ ./gradlew publishToMavenLocal\n----\n\nThis command builds all modules and publishes them to your local Maven cache.\nIt won't run any of the tests.\nIf you want to build everything, use the `build` task:\n\n[source,shell]\n----\n$ ./gradlew build\n----\n\n\n\n== Guides\n\nThe https://spring.io/[spring.io] site contains several guides that show how to use Spring Boot step-by-step:\n\n* https://spring.io/guides/gs/spring-boot/[Building an Application with Spring Boot] is an introductory guide that shows you how to create an application, run it, and add some management services.\n* https://spring.io/guides/gs/actuator-service/[Building a RESTful Web Service with Spring Boot Actuator] is a guide to creating a REST web service and also shows how the server can be configured.\n\n\n\n== License\n\nSpring Boot is Open Source software released under the https://www.apache.org/licenses/LICENSE-2.0.html[Apache 2.0 license].\n",
      "stars_today": 16
    },
    {
      "id": 15452919,
      "name": "go-ethereum",
      "full_name": "ethereum/go-ethereum",
      "description": "Go implementation of the Ethereum protocol",
      "html_url": "https://github.com/ethereum/go-ethereum",
      "stars": 50737,
      "forks": 21756,
      "language": "Go",
      "topics": [
        "blockchain",
        "ethereum",
        "geth",
        "go",
        "p2p"
      ],
      "created_at": "2013-12-26T13:05:46Z",
      "updated_at": "2026-01-28T02:02:01Z",
      "pushed_at": "2026-01-28T01:51:16Z",
      "open_issues": 365,
      "owner": {
        "login": "ethereum",
        "avatar_url": "https://avatars.githubusercontent.com/u/6250754?v=4"
      },
      "readme": "## Go Ethereum\n\nGolang execution layer implementation of the Ethereum protocol.\n\n[![API Reference](\nhttps://pkg.go.dev/badge/github.com/ethereum/go-ethereum\n)](https://pkg.go.dev/github.com/ethereum/go-ethereum?tab=doc)\n[![Go Report Card](https://goreportcard.com/badge/github.com/ethereum/go-ethereum)](https://goreportcard.com/report/github.com/ethereum/go-ethereum)\n[![Travis](https://app.travis-ci.com/ethereum/go-ethereum.svg?branch=master)](https://app.travis-ci.com/github/ethereum/go-ethereum)\n[![Discord](https://img.shields.io/badge/discord-join%20chat-blue.svg)](https://discord.gg/nthXNEv)\n[![Twitter](https://img.shields.io/twitter/follow/go_ethereum)](https://x.com/go_ethereum)\n\nAutomated builds are available for stable releases and the unstable master branch. Binary\narchives are published at https://geth.ethereum.org/downloads/.\n\n## Building the source\n\nFor prerequisites and detailed build instructions please read the [Installation Instructions](https://geth.ethereum.org/docs/getting-started/installing-geth).\n\nBuilding `geth` requires both a Go (version 1.23 or later) and a C compiler. You can install\nthem using your favourite package manager. Once the dependencies are installed, run\n\n```shell\nmake geth\n```\n\nor, to build the full suite of utilities:\n\n```shell\nmake all\n```\n\n## Executables\n\nThe go-ethereum project comes with several wrappers/executables found in the `cmd`\ndirectory.\n\n|  Command   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| :--------: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **`geth`** | Our main Ethereum CLI client. It is the entry point into the Ethereum network (main-, test- or private net), capable of running as a full node (default), archive node (retaining all historical state) or a light node (retrieving data live). It can be used by other processes as a gateway into the Ethereum network via JSON RPC endpoints exposed on top of HTTP, WebSocket and/or IPC transports. `geth --help` and the [CLI page](https://geth.ethereum.org/docs/fundamentals/command-line-options) for command line options. |\n|   `clef`   | Stand-alone signing tool, which can be used as a backend signer for `geth`.                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n|  `devp2p`  | Utilities to interact with nodes on the networking layer, without running a full blockchain.                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n|  `abigen`  | Source code generator to convert Ethereum contract definitions into easy-to-use, compile-time type-safe Go packages. It operates on plain [Ethereum contract ABIs](https://docs.soliditylang.org/en/develop/abi-spec.html) with expanded functionality if the contract bytecode is also available. However, it also accepts Solidity source files, making development much more streamlined. Please see our [Native DApps](https://geth.ethereum.org/docs/developers/dapp-developer/native-bindings) page for details.                                  |\n|   `evm`    | Developer utility version of the EVM (Ethereum Virtual Machine) that is capable of running bytecode snippets within a configurable environment and execution mode. Its purpose is to allow isolated, fine-grained debugging of EVM opcodes (e.g. `evm --code 60ff60ff --debug run`).                                                                                                                                                                                                                                               |\n| `rlpdump`  | Developer utility tool to convert binary RLP ([Recursive Length Prefix](https://ethereum.org/en/developers/docs/data-structures-and-encoding/rlp)) dumps (data encoding used by the Ethereum protocol both network as well as consensus wise) to user-friendlier hierarchical representation (e.g. `rlpdump --hex CE0183FFFFFFC4C304050583616263`).                                                                                                                                                                                |\n\n## Running `geth`\n\nGoing through all the possible command line flags is out of scope here (please consult our\n[CLI Wiki page](https://geth.ethereum.org/docs/fundamentals/command-line-options)),\nbut we've enumerated a few common parameter combos to get you up to speed quickly\non how you can run your own `geth` instance.\n\n### Hardware Requirements\n\nMinimum:\n\n* CPU with 4+ cores\n* 8GB RAM\n* 1TB free storage space to sync the Mainnet\n* 8 MBit/sec download Internet service\n\nRecommended:\n\n* Fast CPU with 8+ cores\n* 16GB+ RAM\n* High-performance SSD with at least 1TB of free space\n* 25+ MBit/sec download Internet service\n\n### Full node on the main Ethereum network\n\nBy far the most common scenario is people wanting to simply interact with the Ethereum\nnetwork: create accounts; transfer funds; deploy and interact with contracts. For this\nparticular use case, the user doesn't care about years-old historical data, so we can\nsync quickly to the current state of the network. To do so:\n\n```shell\n$ geth console\n```\n\nThis command will:\n * Start `geth` in snap sync mode (default, can be changed with the `--syncmode` flag),\n   causing it to download more data in exchange for avoiding processing the entire history\n   of the Ethereum network, which is very CPU intensive.\n * Start the built-in interactive [JavaScript console](https://geth.ethereum.org/docs/interacting-with-geth/javascript-console),\n   (via the trailing `console` subcommand) through which you can interact using [`web3` methods](https://github.com/ChainSafe/web3.js/blob/0.20.7/DOCUMENTATION.md) \n   (note: the `web3` version bundled within `geth` is very old, and not up to date with official docs),\n   as well as `geth`'s own [management APIs](https://geth.ethereum.org/docs/interacting-with-geth/rpc).\n   This tool is optional and if you leave it out you can always attach it to an already running\n   `geth` instance with `geth attach`.\n\n### A Full node on the Holesky test network\n\nTransitioning towards developers, if you'd like to play around with creating Ethereum\ncontracts, you almost certainly would like to do that without any real money involved until\nyou get the hang of the entire system. In other words, instead of attaching to the main\nnetwork, you want to join the **test** network with your node, which is fully equivalent to\nthe main network, but with play-Ether only.\n\n```shell\n$ geth --holesky console\n```\n\nThe `console` subcommand has the same meaning as above and is equally\nuseful on the testnet too.\n\nSpecifying the `--holesky` flag, however, will reconfigure your `geth` instance a bit:\n\n * Instead of connecting to the main Ethereum network, the client will connect to the Holesky \n   test network, which uses different P2P bootnodes, different network IDs and genesis\n   states.\n * Instead of using the default data directory (`~/.ethereum` on Linux for example), `geth`\n   will nest itself one level deeper into a `holesky` subfolder (`~/.ethereum/holesky` on\n   Linux). Note, on OSX and Linux this also means that attaching to a running testnet node\n   requires the use of a custom endpoint since `geth attach` will try to attach to a\n   production node endpoint by default, e.g.,\n   `geth attach <datadir>/holesky/geth.ipc`. Windows users are not affected by\n   this.\n\n*Note: Although some internal protective measures prevent transactions from\ncrossing over between the main network and test network, you should always\nuse separate accounts for play and real money. Unless you manually move\naccounts, `geth` will by default correctly separate the two networks and will not make any\naccounts available between them.*\n\n### Configuration\n\nAs an alternative to passing the numerous flags to the `geth` binary, you can also pass a\nconfiguration file via:\n\n```shell\n$ geth --config /path/to/your_config.toml\n```\n\nTo get an idea of how the file should look like you can use the `dumpconfig` subcommand to\nexport your existing configuration:\n\n```shell\n$ geth --your-favourite-flags dumpconfig\n```\n\n#### Docker quick start\n\nOne of the quickest ways to get Ethereum up and running on your machine is by using\nDocker:\n\n```shell\ndocker run -d --name ethereum-node -v /Users/alice/ethereum:/root \\\n           -p 8545:8545 -p 30303:30303 \\\n           ethereum/client-go\n```\n\nThis will start `geth` in snap-sync mode with a DB memory allowance of 1GB, as the\nabove command does.  It will also create a persistent volume in your home directory for\nsaving your blockchain as well as map the default ports. There is also an `alpine` tag\navailable for a slim version of the image.\n\nDo not forget `--http.addr 0.0.0.0`, if you want to access RPC from other containers\nand/or hosts. By default, `geth` binds to the local interface and RPC endpoints are not\naccessible from the outside.\n\n### Programmatically interfacing `geth` nodes\n\nAs a developer, sooner rather than later you'll want to start interacting with `geth` and the\nEthereum network via your own programs and not manually through the console. To aid\nthis, `geth` has built-in support for a JSON-RPC based APIs ([standard APIs](https://ethereum.org/en/developers/docs/apis/json-rpc/)\nand [`geth` specific APIs](https://geth.ethereum.org/docs/interacting-with-geth/rpc)).\nThese can be exposed via HTTP, WebSockets and IPC (UNIX sockets on UNIX based\nplatforms, and named pipes on Windows).\n\nThe IPC interface is enabled by default and exposes all the APIs supported by `geth`,\nwhereas the HTTP and WS interfaces need to manually be enabled and only expose a\nsubset of APIs due to security reasons. These can be turned on/off and configured as\nyou'd expect.\n\nHTTP based JSON-RPC API options:\n\n  * `--http` Enable the HTTP-RPC server\n  * `--http.addr` HTTP-RPC server listening interface (default: `localhost`)\n  * `--http.port` HTTP-RPC server listening port (default: `8545`)\n  * `--http.api` API's offered over the HTTP-RPC interface (default: `eth,net,web3`)\n  * `--http.corsdomain` Comma separated list of domains from which to accept cross-origin requests (browser enforced)\n  * `--ws` Enable the WS-RPC server\n  * `--ws.addr` WS-RPC server listening interface (default: `localhost`)\n  * `--ws.port` WS-RPC server listening port (default: `8546`)\n  * `--ws.api` API's offered over the WS-RPC interface (default: `eth,net,web3`)\n  * `--ws.origins` Origins from which to accept WebSocket requests\n  * `--ipcdisable` Disable the IPC-RPC server\n  * `--ipcpath` Filename for IPC socket/pipe within the datadir (explicit paths escape it)\n\nYou'll need to use your own programming environments' capabilities (libraries, tools, etc) to\nconnect via HTTP, WS or IPC to a `geth` node configured with the above flags and you'll\nneed to speak [JSON-RPC](https://www.jsonrpc.org/specification) on all transports. You\ncan reuse the same connection for multiple requests!\n\n**Note: Please understand the security implications of opening up an HTTP/WS based\ntransport before doing so! Hackers on the internet are actively trying to subvert\nEthereum nodes with exposed APIs! Further, all browser tabs can access locally\nrunning web servers, so malicious web pages could try to subvert locally available\nAPIs!**\n\n### Operating a private network\n\nMaintaining your own private network is more involved as a lot of configurations taken for\ngranted in the official networks need to be manually set up.\n\nUnfortunately since [the Merge](https://ethereum.org/en/roadmap/merge/) it is no longer possible\nto easily set up a network of geth nodes without also setting up a corresponding beacon chain.\n\nThere are three different solutions depending on your use case:\n\n  * If you are looking for a simple way to test smart contracts from go in your CI, you can use the [Simulated Backend](https://geth.ethereum.org/docs/developers/dapp-developer/native-bindings#blockchain-simulator).\n  * If you want a convenient single node environment for testing, you can use our [Dev Mode](https://geth.ethereum.org/docs/developers/dapp-developer/dev-mode).\n  * If you are looking for a multiple node test network, you can set one up quite easily with [Kurtosis](https://geth.ethereum.org/docs/fundamentals/kurtosis).\n\n## Contribution\n\nThank you for considering helping out with the source code! We welcome contributions\nfrom anyone on the internet, and are grateful for even the smallest of fixes!\n\nIf you'd like to contribute to go-ethereum, please fork, fix, commit and send a pull request\nfor the maintainers to review and merge into the main code base. If you wish to submit\nmore complex changes though, please check up with the core devs first on [our Discord Server](https://discord.gg/invite/nthXNEv)\nto ensure those changes are in line with the general philosophy of the project and/or get\nsome early feedback which can make both your efforts much lighter as well as our review\nand merge procedures quick and simple.\n\nPlease make sure your contributions adhere to our coding guidelines:\n\n * Code must adhere to the official Go [formatting](https://golang.org/doc/effective_go.html#formatting)\n   guidelines (i.e. uses [gofmt](https://golang.org/cmd/gofmt/)).\n * Code must be documented adhering to the official Go [commentary](https://golang.org/doc/effective_go.html#commentary)\n   guidelines.\n * Pull requests need to be based on and opened against the `master` branch.\n * Commit messages should be prefixed with the package(s) they modify.\n   * E.g. \"eth, rpc: make trace configs optional\"\n\nPlease see the [Developers' Guide](https://geth.ethereum.org/docs/developers/geth-developer/dev-guide)\nfor more details on configuring your environment, managing project dependencies, and\ntesting procedures.\n\n### Contributing to geth.ethereum.org\n\nFor contributions to the [go-ethereum website](https://geth.ethereum.org), please checkout and raise pull requests against the `website` branch.\nFor more detailed instructions please see the `website` branch [README](https://github.com/ethereum/go-ethereum/tree/website#readme) or the \n[contributing](https://geth.ethereum.org/docs/developers/geth-developer/contributing) page of the website.\n\n## License\n\nThe go-ethereum library (i.e. all code outside of the `cmd` directory) is licensed under the\n[GNU Lesser General Public License v3.0](https://www.gnu.org/licenses/lgpl-3.0.en.html),\nalso included in our repository in the `COPYING.LESSER` file.\n\nThe go-ethereum binaries (i.e. all code inside of the `cmd` directory) are licensed under the\n[GNU General Public License v3.0](https://www.gnu.org/licenses/gpl-3.0.en.html), also\nincluded in our repository in the `COPYING` file.\n",
      "stars_today": 15
    },
    {
      "id": 359952601,
      "name": "pgvector",
      "full_name": "pgvector/pgvector",
      "description": "Open-source vector similarity search for Postgres",
      "html_url": "https://github.com/pgvector/pgvector",
      "stars": 19506,
      "forks": 1042,
      "language": "C",
      "topics": [
        "approximate-nearest-neighbor-search",
        "nearest-neighbor-search"
      ],
      "created_at": "2021-04-20T21:13:52Z",
      "updated_at": "2026-01-28T01:10:30Z",
      "pushed_at": "2026-01-22T00:41:05Z",
      "open_issues": 13,
      "owner": {
        "login": "pgvector",
        "avatar_url": "https://avatars.githubusercontent.com/u/98363230?v=4"
      },
      "readme": "# pgvector\n\nOpen-source vector similarity search for Postgres\n\nStore your vectors with the rest of your data. Supports:\n\n- exact and approximate nearest neighbor search\n- single-precision, half-precision, binary, and sparse vectors\n- L2 distance, inner product, cosine distance, L1 distance, Hamming distance, and Jaccard distance\n- any [language](#languages) with a Postgres client\n\nPlus [ACID](https://en.wikipedia.org/wiki/ACID) compliance, point-in-time recovery, JOINs, and all of the other [great features](https://www.postgresql.org/about/) of Postgres\n\n[![Build Status](https://github.com/pgvector/pgvector/actions/workflows/build.yml/badge.svg)](https://github.com/pgvector/pgvector/actions)\n\n## Installation\n\n### Linux and Mac\n\nCompile and install the extension (supports Postgres 13+)\n\n```sh\ncd /tmp\ngit clone --branch v0.8.1 https://github.com/pgvector/pgvector.git\ncd pgvector\nmake\nmake install # may need sudo\n```\n\nSee the [installation notes](#installation-notes---linux-and-mac) if you run into issues\n\nYou can also install it with [Docker](#docker), [Homebrew](#homebrew), [PGXN](#pgxn), [APT](#apt), [Yum](#yum), [pkg](#pkg), [APK](#apk), or [conda-forge](#conda-forge), and it comes preinstalled with [Postgres.app](#postgresapp) and many [hosted providers](#hosted-postgres). There are also instructions for [GitHub Actions](https://github.com/pgvector/setup-pgvector).\n\n### Windows\n\nEnsure [C++ support in Visual Studio](https://learn.microsoft.com/en-us/cpp/build/building-on-the-command-line?view=msvc-170#download-and-install-the-tools) is installed and run `x64 Native Tools Command Prompt for VS [version]` as administrator. Then use `nmake` to build:\n\n```cmd\nset \"PGROOT=C:\\Program Files\\PostgreSQL\\18\"\ncd %TEMP%\ngit clone --branch v0.8.1 https://github.com/pgvector/pgvector.git\ncd pgvector\nnmake /F Makefile.win\nnmake /F Makefile.win install\n```\n\nSee the [installation notes](#installation-notes---windows) if you run into issues\n\nYou can also install it with [Docker](#docker) or [conda-forge](#conda-forge).\n\n## Getting Started\n\nEnable the extension (do this once in each database where you want to use it)\n\n```tsql\nCREATE EXTENSION vector;\n```\n\nCreate a vector column with 3 dimensions\n\n```sql\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(3));\n```\n\nInsert vectors\n\n```sql\nINSERT INTO items (embedding) VALUES ('[1,2,3]'), ('[4,5,6]');\n```\n\nGet the nearest neighbors by L2 distance\n\n```sql\nSELECT * FROM items ORDER BY embedding <-> '[3,1,2]' LIMIT 5;\n```\n\nAlso supports inner product (`<#>`), cosine distance (`<=>`), and L1 distance (`<+>`)\n\nNote: `<#>` returns the negative inner product since Postgres only supports `ASC` order index scans on operators\n\n## Storing\n\nCreate a new table with a vector column\n\n```sql\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(3));\n```\n\nOr add a vector column to an existing table\n\n```sql\nALTER TABLE items ADD COLUMN embedding vector(3);\n```\n\nAlso supports [half-precision](#half-precision-vectors), [binary](#binary-vectors), and [sparse](#sparse-vectors) vectors\n\nInsert vectors\n\n```sql\nINSERT INTO items (embedding) VALUES ('[1,2,3]'), ('[4,5,6]');\n```\n\nOr load vectors in bulk using `COPY` ([example](https://github.com/pgvector/pgvector-python/blob/master/examples/loading/example.py))\n\n```sql\nCOPY items (embedding) FROM STDIN WITH (FORMAT BINARY);\n```\n\nUpsert vectors\n\n```sql\nINSERT INTO items (id, embedding) VALUES (1, '[1,2,3]'), (2, '[4,5,6]')\n    ON CONFLICT (id) DO UPDATE SET embedding = EXCLUDED.embedding;\n```\n\nUpdate vectors\n\n```sql\nUPDATE items SET embedding = '[1,2,3]' WHERE id = 1;\n```\n\nDelete vectors\n\n```sql\nDELETE FROM items WHERE id = 1;\n```\n\n## Querying\n\nGet the nearest neighbors to a vector\n\n```sql\nSELECT * FROM items ORDER BY embedding <-> '[3,1,2]' LIMIT 5;\n```\n\nSupported distance functions are:\n\n- `<->` - L2 distance\n- `<#>` - (negative) inner product\n- `<=>` - cosine distance\n- `<+>` - L1 distance\n- `<~>` - Hamming distance (binary vectors)\n- `<%>` - Jaccard distance (binary vectors)\n\nGet the nearest neighbors to a row\n\n```sql\nSELECT * FROM items WHERE id != 1 ORDER BY embedding <-> (SELECT embedding FROM items WHERE id = 1) LIMIT 5;\n```\n\nGet rows within a certain distance\n\n```sql\nSELECT * FROM items WHERE embedding <-> '[3,1,2]' < 5;\n```\n\nNote: Combine with `ORDER BY` and `LIMIT` to use an index\n\n#### Distances\n\nGet the distance\n\n```sql\nSELECT embedding <-> '[3,1,2]' AS distance FROM items;\n```\n\nFor inner product, multiply by -1 (since `<#>` returns the negative inner product)\n\n```tsql\nSELECT (embedding <#> '[3,1,2]') * -1 AS inner_product FROM items;\n```\n\nFor cosine similarity, use 1 - cosine distance\n\n```sql\nSELECT 1 - (embedding <=> '[3,1,2]') AS cosine_similarity FROM items;\n```\n\n#### Aggregates\n\nAverage vectors\n\n```sql\nSELECT AVG(embedding) FROM items;\n```\n\nAverage groups of vectors\n\n```sql\nSELECT category_id, AVG(embedding) FROM items GROUP BY category_id;\n```\n\n## Indexing\n\nBy default, pgvector performs exact nearest neighbor search, which provides perfect recall.\n\nYou can add an index to use approximate nearest neighbor search, which trades some recall for speed. Unlike typical indexes, you will see different results for queries after adding an approximate index.\n\nSupported index types are:\n\n- [HNSW](#hnsw)\n- [IVFFlat](#ivfflat)\n\n## HNSW\n\nAn HNSW index creates a multilayer graph. It has better query performance than IVFFlat (in terms of speed-recall tradeoff), but has slower build times and uses more memory. Also, an index can be created without any data in the table since there isnâ€™t a training step like IVFFlat.\n\nAdd an index for each distance function you want to use.\n\nL2 distance\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_l2_ops);\n```\n\nNote: Use `halfvec_l2_ops` for `halfvec` and `sparsevec_l2_ops` for `sparsevec` (and similar with the other distance functions)\n\nInner product\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_ip_ops);\n```\n\nCosine distance\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_cosine_ops);\n```\n\nL1 distance\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_l1_ops);\n```\n\nHamming distance\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding bit_hamming_ops);\n```\n\nJaccard distance\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding bit_jaccard_ops);\n```\n\nSupported types are:\n\n- `vector` - up to 2,000 dimensions\n- `halfvec` - up to 4,000 dimensions\n- `bit` - up to 64,000 dimensions\n- `sparsevec` - up to 1,000 non-zero elements\n\n### Index Options\n\nSpecify HNSW parameters\n\n- `m` - the max number of connections per layer (16 by default)\n- `ef_construction` - the size of the dynamic candidate list for constructing the graph (64 by default)\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_l2_ops) WITH (m = 16, ef_construction = 64);\n```\n\nA higher value of `ef_construction` provides better recall at the cost of index build time / insert speed.\n\n### Query Options\n\nSpecify the size of the dynamic candidate list for search (40 by default)\n\n```sql\nSET hnsw.ef_search = 100;\n```\n\nA higher value provides better recall at the cost of speed.\n\nUse `SET LOCAL` inside a transaction to set it for a single query\n\n```sql\nBEGIN;\nSET LOCAL hnsw.ef_search = 100;\nSELECT ...\nCOMMIT;\n```\n\n### Index Build Time\n\nIndexes build significantly faster when the graph fits into `maintenance_work_mem`\n\n```sql\nSET maintenance_work_mem = '8GB';\n```\n\nA notice is shown when the graph no longer fits\n\n```text\nNOTICE:  hnsw graph no longer fits into maintenance_work_mem after 100000 tuples\nDETAIL:  Building will take significantly more time.\nHINT:  Increase maintenance_work_mem to speed up builds.\n```\n\nNote: Do not set `maintenance_work_mem` so high that it exhausts the memory on the server\n\nLike other index types, itâ€™s faster to create an index after loading your initial data\n\nYou can also speed up index creation by increasing the number of parallel workers (2 by default)\n\n```sql\nSET max_parallel_maintenance_workers = 7; -- plus leader\n```\n\nFor a large number of workers, you may need to increase `max_parallel_workers` (8 by default)\n\nThe [index options](#index-options) also have a significant impact on build time (use the defaults unless seeing low recall)\n\n### Indexing Progress\n\nCheck [indexing progress](https://www.postgresql.org/docs/current/progress-reporting.html#CREATE-INDEX-PROGRESS-REPORTING)\n\n```sql\nSELECT phase, round(100.0 * blocks_done / nullif(blocks_total, 0), 1) AS \"%\" FROM pg_stat_progress_create_index;\n```\n\nThe phases for HNSW are:\n\n1. `initializing`\n2. `loading tuples`\n\n## IVFFlat\n\nAn IVFFlat index divides vectors into lists, and then searches a subset of those lists that are closest to the query vector. It has faster build times and uses less memory than HNSW, but has lower query performance (in terms of speed-recall tradeoff).\n\nThree keys to achieving good recall are:\n\n1. Create the index *after* the table has some data\n2. Choose an appropriate number of lists - a good place to start is `rows / 1000` for up to 1M rows and `sqrt(rows)` for over 1M rows\n3. When querying, specify an appropriate number of [probes](#query-options) (higher is better for recall, lower is better for speed) - a good place to start is `sqrt(lists)`\n\nAdd an index for each distance function you want to use.\n\nL2 distance\n\n```sql\nCREATE INDEX ON items USING ivfflat (embedding vector_l2_ops) WITH (lists = 100);\n```\n\nNote: Use `halfvec_l2_ops` for `halfvec` (and similar with the other distance functions)\n\nInner product\n\n```sql\nCREATE INDEX ON items USING ivfflat (embedding vector_ip_ops) WITH (lists = 100);\n```\n\nCosine distance\n\n```sql\nCREATE INDEX ON items USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n```\n\nHamming distance\n\n```sql\nCREATE INDEX ON items USING ivfflat (embedding bit_hamming_ops) WITH (lists = 100);\n```\n\nSupported types are:\n\n- `vector` - up to 2,000 dimensions\n- `halfvec` - up to 4,000 dimensions\n- `bit` - up to 64,000 dimensions\n\n### Query Options\n\nSpecify the number of probes (1 by default)\n\n```sql\nSET ivfflat.probes = 10;\n```\n\nA higher value provides better recall at the cost of speed, and it can be set to the number of lists for exact nearest neighbor search (at which point the planner wonâ€™t use the index)\n\nUse `SET LOCAL` inside a transaction to set it for a single query\n\n```sql\nBEGIN;\nSET LOCAL ivfflat.probes = 10;\nSELECT ...\nCOMMIT;\n```\n\n### Index Build Time\n\nSpeed up index creation on large tables by increasing the number of parallel workers (2 by default)\n\n```sql\nSET max_parallel_maintenance_workers = 7; -- plus leader\n```\n\nFor a large number of workers, you may also need to increase `max_parallel_workers` (8 by default)\n\n### Indexing Progress\n\nCheck [indexing progress](https://www.postgresql.org/docs/current/progress-reporting.html#CREATE-INDEX-PROGRESS-REPORTING)\n\n```sql\nSELECT phase, round(100.0 * tuples_done / nullif(tuples_total, 0), 1) AS \"%\" FROM pg_stat_progress_create_index;\n```\n\nThe phases for IVFFlat are:\n\n1. `initializing`\n2. `performing k-means`\n3. `assigning tuples`\n4. `loading tuples`\n\nNote: `%` is only populated during the `loading tuples` phase\n\n## Filtering\n\nThere are a few ways to index nearest neighbor queries with a `WHERE` clause.\n\n```sql\nSELECT * FROM items WHERE category_id = 123 ORDER BY embedding <-> '[3,1,2]' LIMIT 5;\n```\n\nA good place to start is creating an index on the filter column. This can provide fast, exact nearest neighbor search in many cases. Postgres has a number of [index types](https://www.postgresql.org/docs/current/indexes-types.html) for this: B-tree (default), hash, GiST, SP-GiST, GIN, and BRIN.\n\n```sql\nCREATE INDEX ON items (category_id);\n```\n\nFor multiple columns, consider a [multicolumn index](https://www.postgresql.org/docs/current/indexes-multicolumn.html).\n\n```sql\nCREATE INDEX ON items (location_id, category_id);\n```\n\nExact indexes work well for conditions that match a low percentage of rows. Otherwise, [approximate indexes](#indexing) can work better.\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_l2_ops);\n```\n\nWith approximate indexes, filtering is applied *after* the index is scanned. If a condition matches 10% of rows, with HNSW and the default `hnsw.ef_search` of 40, only 4 rows will match on average. For more rows, increase `hnsw.ef_search`.\n\n```sql\nSET hnsw.ef_search = 200;\n```\n\nStarting with 0.8.0, you can enable [iterative index scans](#iterative-index-scans), which will automatically scan more of the index when needed.\n\n```sql\nSET hnsw.iterative_scan = strict_order;\n```\n\nIf filtering by only a few distinct values, consider [partial indexing](https://www.postgresql.org/docs/current/indexes-partial.html).\n\n```sql\nCREATE INDEX ON items USING hnsw (embedding vector_l2_ops) WHERE (category_id = 123);\n```\n\nIf filtering by many different values, consider [partitioning](https://www.postgresql.org/docs/current/ddl-partitioning.html).\n\n```sql\nCREATE TABLE items (embedding vector(3), category_id int) PARTITION BY LIST(category_id);\n```\n\n## Iterative Index Scans\n\nWith approximate indexes, queries with filtering can return less results since filtering is applied *after* the index is scanned. Starting with 0.8.0, you can enable iterative index scans, which will automatically scan more of the index until enough results are found (or it reaches `hnsw.max_scan_tuples` or `ivfflat.max_probes`).\n\nIterative scans can use strict or relaxed ordering.\n\nStrict ensures results are in the exact order by distance\n\n```sql\nSET hnsw.iterative_scan = strict_order;\n```\n\nRelaxed allows results to be slightly out of order by distance, but provides better recall\n\n```sql\nSET hnsw.iterative_scan = relaxed_order;\n# or\nSET ivfflat.iterative_scan = relaxed_order;\n```\n\nWith relaxed ordering, you can use a [materialized CTE](https://www.postgresql.org/docs/current/queries-with.html#QUERIES-WITH-CTE-MATERIALIZATION) to get strict ordering\n\n```sql\nWITH relaxed_results AS MATERIALIZED (\n    SELECT id, embedding <-> '[1,2,3]' AS distance FROM items WHERE category_id = 123 ORDER BY distance LIMIT 5\n) SELECT * FROM relaxed_results ORDER BY distance + 0;\n```\n\nNote: `+ 0` is needed for Postgres 17+\n\nFor queries that filter by distance, use a materialized CTE and place the distance filter outside of it for best performance (due to the [current behavior](https://www.postgresql.org/message-id/flat/CAOdR5yGUoMQ6j7M5hNUXrySzaqZVGf_Ne%2B8fwZMRKTFxU1nbJg%40mail.gmail.com) of the Postgres executor)\n\n```sql\nWITH nearest_results AS MATERIALIZED (\n    SELECT id, embedding <-> '[1,2,3]' AS distance FROM items ORDER BY distance LIMIT 5\n) SELECT * FROM nearest_results WHERE distance < 5 ORDER BY distance;\n```\n\nNote: Place any other filters inside the CTE\n\n### Iterative Scan Options\n\nSince scanning a large portion of an approximate index is expensive, there are options to control when a scan ends.\n\n#### HNSW\n\nSpecify the max number of tuples to visit (20,000 by default)\n\n```sql\nSET hnsw.max_scan_tuples = 20000;\n```\n\nNote: This is approximate and does not affect the initial scan\n\nSpecify the max amount of memory to use, as a multiple of `work_mem` (1 by default)\n\n```sql\nSET hnsw.scan_mem_multiplier = 2;\n```\n\nNote: Try increasing this if increasing `hnsw.max_scan_tuples` does not improve recall\n\n#### IVFFlat\n\nSpecify the max number of probes\n\n```sql\nSET ivfflat.max_probes = 100;\n```\n\nNote: If this is lower than `ivfflat.probes`, `ivfflat.probes` will be used\n\n## Half-Precision Vectors\n\nUse the `halfvec` type to store half-precision vectors\n\n```sql\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding halfvec(3));\n```\n\n## Half-Precision Indexing\n\nIndex vectors at half precision for smaller indexes\n\n```sql\nCREATE INDEX ON items USING hnsw ((embedding::halfvec(3)) halfvec_l2_ops);\n```\n\nGet the nearest neighbors\n\n```sql\nSELECT * FROM items ORDER BY embedding::halfvec(3) <-> '[1,2,3]' LIMIT 5;\n```\n\n## Binary Vectors\n\nUse the `bit` type to store binary vectors ([example](https://github.com/pgvector/pgvector-python/blob/master/examples/imagehash/example.py))\n\n```sql\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding bit(3));\nINSERT INTO items (embedding) VALUES ('000'), ('111');\n```\n\nGet the nearest neighbors by Hamming distance\n\n```sql\nSELECT * FROM items ORDER BY embedding <~> '101' LIMIT 5;\n```\n\nAlso supports Jaccard distance (`<%>`)\n\n## Binary Quantization\n\nUse expression indexing for binary quantization\n\n```sql\nCREATE INDEX ON items USING hnsw ((binary_quantize(embedding)::bit(3)) bit_hamming_ops);\n```\n\nGet the nearest neighbors by Hamming distance\n\n```sql\nSELECT * FROM items ORDER BY binary_quantize(embedding)::bit(3) <~> binary_quantize('[1,-2,3]') LIMIT 5;\n```\n\nRe-rank by the original vectors for better recall\n\n```sql\nSELECT * FROM (\n    SELECT * FROM items ORDER BY binary_quantize(embedding)::bit(3) <~> binary_quantize('[1,-2,3]') LIMIT 20\n) ORDER BY embedding <=> '[1,-2,3]' LIMIT 5;\n```\n\n## Sparse Vectors\n\nUse the `sparsevec` type to store sparse vectors\n\n```sql\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding sparsevec(5));\n```\n\nInsert vectors\n\n```sql\nINSERT INTO items (embedding) VALUES ('{1:1,3:2,5:3}/5'), ('{1:4,3:5,5:6}/5');\n```\n\nThe format is `{index1:value1,index2:value2}/dimensions` and indices start at 1 like SQL arrays\n\nGet the nearest neighbors by L2 distance\n\n```sql\nSELECT * FROM items ORDER BY embedding <-> '{1:3,3:1,5:2}/5' LIMIT 5;\n```\n\n## Hybrid Search\n\nUse together with Postgres [full-text search](https://www.postgresql.org/docs/current/textsearch-intro.html) for hybrid search.\n\n```sql\nSELECT id, content FROM items, plainto_tsquery('hello search') query\n    WHERE textsearch @@ query ORDER BY ts_rank_cd(textsearch, query) DESC LIMIT 5;\n```\n\nYou can use [Reciprocal Rank Fusion](https://github.com/pgvector/pgvector-python/blob/master/examples/hybrid_search/rrf.py) or a [cross-encoder](https://github.com/pgvector/pgvector-python/blob/master/examples/hybrid_search/cross_encoder.py) to combine results.\n\n## Indexing Subvectors\n\nUse expression indexing to index subvectors\n\n```sql\nCREATE INDEX ON items USING hnsw ((subvector(embedding, 1, 3)::vector(3)) vector_cosine_ops);\n```\n\nGet the nearest neighbors by cosine distance\n\n```sql\nSELECT * FROM items ORDER BY subvector(embedding, 1, 3)::vector(3) <=> subvector('[1,2,3,4,5]'::vector, 1, 3) LIMIT 5;\n```\n\nRe-rank by the full vectors for better recall\n\n```sql\nSELECT * FROM (\n    SELECT * FROM items ORDER BY subvector(embedding, 1, 3)::vector(3) <=> subvector('[1,2,3,4,5]'::vector, 1, 3) LIMIT 20\n) ORDER BY embedding <=> '[1,2,3,4,5]' LIMIT 5;\n```\n\n## Performance\n\n### Tuning\n\nUse a tool like [PgTune](https://pgtune.leopard.in.ua/) to set initial values for Postgres server parameters. For instance, `shared_buffers` should typically be 25% of the serverâ€™s memory. You can find the config file with:\n\n```sql\nSHOW config_file;\n```\n\nAnd check individual settings with:\n\n```sql\nSHOW shared_buffers;\n```\n\nBe sure to restart Postgres for changes to take effect.\n\n### Loading\n\nUse `COPY` for bulk loading data ([example](https://github.com/pgvector/pgvector-python/blob/master/examples/loading/example.py)).\n\n```sql\nCOPY items (embedding) FROM STDIN WITH (FORMAT BINARY);\n```\n\nAdd any indexes *after* loading the initial data for best performance.\n\n### Indexing\n\nSee index build time for [HNSW](#index-build-time) and [IVFFlat](#index-build-time-1).\n\nIn production environments, create indexes concurrently to avoid blocking writes.\n\n```sql\nCREATE INDEX CONCURRENTLY ...\n```\n\n### Querying\n\nUse `EXPLAIN (ANALYZE, BUFFERS)` to debug performance.\n\n```sql\nEXPLAIN (ANALYZE, BUFFERS) SELECT * FROM items ORDER BY embedding <-> '[3,1,2]' LIMIT 5;\n```\n\n#### Exact Search\n\nTo speed up queries without an index, increase `max_parallel_workers_per_gather`.\n\n```sql\nSET max_parallel_workers_per_gather = 4;\n```\n\nIf vectors are normalized to length 1 (like [OpenAI embeddings](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use)), use inner product for best performance.\n\n```tsql\nSELECT * FROM items ORDER BY embedding <#> '[3,1,2]' LIMIT 5;\n```\n\n#### Approximate Search\n\nTo speed up queries with an IVFFlat index, increase the number of inverted lists (at the expense of recall).\n\n```sql\nCREATE INDEX ON items USING ivfflat (embedding vector_l2_ops) WITH (lists = 1000);\n```\n\n### Vacuuming\n\nVacuuming can take a while for HNSW indexes. Speed it up by reindexing first.\n\n```sql\nREINDEX INDEX CONCURRENTLY index_name;\nVACUUM table_name;\n```\n\n## Monitoring\n\nMonitor performance with [pg_stat_statements](https://www.postgresql.org/docs/current/pgstatstatements.html) (be sure to add it to `shared_preload_libraries`).\n\n```sql\nCREATE EXTENSION pg_stat_statements;\n```\n\nGet the most time-consuming queries with:\n\n```sql\nSELECT query, calls, ROUND((total_plan_time + total_exec_time) / calls) AS avg_time_ms,\n    ROUND((total_plan_time + total_exec_time) / 60000) AS total_time_min\n    FROM pg_stat_statements ORDER BY total_plan_time + total_exec_time DESC LIMIT 20;\n```\n\nMonitor recall by comparing results from approximate search with exact search.\n\n```sql\nBEGIN;\nSET LOCAL enable_indexscan = off; -- use exact search\nSELECT ...\nCOMMIT;\n```\n\n## Scaling\n\nScale pgvector the same way you scale Postgres.\n\nScale vertically by increasing memory, CPU, and storage on a single instance. Use existing tools to [tune parameters](#tuning) and [monitor performance](#monitoring).\n\nScale horizontally with [replicas](https://www.postgresql.org/docs/current/hot-standby.html), or use [Citus](https://github.com/citusdata/citus) or another approach for sharding ([example](https://github.com/pgvector/pgvector-python/blob/master/examples/citus/example.py)).\n\n## Languages\n\nUse pgvector from any language with a Postgres client. You can even generate and store vectors in one language and query them in another.\n\nLanguage | Libraries / Examples\n--- | ---\nAda | [pgvector-ada](https://github.com/pgvector/pgvector-ada)\nAlgol | [pgvector-algol](https://github.com/pgvector/pgvector-algol)\nC | [pgvector-c](https://github.com/pgvector/pgvector-c)\nC++ | [pgvector-cpp](https://github.com/pgvector/pgvector-cpp)\nC#, F#, Visual Basic | [pgvector-dotnet](https://github.com/pgvector/pgvector-dotnet)\nCOBOL | [pgvector-cobol](https://github.com/pgvector/pgvector-cobol)\nCrystal | [pgvector-crystal](https://github.com/pgvector/pgvector-crystal)\nD | [pgvector-d](https://github.com/pgvector/pgvector-d)\nDart | [pgvector-dart](https://github.com/pgvector/pgvector-dart)\nElixir | [pgvector-elixir](https://github.com/pgvector/pgvector-elixir)\nErlang | [pgvector-erlang](https://github.com/pgvector/pgvector-erlang)\nFortran | [pgvector-fortran](https://github.com/pgvector/pgvector-fortran)\nGleam | [pgvector-gleam](https://github.com/pgvector/pgvector-gleam)\nGo | [pgvector-go](https://github.com/pgvector/pgvector-go)\nHaskell | [pgvector-haskell](https://github.com/pgvector/pgvector-haskell)\nJava, Kotlin, Groovy, Scala | [pgvector-java](https://github.com/pgvector/pgvector-java)\nJavaScript, TypeScript | [pgvector-node](https://github.com/pgvector/pgvector-node)\nJulia | [Pgvector.jl](https://github.com/pgvector/Pgvector.jl)\nLisp | [pgvector-lisp](https://github.com/pgvector/pgvector-lisp)\nLua | [pgvector-lua](https://github.com/pgvector/pgvector-lua)\nNim | [pgvector-nim](https://github.com/pgvector/pgvector-nim)\nOCaml | [pgvector-ocaml](https://github.com/pgvector/pgvector-ocaml)\nPascal | [pgvector-pascal](https://github.com/pgvector/pgvector-pascal)\nPerl | [pgvector-perl](https://github.com/pgvector/pgvector-perl)\nPHP | [pgvector-php](https://github.com/pgvector/pgvector-php)\nProlog | [pgvector-prolog](https://github.com/pgvector/pgvector-prolog)\nPython | [pgvector-python](https://github.com/pgvector/pgvector-python)\nR | [pgvector-r](https://github.com/pgvector/pgvector-r)\nRacket | [pgvector-racket](https://github.com/pgvector/pgvector-racket)\nRaku | [pgvector-raku](https://github.com/pgvector/pgvector-raku)\nRuby | [pgvector-ruby](https://github.com/pgvector/pgvector-ruby), [Neighbor](https://github.com/ankane/neighbor)\nRust | [pgvector-rust](https://github.com/pgvector/pgvector-rust)\nSwift | [pgvector-swift](https://github.com/pgvector/pgvector-swift)\nTcl | [pgvector-tcl](https://github.com/pgvector/pgvector-tcl)\nZig | [pgvector-zig](https://github.com/pgvector/pgvector-zig)\n\n## Frequently Asked Questions\n\n#### How many vectors can be stored in a single table?\n\nA non-partitioned table has a limit of 32 TB by default in Postgres. A partitioned table can have thousands of partitions of that size.\n\n#### Is replication supported?\n\nYes, pgvector uses the write-ahead log (WAL), which allows for replication and point-in-time recovery.\n\n#### What if I want to index vectors with more than 2,000 dimensions?\n\nYou can use [half-precision vectors](#half-precision-vectors) or [half-precision indexing](#half-precision-indexing) to index up to 4,000 dimensions or [binary quantization](#binary-quantization) to index up to 64,000 dimensions. Other options are [indexing subvectors](#indexing-subvectors) (for models that support it) or [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction).\n\n#### Can I store vectors with different dimensions in the same column?\n\nYou can use `vector` as the type (instead of `vector(n)`).\n\n```sql\nCREATE TABLE embeddings (model_id bigint, item_id bigint, embedding vector, PRIMARY KEY (model_id, item_id));\n```\n\nHowever, you can only create indexes on rows with the same number of dimensions (using [expression](https://www.postgresql.org/docs/current/indexes-expressional.html) and [partial](https://www.postgresql.org/docs/current/indexes-partial.html) indexing):\n\n```sql\nCREATE INDEX ON embeddings USING hnsw ((embedding::vector(3)) vector_l2_ops) WHERE (model_id = 123);\n```\n\nand query with:\n\n```sql\nSELECT * FROM embeddings WHERE model_id = 123 ORDER BY embedding::vector(3) <-> '[3,1,2]' LIMIT 5;\n```\n\n#### Can I store vectors with more precision?\n\nYou can use the `double precision[]` or `numeric[]` type to store vectors with more precision.\n\n```sql\nCREATE TABLE items (id bigserial PRIMARY KEY, embedding double precision[]);\n\n-- use {} instead of [] for Postgres arrays\nINSERT INTO items (embedding) VALUES ('{1,2,3}'), ('{4,5,6}');\n```\n\nOptionally, add a [check constraint](https://www.postgresql.org/docs/current/ddl-constraints.html) to ensure data can be converted to the `vector` type and has the expected dimensions.\n\n```sql\nALTER TABLE items ADD CHECK (vector_dims(embedding::vector) = 3);\n```\n\nUse [expression indexing](https://www.postgresql.org/docs/current/indexes-expressional.html) to index (at a lower precision):\n\n```sql\nCREATE INDEX ON items USING hnsw ((embedding::vector(3)) vector_l2_ops);\n```\n\nand query with:\n\n```sql\nSELECT * FROM items ORDER BY embedding::vector(3) <-> '[3,1,2]' LIMIT 5;\n```\n\n#### Do indexes need to fit into memory?\n\nNo, but like other index types, youâ€™ll likely see better performance if they do. You can get the size of an index with:\n\n```sql\nSELECT pg_size_pretty(pg_relation_size('index_name'));\n```\n\n## Troubleshooting\n\n#### Why isnâ€™t a query using an index?\n\nThe query needs to have an `ORDER BY` and `LIMIT`, and the `ORDER BY` must be the result of a distance operator (not an expression) in ascending order.\n\n```sql\n-- index\nORDER BY embedding <=> '[3,1,2]' LIMIT 5;\n\n-- no index\nORDER BY 1 - (embedding <=> '[3,1,2]') DESC LIMIT 5;\n```\n\nYou can encourage the planner to use an index for a query with:\n\n```sql\nBEGIN;\nSET LOCAL enable_seqscan = off;\nSELECT ...\nCOMMIT;\n```\n\nAlso, if the table is small, a table scan may be faster.\n\n#### Why isnâ€™t a query using a parallel table scan?\n\nThe planner doesnâ€™t consider [out-of-line storage](https://www.postgresql.org/docs/current/storage-toast.html) in cost estimates, which can make a serial scan look cheaper. You can reduce the cost of a parallel scan for a query with:\n\n```sql\nBEGIN;\nSET LOCAL min_parallel_table_scan_size = 1;\nSET LOCAL parallel_setup_cost = 1;\nSELECT ...\nCOMMIT;\n```\n\nor choose to store vectors inline:\n\n```sql\nALTER TABLE items ALTER COLUMN embedding SET STORAGE PLAIN;\n```\n\n#### Why are there less results for a query after adding an HNSW index?\n\nResults are limited by the size of the dynamic candidate list (`hnsw.ef_search`), which is 40 by default. There may be even less results due to dead tuples or filtering conditions in the query. Enabling [iterative index scans](#iterative-index-scans) can help address this.\n\nAlso, note that `NULL` vectors are not indexed (as well as zero vectors for cosine distance).\n\n#### Why are there less results for a query after adding an IVFFlat index?\n\nThe index was likely created with too little data for the number of lists. Drop the index until the table has more data.\n\n```sql\nDROP INDEX index_name;\n```\n\nResults can also be limited by the number of probes (`ivfflat.probes`). Enabling [iterative index scans](#iterative-index-scans) can address this.\n\nAlso, note that `NULL` vectors are not indexed (as well as zero vectors for cosine distance).\n\n## Reference\n\n- [Vector](#vector-type)\n- [Halfvec](#halfvec-type)\n- [Bit](#bit-type)\n- [Sparsevec](#sparsevec-type)\n\n### Vector Type\n\nEach vector takes `4 * dimensions + 8` bytes of storage. Each element is a single-precision floating-point number (like the `real` type in Postgres), and all elements must be finite (no `NaN`, `Infinity` or `-Infinity`). Vectors can have up to 16,000 dimensions.\n\n### Vector Operators\n\nOperator | Description | Added\n--- | --- | ---\n\\+ | element-wise addition |\n\\- | element-wise subtraction |\n\\* | element-wise multiplication | 0.5.0\n\\|\\| | concatenate | 0.7.0\n<-> | Euclidean distance |\n<#> | negative inner product |\n<=> | cosine distance |\n<+> | taxicab distance | 0.7.0\n\n### Vector Functions\n\nFunction | Description | Added\n--- | --- | ---\nbinary_quantize(vector) â†’ bit | binary quantize | 0.7.0\ncosine_distance(vector, vector) â†’ double precision | cosine distance |\ninner_product(vector, vector) â†’ double precision | inner product |\nl1_distance(vector, vector) â†’ double precision | taxicab distance | 0.5.0\nl2_distance(vector, vector) â†’ double precision | Euclidean distance |\nl2_normalize(vector) â†’ vector | Normalize with Euclidean norm | 0.7.0\nsubvector(vector, integer, integer) â†’ vector | subvector | 0.7.0\nvector_dims(vector) â†’ integer | number of dimensions |\nvector_norm(vector) â†’ double precision | Euclidean norm |\n\n### Vector Aggregate Functions\n\nFunction | Description | Added\n--- | --- | ---\navg(vector) â†’ vector | average |\nsum(vector) â†’ vector | sum | 0.5.0\n\n### Halfvec Type\n\nEach half vector takes `2 * dimensions + 8` bytes of storage. Each element is a half-precision floating-point number, and all elements must be finite (no `NaN`, `Infinity` or `-Infinity`). Half vectors can have up to 16,000 dimensions.\n\n### Halfvec Operators\n\nOperator | Description | Added\n--- | --- | ---\n\\+ | element-wise addition | 0.7.0\n\\- | element-wise subtraction | 0.7.0\n\\* | element-wise multiplication | 0.7.0\n\\|\\| | concatenate | 0.7.0\n<-> | Euclidean distance | 0.7.0\n<#> | negative inner product | 0.7.0\n<=> | cosine distance | 0.7.0\n<+> | taxicab distance | 0.7.0\n\n### Halfvec Functions\n\nFunction | Description | Added\n--- | --- | ---\nbinary_quantize(halfvec) â†’ bit | binary quantize | 0.7.0\ncosine_distance(halfvec, halfvec) â†’ double precision | cosine distance | 0.7.0\ninner_product(halfvec, halfvec) â†’ double precision | inner product | 0.7.0\nl1_distance(halfvec, halfvec) â†’ double precision | taxicab distance | 0.7.0\nl2_distance(halfvec, halfvec) â†’ double precision | Euclidean distance | 0.7.0\nl2_norm(halfvec) â†’ double precision | Euclidean norm | 0.7.0\nl2_normalize(halfvec) â†’ halfvec | Normalize with Euclidean norm | 0.7.0\nsubvector(halfvec, integer, integer) â†’ halfvec | subvector | 0.7.0\nvector_dims(halfvec) â†’ integer | number of dimensions | 0.7.0\n\n### Halfvec Aggregate Functions\n\nFunction | Description | Added\n--- | --- | ---\navg(halfvec) â†’ halfvec | average | 0.7.0\nsum(halfvec) â†’ halfvec | sum | 0.7.0\n\n### Bit Type\n\nEach bit vector takes `dimensions / 8 + 8` bytes of storage. See the [Postgres docs](https://www.postgresql.org/docs/current/datatype-bit.html) for more info.\n\n### Bit Operators\n\nOperator | Description | Added\n--- | --- | ---\n<~> | Hamming distance | 0.7.0\n<%> | Jaccard distance | 0.7.0\n\n### Bit Functions\n\nFunction | Description | Added\n--- | --- | ---\nhamming_distance(bit, bit) â†’ double precision | Hamming distance | 0.7.0\njaccard_distance(bit, bit) â†’ double precision | Jaccard distance | 0.7.0\n\n### Sparsevec Type\n\nEach sparse vector takes `8 * non-zero elements + 16` bytes of storage. Each element is a single-precision floating-point number, and all elements must be finite (no `NaN`, `Infinity` or `-Infinity`). Sparse vectors can have up to 16,000 non-zero elements.\n\n### Sparsevec Operators\n\nOperator | Description | Added\n--- | --- | ---\n<-> | Euclidean distance | 0.7.0\n<#> | negative inner product | 0.7.0\n<=> | cosine distance | 0.7.0\n<+> | taxicab distance | 0.7.0\n\n### Sparsevec Functions\n\nFunction | Description | Added\n--- | --- | ---\ncosine_distance(sparsevec, sparsevec) â†’ double precision | cosine distance | 0.7.0\ninner_product(sparsevec, sparsevec) â†’ double precision | inner product | 0.7.0\nl1_distance(sparsevec, sparsevec) â†’ double precision | taxicab distance | 0.7.0\nl2_distance(sparsevec, sparsevec) â†’ double precision | Euclidean distance | 0.7.0\nl2_norm(sparsevec) â†’ double precision | Euclidean norm | 0.7.0\nl2_normalize(sparsevec) â†’ sparsevec | Normalize with Euclidean norm | 0.7.0\n\n## Installation Notes - Linux and Mac\n\n### Postgres Location\n\nIf your machine has multiple Postgres installations, specify the path to [pg_config](https://www.postgresql.org/docs/current/app-pgconfig.html) with:\n\n```sh\nexport PG_CONFIG=/Library/PostgreSQL/18/bin/pg_config\n```\n\nThen re-run the installation instructions (run `make clean` before `make` if needed). If `sudo` is needed for `make install`, use:\n\n```sh\nsudo --preserve-env=PG_CONFIG make install\n```\n\nA few common paths on Mac are:\n\n- EDB installer - `/Library/PostgreSQL/18/bin/pg_config`\n- Homebrew (arm64) - `/opt/homebrew/opt/postgresql@18/bin/pg_config`\n- Homebrew (x86-64) - `/usr/local/opt/postgresql@18/bin/pg_config`\n\nNote: Replace `18` with your Postgres server version\n\n### Missing Header\n\nIf compilation fails with `fatal error: postgres.h: No such file or directory`, make sure Postgres development files are installed on the server.\n\nFor Ubuntu and Debian, use:\n\n```sh\nsudo apt install postgresql-server-dev-18\n```\n\nNote: Replace `18` with your Postgres server version\n\n### Missing SDK\n\nIf compilation fails and the output includes `warning: no such sysroot directory` on Mac, your Postgres installation points to a path that no longer exists.\n\n```sh\npg_config --cppflags\n```\n\nReinstall Postgres to fix this.\n\n### Portability\n\nBy default, pgvector compiles with `-march=native` on some platforms for best performance. However, this can lead to `Illegal instruction` errors if trying to run the compiled extension on a different machine.\n\nTo compile for portability, use:\n\n```sh\nmake OPTFLAGS=\"\"\n```\n\n## Installation Notes - Windows\n\n### Missing Header\n\nIf compilation fails with `Cannot open include file: 'postgres.h': No such file or directory`, make sure `PGROOT` is correct.\n\n### Mismatched Architecture\n\nIf compilation fails with `error C2196: case value '4' already used`, make sure youâ€™re using the `x64 Native Tools Command Prompt`. Then run `nmake /F Makefile.win clean` and re-run the installation instructions.\n\n### Missing Symbol\n\nIf linking fails with `unresolved external symbol float_to_shortest_decimal_bufn` with Postgres 17.0-17.2, upgrade to Postgres 17.3+.\n\n### Permissions\n\nIf installation fails with `Access is denied`, re-run the installation instructions as an administrator.\n\n## Additional Installation Methods\n\n### Docker\n\nGet the [Docker image](https://hub.docker.com/r/pgvector/pgvector) with:\n\n```sh\ndocker pull pgvector/pgvector:pg18-trixie\n```\n\nThis adds pgvector to the [Postgres image](https://hub.docker.com/_/postgres) (replace `18` with your Postgres server version, and run it the same way).\n\nSupported tags are:\n\n- `pg18-trixie`, `0.8.1-pg18-trixie`\n- `pg18-bookworm`, `0.8.1-pg18-bookworm`, `pg18`, `0.8.1-pg18`\n- `pg17-trixie`, `0.8.1-pg17-trixie`\n- `pg17-bookworm`, `0.8.1-pg17-bookworm`, `pg17`, `0.8.1-pg17`\n- `pg16-trixie`, `0.8.1-pg16-trixie`\n- `pg16-bookworm`, `0.8.1-pg16-bookworm`, `pg16`, `0.8.1-pg16`\n- `pg15-trixie`, `0.8.1-pg15-trixie`\n- `pg15-bookworm`, `0.8.1-pg15-bookworm`, `pg15`, `0.8.1-pg15`\n- `pg14-trixie`, `0.8.1-pg14-trixie`\n- `pg14-bookworm`, `0.8.1-pg14-bookworm`, `pg14`, `0.8.1-pg14`\n- `pg13-trixie`, `0.8.1-pg13-trixie`\n- `pg13-bookworm`, `0.8.1-pg13-bookworm`, `pg13`, `0.8.1-pg13`\n\nYou can also build the image manually:\n\n```sh\ngit clone --branch v0.8.1 https://github.com/pgvector/pgvector.git\ncd pgvector\ndocker build --pull --build-arg PG_MAJOR=18 -t myuser/pgvector .\n```\n\nIf you increase `maintenance_work_mem`, make sure `--shm-size` is at least that size to avoid an error with parallel HNSW index builds.\n\n```sh\ndocker run --shm-size=1g ...\n```\n\n### Homebrew\n\nWith Homebrew Postgres, you can use:\n\n```sh\nbrew install pgvector\n```\n\nNote: This only adds it to the `postgresql@18` and `postgresql@17` formulas\n\n### PGXN\n\nInstall from the [PostgreSQL Extension Network](https://pgxn.org/dist/vector) with:\n\n```sh\npgxn install vector\n```\n\n### APT\n\nDebian and Ubuntu packages are available from the [PostgreSQL APT Repository](https://wiki.postgresql.org/wiki/Apt). Follow the [setup instructions](https://wiki.postgresql.org/wiki/Apt#Quickstart) and run:\n\n```sh\nsudo apt install postgresql-18-pgvector\n```\n\nNote: Replace `18` with your Postgres server version\n\n### Yum\n\nRPM packages are available from the [PostgreSQL Yum Repository](https://yum.postgresql.org/). Follow the [setup instructions](https://www.postgresql.org/download/linux/redhat/) for your distribution and run:\n\n```sh\nsudo yum install pgvector_18\n# or\nsudo dnf install pgvector_18\n```\n\nNote: Replace `18` with your Postgres server version\n\n### pkg\n\nInstall the FreeBSD package with:\n\n```sh\npkg install postgresql17-pgvector\n```\n\nor the port with:\n\n```sh\ncd /usr/ports/databases/pgvector\nmake install\n```\n\n### APK\n\nInstall the Alpine package with:\n\n```sh\napk add postgresql-pgvector\n```\n\n### conda-forge\n\nWith Conda Postgres, install from [conda-forge](https://anaconda.org/conda-forge/pgvector) with:\n\n```sh\nconda install -c conda-forge pgvector\n```\n\nThis method is [community-maintained](https://github.com/conda-forge/pgvector-feedstock) by [@mmcauliffe](https://github.com/mmcauliffe)\n\n### Postgres.app\n\nDownload the [latest release](https://postgresapp.com/downloads.html) with Postgres 15+.\n\n## Hosted Postgres\n\npgvector is available on [these providers](https://github.com/pgvector/pgvector/issues/54).\n\n## Upgrading\n\n[Install](#installation) the latest version (use the same method as the original installation). Then in each database you want to upgrade, run:\n\n```sql\nALTER EXTENSION vector UPDATE;\n```\n\nYou can check the version in the current database with:\n\n```sql\nSELECT extversion FROM pg_extension WHERE extname = 'vector';\n```\n\n## Thanks\n\nThanks to:\n\n- [PASE: PostgreSQL Ultra-High-Dimensional Approximate Nearest Neighbor Search Extension](https://dl.acm.org/doi/pdf/10.1145/3318464.3386131)\n- [Faiss: A Library for Efficient Similarity Search and Clustering of Dense Vectors](https://github.com/facebookresearch/faiss)\n- [Using the Triangle Inequality to Accelerate k-means](https://cdn.aaai.org/ICML/2003/ICML03-022.pdf)\n- [k-means++: The Advantage of Careful Seeding](https://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf)\n- [Concept Decompositions for Large Sparse Text Data using Clustering](https://www.cs.utexas.edu/users/inderjit/public_papers/concept_mlj.pdf)\n- [Efficient and Robust Approximate Nearest Neighbor Search using Hierarchical Navigable Small World Graphs](https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf)\n\n## History\n\nView the [changelog](https://github.com/pgvector/pgvector/blob/master/CHANGELOG.md)\n\n## Contributing\n\nEveryone is encouraged to help improve this project. Here are a few ways you can help:\n\n- [Report bugs](https://github.com/pgvector/pgvector/issues)\n- Fix bugs and [submit pull requests](https://github.com/pgvector/pgvector/pulls)\n- Write, clarify, or fix documentation\n- Suggest or add new features\n\nTo get started with development:\n\n```sh\ngit clone https://github.com/pgvector/pgvector.git\ncd pgvector\nmake\nmake install\n```\n\nTo run all tests:\n\n```sh\nmake installcheck        # regression tests\nmake prove_installcheck  # TAP tests\n```\n\nTo run single tests:\n\n```sh\nmake installcheck REGRESS=functions                            # regression test\nmake prove_installcheck PROVE_TESTS=test/t/001_ivfflat_wal.pl  # TAP test\n```\n\nTo enable assertions:\n\n```sh\nmake clean && PG_CFLAGS=\"-DUSE_ASSERT_CHECKING\" make && make install\n```\n\nTo enable benchmarking:\n\n```sh\nmake clean && PG_CFLAGS=\"-DIVFFLAT_BENCH\" make && make install\n```\n\nTo show memory usage:\n\n```sh\nmake clean && PG_CFLAGS=\"-DHNSW_MEMORY -DIVFFLAT_MEMORY\" make && make install\n```\n\nTo get k-means metrics:\n\n```sh\nmake clean && PG_CFLAGS=\"-DIVFFLAT_KMEANS_DEBUG\" make && make install\n```\n\nResources for contributors\n\n- [Extension Building Infrastructure](https://www.postgresql.org/docs/current/extend-pgxs.html)\n- [Index Access Method Interface Definition](https://www.postgresql.org/docs/current/indexam.html)\n- [Generic WAL Records](https://www.postgresql.org/docs/current/generic-wal.html)\n",
      "stars_today": 15
    },
    {
      "id": 515368123,
      "name": "burn",
      "full_name": "tracel-ai/burn",
      "description": "Burn is a next generation tensor library and Deep Learning Framework that doesn't compromise on flexibility, efficiency and portability.",
      "html_url": "https://github.com/tracel-ai/burn",
      "stars": 14133,
      "forks": 796,
      "language": "Rust",
      "topics": [
        "autodiff",
        "cross-platform",
        "cuda",
        "deep-learning",
        "kernel-fusion",
        "machine-learning",
        "metal",
        "ndarray",
        "neural-network",
        "onnx",
        "pytorch",
        "rocm",
        "rust",
        "scientific-computing",
        "tensor",
        "vulkan",
        "wasm",
        "webgpu"
      ],
      "created_at": "2022-07-18T23:11:45Z",
      "updated_at": "2026-01-27T23:27:32Z",
      "pushed_at": "2026-01-27T21:33:13Z",
      "open_issues": 274,
      "owner": {
        "login": "tracel-ai",
        "avatar_url": "https://avatars.githubusercontent.com/u/111992358?v=4"
      },
      "readme": "<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/tracel-ai/burn/main/assets/logo-burn-neutral.webp\" width=\"350px\"/>\n\n[![Discord](https://img.shields.io/discord/1038839012602941528.svg?color=7289da&&logo=discord)](https://discord.gg/uPEBbYYDB6)\n[![Current Crates.io Version](https://img.shields.io/crates/v/burn.svg)](https://crates.io/crates/burn)\n[![Minimum Supported Rust Version](https://img.shields.io/crates/msrv/burn)](https://crates.io/crates/burn)\n[![Documentation](https://img.shields.io/badge/docs-latest-blue)](https://burn.dev/docs/burn)\n[![Test Status](https://github.com/tracel-ai/burn/actions/workflows/test.yml/badge.svg)](https://github.com/tracel-ai/burn/actions/workflows/test.yml)\n[![license](https://shields.io/badge/license-MIT%2FApache--2.0-blue)](#license)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/tracel-ai/burn)\n\n[<img src=\"https://www.runblaze.dev/ci-blaze-powered.png\" width=\"125px\"/>](https://www.runblaze.dev)\n\n---\n\n**Burn is a next generation Tensor Library and Deep Learning Framework that doesn't compromise on\n<br /> flexibility, efficiency and portability.**\n\n<br/>\n</div>\n\n<div align=\"left\">\n\nBurn is both a tensor library and a deep learning framework optimized for numerical computing, model\ninference and model training. Burn leverages Rust to perform optimizations normally only available\nin static-graph frameworks, offering optimal speed without impacting flexibility.\n\n## Backend\n\n<div align=\"left\">\n<img align=\"right\" src=\"https://raw.githubusercontent.com/tracel-ai/burn/main/assets/backend-chip.png\" height=\"96px\"/>\n\nBurn strives to be as fast as possible on as many hardwares as possible, with robust\nimplementations. We believe this flexibility is crucial for modern needs where you may train your\nmodels in the cloud, then deploy on customer hardwares, which vary from user to user.\n\n</div>\n\n### Supported Backends\n\nMost backends support all operating systems, so we don't mention them in the tables below.\n\n**GPU Backends:**\n\n|         | CUDA | ROCm | Metal | Vulkan | WebGPU | Candle | LibTorch |\n| ------- | ---- | ---- | ----- | ------ | ------ | ------ | -------- |\n| Nvidia  | â˜‘ï¸   | -    | -     | â˜‘ï¸     | â˜‘ï¸     | â˜‘ï¸     | â˜‘ï¸       |\n| AMD     | -    | â˜‘ï¸   | -     | â˜‘ï¸     | â˜‘ï¸     | -      | â˜‘ï¸       |\n| Apple   | -    | -    | â˜‘ï¸    | -      | â˜‘ï¸     | -      | â˜‘ï¸       |\n| Intel   | -    | -    | -     | â˜‘ï¸     | â˜‘ï¸     | -      | -        |\n| Qualcom | -    | -    | -     | â˜‘ï¸     | â˜‘ï¸     | -      | -        |\n| Wasm    | -    | -    | -     | -      | â˜‘ï¸     | -      | -        |\n\n**CPU Backends:**\n\n|        | Cpu (CubeCL) | NdArray | Candle | LibTorch |\n| ------ | ------------ | ------- | ------ | -------- |\n| X86    | â˜‘ï¸           | â˜‘ï¸      | â˜‘ï¸     | â˜‘ï¸       |\n| Arm    | â˜‘ï¸           | â˜‘ï¸      | â˜‘ï¸     | â˜‘ï¸       |\n| Wasm   | -            | â˜‘ï¸      | â˜‘ï¸     | -        |\n| no-std | -            | â˜‘ï¸      | -      | -        |\n\n<br />\n\nCompared to other frameworks, Burn has a very different approach to supporting many backends. By\ndesign, most code is generic over the Backend trait, which allows us to build Burn with swappable\nbackends. This makes composing backend possible, augmenting them with additional functionalities\nsuch as autodifferentiation and automatic kernel fusion.\n\n<details>\n<summary>\nAutodiff: Backend decorator that brings backpropagation to any backend ğŸ”„\n</summary>\n<br />\n\nContrary to the aforementioned backends, Autodiff is actually a backend _decorator_. This means that\nit cannot exist by itself; it must encapsulate another backend.\n\nThe simple act of wrapping a base backend with Autodiff transparently equips it with\nautodifferentiation support, making it possible to call backward on your model.\n\n```rust\nuse burn::backend::{Autodiff, Wgpu};\nuse burn::tensor::{Distribution, Tensor};\n\nfn main() {\n    type Backend = Autodiff<Wgpu>;\n\n    let device = Default::default();\n\n    let x: Tensor<Backend, 2> = Tensor::random([32, 32], Distribution::Default, &device);\n    let y: Tensor<Backend, 2> = Tensor::random([32, 32], Distribution::Default, &device).require_grad();\n\n    let tmp = x.clone() + y.clone();\n    let tmp = tmp.matmul(x);\n    let tmp = tmp.exp();\n\n    let grads = tmp.backward();\n    let y_grad = y.grad(&grads).unwrap();\n    println!(\"{y_grad}\");\n}\n```\n\nOf note, it is impossible to make the mistake of calling backward on a model that runs on a backend\nthat does not support autodiff (for inference), as this method is only offered by an Autodiff\nbackend.\n\nSee the [Autodiff Backend README](./crates/burn-autodiff/README.md) for more details.\n\n</details>\n\n<details>\n<summary>\nFusion: Backend decorator that brings kernel fusion to all first-party backends\n</summary>\n<br />\n\nThis backend decorator enhances a backend with kernel fusion, provided that the inner backend\nsupports it. Note that you can compose this backend with other backend decorators such as Autodiff.\nAll first-party accelerated backends (like WGPU and CUDA) use Fusion by default (`burn/fusion`\nfeature flag), so you typically don't need to apply it manually.\n\n```rust\n#[cfg(not(feature = \"fusion\"))]\npub type Cuda<F = f32, I = i32> = CubeBackend<CudaRuntime, F, I, u8>;\n\n#[cfg(feature = \"fusion\")]\npub type Cuda<F = f32, I = i32> = burn_fusion::Fusion<CubeBackend<CudaRuntime, F, I, u8>>;\n```\n\nOf note, we plan to implement automatic gradient checkpointing based on compute bound and memory\nbound operations, which will work gracefully with the fusion backend to make your code run even\nfaster during training, see [this issue](https://github.com/tracel-ai/burn/issues/936).\n\nSee the [Fusion Backend README](./crates/burn-fusion/README.md) for more details.\n\n</details>\n\n<details>\n<summary>\nRouter (Beta): Backend decorator that composes multiple backends into a single one\n</summary>\n<br />\n\nThat backend simplifies hardware operability, if for instance you want to execute some operations on\nthe CPU and other operations on the GPU.\n\n```rust\nuse burn::tensor::{Distribution, Tensor};\nuse burn::backend::{\n    NdArray, Router, Wgpu, ndarray::NdArrayDevice, router::duo::MultiDevice, wgpu::WgpuDevice,\n};\n\nfn main() {\n    type Backend = Router<(Wgpu, NdArray)>;\n\n    let device_0 = MultiDevice::B1(WgpuDevice::DiscreteGpu(0));\n    let device_1 = MultiDevice::B2(NdArrayDevice::Cpu);\n\n    let tensor_gpu =\n        Tensor::<Backend, 2>::random([3, 3], burn::tensor::Distribution::Default, &device_0);\n    let tensor_cpu =\n        Tensor::<Backend, 2>::random([3, 3], burn::tensor::Distribution::Default, &device_1);\n}\n\n```\n\n</details>\n\n<details>\n<summary>\nRemote (Beta): Backend decorator for remote backend execution, useful for distributed computations\n</summary>\n<br />\n\nThat backend has two parts, one client and one server. The client sends tensor operations over the\nnetwork to a remote compute backend. You can use any first-party backend as server in a single line\nof code:\n\n```rust\nfn main_server() {\n    // Start a server on port 3000.\n    burn::server::start::<burn::backend::Cuda>(Default::default(), 3000);\n}\n\nfn main_client() {\n    // Create a client that communicate with the server on port 3000.\n    use burn::backend::{Autodiff, RemoteBackend};\n\n    type Backend = Autodiff<RemoteDevice>;\n\n    let device = RemoteDevice::new(\"ws://localhost:3000\");\n    let tensor_gpu =\n        Tensor::<Backend, 2>::random([3, 3], Distribution::Default, &device);\n}\n\n```\n\n</details>\n\n<br />\n\n## Training & Inference\n\n<div align=\"left\">\n<img align=\"right\" src=\"https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-wall.png\" height=\"96px\"/>\n\nThe whole deep learning workflow is made easy with Burn, as you can monitor your training progress\nwith an ergonomic dashboard, and run inference everywhere from embedded devices to large GPU\nclusters.\n\nBurn was built from the ground up with training and inference in mind. It's also worth noting how\nBurn, in comparison to frameworks like PyTorch, simplifies the transition from training to\ndeployment, eliminating the need for code changes.\n\n</div>\n\n<div align=\"center\">\n\n<br />\n\n<a href=\"https://www.youtube.com/watch?v=N9RM5CQbNQc\" target=\"_blank\">\n    <img src=\"https://raw.githubusercontent.com/tracel-ai/burn/main/assets/burn-train-tui.png\" alt=\"Burn Train TUI\" width=\"75%\">\n  </a>\n</div>\n\n<br />\n\n**Click on the following sections to expand ğŸ‘‡**\n\n<details>\n<summary>\nTraining Dashboard ğŸ“ˆ\n</summary>\n<br />\n\nAs you can see in the previous video (click on the picture!), a new terminal UI dashboard based on\nthe [Ratatui](https://github.com/ratatui-org/ratatui) crate allows users to follow their training\nwith ease without having to connect to any external application.\n\nYou can visualize your training and validation metrics updating in real-time and analyze the\nlifelong progression or recent history of any registered metrics using only the arrow keys. Break\nfrom the training loop without crashing, allowing potential checkpoints to be fully written or\nimportant pieces of code to complete without interruption ğŸ›¡\n\n</details>\n\n<details>\n<summary>\nONNX Support ğŸ«\n</summary>\n<br />\n\nBurn supports importing ONNX (Open Neural Network Exchange) models, allowing you to easily port\nmodels from TensorFlow or PyTorch to Burn. The ONNX model is converted into Rust code that uses\nBurn's native APIs, enabling the imported model to run on any Burn backend (CPU, GPU, WebAssembly)\nand benefit from all of Burn's optimizations like automatic kernel fusion.\n\nOur ONNX support is further described in\n[this section of the Burn Book ğŸ”¥](https://burn.dev/books/burn/import/onnx-model.html).\n\n> **Note**: This crate is in active development and currently supports a\n> [limited set of ONNX operators](./crates/burn-onnx/SUPPORTED-ONNX-OPS.md).\n\n</details>\n\n<details>\n<summary>\nImporting PyTorch or Safetensors Models ğŸšš\n</summary>\n<br />\n\nYou can load weights from PyTorch or Safetensors formats directly into your Burn-defined models.\nThis makes it easy to reuse existing models while benefiting from Burn's performance and deployment\nfeatures.\n\nLearn more:\n\n- [Import pre-trained PyTorch models into Burn](https://burn.dev/books/burn/import/pytorch-model.html)\n- [Load models from Safetensors format](https://burn.dev/books/burn/import/safetensors-model.html)\n\n</details>\n\n<details>\n<summary>\nInference in the Browser ğŸŒ\n</summary>\n<br />\n\nSeveral of our backends can run in WebAssembly environments: Candle and NdArray for CPU execution,\nand WGPU for GPU acceleration via WebGPU. This means that you can run inference directly within a\nbrowser. We provide several examples of this:\n\n- [MNIST](./examples/mnist-inference-web) where you can draw digits and a small convnet tries to\n  find which one it is! 2ï¸âƒ£ 7ï¸âƒ£ ğŸ˜°\n- [Image Classification](./examples/image-classification-web) where you can upload images and\n  classify them! ğŸŒ„\n\n</details>\n\n<details>\n<summary>\nEmbedded: <i>no_std</i> support âš™ï¸\n</summary>\n<br />\n\nBurn's core components support [no_std](https://docs.rust-embedded.org/book/intro/no-std.html). This\nmeans it can run in bare metal environment such as embedded devices without an operating system.\n\n> As of now, only the NdArray backend can be used in a _no_std_ environment.\n\n</details>\n\n<br />\n\n### Benchmarks\n\nTo evaluate performance across different backends and track improvements over time, we provide a\ndedicated benchmarking suite.\n\nRun and compare benchmarks using [burn-bench](https://github.com/tracel-ai/burn-bench).\n\n> âš ï¸ **Warning** When using one of the `wgpu` backends, you may encounter compilation errors related\n> to recursive type evaluation. This is due to complex type nesting within the `wgpu` dependency\n> chain. To resolve this issue, add the following line at the top of your `main.rs` or `lib.rs`\n> file:\n>\n> ```rust\n> #![recursion_limit = \"256\"]\n> ```\n>\n> The default recursion limit (128) is often just below the required depth (typically 130-150) due\n> to deeply nested associated types and trait bounds.\n\n## Getting Started\n\n<div align=\"left\">\n<img align=\"right\" src=\"https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-walking.png\" height=\"96px\"/>\n\nJust heard of Burn? You are at the right place! Just continue reading this section and we hope you\ncan get on board really quickly.\n\n</div>\n\n<details>\n<summary>\nThe Burn Book ğŸ”¥\n</summary>\n<br />\n\nTo begin working effectively with Burn, it is crucial to understand its key components and\nphilosophy. This is why we highly recommend new users to read the first sections of\n[The Burn Book ğŸ”¥](https://burn.dev/books/burn/). It provides detailed examples and explanations\ncovering every facet of the framework, including building blocks like tensors, modules, and\noptimizers, all the way to advanced usage, like coding your own GPU kernels.\n\n> The project is constantly evolving, and we try as much as possible to keep the book up to date\n> with new additions. However, we might miss some details sometimes, so if you see something weird,\n> let us know! We also gladly accept Pull Requests ğŸ˜„\n\n</details>\n\n<details>\n<summary>\nExamples ğŸ™\n</summary>\n<br />\n\nLet's start with a code snippet that shows how intuitive the framework is to use! In the following,\nwe declare a neural network module with some parameters along with its forward pass.\n\n```rust\nuse burn::nn;\nuse burn::module::Module;\nuse burn::tensor::backend::Backend;\n\n#[derive(Module, Debug)]\npub struct PositionWiseFeedForward<B: Backend> {\n    linear_inner: nn::Linear<B>,\n    linear_outer: nn::Linear<B>,\n    dropout: nn::Dropout,\n    gelu: nn::Gelu,\n}\n\nimpl<B: Backend> PositionWiseFeedForward<B> {\n    pub fn forward<const D: usize>(&self, input: Tensor<B, D>) -> Tensor<B, D> {\n        let x = self.linear_inner.forward(input);\n        let x = self.gelu.forward(x);\n        let x = self.dropout.forward(x);\n\n        self.linear_outer.forward(x)\n    }\n}\n```\n\nWe have a somewhat large amount of [examples](./examples) in the repository that shows how to use\nthe framework in different scenarios.\n\nFollowing [the book](https://burn.dev/books/burn/):\n\n- [Basic Workflow](./examples/guide) : Creates a custom CNN `Module` to train on the MNIST dataset\n  and use for inference.\n- [Custom Training Loop](./examples/custom-training-loop) : Implements a basic training loop instead\n  of using the `Learner`.\n- [Custom WGPU Kernel](./examples/custom-wgpu-kernel) : Learn how to create your own custom\n  operation with the WGPU backend.\n\nAdditional examples:\n\n- [Custom CSV Dataset](./examples/custom-csv-dataset) : Implements a dataset to parse CSV data for a\n  regression task.\n- [Regression](./examples/simple-regression) : Trains a simple MLP on the California Housing dataset\n  to predict the median house value for a district.\n- [Custom Image Dataset](./examples/custom-image-dataset) : Trains a simple CNN on custom image\n  dataset following a simple folder structure.\n- [Custom Renderer](./examples/custom-renderer) : Implements a custom renderer to display the\n  [`Learner`](./building-blocks/learner.md) progress.\n- [Image Classification Web](./examples/image-classification-web) : Image classification web browser\n  demo using Burn, WGPU and WebAssembly.\n- [MNIST Inference on Web](./examples/mnist-inference-web) : An interactive MNIST inference demo in\n  the browser. The demo is available [online](https://burn.dev/demo/).\n- [MNIST Training](./examples/mnist) : Demonstrates how to train a custom `Module` (MLP) with the\n  `Learner` configured to log metrics and keep training checkpoints.\n- [Named Tensor](./examples/named-tensor) : Performs operations with the experimental `NamedTensor`\n  feature.\n- [ONNX Import Inference](./examples/onnx-inference) : Imports an ONNX model pre-trained on MNIST to\n  perform inference on a sample image with Burn.\n- [PyTorch Import Inference](./examples/import-model-weights) : Imports a PyTorch model pre-trained\n  on MNIST to perform inference on a sample image with Burn.\n- [Text Classification](./examples/text-classification) : Trains a text classification transformer\n  model on the AG News or DbPedia dataset. The trained model can then be used to classify a text\n  sample.\n- [Text Generation](./examples/text-generation) : Trains a text generation transformer model on the\n  DbPedia dataset.\n- [Wasserstein GAN MNIST](./examples/wgan) : Trains a WGAN model to generate new handwritten digits\n  based on MNIST.\n\nFor more practical insights, you can clone the repository and run any of them directly on your\ncomputer!\n\n</details>\n\n<details>\n<summary>\nPre-trained Models ğŸ¤–\n</summary>\n<br />\n\nWe keep an updated and curated list of models and examples built with Burn, see the\n[tracel-ai/models repository](https://github.com/tracel-ai/models) for more details.\n\nDon't see the model you want? Don't hesitate to open an issue, and we may prioritize it. Built a\nmodel using Burn and want to share it? You can also open a Pull Request and add your model under the\ncommunity section!\n\n</details>\n\n<details>\n<summary>\nWhy use Rust for Deep Learning? ğŸ¦€\n</summary>\n<br />\n\nDeep Learning is a special form of software where you need very high level abstractions as well as\nextremely fast execution time. Rust is the perfect candidate for that use case since it provides\nzero-cost abstractions to easily create neural network modules, and fine-grained control over memory\nto optimize every detail.\n\nIt's important that a framework be easy to use at a high level so that its users can focus on\ninnovating in the AI field. However, since running models relies so heavily on computations,\nperformance can't be neglected.\n\nTo this day, the mainstream solution to this problem has been to offer APIs in Python, but rely on\nbindings to low-level languages such as C/C++. This reduces portability, increases complexity and\ncreates frictions between researchers and engineers. We feel like Rust's approach to abstractions\nmakes it versatile enough to tackle this two languages dichotomy.\n\nRust also comes with the Cargo package manager, which makes it incredibly easy to build, test, and\ndeploy from any environment, which is usually a pain in Python.\n\nAlthough Rust has the reputation of being a difficult language at first, we strongly believe it\nleads to more reliable, bug-free solutions built faster (after some practice ğŸ˜…)!\n\n</details>\n\n<br />\n\n> **Deprecation Note**<br />Since `0.14.0`, the internal structure for tensor data has changed. The\n> previous `Data` struct was deprecated and officially removed since `0.17.0` in favor of the new\n> `TensorData` struct, which allows for more flexibility by storing the underlying data as bytes and\n> keeping the data type as a field. If you are using `Data` in your code, make sure to switch to\n> `TensorData`.\n\n<!-- >\n> In the event that you are trying to load a model record saved in a previous version, make sure to\n> enable the `record-backward-compat` feature using a previous version of burn (<=0.16.0). Otherwise,\n> the record won't be deserialized correctly and you will get an error message (which will also point\n> you to the backward compatible feature flag). The backward compatibility was maintained for\n> deserialization (loading), so as soon as you have saved the record again it will be saved according\n> to the new structure and you will be able to upgrade to this version. Please note that binary formats\n> are not backward compatible. Thus, you will need to load your record in a previous version and save it\n> to another of the self-describing record formats before using a compatible version (as described) with the\n> `record-backward-compat` feature flag. -->\n\n<details id=\"deprecation\">\n<summary>\nLoading Model Records From Previous Versions âš ï¸\n</summary>\n<br />\n\nIn the event that you are trying to load a model record saved in a version older than `0.14.0`, make\nsure to use a compatible version (`0.14`, `0.15` or `0.16`) with the `record-backward-compat`\nfeature flag.\n\n```\nfeatures = [..., \"record-backward-compat\"]\n```\n\nOtherwise, the record won't be deserialized correctly and you will get an error message. This error\nwill also point you to the backward compatible feature flag.\n\nThe backward compatibility was maintained for deserialization when loading records. Therefore, as\nsoon as you have saved the record again it will be saved according to the new structure and you can\nupgrade back to the current version\n\nPlease note that binary formats are not backward compatible. Thus, you will need to load your record\nin a previous version and save it in any of the other self-describing record format (e.g., using the\n`NamedMpkFileRecorder`) before using a compatible version (as described) with the\n`record-backward-compat` feature flag.\n\n</details>\n\n## Community\n\n<div align=\"left\">\n<img align=\"right\" src=\"https://raw.githubusercontent.com/tracel-ai/burn/main/assets/ember-community.png\" height=\"96px\"/>\n\nIf you are excited about the project, don't hesitate to join our\n[Discord](https://discord.gg/uPEBbYYDB6)! We try to be as welcoming as possible to everybody from\nany background. You can ask your questions and share what you built with the community!\n\n</div>\n\n<br/>\n\n**Contributing**\n\nBefore contributing, please take a moment to review our\n[code of conduct](https://github.com/tracel-ai/burn/tree/main/CODE-OF-CONDUCT.md). It's also highly\nrecommended to read the\n[architecture overview](https://github.com/tracel-ai/burn/tree/main/contributor-book/src/project-architecture),\nwhich explains some of our architectural decisions. Refer to our\n[contributing guide](/CONTRIBUTING.md) for more details.\n\n## Status\n\nBurn is currently in active development, and there will be breaking changes. While any resulting\nissues are likely to be easy to fix, there are no guarantees at this stage.\n\n## License\n\nBurn is distributed under the terms of both the MIT license and the Apache License (Version 2.0).\nSee [LICENSE-APACHE](./LICENSE-APACHE) and [LICENSE-MIT](./LICENSE-MIT) for details. Opening a pull\nrequest is assumed to signal agreement with these licensing terms.\n\n</div>\n",
      "stars_today": 15
    },
    {
      "id": 384520418,
      "name": "MaaAssistantArknights",
      "full_name": "MaaAssistantArknights/MaaAssistantArknights",
      "description": "ã€Šæ˜æ—¥æ–¹èˆŸã€‹å°åŠ©æ‰‹ï¼Œå…¨æ—¥å¸¸ä¸€é”®é•¿è‰ï¼| A one-click tool for the daily tasks of Arknights, supporting all clients.",
      "html_url": "https://github.com/MaaAssistantArknights/MaaAssistantArknights",
      "stars": 19331,
      "forks": 2457,
      "language": "C++",
      "topics": [
        "arknights",
        "computer-vision",
        "maa"
      ],
      "created_at": "2021-07-09T18:26:38Z",
      "updated_at": "2026-01-27T21:04:39Z",
      "pushed_at": "2026-01-27T22:24:36Z",
      "open_issues": 567,
      "owner": {
        "login": "MaaAssistantArknights",
        "avatar_url": "https://avatars.githubusercontent.com/u/97677443?v=4"
      },
      "readme": "<!-- markdownlint-disable -->\n\n<div align=\"center\">\n\n<img alt=\"LOGO\" src=\"./docs/.vuepress/public/images/maa-logo_512x512.png\" width=\"256\" height=\"256\" />\n\n# MAA\n\n<br>\n<div>\n    <img alt=\"C++\" src=\"https://img.shields.io/badge/C++-20-%2300599C?logo=cplusplus\">\n</div>\n<div>\n    <img alt=\"platform\" src=\"https://img.shields.io/badge/platform-Windows%20%7C%20Linux%20%7C%20macOS-blueviolet\">\n</div>\n<div>\n    <img alt=\"license\" src=\"https://img.shields.io/github/license/MaaAssistantArknights/MaaAssistantArknights\">\n    <img alt=\"commit\" src=\"https://img.shields.io/github/commit-activity/m/MaaAssistantArknights/MaaAssistantArknights?color=%23ff69b4\">\n</div>\n<div>\n    <img alt=\"stars\" src=\"https://img.shields.io/github/stars/MaaAssistantArknights/MaaAssistantArknights?style=social\">\n    <img alt=\"GitHub all releases\" src=\"https://img.shields.io/github/downloads/MaaAssistantArknights/MaaAssistantArknights/total?style=social\">\n</div>\n<div>\n    <a href=\"https://deepwiki.com/MaaAssistantArknights/MaaAssistantArknights\"><img src=\"https://deepwiki.com/badge.svg\" alt=\"Ask DeepWiki\"></a>\n</div>\n<br>\n\n<!-- markdownlint-restore -->\n\n[ç®€ä½“ä¸­æ–‡](https://docs.maa.plus/zh-cn/) | [ç¹é«”ä¸­æ–‡](https://docs.maa.plus/zh-tw/) | [English](https://docs.maa.plus/en-us/) | [æ—¥æœ¬èª](https://docs.maa.plus/ja-jp/) | [í•œêµ­ì–´](https://docs.maa.plus/ko-kr/)\n\nMAA çš„æ„æ€æ˜¯ MAA Assistant Arknights\n\nä¸€æ¬¾æ˜æ—¥æ–¹èˆŸæ¸¸æˆå°åŠ©æ‰‹\n\nåŸºäºå›¾åƒè¯†åˆ«æŠ€æœ¯ï¼Œä¸€é”®å®Œæˆå…¨éƒ¨æ—¥å¸¸ä»»åŠ¡ï¼\n\nç»èµæ›´æ–°ä¸­ âœ¿âœ¿ãƒ½(Â°â–½Â°)ãƒâœ¿\n\n</div>\n\n## ä¸‹è½½ä¸å®‰è£…\n\nè¯·é˜…è¯» [æ–‡æ¡£](https://docs.maa.plus/zh-cn/manual/newbie.html) åå‰å¾€ [å®˜ç½‘](https://maa.plus) æˆ– [Releases](https://github.com/MaaAssistantArknights/MaaAssistantArknights/releases) ä¸‹è½½ï¼Œå¹¶å‚è€ƒ [æ–°æ‰‹ä¸Šè·¯](https://docs.maa.plus/zh-cn/manual/newbie.html) è¿›è¡Œå®‰è£…ã€‚\n\n## äº®ç‚¹åŠŸèƒ½\n\n- ç†æ™ºä½œæˆ˜ï¼Œæ‰è½è¯†åˆ«åŠä¸Šä¼  [ä¼é¹…ç‰©æµ](https://penguin-stats.cn/)ï¼Œ[ä¸€å›¾æµ](https://ark.yituliu.cn/)\n- æ™ºèƒ½åŸºå»ºæ¢ç­ï¼Œè‡ªåŠ¨è®¡ç®—å¹²å‘˜æ•ˆç‡ï¼Œå•è®¾æ–½å†…æœ€ä¼˜è§£ï¼›åŒæ—¶ä¹Ÿæ”¯æŒ [è‡ªå®šä¹‰æ’ç­](https://docs.maa.plus/zh-cn/protocol/base-scheduling-schema.html)\n- è‡ªåŠ¨å…¬æ‹›ï¼Œå¯é€‰ä½¿ç”¨åŠ æ€¥è®¸å¯ï¼Œä¸€æ¬¡å…¨éƒ¨åˆ·å®Œï¼å…¬æ‹›æ•°æ®è‡ªåŠ¨ä¸Šä¼  [ä¼é¹…ç‰©æµ](https://penguin-stats.cn/result/stage/recruit/recruit)ï¼Œ[ä¸€å›¾æµ](https://ark.yituliu.cn/survey/maarecruitdata)\n- æ”¯æŒæ‰‹åŠ¨è¯†åˆ«å…¬æ‹›ç•Œé¢ï¼Œæ–¹ä¾¿å¯¹é«˜æ˜Ÿå…¬æ‹›åšå‡ºé€‰æ‹© ~~ï¼ˆä½ çš„è¿™ä¸ªé«˜èµ„å›è´¹å‡ºçš„æ˜¯æ¨ç‹å‘¢è¿˜æ˜¯æ¨ç‹å‘¢ï¼‰~~\n- æ”¯æŒè¯†åˆ«å¹²å‘˜åˆ—è¡¨ï¼Œç»Ÿè®¡å·²æœ‰å’Œæœªæœ‰å¹²å‘˜åŠæ½œèƒ½ï¼Œå¹¶åœ¨å…¬æ‹›è¯†åˆ«æ˜¾ç¤º\n- æ”¯æŒè¯†åˆ«å…»æˆææ–™ï¼Œå¹¶å¯¼å‡ºè‡³ [ä¼é¹…ç‰©æµåˆ·å›¾è§„åˆ’](https://penguin-stats.cn/planner)ã€[æ˜æ—¥æ–¹èˆŸå·¥å…·ç®±](https://arkntools.app/#/material)ã€[ARK-NIGHTS å¹²å‘˜åŸ¹å…»è¡¨](https://ark-nights.com/settings)\n- è®¿é—®å¥½å‹ã€æ”¶å–ä¿¡ç”¨åŠè´­ç‰©ã€é¢†å–æ—¥å¸¸å¥–åŠ±ç­‰ï¼Œä¸€é”®å…¨æ—¥å¸¸è‡ªåŠ¨é•¿è‰\n- è‚‰é¸½å…¨è‡ªåŠ¨åˆ·æºçŸ³é”­å’Œç­‰çº§ï¼Œè‡ªåŠ¨çƒ§æ°´å’Œå‡¹ç›´å‡ï¼Œæ™ºèƒ½è¯†åˆ«å¹²å‘˜åŠç»ƒåº¦\n- é€‰æ‹©ä½œä¸š JSON æ–‡ä»¶ï¼Œè‡ªåŠ¨æŠ„ä½œä¸šï¼Œ [è§†é¢‘æ¼”ç¤º](https://www.bilibili.com/video/BV1H841177Fk/)\n- æ”¯æŒ C, Python, Java, Rust, Golang, Java HTTP, Rust HTTP ç­‰å¤šç§æ¥å£ï¼Œæ–¹ä¾¿é›†æˆè°ƒç”¨ï¼Œè‡ªå®šä¹‰ä½ çš„ MAAï¼\n\n<!-- markdownlint-disable -->\n\n<details><summary>è¯ä¸å¤šè¯´ï¼Œçœ‹å›¾ï¼</summary>\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./docs/.vuepress/public/images/zh-cn/readme/1-dark.png\">\n  <img alt=\"zh1\" src=\"./docs/.vuepress/public/images/zh-cn/readme/1-light.png\">\n</picture>\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./docs/.vuepress/public/images/zh-cn/readme/2-dark.png\">\n  <img alt=\"zh2\" src=\"./docs/.vuepress/public/images/zh-cn/readme/2-light.png\">\n</picture>\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./docs/.vuepress/public/images/zh-cn/readme/3-dark.png\">\n  <img alt=\"zh3\" src=\"./docs/.vuepress/public/images/zh-cn/readme/3-light.png\">\n</picture>\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./docs/.vuepress/public/images/zh-cn/readme/4-dark.png\">\n  <img alt=\"zh4\" src=\"./docs/.vuepress/public/images/zh-cn/readme/4-light.png\">\n</picture>\n\n</details>\n\n<!-- markdownlint-restore -->\n\n## ä½¿ç”¨è¯´æ˜\n\n### åŠŸèƒ½ä»‹ç»\n\nè¯·å‚é˜… [ç”¨æˆ·æ‰‹å†Œ](https://docs.maa.plus/zh-cn/manual/)ã€‚\n\n### å¤–æœæ”¯æŒ\n\nç›®å‰å›½é™…æœï¼ˆç¾æœï¼‰ã€æ—¥æœã€éŸ©æœã€ç¹ä¸­æœçš„ç»å¤§éƒ¨åˆ†åŠŸèƒ½å‡å·²æ”¯æŒã€‚ä½†ç”±äºå¤–æœç”¨æˆ·è¾ƒå°‘åŠé¡¹ç›®äººæ‰‹ä¸è¶³ï¼Œå¾ˆå¤šåŠŸèƒ½å¹¶æ²¡æœ‰è¿›è¡Œå…¨é¢çš„æµ‹è¯•ï¼Œæ‰€ä»¥è¯·è‡ªè¡Œä½“éªŒã€‚  \nè‹¥æ‚¨é‡åˆ°äº† Bugï¼Œæˆ–å¯¹æŸä¸ªåŠŸèƒ½æœ‰å¼ºéœ€æ±‚ï¼Œæ¬¢è¿åœ¨ [Issues](https://github.com/MaaAssistantArknights/MaaAssistantArknights/issues) å’Œ [è®¨è®ºåŒº](https://github.com/MaaAssistantArknights/MaaAssistantArknights/discussions) å‚¬æ›´ï¼›æˆ–åŠ å…¥æˆ‘ä»¬ä¸€èµ·å»ºè®¾ MAAï¼è¯·å‚é˜… [å¤–æœé€‚é…æ•™ç¨‹](https://docs.maa.plus/zh-cn/develop/overseas-client-adaptation.html)\n\n### CLI æ”¯æŒ\n\nMAA æ”¯æŒå‘½ä»¤è¡Œç•Œé¢ï¼ˆCLIï¼‰æ“ä½œï¼Œæ”¯æŒ Linuxï¼ŒmacOS å’Œ Windowsï¼Œå¯ç”¨äºè‡ªåŠ¨åŒ–è„šæœ¬æˆ–åœ¨æ— å›¾å½¢ç•Œé¢çš„æœåŠ¡å™¨ä¸Šä½¿ç”¨ã€‚è¯·å‚é˜… [CLI ä½¿ç”¨æŒ‡å—](https://docs.maa.plus/zh-cn/manual/cli/)\n\n## åŠ å…¥æˆ‘ä»¬\n\n### ä¸»è¦å…³è”é¡¹ç›®\n\n- å…¨æ–°æ¡†æ¶ï¼š[MaaFramework](https://github.com/MaaXYZ/MaaFramework)\n- [ä½œä¸šç«™](https://prts.plus) å‰ç«¯ï¼š[zoot-plus-frontend](https://github.com/ZOOT-Plus/zoot-plus-frontend)\n- [ä½œä¸šç«™](https://prts.plus) åç«¯ï¼š[ZootPlusBackend](https://github.com/ZOOT-Plus/ZootPlusBackend)\n- [å®˜ç½‘](https://maa.plus)ï¼š[å‰ç«¯](https://github.com/MaaAssistantArknights/maa-website)\n- æ·±åº¦å­¦ä¹ ï¼š[MaaAI](https://github.com/MaaAssistantArknights/MaaAI)\n\n### å¤šè¯­è¨€ (i18n)\n\nMAA ä»¥ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰ä¸ºç¬¬ä¸€è¯­è¨€ï¼Œç¿»è¯‘è¯æ¡å‡ä»¥ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰ä¸ºå‡†ã€‚\n\n### å‚ä¸å¼€å‘\n\nè¯·å‚é˜… [å¼€å‘æŒ‡å—](https://docs.maa.plus/zh-cn/develop/development.html)ã€‚\n\n### API\n\n- [C æ¥å£](include/AsstCaller.h)ï¼š[é›†æˆç¤ºä¾‹](src/Cpp/main.cpp)\n- [Python æ¥å£](src/Python/asst/asst.py)ï¼š[é›†æˆç¤ºä¾‹](src/Python/sample.py)\n- [Golang æ¥å£](src/Golang)ï¼š[é›†æˆç¤ºä¾‹](src/Golang/maa/maa.go)\n- [Dart æ¥å£](src/Dart)\n- [Java æ¥å£](src/Java/src/main/java/com/iguigui/maaj/easySample/MaaCore.java)ï¼š[é›†æˆç¤ºä¾‹](src/Java/src/main/java/com/iguigui/maaj/easySample/MaaJavaSample.java)\n- [Java HTTP æ¥å£](src/Java/Readme.md)\n- [Rust æ¥å£](src/Rust/src/maa_sys)ï¼š[HTTP æ¥å£](src/Rust)\n- [TypeScript æ¥å£](https://github.com/MaaAssistantArknights/MaaX/tree/main/packages/main/coreLoader)\n- [Woolang æ¥å£](src/Woolang/maa.wo)ï¼š[é›†æˆç¤ºä¾‹](src/Woolang/demo.wo)\n- [é›†æˆæ–‡æ¡£](https://docs.maa.plus/zh-cn/protocol/integration.html)\n- [å›è°ƒæ¶ˆæ¯åè®®](https://docs.maa.plus/zh-cn/protocol/callback-schema.html)\n- [ä»»åŠ¡æµç¨‹åè®®](https://docs.maa.plus/zh-cn/protocol/task-schema.html)\n- [è‡ªåŠ¨æŠ„ä½œä¸šåè®®](https://docs.maa.plus/zh-cn/protocol/copilot-schema.html)\n\n### å¤–æœé€‚é…\n\nè¯·å‚é˜… [å¤–æœé€‚é…æ•™ç¨‹](https://docs.maa.plus/zh-cn/develop/overseas-client-adaptation.html)ï¼Œå¯¹äºå›½æœå·²æ”¯æŒçš„åŠŸèƒ½ï¼Œç»å¤§éƒ¨åˆ†çš„å¤–æœé€‚é…å·¥ä½œä»…éœ€è¦æˆªå›¾ + ç®€å•çš„ JSON ä¿®æ”¹å³å¯ã€‚\n\n### Issue bot\n\nè¯·å‚é˜… [Issue Bot ä½¿ç”¨æ–¹æ³•](https://docs.maa.plus/zh-cn/develop/issue-bot-usage.html)\n\n## è‡´è°¢\n\n### å¼€æºåº“\n\n- å›¾åƒè¯†åˆ«åº“ï¼š[opencv](https://github.com/opencv/opencv.git)\n- ~~æ–‡å­—è¯†åˆ«åº“ï¼š[chineseocr_lite](https://github.com/DayBreak-u/chineseocr_lite.git)~~\n- æ–‡å­—è¯†åˆ«åº“ï¼š[PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)\n- æ·±åº¦å­¦ä¹ éƒ¨ç½²åº“ï¼š[FastDeploy](https://github.com/PaddlePaddle/FastDeploy)\n- æœºå™¨å­¦ä¹ åŠ é€Ÿå™¨ï¼š[onnxruntime](https://github.com/microsoft/onnxruntime)\n- ~~å…³å¡æ‰è½è¯†åˆ«ï¼š[ä¼é¹…ç‰©æµè¯†åˆ«](https://github.com/penguin-statistics/recognizer)~~\n- åœ°å›¾æ ¼å­è¯†åˆ«ï¼š[Arknights-Tile-Pos](https://github.com/yuanyan3060/Arknights-Tile-Pos)\n- C++ JSON åº“ï¼š[meojson](https://github.com/MistEO/meojson.git)\n- C++ è¿ç®—ç¬¦è§£æå™¨ï¼š[calculator](https://github.com/kimwalisch/calculator)\n- ~~C++ base64 ç¼–è§£ç ï¼š[cpp-base64](https://github.com/ReneNyffenegger/cpp-base64)~~\n- C++ è§£å‹å‹ç¼©åº“ï¼š[zlib](https://github.com/madler/zlib)\n- C++ Gzip å°è£…ï¼š[gzip-hpp](https://github.com/mapbox/gzip-hpp)\n- å®‰å“è§¦æ§äº‹ä»¶å™¨ï¼š[Minitouch](https://github.com/DeviceFarmer/minitouch)\n- å®‰å“è§¦æ§äº‹ä»¶å™¨ï¼š[MaaTouch](https://github.com/MaaAssistantArknights/MaaTouch)\n- WPF MVVM æ¡†æ¶ï¼š[Stylet](https://github.com/canton7/Stylet)\n- WPF æ§ä»¶åº“ï¼š[HandyControl](https://github.com/HandyOrg/HandyControl) -> [HandyControls](https://github.com/ghost1372/HandyControls)\n- C# æ—¥å¿—ï¼š[Serilog](https://github.com/serilog/serilog)\n- C# JSON åº“ï¼š[Newtonsoft.Json](https://github.com/JamesNK/Newtonsoft.Json) & [System.Text.Json](https://github.com/dotnet/runtime)\n- ~~ä¸‹è½½å™¨ï¼š[aria2](https://github.com/aria2/aria2)~~\n\n### æ•°æ®æº\n\n- ~~å…¬å¼€æ‹›å‹Ÿæ•°æ®ï¼š[æ˜æ—¥æ–¹èˆŸå·¥å…·ç®±](https://www.bigfun.cn/tools/aktools/hr)~~\n- ~~å¹²å‘˜åŠåŸºå»ºæ•°æ®ï¼š[PRTS Wiki](http://prts.wiki/)~~\n- å…³å¡æ•°æ®ï¼š[ä¼é¹…ç‰©æµæ•°æ®ç»Ÿè®¡](https://penguin-stats.cn/)\n- æ¸¸æˆæ•°æ®åŠèµ„æºï¼š[æ˜æ—¥æ–¹èˆŸå®¢æˆ·ç«¯ç´ æ](https://github.com/yuanyan3060/ArknightsGameResource)\n- æ¸¸æˆæ•°æ®ï¼š[ã€Šæ˜æ—¥æ–¹èˆŸã€‹Yostaræ¸¸æˆæ•°æ®](https://github.com/ArknightsAssets/ArknightsGamedata)\n\n### è´¡çŒ®/å‚ä¸è€…\n\næ„Ÿè°¢æ‰€æœ‰å‚ä¸åˆ°å¼€å‘/æµ‹è¯•ä¸­çš„æœ‹å‹ä»¬ï¼Œæ˜¯å¤§å®¶çš„å¸®åŠ©è®© MAA è¶Šæ¥è¶Šå¥½ï¼ (\\*Â´â–½ï½€)ãƒãƒ\n\n[![Contributors](https://contributors-img.web.app/image?repo=MaaAssistantArknights/MaaAssistantArknights&max=105&columns=15)](https://github.com/MaaAssistantArknights/MaaAssistantArknights/graphs/contributors)\n\n## å£°æ˜\n\n- æœ¬è½¯ä»¶ä½¿ç”¨ [GNU Affero General Public License v3.0 only](https://spdx.org/licenses/AGPL-3.0-only.html) å¼€æºï¼Œå¹¶é™„å¸¦é¢å¤– [ç”¨æˆ·åè®®](https://github.com/MaaAssistantArknights/MaaAssistantArknights/blob/dev/terms-of-service.md)ã€‚\n- æœ¬è½¯ä»¶ logo å¹¶éä½¿ç”¨ AGPL 3.0 åè®®å¼€æºï¼Œ[è€—æ¯›](https://weibo.com/u/3251357314)ã€vie ä¸¤ä½ç”»å¸ˆåŠè½¯ä»¶å…¨ä½“å¼€å‘è€…ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚ä¸å¾—ä»¥ AGPL 3.0 åè®®å·²æˆæƒä¸ºç”±åœ¨æœªç»æˆæƒçš„æƒ…å†µä¸‹ä½¿ç”¨æœ¬è½¯ä»¶ logoï¼Œä¸å¾—åœ¨æœªç»æˆæƒçš„æƒ…å†µä¸‹å°†æœ¬è½¯ä»¶ logo ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚\n- æœ¬è½¯ä»¶å¼€æºã€å…è´¹ï¼Œä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ã€‚è‹¥æ‚¨é‡åˆ°å•†å®¶ä½¿ç”¨æœ¬è½¯ä»¶è¿›è¡Œä»£ç»ƒå¹¶æ”¶è´¹ï¼Œå¯èƒ½æ˜¯è®¾å¤‡ä¸æ—¶é—´ç­‰è´¹ç”¨ï¼Œäº§ç”Ÿçš„é—®é¢˜åŠåæœä¸æœ¬è½¯ä»¶æ— å…³ã€‚\n\n### DirectML æ”¯æŒè¯´æ˜\n\næœ¬è½¯ä»¶æ”¯æŒ GPU åŠ é€ŸåŠŸèƒ½ï¼Œå…¶åœ¨ Windows å¹³å°ä¸Šä¾èµ–äº Microsoft æä¾›çš„ç‹¬ç«‹ç»„ä»¶ [DirectML](https://learn.microsoft.com/en-us/windows/ai/directml/)ã€‚DirectML å¹¶éæœ¬é¡¹ç›®çš„å¼€æºéƒ¨åˆ†ï¼Œä¹Ÿä¸å— AGPL 3.0 çš„çº¦æŸã€‚ä¸ºæ–¹ä¾¿ç”¨æˆ·ï¼Œæˆ‘ä»¬éšå®‰è£…åŒ…é™„å¸¦äº†ä¸€ä¸ªæœªç»ä¿®æ”¹çš„ DirectML.dll æ–‡ä»¶ã€‚å¦‚æœæ‚¨æ— éœ€ GPU åŠ é€ŸåŠŸèƒ½ï¼Œå¯å®‰å…¨åˆ é™¤è¯¥ DLL æ–‡ä»¶ï¼Œè½¯ä»¶çš„æ ¸å¿ƒåŠŸèƒ½ä»å¯æ­£å¸¸è¿è¡Œã€‚\n\n## å¹¿å‘Š\n\nç”¨æˆ·äº¤æµ QQ ç¾¤ï¼š[MAA ä½¿ç”¨ & ç²¥æ¸¸äº¤æµ QQ ç¾¤](https://api.maa.plus/MaaAssistantArknights/api/qqgroup/index.html)  \nDiscord æœåŠ¡å™¨: [é‚€è¯·é“¾æ¥](https://discord.gg/23DfZ9uA4V)  \nç”¨æˆ·äº¤æµ TG ç¾¤ï¼š[Telegram ç¾¤](https://t.me/+Mgc2Zngr-hs3ZjU1)  \nè‡ªåŠ¨æˆ˜æ–— JSON ä½œä¸šåˆ†äº«ï¼š[prts.plus](https://prts.plus)  \nBilibili ç›´æ’­é—´ï¼š[MrEO ç›´æ’­é—´](https://live.bilibili.com/2808861) ç›´æ’­æ•²ä»£ç  & [MAA-Official ç›´æ’­é—´](https://live.bilibili.com/27548877) æ¸¸æˆ/æ‚è°ˆ\n\næŠ€æœ¯ç¾¤ï¼ˆèˆŸæ— å…³ã€ç¦æ°´ï¼‰ï¼š[å†…å·åœ°ç‹±ï¼(QQ ç¾¤)](https://jq.qq.com/?_wv=1027&k=ypbzXcA2)  \nå¼€å‘è€…ç¾¤ï¼š[QQ ç¾¤](https://jq.qq.com/?_wv=1027&k=JM9oCk3C)\n\nå¦‚æœè§‰å¾—è½¯ä»¶å¯¹ä½ æœ‰å¸®åŠ©ï¼Œå¸®å¿™ç‚¹ä¸ª Star å§ï¼~ï¼ˆç½‘é¡µæœ€ä¸Šæ–¹å³ä¸Šè§’çš„å°æ˜Ÿæ˜Ÿï¼‰ï¼Œè¿™å°±æ˜¯å¯¹æˆ‘ä»¬æœ€å¤§çš„æ”¯æŒäº†ï¼\n",
      "stars_today": 14
    },
    {
      "id": 958136139,
      "name": "gallery",
      "full_name": "google-ai-edge/gallery",
      "description": "A gallery that showcases on-device ML/GenAI use cases and allows people to try and use models locally.",
      "html_url": "https://github.com/google-ai-edge/gallery",
      "stars": 15018,
      "forks": 1283,
      "language": "Kotlin",
      "topics": [],
      "created_at": "2025-03-31T17:47:28Z",
      "updated_at": "2026-01-28T00:15:02Z",
      "pushed_at": "2026-01-22T18:20:27Z",
      "open_issues": 157,
      "owner": {
        "login": "google-ai-edge",
        "avatar_url": "https://avatars.githubusercontent.com/u/150697620?v=4"
      },
      "readme": "# Google AI Edge Gallery âœ¨\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)\n[![GitHub release (latest by date)](https://img.shields.io/github/v/release/google-ai-edge/gallery)](https://github.com/google-ai-edge/gallery/releases)\n\n**Explore, Experience, and Evaluate the Future of On-Device Generative AI with Google AI Edge.**\n\nThe Google AI Edge Gallery is an experimental app that puts the power of cutting-edge Generative AI models directly into your hands, running entirely on your Android *(available now)* and iOS *(available now via TestFlight)* devices. Dive into a world of creative and practical AI use cases, all running locally, without needing an internet connection once the model is loaded. Experiment with different models, chat, ask questions with images and audio clip, explore prompts, and more!\n\nInstall the app today from Google Play\n\n<a href='https://play.google.com/store/apps/details?id=com.google.ai.edge.gallery'><img alt='Get it on Google Play' width=\"250\" src='https://play.google.com/intl/en_us/badges/static/images/badges/en_badge_web_generic.png'/></a>\n\nFor users without Google Play access, install the apk from the [**latest release**](https://github.com/google-ai-edge/gallery/releases/latest/)\n\n> [!IMPORTANT]\n> You must uninstall all previous versions of the app before installing this one. Past versions will no longer be working and supported.\n\n## iOS Testing via TestFlight\n\nWe're excited to announce that the app is now available for testing on iOS through TestFlight! We invite you to be among the first to try it out and share your feedback.\n\n***How to Join***:\n\n- Follow this [**public invitation link**](https://testflight.apple.com/join/nAtSQKTF) to get access.\n\n- Availability: Access is on a first-come, first-served basis. TestFlight currently limits the number of testers to 10,000.\n  \n- Supported device models: iOS devices with at least 6GB of RAM.\n\nWe appreciate your help with this early testing phase. Your feedback is invaluable as we work to improve the app. Once we've gathered and addressed all the feedback, we aim to officially launch on the App Store early 2026.\n\n<img width=\"480\" alt=\"01\" src=\"https://github.com/user-attachments/assets/09dbcf7e-a298-4063-920e-bfc88591f4a2\" />\n<img width=\"480\" alt=\"02\" src=\"https://github.com/user-attachments/assets/e2986bba-f807-42e1-9d5e-a5a978fa97e9\" />\n<img width=\"480\" alt=\"03\" src=\"https://github.com/user-attachments/assets/ad3aa9ab-e3b6-4a12-bbd4-885bb202aa0f\" />\n<img width=\"480\" alt=\"04\" src=\"https://github.com/user-attachments/assets/6441e752-e5f5-4753-9611-fa0122cdae49\" />\n<img width=\"480\" alt=\"05\" src=\"https://github.com/user-attachments/assets/a5ebcf15-640a-4c11-93ce-b92fe365f1a3\" />\n<img width=\"480\" alt=\"06\" src=\"https://github.com/user-attachments/assets/973c7a66-1906-400e-8fac-ee9b13b21aa1\" />\n<img width=\"480\" alt=\"07\" src=\"https://github.com/user-attachments/assets/d3227bc6-8d78-47a1-bbfa-93f009117882\" />\n\n## âœ¨ Core Features\n\n*   **ğŸ“± Run Locally, Fully Offline:** Experience the magic of GenAI without an internet connection. All processing happens directly on your device.\n*   **ğŸ¤– Choose Your Model:** Easily switch between different models from Hugging Face and compare their performance.\n*   **ğŸŒ» Tiny Garden**: Play an experimental and fully offline mini game that uses natural language to plant, water, and harvest flowers.\n*   **ğŸ“³ Mobile Actions**: Use our [open-source recipe](https://github.com/google-gemini/gemma-cookbook/blob/main/FunctionGemma/%5BFunctionGemma%5DFinetune_FunctionGemma_270M_for_Mobile_Actions_with_Hugging_Face.ipynb) to learn model fine-tuning, then load it in app to unlock offline device controls.\n*   **ğŸ–¼ï¸ Ask Image:** Upload images and ask questions about them. Get descriptions, solve problems, or identify objects.\n*   **ğŸ™ï¸ Audio Scribe:** Transcribe an uploaded or recorded audio clip into text or translate it into another language.\n*   **âœï¸ Prompt Lab:** Summarize, rewrite, generate code, or use freeform prompts to explore single-turn LLM use cases.\n*   **ğŸ’¬ AI Chat:** Engage in multi-turn conversations.\n*   **ğŸ“Š Performance Insights:** Real-time benchmarks (TTFT, decode speed, latency).\n*   **ğŸ§© Bring Your Own Model:** Test your local LiteRT `.litertlm` models.\n*   **ğŸ”— Developer Resources:** Quick links to model cards and source code.\n\n## ğŸ Get Started in Minutes!\n\n1. **Check OS Requirement**: Android 12 and up\n2.  **Download the App:**\n    - Install the app from [Google Play](https://play.google.com/store/apps/details?id=com.google.ai.edge.gallery).\n    - For users without Google Play access: install the apk from the [**latest release**](https://github.com/google-ai-edge/gallery/releases/latest/)\n3.  **Install & Explore:** For detailed installation instructions (including for corporate devices) and a full user guide, head over to our [**Project Wiki**](https://github.com/google-ai-edge/gallery/wiki)!\n\n## ğŸ› ï¸ Technology Highlights\n\n*   **Google AI Edge:** Core APIs and tools for on-device ML.\n*   **LiteRT:** Lightweight runtime for optimized model execution.\n*   **LLM Inference API:** Powering on-device Large Language Models.\n*   **Hugging Face Integration:** For model discovery and download.\n\n## âŒ¨ï¸ Development\n\nCheck out the [development notes](DEVELOPMENT.md) for instructions about how to build the app locally.\n\n## ğŸ¤ Feedback\n\nThis is an **experimental Beta release**, and your input is crucial!\n\n*   ğŸ **Found a bug?** [Report it here!](https://github.com/google-ai-edge/gallery/issues/new?assignees=&labels=bug&template=bug_report.md&title=%5BBUG%5D)\n*   ğŸ’¡ **Have an idea?** [Suggest a feature!](https://github.com/google-ai-edge/gallery/issues/new?assignees=&labels=enhancement&template=feature_request.md&title=%5BFEATURE%5D)\n\n## ğŸ“„ License\n\nLicensed under the Apache License, Version 2.0. See the [LICENSE](LICENSE) file for details.\n\n## ğŸ”— Useful Links\n\n*   [**Project Wiki (Detailed Guides)**](https://github.com/google-ai-edge/gallery/wiki)\n*   [Hugging Face LiteRT Community](https://huggingface.co/litert-community)\n*   [LLM Inference guide for Android](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/android)\n*   [LiteRT-LM](https://github.com/google-ai-edge/LiteRT-LM)\n*   [Google AI Edge Documentation](https://ai.google.dev/edge)\n",
      "stars_today": 14
    },
    {
      "id": 659402878,
      "name": "spring-ai",
      "full_name": "spring-projects/spring-ai",
      "description": "An Application Framework for AI Engineering",
      "html_url": "https://github.com/spring-projects/spring-ai",
      "stars": 7792,
      "forks": 2251,
      "language": "Java",
      "topics": [
        "artificial-intelligence",
        "hacktoberfest",
        "java",
        "spring-ai"
      ],
      "created_at": "2023-06-27T18:57:29Z",
      "updated_at": "2026-01-28T01:15:12Z",
      "pushed_at": "2026-01-26T17:06:31Z",
      "open_issues": 1163,
      "owner": {
        "login": "spring-projects",
        "avatar_url": "https://avatars.githubusercontent.com/u/317776?v=4"
      },
      "readme": "# Spring AI [![build status](https://github.com/spring-projects/spring-ai/actions/workflows/continuous-integration.yml/badge.svg)](https://github.com/spring-projects/spring-ai/actions/workflows/continuous-integration.yml) [![build status](https://github.com/spring-projects/spring-ai-integration-tests/actions/workflows/spring-ai-integration-tests.yml/badge.svg)](https://github.com/spring-projects/spring-ai-integration-tests/actions/workflows/spring-ai-integration-tests.yml) [![Maven Central](https://img.shields.io/maven-central/v/org.springframework.ai/spring-ai-model?label=Maven%20Central&versionPrefix=2.0)](https://central.sonatype.com/artifact/org.springframework.ai/spring-ai-model)\n\n### Spring Boot Version Compatibility\n\n> **Spring AI 2.x.x** ([main](https://github.com/spring-projects/spring-ai/tree/main) branch) - Spring Boot `4.x`\n>\n> **Spring AI 1.1.x** ([1.1.x](https://github.com/spring-projects/spring-ai/tree/1.1.x) branch) - Spring Boot `3.5.x`\n\n\nThe Spring AI project provides a Spring-friendly API and abstractions for developing AI applications.\n\nIts goal is to apply to the AI domain Spring ecosystem design principles such as portability and modular design and promote using POJOs as the building blocks of an application to the AI domain.\n\n![spring-ai-integration-diagram-3](https://docs.spring.io/spring-ai/reference/_images/spring-ai-integration-diagram-3.svg)\n\n> At its core, Spring AI addresses the fundamental challenge of AI integration: Connecting your enterprise __Data__ and __APIs__ with the __AI Models__.\n\nThe project draws inspiration from notable Python projects, such as [LangChain](https://docs.langchain.com/docs/) and [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/getting_started/concepts.html), but Spring AI is not a direct port of those projects. The project was founded with the belief that the next wave of Generative AI applications will not be only for Python developers but will be ubiquitous across many programming languages.\n\nYou can check out the blog post [Why Spring AI](https://spring.io/blog/2024/11/19/why-spring-ai) for additional motivations.\n\nThis is a high level feature overview.\nYou can find more details in the [Reference Documentation](https://docs.spring.io/spring-ai/reference/)\n\n* Support for all major [AI Model providers](https://docs.spring.io/spring-ai/reference/api/index.html) such as Anthropic, OpenAI, Microsoft, Amazon, Google, and Ollama. Supported model types include:\n  - [Chat Completion](https://docs.spring.io/spring-ai/reference/api/chatmodel.html)\n  - [Embedding](https://docs.spring.io/spring-ai/reference/api/embeddings.html)\n  - [Text to Image](https://docs.spring.io/spring-ai/reference/api/imageclient.html)\n  - [Audio Transcription](https://docs.spring.io/spring-ai/reference/api/audio/transcriptions.html)\n  - [Text to Speech](https://docs.spring.io/spring-ai/reference/api/audio/speech.html)\n  - [Moderation](https://docs.spring.io/spring-ai/reference/api/index.html#api/moderation)\n  - **Latest Models**: GPT-5, and other cutting-edge models for advanced AI applications.\n* Portable API support across AI providers for both synchronous and streaming options. Access to [model-specific features](https://docs.spring.io/spring-ai/reference/api/chatmodel.html#_chat_options) is also available.\n* [Structured Outputs](https://docs.spring.io/spring-ai/reference/api/structured-output-converter.html) - Mapping of AI Model output to POJOs.\n* Support for all major [Vector Database providers](https://docs.spring.io/spring-ai/reference/api/vectordbs.html) such as *Apache Cassandra, Azure Vector Search, Chroma, Elasticsearch, Milvus, MongoDB Atlas, MariaDB, Neo4j, Oracle, PostgreSQL/PGVector, Pinecone, Qdrant, Redis, and Weaviate*.\n* Portable API across Vector Store providers, including a novel SQL-like [metadata filter API](https://docs.spring.io/spring-ai/reference/api/vectordbs.html#metadata-filters).\n* [Tools/Function Calling](https://docs.spring.io/spring-ai/reference/api/tools.html) - permits the model to request the execution of client-side tools and functions, thereby accessing necessary real-time information as required.\n* [Observability](https://docs.spring.io/spring-ai/reference/observability/index.html) - Provides insights into AI-related operations.\n* Document injection [ETL framework](https://docs.spring.io/spring-ai/reference/api/etl-pipeline.html) for Data Engineering.\n* [AI Model Evaluation](https://docs.spring.io/spring-ai/reference/api/testing.html) - Utilities to help evaluate generated content and protect against hallucinated response.\n* [ChatClient API](https://docs.spring.io/spring-ai/reference/api/chatclient.html) - Fluent API for communicating with AI Chat Models, idiomatically similar to the WebClient and RestClient APIs.\n* [Advisors API](https://docs.spring.io/spring-ai/reference/api/advisors.html) - Encapsulates recurring Generative AI patterns, transforms data sent to and from Language Models (LLMs), and provides portability across various models and use cases.\n* Support for [Chat Conversation Memory](https://docs.spring.io/spring-ai/reference/api/chatclient.html#_chat_memory) and [Retrieval Augmented Generation (RAG)](https://docs.spring.io/spring-ai/reference/api/chatclient.html#_retrieval_augmented_generation).\n* Spring Boot Auto Configuration and Starters for all AI Models and Vector Stores - use the [start.spring.io](https://start.spring.io/) to select the Model or Vector-store of choice. \n\n## Getting Started\n\nPlease refer to the [Getting Started Guide](https://docs.spring.io/spring-ai/reference/getting-started.html) for instruction on adding your dependencies.\n\n## Project Resources\n\n* [Documentation](https://docs.spring.io/spring-ai/reference/)\n* [Issues](https://github.com/spring-projects/spring-ai/issues)\n<!-- * [Discussions](https://github.com/spring-projects/spring-ai/discussions) - Go here if you have a question, suggestion, or feedback! -->\n* [Awesome Spring AI](https://github.com/spring-ai-community/awesome-spring-ai) - A curated list of awesome resources, tools, tutorials, and projects for building generative AI applications using Spring AI\n* [Spring AI Examples](https://github.com/spring-projects/spring-ai-examples) contains example projects that explain specific features in more detail.\n* [Spring AI Community](https://github.com/spring-ai-community) - A community-driven organization for building Spring-based integrations with AI models, agents, vector databases, and more.\n\n## Breaking changes\n\n* Refer to the [upgrade notes](https://docs.spring.io/spring-ai/reference/upgrade-notes.html) to see how to upgrade to 1.0.0.M1 or higher.\n\n## Cloning the repo\n\nThis repository contains [large model files](https://github.com/spring-projects/spring-ai/tree/main/models/spring-ai-transformers/src/main/resources/onnx/all-MiniLM-L6-v2).\nTo clone it you have to either:\n\n- Ignore the large files (won't affect the spring-ai behaviour) :  `GIT_LFS_SKIP_SMUDGE=1 git clone git@github.com:spring-projects/spring-ai.git`.\n- Or install the [Git Large File Storage](https://git-lfs.com/) before cloning the repo.\n\n\n## Building\n\nThe project targets and build artifacts compatible with Java 17+, but requires JDK 21\nto build. This is enforced by the maven enforcer plugin.\n\nTo build with running unit tests\n\n```shell\n./mvnw clean package\n```\n\nTo build including integration tests.\n\n```shell\n./mvnw clean verify -Pintegration-tests\n```\n\nNote that you should set API key environment variables for OpenAI or other model providers before running.  If the API key isn't set for a specific model provider, the integration test is skipped.\n\nTo run a specific integration test allowing for up to two attempts to succeed.  This is useful when a hosted service is not reliable or times out.\n```shell\n./mvnw -pl vector-stores/spring-ai-pgvector-store -Pintegration-tests -Dfailsafe.rerunFailingTestsCount=2 -Dit.test=PgVectorStoreIT verify\n```\n\n### Integration Tests\nThere are many integration tests ,so it often isn't realistic to run them all at once.\n\nA quick pass through the most important pathways that runs integration tests for\n\n* OpenAI models \n* OpenAI autoconfiguration\n* PGVector\n* Chroma\n\ncan be done with the profile `-Pci-fast-integration-tests` and is used in the main CI build of this project.\n\nA full integration test is done twice a day in the [Spring AI Integration Test Repository](https://github.com/spring-projects/spring-ai-integration-tests)\n\nOne way to run integration tests on part of the code is to first do a quick compile and install of the project\n\n```shell\n./mvnw spring-javaformat:apply clean install -DskipTests -Dmaven.javadoc.skip=true\n```\nThen run the integration test for a specific module using the `-pl` option\n\n```shell\n./mvnw verify -Pintegration-tests -pl spring-ai-spring-boot-testcontainers\n```\n\n### Documentation\n\nTo build the docs\n```shell\n./mvnw -pl spring-ai-docs antora\n```\n\nThe docs are then in the directory `spring-ai-docs/target/antora/site/index.html`\n\n### Formatting the Source Code\n\nTo reformat using the [java-format plugin](https://github.com/spring-io/spring-javaformat)\n```shell\n./mvnw spring-javaformat:apply\n```\n### Updating License Headers\n\nTo update the year on license headers using the [license-maven-plugin](https://oss.carbou.me/license-maven-plugin/#goals)\n```shell\n./mvnw license:update-file-header -Plicense\n```\n### Javadocs\n\nTo check javadocs using the [javadoc:javadoc](https://maven.apache.org/plugins/maven-javadoc-plugin/)\n```shell\n./mvnw javadoc:javadoc -Pjavadoc\n```\n### Enabling Checkstyle\n\nCheckstyles are currently disabled, but you can enable them by doing the following:\n\n```shell\n./mvnw clean package -DskipTests -Ddisable.checks=false\n```\n\n#### Source Code Style\n\nSpring AI source code checkstyle tries to follow the checkstyle guidelines used by the core Spring Framework project with some exceptions.\nThe wiki pages\n[Code Style](https://github.com/spring-projects/spring-framework/wiki/Code-Style) and\n[IntelliJ IDEA Editor Settings](https://github.com/spring-projects/spring-framework/wiki/IntelliJ-IDEA-Editor-Settings)\ndefine the source file coding standards we use along with some IDEA editor settings we customize.\n\n## Contributing\n\nYour contributions are always welcome! Please read the [contribution guidelines](CONTRIBUTING.adoc) first.\n",
      "stars_today": 14
    },
    {
      "id": 676377723,
      "name": "DevOps-Projects",
      "full_name": "NotHarshhaa/DevOps-Projects",
      "description": "ğ‘«ğ’†ğ’—ğ‘¶ğ’‘ğ’” ğ‘¹ğ’†ğ’‚ğ’ ğ‘¾ğ’ğ’“ğ’ğ’… ğ‘·ğ’“ğ’ğ’‹ğ’†ğ’„ğ’•ğ’” ğ’‡ğ’ğ’“ ğ‘¨ğ’”ğ’‘ğ’Šğ’“ğ’Šğ’ğ’ˆ ğ‘«ğ’†ğ’—ğ‘¶ğ’‘ğ’” ğ‘¬ğ’ğ’ˆğ’Šğ’ğ’†ğ’†ğ’“ğ’” [ğ‘©ğ’†ğ’ˆğ’Šğ’ğ’ğ’†ğ’“ ğ’•ğ’ ğ‘¨ğ’…ğ’—ğ’‚ğ’ğ’„ğ’†ğ’…]",
      "html_url": "https://github.com/NotHarshhaa/DevOps-Projects",
      "stars": 3392,
      "forks": 3482,
      "language": "Java",
      "topics": [
        "devops",
        "devops-learning",
        "devops-poc",
        "devops-project",
        "devops-realtime",
        "devops-tools",
        "practical-devops",
        "projects-list",
        "realtime-devops-projects",
        "realtimeprojects"
      ],
      "created_at": "2023-08-09T04:19:38Z",
      "updated_at": "2026-01-27T21:16:47Z",
      "pushed_at": "2026-01-25T12:55:09Z",
      "open_issues": 15,
      "owner": {
        "login": "NotHarshhaa",
        "avatar_url": "https://avatars.githubusercontent.com/u/112948305?v=4"
      },
      "readme": "# **Real-World DevOps/Cloud Projects For Learning from Beginner to Advanced** â™\n\n<p align=\"center\">\n  <a href=\"https://trendshift.io/repositories/13316\" target=\"_blank\" rel=\"noopener noreferrer\">\n    <img src=\"https://trendshift.io/api/badge/repositories/13316\" alt=\"NotHarshhaa/DevOps-Projects | Trendshift\" style=\"width:350px;height:80px;max-width:100%;\" />\n  </a>\n</p>\n\n[![Forks][forks-shield]][forks-url]\n[![Stars][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n[![Last Commit][commit-shield]][commit-url]\n[![Code of Conduct][coc-shield]][coc-url]\n[![Contributing][contrib-shield]][contrib-url]\n\n<!-- MARKDOWN LINKS & IMAGES -->\n[forks-shield]: https://img.shields.io/github/forks/NotHarshhaa/DevOps-Projects?style=for-the-badge&logo=github&logoColor=white&color=orange\n[forks-url]: https://github.com/NotHarshhaa/DevOps-Projects/network/members\n\n[stars-shield]: https://img.shields.io/github/stars/NotHarshhaa/DevOps-Projects.svg?style=for-the-badge&logo=github&logoColor=white&color=brightgreen\n[stars-url]: https://github.com/NotHarshhaa/DevOps-Projects/stargazers\n\n[issues-shield]: https://img.shields.io/github/issues/NotHarshhaa/DevOps-Projects?style=for-the-badge&logo=github&logoColor=white&color=blue\n[issues-url]: https://github.com/NotHarshhaa/DevOps-Projects/issues\n\n[commit-shield]: https://img.shields.io/github/last-commit/NotHarshhaa/DevOps-Projects?style=for-the-badge&logo=git&logoColor=white&color=ff69b4\n[commit-url]: https://github.com/NotHarshhaa/DevOps-Projects/commits/master\n\n[coc-shield]: https://img.shields.io/badge/Code%20of%20Conduct-Enforced-blueviolet?style=for-the-badge&logo=handshake&logoColor=white\n[coc-url]: https://github.com/NotHarshhaa/DevOps-Projects/blob/master/CODE_OF_CONDUCT.md\n\n[contrib-shield]: https://img.shields.io/badge/Contributions-Welcome-ff69b4?style=for-the-badge&logo=gitbook&logoColor=white\n[contrib-url]: https://github.com/NotHarshhaa/DevOps-Projects/blob/master/CONTRIBUTING.md\n\n![DevOps-Projects](https://imgur.com/tlMOmn0.png)\n\n## ğŸ‘¥ **Project Ownership**\n\n<a href=\"https://github.com/NotHarshhaa/DevOps-Projects/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=NotHarshhaa/DevOps-Projects\" />\n</a>\n\n---\n\n## ğŸŒŸ **Star History**\n\n[![Star History Chart](https://api.star-history.com/svg?repos=NotHarshhaa/DevOps-Projects&type=Date)](https://www.star-history.com/#NotHarshhaa/DevOps-Projects&Date)\n\n---\n\n_*Welcome to the ultimate resource for **learning DevOps through hands-on projects!** This repository is designed to cater to aspiring **DevOps engineers** of all skill levels, from beginners taking their first steps in the field to advanced users looking to deepen their knowledge and expertise.*_\n\n![Welcome Badge](https://img.shields.io/badge/ğŸš€%20Welcome-Learn%20DevOps%20Through%20Hands--On%20Projects-10b981?style=for-the-badge&logo=opsgenie&logoColor=white)\n\n---\n\n## ğŸ§  **Purpose of the Repository**\n\nThis repository serves as a comprehensive resource for aspiring DevOps engineers to learn and implement real-world DevOps projects. It includes guides and solutions for deploying scalable systems, such as deploying a Java application on AWS using a 3-tier architecture and setting up scalable VPC architectures in the cloud.\n\nThe README files provide detailed instructions for implementing these projects, emphasizing practical deployment steps, pre-requisites, and validation processes. For example, one project focuses on deploying a Java-based login application integrated with a MySQL database, while another covers creating modular VPC network setups leveraging AWS services.\n\n![Purpose of Repository](https://img.shields.io/badge/ğŸ¯%20Purpose-Comprehensive%20DevOps%20Learning%20Hub-8b5cf6?style=for-the-badge&logo=target&logoColor=white)\n\n---\n\n## ğŸ” **Analysis of Features and Technologies**\n\nThe repository demonstrates extensive use of DevOps concepts and tools, focusing on AWS cloud infrastructure and automation. It features technologies such as:\n\n- **EC2, RDS, VPC, Auto Scaling, IAM roles**\n- **Maven, SonarCloud, JFrog Artifactory**\n- **Monitoring via CloudWatch**\n- **Custom AMIs, automation scripts**\n\nThese elements showcase a robust implementation of scalable, secure, and automated systems aligned with real-world DevOps practices.\n\n![Analysis of Features](https://img.shields.io/badge/ğŸ› ï¸%20Features%20&%20Technologies-AWS%2C%20CI%2FCD%2C%20Automation-10b981?style=for-the-badge&logo=amazonaws&logoColor=white)\n\n---\n\n## ğŸŒ **Real-time DevOps/Cloud Projects Showcase**\n\nTo improve readability and accessibility for users, this repository is also available as a modern and responsive web interface.\n\nA website showcasing a curated list of major real-time DevOps and Cloud projects, ranging from beginner to advanced levels. Built using **Next.js** and styled with **Tailwind CSS**, this project leverages a modern starter template for fast and responsive development. Perfect for learning and exploring hands-on DevOps and Cloud concepts!\n\nğŸ”— **Explore the site**: [projects.prodevopsguytech.com](https://projects.prodevopsguytech.com)\n\n![Showcase Website](https://img.shields.io/badge/ğŸŒ%20Project%20Showcase-Next.js%20+%20Tailwind%20UI-0ea5e9?style=for-the-badge&logo=vercel&logoColor=white)\n\n---\n\n## ğŸ”— **Related AWS Projects Repository**\n\nFor comprehensive AWS-specific projects and hands-on learning experiences, visit our dedicated AWS Projects repository:\n\n**AWS Projects Repository Highlights:**\n\n* **Real-world AWS Projects** from beginner to advanced levels\n* **AWS DevOps Focus** with practical implementation guides\n* **Hands-on Learning** with AWS services and best practices\n* **Industry-Relevant** projects covering EC2, VPC, RDS, Lambda, and more\n* **Community Driven** with active contributions and AWS expertise\n\nğŸ”— **Visit the AWS repository**: [AWS-Projects](https://github.com/NotHarshhaa/AWS-Projects)\n\n![AWS Projects](https://img.shields.io/badge/â˜ï¸%20AWS%20Projects-Dedicated%20AWS%20Learning%20Hub-ff9900?style=for-the-badge&logo=amazonaws&logoColor=white)\n\n---\n\n## **Repository Contents for DevOps Projects from Beginner to Advanced Levels**\n\n> [!IMPORTANT]\n>\n> _This repository contains a comprehensive collection of DevOps projects, each meticulously crafted to provide a hands-on learning experience. The projects are categorized into different skill levels to ensure that everyone, regardless of their current expertise, can find a suitable starting point and progressively enhance their skills._\n>\n> - **Beginner Projects:** Simple, foundational projects that introduce basic DevOps concepts and tools.\n> - **Intermediate Projects:** More complex projects that require a good understanding of DevOps fundamentals.\n> - **Advanced Projects:** Challenging projects designed to push your limits and deepen your understanding of sophisticated DevOps practices.\n\n![DevOps Levels](https://img.shields.io/badge/ğŸ“‚%20DevOps%20Projects-Beginner%20to%20Advanced-blueviolet?style=for-the-badge&logo=github&logoColor=white)\n\n---\n\n## **Integration of DevOps Technology with Other Technologies**\n\n> [!NOTE]\n> _In the modern tech landscape, DevOps doesn't exist in isolation. It intersects with a variety of other technologies, enhancing and being enhanced by them. This repository includes projects that integrate DevOps with several key technologies, allowing you to see how these integrations work in real-world scenarios._\n>\n> - **Machine Learning:** Implement DevOps practices to manage and deploy machine learning models efficiently.\n> - **Version Control with Git & GitHub:** Learn how to manage code versions and collaborate with others using Git and GitHub.\n> - **CI/CD Pipelines:** Set up continuous integration and continuous deployment pipelines to automate the testing and deployment of your applications.\n> - **Cloud Platforms (AWS, Azure, GCP):** Deploy applications on cloud platforms and leverage their services for scalability and reliability.\n> - **Containerization (Docker, Kubernetes):** Use container technologies to ensure that your applications run consistently across different environments.\n\n![Integration](https://img.shields.io/badge/ğŸ”—%20DevOps%20+%20Other%20Technologies-Seamless%20Integration-green?style=for-the-badge&logo=git&logoColor=white)\n\n---\n\n## **Project Scope**\n\n> [!IMPORTANT]\n> The projects span a wide array of topics within the DevOps domain, each designed to provide practical experience and insights into real-world scenarios. Hereâ€™s a detailed look at the areas covered:\n>\n> - **Automated Deployment:** Learn how to automate the deployment process to ensure that your applications are deployed quickly and reliably.\n> - **Continuous Integration & Continuous Deployment (CI/CD):** Understand how to set up and manage CI/CD pipelines to automate the testing and deployment of your code.\n> - **Infrastructure as Code (IaC):** Use tools like Terraform and CloudFormation to manage your infrastructure through code, ensuring consistency and scalability.\n> - **Monitoring & Logging:** Implement monitoring and logging solutions to keep track of your applicationsâ€™ performance and troubleshoot issues.\n> - **Security & Compliance:** Learn how to incorporate security practices into your DevOps workflows to ensure that your applications are secure and compliant with regulations.\n> - **Scalability & Performance Optimization:** Understand how to scale your applications and optimize their performance to handle increasing loads.\n\n![Project Scope](https://img.shields.io/badge/ğŸ› ï¸%20Project%20Scope-Hands--on%20DevOps%20Coverage-blue?style=for-the-badge&logo=vercel&logoColor=white)\n\n---\n\n## **Why Explore This Repository?**\n\n> [!NOTE]\n> This repository is a treasure trove of learning opportunities, tailored to help you grow in the DevOps field. Here's why you should dive in:\n>\n> - **Hands-on Experience:** Each project is designed to provide you with practical, hands-on experience. You'll work through real-world challenges and gain the skills you need to succeed in the industry.\n> - **Skill Enhancement:** Whether you're just starting or looking to build on existing skills, the projects are structured to guide you through a learning path that will enhance your capabilities.\n> - **Industry Relevance:** Stay up-to-date with the latest trends and technologies in DevOps. The projects reflect current industry practices, ensuring that what you learn is relevant and applicable.\n> - **Community Engagement:** Join a community of like-minded learners and professionals. Share your projects, seek feedback, and collaborate on exciting DevOps initiatives.\n\n![Why Explore This Repository](https://img.shields.io/badge/ğŸ“š%20Why%20Explore%20This%20Repository%3F-Unlock%20DevOps%20Mastery-brightgreen?style=for-the-badge&logo=bookstack&logoColor=white)\n\n---\n\n## **Code of Conduct**\n\n> [!CAUTION]\n>\n> We are committed to fostering a welcoming and respectful environment for all contributors. Please take a moment to review our [Code of Conduct](./CODE_OF_CONDUCT.md) before participating in this community.\n\n[![Code of Conduct](https://img.shields.io/badge/Code%20of%20Conduct-Enforced-blueviolet?style=for-the-badge&logo=handshake&logoColor=white)](https://github.com/NotHarshhaa/DevOps-Projects/blob/master/CODE_OF_CONDUCT.md)\n\n---\n\n## **Contribute and Collaborate**\n\n> [!TIP]\n> This repository thrives on community contributions and collaboration. Hereâ€™s how you can get involved:\n>\n> - **Fork the Repository:** Create your own copy of the repository to work on.\n> - **Submit Pull Requests:** Contribute your projects or improvements to existing projects by submitting pull requests.\n> - **Engage with Others:** Participate in discussions, provide feedback on othersâ€™ projects, and collaborate to create better solutions.\n> - **Share Your Knowledge:** If youâ€™ve developed a new project or learned something valuable, share it with the community. Your contributions can help others in their learning journey.\n>\n> **We follow best practices for contribution.**\n\n[![Contributing](https://img.shields.io/badge/Contribute-Guide-ff69b4?style=for-the-badge&logo=gitbook&logoColor=white)](https://github.com/NotHarshhaa/DevOps-Projects/blob/master/CONTRIBUTING.md)\n\n---\n\n## ğŸŒ **Join the Community**\n\n> [!IMPORTANT]\n> Be a part of our active DevOps community:\n\n[![Join Telegram](https://img.shields.io/badge/Join%20Us%20on-Telegram-26A5E4?style=for-the-badge&logo=telegram&logoColor=white)](https://t.me/prodevopsguy) \n[![Follow on GitHub](https://img.shields.io/badge/Follow%20me%20on-GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/NotHarshhaa)\n\n---\n\n## â­ **Hit the Star!**\n\n**If you find this helpful, donâ€™t forget to give this repository a star. Your support matters!** â­\n\n![Star Badge](https://img.shields.io/badge/â­%20Support-Give%20a%20Star%20if%20You%20Like%20It-ffd700?style=for-the-badge&logo=github&logoColor=white)\n\n---\n\n## ğŸ› ï¸ **Author & Community**\n\nThis project is crafted by **[Harshhaa](https://github.com/NotHarshhaa)** ğŸ’¡  \nIâ€™d love to hear your feedback! Feel free to share your thoughts.  \n\n![Author Badge](https://img.shields.io/badge/ğŸ› ï¸%20Author%20&%20Community-Crafted%20with%20Passion%20by%20Harshhaa-8a2be2?style=for-the-badge&logo=github&logoColor=white)\n\n---\n\n## ğŸ“§ **Connect with me:**\n\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-%230077B5.svg?style=for-the-badge&logo=linkedin&logoColor=white)](https://linkedin.com/in/harshhaa-vardhan-reddy) [![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/NotHarshhaa) [![Telegram](https://img.shields.io/badge/Telegram-26A5E4?style=for-the-badge&logo=telegram&logoColor=white)](https://t.me/prodevopsguy) [![Dev.to](https://img.shields.io/badge/Dev.to-0A0A0A?style=for-the-badge&logo=dev.to&logoColor=white)](https://dev.to/notharshhaa) [![Hashnode](https://img.shields.io/badge/Hashnode-2962FF?style=for-the-badge&logo=hashnode&logoColor=white)](https://hashnode.com/@prodevopsguy)  \n\n---\n\n## ğŸ“¢ **Stay Connected**\n\n![Follow Me](https://imgur.com/2j7GSPs.png)\n\n![Stay Connected](https://img.shields.io/badge/ğŸ“¢%20Stay%20Connected-Join%20our%20DevOps%20Community-orange?style=for-the-badge&logo=telegram&logoColor=white)\n",
      "stars_today": 14
    },
    {
      "id": 708994262,
      "name": "Zelda64Recomp",
      "full_name": "Zelda64Recomp/Zelda64Recomp",
      "description": "Static recompilation of Majora's Mask (and soon Ocarina of Time) for PC (Windows/Linux/Mac)",
      "html_url": "https://github.com/Zelda64Recomp/Zelda64Recomp",
      "stars": 6725,
      "forks": 309,
      "language": "C",
      "topics": [],
      "created_at": "2023-10-23T19:54:50Z",
      "updated_at": "2026-01-28T01:10:17Z",
      "pushed_at": "2025-12-29T23:19:18Z",
      "open_issues": 114,
      "owner": {
        "login": "Zelda64Recomp",
        "avatar_url": "https://avatars.githubusercontent.com/u/170279347?v=4"
      },
      "readme": "# Zelda 64: Recompiled\nZelda 64: Recompiled is a project that uses [N64: Recompiled](https://github.com/Mr-Wiseguy/N64Recomp) to **statically recompile** Majora's Mask (and soon Ocarina of Time) into a native port with many new features, enhancements, and extensive mod support. This project uses [RT64](https://github.com/rt64/rt64) as the rendering engine to provide graphical enhancements.\n\n### [Check out the latest release here](https://github.com/Mr-Wiseguy/Zelda64Recomp/releases/latest).\n\nJoin the [N64: Recompiled Community Discord](https://discord.gg/AWZThJ4dPf) to discuss this and other N64: Recompiled projects!\n\n[![Discord Invitation](https://discordapp.com/api/guilds/1374083583739826328/widget.png?style=banner2 'N64 Recomp')](https://discord.gg/AWZThJ4dPf)\n\n### **This repository and its releases do not contain game assets. The original game is required to build or run this project.**\n\n<div align=\"left\" valign=\"middle\">\n<a href=\"https://runblaze.dev\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://www.runblaze.dev/logo_dark.png\">\n   <img align=\"right\" src=\"https://www.runblaze.dev/logo_light.png\" height=\"102px\"/>\n </picture>\n</a>\n\n<br style=\"display: none;\"/>\n\n_Special thanks to [Blaze](https://runblaze.dev) for their support of this project. They provide high-performance Linux (AMD64 & ARM64) and Apple Silicon macOS runners for GitHub Actions, greatly reducing our automated build times._\n\n</div>\n\n## Table of Contents\n* [System Requirements](#system-requirements)\n* [Features](#features)\n  * [Plug and Play](#plug-and-play)\n  * [Fully Intact N64 Effects](#fully-intact-n64-effects)\n  * [Easy-to-Use Menus](#easy-to-use-menus)\n  * [High Framerate Support](#high-framerate-support)\n  * [Widescreen and Ultrawide Support](#widescreen-and-ultrawide-support)\n  * [Mod Support](#mod-support)\n  * [Dual Analog Camera](#dual-analog-camera)\n  * [Gyro Aim](#gyro-aim)\n  * [Additional Control Options](#additional-control-options)\n  * [Autosaving](#autosaving)\n  * [Low Input Lag](#low-input-lag)\n  * [Instant Load Times](#instant-load-times)\n  * [Linux and Steam Deck Support](#linux-and-steam-deck-support)\n* [Planned Features](#planned-features)\n* [FAQ](#faq)\n* [Known Issues](#known-issues)\n* [Building](#building)\n* [Libraries Used and Projects Referenced](#libraries-used-and-projects-referenced)\n\n## System Requirements\nA GPU supporting Direct3D 12.0 (Shader Model 6), Vulkan 1.2, or Metal Argument Buffers Tier 2 support is required to run this project. The oldest GPUs that should be supported for each vendor are:\n* GeForce GT 630\n* Radeon HD 7750 (the one from 2012, not to be confused with the RX 7000 series) and newer\n* Intel HD 510 (Skylake)\n* A Mac with Apple Silicon or an Intel 7th Gen CPU with MacOS 13.0+\n\nOn x86-64 PCs, a CPU supporting the SSE4.1 instruction set is also required (Intel Core 2 Penryn series or AMD Bulldozer and newer). ARM64 builds will work on any ARM64 CPU.\n\nIf you have issues with crashes on startup, make sure your graphics drivers are fully up to date. \n\n## Features\n\n#### Plug and Play\nSimply provide your copy of the North American version of the game in the main menu and start playing! This project will automatically load assets from the provided copy, so there is no need to go through a separate extraction step or build the game yourself. Other versions of the game may be supported in the future.\n\n#### Fully Intact N64 Effects\nA lot of care was put into RT64 to make sure all graphical effects were rendered exactly as they did originally on the N64. No workarounds or \"hacks\" were made to replicate these effects, with the only modifications to them being made for enhancement purposes such as widescreen support. This includes framebuffer effects like the grayscale cutscenes and the Deku bubble projectile, depth effects like the lens of truth, decals such as shadows or impact textures, accurate lighting, shading effects like the fire arrows and bomb explosions, and various textures that are often rendered incorrectly.\n\n#### Easy-to-Use Menus\nGameplay settings, graphics settings, input mappings, and audio settings can all be configured with the in-game config menu. The menus can all be used with mouse, controller, or keyboard for maximum convenience.\n\n#### High Framerate Support\nPlay at any framerate you want thanks to functionality provided by RT64! Game objects and terrain, texture scrolling, screen effects, and most HUD elements are all rendered at high framerates. By default, this project is configured to run at your monitor's refresh rate. You can also play at the original framerate of the game if you prefer. **Changing framerate has no effect on gameplay.**\n\n**Note**: External framerate limiters (such as the NVIDIA Control Panel) are known to potentially cause problems, so if you notice any stuttering then turn them off and use the manual framerate slider in the in-game graphics menu instead.\n\n#### Widescreen and Ultrawide Support\nAny aspect ratio is supported, with most effects modded to work correctly in widescreen. The HUD can also be positioned at 16:9 when using ultrawide aspect ratios if preferred.\n\n**Note**: Some animation quirks can be seen at the edges of the screen in certain cutscenes when using very wide aspect ratios.\n\n#### Mod Support\nInstall community made mods and texture packs! Mods can change any part of the game, including adding completely new features and content. You can install mods by simply dragging the mod files onto the game window before starting the game or by clicking the **Install Mods** button in the mod menu. Mods can be toggled in the mod menu, and some mods can be configured there as well.\n\nMany mods are available on the project's Thunderstore page: https://thunderstore.io/c/zelda-64-recompiled/. The Thunderstore mod manager/r2modman is not required or supported, so be sure to click the \"Manual Download\" button when downloading a mod instead of the \"Install with Mod Manager\" button.\n\nIf you're interested in making mods for this project, check out [the mod template](https://github.com/Zelda64Recomp/MMRecompModTemplate) and [the modding documentation](https://hackmd.io/fMDiGEJ9TBSjomuZZOgzNg). If you're interested in making texture packs, check out [the RT64 documentation](https://github.com/rt64/rt64/blob/main/TEXTURE-PACKS.md).\n\n#### Dual Analog Camera\nPlay with a dual analog control layout like later entries in the series! When this option is enabled, the right stick will control the camera. You can still have the C-Buttons mapped to the right stick if you so wish, so long as you also map them to other buttons on the controller. The right stick C-button inputs will be \"silenced\", except when you take out the ocarina, so you can still play the ocarina with the right stick.\n\n#### Gyro Aim\nWhen playing with a supported controller, first-person items such as the bow can be aimed with your controller's gyro sensor. This includes (but is not limited to) controllers such as the Dualshock 4, Dualsense, Switch Pro, and most third party Switch controllers (such as the 8BitDo Pro 2 in Switch mode).\n\n**Note**: Gamepad mappers such as BetterJoy or DS4Windows may intercept gyro data and prevent the game from receiving it. Most controllers are natively supported, so turning gamepad mappers off is recommended if you want to use gyro.\n\n#### Additional Control Options\nCustomize your experience by setting your stick deadzone to your liking, as well as adjusting the X and Y axis inversion for both aiming and the optional dual analog camera.\n\n#### Autosaving\nNever worry about losing progress if your power goes out thanks to autosaving! The autosave system is designed to respect Majora's Mask's original save system and maintain the intention of owl saves by triggering automatically and replacing the previous autosave or owl save. However, if you'd still rather play with the untouched save system, simply turn off autosaving in the ingame menu.\n\n#### Low Input Lag\nThis project has been optimized to have as little input lag as possible, making the game feel more responsive than ever!\n\n#### Instant Load Times\nSaving and loading files, going from place to place, and pausing all happen in the blink of an eye thanks to the game running natively on modern hardware.\n\n#### Linux and Steam Deck Support\nA Linux binary as well as a Flatpak is available for playing on most up-to-date distros, including on the Steam Deck.\n\nTo play on Steam Deck, extract the Linux build onto your deck. Then, in desktop mode, right click the Zelda64Recompiled executable file and select \"Add to Steam\". From there, you can return to Gaming mode and configure the controls as needed. See the [Steam Deck gyro aim FAQ section](#how-do-i-set-up-gyro-aiming-on-steam-deck) for more detailed instructions.\n\n## Planned Features\n* Ocarina of Time support\n* Ray Tracing and Higher Quality Model Replacements (via RT64)\n* Multi-language support with support for loading custom translations\n\n## FAQ\n\n#### What is static recompilation?\nStatic recompilation is the process of automatically translating an application from one platform to another. For more details, check out the full description of how this project's recompilation works here: [N64: Recompiled](https://github.com/Mr-Wiseguy/N64Recomp).\n\n#### How is this related to the decompilation project?\nUnlike N64 ports in the past, this project is not based on the source code provided by a decompilation of the game. This is because static recompilation bypasses the need for decompiled source code when making a port, allowing ports to be made **without source code**. However, the reverse engineering work done by the decompilation team was invaluable for providing some of the enhancements featured in this project. For this reason, the project uses headers and some functions from the decompilation project in order to make modifications to the game. Many thanks to the decompilation team for all of the hard work they've done.\n\n#### How do I set up gyro aiming on Steam Deck?\nThis project provides mouse aiming as a way to allow using gyro on Steam Deck, as the Steam Deck's gyro sensors cannot be read directly. First, launch the game in Gaming Mode, press the Steam button and go to \"Controller Settings\". Choose \"Controller Settings\" again in the menu that follows, and then set \"Gyro Behavior\" to \"As Mouse\".\n\n![Controller Settings menu](docs/deck_gyro_1.jpg)\n\nYou'll probably also want to change the default behavior so that you don't need to be touching the right stick to allow gyro input. To do so, click on the Gear icon to the right of \"Gyro Behavior\" and ensure that \"Gyro Activation Buttons\" is set to \"None Selected (Gyro Always On).\" If this isn't the case, then select that option and then press \"Select None\" in the following menu.\n\n#### Where is the savefile stored?\n- Windows: `%LOCALAPPDATA%\\Zelda64Recompiled\\saves`\n- Linux: `~/.config/Zelda64Recompiled/saves`\n- macOS: `~/Library/Application Support/Zelda64Recompiled/saves`\n\n#### How do I choose a different ROM?\n**You don't.** This project is **only** a port of Majora's Mask (and Ocarina of Time in the future), and it will only accept one specific ROM: the US version of the N64 release of Majora's Mask. ROMs in formats other than .z64 will be automatically converted, as long as it is the correct ROM. **It is not an emulator and it cannot run any arbitrary ROM.**\n\nInstead, you can change the game by installing mods. See the [mod support](#mod-support) section for details.\n\n#### Does this project have a randomizer?\nYes, there is a randomizer available as a mod for this project which can be found at https://github.com/RecompRando/MMRecompRando/releases/latest. Simply download MMRecompRando.zip from the releases and install it like any other mod.\n\n#### Can you run this project as a portable application?\nYes, if you place a file named `portable.txt` in the same folder as the executable then this project will run in portable mode. In portable mode, the save files, config files, and mods are placed in the same folder as the executable.\n\n## Known Issues\n* Overlays such as MSI Afterburner and other software such as Wallpaper Engine can cause performance issues with this project that prevent the game from rendering correctly. Disabling such software is recommended.\n\n## Building\nBuilding is not required to play this project, as prebuilt binaries (which do not contain game assets) can be found in the [Releases](https://github.com/Mr-Wiseguy/Zelda64Recomp/releases) section. Instructions on how to build this project can be found in the [BUILDING.md](BUILDING.md) file.\n\n## Libraries Used and Projects Referenced\n* [RT64](https://github.com/rt64/rt64) for the project's rendering engine\n* [RmlUi](https://github.com/mikke89/RmlUi) for building the menus and launcher\n* [lunasvg](https://github.com/sammycage/lunasvg) for SVG rendering, used by RmlUi\n* [FreeType](https://freetype.org/) for font rendering, used by RmlUi  \n* [moodycamel::ConcurrentQueue](https://github.com/cameron314/concurrentqueue) for semaphores and fast, lock-free MPMC queues\n* [Gamepad Motion Helpers](https://github.com/JibbSmart/GamepadMotionHelpers) for sensor fusion and calibration algorithms to implement gyro aiming\n* [Majora's Mask Decompilation](https://github.com/zeldaret/mm) for headers and some function definitions, used for making patches or some enhancements\n* [Ares emulator](https://github.com/ares-emulator/ares) for RSP vector instruction reference implementations, used in RSP recompilation\n",
      "stars_today": 14
    },
    {
      "id": 22887094,
      "name": "tesseract",
      "full_name": "tesseract-ocr/tesseract",
      "description": "Tesseract Open Source OCR Engine (main repository)",
      "html_url": "https://github.com/tesseract-ocr/tesseract",
      "stars": 72107,
      "forks": 10473,
      "language": "C++",
      "topics": [
        "hacktoberfest",
        "lstm",
        "machine-learning",
        "ocr",
        "ocr-engine",
        "tesseract",
        "tesseract-ocr"
      ],
      "created_at": "2014-08-12T18:04:59Z",
      "updated_at": "2026-01-28T01:42:38Z",
      "pushed_at": "2026-01-08T02:30:46Z",
      "open_issues": 462,
      "owner": {
        "login": "tesseract-ocr",
        "avatar_url": "https://avatars.githubusercontent.com/u/8401422?v=4"
      },
      "readme": "# Tesseract OCR\n\n[![Coverity Scan Build Status](https://scan.coverity.com/projects/tesseract-ocr/badge.svg)](https://scan.coverity.com/projects/tesseract-ocr)\n[![CodeQL](https://github.com/tesseract-ocr/tesseract/workflows/CodeQL/badge.svg)](https://github.com/tesseract-ocr/tesseract/security/code-scanning)\n[![OSS-Fuzz](https://img.shields.io/badge/oss--fuzz-fuzzing-brightgreen)](https://issues.oss-fuzz.com/issues?q=is:open%20title:tesseract-ocr)\n\\\n[![GitHub license](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](https://raw.githubusercontent.com/tesseract-ocr/tesseract/main/LICENSE)\n[![Downloads](https://img.shields.io/badge/download-all%20releases-brightgreen.svg)](https://github.com/tesseract-ocr/tesseract/releases/)\n\n## Table of Contents\n\n* [Tesseract OCR](#tesseract-ocr)\n  * [About](#about)\n  * [Brief history](#brief-history)\n  * [Installing Tesseract](#installing-tesseract)\n  * [Running Tesseract](#running-tesseract)\n  * [For developers](#for-developers)\n  * [Support](#support)\n  * [License](#license)\n  * [Dependencies](#dependencies)\n  * [Latest Version of README](#latest-version-of-readme)\n\n## About\n\nThis package contains an **OCR engine** - `libtesseract` and a **command line program** - `tesseract`.\n\nTesseract 4 adds a new neural net (LSTM) based [OCR engine](https://en.wikipedia.org/wiki/Optical_character_recognition) which is focused on line recognition, but also still supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. Compatibility with Tesseract 3 is enabled by using the Legacy OCR Engine mode (--oem 0).\nIt also needs [traineddata](https://tesseract-ocr.github.io/tessdoc/Data-Files.html) files which support the legacy engine, for example those from the [tessdata](https://github.com/tesseract-ocr/tessdata) repository.\n\nStefan Weil is the current lead developer. Ray Smith was the lead developer until 2017. The maintainer is Zdenko Podobny. For a list of contributors see [AUTHORS](https://github.com/tesseract-ocr/tesseract/blob/main/AUTHORS)\nand GitHub's log of [contributors](https://github.com/tesseract-ocr/tesseract/graphs/contributors).\n\nTesseract has **unicode (UTF-8) support**, and can **recognize [more than 100 languages](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html)** \"out of the box\".\n\nTesseract supports **[various image formats](https://tesseract-ocr.github.io/tessdoc/InputFormats)** including PNG, JPEG and TIFF.\n\nTesseract supports **various output formats**: plain text, hOCR (HTML), PDF, invisible-text-only PDF, TSV, ALTO and PAGE.\n\nYou should note that in many cases, in order to get better OCR results, you'll need to **[improve the quality](https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html) of the image** you are giving Tesseract.\n\nThis project **does not include a GUI application**. If you need one, please see the [3rdParty](https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty.html) documentation.\n\nTesseract **can be trained to recognize other languages**.\nSee [Tesseract Training](https://tesseract-ocr.github.io/tessdoc/Training-Tesseract.html) for more information.\n\n## Brief history\n\nTesseract was originally developed at Hewlett-Packard Laboratories Bristol UK and at Hewlett-Packard Co, Greeley Colorado USA between 1985 and 1994, with some more changes made in 1996 to port to Windows, and some C++izing in 1998. In 2005 Tesseract was open sourced by HP. From 2006 until August 2017 it was developed by Google.\n\nMajor version 5 is the current stable version and started with release\n[5.0.0](https://github.com/tesseract-ocr/tesseract/releases/tag/5.0.0) on November 30, 2021. Newer minor versions and bugfix versions are available from\n[GitHub](https://github.com/tesseract-ocr/tesseract/releases/).\n\nLatest source code is available from [main branch on GitHub](https://github.com/tesseract-ocr/tesseract/tree/main).\nOpen issues can be found in [issue tracker](https://github.com/tesseract-ocr/tesseract/issues),\nand [planning documentation](https://tesseract-ocr.github.io/tessdoc/Planning.html).\n\nSee **[Release Notes](https://tesseract-ocr.github.io/tessdoc/ReleaseNotes.html)**\nand **[Change Log](https://github.com/tesseract-ocr/tesseract/blob/main/ChangeLog)** for more details of the releases.\n\n## Installing Tesseract\n\nYou can either [Install Tesseract via pre-built binary package](https://tesseract-ocr.github.io/tessdoc/Installation.html)\nor [build it from source](https://tesseract-ocr.github.io/tessdoc/Compiling.html).\n\nBefore building Tesseract from source, please check that your system has a compiler which is one of the [supported compilers](https://tesseract-ocr.github.io/tessdoc/supported-compilers.html).\n\n## Running Tesseract\n\nBasic **[command line usage](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html)**:\n\n    tesseract imagename outputbase [-l lang] [--oem ocrenginemode] [--psm pagesegmode] [configfiles...]\n\nFor more information about the various command line options use `tesseract --help` or `man tesseract`.\n\nExamples can be found in the [documentation](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html#simplest-invocation-to-ocr-an-image).\n\n## For developers\n\nDevelopers can use `libtesseract` [C](https://github.com/tesseract-ocr/tesseract/blob/main/include/tesseract/capi.h) or\n[C++](https://github.com/tesseract-ocr/tesseract/blob/main/include/tesseract/baseapi.h) API to build their own application. If you need bindings to `libtesseract` for other programming languages, please see the\n[wrapper](https://tesseract-ocr.github.io/tessdoc/AddOns.html#tesseract-wrappers) section in the AddOns documentation.\n\nDocumentation of Tesseract generated from source code by doxygen can be found on [tesseract-ocr.github.io](https://tesseract-ocr.github.io/).\n\n## Support\n\nBefore you submit an issue, please review **[the guidelines for this repository](https://github.com/tesseract-ocr/tesseract/blob/main/CONTRIBUTING.md)**.\n\nFor support, first read the [documentation](https://tesseract-ocr.github.io/tessdoc/),\nparticularly the [FAQ](https://tesseract-ocr.github.io/tessdoc/FAQ.html) to see if your problem is addressed there.\nIf not, search the [Tesseract user forum](https://groups.google.com/g/tesseract-ocr), the [Tesseract developer forum](https://groups.google.com/g/tesseract-dev) and [past issues](https://github.com/tesseract-ocr/tesseract/issues), and if you still can't find what you need, ask for support in the mailing-lists.\n\nMailing-lists:\n\n* [tesseract-ocr](https://groups.google.com/g/tesseract-ocr) - For tesseract users.\n* [tesseract-dev](https://groups.google.com/g/tesseract-dev) - For tesseract developers.\n\nPlease report an issue only for a **bug**, not for asking questions.\n\n## License\n\n    The code in this repository is licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n**NOTE**: This software depends on other packages that may be licensed under different open source licenses.\n\nTesseract uses [Leptonica library](http://leptonica.com/) which essentially\nuses a [BSD 2-clause license](http://leptonica.com/about-the-license.html).\n\n## Dependencies\n\nTesseract uses [Leptonica library](https://github.com/DanBloomberg/leptonica)\nfor opening input images (e.g. not documents like pdf).\nIt is suggested to use leptonica with built-in support for [zlib](https://zlib.net),\n[png](https://sourceforge.net/projects/libpng) and\n[tiff](http://www.simplesystems.org/libtiff) (for multipage tiff).\n\n## Latest Version of README\n\nFor the latest online version of the README.md see:\n\n<https://github.com/tesseract-ocr/tesseract/blob/main/README.md>\n",
      "stars_today": 13
    },
    {
      "id": 31288958,
      "name": "vault",
      "full_name": "hashicorp/vault",
      "description": "A tool for secrets management, encryption as a service, and privileged access management",
      "html_url": "https://github.com/hashicorp/vault",
      "stars": 34123,
      "forks": 4523,
      "language": "Go",
      "topics": [
        "go",
        "secrets",
        "vault"
      ],
      "created_at": "2015-02-25T00:15:59Z",
      "updated_at": "2026-01-28T02:11:02Z",
      "pushed_at": "2026-01-28T01:30:46Z",
      "open_issues": 1423,
      "owner": {
        "login": "hashicorp",
        "avatar_url": "https://avatars.githubusercontent.com/u/761456?v=4"
      },
      "readme": "# Vault [![build](https://github.com/hashicorp/vault/actions/workflows/build.yml/badge.svg)](https://github.com/hashicorp/vault/actions/workflows/build.yml) [![ci](https://github.com/hashicorp/vault/actions/workflows/ci.yml/badge.svg)](https://github.com/hashicorp/vault/actions/workflows/ci.yml)  [![vault enterprise](https://img.shields.io/badge/vault-enterprise-yellow.svg?colorB=7c8797&colorA=000000)](https://www.hashicorp.com/products/vault/?utm_source=github&utm_medium=banner&utm_campaign=github-vault-enterprise)\n\n----\n\n**Please note**: We take Vault's security and our users' trust very seriously. If you believe you have found a security issue in Vault, _please responsibly disclose_ by contacting us at [security@hashicorp.com](mailto:security@hashicorp.com).\n\n----\n\n- Website: [developer.hashicorp.com/vault](https://developer.hashicorp.com/vault)\n- Announcement list: [Google Groups](https://groups.google.com/group/hashicorp-announce)\n- Discussion forum: [Discuss](https://discuss.hashicorp.com/c/vault)\n- Documentation: [https://developer.hashicorp.com/vault/docs](https://developer.hashicorp.com/vault/docs)\n- Tutorials: [https://developer.hashicorp.com/vault/tutorials](https://developer.hashicorp.com/vault/tutorials)\n- Certification exam: [https://developer.hashicorp.com/certifications/security-automation](https://developer.hashicorp.com/certifications/security-automation)\n- Documentation source: [https://github.com/hashicorp/web-unified-docs](https://github.com/hashicorp/web-unified-docs)\n\n<img width=\"300\" alt=\"Vault Logo\" src=\"https://github.com/hashicorp/vault/blob/f22d202cde2018f9455dec755118a9b84586e082/Vault_PrimaryLogo_Black.png\">\n\nVault is a tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, certificates, and more. Vault provides a unified interface to any secret, while providing tight access control and recording a detailed audit log.\n\nA modern system requires access to a multitude of secrets: database credentials, API keys for external services, credentials for service-oriented architecture communication, etc. Understanding who is accessing what secrets is already very difficult and platform-specific. Adding on key rolling, secure storage, and detailed audit logs is almost impossible without a custom solution. This is where Vault steps in.\n\nThe key features of Vault are:\n\n* **Secure Secret Storage**: Vault can store arbitrary key/value pairs. Vault encrypts data before writing it to persistent\n  storage, so gaining access to the raw storage isn't enough to access\n  your secrets. Vault can write to disk, [Consul](https://www.consul.io),\n  and more.\n\n* **Dynamic Secrets**: Vault can generate secrets on-demand for some\n  systems, such as AWS or SQL databases. For example, when an application\n  needs to access an S3 bucket, it asks Vault for credentials, and Vault\n  will generate an AWS keypair with valid permissions on demand. After\n  creating these dynamic secrets, Vault will also automatically revoke them\n  after the lease is up.\n\n* **Data Encryption**: Vault can encrypt and decrypt data without storing\n  it. This allows security teams to define encryption parameters and\n  developers to store encrypted data in a location such as a SQL database without\n  having to design their own encryption methods.\n\n* **Leasing and Renewal**: Vault associates a **lease** with each secret.\n  At the end of the lease, Vault automatically revokes the\n  secret. Clients are able to renew leases via built-in renew APIs.\n\n* **Revocation**: Vault has built-in support for secret revocation. Vault\n  can revoke not only single secrets, but a tree of secrets, for example,\n  all secrets read by a specific user, or all secrets of a particular type.\n  Revocation assists in key rolling as well as locking down systems in the\n  case of an intrusion.\n\nDocumentation, Getting Started, and Certification Exams\n-------------------------------\n\nDocumentation is available on the [Vault website](https://developer.hashicorp.com/vault/docs).\n\nIf you're new to Vault and want to get started with security automation, please\ncheck out our [Getting Started guides](https://learn.hashicorp.com/collections/vault/getting-started)\non HashiCorp's learning platform. There are also [additional guides](https://learn.hashicorp.com/vault)\nto continue your learning.\n\nFor examples of how to interact with Vault from inside your application in different programming languages, see the [vault-examples](https://github.com/hashicorp/vault-examples) repo. An out-of-the-box [sample application](https://github.com/hashicorp/hello-vault-go) is also available.\n\nShow off your Vault knowledge by passing a certification exam. Visit the\n[certification page](https://www.hashicorp.com/certification/#hashicorp-certified-vault-associate)\nfor information about exams and find [study materials](https://learn.hashicorp.com/collections/vault/certification)\non HashiCorp's learning platform.\n\nDeveloping Vault\n--------------------\n\nIf you wish to work on Vault itself or any of its built-in systems, you'll\nfirst need [Go](https://www.golang.org) installed on your machine.\n\nFor local dev first make sure Go is properly installed, including setting up a\n[GOPATH](https://golang.org/doc/code.html#GOPATH), then setting the \n[GOBIN](https://pkg.go.dev/cmd/go#hdr-Environment_variables) variable to `$GOPATH/bin`. \nEnsure that `$GOPATH/bin` is in your path as some distributions bundle the old version \nof build tools. \n\nNext, clone this repository. Vault uses [Go Modules](https://github.com/golang/go/wiki/Modules),\nso it is recommended that you clone the repository ***outside*** of the GOPATH.\nYou can then download any required build tools by bootstrapping your environment:\n\n```sh\n$ make bootstrap\n...\n```\n\nTo compile a development version of Vault, run `make` or `make dev`. This will\nput the Vault binary in the `bin` and `$GOPATH/bin` folders:\n\n```sh\n$ make dev\n...\n$ bin/vault\n...\n```\n\nTo compile a development version of Vault with the UI, run `make static-dist dev-ui`. This will\nput the Vault binary in the `bin` and `$GOPATH/bin` folders:\n\n```sh\n$ make static-dist dev-ui\n...\n$ bin/vault\n...\n```\n\nTo run tests, type `make test`. Note: this requires Docker to be installed. If\nthis exits with exit status 0, then everything is working!\n\n```sh\n$ make test\n...\n```\n\nIf you're developing a specific package, you can run tests for just that\npackage by specifying the `TEST` variable. For example below, only\n`vault` package tests will be run.\n\n```sh\n$ make test TEST=./vault\n...\n```\n\n### Troubleshooting\n\nIf you encounter an error like `could not read Username for 'https://github.com'` you may need to adjust your git config like so:\n\n```sh\n$ git config --global --add url.\"git@github.com:\".insteadOf \"https://github.com/\"\n```\n\n\n### Importing Vault\n\nThis repository publishes two libraries that may be imported by other projects:\n`github.com/hashicorp/vault/api` and `github.com/hashicorp/vault/sdk`.\n\nNote that this repository also contains Vault (the product), and as with most Go\nprojects, Vault uses Go modules to manage its dependencies. The mechanism to do\nthat is the [go.mod](./go.mod) file. As it happens, the presence of that file\nalso makes it theoretically possible to import Vault as a dependency into other\nprojects. Some other projects have made a practice of doing so in order to take\nadvantage of testing tooling that was developed for testing Vault itself. This\nis not, and has never been, a supported way to use the Vault project. We aren't \nlikely to fix bugs relating to failure to import `github.com/hashicorp/vault` \ninto your project.\n\nSee also the section \"Docker-based tests\" below.\n\n### Acceptance Tests\n\nVault has comprehensive [acceptance tests](https://en.wikipedia.org/wiki/Acceptance_testing)\ncovering most of the features of the secret and auth methods.\n\nIf you're working on a feature of a secret or auth method and want to\nverify it is functioning (and also hasn't broken anything else), we recommend\nrunning the acceptance tests.\n\n**Warning:** The acceptance tests create/destroy/modify *real resources*, which\nmay incur real costs in some cases. In the presence of a bug, it is technically\npossible that broken backends could leave dangling data behind. Therefore,\nplease run the acceptance tests at your own risk. At the very least,\nwe recommend running them in their own private account for whatever backend\nyou're testing.\n\nTo run the acceptance tests, invoke `make testacc`:\n\n```sh\n$ make testacc TEST=./builtin/logical/consul\n...\n```\n\nThe `TEST` variable is required, and you should specify the folder where the\nbackend is. The `TESTARGS` variable is recommended to filter down to a specific\nresource to test, since testing all of them at once can sometimes take a very\nlong time.\n\nAcceptance tests typically require other environment variables to be set for\nthings such as access keys. The test itself should error early and tell\nyou what to set, so it is not documented here.\n\nFor more information on Vault Enterprise features, visit the [Vault Enterprise site](https://www.hashicorp.com/products/vault/?utm_source=github&utm_medium=referral&utm_campaign=github-vault-enterprise).\n\n### Docker-based Tests\n\nWe have created an experimental new testing mechanism inspired by NewTestCluster.\nAn example of how to use it:\n\n```go\nimport (\n  \"testing\"\n  \"github.com/hashicorp/vault/sdk/helper/testcluster/docker\"\n)\n\nfunc Test_Something_With_Docker(t *testing.T) {\n  opts := &docker.DockerClusterOptions{\n    ImageRepo: \"hashicorp/vault\", // or \"hashicorp/vault-enterprise\"\n    ImageTag:    \"latest\",\n  }\n  cluster := docker.NewTestDockerCluster(t, opts)\n  defer cluster.Cleanup()\n  \n  client := cluster.Nodes()[0].APIClient()\n  _, err := client.Logical().Read(\"sys/storage/raft/configuration\")\n  if err != nil {\n    t.Fatal(err)\n  }\n}\n```\n\nOr for Enterprise:\n\n```go\nimport (\n  \"testing\"\n  \"github.com/hashicorp/vault/sdk/helper/testcluster/docker\"\n)\n\nfunc Test_Something_With_Docker(t *testing.T) {\n  opts := &docker.DockerClusterOptions{\n    ImageRepo: \"hashicorp/vault-enterprise\",\n    ImageTag:  \"latest\",\n\tVaultLicense: licenseString, // not a path, the actual license bytes\n  }\n  cluster := docker.NewTestDockerCluster(t, opts)\n  defer cluster.Cleanup()\n}\n```\n\nHere is a more realistic example of how we use it in practice.  DefaultOptions uses \n`hashicorp/vault`:`latest` as the repo and tag, but it also looks at the environment\nvariable VAULT_BINARY. If populated, it will copy the local file referenced by\nVAULT_BINARY into the container. This is useful when testing local changes.\n\nInstead of setting the VaultLicense option, you can set the VAULT_LICENSE_CI environment\nvariable, which is better than committing a license to version control.\n\nOptionally you can set COMMIT_SHA, which will be appended to the image name we\nbuild as a debugging convenience.\n\n```go\nfunc Test_Custom_Build_With_Docker(t *testing.T) {\n  opts := docker.DefaultOptions(t)\n  cluster := docker.NewTestDockerCluster(t, opts)\n  defer cluster.Cleanup()\n}\n```\n\nThere are a variety of helpers in the `github.com/hashicorp/vault/sdk/helper/testcluster`\npackage, e.g. these tests below will create a pair of 3-node clusters and link them using\nPR or DR replication respectively, and fail if the replication state doesn't become healthy\nbefore the passed context expires.\n\nAgain, as written, these depend on having a Vault Enterprise binary locally and the env\nvar VAULT_BINARY set to point to it, as well as having VAULT_LICENSE_CI set.\n\n```go\nfunc TestStandardPerfReplication_Docker(t *testing.T) {\n  opts := docker.DefaultOptions(t)\n  r, err := docker.NewReplicationSetDocker(t, opts)\n  if err != nil {\n      t.Fatal(err)\n  }\n  defer r.Cleanup()\n\n  ctx, cancel := context.WithTimeout(context.Background(), time.Minute)\n  defer cancel()\n  err = r.StandardPerfReplication(ctx)\n  if err != nil {\n    t.Fatal(err)\n  }\n}\n\nfunc TestStandardDRReplication_Docker(t *testing.T) {\n  opts := docker.DefaultOptions(t)\n  r, err := docker.NewReplicationSetDocker(t, opts)\n  if err != nil {\n    t.Fatal(err)\n  }\n  defer r.Cleanup()\n\n  ctx, cancel := context.WithTimeout(context.Background(), time.Minute)\n  defer cancel()\n  err = r.StandardDRReplication(ctx)\n  if err != nil {\n    t.Fatal(err)\n  }\n}\n```\n\nFinally, here's an example of running an existing OSS docker test with a custom binary:\n\n```bash\n$ GOOS=linux make dev\n$ VAULT_BINARY=$(pwd)/bin/vault go test -run 'TestRaft_Configuration_Docker' ./vault/external_tests/raft/raft_binary\nok      github.com/hashicorp/vault/vault/external_tests/raft/raft_binary        20.960s\n```\n",
      "stars_today": 13
    },
    {
      "id": 193162629,
      "name": "Rectangle",
      "full_name": "rxhanson/Rectangle",
      "description": "Move and resize windows on macOS with keyboard shortcuts and snap areas",
      "html_url": "https://github.com/rxhanson/Rectangle",
      "stars": 28296,
      "forks": 881,
      "language": "Swift",
      "topics": [],
      "created_at": "2019-06-21T21:34:53Z",
      "updated_at": "2026-01-28T00:39:41Z",
      "pushed_at": "2026-01-04T20:40:58Z",
      "open_issues": 98,
      "owner": {
        "login": "rxhanson",
        "avatar_url": "https://avatars.githubusercontent.com/u/13651296?v=4"
      },
      "readme": "# Rectangle\n\n[![Build](https://github.com/rxhanson/Rectangle/actions/workflows/build.yml/badge.svg)](https://github.com/rxhanson/Rectangle/actions/workflows/build.yml)\n\nRectangle is a window management app based on Spectacle, written in Swift.\n\n<img width=\"962\" height=\"886\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e8d88e5f-7d4f-43bc-a82e-146c42f92d68\" />\n\n## System Requirements\n\nRectangle supports macOS v10.15+. The last version that is supported for macOS 10.13 and 10.14 is https://github.com/rxhanson/Rectangle/releases/tag/v0.73.\n\n## Installation\n\nYou can download the latest dmg from <https://rectangleapp.com> or the [Releases page](https://github.com/rxhanson/Rectangle/releases).\n\nOr install with brew cask:\n\n```bash\nbrew install --cask rectangle\n```\n\n## How to use it\n\nThe [keyboard shortcuts](https://support.apple.com/guide/mac-help/what-are-those-symbols-shown-in-menus-cpmh0011/mac) are self explanatory, but the snap areas can use some explanation if you've never used them on Windows or other window management apps.\n\nDrag a window to the edge of the screen. When the mouse cursor reaches the edge of the screen, you'll see a footprint that Rectangle will attempt to resize and move the window to when the click is released.\n\n| Snap Area                                              | Resulting Action                       |\n|--------------------------------------------------------|----------------------------------------|\n| Left or right edge                                     | Left or right half                     |\n| Top                                                    | Maximize                               |\n| Corners                                                | Quarter in respective corner           |\n| Left or right edge, just above or below a corner       | Top or bottom half                     |\n| Bottom left, center, or right third                    | Respective third                       |\n| Bottom left or right third, then drag to bottom center | First or last two thirds, respectively |\n\n### Ignore an app\n\nIgnoring an app means that when the app is frontmost, keyboard shortcuts are un-registered from macOS. When the app is no longer frontmost, keyboard shortcuts are re-registered with macOS. This is useful for apps that have the same shortcuts like Rectangle and you do not want to change them.\n\n1. Focus the app that you want to ignore (make a window from that app frontmost).\n1. Open the Rectangle menu and select \"Ignore app\"\n\nTo un-ignore an app that you have selected to ignore, simply bring that app frontmost again, open the Rectangle menu, and deselect \"Ignore\".\n\n## Execute an action by URL\n\nOpen the URL `rectangle://execute-action?name=[name]`. Do not activate Rectangle if possible.\n\nAvailable values for `[name]`: `left-half`, `right-half`, `center-half`, `top-half`, `bottom-half`, `top-left`, `top-right`, `bottom-left`, `bottom-right`, `first-third`, `center-third`, `last-third`, `first-two-thirds`, `last-two-thirds`, `maximize`, `almost-maximize`, `maximize-height`, `smaller`, `larger`, `center`, `center-prominently`, `restore`, `next-display`, `previous-display`, `move-left`, `move-right`, `move-up`, `move-down`, `first-fourth`, `second-fourth`, `third-fourth`, `last-fourth`, `first-three-fourths`, `last-three-fourths`, `top-left-sixth`, `top-center-sixth`, `top-right-sixth`, `bottom-left-sixth`, `bottom-center-sixth`, `bottom-right-sixth`, `specified`, `reverse-all`, `top-left-ninth`, `top-center-ninth`, `top-right-ninth`, `middle-left-ninth`, `middle-center-ninth`, `middle-right-ninth`, `bottom-left-ninth`, `bottom-center-ninth`, `bottom-right-ninth`, `top-left-third`, `top-right-third`, `bottom-left-third`, `bottom-right-third`, `top-left-eighth`, `top-center-left-eighth`, `top-center-right-eighth`, `top-right-eighth`, `bottom-left-eighth`, `bottom-center-left-eighth`, `bottom-center-right-eighth`, `bottom-right-eighth`, `tile-all`, `cascade-all`, `cascade-active-app`\n\nExample, from a shell: `open -g \"rectangle://execute-action?name=left-half\"`\n\nURLs can also be used to ignore/unignore apps. \n\n```\nrectangle://execute-task?name=ignore-app\nrectangle://execute-task?name=unignore-app\n```\nA bundle identifier can also be specified, for example:\n```\nrectangle://execute-task?name=ignore-app&app-bundle-id=com.apple.Safari\n```\n\n## Terminal Commands for Hidden Preferences\n\nSee [TerminalCommands.md](TerminalCommands.md)\n\n## Differences with Spectacle\n\n* Rectangle uses [MASShortcut](https://github.com/rxhanson/MASShortcut) for keyboard shortcut recording. Spectacle used its own shortcut recorder.\n* Rectangle has additional window actions: move windows to each edge without resizing, maximize only the height of a window, almost maximizing a window.\n* Next/prev screen thirds is replaced with explicitly first third, first two thirds, center third, last two thirds, and last third. Screen orientation is taken into account, as in first third will be left third on landscape and top third on portrait.\n  * You can however emulate Spectacle's third cycling using first and last third actions. So, if you repeatedly execute first third, it will cycle through thirds (first, center, last) and vice-versa with the last third.\n* There's an option to have windows traverse across displays on subsequent left or right executions.\n* Windows will snap when dragged to edges/corners of the screen. This can be disabled.\n\n## Common Known Issues\n\n### Rectangle doesn't have the ability to move to other desktops/spaces\n\nApple never released a public API for doing this. Rectangle Pro has next/prev Space actions, but there are no plans to add those into Rectangle at this time.\n\n### Window resizing is off slightly for iTerm2\n\nBy default iTerm2 will only resize in increments of character widths. There might be a setting inside iTerm2 to disable this, but you can change it with the following command.\n\n```bash\ndefaults write com.googlecode.iterm2 DisableWindowSizeSnap -integer 1\n```\n\n### Rectangle appears to cause Notification Center to freeze\n\nThis appears to affect only a small amount of users. To prevent this from happening, uncheck the box for \"Snap windows by dragging\".\nSee issue [317](https://github.com/rxhanson/Rectangle/issues/317).\n\n### Troubleshooting\n\nIf windows aren't resizing or moving as you expect, here's some initial steps to get to the bottom of it. Most issues of this type have been caused by other apps.\n\n1. Enable debug logging, as per the instructions in the following section.\n1. The logs are pretty straightforward. If your calculated rect and your resulting rect are identical, chances are that there is another application causing issues. Save your logs if needed to attach to an issue if you create one.\n1. Make sure macOS is up to date.\n1. Lock and unlock your Mac\n1. Restart your Mac (this often fixes things right after a macOS update).\n1. Make sure there are no other window manager applications running.\n1. Make sure that the app whose windows are not behaving properly does not have any conflicting keyboard shortcuts.\n1. Try using the menu items to execute a window action or changing the keyboard shortcut to something different so we can tell if it's a keyboard shortcut issue or not.\n1. If you suspect there may be another application causing issues, try creating and logging in as a new macOS user.\n\n#### Try resetting the macOS accessibility permissions for Rectangle:\n\n```bash\ntccutil reset All com.knollsoft.Rectangle\n```\n\nOr, this can be done with the following steps instead of the tccutil terminal command.\n1. Close Rectangle if it's running\n2. In System Settings -> Privacy & Security -> Accessibility, first disable Rectangle, then remove it with the minus button. (it's important to do both of those steps in that order)\n3. Restart your mac.\n4. Launch Rectangle and enable settings for it as prompted.\n\n## View Debug Logging\n\n1. Hold down the alt (option) key with the Rectangle menu open.\n1. Select the \"View Logging...\" menu item, which is in place of the \"About\" menu item.\n1. Logging will appear in the window as you perform Rectangle commands.\n\n## Import & export JSON config\n\nThere are buttons for importing and exporting the config as a JSON file in the settings tab of the preferences window. \n\nUpon launch, Rectangle will load a config file at `~/Library/Application Support/Rectangle/RectangleConfig.json` if it is present and will rename that file with a time/date stamp so that it isn't read on subsequent launches.\n\n## Preferences Storage\n\nThe configuration for Rectangle is stored using NSUserDefaults, meaning it is stored in the following location:\n`~/Library/Preferences/com.knollsoft.Rectangle.plist`\nNote that shortcuts in v0.41+ are stored in a different format and will not load in prior versions.\n\nThat file can be backed up or transferred to other machines.\n\nIf you are using Rectangle v0.44+, you can also use the import/export button in the Preferences pane to share to your preferences and keyboard shortcuts across machines using a JSON file.\n\n> [!NOTE]  \n> If you are having issues with configuration options persisting after an application restart and you've installed using Homebrew, you will need to uninstall and reinstall with the `--zap` flag.\n\n```\nbrew uninstall --zap rectangle\nbrew install rectangle\n```\n\n## Uninstallation\n\nRectangle can be uninstalled by quitting the app and moving it to the trash. You can remove the Rectangle defaults from your machine with the following terminal command:\n\n```bash\ndefaults delete com.knollsoft.Rectangle\n```\n\n> [!TIP]  \n> If you are uninstalling after installing with Homebrew, you should include the `--zap` flag to ensure it removes the plist entries too. \n\n```\nbrew uninstall --zap rectangle\n```\n\n---\n\n## Contributing\n\nLogic from Rectangle is used in the [Multitouch](https://multitouch.app) app. The [Rectangle Pro](https://rectangleapp.com/pro) app is entirely built on top of Rectangle. If you contribute significant code or localizations that get merged into Rectangle, send me an email for a free license of Multitouch or Rectangle Pro. Contributors to Sparkle, MASShortcut, or Spectacle can also receive free Multitouch or Rectangle Pro licenses.\n\n### Contributing additional sizes and positions\n\nRectangle's UI is intentionally simple. If you want to add a size and position that's not in the Shortcuts tab, then you can now add them into the \"Extra Shortcuts\" section accessed via the ellipsis button at the bottom of the General tab.\n\n### Localization\n\nIf you would like to contribute to localization, all of the translations are held in the Main.strings.\n\nPull requests for new localizations or improvements on existing localizations are welcome.\n\n### Running the app in Xcode (for developers)\n\nRectangle uses [Swift Package Manager](https://www.swift.org/package-manager/) to install Sparkle and MASShortcut.\n\nThe original repository for MASShortcut was archived, so Rectangle uses my [fork](https://github.com/rxhanson/MASShortcut). If you want to make any changes that involve MASShortcut, please make a pull request on my fork. \n\nDue to the addition of the Liquid Glass icon with a fallback for older versions of macOS, there will be a build failure on macOS versions < 26. You can delete the \"Asset Catalog Other Flags\" to build locally on versions < 26 (but don't check that change in if you create a pull request).\n\n## Credits\n\nAs mentioned above, Rectangle uses a forked version of [MASShortcut](https://github.com/rxhanson/MASShortcut), which still works great, and it uses [Sparkle](https://sparkle-project.org) for updates. \n\nThe Big Sur variant of the Rectangle app icon was created by Giovanni Maria Cusaro (@gmcusaro). The Liquid Glass variant of the app icon was created by [Alexander KÃ¤ÃŸner](https://www.alexkaessner.de) (@alexkaessner).\n\nAnd of course, there's been a lot of community contributions over the years :)\n",
      "stars_today": 13
    },
    {
      "id": 2990192,
      "name": "Signal-Android",
      "full_name": "signalapp/Signal-Android",
      "description": "A private messenger for Android.",
      "html_url": "https://github.com/signalapp/Signal-Android",
      "stars": 28224,
      "forks": 6649,
      "language": "Kotlin",
      "topics": [],
      "created_at": "2011-12-15T20:01:12Z",
      "updated_at": "2026-01-28T00:41:15Z",
      "pushed_at": "2026-01-23T22:14:31Z",
      "open_issues": 446,
      "owner": {
        "login": "signalapp",
        "avatar_url": "https://avatars.githubusercontent.com/u/702459?v=4"
      },
      "readme": "# Signal Android\n\nSignal is a simple, powerful, and secure messenger that uses your phone's data connection (WiFi/3G/4G/5G) to communicate securely.\n\nMillions of people use Signal every day for free and instantaneous communication anywhere in the world. Send and receive high-fidelity messages, participate in HD voice/video calls, and explore a growing set of new features that help you stay connected. \n\nSignalâ€™s advanced privacy-preserving technology is always enabled, so you can focus on sharing the moments that matter with the people who matter to you.\n\nCurrently available on the [Play Store](https://play.google.com/store/apps/details?id=org.thoughtcrime.securesms) and [signal.org](https://signal.org/android/apk/).\n\n<a href='https://play.google.com/store/apps/details?id=org.thoughtcrime.securesms&pcampaignid=MKT-Other-global-all-co-prtnr-py-PartBadge-Mar2515-1'><img alt='Get it on Google Play' src='https://play.google.com/intl/en_us/badges/images/generic/en_badge_web_generic.png' height='80px'/></a>\n\nAlso available on [iOS](https://github.com/signalapp/signal-ios) and [Desktop](https://github.com/signalapp/signal-desktop).\n\n## Contributing Bug Reports\nWe use GitHub for bug tracking. Please search the existing issues for your bug and create a new one if the issue is not yet tracked!\n\nhttps://github.com/signalapp/Signal-Android/issues\n\n## Joining the Beta\nWant to live life on the bleeding edge and help out with testing?\n\nYou can subscribe to Signal Android Beta releases here:\nhttps://play.google.com/apps/testing/org.thoughtcrime.securesms\n\nIf you're interested in a life of peace and tranquility, stick with the standard releases.\n\n## Contributing Translations\nInterested in helping translate Signal? Contribute here:\n\nhttps://community.signalusers.org/c/translation-feedback/\n\n## Contributing Code\n\nIf you're new to the Signal codebase, we recommend going through our issues and picking out a simple bug to fix in order to get yourself familiar. Also please have a look at the [CONTRIBUTING.md](https://github.com/signalapp/Signal-Android/blob/main/CONTRIBUTING.md), that might answer some of your questions.\n\nFor larger changes and feature ideas, we ask that you propose it on the [unofficial Community Forum](https://community.signalusers.org) for a high-level discussion with the wider community before implementation.\n\n## Contributing Ideas\nHave something you want to say about Signal projects or want to be part of the conversation? Get involved in the [community forum](https://community.signalusers.org).\n\nHelp\n====\n## Support\nFor troubleshooting and questions, please visit our support center!\n\nhttps://support.signal.org/\n\n## Documentation\nLooking for documentation? Check out the wiki!\n\nhttps://github.com/signalapp/Signal-Android/wiki\n\n# Legal things\n## Cryptography Notice\n\nThis distribution includes cryptographic software. The country in which you currently reside may have restrictions on the import, possession, use, and/or re-export to another country, of encryption software.\nBEFORE using any encryption software, please check your country's laws, regulations and policies concerning the import, possession, or use, and re-export of encryption software, to see if this is permitted.\nSee <http://www.wassenaar.org/> for more information.\n\nThe U.S. Government Department of Commerce, Bureau of Industry and Security (BIS), has classified this software as Export Commodity Control Number (ECCN) 5D002.C.1, which includes information security software using or performing cryptographic functions with asymmetric algorithms.\nThe form and manner of this distribution makes it eligible for export under the License Exception ENC Technology Software Unrestricted (TSU) exception (see the BIS Export Administration Regulations, Section 740.13) for both object code and source code.\n\n## License\n\nCopyright 2013-2025 Signal Messenger, LLC\n\nLicensed under the GNU AGPLv3: https://www.gnu.org/licenses/agpl-3.0.html\n\nGoogle Play and the Google Play logo are trademarks of Google LLC.\n",
      "stars_today": 12
    },
    {
      "id": 133134007,
      "name": "openapi-generator",
      "full_name": "OpenAPITools/openapi-generator",
      "description": "OpenAPI Generator allows generation of API client libraries (SDK generation), server stubs, documentation and configuration automatically given an OpenAPI Spec (v2, v3)",
      "html_url": "https://github.com/OpenAPITools/openapi-generator",
      "stars": 25705,
      "forks": 7370,
      "language": "Java",
      "topics": [
        "api",
        "api-client",
        "api-server",
        "generator",
        "hacktoberfest",
        "openapi",
        "openapi-generator",
        "openapi3",
        "rest",
        "rest-api",
        "rest-client",
        "restful-api",
        "sdk"
      ],
      "created_at": "2018-05-12T09:57:56Z",
      "updated_at": "2026-01-27T22:21:13Z",
      "pushed_at": "2026-01-27T20:06:14Z",
      "open_issues": 5577,
      "owner": {
        "login": "OpenAPITools",
        "avatar_url": "https://avatars.githubusercontent.com/u/37325267?v=4"
      },
      "readme": "<h1 align=\"center\">OpenAPI Generator</h1>\n\n\n<div align=\"center\">\n\n[![Stable releases in Maven Central](https://img.shields.io/maven-metadata/v/https/repo1.maven.org/maven2/org/openapitools/openapi-generator/maven-metadata.xml.svg)](http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.openapitools%22%20AND%20a%3A%22openapi-generator%22)\n[![Apache 2.0 License](https://img.shields.io/badge/License-Apache%202.0-orange)](./LICENSE)\n[![Open Collective backers](https://img.shields.io/opencollective/backers/openapi_generator?color=orange&label=OpenCollective%20Backers)](https://opencollective.com/openapi_generator)\n[![Join the Slack chat room](https://img.shields.io/badge/Slack-Join%20the%20chat%20room-orange)](https://join.slack.com/t/openapi-generator/shared_invite/zt-36ucx4ybl-jYrN6euoYn6zxXNZdldoZA)\n[![Follow OpenAPI Generator Twitter account to get the latest update](https://img.shields.io/twitter/follow/oas_generator.svg?style=social&label=Follow)](https://twitter.com/oas_generator)\n[![Contribute with Gitpod](https://img.shields.io/badge/Contribute%20with-Gitpod-908a85?logo=gitpod)](https://gitpod.io/#https://github.com/OpenAPITools/openapi-generator)\n[![Conan Center](https://shields.io/conan/v/openapi-generator)](https://conan.io/center/recipes/openapi-generator)\n[![Revved up by Develocity](https://img.shields.io/badge/Revved%20up%20by-Develocity-06A0CE?logo=Gradle&labelColor=02303A)](https://ge.openapi-generator.tech/scans)\n</div>\n\n<div align=\"center\">\n\n[Master](https://github.com/OpenAPITools/openapi-generator/tree/master) (`7.20.0`):\n[![Integration Test2](https://circleci.com/gh/OpenAPITools/openapi-generator.svg?style=shield)](https://circleci.com/gh/OpenAPITools/openapi-generator)\n[![Bitrise](https://img.shields.io/bitrise/4a2b10a819d12b67/master?label=bitrise%3A%20Swift+4,5&token=859FMDR8QHwabCzwvZK6vQ)](https://app.bitrise.io/app/4a2b10a819d12b67)\n\n</div>\n\n<div align=\"center\">\n\n:star::star::star: If you would like to contribute, please refer to [guidelines](CONTRIBUTING.md) and a list of [open tasks](https://github.com/openapitools/openapi-generator/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22). :star::star::star:\n\n:bangbang: To migrate from Swagger Codegen to OpenAPI Generator, please refer to the [migration guide](docs/migration-from-swagger-codegen.md) :bangbang:\n\n:notebook_with_decorative_cover: For more information, please refer to the [Wiki page](https://github.com/openapitools/openapi-generator/wiki) and [FAQ](https://github.com/openapitools/openapi-generator/wiki/FAQ) :notebook_with_decorative_cover:\n\n:notebook_with_decorative_cover: The eBook [A Beginner's Guide to Code Generation for REST APIs](https://gum.co/openapi_generator_ebook) is a good starting point for beginners :notebook_with_decorative_cover:\n\n:warning: If the OpenAPI spec, templates or any input (e.g. options, environment variables) is obtained from an untrusted source or environment, please make sure you've reviewed these inputs before using OpenAPI Generator to generate the API client, server stub or documentation to avoid potential security issues (e.g. [code injection](https://en.wikipedia.org/wiki/Code_injection)). For security vulnerabilities, please contact [team@openapitools.org](mailto:team@openapitools.org). :warning:\n\n:bangbang: Both \"OpenAPI Tools\" (https://OpenAPITools.org - the parent organization of OpenAPI Generator) and \"OpenAPI Generator\" are not affiliated with OpenAPI Initiative (OAI) :bangbang:\n\n</div>\n\n## Sponsors\n\nIf you find OpenAPI Generator useful for work, please consider asking your company to support this Open Source project by [becoming a sponsor](https://opencollective.com/openapi_generator). You can also individually sponsor the project by [becoming a backer](https://opencollective.com/openapi_generator).\n\n#### Thank you to our bronze sponsors!\n\n[![NamSor](https://openapi-generator.tech/img/companies/namsor.png)](https://www.namsor.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[![LightBow](https://openapi-generator.tech/img/companies/lightbow.png)](https://www.lightbow.net/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/docspring.png\" width=\"128\" height=\"128\">](https://docspring.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/datadog.png\" width=\"128\" height=\"128\">](https://datadoghq.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/thales.jpg\" width=\"128\" height=\"128\">](https://cpl.thalesgroup.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/apideck.jpg\" width=\"128\" height=\"128\">](https://www.apideck.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/pexa.png\" width=\"128\" height=\"128\">](https://www.pexa.com.au/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/numary.png\" width=\"128\" height=\"128\">](https://www.numary.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/onesignal.png\" width=\"128\" height=\"128\">](https://www.onesignal.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/virtualansoftware.png\" width=\"128\" height=\"128\">](https://www.virtualansoftware.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/mergedev.jpeg\" width=\"128\" height=\"128\">](https://www.merge.dev/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/burkert.jpg\" width=\"128\" height=\"128\">](https://www.burkert.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/finbourne.png\" width=\"128\" height=\"128\">](https://www.finbourne.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/bumpsh.png\" width=\"128\" height=\"128\">](https://bump.sh/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/bileto.png\" width=\"128\" height=\"128\">](https://www.bileto.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/bairesdev.png\" width=\"128\" height=\"128\">](https://www.bairesdev.com/sponsoring-open-source-projects/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/dmtech.jpeg\" width=\"128\" height=\"128\">](https://www.dmtech.de/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/adyen.png\" width=\"128\" height=\"128\">](https://adyen.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/fornex.png\" width=\"128\" height=\"128\">](https://fornex.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/alloyautomation.png\" width=\"128\" height=\"128\">](https://runalloy.com/signup?utm_source=github&utm_medium=referral&utm_campaign=1524_openapigenerator)\n[<img src=\"https://openapi-generator.tech/img/companies/ssstwitter.png\" width=\"128\" height=\"128\">](https://ssstwitter.com/?utm_source=github&utm_medium=referral&utm_campaign=sponsor)\n[<img src=\"https://openapi-generator.tech/img/companies/svix.png\" width=\"128\" height=\"128\">](https://www.svix.com/?utm_source=openapi-generator&utm_medium=sponsorship&utm_campaign=oss-sponsorship)\n[<img src=\"https://openapi-generator.tech/img/companies/litslink.png\" width=\"128\" height=\"128\">](https://litslink.com/services/artificial-intelligence?utm_source=openapi-generator&utm_medium=sponsorship&utm_campaign=oss-sponsorship)\n[<img src=\"https://openapi-generator.tech/img/companies/designli.jpg\" width=\"128\" height=\"128\">](https://designli.co?utm_source=openapi-generator&utm_medium=sponsorship&utm_campaign=oss-sponsorship)\n[<img src=\"https://openapi-generator.tech/img/companies/itm.png\" width=\"128\" height=\"128\">](https://opensource.muenchen.de?utm_source=openapi-generator&utm_medium=sponsorship&utm_campaign=oss-sponsorship)\n[<img src=\"https://openapi-generator.tech/img/companies/kong.png\" width=\"128\" height=\"128\">](https://konghq.com/products/kong-konnect?utm_medium=referral&utm_source=github&utm_campaign=platform&utm_content=openapi-generator)\n[<img src=\"https://openapi-generator.tech/img/companies/route4me.png\" width=\"128\" height=\"128\">](https://route4me.com/?utm_source=openapi-generator&utm_medium=sponsorship&utm_campaign=oss-sponsorship)\n[<img src=\"https://openapi-generator.tech/img/companies/dm.png\" width=\"128\" height=\"128\">](https://www.dotcom-monitor.com/sponsoring-open-source-projects/?utm_source=openapi-generator&utm_medium=sponsorship&utm_campaign=oss-sponsorship)\n[<img src=\"https://openapi-generator.tech/img/companies/clickit.jpg\" width=\"128\" height=\"128\">](https://www.clickittech.com/?utm_source=openapi-generator&utm_medium=sponsorship&utm_campaign=oss-sponsorship)\n[<img src=\"https://openapi-generator.tech/img/companies/unified_to.jpg\" width=\"128\" height=\"128\">](https://unified.to/?utm_source=openapi-generator&utm_medium=sponsorship&utm_campaign=oss-sponsorship)\n[<img src=\"https://openapi-generator.tech/img/companies/savetwt.jpg\" width=\"128\" height=\"128\">](https://savetwt.com/?utm_source=openapi-generator&utm_medium=sponsorship&utm_campaign=oss-sponsorship)\n[<img src=\"https://openapi-generator.tech/img/companies/serpapi.png\" width=\"128\" height=\"128\">](https://serpapi.com/?utm_source=openapi-generator&utm_medium=sponsorship&utm_campaign=oss-sponsorship)\n\n#### Thank you GoDaddy for sponsoring the domain names, Linode for sponsoring the VPS, Checkly for sponsoring the API monitoring and Gradle for sponsoring Develocity\n\n[<img src=\"https://openapi-generator.tech/img/companies/godaddy.png\" width=\"150\">](https://www.godaddy.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[![Linode](https://www.linode.com/media/images/logos/standard/light/linode-logo_standard_light_small.png)](https://www.linode.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRAhEYadUyZYzGUotZiSdXkVMqqLGuohyixLl4eUpUV6pAbUULL\" width=\"150\">](https://checklyhq.com/?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Gradle_logo.png/320px-Gradle_logo.png\" width=\"150\">](https://gradle.org?utm_source=openapi_generator&utm_medium=github_webpage&utm_campaign=sponsor)\n\n## Overview\n\nOpenAPI Generator allows generation of API client libraries (SDK generation), server stubs,  documentation and configuration automatically given an [OpenAPI Spec](https://github.com/OAI/OpenAPI-Specification) (both 2.0 and 3.0 are supported). Currently, the following languages/frameworks are supported:\n\n|                                  | Languages/Frameworks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| -------------------------------- |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **API clients**                  | **ActionScript**, **Ada**, **Apex**, **Bash**, **C**, **C#** (.net 2.0, 3.5 or later, .NET Standard 1.3 - 2.1, .NET Core 3.1, .NET 5.0. Libraries: RestSharp, GenericHost, HttpClient), **C++** (Arduino, cpp-restsdk, Qt5, Tizen, Unreal Engine 4), **Clojure**, **Crystal**, **Dart**, **Elixir**, **Elm**, **Eiffel**, **Erlang**, **Go**, **Groovy**, **Haskell** (http-client, Servant), **Java** (Apache HttpClient 4.x, Apache HttpClient 5.x, Jersey2.x, OkHttp, Retrofit1.x, Retrofit2.x, Feign, RestTemplate, RESTEasy, Vertx, Google API Client Library for Java, Rest-assured, Spring 5 Web Client, Spring 6 RestClient, MicroProfile Rest Client, Helidon), **Jetbrains HTTP Client**, **Julia**, **k6**, **Kotlin**, **Lua**, **N4JS**, **Nim**, **Node.js/JavaScript** (ES5, ES6, AngularJS with Google Closure Compiler annotations, Flow types, Apollo GraphQL DataStore), **Objective-C**, **OCaml**, **Perl**, **PHP**, **PowerShell**, **Python**, **R**, **Ruby**, **Rust** (hyper, reqwest, rust-server), **Scala** (akka, http4s, scalaz, sttp, swagger-async-httpclient, pekko), **Swift** (2.x, 3.x, 4.x, 5.x, 6.x), **Typescript** (AngularJS, Angular (9.x - 19.x), Aurelia, Axios, Fetch, Inversify, jQuery, Nestjs, Node, redux-query, Rxjs), **XoJo**, **Zapier** |\n| **Server stubs**                 | **Ada**, **C#** (ASP.NET Core, Azure Functions), **C++** (Oat++, Pistache, Restbed, Qt5 QHTTPEngine), **Erlang**, **F#** (Giraffe), **Go** (net/http, Gin, Echo), **Haskell** (Servant, Yesod), **Java** (MSF4J, Spring, Undertow, JAX-RS: CDI, CXF, Inflector, Jersey, RestEasy, Play Framework, [PKMST](https://github.com/ProKarma-Inc/pkmst-getting-started-examples), [Vert.x](https://vertx.io/), [Apache Camel](https://camel.apache.org/), [Helidon](https://helidon.io/)), **Julia**, **Kotlin** (Spring Boot, [Ktor](https://github.com/ktorio/ktor), [Vert.x](https://vertx.io/)), **PHP** ([Flight](https://docs.flightphp.com/), Laravel, Lumen, [Mezzio (fka Zend Expressive)](https://github.com/mezzio/mezzio), Slim, Silex, [Symfony](https://symfony.com/)), **Python** (FastAPI, Flask), **NodeJS**, **Ruby** (Sinatra, Rails5), **Rust** ([rust-server](https://openapi-generator.tech/docs/generators/rust-server/)), **Scala** (Akka, [Finch](https://github.com/finagle/finch), [Lagom](https://github.com/lagom/lagom), [Play](https://www.playframework.com/), [Cask](https://github.com/com-lihaoyi/cask), Scalatra)                                                                                                                                                    |\n| **API documentation generators** | **HTML**, **Confluence Wiki**, **Asciidoc**, **Markdown**, **PlantUML**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n| **Configuration files**          | [**Apache2**](https://httpd.apache.org/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| **Others**                       | **GraphQL**, **JMeter**, **Ktorm**, **MySQL Schema**, **Postman Collection**, **Protocol Buffer**, **WSDL**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n\n## Table of contents\n\n- [Sponsors](#sponsors)\n    - [Thank you to our bronze sponsors!](#thank-you-to-our-bronze-sponsors)\n    - [Thank you GoDaddy for sponsoring the domain names, Linode for sponsoring the VPS, Checkly for sponsoring the API monitoring and Gradle for sponsoring Develocity](#thank-you-godaddy-for-sponsoring-the-domain-names-linode-for-sponsoring-the-vps-checkly-for-sponsoring-the-api-monitoring-and-gradle-for-sponsoring-develocity)\n- [Overview](#overview)\n- [Table of contents](#table-of-contents)\n- [1 - Installation](#1---installation)\n  - [1.1 - Compatibility](#11---compatibility)\n- [1.2 - Artifacts on Maven Central](#12---artifacts-on-maven-central)\n  - [1.3 - Download JAR](#13---download-jar)\n  - [Launcher Script](#launcher-script)\n  - [1.4 - Build Projects](#14---build-projects)\n    - [Nix users](#nix-users)\n  - [1.5 - Homebrew](#15---homebrew)\n  - [1.6 - Docker](#16---docker)\n    - [Public Pre-built Docker images](#public-pre-built-docker-images)\n    - [OpenAPI Generator CLI Docker Image](#openapi-generator-cli-docker-image)\n    - [OpenAPI Generator Online Docker Image](#openapi-generator-online-docker-image)\n    - [Development in docker](#development-in-docker)\n      - [Troubleshooting](#troubleshooting)\n    - [Run Docker in Vagrant](#run-docker-in-vagrant)\n  - [1.7 - NPM](#17---npm)\n  - [1.8 - pip](#18---pip)\n- [2 - Getting Started](#2---getting-started)\n- [3 - Usage](#3---usage)\n  - [To generate a sample client library](#to-generate-a-sample-client-library)\n  - [3.1 - Customization](#31---customization)\n  - [3.2 - Workflow Integration (Maven, Gradle, Github, CI/CD)](#32---workflow-integration-maven-gradle-github-cicd)\n  - [3.3 - Online OpenAPI generator](#33---online-openapi-generator)\n  - [3.4 - License information on Generated Code](#34---license-information-on-generated-code)\n  - [3.5 - IDE Integration](#35---ide-integration)\n- [4 - Companies/Projects using OpenAPI Generator](#4---companiesprojects-using-openapi-generator)\n- [5 - Presentations/Videos/Tutorials/Books](#5---presentationsvideostutorialsbooks)\n- [6 - About Us](#6---about-us)\n  - [6.1 - OpenAPI Generator Core Team](#61---openapi-generator-core-team)\n    - [Core Team Members](#core-team-members)\n    - [Template Creator](#template-creator)\n    - [How to join the core team](#how-to-join-the-core-team)\n  - [6.2 - OpenAPI Generator Technical Committee](#62---openapi-generator-technical-committee)\n    - [Members of Technical Committee](#members-of-technical-committee)\n  - [6.3 - History of OpenAPI Generator](#63---history-of-openapi-generator)\n    - [Founding Members (alphabetical order):](#founding-members-alphabetical-order)\n- [7 - License](#7---license)\n\n## [1 - Installation](#table-of-contents)\n\n### [1.1 - Compatibility](#table-of-contents)\n\nThe OpenAPI Specification has undergone 3 revisions since initial creation in 2010.  The openapi-generator project has the following compatibilities with the OpenAPI Specification:\n\n| OpenAPI Generator Version                                                                                                                                 | Release Date | Notes                                             |\n| --------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------ | ------------------------------------------------- |\n| 7.20.0 (upcoming minor release) [SNAPSHOT](https://github.com/OpenAPITools/openapi-generator/wiki/FAQ#how-to-test-with-the-latest-master-of-openapi-generator) | 20.02.2026   | Minor release with breaking changes (with fallback) |\n| [7.19.0](https://github.com/OpenAPITools/openapi-generator/releases/tag/v7.19.0) (latest stable release)                                                    | 20.01.2026   | Minor release with breaking changes (with fallback) |\n| [6.6.0](https://github.com/OpenAPITools/openapi-generator/releases/tag/v6.6.0)                                                    | 11.05.2023   | Minor release with breaking changes (with fallback) |\n| [5.4.0](https://github.com/OpenAPITools/openapi-generator/releases/tag/v5.4.0)                                                    | 31.01.2022   | Minor release with breaking changes (with fallback) |\n| [4.3.1](https://github.com/OpenAPITools/openapi-generator/releases/tag/v4.3.1)                                                    | 06.05.2020   | Patch release (enhancements, bug fixes, etc)                       |\n\nOpenAPI Spec compatibility: 1.0, 1.1, 1.2, 2.0, 3.0, 3.1 (beta support)\n\n(We do not publish daily/nightly build. Please use SNAPSHOT instead)\n\nFor old releases, please refer to the [**Release**](https://github.com/OpenAPITools/openapi-generator/releases) page.\n\nFor decommissioned generators/libraries/frameworks, please refer to [the \"Decommission\" label](https://github.com/OpenAPITools/openapi-generator/issues?q=label%3ADecommission+is%3Amerged+) in the pull request page.\n\n## [1.2 - Artifacts on Maven Central](#table-of-contents)\n\nYou can find our released artifacts on maven central:\n\n**Core:**\n```xml\n<dependency>\n    <groupId>org.openapitools</groupId>\n    <artifactId>openapi-generator</artifactId>\n    <version>${openapi-generator-version}</version>\n</dependency>\n```\nSee the different versions of the [openapi-generator](https://search.maven.org/artifact/org.openapitools/openapi-generator) artifact available on maven central.\n\n**Cli:**\n```xml\n<dependency>\n    <groupId>org.openapitools</groupId>\n    <artifactId>openapi-generator-cli</artifactId>\n    <version>${openapi-generator-version}</version>\n</dependency>\n```\nSee the different versions of the [openapi-generator-cli](https://search.maven.org/artifact/org.openapitools/openapi-generator-cli) artifact available on maven central.\n\n**Maven plugin:**\n```xml\n<dependency>\n    <groupId>org.openapitools</groupId>\n    <artifactId>openapi-generator-maven-plugin</artifactId>\n    <version>${openapi-generator-version}</version>\n</dependency>\n```\n* See the different versions of the [openapi-generator-maven-plugin](https://search.maven.org/artifact/org.openapitools/openapi-generator-maven-plugin) artifact available on maven central.\n* [Readme](https://github.com/OpenAPITools/openapi-generator/blob/master/modules/openapi-generator-maven-plugin/README.md)\n\n**Gradle plugin:**\n```xml\n<dependency>\n    <groupId>org.openapitools</groupId>\n    <artifactId>openapi-generator-gradle-plugin</artifactId>\n    <version>${openapi-generator-version}</version>\n</dependency>\n```\n* See the different versions of the [openapi-generator-gradle-plugin](https://search.maven.org/artifact/org.openapitools/openapi-generator-gradle-plugin) artifact available on maven central.\n* [Readme](https://github.com/OpenAPITools/openapi-generator/blob/master/modules/openapi-generator-gradle-plugin/README.adoc)\n\n### [1.3 - Download JAR](#table-of-contents)\n<!-- RELEASE_VERSION -->\nIf you're looking for the latest stable version, you can grab it directly from Maven.org (Java 11 runtime at a minimum):\n\nJAR location: `https://repo1.maven.org/maven2/org/openapitools/openapi-generator-cli/7.19.0/openapi-generator-cli-7.19.0.jar`\n\nFor **Mac/Linux** users:\n```sh\nwget https://repo1.maven.org/maven2/org/openapitools/openapi-generator-cli/7.19.0/openapi-generator-cli-7.19.0.jar -O openapi-generator-cli.jar\n```\n\nFor **Windows** users, you will need to install [wget](http://gnuwin32.sourceforge.net/packages/wget.htm) or you can use Invoke-WebRequest in PowerShell (3.0+), e.g.\n```\nInvoke-WebRequest -OutFile openapi-generator-cli.jar https://repo1.maven.org/maven2/org/openapitools/openapi-generator-cli/7.19.0/openapi-generator-cli-7.19.0.jar\n```\n\nAfter downloading the JAR, run `java -jar openapi-generator-cli.jar help` to show the usage.\n\nFor Mac users, please make sure Java 11 is installed (Tips: run `java -version` to check the version), and export `JAVA_HOME` in order to use the supported Java version:\n```sh\nexport JAVA_HOME=`/usr/libexec/java_home -v 1.11`\nexport PATH=${JAVA_HOME}/bin:$PATH\n```\n\n<!-- /RELEASE_VERSION -->\n### Launcher Script\n\nOne downside to manual jar downloads is that you don't keep up-to-date with the latest released version. We have a Bash launcher script at [bin/utils/openapi-generator.cli.sh](./bin/utils/openapi-generator-cli.sh) which resolves this issue.\n\nTo install the launcher script, copy the contents of the script to a location on your path and make the script executable.\n\nAn example of setting this up (NOTE: Always evaluate scripts curled from external systems before executing them).\n\n```\nmkdir -p ~/bin/openapitools\ncurl https://raw.githubusercontent.com/OpenAPITools/openapi-generator/master/bin/utils/openapi-generator-cli.sh > ~/bin/openapitools/openapi-generator-cli\nchmod u+x ~/bin/openapitools/openapi-generator-cli\nexport PATH=$PATH:~/bin/openapitools/\n```\n\nNow, `openapi-generator-cli` is \"installed\". On invocation, it will query the GitHub repository for the most recently released version. If this matches the last downloaded jar,\nit will execute as normal. If a newer version is found, the script will download the latest release and execute it.\n\nIf you need to invoke an older version of the generator, you can define the variable `OPENAPI_GENERATOR_VERSION` either ad hoc or globally. You can export this variable if you'd like to persist a specific release version.\n\nExamples:\n\n```\n# Execute latest released openapi-generator-cli\nopenapi-generator-cli version\n\n# Execute version 4.1.0 for the current invocation, regardless of the latest released version\nOPENAPI_GENERATOR_VERSION=4.1.0 openapi-generator-cli version\n\n# Execute version 4.1.0-SNAPSHOT for the current invocation\nOPENAPI_GENERATOR_VERSION=4.1.0-SNAPSHOT openapi-generator-cli version\n\n# Execute version 4.0.2 for every invocation in the current shell session\nexport OPENAPI_GENERATOR_VERSION=4.0.2\nopenapi-generator-cli version # is 4.0.2\nopenapi-generator-cli version # is also 4.0.2\n\n# To \"install\" a specific version, set the variable in .bashrc/.bash_profile\necho \"export OPENAPI_GENERATOR_VERSION=4.0.2\" >> ~/.bashrc\nsource ~/.bashrc\nopenapi-generator-cli version # is always 4.0.2, unless any of the above overrides are done ad hoc\n```\n\n### [1.4 - Build Projects](#table-of-contents)\n\nTo build from source, you need the following installed and available in your `$PATH:`\n\n* [Java 11](https://adoptium.net/)\n\n* [Apache Maven 3.8.8 or greater](https://maven.apache.org/) (optional)\n\nAfter cloning the project, you can build it from source using [maven wrapper](https://maven.apache.org/wrapper/):\n\n- Linux: `./mvnw clean install`\n- Windows: `mvnw.cmd clean install`\n\n#### Nix users\n\nIf you're a nix user, you can enter OpenAPI Generator shell, by typing:\n```sh\nnix develop\n```\nIt will enter a shell with Java 11 installed.\n\nDirenv supports automatically loading of the nix developer shell, so if you're using direnv too, type:\n```sh\ndirenv allow\n```\nand have `java` and `mvn` set up with correct versions each time you enter project directory.\n\nThe default build contains minimal static analysis (via CheckStyle). To run your build with PMD and Spotbugs, use the `static-analysis` profile:\n\n- Linux: `./mvnw -Pstatic-analysis clean install`\n- Windows: `mvnw.cmd -Pstatic-analysis clean install`\n\n### [1.5 - Homebrew](#table-of-contents)\n\nTo install, run `brew install openapi-generator`\n\nHere is an example usage to generate a Ruby client:\n```sh\nopenapi-generator generate -i https://raw.githubusercontent.com/openapitools/openapi-generator/master/modules/openapi-generator/src/test/resources/3_0/petstore.yaml -g ruby -o /tmp/test/\n```\n\nTo reinstall with the latest master, run `brew uninstall openapi-generator && brew install --HEAD openapi-generator`\n\nTo install OpenJDK (pre-requisites), please run\n```sh\nbrew tap AdoptOpenJDK/openjdk\nbrew install --cask adoptopenjdk11\nexport JAVA_HOME=`/usr/libexec/java_home -v 1.11`\n```\n\nor download installer via https://adoptium.net/\n\nTo install Maven (optional), please run\n```sh\nbrew install maven\n```\n\n### [1.6 - Docker](#table-of-contents)\n\n#### Public Pre-built Docker images\n\n - [https://hub.docker.com/r/openapitools/openapi-generator-cli/](https://hub.docker.com/r/openapitools/openapi-generator-cli/) (official CLI)\n - [https://hub.docker.com/r/openapitools/openapi-generator-online/](https://hub.docker.com/r/openapitools/openapi-generator-online/) (official web service)\n\n\n#### OpenAPI Generator CLI Docker Image\n\nThe OpenAPI Generator image acts as a standalone executable. It can be used as an alternative to installing via homebrew, or for developers who are unable to install Java or upgrade the installed version.\n\nTo generate code with this image, you'll need to mount a local location as a volume.\n\nExample:\n\n```sh\ndocker run --rm -v \"${PWD}:/local\" openapitools/openapi-generator-cli generate \\\n    -i https://raw.githubusercontent.com/openapitools/openapi-generator/master/modules/openapi-generator/src/test/resources/3_0/petstore.yaml \\\n    -g go \\\n    -o /local/out/go\n```\n\nThe generated code will be located under `./out/go` in the current directory.\n\n#### OpenAPI Generator Online Docker Image\n\nThe openapi-generator-online image can act as a self-hosted web application and API for generating code. This container can be incorporated into a CI pipeline, and requires at least two HTTP requests and some docker orchestration to access generated code.\n\nExample usage:\n\n```sh\n# Start container at port 8888 and save the container id\n> CID=$(docker run -d -p 8888:8080 openapitools/openapi-generator-online)\n\n# allow for startup\n> sleep 10\n\n# Get the IP of the running container (optional)\nGEN_IP=$(docker inspect --format '{{.NetworkSettings.IPAddress}}'  $CID)\n\n# Execute an HTTP request to generate a Ruby client\n> curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' \\\n-d '{\"openAPIUrl\": \"https://raw.githubusercontent.com/openapitools/openapi-generator/master/modules/openapi-generator/src/test/resources/3_0/petstore.yaml\"}' \\\n'http://localhost:8888/api/gen/clients/ruby'\n\n{\"code\":\"c2d483.3.4672-40e9-91df-b9ffd18d22b8\",\"link\":\"http://localhost:8888/api/gen/download/c2d483.3.4672-40e9-91df-b9ffd18d22b8\"}\n\n# Download the generated zip file\n> wget http://localhost:8888/api/gen/download/c2d483.3.4672-40e9-91df-b9ffd18d22b8\n\n# Unzip the file\n> unzip c2d483.3.4672-40e9-91df-b9ffd18d22b8\n\n# Shutdown the openapi generator image\n> docker stop $CID && docker rm $CID\n```\n\n#### Development in docker\n\nYou can use `run-in-docker.sh` to do all development. This script maps your local repository to `/gen`\nin the docker container. It also maps `~/.m2/repository` to the appropriate container location.\n\nTo execute `mvn package`:\n\n```sh\ngit clone https://github.com/openapitools/openapi-generator\ncd openapi-generator\n./run-in-docker.sh mvn package\n```\n\nBuild artifacts are now accessible in your working directory.\n\nOnce built, `run-in-docker.sh` will act as an executable for openapi-generator-cli. To generate code, you'll need to output to a directory under `/gen` (e.g. `/gen/out`). For example:\n\n```sh\n./run-in-docker.sh help # Executes 'help' command for openapi-generator-cli\n./run-in-docker.sh list # Executes 'list' command for openapi-generator-cli\n./run-in-docker.sh generate -i modules/openapi-generator/src/test/resources/3_0/petstore.yaml \\\n    -g go -o /gen/out/go-petstore -p packageName=petstore # generates go client, outputs locally to ./out/go-petstore\n```\n\n##### Troubleshooting\n\nIf an error like this occurs, just execute the **./mvnw clean install -U** command:\n\n> org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project openapi-generator: A type incompatibility occurred while executing org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test: java.lang.ExceptionInInitializerError cannot be cast to java.io.IOException\n\n```sh\n./run-in-docker.sh ./mvnw clean install -U\n```\n\n> Failed to execute goal org.fortasoft:gradle-maven-plugin:1.0.8:invoke (default) on project openapi-generator-gradle-plugin-mvn-wrapper: org.gradle.tooling.BuildException: Could not execute build using Gradle distribution 'https://services.gradle.org/distributions/gradle-4.7-bin.zip'\n\nRight now: no solution for this one :|\n\n#### Run Docker in Vagrant\nPrerequisite: install [Vagrant](https://www.vagrantup.com/downloads.html) and [VirtualBox](https://www.virtualbox.org/wiki/Downloads).\n ```sh\ngit clone https://github.com/openapitools/openapi-generator.git\ncd openapi-generator\nvagrant up\nvagrant ssh\ncd /vagrant\n./run-in-docker.sh ./mvnw package\n```\n\n### [1.7 - NPM](#table-of-contents)\n\nThere is also an [NPM package wrapper](https://www.npmjs.com/package/@openapitools/openapi-generator-cli) available for different platforms (e.g. Linux, Mac, Windows). (JVM is still required)\nPlease see the [project's README](https://github.com/openapitools/openapi-generator-cli) there for more information.\n\nInstall it globally to get the CLI available on the command line:\n\n```sh\nnpm install @openapitools/openapi-generator-cli -g\nopenapi-generator-cli version\n```\n\n<!-- RELEASE_VERSION -->\nTo use a specific version of \"openapi-generator-cli\"\n\n```sh\nopenapi-generator-cli version-manager set 7.19.0\n```\n\nOr install it as dev-dependency:\n\n```sh\nnpm install @openapitools/openapi-generator-cli -D\n```\n<!-- /RELEASE_VERSION -->\n\nYou can use [locally built JARs](https://github.com/OpenAPITools/openapi-generator-cli?tab=readme-ov-file#use-locally-built-jar) or [`SNAPSHOT` versions](https://github.com/OpenAPITools/openapi-generator-cli?tab=readme-ov-file#use-nightly-snapshot-build) as well.\n\n### [1.8 - pip](#table-of-contents)\n\n\n> **Platform(s)**: Linux, macOS, Windows\n**Install** via [PyPI](https://pypi.org/) (`java` executable is needed to run):\n\n```\npip install openapi-generator-cli\n```\n\nTo install a specific version\n```\npip install openapi-generator-cli==7.19.0\n```\n\nYou can also install with [jdk4py](https://github.com/activeviam/jdk4py) instead of java binary. (python>=3.10 is required)\n\n```\npip install openapi-generator-cli[jdk4py]\n```\n\nRef: https://github.com/openAPITools/openapi-generator-pip\n\n## [2 - Getting Started](#table-of-contents)\n\nTo generate a PHP client for [petstore.yaml](https://raw.githubusercontent.com/openapitools/openapi-generator/master/modules/openapi-generator/src/test/resources/3_0/petstore.yaml), please run the following\n```sh\ngit clone https://github.com/openapitools/openapi-generator\ncd openapi-generator\n./mvnw clean package\njava -jar modules/openapi-generator-cli/target/openapi-generator-cli.jar generate \\\n   -i https://raw.githubusercontent.com/openapitools/openapi-generator/master/modules/openapi-generator/src/test/resources/3_0/petstore.yaml \\\n   -g php \\\n   -o /var/tmp/php_api_client\n```\n(if you're on Windows, replace the last command with `java -jar modules\\openapi-generator-cli\\target\\openapi-generator-cli.jar generate -i https://raw.githubusercontent.com/openapitools/openapi-generator/master/modules/openapi-generator/src/test/resources/3_0/petstore.yaml -g php -o c:\\temp\\php_api_client`)\n\n<!-- RELEASE_VERSION -->\nYou can also download the JAR (latest release) directly from [maven.org](https://repo1.maven.org/maven2/org/openapitools/openapi-generator-cli/7.19.0/openapi-generator-cli-7.19.0.jar)\n<!-- /RELEASE_VERSION -->\n\nTo get a list of **general** options available, please run `java -jar modules/openapi-generator-cli/target/openapi-generator-cli.jar help generate`\n\nTo get a list of PHP specified options (which can be passed to the generator with a config file via the `-c` option), please run `java -jar modules/openapi-generator-cli/target/openapi-generator-cli.jar config-help -g php`\n\n## [3 - Usage](#table-of-contents)\n\n### To generate a sample client library\nYou can build a client against the [Petstore API](https://raw.githubusercontent.com/openapitools/openapi-generator/master/modules/openapi-generator/src/test/resources/3_0/petstore.yaml) as follows:\n\n```sh\n./bin/generate-samples.sh ./bin/configs/java-okhttp-gson.yaml\n```\n\n(On Windows, please install [GIT Bash for Windows](https://gitforwindows.org/) to run the command above)\n\nThis script uses the default library, which is `okhttp-gson`. It will run the generator with this command:\n\n```sh\njava -jar modules/openapi-generator-cli/target/openapi-generator-cli.jar generate \\\n  -i https://raw.githubusercontent.com/openapitools/openapi-generator/master/modules/openapi-generator/src/test/resources/3_0/petstore.yaml \\\n  -g java \\\n  -t modules/openapi-generator/src/main/resources/Java \\\n  --additional-properties artifactId=petstore-okhttp-gson,hideGenerationTimestamp=true \\\n  -o samples/client/petstore/java/okhttp-gson\n```\n\nwith a number of options. [The java options are documented here.](docs/generators/java.md)\n\nYou can also get the options with the `help generate` command (below only shows partial results):\n\n```\nNAME\n        openapi-generator-cli generate - Generate code with the specified\n        generator.\n\nSYNOPSIS\n        openapi-generator-cli generate\n                [(-a <authorization> | --auth <authorization>)]\n                [--api-name-suffix <api name suffix>] [--api-package <api package>]\n                [--artifact-id <artifact id>] [--artifact-version <artifact version>]\n                [(-c <configuration file> | --config <configuration file>)] [--dry-run]\n                [(-e <templating engine> | --engine <templating engine>)]\n                [--enable-post-process-file]\n                [(-g <generator name> | --generator-name <generator name>)]\n                [--generate-alias-as-model] [--git-host <git host>]\n                [--git-repo-id <git repo id>] [--git-user-id <git user id>]\n                [--global-property <global properties>...] [--group-id <group id>]\n                [--http-user-agent <http user agent>]\n                [(-i <spec file> | --input-spec <spec file>)]\n                [--ignore-file-override <ignore file override location>]\n                [--import-mappings <import mappings>...]\n                [--instantiation-types <instantiation types>...]\n                [--invoker-package <invoker package>]\n                [--language-specific-primitives <language specific primitives>...]\n                [--legacy-discriminator-behavior] [--library <library>]\n                [--log-to-stderr] [--minimal-update]\n                [--model-name-prefix <model name prefix>]\n                [--model-name-suffix <model name suffix>]\n                [--model-package <model package>]\n                [(-o <output directory> | --output <output directory>)] [(-p <additional properties> | --additional-properties <additional properties>)...]\n                [--package-name <package name>] [--release-note <release note>]\n                [--remove-operation-id-prefix]\n                [--reserved-words-mappings <reserved word mappings>...]\n                [(-s | --skip-overwrite)] [--server-variables <server variables>...]\n                [--skip-validate-spec] [--strict-spec <true/false strict behavior>]\n                [(-t <template directory> | --template-dir <template directory>)]\n                [--type-mappings <type mappings>...] [(-v | --verbose)]\n\nOPTIONS\n        -a <authorization>, --auth <authorization>\n            adds authorization headers when fetching the OpenAPI definitions\n            remotely. Pass in a URL-encoded string of name:header with a comma\n            separating multiple values\n\n...... (results omitted)\n\n        -v, --verbose\n            verbose mode\n\n```\n\nYou can then compile and run the client, as well as unit tests against it:\n\n```sh\ncd samples/client/petstore/java/okhttp-gson\nmvn package\n```\n\nOther generators have [samples](https://github.com/OpenAPITools/openapi-generator/tree/master/samples) too.\n\n### [3.1 - Customization](#table-of-contents)\n\nPlease refer to [customization.md](docs/customization.md) on how to customize the output (e.g. package name, version)\n\n### [3.2 - Workflow Integration (Maven, Gradle, Github, CI/CD)](#table-of-contents)\n\nPlease refer to [integration.md](docs/integration.md) on how to integrate OpenAPI generator with Maven, Gradle, sbt, Bazel, Github and CI/CD.\n\n### [3.3 - Online OpenAPI generator](#table-of-contents)\n\nHere are the public online services:\n\n- latest stable version: https://api.openapi-generator.tech\n- latest master: https://api-latest-master.openapi-generator.tech (updated with latest master every hour)\n\nThe server is sponsored by [Linode](https://www.linode.com/) [![Linode Logo](https://www.linode.com/media/images/logos/standard/light/linode-logo_standard_light_small.png)](https://www.linode.com/)\n\n(These services are beta and do not have any guarantee on service level)\n\nPlease refer to [online.md](docs/online.md) on how to run and use the `openapi-generator-online` - a web service for `openapi-generator`.\n\n### [3.4 - License information on Generated Code](#table-of-contents)\n\nThe OpenAPI Generator project is intended as a benefit for users of the Open API Specification.  The project itself has the [License](#7---license) as specified. In addition, please understand the following points:\n\n* The templates included with this project are subject to the [License](#7---license).\n* Generated code is intentionally _not_ subject to the parent project license\n\nWhen code is generated from this project, it shall be considered **AS IS** and owned by the user of the software.  There are no warranties--expressed or implied--for generated code.  You can do what you wish with it, and once generated, the code is your responsibility and subject to the licensing terms that you deem appropriate.\n\n### [3.5 - IDE Integration](#table-of-contents)\n\nHere is a list of community-contributed IDE plug-ins that integrate with OpenAPI Generator:\n\n- Eclipse: [Codewind OpenAPI Tools for Eclipse](https://www.eclipse.org/codewind/open-api-tools-for-eclipse.html) by [IBM](https://www.ibm.com)\n- IntelliJ IDEA: [OpenAPI Generator](https://plugins.jetbrains.com/plugin/8433-openapi-generator) by [Jim Schubert](https://jimschubert.us/#/)\n- IntelliJ IDEA: [Senya Editor](https://plugins.jetbrains.com/plugin/10690-senya-editor) by [senya.io](https://senya.io)\n- [RepreZen API Studio](https://www.reprezen.com/)\n- Visual Studio: [REST API Client Code Generator](https://marketplace.visualstudio.com/items?itemName=ChristianResmaHelle.ApiClientCodeGenerator) by [Christian Resma Helle](https://christian-helle.blogspot.com/)\n- Visual Studio Code: [Codewind OpenAPI Tools](https://marketplace.visualstudio.com/items?itemName=IBM.codewind-openapi-tools) by [IBM](https://marketplace.visualstudio.com/publishers/IBM)\n\n\n## [4 - Companies/Projects using OpenAPI Generator](#table-of-contents)\nHere are some companies/projects (alphabetical order) using OpenAPI Generator in production. To add your company/project to the list, please visit [README.md](README.md) and click on the icon to edit the page.\n\n- [Aalborg University](https://www.aau.dk)\n- [act coding](https://github.com/actcoding)\n- [Adaptant Solutions AG](https://www.adaptant.io/)\n- [adesso SE](https://www.adesso.de/)\n- [adorsys GmbH & Co.KG](https://adorsys.com/)\n- [Adyen](https://www.adyen.com/)\n- [Agoda](https://www.agoda.com/)\n- [Airthings](https://www.airthings.com/)\n- [Aleri Solutions Gmbh](https://www.aleri.de/)\n- [Allianz](https://www.allianz.com)\n- [Angular.Schule](https://angular.schule/)\n- [Aqovia](https://aqovia.com/)\n- [Australia and New Zealand Banking Group (ANZ)](http://www.anz.com/)\n- [Arduino](https://www.arduino.cc/)\n- [ASKUL](https://www.askul.co.jp)\n- [Amazon Web Services (AWS)](https://aws.amazon.com/)\n- [b<>com](https://b-com.com/en)\n- [ç™¾åº¦è¥é”€](https://e.baidu.com)\n- [Bandwidth](https://dev.bandwidth.com)\n- [Banzai Cloud](https://banzaicloud.com)\n- [BIMData.io](https://bimdata.io)\n- [Bithost GmbH](https://www.bithost.ch)\n- [Bosch Connected Industry](https://www.bosch-connected-industry.com)\n- [Boxever](https://www.boxever.com/)\n- [Brevy](https://www.brevy.com)\n- [Bunker Holding Group](https://www.bunker-holding.com/)\n- [California State University, Northridge](https://www.csun.edu)\n- [CAM](https://www.cam-inc.co.jp/)\n- [Camptocamp](https://www.camptocamp.com/en)\n- [Carlsberg Group](https://www.carlsberggroup.com/)\n- [CERN](https://home.cern/)\n- [Christopher Queen Consulting](https://www.christopherqueenconsulting.com/)\n- [Cisco](https://www.cisco.com/)\n- [codecentric AG](https://www.codecentric.de/)\n- [CoinAPI](https://www.coinapi.io/)\n- [Commencis](https://www.commencis.com/)\n- [ConfigCat](https://configcat.com/)\n- [cronn GmbH](https://www.cronn.de/)\n- [Crossover Health](https://crossoverhealth.com/)\n- [Cupix](https://www.cupix.com/)\n- [Datadog](https://www.datadoghq.com)\n- [DB Systel](https://www.dbsystel.de)\n- [Deeporute.ai](https://www.deeproute.ai/)\n- [Devsupply](https://www.devsupply.com/)\n- [dmTECH GmbH](https://www.dmTECH.de)\n- [DocSpring](https://docspring.com/)\n- [dwango](https://dwango.co.jp/)\n- [Edge Impulse](https://www.edgeimpulse.com/)\n- [Element AI](https://www.elementai.com/)\n- [Embotics](https://www.embotics.com/)\n- [emineo](https://www.emineo.ch)\n- [fastly](https://www.fastly.com/)\n- [Fenergo](https://www.fenergo.com/)\n- [freee](https://corp.freee.co.jp/en/)\n- [FreshCells](https://www.freshcells.de/)\n- [Fuse](https://www.fuse.no/)\n- [Gantner](https://www.gantner.com)\n- [GenFlow](https://github.com/RepreZen/GenFlow)\n- [GetYourGuide](https://www.getyourguide.com/)\n- [Glovo](https://glovoapp.com/)\n- [GMO Pepabo](https://pepabo.com/en/)\n- [GoDaddy](https://godaddy.com)\n- [Gumtree](https://gumtree.com)\n- [Here](https://developer.here.com/)\n- [IBM](https://www.ibm.com/)\n- [Instana](https://www.instana.com)\n- [Interxion](https://www.interxion.com)\n- [Inquisico](https://inquisico.com)\n- [JustStar](https://www.juststarinfo.com)\n- [k6.io](https://k6.io/)\n- [Klarna](https://www.klarna.com/)\n- [Kronsoft Development](https://www.kronsoft.ro/home/)\n- [Kubernetes](https://kubernetes.io)\n- [Landeshauptstadt MÃ¼nchen - it@M](https://muenchen.digital/it-at-m/)\n- [Linode](https://www.linode.com/)\n- [Logicdrop](https://www.logicdrop.com)\n- [Lumeris](https://www.lumeris.com)\n- [LVM Versicherungen](https://www.lvm.de)\n- [MailSlurp](https://www.mailslurp.com)\n- [Manticore Search](https://manticoresearch.com)\n- [Mastercard](https://developers.mastercard.com)\n- [MÃ©diavision](https://www.mediavision.fr/)\n- [Metaswitch](https://www.metaswitch.com/)\n- [MoonVision](https://www.moonvision.io/)\n- [Myworkout](https://myworkout.com)\n- [NamSor](https://www.namsor.com/)\n- [Neverfail](https://www.neverfail.com/)\n- [NeuerEnergy](https://neuerenergy.com)\n- [Nokia](https://www.nokia.com/)\n- [OneSignal](https://www.onesignal.com/)\n- [Options Clearing Corporation (OCC)](https://www.theocc.com/)\n- [Openet](https://www.openet.com/)\n- [openVALIDATION](https://openvalidation.io/)\n- [Oracle](https://www.oracle.com/)\n- [Paxos](https://www.paxos.com)\n- [Plaid](https://plaid.com)\n- [PLAID, Inc.](https://plaid.co.jp/)\n- [Pinterest](https://www.pinterest.com)\n- [Ponicode](https://ponicode.dev/)\n- [Pricefx](https://www.pricefx.com/)\n- [PrintNanny](https://www.print-nanny.com/)\n- [Prometheus/Alertmanager](https://github.com/prometheus/alertmanager)\n- [Qavar](https://www.qavar.com)\n- [QEDIT](https://qed-it.com)\n- [Qovery](https://qovery.com)\n- [Qulix Systems](https://www.qulix.com)\n- [Raksul](https://corp.raksul.com)\n- [Raiffeisen Schweiz Genossenschaft](https://www.raiffeisen.ch)\n- [RedHat](https://www.redhat.com)\n- [RepreZen API Studio](https://www.reprezen.com/swagger-openapi-code-generation-api-first-microservices-enterprise-development)\n- [REST United](https://restunited.com)\n- [Robocorp](https://www.robocorp.com)\n- [Robotinfra](https://www.robotinfra.com)\n- [Sarvika Technologies Pvt. Ltd.](https://www.sarvika.com)\n- [SearchApi](https://www.searchapi.io/)\n- [SmartHR](https://smarthr.co.jp/)\n- [Sony Interactive Entertainment](https://www.sie.com/en/index.html)\n- [Splitit](https://www.splitit.com/)\n- [Stingray](http://www.stingray.com)\n- [Suva](https://www.suva.ch/)\n- [Svix](https://www.svix.com/)\n- [Telstra](https://dev.telstra.com)\n- [Tencent](https://www.tencent.com)\n- [The University of Aizu](https://www.u-aizu.ac.jp/en/)\n- [TINQIN](https://www.tinqin.com/)\n- [Translucent ApS](https://www.translucent.dk)\n- [TravelTime platform](https://www.traveltimeplatform.com/)\n- [TribalScale](https://www.tribalscale.com)\n- [Trifork](https://trifork.com)\n- [TUI InfoTec GmbH](http://www.tui-infotec.com/)\n- [Twilio](https://www.twilio.com/)\n- [Twitter](https://twitter.com)\n- [unblu inc.](https://www.unblu.com/)\n- [Veamly](https://www.veamly.com/)\n- [VMWare](https://www.vmware.com/)\n- [wbt-solutions](https://www.wbt-solutions.de/)\n- [Woleet](https://www.woleet.io/)\n- [WSO2](https://wso2.com/)\n- [Vouchery.io](https://vouchery.io)\n- [Xero](https://www.xero.com/)\n- [Yahoo Japan](https://www.yahoo.co.jp/)\n- [viadee](https://www.viadee.de/)\n- [Vonage](https://vonage.com)\n- [YITU Technology](https://www.yitutech.com/)\n- [Yelp](https://www.yelp.com/)\n- [Zalando](https://www.zalando.com)\n- [3DS Outscale](https://www.outscale.com/)\n\n## [5 - Presentations/Videos/Tutorials/Books](#table-of-contents)\n\n- 2018/05/12 - [OpenAPI Generator - community drivenã§æˆé•·ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿](https://ackintosh.github.io/blog/2018/05/12/openapi-generator/) by [ä¸­é‡æšäºº](https://github.com/ackintosh)\n- 2018/05/15 - [Starting a new open-source project](http://jmini.github.io/blog/2018/2018-05-15_new-open-source-project.html) by [Jeremie Bresson](https://github.com/jmini)\n- 2018/05/15 - [REST APIä»•æ§˜ã‹ã‚‰APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚„ã‚¹ã‚¿ãƒ–ã‚µãƒ¼ãƒã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã€ŒOpenAPI Generatorã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§å…¬é–‹ã€‚Swagger Codegenã‹ã‚‰ã®ãƒ•ã‚©ãƒ¼ã‚¯](https://www.publickey1.jp/blog/18/rest_apiapiopenapi_generatorswagger_generator.html) by [Publickey](https://www.publickey1.jp)\n- 2018/06/08 - [Swagger Codegen is now OpenAPI Generator](https://angular.schule/blog/2018-06-swagger-codegen-is-now-openapi-generator) by [JohannesHoppe](https://github.com/JohannesHoppe)\n- 2018/06/21 - [Connect your JHipster apps to the world of APIs with OpenAPI and gRPC](https://fr.slideshare.net/chbornet/jhipster-conf-2018-connect-your-jhipster-apps-to-the-world-of-apis-with-openapi-and-grpc) by [Christophe Bornet](https://github.com/cbornet) at [JHipster Conf 2018](https://jhipster-conf.github.io/)\n- 2018/06/22 - [OpenAPI Generator ã§ Gatling Client ã‚’ç”Ÿæˆã—ã¦ã¿ãŸ](https://rohki.hatenablog.com/entry/2018/06/22/073000) at [ã‚½ãƒ¢ã‚µãƒ³](https://rohki.hatenablog.com/)\n- 2018/06/27 - [Lessons Learned from Leading an Open-Source Project Supporting 30+ Programming Languages](https://speakerdeck.com/wing328/lessons-learned-from-leading-an-open-source-project-supporting-30-plus-programming-languages) - [William Cheng](https://github.com/wing328) at [LinuxCon + ContainerCon + CloudOpen China 2018](http://bit.ly/2waDKKX)\n- 2018/07/19 - [OpenAPI Generator Contribution Quickstart - RingCentral Go SDK](https://medium.com/ringcentral-developers/openapi-generator-for-go-contribution-quickstart-8cc72bf37b53) by [John Wang](https://github.com/grokify)\n- 2018/08/22 - [OpenAPI Generatorã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆãªã©ã®ãƒ¡ãƒ¢](https://yinm.info/20180822/) by [Yusuke Iinuma](https://github.com/yinm)\n- 2018/09/12 - [RepreZen and OpenAPI 3.0: Now is the Time](https://www.reprezen.com/blog/reprezen-openapi-3.0-upgrade-now-is-the-time) by [Miles Daffin](https://www.reprezen.com/blog/author/miles-daffin)\n- 2018/10/31 - [A node package wrapper for openapi-generator](https://github.com/HarmoWatch/openapi-generator-cli)\n- 2018/11/03 - [OpenAPI Generator + golang + Flutter ã§ã‚¢ãƒ—ãƒªé–‹ç™º](http://ryuichi111std.hatenablog.com/entry/2018/11/03/214005) by [Ryuichi Daigo](https://github.com/ryuichi111)\n- 2018/11/15 - [åŸºäºopenapi3.0çš„yamlæ–‡ä»¶ç”Ÿæˆjavaä»£ç çš„ä¸€æ¬¡å®è·µ](https://blog.csdn.net/yzy199391/article/details/84023982) by [ç„±é­”ç‹](https://me.csdn.net/yzy199391)\n- 2018/11/18 - [Generating PHP library code from OpenAPI](https://lornajane.net/posts/2018/generating-php-library-code-from-openapi) by [Lorna Jane](https://lornajane.net/) at [LORNAJANE Blog](https://lornajane.net/blog)\n- 2018/11/19 - [OpenAPIs are everywhere](https://youtu.be/-lDot4Yn7Dg) by [Jeremie Bresson (Unblu)](https://github.com/jmini) at [EclipseCon Europe 2018](https://www.eclipsecon.org/europe2018)\n- 2018/12/09 - [openapi-generator ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹æ–¹æ³•](https://qiita.com/watiko/items/0961287c02eac9211572) by [@watiko](https://qiita.com/watiko)\n- 2019/01/03 - [Calling a Swagger service from Apex using openapi-generator](https://lekkimworld.com/2019/01/03/calling-a-swagger-service-from-apex-using-openapi-generator/) by [Mikkel Flindt Heisterberg](https://lekkimworld.com)\n- 2019/01/13 - [OpenAPI Generatorã§RESTful APIã®å®šç¾©æ›¸ã‹ã‚‰è‰²ã€…è‡ªå‹•ç”Ÿæˆã™ã‚‹](https://ky-yk-d.hatenablog.com/entry/2019/01/13/234108) by [@ky_yk_d](https://twitter.com/ky_yk_d)\n- 2019/01/20 - [Contract-First API Development with OpenAPI Generator and Connexion](https://medium.com/commencis/contract-first-api-development-with-openapi-generator-and-connexion-b21bbf2f9244) by [Anil Can Aydin](https://github.com/anlcnydn)\n- 2019/01/30 - [Rapid Application Development With API First Approach Using Open-API Generator](https://dzone.com/articles/rapid-api-development-using-open-api-generator) by [Milan Sonkar](https://dzone.com/users/828329/milan_sonkar.html)\n- 2019/02/02 - [å¹³é™ã‚’ä¿ã¡ã€ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã›ã‚ˆ ã€œ OpenAPI Generatorèª•ç”Ÿã®èƒŒæ™¯ã¨è»Œè·¡ ã€œ](https://speakerdeck.com/akihito_nakano/gunmaweb34) by [ä¸­é‡æšäºº](https://github.com/ackintosh) at [Gunma.web #34 ã‚¹ã‚­ãƒ¼ãƒé§†å‹•é–‹ç™º](https://gunmaweb.connpass.com/event/113974/)\n- 2019/02/20 - [An adventure in OpenAPI V3 code generation](https://mux.com/blog/an-adventure-in-openapi-v3-api-code-generation/) by [Phil Cluff](https://mux.com/blog/author/philc/)\n- 2019/02/26 - [Building API Services: A Beginnerâ€™s Guide](https://medium.com/google-cloud/building-api-services-a-beginners-guide-7274ae4c547f) by [Ratros Y.](https://medium.com/@ratrosy) in [Google Cloud Platform Blog](https://medium.com/google-cloud)\n- 2019/02/26 - [Building APIs with OpenAPI: Continued](https://medium.com/@ratrosy/building-apis-with-openapi-continued-5d0faaed32eb) by [Ratros Y.](https://medium.com/@ratrosy) in [Google Cloud Platform Blog](https://medium.com/google-cloud)\n- 2019-03-07 - [OpenAPI Generator ã§ Spring Boot ã¨ Angular ã‚’ã‚¿ã‚¤ãƒ—ã‚»ãƒ¼ãƒ•ã«ç¹‹ã](https://qiita.com/chibato/items/e4a748db12409b40c02f) by [Tomofumi Chiba](https://github.com/chibat)\n- 2019-03-16 - [A Quick introduction to manual OpenAPI V3](https://vadosware.io/post/quick-intro-to-manual-openapi-v3/) by [vados](https://github.com/t3hmrman) at [VADOSWARE](https://vadosware.io)\n- 2019-03-25 - [Access any REST service with the SAP S/4HANA Cloud SDK](https://blogs.sap.com/2019/03/25/integrate-sap-s4hana-cloud-sdk-with-open-api/) by [Alexander Duemont](https://people.sap.com/alexander.duemont)\n- 2019-03-25 - [OpenAPI generatorã‚’è©¦ã—ã¦ã¿ã‚‹](https://qiita.com/amuyikam/items/e8a45daae59c68be0fc8) by [@amuyikam](https://twitter.com/amuyikam)\n- 2019-03-27 - [OpenAPI3ã‚’ä½¿ã£ã¦ã¿ã‚ˆã†ï¼Goè¨€èªã§ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã‚¹ã‚¿ãƒ–ã®è‡ªå‹•ç”Ÿæˆã¾ã§ï¼](https://techblog.zozo.com/entry/openapi3/go) by [@gold_kou](https://twitter.com/gold_kou)\n- 2019-04-17 - [OpenAPIã«ã‚ˆã‚‹ã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆé–‹ç™ºã®å®Ÿæ–½ã‚µãƒ³ãƒ—ãƒ«ã¨Cloud Runã«ã¤ã„ã¦](https://tech-blog.optim.co.jp/entry/2019/04/17/174000) by [@yukey1031](https://twitter.com/yukey1031)\n- 2019-04-18 - [How to use OpenAPI3 for API developer (RubyKaigi 2019)](https://speakerdeck.com/ota42y/how-to-use-openapi3-for-api-developer) by [@ota42y](https://twitter.com/ota42y) at [RubyKaigi 2019](https://rubykaigi.org/2019)\n- 2019-04-29 - [A Beginner's Guide to Code Generation for REST APIs (OpenAPI Generator)](https://gum.co/openapi_generator_ebook) by [William Cheng](https://twitter.com/wing328)\n- 2019-05-01 - [Design and generate a REST API from Swagger / OpenAPI in Java, Python, C# and more](https://simply-how.com/design-and-generate-api-code-from-openapi) by [Simply How](https://simply-how.com/)\n- 2019-05-17 - [Generate Spring Boot REST API using Swagger/OpenAPI](https://www.47northlabs.com/knowledge-base/generate-spring-boot-rest-api-using-swagger-openapi/) by [Antonie Zafirov](https://www.47northlabs.com/author/antonie-zafirov/)\n- 2019-05-22 - [REST APIsä»£ç ç”ŸæˆæŒ‡å—(OpenAPI Generator)](https://gum.co/openapi_generator_ebook_gb) by [William Cheng](https://twitter.com/wing328), [Xin Meng](https://github.com/xmeng1)\n- 2019-05-24 - [REST API ä»£ç¢¼ç”ŸæˆæŒ‡å— (OpenAPI Generator)](https://gum.co/openapi_generator_ebook_big5) by [William Cheng](https://twitter.com/wing328)\n- 2019-06-24 - [Kubernetes Clients and OpenAPI Generator](https://speakerdeck.com/wing328/kubernetes-clients-and-openapi-generator) by [William Cheng](https://twitter.com/wing328) at [Kubernetes Contributor Summits Shanghai 2019](https://www.lfasiallc.com/events/contributors-summit-china-2019/)\n- 2019-06-28 [Codewind OpenAPI Tools](https://marketplace.eclipse.org/content/codewind-openapi-tools) in [Eclipse Marketplace](https://marketplace.eclipse.org/) by IBM\n- 2019-06-29 [Codewind OpenAPI Tools](https://marketplace.visualstudio.com/items?itemName=IBM.codewind-openapi-tools) in [Visual Studio Marketplace](https://marketplace.visualstudio.com/) by IBM\n- 2019-07-04 - [REST API ã®ãŸã‚ã®ã‚³ãƒ¼ãƒˆã‚™ç”Ÿæˆå…¥é–€ (OpenAPI Generator)](https://gum.co/openapi_generator_ebook_big5) by [William Cheng](https://twitter.com/wing328), [ä¸­é‡æšäºº](https://github.com/ackintosh), [å’Œç”°æ‹“æœ—](https://github.com/taxpon)\n- 2019-07-08 - [OpenAPI Generator ã«ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆã—ãŸã‚‰ç¤¾åãŒè¼‰ã£ãŸè©±ã€‚(CAM) - CAM TECH BLOG](https://tech.cam-inc.co.jp/entry/2019/07/08/140000) by [CAM, Inc.](https://www.cam-inc.co.jp/)\n- 2019-07-14 - [OpenAPI Generatorã§Pythonã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½œæˆã—ãŸ](https://qiita.com/yuji38kwmt/items/dfb929316a1335a161c0) by [yuji38kwmt](https://qiita.com/yuji38kwmt)\n- 2019-07-19 - [Developer Experience (DX) for Open-Source Projects: How to Engage Developers and Build a Growing Developer Community](https://speakerdeck.com/wing328/developer-experience-dx-for-open-source-projects-english-japanese) by [William Cheng](https://twitter.com/wing328), [ä¸­é‡æšäºº](https://github.com/ackintosh) at [Open Source Summit Japan 2019](https://events.linuxfoundation.org/events/open-source-summit-japan-2019/)\n- 2019-08-14 - [Our OpenAPI journey with Standardizing SDKs](https://bitmovin.com/our-openapi-journey-with-standardizing-sdks/) by [Sebastian Burgstaller](https://bitmovin.com/author/sburgstaller/) at [Bitmovin](https://www.bitmovin.com)\n- 2019-08-15 - [APIã®ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•ç”Ÿæˆã•ã›ãŸã„ã ã‘ãªã‚‰gRPCã§ãªãã¦ã‚‚ã‚ˆããªã„?](https://www.m3tech.blog/entry/2019/08/15/110000) by [M3, Inc.](https://corporate.m3.com/)\n- 2019-08-22 - [ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã«ãŠã‘ã‚‹Web APIã‚¹ã‚­ãƒ¼ãƒã®ç®¡ç†â”€ GraphQLã€gRPCã€OpenAPIã®ç‰¹å¾´ã¨ä½¿ã„ã©ã“ã‚](https://employment.en-japan.com/engineerhub/entry/2019/08/22/103000) by [@ota42y](https://twitter.com/ota42y)\n- 2019-08-24 - [Swaggerãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰OpenAPI Generatorã‚’ä½¿ã£ã¦ãƒ¢ãƒƒã‚¯ã‚µãƒ¼ãƒãƒ¼ä½œæˆ](https://qiita.com/masayoshi0222/items/4845e4c715d04587c104) by [å‚æœ¬æ­£ç¾©](https://qiita.com/masayoshi0222)\n- 2019-08-29 - [OpenAPIåˆæ¢](https://cloud.tencent.com/developer/article/1495986) by [peakxie](https://cloud.tencent.com/developer/user/1113152) at [è…¾è®¯äº‘ç¤¾åŒº](https://cloud.tencent.com/developer)\n- 2019-08-29 - [å…¨é¢è¿›åŒ–ï¼šKubernetes CRD 1.16 GAå‰ç»](https://www.servicemesher.com/blog/kubernetes-1.16-crd-ga-preview/) by [Min Kim](https://github.com/yue9944882) at [ServiceMesher Blog](https://www.servicemesher.com/blog/)\n- 2019-09-01 - [Creating a PHP-Slim server using OpenAPI (Youtube video)](https://www.youtube.com/watch?v=5cJtbIrsYkg) by [Daniel Persson](https://www.youtube.com/channel/UCnG-TN23lswO6QbvWhMtxpA)\n- 2019-09-06 - [Vert.x and OpenAPI](https://wissel.net/blog/2019/09/vertx-and-openapi.html) by [Stephan H Wissel](https://twitter.com/notessensei) at [wissel.net blog](https://wissel.net)\n- 2019-09-09 - [Cloud-native development - Creating RESTful microservices](https://cloud.ibm.com/docs/cloud-native?topic=cloud-native-rest-api) in [IBM Cloud Docs](https://cloud.ibm.com/docs)\n- 2019-09-14 - [Generating and Configuring a Mastercard API Client](https://developer.mastercard.com/platform/documentation/generating-and-configuring-a-mastercard-api-client/) at [Mastercard Developers Platform](https://developer.mastercard.com/platform/documentation/)\n- 2019-09-15 - [OpenAPI(Swagger)å°å…¥ä¸‹èª¿ã¹](https://qiita.com/ShoichiKuraoka/items/f1f7a3c2376f7cd9c56a) by [Shoichi Kuraoka](https://qiita.com/ShoichiKuraoka)\n- 2019-09-17 - [Tutorial: Documenting http4k APIs with OpenApi3](https://www.http4k.org/tutorials/documenting_apis_with_openapi/) by [http4k](https://www.http4k.org/)\n- 2019-09-22 - [OpenAPI 3ã‚’å®Œå…¨ã«ç†è§£ã§ãã‚‹æœ¬](https://booth.pm/ja/items/1571902) by [@ota42y](https://twitter.com/ota42y)\n- 2019-09-22 - [RESTful APIs: Tutorial of OpenAPI Specification](https://medium.com/@amirm.lavasani/restful-apis-tutorial-of-openapi-specification-eeada0e3901d) by [Amir Lavasani](https://medium.com/@amirm.lavasani)\n- 2019-09-22 - [Redefining SDKs as software diversity kits](https://devrel.net/dev-rel/redefining-sdks-as-software-diversity-kits) by [Sid Maestre (Xero)](https://twitter.com/sidneyallen) at [DevRelCon San Francisco 2019](https://sf2019.devrel.net/)\n- 2019-09-23 - [swaggerã‹ã‚‰OpenApi Generatorã§Springã®ã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•ç”Ÿæˆ](https://qiita.com/littleFeet/items/492df2ad68a0799a5e5e) by [@littleFeet](https://qiita.com/littleFeet) at [Qiita](https://qiita.com/)\n- 2019-09-24 - [Eine Stunde was mit Api First!](https://www.slideshare.net/JanWeinschenker/eine-stunde-was-mit-api-first) by [@janweinschenker](https://twitter.com/janweinschenker) at [Java Forum Nord](https://javaforumnord.de/)\n- 2019-10-09 - [openapi-generator ã§ç”Ÿæˆã—ãŸ Go ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ Bearer èªè¨¼ã‚’ã™ã‚‹](https://autopp-tech.hatenablog.com/entry/2019/10/09/222039) by [Akira Tanimura](https://github.com/autopp)\n- 2019-10-10 - [Automatic Generation of REST Clients](https://www.meetup.com/fr-FR/Criteo-Labs-Tech-Talks/events/264775768/) by Thomas Peyrard, Senior Software Engineer at Criteo in [Full-Stack Tech Talks (Meetup)](https://www.meetup.com/fr-FR/Criteo-Labs-Tech-Talks/events/264775768/)\n- 2019-10-12 - [OpenApiè‡ªåŠ¨ç”Ÿæˆclient](https://blog.csdn.net/wxid2798226/article/details/102527467) by [éƒ‘æ³½æ´²](https://me.csdn.net/wxid2798226)\n- 2019-10-16 - [How to ship APIs faster?](https://medium.com/@accounts_76224/how-to-ship-apis-faster-cabef2f819e4) by [Simon Guilliams @ PoniCode](https://ponicode.dev)\n- 2019-10-22 - [OpenAPI + Spring Boot(Kotlin)ã§ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰APIã‚’ä½œæˆã™ã‚‹](https://qiita.com/boronngo/items/4b78b92526209daeaee9) by [Yuki Furukawa](https://twitter.com/yuki_furukawa5)\n- 2019-10-24 - [Microprofile OpenAPI - Code First or Design First?](https://github.com/pe-st/apidocs/blob/master/MicroProfile-OpenAPI-all-slides.pdf) by [Peter [pÉ›ÊƒÉ™] Steiner](https://twitter.com/pesche) at [eclipsecon Europe 2019](https://www.eclipsecon.org/europe2019/sessions/microprofile-openapi-code-first-or-design-first)\n- 2019-11-06 - [Generating API clients based on OpenAPI v3 specifications](https://98elements.com/blog/generating-api-clients-based-on-openapi-v3-specifications) by [Dominik JastrzÄ™bski @ 98elements](https://98elements.com)\n- 2019-11-06 - [OpenAPIã‚’åˆ©ç”¨ã—ã¦è‡ªå‰ã®APIã‚µãƒ¼ãƒãƒ¼(Sinatra)ã‚’ç§»æ¤ã—ãŸæ™‚ã®ãƒ¡ãƒ¢](https://qiita.com/YasuhiroABE/items/c73920eab2d9d6e97fd9) by [Yasuhiro ABE](https://twitter.com/YasuhiroABE)\n- 2019-11-07 - [API First development with OpenAPI - You should you practise it !?](https://www.youtube.com/watch?v=F9iF3a1Z8Y8) by [Nick Van Hoof](https://www.nickvanhoof.com/) at [Devoxx Belgium 2019](https://devoxx.be/)\n- 2019-11-08 - [JHipster beyond CRUD - API-First for Enterprises by Enrico Costanzi](https://www.youtube.com/watch?v=m28JFovKQ20) by [Enrico Costanzi](https://twitter.com/enricocostanzi) at [JHipster Conf 2019 in Paris](https://jhipster-conf.github.io/)\n- 2019-11-11 - [TypeScript REST APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ](https://qiita.com/unhurried/items/7b74f7d3c43545dadd2b) by [@unhurried](https://qiita.com/unhurried)\n- 2019-11-11 - [One Spec to Rule them all - OpenAPI in Action](https://www.youtube.com/watch?v=MMay_nht8ec) by [Andreas Litt](https://github.com/littldr) at [code.talks 2019](https://www.codetalks.com/)\n- 2019-11-13 - [OpenAPI 3.0 Editor And Generator With A Spring Boot Example](https://simply-how.com/design-and-generate-api-code-from-openapi) at [Simply How](https://simply-how.com/)\n- 2019-11-17 - [OpenAPI Generator YouTube playlist](https://www.youtube.com/playlist?list=PLtJyHVMdzfF6fBkOUV5VDVErP23CGgHIy) at [YouTube](https://www.youtube.com)\n- 2019-11-20 - [Introduction to OpenAPI](https://noti.st/lornajane/HvDH7U/introduction-to-openapi) by [Lorna Mitchell](https://twitter.com/lornajane) at [GOTO Copenhagen 2019](https://gotocph.com/2019/)\n- 2019-11-20 - [How to Generate Angular code from OpenAPI specifications](https://dotnetthoughts.net/how-to-generate-angular-code-from-openapi-specifications/) by Anuraj\n- 2019-11-23 - [Swagger ã§ã¯ãªã„ OpenAPI Specification 3.0 ã«ã‚ˆã‚‹ API ã‚µãƒ¼ãƒãƒ¼é–‹ç™º](https://www.slideshare.net/techblogyahoo/swagger-openapi-specification-30-api) by [Tetsuya Morimoto](https://github.com/t2y) at [JJUG CCC 2019 Fall](https://ccc2019fall.java-users.jp/)\n- 2019-11-24 - [Accelerate Flutter development with OpenAPI and Dart code generation](https://medium.com/@irinasouthwell_220/accelerate-flutter-development-with-openapi-and-dart-code-generation-1f16f8329a6a) by [Irina Southwell](https://medium.com/@irinasouthwell_220)\n- 2019-11-25 - [openapi-generatorã§æ‰‹è»½ã«ã‚¹ã‚¿ãƒ–ã‚µãƒ¼ãƒã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç”Ÿæˆ](https://qiita.com/pochopocho13/items/8db662e1934fb2b408b8) by [@pochopocho13](https://twitter.com/pochopocho13)\n- 2019-11-26 - [CordaCon 2019 Highlights: Braid Server and OpenAPI Generator for Corda Client APIâ€™s](https://blog.b9lab.com/cordacon-2019-highlights-braid-server-and-openapi-generator-for-corda-flows-api-s-d24179ccb27c) by [Adel Rustum](https://blog.b9lab.com/@adelrestom) at [B9lab](https://blog.b9lab.com/)\n- 2019-12-03 - [A Road to Less Coding: Auto-Generate APILibrary](https://www.corda.net/blog/a-road-to-less-coding-auto-generate-apilibrary/) at [Corda Blog](https://www.corda.net/blog/)\n- 2019-12-04 - [Angularï¼‹NestJSï¼‹OpenAPIï¼ˆSwaggerï¼‰ã§ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã‚’è¦–é‡ã«å…¥ã‚ŒãŸç’°å¢ƒã‚’è€ƒãˆã‚‹](https://qiita.com/teracy55/items/0327c7a170ec772970c6) by [ã¦ã‚‰ã—ãƒ¼](https://twitter.com/teracy55)\n- 2019-12-05 - [Code generation on the Java VM](https://speakerdeck.com/sullis/code-generation-on-the-java-vm-2019-12-05) by [Sean Sullivan](https://speakerdeck.com/sullis)\n- 2019-12-17 - [OpenAPI Generator ã§ OAuth2 ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ç™ºè¡Œã®ã‚³ãƒ¼ãƒ‰ã¾ã§ç”Ÿæˆã—ã¦ã¿ã‚‹](https://www.techscore.com/blog/2019/12/17/openapi-generator-oauth2-accesstoken/) by [TECHSCORE](https://www.techscore.com/blog/)\n- 2019-12-23 - [Use Ada for Your Web Development](https://www.electronicdesign.com/technologies/embedded-revolution/article/21119177/use-ada-for-your-web-development) by [Stephane Carrez](https://github.com/stcarrez)\n- 2019-12-23 - [OpenAPIã®ã‚¹ã‚­ãƒ¼ãƒã‚’åˆ†å‰²ãƒ»æ§‹é€ åŒ–ã—ã¦ã„ãæ–¹æ³•](https://gift-tech.co.jp/articles/structured-openapi-schema) by [å°é£¯å¡šé”ä¹Ÿ](https://github.com/t2h5) at [GiFT, Inc](https://gift-tech.co.jp/)\n- 2020-01-17 - [OpenAPI demo for Pulp 3.0 GA](https://www.youtube.com/watch?v=mFBP-M0ZPfw&t=178s) by [Pulp](https://www.youtube.com/channel/UCI43Ffs4VPDv7awXvvBJfRQ) at [Youtube](https://www.youtube.com/)\n- 2020-01-19 - [Why document a REST API as code?](https://dev.to/rolfstreefkerk/why-document-a-rest-api-as-code-5e7p) by [Rolf Streefkerk](https://github.com/rpstreef) at [DEV Community](https://dev.to)\n- 2020-01-28 - [Get Your Serverless Swagger Back with OpenAPI](https://dev.to/matttyler/get-your-serverless-swagger-back-with-openapi-48gc) by [Matt Tyler](https://dev.to/matttyler)\n- 2020-01-30 - [OpenAPI Generatorã¸ã®ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆ](https://www.yutaka0m.work/entry/2020/01/30/163905) by [yutaka0m](https://github.com/yutaka0m)\n- 2020-02-01 - [Using OpenAPI to Maximise Your Pulp 3 Experience](https://fosdem.org/2020/schedule/event/openapi/) by [Dennis Kliban](https://github.com/dkliban/) at [FOSDEM](https://fosdem.org/)\n- 2020-02-07 - [Why you should use OpenAPI for your API design](https://www.youtube.com/watch?v=zhb7vUApLW8&t=927s) by [Nick Van Hoof](https://apiconference.net/speaker/nick-van-hoof/) at [API Conference](https://apiconference.net/)\n- 2020-02-17 - [Rubynetes: using OpenAPI to validate Kubernetes configs](https://www.brightbox.com/blog/2020/02/17/using-openapi-to-validate-kubernetes-configs/) by Neil Wilson at [Brightbox](https://www.brightbox.com/)\n- 2020-02-20 - [Building SDKs for the future](https://devblog.xero.com/building-sdks-for-the-future-b79ff726dfd6) by [Sid Maestre (Xero)](https://twitter.com/sidneyallen)\n- 2020-02-27 - [Nuxtåˆ©ç”¨ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã§IE11ã¨ä»²è‰¯ãã™ã‚‹ãŸã‚ã®E2E](https://tech.medpeer.co.jp/entry/e2e-ie11) at [Medpeer.co.jp Tech Blog](https://tech.medpeer.co.jp/)\n- 2020-02-29 - [Providing Support to IoT Devices Deployed in Disconnected Rural Environment (Conference paper)](https://link.springer.com/chapter/10.1007/978-3-030-41494-8_14) by Sergio Laso, Daniel Flores-MartÃ­n, Juan Luis HerreraCarlos, CanalJuan Manuel, MurilloJavier Berrocal\n- 2020-03-02 - [How To Generate Angular & Spring Code From OpenAPI Specification](https://www.mokkapps.de/blog/how-to-generate-angular-and-spring-code-from-open-api-specification/) by [Michael Hoffmann](https://www.mokkapps.de/)\n- 2020-03-02 - [OpenAPI Generator + TypeScript ã§å§‹ã‚ã‚‹è‡ªå‹•ç”Ÿæˆã®å‹ã«å®ˆã‚‰ã‚ŒãŸè±Šã‹ãªã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç”Ÿæ´»](https://gift-tech.co.jp/articles/openapi-generator-typescript) by [äº”ç™¾è”µ ç›´æ¨¹](https://gift-tech.co.jp/members/naokiioroi) at [GiFTæ ªå¼ä¼šç¤¾](https://gift-tech.co.jp/)\n- 2020-03-10 - [OpenAPI Generator Meetup #1](https://speakerdeck.com/akihito_nakano/openapi-generator-meetup-number-1) by [ä¸­é‡æšäºº](https://github.com/ackintosh) at [OpenAPI Generator Meetup #1](https://openapi-generator-meetup.connpass.com/event/168187/)\n- 2020-03-15 - [Load Testing Your API with Swagger/OpenAPI and k6](https://k6.io/blog/load-testing-your-api-with-swagger-openapi-and-k6)\n- 2020-04-13 - [ä¿ºçš„ã€OASã€‘ã¨ã®å‘ãåˆã„æ–¹ (çˆ†é€Ÿã§OpenAPIã¨å‹é”ã«ãªã‚ã†)](https://tech-blog.optim.co.jp/entry/2020/04/13/100000) in [OPTim Blog](https://tech-blog.optim.co.jp/)\n- 2020-04-22 - [Introduction to OpenAPI Generator](https://nordicapis.com/introduction-to-openapi-generator/) by [Kristopher Sandoval](https://nordicapis.com/author/sandovaleffect/) in [Nordic APIs](https://nordicapis.com/)\n- 2020-04-27 - [How we use Open API v3 specification to auto-generate API documentation, code-snippets and clients](https://medium.com/pdf-generator-api/how-we-use-open-api-v3-specification-to-auto-generate-api-documentation-code-snippets-and-clients-d127a3cea784) by [Tanel TÃ¤hepÃµld](https://medium.com/@tanel.tahepold)\n- 2020-05-09 - [OpenAPIã§ãŠæ‰‹è»½ã«ãƒ¢ãƒƒã‚¯APIã‚µãƒ¼ãƒãƒ¼ã‚’å‹•ã‹ã™](https://qiita.com/kasa_le/items/97ca6a8dd4605695c25c) by [Sachie Kamba](https://qiita.com/kasa_le)\n- 2020-05-18 - [Spring Boot REST with OpenAPI 3](https://dev.to/alfonzjanfrithz/spring-boot-rest-with-openapi-3-59jm) by [Alfonz Jan Frithz](https://dev.to/alfonzjanfrithz)\n- 2020-05-19 - [Dead Simple APIs with Open API](https://www.youtube.com/watch?v=sIaXmR6xRAw) by [Chris Tankersley](https://github.com/dragonmantank) at [Nexmo](https://developer.nexmo.com/)\n- 2020-05-22 - [TypeScript REST API Client](https://dev.to/unhurried/typescript-rest-api-client-4in3) by [\"unhurried\"](https://dev.to/unhurried)\n- 2020-05-28 - [ã€ä½¿ç”¨ lotify + Swagger å»ºç½®å¯å…±ç”¨çš„ LINE Notify botã€‘ - #NiJia @ Chatbot Developer Taiwan ç¬¬ #19 å°èš](https://www.youtube.com/watch?v=agYVz6dzh1I) by [Chatbot Developer Taiwan](https://www.youtube.com/channel/UCxeYUyZNnHmpX23YNF-ewvw)\n- 2020-05-28 - [Building APIs with Laravel using OpenAPI](https://www.youtube.com/watch?v=xexLvQqAhiA) by [Chris Tankersley](https://github.com/dragonmantank) at [Laracon EU](https://laracon.eu/)\n- 2020-06-12 - [Interoperability by construction: code generation for Arrowhead Clients](https://ieeexplore.ieee.org/document/9274746) by Michele Albano, Brian Nielsen at [2020 IEEE Conference on Industrial Cyberphysical Systems (ICPS)](https://ieeexplore.ieee.org/xpl/conhome/9274544/proceeding)\n- 2020-06-23 - [æ–°è¦ã‚µãƒ¼ãƒãƒ¼ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«TypeScriptã‚’æ¡ç”¨ã—ã¦ã¿ãŸ](https://www.cam-inc.co.jp/news/20200623) at [CAM Tech Blog](https://www.cam-inc.co.jp/news/tech-blog/)\n- 2020-06-29 - [Artifact Abstract: Deployment of APIs on Android Mobile Devices and Microcontrollers](https://ieeexplore.ieee.org/document/9127353) by [Sergio Laso ; Marino Linaje ; Jose Garcia-Alonso ; Juan M. Murillo ; Javier Berrocal](https://ieeexplore.ieee.org/document/9127353/authors#authors) at [2020 IEEE International Conference on Pervasive Computing and Communications (PerCom)](https://ieeexplore.ieee.org/xpl/conhome/9125449/proceeding)\n- 2020-07-07 - [5 Best API Documentation Tools](https://blog.dreamfactory.com/5-best-api-documentation-tools/) by Susanna Bouse at [DreamFactory Blog](https://blog.dreamfactory.com/)\n- 2020-07-12 - [Open API 3.0ã®å®šç¾©ã‹ã‚‰golangã®ã‚µãƒ¼ãƒã‚³ãƒ¼ãƒ‰ã®ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã‚’ä½œæˆã™ã‚‹](https://qiita.com/professor/items/4cbd04ec084d13057bc2) by [@professor (Qiita Blog)](https://qiita.com/professor)\n- 2020-07-20 - [Datadog API client libraries now available for Java and Go](https://www.datadoghq.com/blog/java-go-libraries/) by Jordan Obey at [Datadog Blog](https://www.datadoghq.com/blog)\n- 2020-07-23 - [Generate Client SDK for .NET Core using Open Api](https://dev.to/no0law1/generate-client-sdk-for-net-core-using-open-api-2dgh) by [Nuno Reis](https://dev.to/no0law1)\n- 2020-07-26 - [Dartã®http_interceptorãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã†ã¨é…åˆ—ã®ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ¶ˆãˆã¦ã—ã¾ã†ä»¶ã®å¿œæ€¥å‡¦ç½®](https://qiita.com/gyamoto/items/eeeff81b6770487319ed) by [@gyamoto](https://qiita.com/gyamoto)\n- 2020-08-01 - [Generate Angular ReactiveForms from Swagger/OpenAPI](https://dev.to/martinmcwhorter/generate-angular-reactiveforms-from-swagger-openapi-35h9) by [Martin McWhorter](https://dev.to/martinmcwhorter)\n- 2020-08-03 - [Criando Bibliotecas para APIs RESTful com OpenAPI, Swagger Editor e OpenAPI Generator](https://medium.com/@everisBrasil/criando-bibliotecas-para-apis-restful-com-openapi-swagger-editor-e-openapi-generator-75349a6420fd) by [everis Brasil (an NTT DATA Company)](https://medium.com/@everisBrasil)\n- 2020-08-19 - [ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã‚’é€£æºã—ã¦ã¿ã‚ˆã†](https://thinkit.co.jp/article/17704) by [å²¡äº• è£•çŸ¢(ãŠã‹ã„ ã‚†ã†ã‚„)](https://thinkit.co.jp/author/17588), [æ³‰ å‹(ã„ãšã¿ ã¾ã•ã‚‹)](https://thinkit.co.jp/author/17705) at [Think ITï¼ˆã‚·ãƒ³ã‚¯ã‚¤ãƒƒãƒˆï¼‰](https://thinkit.co.jp/)\n- 2020-08-25 - [OpenAPI Generator ã¨ TypeScript ã§å‹å®‰å…¨ã«ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰é–‹ç™ºã‚’ã—ã¦ã„ã‚‹è©±](https://tech.smarthr.jp/entry/2020/08/25/135631) at [SmartHR Tech Blog](https://tech.smarthr.jp/)\n- 2020-09-10 - [Introduction to OpenAPI with Instana](https://www.instana.com/blog/introduction-to-openapi-with-instana/) by [Cedric Ziel](https://www.instana.com/blog/author/cedricziel/) at [Instana Blog](https://www.instana.com/blog/)\n- 2020-09-17 - [Generate PowerShellSDK using openapi-generator](https://medium.com/@ghufz.learn/generate-powershellsdk-using-openapi-generator-33b700891e33) by [Ghufran Zahidi](https://medium.com/@ghufz.learn)\n- 2020-09-24 - [How to automate API code generation (OpenAPI/Swagger) and boost productivity - Tutorial with React Native featuring TypeScript](https://medium.com/@sceleski/how-to-automate-api-code-generation-openapi-swagger-and-boost-productivity-1176a0056d8a) by [Sanjin Celeski](https://medium.com/@sceleski)\n- 2020-09-25 - [Generate OpenAPI Angular Client](https://medium.com/@pguso/generate-openapi-angular-client-8c9288e8bbd4) by [Patric](https://medium.com/@pguso)\n- 2020-10-24 - [Working with Microsoft Identity - React Native Client](https://www.josephguadagno.net/2020/10/24/working-with-microsoft-identity-react-native-client) by [Joseph Guadagno](https://www.josephguadagno.net/)\n- 2020-10-31 - [[B2] OpenAPI Specificationìœ¼ë¡œ íƒ€ì…-ì„¸ì´í”„í•˜ê²Œ API ê°œë°œí•˜ê¸°: í¬ë§í¸ VS ì ˆë§í¸](https://www.youtube.com/watch?v=J4JHLESAiFk) by ìµœíƒœê±´ at [FEConf 2020](https://2020.feconf.kr/)\n- 2020-11-05 - [Automated REST-Api Code Generation: Wie IT-Systeme miteinander sprechen](https://www.massiveart.com/blog/automated-rest-api-code-generation-wie-it-systeme-miteinander-sprechen) by Stefan Rottensteiner at [MASSIVE ART Blog](https://www.massiveart.com/blog)\n- 2020-12-01 - [OpenAPI Generatorã§Goã®APIã‚µãƒ¼ãƒãƒ¼/ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚³ãƒ¼ãƒ‰ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹](https://qiita.com/saki-engineering/items/b20d8b6074c4da9664a5) by [@saki-engineering](https://qiita.com/saki-engineering)\n- 2020-12-04 - [Scaling the Test Coverage of OpenAPI Generator for 30+ Programming Languages](https://www.youtube.com/watch?v=7Lke9dHRqT0) by [William Cheng](https://github.com/wing328) at [Open Source Summit Japan + Automotive Linux Summit 2020](https://events.linuxfoundation.org/archive/2020/open-source-summit-japan/) ([Slides](https://speakerdeck.com/wing328/scaling-the-test-coverage-of-openapi-generator-for-30-plus-programming-languages))\n- 2020-12-09 - [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«OpenAPI Generatorã§è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸå‹ä»˜ãAPI Clientã‚’å°å…¥ã—ãŸè©±](https://qiita.com/yoshifujiT/items/905c18700ede23f40840) by [@yoshifujiT](https://github.com/yoshifujiT)\n- 2020-12-15 - [Next.js + NestJS + GraphQLã§å¤‰åŒ–ã«è¿½å¾“ã™ã‚‹ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã¸ ã€œ ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚¯ãƒ¼ãƒãƒ³ã®äº‹ä¾‹ç´¹ä»‹](https://techblog.yahoo.co.jp/entry/2020121530052952/) by [å°å€‰ é™¸](https://github.com/ogugu9) at [Yahoo! JAPAN Tech Blog](https://techblog.yahoo.co.jp/)\n- 2021-01-08 - [Hello, New API â€“ Part 1](https://www.nginx.com/blog/hello-new-api-part-1/) by [Jeremy Schulman](https://www.nginx.com/people/jeremy-schulman/) at [Major League Baseball](https://www.mlb.com)\n- 2021-01-18 - [ã€Œã‚¢ãƒ—ãƒªé–‹ç™ºã‚ã‚‹ã‚ã‚‹ã€ã‚’ç–‘ã†ã“ã¨ã‹ã‚‰å§‹ã¾ã£ãŸã€API Clientã‚³ãƒ¼ãƒ‰ã®è‡ªå‹•ç”Ÿæˆã€ãƒ‡ãƒ–ã‚¹ãƒˆ2020ã€‘](https://codezine.jp/article/detail/13406?p=2) by [CodeZineç·¨é›†éƒ¨](https://codezine.jp/author/1)\n- 2021-02-05 - [REST-API-Roundtrip with SpringDoc and OpenAPI Generator](https://blog.viadee.de/en/rest-api-roundtrip) by [Benjamin Klatt](https://twitter.com/benklatt) at [viadee](https://www.viadee.de/en/)\n- 2021-02-17 - [REST-API-Roundtrip with SpringDoc and OpenAPI Generator](https://medium.com/nerd-for-tech/rest-api-roundtrip-with-springdoc-and-openapi-generator-30bd27ccf698) by [cloud @viadee](https://cloud-viadee.medium.com/)\n- 2021-03-08 - [OpenAPI Generator å·¥å…·çš„èººå‘å°è¯•](https://blog.csdn.net/u013019701/article/details/114531975) by [ç‹¬å®¶é›¨å¤©](https://blog.csdn.net/u013019701) at [CSDNå®˜æ–¹åšå®¢](https://blog.csdn.net/)\n- 2021-03-16 - [å¦‚ä½•åŸºäº Swagger ä½¿ç”¨ OpenAPI Generator ç”Ÿæˆ JMeter è„šæœ¬ï¼Ÿ](https://cloud.tencent.com/developer/article/1802704) by [é«˜æ¥¼Zee](https://cloud.tencent.com/developer/user/5836255) at [è…¾è®¯äº‘ä¸“æ ](https://cloud.tencent.com/developer/column)\n- 2021-03-24 - [openapi-generator-cli ã«ã‚ˆã‚‹ TypeScript å‹å®šç¾©](https://zenn.dev/takepepe/articles/openapi-generator-cli-ts) by [Takefumi Yoshii](https://zenn.dev/takepepe)\n- 2021-03-28 - [Trying out NestJS part 4: Generate Typescript clients from OpenAPI documents](https://dev.to/arnaudcortisse/trying-out-nestjs-part-4-generate-typescript-clients-from-openapi-documents-28mk) by [Arnaud Cortisse](https://dev.to/arnaudcortisse)\n- 2021-03-31 - [Open API Server Implementation Using OpenAPI Generator](https://www.baeldung.com/java-openapi-generator-server) at [Baeldung](https://www.baeldung.com/)\n- 2021-03-31 - [ä½¿ç”¨OpenAPI Generatorå¯¦ç¾Open API Server](https://www.1ju.org/article/java-openapi-generator-server) at [å„„èšç¶²](https://www.1ju.org/)\n- 2021-04-19 - [Introducing Twilioâ€™s OpenAPI Specification Beta](https://www.twilio.com/blog/introducing-twilio-open-api-specification-beta) by [GARETH PAUL JONES](https://www.twilio.com/blog/author/gpj) at [Twilio Blog](https://www.twilio.com/blog)\n- 2021-04-22 - [Leveraging OpenApi strengths in a Micro-Service environment](https://medium.com/unibuddy-technology-blog/leveraging-openapi-strengths-in-a-micro-service-environment-3d7f9e7c26ff) by Nicolas Jellab at [Unibuddy Technology Blog](https://medium.com/unibuddy-technology-blog)\n- 2021-04-27 - [From zero to publishing PowerShell API clients in PowerShell Gallery within minutes](https://speakerdeck.com/wing328/from-zero-to-publishing-powershell-api-clients-in-powershell-gallery-within-minutes) by [William Cheng](https://github.com/wing328) at [PowerShell + DevOps Global Summit 2021](https://events.devopscollective.org/event/powershell-devops-global-summit-2021/)\n- 2021-05-31 - [Flutterã§Open Api Generator(Swagger)ã‚’ä½¿ã†](https://aakira.app/blog/2021/05/flutter-open-api/) by [AAkira](https://twitter.com/_a_akira)\n- 2021-06-22 - [Rest API Documentation and Client Generation With OpenAPI](https://dzone.com/articles/rest-api-documentation-and-client-generation-with) by [Prasanth Gullapalli](https://dzone.com/users/1011797/prasanthnath.g@gmail.com.html)\n- 2021-07-16 - [éŠ€è¡Œäº‹æ¥­ã®ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰é–‹ç™ºã«ã¤ã„ã¦ / LINE äº¬éƒ½é–‹ç™ºå®¤ ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢æ¡ç”¨èª¬æ˜ä¼š](https://www.youtube.com/watch?v=YrrKQHxLPpQ) by é‡ç”°èª äºº, Robert Mitchell\n- 2021-07-19 - [OpenAPI code generation with kotlin](https://sylhare.github.io/2021/07/19/Openapi-swagger-codegen-with-kotlin.html) by [sylhare](https://github.com/sylhare)\n- 2021-07-29 - [How To Rewrite a Huge Codebase](https://dzone.com/articles/how-to-rewrite-a-huge-code-base) by [Curtis Poe](https://dzone.com/users/4565446/publiusovidius.html)\n- 2021-08-21 - [Generating Client APIs using Swagger Part 1](https://medium.com/@flowsquad/generating-client-apis-using-swagger-part-1-2d46f13f5e92) by [FlowSquad.io](https://medium.com/@flowsquad)\n- 2021-09-11 - [Invoking AWS ParallelCluster API](https://docs.aws.amazon.com/parallelcluster/latest/ug/api-reference-v3.html) at [AWS ParallelCluster API official documentation](https://docs.aws.amazon.com/parallelcluster/latest/ug/api-reference-v3.html)\n- 2021-09-20 - [OpenAPI Generator - The Babel Fish of the API World](https://www.youtube.com/watch?v=s2zMtwd5klg) by [Cliffano Subagio (Principal Engineer at Shine Solutions)](https://github.com/cliffano) at [Apidays LIVE Australia 2021](https://www.apidays.global/australia2021/)\n- 2021-10-02 - [How to Write Fewer Lines of Code with the OpenAPI Generator](https://hackernoon.com/how-to-write-fewer-lines-of-code-with-the-openapi-generator) by [Mikhail Alfa](https://hackernoon.com/u/alphamikle)\n- 2021-10-12 - [OpenAPI Generator : 4000 Ã©toiles sur GitHub et des spaghettis](https://www.youtube.com/watch?v=9hEsNBSqTFk) by [JÃ©rÃ©mie Bresson](https://github.com/jmini) at [Devoxx FR 2021](https://cfp.devoxx.fr/2021/speaker/jeremie_bresson)\n- 2021-10-17 - [Generate a TypeScript HTTP Client From An OpenAPI Spec In DotNET 5](https://richardwillis.info/blog/generate-a-type-script-http-client-from-an-open-api-spec-in-dot-net-5) by [Richard Willis](https://github.com/badsyntax)\n- 2021-11-06 - [ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã®é–‹ç™ºã§æ„è­˜ã—ãŸã“ã¨](https://zenn.dev/woo_noo/articles/5cb09f8e2899ae782ad1) by [woo-noo](https://zenn.dev/woo_noo)\n- 2021-11-09 - [Effective Software Development using OpenAPI Generator](https://apexlabs.ai/post/effective-software-development-using-openapi-generator) by Ajil Oomme\n- 2021-12-07 - [An Introduction to OpenAPI](https://betterprogramming.pub/4-use-cases-of-openapi-which-are-good-to-know-1a041f4ad71e) by [Na'aman Hirschfeld](https://naamanhirschfeld.medium.com/)\n- 2022-01-02 - [Towards a secure API client generator for IoT devices](https://arxiv.org/abs/2201.00270) by Anders Aaen Springborg, Martin Kaldahl Andersen, Kaare Holland Hattel, Michele Albano\n- 2022-02-02 - [Use OpenApi generator to share your models between Flutter and your backend](https://www.youtube.com/watch?v=kPW7ccu9Yvk) by [Guillaume Bernos](https://feb2022.fluttervikings.com/speakers/guillaume_bernos) at [Flutter Vikings Conference 2022 (Hybrid)](https://feb2022.fluttervikings.com/)\n- 2022-03-15 - [OpenAPI Specã§ãƒã‚¤ãƒ•ãƒ³åŒºåˆ‡ã‚Šã®Enumå€¤ã‚’OpenAPI Generatorã§å‡ºåŠ›ã™ã‚‹ã¨ã€ãƒã‚¤ãƒ•ãƒ³åŒºåˆ‡ã‚Šã®ã¾ã¾å‡ºåŠ›ã•ã‚Œã‚‹](https://qiita.com/yuji38kwmt/items/824d74d4889055ab37d8) by [yuji38kwmt](https://qiita.com/yuji38kwmt)\n- 2022-04-01 - [OpenAPI Generatorã®ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã¨Spring Frameworkã®ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’å…±å­˜ã•ã›ã‚‹](https://techblog.zozo.com/entry/coexistence-of-openapi-and-spring) in [ZOZO Tech Blog](https://techblog.zozo.com/)\n- 2022-04-06 - [Effective Software Development using OpenAPI Generator](https://apexlabs.ai/post/openapi-generator) by Ajil Oommen (Senior Flutter Developer)\n- 2022-05-13 - [A Path From an API To Client Libraries](https://www.youtube.com/watch?v=XC8oVn_efTw) by [Filip Srnec](https://www.devoxx.co.uk/talk/?id=11211) at Infobip\n- 2022-06-01 - [API First, using OpenAPI and Spring Boot](https://medium.com/xgeeks/api-first-using-openapi-and-spring-boot-2602c04bb0d3) by [Micael EstrÃ¡zulas Vianna](https://estrazulas.medium.com/)\n- 2022-06-10 - [Autogenerating Clients with FastAPI and Github Actions](https://www.propelauth.com/post/autogenerating-clients-with-fastapi-and-github-actions) by [Andrew Israel](https://www.propelauth.com/author/andrew)\n- 2022-06-12 - [Mustache templates with OpenAPI specs](https://medium.com/geekculture/mustache-templates-with-openapi-specs-f24711c67dec) by [Beppe Catanese](https://github.com/gcatanese)\n- 2022-07-01 - [Generate API contract using OpenAPI Generator Maven plugin](https://huongdanjava.com/generate-api-contract-using-openapi-generator-maven-plugin.html) by [Khanh Nguyen](https://huongdanjava.com/)\n- 2022-07-22 - [ä½¿ç”¨OpenAPI Generator Maven pluginå¼€å‘apiä¼˜å…ˆçš„javaå®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯ä»£ç ](https://blog.roccoshi.top/2022/java/openapi-generator%E7%9A%84%E4%BD%BF%E7%94%A8/) by [Lincest](https://github.com/Lincest)\n- 2022-08-01 - [Tutorial: Etsy Open API v3 (ruby)](https://blog.tjoyal.dev/etsy-open-api-v3/) by [Thierry Joyal](https://github.com/tjoyal)\n- 2022-09-03 - [OpenAPI Generator For Go Web Development](https://blog.kevinhu.me/2022/09/03/03-openapi-generator/) by [Kevin Hu](https://twitter.com/Oldgunix)\n- 2022-10-01 - [OpenAPI Generatorã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ãŸã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆSwagger Codegenã¨ã»ã¼åŒã˜ï¼‰](https://nainaistar.hatenablog.com/entry/2022/10/03/120000) by [ãã‚Šä¸¸](https://twitter.com/nainaistar)\n- 2022-10-21 - [Kotlinï¼ˆSpring Bootï¼‰ã® API ã‚’ OpenAPI Generator ã§è‡ªå‹•ç”Ÿæˆ](https://zenn.dev/msksgm/articles/20221021-kotlin-spring-openapi-generator) by [msksgm](https://zenn.dev/msksgm)\n- 2022-10-26 - [Quarkus Insights #106: Quarkiverse Extension Spotlight: OpenApi Generator](https://www.youtube.com/watch?v=_s_if69t2iQ) by [Quarkusio](https://www.youtube.com/c/Quarkusio)\n- 2022-11-28 - [The REST API implementation flow](https://tmsvr.com/openapi-code-generation-for-rest-apis/) by [Imre TÃ¶mÃ¶svÃ¡ri](https://tmsvr.com/author/imre/)\n- 2022-12-13 - [API-First with Spring WebFlux and OpenAPI Generator](https://boottechnologies-ci.medium.com/api-first-with-spring-webflux-and-openapi-generator-38b7804c4ed4) by [Eric Anicet](https://boottechnologies-ci.medium.com/)\n- 2023-01-06 - [Major Improvements with Helidon and OpenAPI](https://medium.com/helidon/major-improvements-with-helidon-and-openapi-f76a0951508e) by [Tim Quinn](https://medium.com/@tquinno600)\n- 2023-02-02 - [Replacing Postman with the Jetbrains HTTP Client](https://lengrand.fr/replacing-postman-in-seconds-with-the-jetbrains-http-client/) by [julien Lengrand-Lambert](https://github.com/jlengrand)\n- 2023-03-15 - [OpenAPI Generatorã«é©ã—ãŸOpenAPIã®æ›¸ãæ–¹](https://techblog.zozo.com/entry/how-to-write-openapi-for-openapi-generator) by [ZOZO Tech Blog](https://techblog.zozo.com/)\n- 2023-03-19 - [EXOGEM: Extending OpenAPI Generator for Monitoring of RESTful APIs](https://link.springer.com/chapter/10.1007/978-3-031-26507-5_10) by Daniel Friis Holtebo, Jannik Lucas Sommer, Magnus MÃ¸lgaard Lund, Alessandro Tibo, Junior Dongo & Michele Albano at \"ICSOC 2022: Service-Oriented Computing â€“ ICSOC 2022 Workshops\"\n- 2023-03-28 - [API-First Design with OpenAPI Generator](https://www.linkedin.com/pulse/api-first-design-openapi-generator-jonathan-manera/) by [Jonathan Manera](https://www.linkedin.com/in/manerajona/)\n- 2023-03-28 - [ãƒãƒ³ã‚ºã‚ªãƒ³ã§å­¦ã¶ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰ Kotlinï¼ˆSpring Boot&Arrow&OpenAPI Generatorï¼‰v1.0.1](https://zenn.dev/msksgm/books/implementing-server-side-kotlin-development) by [msk](https://zenn.dev/msksgm)\n- 2023-04-01 - [OpenAPI Client Code Generation](https://testingboss.com/blog/openapi-client-generation/) by Kwo Ding\n- 2023-04-27 - [Create an Angular Client using OpenAPI Specifications](Create an Angular Client using OpenAPI Specifications) by [Patric](https://pguso.medium.com/)\n- 2023-05-16 - [Adyen for Java developers](https://www.adyen.com/blog/adyen-java-library) by [Beppe Catanese, Developer Advocate, Adyen](https://github.com/gcatanese)\n- 2023-05-18 - [å¦‚ä½•åŸºäº Swagger ä½¿ç”¨ OpenAPI Generator ç”Ÿæˆ JMeter è„šæœ¬ï¼Ÿ](https://blog.51cto.com/u_15181572/6294974) by [é«˜æ¥¼ï¼ˆZee)](https://blog.51cto.com/u_15181572)\n- 2023-06-28 - [Generate API contract using OpenAPI Generator Maven plugin](https://huongdanjava.com/generate-api-contract-using-openapi-generator-maven-plugin.html) by [Khanh Nguyen](https://huongdanjava.com/)\n- 2023-06-30 - [Generate Client SDKs with OpenApi Generator in Springboot](https://medium.com/@ramavathvinayak/generate-client-sdks-with-openapi-generator-in-springboot-f9f012e73c0b) by [Vinayak Ramavath](https://medium.com/@ramavathvinayak)\n- 2023-12-10 - [Unityã§OpenAPI Generatorã‚’ä½¿ã†](https://www.youtube.com/watch?v=CbNwKVV5LRM) by [Soup Tori](https://www.youtube.com/@souptori8417)\n- 2024-01-24 - [Comment gÃ©nÃ©rer des stubs wiremock avec openapi generator](https://www.youtube.com/watch?v=0jhONfBrcKw) by [Alexis Couvreur](https://github.com/acouvreur)\n- 2024-03-04 - [Generating TypeScript Types with OpenAPI for REST API Consumption](https://www.pullrequest.com/blog/generating-typescript-types-with-openapi-for-rest-api-consumption/) by [PullRequest](https://www.pullrequest.com/)\n- 2024-03-07 - [Fully typed Web Apps with OpenAPI (Part 1)](https://medium.com/@gfox1984/fully-typed-web-apps-with-openapi-part-1-595d55766670) by [Guillaume Renard](https://medium.com/@gfox1984)\n- 2024-03-08 - [Laravel OpenAPIã«ã‚ˆã‚‹ \"è¾›ããªã„\" ã‚¹ã‚­ãƒ¼ãƒé§†å‹•é–‹ç™º](https://fortee.jp/phperkaigi-2024/proposal/9e2e6c38-d078-4efa-99b4-83ebf9033b34) by [KentarouTakeda](https://twitter.com/KentarouTakeda)\n- 2024-04-04 - [Working with OpenAPI using Rust](https://www.shuttle.dev/blog/2024/04/04/using-openapi-rust) by [Joshua Mo](https://twitter.com/joshmo_dev)\n- 2024-04-08 - [Implement API first strategy with OpenAPI generator plugin](https://medium.com/javarevisited/implement-api-first-strategy-with-openapi-generator-plugin-e4bbe7f0d778) by [Rui Zhou](https://medium.com/@wirelesser)\n- 2024-05-06 - [OpenAPI Generator Custom Templates](https://www.javacodegeeks.com/openapi-generator-custom-templates.html) by [Mary Zheng](https://www.javacodegeeks.com/author/mary-zheng)\n- 2025-02-09 - [Custom validation with OpenApiGenerator and Spring Boot 3](https://medium.com/@jugurtha.aitoufella/custom-validation-with-openapigenerator-and-spring-boot-3-34a656e815c8) by [Jugurtha Aitoufella](https://medium.com/@jugurtha.aitoufella)\n- 2025-02-20 - [Optimizing API Integration in a Large React Application Using OpenAPI Generator](https://www.youtube.com/watch?v=-B33pQnGQUI) by Stefano Marzo\n\n\n## [6 - About Us](#table-of-contents)\n\nWhat's the design philosophy or principle behind OpenAPI Generator?\n\nWe focus on developer experience. The generators should produce code, config, documentation, and more that are easily understandable and consumable by users. We focused on simple use cases to start with (bottom-up approach). Since then the project and the community have grown a lot: 600k weekly downloads via NPM CLI wrapper, 30M downloads via openapi-generator-cli docker image just to highlight a few. We've gradually supported more features (e.g. oneOf, anyOf introduced in OpenAPI 3.0) in various generators and we will continue this approach to deliver something based on our understanding of user demand and what they want, and continue to add support of new features introduced in OpenAPI specification (such as v3.1 and future versions of the OpenAPI specification).\n\n### [6.1 - OpenAPI Generator Core Team](#table-of-contents)\n\nOpenAPI Generator core team members are contributors who have been making significant contributions (review issues, fix bugs, make enhancements, etc) to the project on a regular basis.\n\n#### Core Team Members\n* [@wing328](https://github.com/wing328) (2015/07) [:heart:](https://www.patreon.com/wing328)\n* [@jimschubert](https://github.com/jimschubert) (2016/05) [:heart:](https://www.patreon.com/jimschubert)\n* [@cbornet](https://github.com/cbornet) (2016/05)\n* [@jmini](https://github.com/jmini) (2018/04)  [:heart:](https://www.patreon.com/jmini)\n* [@etherealjoy](https://github.com/etherealjoy) (2019/06)\n\n:heart: = Link to support the contributor directly\n\n#### Template Creator\n\n**NOTE**: Embedded templates are only supported in _Mustache_ format. Support for all other formats is experimental and subject to change at any time.\n\nHere is a list of template creators:\n * API Clients:\n   * Ada: @stcarrez\n   * Apex: @asnelling\n   * Bash: @bkryza\n   * C: @PowerOfCreation @zhemant [:heart:](https://www.patreon.com/zhemant)\n   * C++ Oat++: @Kraust\n   * C++ REST: @Danielku15\n   * C++ Tiny: @AndersSpringborg @kaareHH @michelealbano @mkakbas\n   * C++ UE4: @Kahncode\n   * C# (.NET 2.0): @who\n   * C# (.NET Standard 1.3 ): @Gronsak\n   * C# (.NET 4.5 refactored): @jimschubert [:heart:](https://www.patreon.com/jimschubert)\n   * C# (GenericHost): @devhl-labs\n   * C# (HttpClient): @Blackclaws\n   * Clojure: @xhh\n   * Crystal: @wing328\n   * Dart: @yissachar\n   * Dart (refactor): @joernahrens\n   * Dart 2: @swipesight\n   * Dart (Jaguar): @jaumard\n   * Dart (Dio): @josh-burton\n   * Elixir: @niku\n   * Elm: @eriktim\n   * Eiffel: @jvelilla\n   * Erlang: @tsloughter\n   * Erlang (PropEr): @jfacorro @robertoaloi\n   * Groovy: @victorgit\n   * Go: @wing328 [:heart:](https://www.patreon.com/wing328)\n   * Go (rewritten in 2.3.0): @antihax\n   * Godot (GDScript): @Goutte [:heart:](https://liberapay.com/Goutte)\n   * Haskell (http-client): @jonschoning\n   * Java (Feign): @davidkiss\n   * Java (Retrofit): @0legg\n   * Java (Retrofit2): @emilianobonassi\n   * Java (Jersey2): @xhh\n   * Java (okhttp-gson): @xhh\n   * Java (RestTemplate): @nbruno\n   * Java (Spring 5 WebClient): @daonomic\n   * Java (Spring 6 RestClient): @nicklas2751\n   * Java (RESTEasy): @gayathrigs\n   * Java (Vertx): @lopesmcc\n   * Java (Google APIs Client Library): @charlescapps\n   * Java (Rest-assured): @viclovsky\n   * Java (Java 11 Native HTTP client): @bbdouglas\n   * Java (Apache HttpClient 5.x): @harrywhite4 @andrevegas\n   * Java (Helidon): @spericas @tjquinno @tvallin\n   * Javascript/NodeJS: @jfiala\n   * JavaScript (Apollo DataSource): @erithmetic\n   * JavaScript (Closure-annotated Angular) @achew22\n   * JavaScript (Flow types) @jaypea\n   * Jetbrains HTTP Client : @jlengrand\n   * JMeter: @davidkiss\n   * Julia: @tanmaykm\n   * Kotlin: @jimschubert [:heart:](https://www.patreon.com/jimschubert)\n   * Kotlin (MultiPlatform): @andrewemery\n   * Kotlin (Volley): @alisters\n   * Kotlin (jvm-spring-webclient): @stefankoppier\n   * Kotlin (jvm-spring-restclient): @stefankoppier\n   * Lua: @daurnimator\n   * N4JS: @mmews-n4\n   * Nim: @hokamoto\n   * OCaml: @cgensoul\n   * Perl: @wing328 [:heart:](https://www.patreon.com/wing328)\n   * PHP (Guzzle): @baartosz\n   * PHP (with Data Transfer): @Articus\n   * PowerShell: @beatcracker\n   * PowerShell (refactored in 5.0.0): @wing328\n   * Python: @spacether [:heart:][spacether sponsorship]\n   * Python-Experimental: @spacether [:heart:][spacether sponsorship]\n   * Python (refactored in 7.0.0): @wing328\n   * R: @ramnov\n   * Ruby (Faraday): @meganemura @dkliban\n   * Ruby (HTTPX): @honeyryderchuck\n   * Rust: @farcaller\n   * Rust (rust-server): @metaswitch\n   * Scala (scalaz & http4s): @tbrown1979\n   * Scala (Akka): @cchafer\n   * Scala (sttp): @chameleon82\n   * Scala (sttp4): @flsh86\n   * Scala (scala-sttp4-jsoniter): @lbialy\n   * Scala (Pekko): @mickaelmagniez\n   * Scala (http4s): @JennyLeahy\n   * Swift: @tkqubo\n   * Swift 3: @hexelon\n   * Swift 4: @ehyche\n   * Swift 5: @4brunu\n   * Swift 6: @4brunu\n   * Swift Combine: @dydus0x14\n   * TypeScript (Angular1): @mhardorf\n   * TypeScript (Angular2): @roni-frantchi\n   * TypeScript (Angular6): @akehir\n   * TypeScript (Angular7): @topce\n   * TypeScript (Axios): @nicokoenig\n   * TypeScript (Fetch): @leonyu\n   * TypeScript (Inversify): @gualtierim\n   * TypeScript (jQuery): @bherila\n   * TypeScript (Nestjs): @vfrank66\n   * TypeScript (Node):  @mhardorf\n   * TypeScript (Rxjs): @denyo\n   * TypeScript (redux-query): @petejohansonxo\n   * Xojo: @Topheee\n   * Zapier: @valmoz, @emajo\n * Server Stubs\n   * Ada: @stcarrez\n   * C# ASP.NET 5: @jimschubert [:heart:](https://www.patreon.com/jimschubert)\n   * C# ASP.NET Core 3.0: @A-Joshi\n   * C# APS.NET Core 3.1: @phatcher\n   * C# Azure functions: @Abrhm7786\n   * C# NancyFX: @mstefaniuk\n   * C++ (Qt5 QHttpEngine): @etherealjoy\n   * C++ Oat++: @Kraust\n   * C++ Pistache: @sebymiano\n   * C++ Restbed: @stkrwork\n   * Erlang Server: @galaxie @nelsonvides\n   * F# (Giraffe) Server: @nmfisher\n   * Go Server: @guohuang\n   * Go Server (refactored in 7.0.0): @lwj5\n   * Go (Echo) Server: @ph4r5h4d\n   * Go (Gin) Server: @kemokemo\n   * GraphQL Express Server: @renepardon\n   * Haskell Servant: @algas\n   * Haskell Yesod: @yotsuya\n   * Java Camel: @carnevalegiacomo\n   * Java Dubbo: @redoom\n   * Java MSF4J: @sanjeewa-malalgoda\n   * Java Spring Boot: @diyfr\n   * Java Undertow: @stevehu\n   * Java Play Framework: @JFCote\n   * Java PKMST: @anshu2185 @sanshuman @rkumar-pk @ninodpillai\n   * Java Vert.x: @lwlee2608\n   * Java Micronaut: @andriy-dmytruk\n   * Java Helidon: @spericas @tjquinno @tvallin\n   * Java WireMock: [@acouvreur](https://github.com/acouvreur)\n   * JAX-RS RestEasy: @chameleon82\n   * JAX-RS CXF: @hiveship\n   * JAX-RS CXF (CDI): @nickcmaynard\n   * JAX-RS RestEasy (JBoss EAP): @jfiala\n   * Julia: @tanmaykm\n   * Kotlin: @jimschubert [:heart:](https://www.patreon.com/jimschubert)\n   * Kotlin (Spring Boot): @dr4ke616\n   * Kotlin (Vertx): @Wooyme\n   * Kotlin (JAX-RS): @anttileppa\n   * Kotlin Misk: @andrewwilsonnew @guiarn\n   * Kotlin WireMock: @stefankoppier\n   * NodeJS Express: @YishTish\n   * PHP Flight: @daniel-sc\n   * PHP Laravel: @renepardon\n   * PHP Laravel (refactor in 7.12.0): @gijs-blanken\n   * PHP Lumen: @abcsun\n   * PHP Mezzio (with Path Handler): @Articus\n   * PHP Slim: @jfastnacht\n   * PHP Slim4: [@ybelenko](https://github.com/ybelenko)\n   * PHP Symfony: @ksm2\n   * PHP Symfony6: @BenjaminHae\n   * Python FastAPI: @krjakbrjak\n   * Python AIOHTTP:\n   * Ruby on Rails 5: @zlx\n   * Rust (rust-server): @metaswitch\n   * Rust (rust-axum): @linxGnu\n   * Scala Akka: @Bouillie\n   * Scala Cask: @aaronp\n   * Scala Finch: @jimschubert [:heart:](https://www.patreon.com/jimschubert)\n   * Scala Lagom: @gmkumar2005\n   * Scala Play: @adigerber\n   * TypeScript NestJS: @aryobenholzner\n * Documentation\n   * AsciiDoc: @man-at-home\n   * HTML Doc 2: @jhitchcock\n   * Confluence Wiki: @jhitchcock\n   * PlantUML: @pburls\n * Configuration\n   * Apache2: @stkrwork\n   * k6: @mostafa\n * Schema\n   * Avro: @sgadouar\n   * GraphQL: @wing328 [:heart:](https://www.patreon.com/wing328)\n   * Ktorm: @Luiz-Monad\n   * MySQL: [@ybelenko](https://github.com/ybelenko)\n   * PostgreSQL: [@iri](https://github.com/iri)\n   * Postman Collection: @gcatanese\n   * Protocol Buffer: @wing328\n   * WSDL: @adessoDpd\n\n:heart: = Link to support the contributor directly\n\n#### How to join the core team\n\nHere are the requirements to become a core team member:\n- rank within top 50 in https://github.com/openapitools/openapi-generator/graphs/contributors\n  - to contribute, here are some good [starting points](https://github.com/openapitools/openapi-generator/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)\n- regular contributions to the project\n  - about 3 hours per week\n  - for contribution, it can be addressing issues, reviewing PRs submitted by others, submitting PR to fix bugs or make enhancements, etc\n  - must be active in the past 3 months at the time of application\n\n To join the core team, please reach out to team@openapitools.org for more information.\n\n To become a Template Creator, simply submit a PR for new API client (e.g. Rust, Elixir) or server stub (e.g. Ruby Grape) generator.\n\n### [6.2 - OpenAPI Generator Technical Committee](#table-of-contents)\n\nMembers of the OpenAPI Generator technical committee shoulder the following responsibilities:\n\n- Provides guidance and direction to other users\n- Reviews pull requests and issues\n- Improves the generator by making enhancements, fixing bugs or updating documentations\n- Sets the technical direction of the generator\n\nWho is eligible? Those who want to join must have at least 3 PRs merged into a generator. (Exceptions can be granted to template creators or contributors who have made a lot of code changes with less than 3 merged PRs)\n\nIf you want to join the committee, please kindly apply by sending an email to team@openapitools.org with your Github ID.\n\n#### Members of Technical Committee\n\n| Languages/Generators  | Member (join date)                                                                                                                                                                                                                                    |\n|:----------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ActionScript          |                                                                                                                                                                                                                                                       |\n| Ada                   | @stcarrez (2018/02) @michelealbano (2018/02)                                                                                                                                                                                                          |\n| Android               | @jaz-ah (2017/09)                                                                                                                                                                                                                                     |\n| Apex                  |                                                                                                                                                                                                                                                       |\n| Bash                  | @frol (2017/07) @bkryza (2017/08) @kenjones-cisco (2017/09)                                                                                                                                                                                           |\n| C                     | @zhemant (2018/11) @ityuhui (2019/12) @michelealbano (2020/03) @eafer (2024/12)                                                                                                                                                                                        |\n| C++                   | @ravinikam (2017/07) @stkrwork (2017/07) @etherealjoy (2018/02) @martindelille (2018/03) @muttleyxd (2019/08) @aminya (2025/05)                                                                                                                                         |\n| C#                    | @mandrean (2017/08) @shibayan (2020/02) @Blackclaws (2021/03) @lucamazzanti (2021/05) @iBicha (2023/07)                                                                                                                                          |\n| Clojure               |                                                                                                                                                                                                                                                       |\n| Crystal               | @cyangle (2021/01)                                                                                                                                                                                                                                    |\n| Dart                  | @jaumard (2018/09) @josh-burton (2019/12) @amondnet (2019/12) @sbu-WBT (2020/12) @kuhnroyal (2020/12) @agilob (2020/12) @ahmednfwela (2021/08)                                                                                                        |\n| Eiffel                | @jvelilla (2017/09)                                                                                                                                                                                                                                   |\n| Elixir                | @mrmstn (2018/12)                                                                                                                                                                                                                                     |\n| Elm                   | @eriktim (2018/09)                                                                                                                                                                                                                                    |\n| Erlang                | @tsloughter (2017/11) @jfacorro (2018/10) @robertoaloi (2018/10) @nelsonvides (2024/09)                                                                                                                                                               |\n| F#                    | @nmfisher (2019/05)                                                                                                                                                                                                                                   |\n| Go                    | @antihax (2017/11) @grokify (2018/07) @kemokemo (2018/09) @jirikuncar (2021/01) @ph4r5h4d (2021/04) @lwj5 (2023/04)                                                                                                                                                   |\n| GraphQL               | @renepardon (2018/12)                                                                                                                                                                                                                                 |\n| Groovy                |                                                                                                                                                                                                                                                       |\n| Haskell               |                                                                                                                                                                                                                                                       |\n| Java                  | @bbdouglas (2017/07) @sreeshas (2017/08) @jfiala (2017/08) @lukoyanov (2017/09) @cbornet (2017/09) @jeff9finger (2018/01) @karismann (2019/03) @Zomzog (2019/04) @lwlee2608 (2019/10) @martin-mfg (2023/08)                                                                 |\n| Java Spring           | @cachescrubber (2022/02) @welshm (2022/02) @MelleD (2022/02) @atextor (2022/02) @manedev79 (2022/02) @javisst (2022/02) @borsch (2022/02) @banlevente (2022/02) @Zomzog (2022/09) @martin-mfg (2023/08)                                                                     |\n| JMeter                | @kannkyo (2021/01)                                                                                                                                                                                                                                    |\n| Jetbrains HTTP Client | @jlengrand (2023/01)                                                                                                                                                                                                                                  |\n| Julia                 | @tanmaykm (2023/01)                                                                                                                                                                                                                                   |\n| Kotlin                | @karismann (2019/03) @Zomzog (2019/04) @andrewemery (2019/10) @4brunu (2019/11) @yutaka0m (2020/03) @stefankoppier (2022/06) @e5l (2024/10)                                          |\n| Lua                   | @daurnimator (2017/08)                                                                                                                                                                                                                                |\n| N4JS                  | @mmews-n4 (2023/03)                                                                                                                                                                                      |\n| Nim                   |                                                                                                                                                                                                                                                       |\n| NodeJS/Javascript     | @CodeNinjai (2017/07) @frol (2017/07) @cliffano (2017/07)                                                                                                                                                                                             |\n| ObjC                  |                                                                                                                                                                                                                                                       |\n| OCaml                 | @cgensoul (2019/08), @sir4ur0n (2025/08)                                                                                                                                                                                                              |\n| Perl                  | @wing328 (2017/07) [:heart:](https://www.patreon.com/wing328) @yue9944882 (2019/06)                                                                                                                                                                   |\n| PHP                   | @jebentier (2017/07), @dkarlovi (2017/07), @mandrean (2017/08), @jfastnacht (2017/09), [@ybelenko](https://github.com/ybelenko) (2018/07), @renepardon (2018/12)                                                                                      |\n| PowerShell            | @wing328 (2020/05)                                                                                                                                                                                                                                    |\n| Python                | @cbornet (2017/09) @tomplus (2018/10) @krjakbrjak (2023/02) @fa0311 (2023/10) @multani (2023/10) |\n| R                     | @Ramanth (2019/07) @saigiridhar21 (2019/07)                                                                                                                                                                                                           |\n| Ruby                  | @cliffano (2017/07) @zlx (2017/09) @autopp (2019/02)                                                                                                                                                                                                  |\n| Rust                  | @frol (2017/07) @farcaller (2017/08) @richardwhiuk (2019/07) @paladinzh (2020/05) @jacob-pro (2022/10) @dsteeley (2025/07)                                                                                                                                               |\n| Scala                 | @clasnake (2017/07), @shijinkui  (2018/01), @ramzimaalej (2018/03), @chameleon82 (2020/03), @Bouillie (2020/04) @fish86 (2023/06)                                                               |\n| Swift                 | @jgavris (2017/07) @ehyche (2017/08) @Edubits (2017/09) @jaz-ah (2017/09) @4brunu (2019/11) @dydus0x14 (2023/06)                                                                                                                                                           |\n| TypeScript            | @TiFu (2017/07) @taxpon (2017/07) @sebastianhaas (2017/07) @kenisteward (2017/07) @Vrolijkx (2017/09) @macjohnny (2018/01) @topce (2018/10) @akehir (2019/07) @petejohansonxo (2019/11) @amakhrov (2020/02) @davidgamero (2022/03) @mkusaka (2022/04) @joscha (2024/10)    |\n| Xojo                  | @Topheee (2023/04)                                                                                                                                                                                                                                    |\n\n\nPast Members of Technical Committee:\n| Languages/Generators         | Member (join date)                                                                                                                                                                                                                |\n| :---------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Python            | @taxpon (2017/07) @frol (2017/07) @mbohlool (2017/07) @cbornet (2017/09) @kenjones-cisco (2017/11) @tomplus (2018/10) @arun-nalla (2019/11)  |\n\n\n:heart: = Link to support the contributor directly\n\n### [6.3 - History of OpenAPI Generator](#table-of-contents)\n\nOpenAPI Generator is a fork of [Swagger Codegen](https://github.com/swagger-api/swagger-codegen). In view of the issues with the Swagger Codegen 3.0.0 (beta) release and the disagreement on the project's direction, more than 40 top contributors and template creators of Swagger Codegen decided to fork Swagger Codegen and maintain a community-driven version called \"OpenAPI Generator\". Please refer to the [Q&A](docs/qna.md) for more information.\n\n#### Founding Members (alphabetical order):\n\n- [Akihito Nakano](https://github.com/ackintosh)\n- [Artem Ocheredko](https://github.com/galaxie)\n- [Arthur Mogliev](https://github.com/Articus)\n- [Bartek Kryza](https://github.com/bkryza)\n- [Ben Wells](https://github.com/bvwells)\n- [Benjamin Gill](https://github.com/bjgill)\n- [Christophe Bornet](https://github.com/cbornet)\n- [Cliffano Subagio](https://github.com/cliffano)\n- [Daiki Matsudate](https://github.com/d-date)\n- [Daniel](https://github.com/Danielku15)\n- [Emiliano Bonassi](https://github.com/emilianobonassi)\n- [Erik Timmers](https://github.com/eriktim)\n- [Esteban Gehring](https://github.com/macjohnny)\n- [Gustavo Paz](https://github.com/gustavoapaz)\n- [Javier Velilla](https://github.com/jvelilla)\n- [Jean-FranÃ§ois CÃ´tÃ©](https://github.com/JFCote)\n- [Jim Schubert](https://github.com/jimschubert)\n- [Jon Schoning](https://github.com/jonschoning)\n- [JÃ©rÃ©mie Bresson](https://github.com/jmini) [:heart:](https://www.patreon.com/jmini)\n- [JÃ¶rn Ahrens](https://github.com/jayearn)\n- [Keni Steward](https://github.com/kenisteward)\n- [Marcin Stefaniuk](https://github.com/mstefaniuk)\n- [Martin Delille](https://github.com/MartinDelille)\n- [Masahiro Yamauchi](https://github.com/algas)\n- [Michele Albano](https://github.com/michelealbano)\n- [Ramzi Maalej](https://github.com/ramzimaalej)\n- [Ravindra Nikam](https://github.com/ravinikam)\n- [Ricardo Cardona](https://github.com/ricardona)\n- [Sebastian Haas](https://github.com/sebastianhaas)\n- [Sebastian Mandrean](https://github.com/mandrean)\n- [Sreenidhi Sreesha](https://github.com/sreeshas)\n- [Stefan Krismann](https://github.com/stkrwork)\n- [Stephane Carrez](https://github.com/stcarrez)\n- [Takuro Wada](https://github.com/taxpon)\n- [Tomasz Prus](https://github.com/tomplus)\n- [Tristan Sloughter](https://github.com/tsloughter)\n- [Victor Orlovsky](https://github.com/viclovsky)\n- [Victor Trakhtenberg](https://github.com/victorgit)\n- [Vlad Frolov](https://github.com/frol)\n- [Vladimir Pouzanov](https://github.com/farcaller)\n- [William Cheng](https://github.com/wing328)\n- [Xin Meng](https://github.com/xmeng1) [:heart:](https://www.patreon.com/user/overview?u=16435385)\n- [Xu Hui Hui](https://github.com/xhh)\n- [antihax](https://github.com/antihax)\n- [beatcracker](https://github.com/beatcracker)\n- [daurnimator](https:/github.com/daurnimator)\n- [etherealjoy](https://github.com/etherealjoy)\n- [jfiala](https://github.com/jfiala)\n- [lukoyanov](https://github.com/lukoyanov)\n\n:heart: = Link to support the contributor directly\n\n## [7 - License](#table-of-contents)\n-------\n\nCopyright 2018 OpenAPI-Generator Contributors (https://openapi-generator.tech)\nCopyright 2018 SmartBear Software\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at [apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n---\n",
      "stars_today": 12
    },
    {
      "id": 55626935,
      "name": "WSL",
      "full_name": "microsoft/WSL",
      "description": "Windows Subsystem for Linux",
      "html_url": "https://github.com/microsoft/WSL",
      "stars": 30962,
      "forks": 1592,
      "language": "C++",
      "topics": [],
      "created_at": "2016-04-06T17:32:56Z",
      "updated_at": "2026-01-28T00:00:51Z",
      "pushed_at": "2026-01-28T00:59:09Z",
      "open_issues": 949,
      "owner": {
        "login": "microsoft",
        "avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4"
      },
      "readme": "# Welcome to the Windows Subsystem for Linux (WSL) repository\n\n<p align=\"center\">\n  <img src=\"./Images/Square44x44Logo.targetsize-256.png\" alt=\"WSL logo\"/>\n</p>\n\n[Learn more about WSL](https://aka.ms/wsldocs) | [Downloads & Release notes](https://github.com/microsoft/WSL/releases) | [Contributing to WSL](./CONTRIBUTING.md)\n\n## About\n\nWindows Subsystem for Linux (WSL) is a powerful way for you to run your Linux command-line tools, utilities and applications, all unmodified and directly on Windows without the overhead of a traditional virtual machine or dual boot setup.\n\nYou can install WSL right away by running this command inside of your Windows command line:\n\n```powershell\nwsl --install\n```\n\nYou can learn more about [best practices for setup](https://learn.microsoft.com/windows/wsl/setup/environment), [overviews of WSL](https://learn.microsoft.com/windows/wsl/about) and more at our [WSL documentation page](https://learn.microsoft.com/windows/wsl/).\n\n## Related repositories\n\nWSL also has related open source repositories:\n\n- [microsoft/WSL2-Linux-Kernel](https://github.com/microsoft/WSL2-Linux-Kernel) - The Linux kernel shipped with WSL\n- [microsoft/WSLg](https://github.com/microsoft/wslg) - Support for Linux GUI apps in WSL\n- [microsoftdocs/wsl](https://github.com/microsoftdocs/wsl) - WSL documentation at aka.ms/wsldocs\n\n## Contributing\n\nThis project welcomes contributions of all types, including coding features / bug fixes, documentation fixes, design proposals and more. \n\nWe ask that before you start working on a contribution, please read our [Contributor's Guide](./CONTRIBUTING.md).\n\nFor guidance on developing for WSL, please read the [developer docs](./doc/docs/dev-loop.md) for instructions on how to build WSL from source and details on its architecture.\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](./CODE_OF_CONDUCT.md)\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoftâ€™s Trademark & Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-partyâ€™s policies.\n\n## Privacy and telemetry\n\nThe application logs basic diagnostic data (telemetry). For more information on privacy and what we collect, see our [data and privacy documentation](DATA_AND_PRIVACY.md).\n\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoftâ€™s privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.",
      "stars_today": 12
    },
    {
      "id": 272457606,
      "name": "cosmopolitan",
      "full_name": "jart/cosmopolitan",
      "description": "build-once run-anywhere c library",
      "html_url": "https://github.com/jart/cosmopolitan",
      "stars": 20482,
      "forks": 734,
      "language": "C",
      "topics": [
        "bios",
        "containers",
        "darwin",
        "efi",
        "freebsd",
        "libc",
        "linux",
        "netbsd",
        "openbsd",
        "polyglot",
        "windows",
        "zip"
      ],
      "created_at": "2020-06-15T14:16:13Z",
      "updated_at": "2026-01-27T18:39:20Z",
      "pushed_at": "2026-01-25T02:37:00Z",
      "open_issues": 193,
      "owner": {
        "login": "jart",
        "avatar_url": "https://avatars.githubusercontent.com/u/49262?v=4"
      },
      "readme": "![Cosmopolitan Honeybadger](usr/share/img/honeybadger.png)\n\n[![build](https://github.com/jart/cosmopolitan/actions/workflows/build.yml/badge.svg)](https://github.com/jart/cosmopolitan/actions/workflows/build.yml)\n# Cosmopolitan\n\n[Cosmopolitan Libc](https://justine.lol/cosmopolitan/index.html) makes C/C++\na build-once run-anywhere language, like Java, except it doesn't need an\ninterpreter or virtual machine. Instead, it reconfigures stock GCC and\nClang to output a POSIX-approved polyglot format that runs natively on\nLinux + Mac + Windows + FreeBSD + OpenBSD 7.3 + NetBSD + BIOS with the\nbest possible performance and the tiniest footprint imaginable.\n\n## Background\n\nFor an introduction to this project, please read the [actually portable\nexecutable](https://justine.lol/ape.html) blog post and [cosmopolitan\nlibc](https://justine.lol/cosmopolitan/index.html) website. We also have\n[API\ndocumentation](https://justine.lol/cosmopolitan/documentation.html).\n\n## Getting Started\n\nYou can start by obtaining a release of our `cosmocc` compiler from\n<https://cosmo.zip/pub/cosmocc/>.\n\n```sh\nmkdir -p cosmocc\ncd cosmocc\nwget https://cosmo.zip/pub/cosmocc/cosmocc.zip\nunzip cosmocc.zip\n```\n\nHere's an example program we can write:\n\n```c\n// hello.c\n#include <stdio.h>\n\nint main() {\n  printf(\"hello world\\n\");\n}\n```\n\nIt can be compiled as follows:\n\n```sh\ncosmocc -o hello hello.c\n./hello\n```\n\nThe Cosmopolitan Libc runtime links some heavyweight troubleshooting\nfeatures by default, which are very useful for developers and admins.\nHere's how you can log system calls:\n\n```sh\n./hello --strace\n```\n\nHere's how you can get a much more verbose log of function calls:\n\n```sh\n./hello --ftrace\n```\n\nYou can use the Cosmopolitan's toolchain to build conventional open\nsource projects which use autotools. This strategy normally works:\n\n```sh\nexport CC=x86_64-unknown-cosmo-cc\nexport CXX=x86_64-unknown-cosmo-c++\n./configure --prefix=/opt/cosmos/x86_64\nmake -j\nmake install\n```\n\n## Cosmopolitan Source Builds\n\nCosmopolitan can be compiled from source on any of our supported\nplatforms. The Makefile will download cosmocc automatically.\n\nIt's recommended that you install a systemwide APE Loader. This command\nrequires `sudo` access to copy the `ape` command to a system folder and\nregister with binfmt_misc on Linux, for even more performance.\n\n```sh\nape/apeinstall.sh\n```\n\nYou can now build the mono repo with any modern version of GNU Make. To\nbootstrap your build, you can install Cosmopolitan Make from this site:\n\nhttps://cosmo.zip/pub/cosmos/bin/make\n\nE.g.:\n\n```sh\ncurl -LO https://cosmo.zip/pub/cosmos/bin/make\n./make -j8\no//examples/hello\n```\n\nAfter you've built the repo once, you can also use the make from your\ncosmocc at `.cosmocc/current/bin/make`. You might even prefer to alias\nmake to `$COSMO/.cosmocc/current/bin/make`.\n\nSince the Cosmopolitan repository is very large, you might only want to\nbuild one particular thing. Here's an example of a target that can be\ncompiled relatively quickly, which is a simple POSIX test that only\ndepends on core LIBC packages.\n\n```sh\nrm -rf o//libc o//test\n.cosmocc/current/bin/make o//test/posix/signal_test\no//test/posix/signal_test\n```\n\nSometimes it's desirable to build a subset of targets, without having to\nlist out each individual one. For example if you wanted to build and run\nall the unit tests in the `TEST_POSIX` package, you could say:\n\n```sh\n.cosmocc/current/bin/make o//test/posix\n```\n\nCosmopolitan provides a variety of build modes. For example, if you want\nreally tiny binaries (as small as 12kb in size) then you'd say:\n\n```sh\n.cosmocc/current/bin/make m=tiny\n```\n\nYou can furthermore cut out the bloat of other operating systems, and\nhave Cosmopolitan become much more similar to Musl Libc.\n\n```sh\n.cosmocc/current/bin/make m=tinylinux\n```\n\nFor further details, see [//build/config.mk](build/config.mk).\n\n## Debugging\n\nTo print a log of system calls to stderr:\n\n```sh\ncosmocc -o hello hello.c\n./hello --strace\n```\n\nTo print a log of function calls to stderr:\n\n```sh\ncosmocc -o hello hello.c\n./hello --ftrace\n```\n\nBoth strace and ftrace use the unbreakable kprintf() facility, which is\nable to be sent to a file by setting an environment variable.\n\n```sh\nexport KPRINTF_LOG=log\n./hello --strace\n```\n\n## GDB\n\nHere's the recommended `~/.gdbinit` config:\n\n```gdb\nset host-charset UTF-8\nset target-charset UTF-8\nset target-wide-charset UTF-8\nset osabi none\nset complaints 0\nset confirm off\nset history save on\nset history filename ~/.gdb_history\ndefine asm\n  layout asm\n  layout reg\nend\ndefine src\n  layout src\n  layout reg\nend\nsrc\n```\n\nYou normally run the `.dbg` file under gdb. If you need to debug the\n`` file itself, then you can load the debug symbols independently as\n\n```sh\ngdb foo -ex 'add-symbol-file foo.dbg 0x401000'\n```\n\n## Platform Notes\n\n### Shells\n\nIf you use zsh and have trouble running APE programs try `sh -c ./prog`\nor simply upgrade to zsh 5.9+ (since we patched it two years ago). The\nsame is the case for Python `subprocess`, old versions of fish, etc.\n\n### Linux\n\nSome Linux systems are configured to launch MZ executables under WINE.\nOther distros configure their stock installs so that APE programs will\nprint \"run-detectors: unable to find an interpreter\". For example:\n\n```sh\njart@ubuntu:~$ wget https://cosmo.zip/pub/cosmos/bin/dash\njart@ubuntu:~$ chmod +x dash\njart@ubuntu:~$ ./dash\nrun-detectors: unable to find an interpreter for ./dash\n```\n\nYou can fix that by registering APE with `binfmt_misc`:\n\n```sh\nsudo wget -O /usr/bin/ape https://cosmo.zip/pub/cosmos/bin/ape-$(uname -m).elf\nsudo chmod +x /usr/bin/ape\nsudo sh -c \"echo ':APE:M::MZqFpD::/usr/bin/ape:' >/proc/sys/fs/binfmt_misc/register\"\nsudo sh -c \"echo ':APE-jart:M::jartsr::/usr/bin/ape:' >/proc/sys/fs/binfmt_misc/register\"\n```\n\nYou should be good now. APE will not only work, it'll launch executables\n400Âµs faster now too. However if things still didn't work out, it's also\npossible to disable `binfmt_misc` as follows:\n\n```sh\nsudo sh -c 'echo -1 > /proc/sys/fs/binfmt_misc/cli'     # remove Ubuntu's MZ interpreter\nsudo sh -c 'echo -1 > /proc/sys/fs/binfmt_misc/status'  # remove ALL binfmt_misc entries\n```\n\n### WSL\n\nIt's normally unsafe to use APE in a WSL environment, because it tries\nto run MZ executables as WIN32 binaries within the WSL environment. In\norder to make it safe to use Cosmopolitan software on WSL, run this:\n\n```sh\nsudo sh -c \"echo -1 > /proc/sys/fs/binfmt_misc/WSLInterop\"\n```\n\n## Discord Chatroom\n\nThe Cosmopolitan development team collaborates on the Redbean Discord\nserver. You're welcome to join us! <https://discord.gg/FwAVVu7eJ4>\n\n## Support Vector\n\n| Platform       | Min Version    | Circa |\n| :---           | ---:           | ---:  |\n| AMD            | K8             | 2003  |\n| Intel          | Core           | 2006  |\n| Linux          | 2.6.18         | 2007  |\n| Windows        | 8 [1]          | 2012  |\n| Darwin (macOS) | 23.1.0+        | 2023  |\n| OpenBSD        | 7.3 or earlier | 2023  |\n| FreeBSD        | 13             | 2020  |\n| NetBSD         | 9.2            | 2021  |\n\n[1] See our [vista branch](https://github.com/jart/cosmopolitan/tree/vista)\n    for a community supported version of Cosmopolitan that works on Windows\n    Vista and Windows 7.\n\n## Special Thanks\n\nFunding for this project is crowdsourced using\n[GitHub Sponsors](https://github.com/sponsors/jart) and\n[Patreon](https://www.patreon.com/jart). Your support is what makes this\nproject possible. Thank you! We'd also like to give special thanks to\nthe following groups and individuals:\n\n- [Joe Drumgoole](https://github.com/jdrumgoole)\n- [Rob Figueiredo](https://github.com/robfig)\n- [Wasmer](https://wasmer.io/)\n\nFor publicly sponsoring our work at the highest tier.\n",
      "stars_today": 12
    },
    {
      "id": 1148753,
      "name": "spring-framework",
      "full_name": "spring-projects/spring-framework",
      "description": "Spring Framework",
      "html_url": "https://github.com/spring-projects/spring-framework",
      "stars": 59556,
      "forks": 38917,
      "language": "Java",
      "topics": [
        "framework",
        "spring",
        "spring-framework"
      ],
      "created_at": "2010-12-08T04:04:45Z",
      "updated_at": "2026-01-28T01:54:05Z",
      "pushed_at": "2026-01-27T20:25:38Z",
      "open_issues": 336,
      "owner": {
        "login": "spring-projects",
        "avatar_url": "https://avatars.githubusercontent.com/u/317776?v=4"
      },
      "readme": "# <img src=\"framework-docs/src/docs/spring-framework.png\" width=\"80\" height=\"80\"> Spring Framework [![Build Status](https://github.com/spring-projects/spring-framework/actions/workflows/build-and-deploy-snapshot.yml/badge.svg?branch=main)](https://github.com/spring-projects/spring-framework/actions/workflows/build-and-deploy-snapshot.yml?query=branch%3Amain) [![Revved up by Develocity](https://img.shields.io/badge/Revved%20up%20by-Develocity-06A0CE?logo=Gradle&labelColor=02303A)](https://ge.spring.io/scans?search.rootProjectNames=spring)\n\nThis is the home of the Spring Framework: the foundation for all [Spring projects](https://spring.io/projects). Collectively the Spring Framework and the family of Spring projects are often referred to simply as \"Spring\". \n\nSpring provides everything required beyond the Java programming language for creating enterprise applications for a wide range of scenarios and architectures. Please read the [Overview](https://docs.spring.io/spring-framework/reference/overview.html) section of the reference documentation for a more complete introduction.\n\n## Code of Conduct\n\nThis project is governed by the [Spring Code of Conduct](https://github.com/spring-projects/spring-framework/?tab=coc-ov-file#contributor-code-of-conduct). By participating, you are expected to uphold this code of conduct. Please report unacceptable behavior to spring-code-of-conduct@spring.io.\n\n## Access to Binaries\n\nFor access to artifacts or a distribution zip, see the [Spring Framework Artifacts](https://github.com/spring-projects/spring-framework/wiki/Spring-Framework-Artifacts) wiki page.\n\n## Documentation\n\nThe Spring Framework maintains reference documentation ([published](https://docs.spring.io/spring-framework/reference/) and [source](framework-docs/modules/ROOT)), GitHub [wiki pages](https://github.com/spring-projects/spring-framework/wiki), and an\n[API reference](https://docs.spring.io/spring-framework/docs/current/javadoc-api/). There are also [guides and tutorials](https://spring.io/guides) across Spring projects.\n\n## Micro-Benchmarks\n\nSee the [Micro-Benchmarks](https://github.com/spring-projects/spring-framework/wiki/Micro-Benchmarks) wiki page.\n\n## Build from Source\n\nSee the [Build from Source](https://github.com/spring-projects/spring-framework/wiki/Build-from-Source) wiki page and the [CONTRIBUTING.md](CONTRIBUTING.md) file.\n\n## Continuous Integration Builds\n\nCI builds are defined with [GitHub Actions workflows](.github/workflows).\n\n## Stay in Touch\n\nFollow [@SpringCentral](https://twitter.com/springcentral), [@SpringFramework](https://twitter.com/springframework), and its [team members](https://twitter.com/springframework/lists/team/members) on ğ•. In-depth articles can be found at [The Spring Blog](https://spring.io/blog/), and releases are announced via our [releases feed](https://spring.io/blog/category/releases).\n\n## License\n\nThe Spring Framework is released under version 2.0 of the [Apache License](https://www.apache.org/licenses/LICENSE-2.0).\n",
      "stars_today": 11
    },
    {
      "id": 123235,
      "name": "connectbot",
      "full_name": "connectbot/connectbot",
      "description": "ConnectBot is the first SSH client for Android.",
      "html_url": "https://github.com/connectbot/connectbot",
      "stars": 2976,
      "forks": 670,
      "language": "Kotlin",
      "topics": [
        "android",
        "connectbot",
        "java",
        "ssh",
        "ssh-client",
        "ssh2"
      ],
      "created_at": "2009-02-06T17:52:35Z",
      "updated_at": "2026-01-28T00:35:56Z",
      "pushed_at": "2026-01-28T01:05:00Z",
      "open_issues": 268,
      "owner": {
        "login": "connectbot",
        "avatar_url": "https://avatars.githubusercontent.com/u/5605419?v=4"
      },
      "readme": "[![Build Status](https://github.com/connectbot/connectbot/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/connectbot/connectbot/actions/workflows/ci.yml)\n\n# ConnectBot\n\nConnectBot is a [Secure Shell](https://en.wikipedia.org/wiki/Secure_Shell)\nclient for Android that lets you connect to remote servers over a\ncryptographically secure link.\n\n\n## How to Install\n\n### Google Play\n\n[![Get it on Google Play][2]][1]\n\n  [1]: https://play.google.com/store/apps/details?id=org.connectbot\n  [2]: https://developer.android.com/images/brand/en_generic_rgb_wo_60.png\n\nThe easiest way to get ConnectBot is to [install from Google Play Store][1].\nIf you have installed from a downloaded APK, Google Play Store can upgrade\nyour installed version to the latest version. However, once it has upgraded\n*you can't install a version from the releases on GitHub anymore.*\n\n\n### Download a release\n\nConnectBot can be downloaded from [releases](\nhttps://github.com/connectbot/connectbot/releases) on GitHub. There are\ntwo versions:\n\n-  \"`google`\" &mdash; for a version that uses Google Play Services\nto handle upgrading the cryptography provider\n-  \"`oss`\" &mdash; includes the cryptography provider in the APK which\n   increases its size by a few megabytes.\n## Compiling\n\n### Android Studio\n\nConnectBot is most easily developed in [Android Studio](\nhttps://developer.android.com/studio/). You can import this project\ndirectly from its project creation screen by importing from the GitHub URL.\n\n### Command line\n\nTo compile ConnectBot using `gradlew`, you must first specify where your\nAndroid SDK is via the `ANDROID_SDK_HOME` environment variable. Then\nyou can invoke the Gradle wrapper to build:\n\n```sh\n./gradlew build\n```\n\n### Continuous Integration\n\nConnectBot uses [GitHub Actions](https://github.com/connectbot/connectbot/actions)\nfor continuous integration. The workflow is defined in\n`.github/workflows/ci.yml`.\n\n#### Running Workflows Locally with act\n\nIn general, simply running `./gradlew build` should cover all the\nchecks run in the GitHub Actions continuous integration workflow, but you can\nrun GitHub Actions workflows locally using [`nektos/act`](https://github.com/nektos/act).\nThis requires Docker to be installed and running.\n\nTo run the main CI workflow (`ci.yml`):\n\n```sh\nact -W .github/workflows/ci.yml\n```\n\n\n## Translations\n\nIf you'd like to correct or contribute new translations to ConnectBot,\nthen head on over to [ConnectBot's translations project](\nhttps://translations.launchpad.net/connectbot/trunk/+pots/fortune)\n",
      "stars_today": 11
    },
    {
      "id": 35418187,
      "name": "signal-cli",
      "full_name": "AsamK/signal-cli",
      "description": "signal-cli provides an unofficial commandline, JSON-RPC and dbus interface for the Signal messenger.",
      "html_url": "https://github.com/AsamK/signal-cli",
      "stars": 4035,
      "forks": 361,
      "language": "Java",
      "topics": [
        "commandline",
        "dbus",
        "java",
        "json-rpc",
        "messaging",
        "signal",
        "signal-cli"
      ],
      "created_at": "2015-05-11T10:49:42Z",
      "updated_at": "2026-01-28T02:08:52Z",
      "pushed_at": "2026-01-24T16:24:36Z",
      "open_issues": 87,
      "owner": {
        "login": "AsamK",
        "avatar_url": "https://avatars.githubusercontent.com/u/2340865?v=4"
      },
      "readme": "# signal-cli\n\nsignal-cli is a commandline interface for the [Signal messenger](https://signal.org/).\nIt supports registering, verifying, sending and receiving messages.\nsignal-cli uses a [patched libsignal-service-java](https://github.com/Turasa/libsignal-service-java),\nextracted from the [Signal-Android source code](https://github.com/signalapp/Signal-Android/tree/main/libsignal-service).\nFor registering you need a phone number where you can receive SMS or incoming calls.\n\nsignal-cli is primarily intended to be used on servers to notify admins of important events.\nFor this use-case, it has a daemon mode with JSON-RPC interface ([man page](https://github.com/AsamK/signal-cli/blob/master/man/signal-cli-jsonrpc.5.adoc))\nand D-BUS interface ([man page](https://github.com/AsamK/signal-cli/blob/master/man/signal-cli-dbus.5.adoc)).\nFor the JSON-RPC interface there's also a simple [example client](https://github.com/AsamK/signal-cli/tree/master/client), written in Rust.\n\nsignal-cli needs to be kept up-to-date to keep up with Signal-Server changes.\nThe official Signal clients expire after three months and then the Signal-Server can make incompatible changes.\nSo signal-cli releases older than three months may not work correctly.\n\n## Installation\n\nYou can [build signal-cli](#building) yourself or use\nthe [provided binary files](https://github.com/AsamK/signal-cli/releases/latest), which should work on Linux, macOS and\nWindows. There's also a [docker image and some Linux packages](https://github.com/AsamK/signal-cli/wiki/Binary-distributions) provided by the community.\n\nSystem requirements:\n\n- at least Java Runtime Environment (JRE) 25\n- native library: libsignal-client\n\n  The native libs are bundled for x86_64 Linux (with recent enough glibc), Windows and MacOS. For other\n  systems/architectures\n  see: [Provide native lib for libsignal](https://github.com/AsamK/signal-cli/wiki/Provide-native-lib-for-libsignal)\n\n### Install system-wide on Linux [ JVM build ]\n\nSee [latest version](https://github.com/AsamK/signal-cli/releases).\n\n```sh\nVERSION=$(curl -Ls -o /dev/null -w %{url_effective} https://github.com/AsamK/signal-cli/releases/latest | sed -e 's/^.*\\/v//')\ncurl -L -O https://github.com/AsamK/signal-cli/releases/download/v\"${VERSION}\"/signal-cli-\"${VERSION}\".tar.gz\nsudo tar xf signal-cli-\"${VERSION}\".tar.gz -C /opt\nsudo ln -sf /opt/signal-cli-\"${VERSION}\"/bin/signal-cli /usr/local/bin/\n```\n\n### Install system-wide on Linux [ GraalVM native build ]\n\n```sh\nVERSION=$(curl -Ls -o /dev/null -w %{url_effective} https://github.com/AsamK/signal-cli/releases/latest | sed -e 's/^.*\\/v//')\ncurl -L -O https://github.com/AsamK/signal-cli/releases/download/v\"${VERSION}\"/signal-cli-\"${VERSION}\"-Linux-native.tar.gz\nsudo tar xf signal-cli-\"${VERSION}\"-Linux-native.tar.gz -C /opt\nsudo ln -sf /opt/signal-cli /usr/local/bin/\n```\n\nYou can find further instructions on the Wiki:\n\n- [Quickstart](https://github.com/AsamK/signal-cli/wiki/Quickstart)\n\n## Usage\n\nFor a complete usage overview please read\nthe [man page](https://github.com/AsamK/signal-cli/blob/master/man/signal-cli.1.adoc) and\nthe [wiki](https://github.com/AsamK/signal-cli/wiki).\n\nImportant: The ACCOUNT is your phone number in international format and must include the country calling code. Hence it\nshould start with a \"+\" sign. (See [Wikipedia](https://en.wikipedia.org/wiki/List_of_country_calling_codes) for a list\nof all country codes.)\n\n* Register a number (with SMS verification)\n\n      signal-cli -a ACCOUNT register\n\n  You can register Signal using a landline number. In this case, you need to follow the procedure below:\n   * Attempt a SMS verification process first (`signal-cli -a ACCOUNT register`)\n     * You will get an error `400 (InvalidTransportModeException)`, this is normal\n   * Wait 60 seconds\n   * Attempt a voice call verification by adding the `--voice` switch and wait for the call:\n\n     ```sh\n     signal-cli -a ACCOUNT register --voice\n     ```\n\n  Registering may require solving a CAPTCHA\n  challenge: [Registration with captcha](https://github.com/AsamK/signal-cli/wiki/Registration-with-captcha)\n\n* Verify the number using the code received via SMS or voice, optionally add `--pin PIN_CODE` if you've added a pin code\n  to your account\n\n      signal-cli -a ACCOUNT verify CODE\n\n* Send a message\n\n     ```sh\n     signal-cli -a ACCOUNT send -m \"This is a message\" RECIPIENT\n     ```\n\n* Send a message to a username, usernames need to be prefixed with `u:`\n\n     ```sh\n     signal-cli -a ACCOUNT send -m \"This is a message\" u:USERNAME.000\n     ```\n\n* Pipe the message content from another process.\n\n      uname -a | signal-cli -a ACCOUNT send --message-from-stdin RECIPIENT\n\n* Receive messages\n\n      signal-cli -a ACCOUNT receive\n\n**Hint**: The Signal protocol expects that incoming messages are regularly received (using `daemon` or `receive`\ncommand). This is required for the encryption to work efficiently and for getting updates to groups, expiration timer\nand other features.\n\n## Storage\n\nThe password and cryptographic keys are created when registering and stored in the current users home directory:\n\n    $XDG_DATA_HOME/signal-cli/data/\n    $HOME/.local/share/signal-cli/data/\n\n## Building\n\nThis project uses [Gradle](http://gradle.org) for building and maintaining dependencies. If you have a recent gradle\nversion installed, you can replace `./gradlew` with `gradle` in the following steps.\n\n1. Checkout the source somewhere on your filesystem with\n\n       git clone https://github.com/AsamK/signal-cli.git\n\n2. Execute Gradle:\n\n       ./gradlew build\n\n   2a. Create shell wrapper in *build/install/signal-cli/bin*:\n\n       ./gradlew installDist\n\n   2b. Create tar file in *build/distributions*:\n\n       ./gradlew distTar\n\n   2c. Create a fat tar file in *build/libs/signal-cli-fat*:\n\n       ./gradlew fatJar\n\n   2d. Compile and run signal-cli:\n\n      ```sh\n      ./gradlew run --args=\"--help\"\n      ```\n\n### Building a native binary with GraalVM (EXPERIMENTAL)\n\nIt is possible to build a native binary with [GraalVM](https://www.graalvm.org). This is still experimental and will not\nwork in all situations.\n\n1. [Install GraalVM and setup the environment](https://www.graalvm.org/docs/getting-started/#install-graalvm)\n2. Execute Gradle:\n\n       ./gradlew nativeCompile\n\n   The binary is available at *build/native/nativeCompile/signal-cli*\n\n## FAQ and Troubleshooting\n\nFor frequently asked questions and issues have a look at the [wiki](https://github.com/AsamK/signal-cli/wiki/FAQ).\n\n## License\n\nThis project uses libsignal-service-java from Open Whisper Systems:\n\nhttps://github.com/WhisperSystems/libsignal-service-java\n\nLicensed under the GPLv3: http://www.gnu.org/licenses/gpl-3.0.html\n",
      "stars_today": 11
    },
    {
      "id": 16652218,
      "name": "overleaf",
      "full_name": "overleaf/overleaf",
      "description": "A web-based collaborative LaTeX editor",
      "html_url": "https://github.com/overleaf/overleaf",
      "stars": 17212,
      "forks": 1861,
      "language": "JavaScript",
      "topics": [],
      "created_at": "2014-02-08T20:20:34Z",
      "updated_at": "2026-01-28T00:12:35Z",
      "pushed_at": "2026-01-27T09:07:48Z",
      "open_issues": 159,
      "owner": {
        "login": "overleaf",
        "avatar_url": "https://avatars.githubusercontent.com/u/6359919?v=4"
      },
      "readme": "<h1 align=\"center\">\n  <br>\n  <a href=\"https://www.overleaf.com\"><img src=\"doc/logo.png\" alt=\"Overleaf\" width=\"300\"></a>\n</h1>\n\n<h4 align=\"center\">An open-source online real-time collaborative LaTeX editor.</h4>\n\n<p align=\"center\">\n  <a href=\"https://github.com/overleaf/overleaf/wiki\">Wiki</a> â€¢\n  <a href=\"https://www.overleaf.com/for/enterprises\">Server Pro</a> â€¢\n  <a href=\"#contributing\">Contributing</a> â€¢\n  <a href=\"https://mailchi.mp/overleaf.com/community-edition-and-server-pro\">Mailing List</a> â€¢\n  <a href=\"#authors\">Authors</a> â€¢\n  <a href=\"#license\">License</a>\n</p>\n\n<img src=\"doc/screenshot.png\" alt=\"A screenshot of a project being edited in Overleaf Community Edition\">\n<p align=\"center\">\n  Figure 1: A screenshot of a project being edited in Overleaf Community Edition.\n</p>\n\n## Community Edition\n\n[Overleaf](https://www.overleaf.com) is an open-source online real-time collaborative LaTeX editor. We run a hosted version at [www.overleaf.com](https://www.overleaf.com), but you can also run your own local version, and contribute to the development of Overleaf.\n\n> [!CAUTION]\n> Overleaf Community Edition is intended for use in environments where **all** users are trusted. Community Edition is **not** appropriate for scenarios where isolation of users is required due to Sandbox Compiles not being available. When not using Sandboxed Compiles, users have full read and write access to the `sharelatex` container resources (filesystem, network, environment variables) when running LaTeX compiles.\n\nFor more information on Sandbox Compiles check out our [documentation](https://docs.overleaf.com/on-premises/configuration/overleaf-toolkit/server-pro-only-configuration/sandboxed-compiles).\n\n## Enterprise\n\nIf you want help installing and maintaining Overleaf in your lab or workplace, we offer an officially supported version called [Overleaf Server Pro](https://www.overleaf.com/for/enterprises). It also includes more features for security (SSO with LDAP or SAML), administration and collaboration (e.g. tracked changes). [Find out more!](https://www.overleaf.com/for/enterprises)\n\n## Keeping up to date\n\nSign up to the [mailing list](https://mailchi.mp/overleaf.com/community-edition-and-server-pro) to get updates on Overleaf releases and development.\n\n## Installation\n\nWe have detailed installation instructions in the [Overleaf Toolkit](https://github.com/overleaf/toolkit/).\n\n## Upgrading\n\nIf you are upgrading from a previous version of Overleaf, please see the [Release Notes section on the Wiki](https://github.com/overleaf/overleaf/wiki#release-notes) for all of the versions between your current version and the version you are upgrading to.\n\n## Overleaf Docker Image\n\nThis repo contains two dockerfiles, [`Dockerfile-base`](server-ce/Dockerfile-base), which builds the\n`sharelatex/sharelatex-base` image, and [`Dockerfile`](server-ce/Dockerfile) which builds the\n`sharelatex/sharelatex` (or \"community\") image.\n\nThe Base image generally contains the basic dependencies like `wget`, plus `texlive`.\nWe split this out because it's a pretty heavy set of\ndependencies, and it's nice to not have to rebuild all of that every time.\n\nThe `sharelatex/sharelatex` image extends the base image and adds the actual Overleaf code\nand services.\n\nUse `make build-base` and `make build-community` from `server-ce/` to build these images.\n\nWe use the [Phusion base-image](https://github.com/phusion/baseimage-docker)\n(which is extended by our `base` image) to provide us with a VM-like container\nin which to run the Overleaf services. Baseimage uses the `runit` service\nmanager to manage services, and we add our init-scripts from the `server-ce/runit`\nfolder.\n\n\n## Contributing\n\nPlease see the [CONTRIBUTING](CONTRIBUTING.md) file for information on contributing to the development of Overleaf.\n\n## Authors\n\n[The Overleaf Team](https://www.overleaf.com/about)\n\n## License\n\nThe code in this repository is released under the GNU AFFERO GENERAL PUBLIC LICENSE, version 3. A copy can be found in the [`LICENSE`](LICENSE) file.\n\nCopyright (c) Overleaf, 2014-2025.\n",
      "stars_today": 10
    },
    {
      "id": 858363788,
      "name": "home-assistant-matter-hub",
      "full_name": "t0bst4r/home-assistant-matter-hub",
      "description": "Publish your Home-Assistant Instance using Matter.",
      "html_url": "https://github.com/t0bst4r/home-assistant-matter-hub",
      "stars": 1461,
      "forks": 136,
      "language": "TypeScript",
      "topics": [],
      "created_at": "2024-09-16T19:06:28Z",
      "updated_at": "2026-01-28T01:22:07Z",
      "pushed_at": "2026-01-25T12:01:56Z",
      "open_issues": 170,
      "owner": {
        "login": "t0bst4r",
        "avatar_url": "https://avatars.githubusercontent.com/u/82281152?v=4"
      },
      "readme": "# Home-Assistant-Matter-Hub\n\n![\"Home-Assistant-Matter-Hub\"](./docs/assets/hamh-logo-small.png)\n\n---\n\n> [!IMPORTANT]  \n> âš ï¸ **Project Status: End of Maintenance**\n>\n> As of **January 2026**, this project is no longer actively maintained.\n>\n> I previously announced a search for a new maintainer, but unfortunately no one has stepped forward\n> to take over the project. Due to personal time constraints, I am no longer able to continue development or provide support.\n>\n> **What this means:**\n> - âŒ No further feature development\n> - âŒ No bug fixes or updates\n> - âŒ No guaranteed support\n>\n> The repository will remain available for reference and forking.\n>\n> ğŸ’¡ I would be very happy to see this project continued by the community.  \n> If you plan to fork it and continue development: **may the best fork prevail.**\n>\n> Thank you to everyone who used, tested, and contributed to this project â¤ï¸\n\n\n---\n\n## About\n\nThis project simulates bridges to publish your entities from Home Assistant to any Matter-compatible controller like\nAlexa, Apple Home or Google Home. Using Matter, those can be connected easily using local communication without the need\nof port forwarding etc.\n\n---\n\n## Documentation\n\nPlease see the [documentation](https://t0bst4r.github.io/home-assistant-matter-hub) for installation instructions,\nknown issues, limitations and guides.\n\n---\n",
      "stars_today": 10
    },
    {
      "id": 253044228,
      "name": "nuclei-templates",
      "full_name": "projectdiscovery/nuclei-templates",
      "description": "Community curated list of templates for the nuclei engine to find security vulnerabilities.",
      "html_url": "https://github.com/projectdiscovery/nuclei-templates",
      "stars": 11852,
      "forks": 3332,
      "language": "JavaScript",
      "topics": [
        "bugbounty",
        "exploit-development",
        "exploits",
        "fingerprint",
        "hacktoberfest",
        "nuclei",
        "nuclei-checks",
        "nuclei-templates",
        "security",
        "vulnerability-detection"
      ],
      "created_at": "2020-04-04T16:21:34Z",
      "updated_at": "2026-01-27T22:20:51Z",
      "pushed_at": "2026-01-27T22:20:47Z",
      "open_issues": 150,
      "owner": {
        "login": "projectdiscovery",
        "avatar_url": "https://avatars.githubusercontent.com/u/50994705?v=4"
      },
      "readme": "\n\n<h1 align=\"center\">\nNuclei Templates\n</h1>\n<h4 align=\"center\">Community curated list of templates for the nuclei engine to find security vulnerabilities in applications.</h4>\n\n\n<p align=\"center\">\n<a href=\"https://github.com/projectdiscovery/nuclei-templates/issues\"><img src=\"https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\"></a>\n<a href=\"https://github.com/projectdiscovery/nuclei-templates/releases\"><img src=\"https://img.shields.io/github/release/projectdiscovery/nuclei-templates\"></a>\n<a href=\"https://twitter.com/pdnuclei\"><img src=\"https://img.shields.io/twitter/follow/pdnuclei.svg?logo=twitter\"></a>\n<a href=\"https://discord.gg/projectdiscovery\"><img src=\"https://img.shields.io/discord/695645237418131507.svg?logo=discord\"></a>\n</p>\n      \n<p align=\"center\">\n  <a href=\"https://docs.projectdiscovery.io/templates/introduction\">Documentation</a> â€¢\n  <a href=\"#-contributions\">Contributions</a> â€¢\n  <a href=\"#-discussion\">Discussion</a> â€¢\n  <a href=\"#-community\">Community</a> â€¢\n  <a href=\"https://docs.projectdiscovery.io/templates/faq\">FAQs</a> â€¢\n  <a href=\"https://discord.gg/projectdiscovery\">Join Discord</a>\n</p>\n\n----\n\nTemplates are the core of the [nuclei scanner](https://github.com/projectdiscovery/nuclei) which powers the actual scanning engine.\nThis repository stores and houses various templates for the scanner provided by our team, as well as contributed by the community.\nWe hope that you also contribute by sending templates via **pull requests** or [Github issues](https://github.com/projectdiscovery/nuclei-templates/issues/new?assignees=&labels=&template=submit-template.md&title=%5Bnuclei-template%5D+) to grow the list.\n\n\n## Nuclei Templates overview\n\n\nAn overview of the nuclei template project, including statistics on unique tags, author, directory, severity, and type of templates. The table below contains the top ten statistics for each matrix; an expanded version of this is [available here](TEMPLATES-STATS.md), and also available in [JSON](TEMPLATES-STATS.json) format for integration.\n\n<table>\n<tr>\n<td>\n\n### ğŸš¨ Known Exploited Vulnerabilities (KEV) Coverage\n\nNuclei templates provide coverage for vulnerabilities actively exploited in the wild:\n\n| **KEV Source** | **Templates** | **Description** |\n|----------------|---------------|-----------------|\n| ğŸ”´ **CISA KEV** | **454** | [CISA Known Exploited Vulnerabilities Catalog](https://www.cisa.gov/known-exploited-vulnerabilities-catalog) |\n| ğŸŸ  **VulnCheck KEV** | **1449** | [VulnCheck KEV](https://vulncheck.com/kev) - Enhanced vulnerability intelligence |\n| ğŸŸ¢ **Both Sources** | **407** | Templates covering vulnerabilities in both catalogs |\n\n> ğŸ’¡ **Total unique KEV templates: 1496** - Use `nuclei -tags kev,vkev` to scan for actively exploited vulnerabilities\n\n---\n\n## Nuclei Templates Top 10 statistics\n\n|    TAG    | COUNT |    AUTHOR     | COUNT | DIRECTORY  | COUNT | SEVERITY | COUNT | TYPE | COUNT |\n|-----------|-------|---------------|-------|------------|-------|----------|-------|------|-------|\n| vuln      |  6468 | dhiyaneshdk   |  1894 | http       |  9281 | info     |  4353 | file |   436 |\n| cve       |  3587 | daffainfo     |   905 | cloud      |   659 | high     |  2552 | dns  |    26 |\n| discovery |  3265 | princechaddha |   854 | file       |   436 | medium   |  2457 |      |       |\n| vkev      |  1394 | dwisiswant0   |   805 | network    |   259 | critical |  1555 |      |       |\n| panel     |  1365 | ritikchaddha  |   678 | code       |   251 | low      |   330 |      |       |\n| xss       |  1269 | pussycat0x    |   675 | dast       |   240 | unknown  |    54 |      |       |\n| wordpress |  1261 | pikpikcu      |   353 | workflows  |   205 |          |       |      |       |\n| exposure  |  1141 | pdteam        |   314 | javascript |    92 |          |       |      |       |\n| wp-plugin |  1103 | pdresearch    |   275 | ssl        |    38 |          |       |      |       |\n| osint     |   848 | iamnoooob     |   263 | dns        |    23 |          |       |      |       |\n\n**873 directories, 11997 files**.\n\n</td>\n</tr>\n</table>\n\nğŸ“– Documentation\n-----\n\nPlease navigate to https://nuclei.projectdiscovery.io for detailed documentation to **build** new or your own **custom** templates.\nWe have also added a set of templates to help you understand how things work.\n\nğŸ’ª Contributions\n-----\n\nNuclei-templates is powered by major contributions from the community.\n[Template contributions ](https://github.com/projectdiscovery/nuclei-templates/issues/new?assignees=&labels=&template=submit-template.md&title=%5Bnuclei-template%5D+), [Feature Requests](https://github.com/projectdiscovery/nuclei-templates/issues/new?assignees=&labels=&template=feature_request.md&title=%5BFeature%5D+) and [Bug Reports](https://github.com/projectdiscovery/nuclei-templates/issues/new?assignees=&labels=&template=bug_report.md&title=%5BBug%5D+) are more than welcome.\n\n![Alt](https://repobeats.axiom.co/api/embed/55ee65543bb9a0f9c797626c4e66d472a517d17c.svg \"Repobeats analytics image\")\n\nğŸ’¬ Discussion\n-----\n\nHave questions / doubts / ideas to discuss?\nFeel free to open a discussion on [Github discussions](https://github.com/projectdiscovery/nuclei-templates/discussions) board.\n\nğŸ‘¨â€ğŸ’» Community\n-----\n\nYou are welcome to join the active [Discord Community](https://discord.gg/projectdiscovery) to discuss directly with project maintainers and share things with others around security and automation.\nAdditionally, you may follow us on [Twitter](https://twitter.com/pdnuclei) to be updated on all the things about Nuclei.\n\n\n<p align=\"center\">\n<a href=\"https://github.com/projectdiscovery/nuclei-templates/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=projectdiscovery/nuclei-templates&max=300\">\n</a>\n</p>\n\n\nThanks again for your contribution and keeping this community vibrant. :heart:\n",
      "stars_today": 9
    },
    {
      "id": 127023441,
      "name": "cuda-samples",
      "full_name": "NVIDIA/cuda-samples",
      "description": "Samples for CUDA Developers which demonstrates features in CUDA Toolkit",
      "html_url": "https://github.com/NVIDIA/cuda-samples",
      "stars": 8771,
      "forks": 2253,
      "language": "C",
      "topics": [
        "cuda",
        "cuda-driver-api",
        "cuda-kernels",
        "cuda-opengl"
      ],
      "created_at": "2018-03-27T17:36:24Z",
      "updated_at": "2026-01-27T22:23:39Z",
      "pushed_at": "2026-01-06T17:29:53Z",
      "open_issues": 101,
      "owner": {
        "login": "NVIDIA",
        "avatar_url": "https://avatars.githubusercontent.com/u/1728152?v=4"
      },
      "readme": "# CUDA Samples\n\nSamples for CUDA Developers which demonstrates features in CUDA Toolkit. This version supports [CUDA Toolkit 13.1](https://developer.nvidia.com/cuda-downloads).\n\n## Release Notes\n\nThis section describes the release notes for the CUDA Samples on GitHub only.\n\n### Change Log\n\n### [Revision History](./CHANGELOG.md)\n\n## Getting Started\n\n### Prerequisites\n\nDownload and install the [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads) for your corresponding platform.\nFor system requirements and installation instructions of cuda toolkit, please refer to the [Linux Installation Guide](http://docs.nvidia.com/cuda/cuda-installation-guide-linux/), and the [Windows Installation Guide](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html).\n\n### Getting the CUDA Samples\n\nUsing git clone the repository of CUDA Samples using the command below.\n```\ngit clone https://github.com/NVIDIA/cuda-samples.git\n```\n\nWithout using git the easiest way to use these samples is to download the zip file containing the current version by clicking the \"Download ZIP\" button on the repo page. You can then unzip the entire archive and use the samples.\n\n## Building CUDA Samples\n\n### Building CUDA Samples\n\nThe CUDA Samples are built using CMake. Follow the instructions below for building on Linux, Windows, and for cross-compilation to Tegra devices.\n\n### Linux\n\nEnsure that CMake (version 3.20 or later) is installed. Install it using your package manager if necessary:\n\ne.g.\n```sudo apt install cmake```\n\nNavigate to the root of the cloned repository and create a build directory:\n```\nmkdir build && cd build\n```\nConfigure the project with CMake:\n```\ncmake ..\n```\nBuild the samples:\n```\nmake -j$(nproc)\n```\nRun the samples from their respective directories in the build folder. You can also follow this process from and subdirectory of the samples repo, or from within any individual sample.\n\n### Windows\n\nLanguage services for CMake are available in Visual Studio 2019 version 16.5 or later, and you can directly import the CUDA samples repository from either the root level or from any\nsubdirectory or individual sample.\n\nTo build from the command line, open the `x64 Native Tools Command Prompt for VS` provided with your Visual Studio installation.\n\nNavigate to the root of the cloned repository and create a build directory:\n```\nmkdir build && cd build\n```\nConfigure the project with CMake - for example:\n```\ncmake .. -G \"Visual Studio 16 2019\" -A x64\n```\nOpen the generated solution file CUDA_Samples.sln in Visual Studio. Build the samples by selecting the desired configuration (e.g., Debug or Release) and pressing F7 (Build Solution).\n\nRun the samples from the output directories specified in Visual Studio.\n\n### Enabling On-GPU Debugging\n\nNVIDIA GPUs support on-GPU debugging through cuda-gdb. Enabling this may significantly affect application performance as certain compiler optimizations are disabled\nin this configuration, hence it's not on by default. Enablement of on-device debugging is controlled via the `-G` switch to nvcc.\n\nTo enable cuda-gdb for samples builds, define the `ENABLE_CUDA_DEBUG` flag on the CMake command line. For example:\n\n```\ncmake -DENABLE_CUDA_DEBUG=True ...\n```\n\n### Platform-Specific Samples\n\nSome CUDA samples are specific to certain platforms, and require passing flags into CMake to enable. In particular, we define the following platform-specific flags:\n\n* `BUILD_TEGRA` - for Tegra-specific samples\n\nTo build these samples, set the variables either on the command line or through your CMake GUI. For example:\n\n```\ncmake -DBUILD_TEGRA=True ..\n```\n\n### Cross-Compilation for Tegra Platforms\n\nInstall the NVIDIA toolchain and cross-compilation environment for Tegra devices as described in the Tegra Development Guide.\n\nEnsure that CMake (version 3.20 or later) is installed.\n\nNavigate to the root of the cloned repository and create a build directory:\n```\nmkdir build && cd build\n```\nConfigure the project with CMake, specifying the Tegra toolchain file. And you can use -DTARGET_FS to point to the target file system root path for necessary include and library files:\n```\ncmake .. -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/toolchain-aarch64-linux.cmake -DTARGET_FS=/path/to/target/system/file/system\n```\nBuild the samples:\n```\nmake -j$(nproc)\n```\nTransfer the built binaries to the Tegra device and execute them there.\n\n\n### Cross Building for Automotive Linux Platforms from the DriveOS Docker containers\n\nTo build CUDA samples to the target platform from the DriveOS Docker containers, use the following instructions.\n\nMount the target Root Filesystem (RFS) in the container so that the CUDA cmake process has the correct paths to CUDA and other system libraries required to build the samples.\n\nCreate a temporary directory, `<temp>` is any temporary directory of your choosing, for example, you can use `/drive/temp`:\n\n```\n$ mkdir /drive/<temp>\n```\n\nMount the filesystem by running the following command:\n\n```\n$ mount /drive/drive-linux/filesystem/targetfs-images/dev_nsr_desktop_ubuntu-24.04_thor_rfs.img /drive/temp\n```\n\nConfigure the project by running the following cmake command:\n\n```\n$ mkdir build && cd build\n$ cmake .. -DBUILD_TEGRA=True \\\n  -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \\\n  -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/toolchain-aarch64-linux.cmake \\\n  -DTARGET_FS=/drive/temp \\\n  -DCMAKE_LIBRARY_PATH=/drive/temp/usr/local/cuda-13.1/thor/lib64/ \\\n  -DCMAKE_INCLUDE_PATH=/drive/temp/usr/local/cuda-13.1/thor/include/\n```\n\nPlease note that the following libraries are not pre-installed in the DriveOS dev-nsr target filesystem:\n* libdrm-dev\n* Vulkan\n\nThis causes the cmake command to throw errors related to the missing files, and as a result, the related samples will not build in later steps. This issue will be addressed in a future DriveOS release.\n\nTo build the samples with ignore the error mentioned above, you can use `--ignore-errors`/`--keep-going` or comment out the comment out the corresponding `add_subdirectory` command in the CMakeLists.txt in the parent folder for the samples requiring Vulkan and libdrm_dev:\n\n```\n$ make -j$(nproc) --ignore-errors # or --keep-going\n```\n\n```\n# In Samples/5_Domain_Specific/CMakeList.txt\n# add_subdirectory(simpleGL)\n# add_subdirectory(simpleVulkan)\n# add_subdirectory(simpleVulkanMMAP)\n\n# In Samples/8_Platform_Specific/Tegra/CMakeList.txt\n# add_subdirectory(simpleGLES_EGLOutput)\n```\n\n### QNX\n\nCross-compilation for QNX with CMake is supported in the CUDA 13.0 samples release and newer. An example build for\nthe Tegra Thor QNX platform might look like this:\n\n```\n$ mkdir build\n$ cd build\n\nQNX_HOST=/path/to/qnx/host \\\nQNX_TARGET=/path/to/qnx/target \\\ncmake .. \\\n-DBUILD_TEGRA=True \\\n-DCMAKE_CUDA_COMPILER=/usr/local/cuda-safe-13.0/bin/nvcc \\\n-DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/toolchain-aarch64-qnx.cmake \\\n-DCMAKE_LIBRARY_PATH=/usr/local/cuda-safe-13.0/thor/targets/aarch64-qnx/lib/stubs/ \\\n-DCMAKE_INCLUDE_PATH=/usr/local/cuda-safe-13.0/thor/targets/aarch64-qnx/include/\n```\n\n### Forward Compatibility\n\nTo build samples with new CUDA Toolkit(CUDA 13.0 or later) and UMD(Version 580 or later) and old KMD(Version 550 or earlier)ï¼Œyou need to set the `CMAKE_PREFIX_PATH` for using new driver library, the command might like this:\n\n```\ncmake -DCMAKE_PREFIX_PATH=/usr/local/cuda/lib64/stubs/ ..\n```\n\n## Install Samples\n\n### Installation Path Structure\n\nThe installation system automatically organizes samples into a structured directory layout based on:\n- **Target Architecture**: ${CMAKE_SYSTEM_PROCESSOR}, e.g. `x64`, `aarch64`, `amd64`, etc.\n- **Target OS**: `linux`, `windows`, `darwin`, `qnx`\n- **Build Type**: `release`, `debug`, etc.\n\nThe default installation path is: `build/bin/${TARGET_ARCH}/${TARGET_OS}/${BUILD_TYPE}`\n\n**Examples:**\n- Linux x86_64 Release: `build/bin/x64/linux/release`\n- Linux aarch64 Release: `build/bin/aarch64/linux/release`\n- Windows amd64 Release: `build/bin/amd64/windows/release`\n\n### Customizing Installation Paths\n\nYou can customize the installation location using CMake variables during the configuration step:\n\n- `CMAKE_INSTALL_PREFIX`: Changes the root installation directory (default: `build/bin`)\n  ```\n  cmake -DCMAKE_INSTALL_PREFIX=/custom/path ..\n  ```\n  This will install to: `/custom/path/${TARGET_ARCH}/${TARGET_OS}/${BUILD_TYPE}`\n\n- `CUDA_SAMPLES_INSTALL_DIR`: Specifies the exact final installation directory (overrides the structured path)\n  ```\n  cmake -DCUDA_SAMPLES_INSTALL_DIR=/exact/install/path ..\n  ```\n\n### Install Samples on Linux\n\n**Prerequisites:** You must first configure the project with CMake as described in the [Building CUDA Samples - Linux](#linux) or [Building]section.\n\nAfter configuring and building, install the samples:\n\n```\ncd build/\nmake install\n```\n\n### Install Samples on Windows\n\n**Prerequisites:** You must first configure the project with CMake as described in the [Building CUDA Samples - Windows](#windows) section.\n\n#### Using Command Line\n\nAfter configuring with CMake, build and install from the `x64 Native Tools Command Prompt for VS`:\n\n```cmd\ncd build\ncmake --build . --config Release\ncmake --install . --config Release\n```\n\n**Note:** Replace `Release` with `Debug` if you want to install debug builds. For multi-configuration generators (like Visual Studio), the `--config` flag determines which build type to install.\n\n#### Using Visual Studio IDE\n\nAlternatively, open the generated solution file `CUDA_Samples.sln` in Visual Studio:\n1. Select the desired configuration (`Release` or `Debug`)\n2. Build the solution (F7 or Build > Build Solution)\n3. Right-click on the `INSTALL` target under `CMakePredefinedTargets` in Solution Explorer\n4. Select \"Build\"\n\n## Running All Samples as Tests\n\nIt's important to note that the CUDA samples are _not_ intended as a validation suite for CUDA. They do not cover corner cases, they do not completely cover the\nruntime and driver APIs, are not intended for performance benchmarking, etc. That said, it can sometimes be useful to run all of the samples as a quick sanity check and\nwe provide a script to do so, `run_tests.py`.\n\nThis Python3 script finds all executables in a subdirectory you choose, matching application names with command line arguments specified in `test_args.json`. It accepts\nthe following command line arguments:\n\n| Switch     | Purpose                                                                                                        | Example                 |\n| ---------- | -------------------------------------------------------------------------------------------------------------- | ----------------------- |\n| --dir      | Specify the root directory to search for executables (recursively)                                             | --dir ./build/Samples   |\n| --config   | JSON configuration file for executable arguments                                                               | --config test_args.json |\n| --output   | Output directory for test results (stdout saved to .txt files - directory will be created if it doesn't exist) | --output ./test         |\n| --args     | Global arguments to pass to all executables (not currently used)                                               | --args arg_1 arg_2 ...  |\n| --parallel | Number of applications to execute in parallel.                                                                 | --parallel 8            |\n\n\nApplication configurations are loaded from `test_args.json` and matched against executable names (discarding the `.exe` extension on Windows).\n\nThe script returns 0 on success, or the first non-zero error code encountered during testing on failure. It will also print a condensed list of samples that failed, if any.\n\nThere are three primary modes of configuration:\n\n**Skip**\n\nAn executable configured with \"skip\" will not be executed. These generally rely on having attached graphical displays and are not suited to this kind of automation.\n\nConfiguration example:\n```json\n\"fluidsGL\": {\n    \"skip\": true\n}\n```\n\nYou will see:\n```\nSkipping fluidsGL (marked as skip in config)\n```\n\n**Single Run**\n\nFor executables to run one time only with arguments, specify each argument as a list entry. Each entry in the JSON file will be appended to the command line, separated\nby a space.\n\nAll applications execute from their current directory, so all paths are relative to the application's location.\n\nNote that if an application needs no arguments, this entry is optional. An executable found without a matching entry in the JSON will just run as `./application` from its\ncurrent directory.\n\nConfiguration example:\n```json\n\"ptxgen\": {\n    \"args\": [\n        \"test.ll\",\n        \"-arch=compute_75\"\n    ]\n}\n```\n\nYou will see:\n```\nRunning ptxgen\n    Command: ./ptxgen test.ll -arch=compute_75\n    Test completed with return code 0\n```\n\n**Multiple Runs**\n\nFor executables to run multiple times with different command line arguments, specify any number of sets of args within a \"runs\" list.\n\nAs with single runs, all applications execute from their current directory, so all paths are relative to the application's location.\n\nConfiguration example:\n```json\n\"recursiveGaussian\": {\n    \"runs\": [\n        {\n            \"args\": [\n                \"-sigma=10\",\n                \"-file=data/ref_10.ppm\"\n            ]\n        },\n        {\n            \"args\": [\n                \"-sigma=14\",\n                \"-file=data/ref_14.ppm\"\n            ]\n        },\n        {\n            \"args\": [\n                \"-sigma=18\",\n                \"-file=data/ref_18.ppm\"\n            ]\n        },\n        {\n            \"args\": [\n                \"-sigma=22\",\n                \"-file=data/ref_22.ppm\"\n            ]\n        }\n    ]\n}\n```\n\nYou will see:\n```\nRunning recursiveGaussian (run 1/4)\n    Command: ./recursiveGaussian -sigma=10 -file=data/ref_10.ppm\n    Test completed with return code 0\nRunning recursiveGaussian (run 2/4)\n    Command: ./recursiveGaussian -sigma=14 -file=data/ref_14.ppm\n    Test completed with return code 0\nRunning recursiveGaussian (run 3/4)\n    Command: ./recursiveGaussian -sigma=18 -file=data/ref_18.ppm\n    Test completed with return code 0\nRunning recursiveGaussian (run 4/4)\n    Command: ./recursiveGaussian -sigma=22 -file=data/ref_22.ppm\n    Test completed with return code 0\n```\n\n### Example Usage\n\nHere is an example set of commands to build and test all of the samples.\n\nFirst, build:\n```bash\nmkdir build\ncd build\ncmake ..\nmake -j$(nproc)\n```\n\nNow, return to the samples root directory and run the test script:\n```bash\ncd ..\npython3 run_tests.py --output ./test --dir ./build/Samples --config test_args.json\n```\n\nIf all applications run successfully, you will see something similar to this (the specific number of samples will depend on your build type\nand system configuration):\n\n```\nTest Summary:\nRan 199 test runs for 180 executables.\nAll test runs passed!\n```\n\nIf some samples fail, you will see something like this:\n\n```\nTest Summary:\nRan 199 test runs for 180 executables.\nFailed runs (2):\n  bicubicTexture (run 1/5): Failed (code 1)\n  Mandelbrot (run 1/2): Failed (code 1)\n```\n\nYou can inspect the stdout logs in the output directory (generally `APM_<application_name>.txt` or `APM_<application_name>.run<n>.txt`) to help\ndetermine what may have gone wrong from the output logs. Please file issues against the samples repository if you believe a sample is failing\nincorrectly on your system.\n\n## Samples list\n\n### [0. Introduction](./Samples/0_Introduction/README.md)\nBasic CUDA samples for beginners that illustrate key concepts with using CUDA and CUDA runtime APIs.\n\n### [1. Utilities](./Samples/1_Utilities/README.md)\nUtility samples that demonstrate how to query device capabilities and measure GPU/CPU bandwidth.\n\n### [2. Concepts and Techniques](./Samples/2_Concepts_and_Techniques/README.md)\nSamples that demonstrate CUDA related concepts and common problem solving techniques.\n\n### [3. CUDA Features](./Samples/3_CUDA_Features/README.md)\nSamples that demonstrate CUDA Features (Cooperative Groups, CUDA Dynamic Parallelism, CUDA Graphs etc).\n\n### [4. CUDA Libraries](./Samples/4_CUDA_Libraries/README.md)\nSamples that demonstrate how to use CUDA platform libraries (NPP, NVJPEG, NVGRAPH cuBLAS, cuFFT, cuSPARSE, cuSOLVER and cuRAND).\n\n### [5. Domain Specific](./Samples/5_Domain_Specific/README.md)\nSamples that are specific to domain (Graphics, Finance, Image Processing).\n\n### [6. Performance](./Samples/6_Performance/README.md)\nSamples that demonstrate performance optimization.\n\n### [7. libNVVM](./Samples/7_libNVVM/README.md)\nSamples that demonstrate the use of libNVVVM and NVVM IR.\n\n### [8. Platform Specific](./Samples/8_Platform_Specific/Tegra/README.md)\nSamples that are specific to certain platforms (Tegra, cuDLA, NvMedia, NvSci, OpenGL ES).\n\n## Dependencies\n\nSome CUDA Samples rely on third-party applications and/or libraries, or features provided by the CUDA Toolkit and Driver, to either build or execute. These dependencies are listed below.\n\nIf a sample has a third-party dependency that is available on the system, but is not installed, the sample will waive itself at build time.\n\nEach sample's dependencies are listed in its README's Dependencies section.\n\n### Third-Party Dependencies\n\nThese third-party dependencies are required by some CUDA samples. If available, these dependencies are either installed on your system automatically, or are installable via your system's package manager (Linux) or a third-party website.\n\n#### FreeImage\n\nFreeImage is an open source imaging library. FreeImage can usually be installed on Linux using your distribution's package manager system. FreeImage can also be downloaded from the FreeImage website.\n\nTo set up FreeImage on a Windows system, extract the FreeImage DLL distribution into the folder `./Common/FreeImage/Dist/x64` such that it contains the .h and .lib files. Copy the .dll file to the Release/ Debug/ execution folder or pass the FreeImage folder when cmake configuring with the `-DFreeImage_INCLUDE_DIR` and `-DFreeImage_LIBRARY` options.\n\n#### Message Passing Interface\n\nMPI (Message Passing Interface) is an API for communicating data between distributed processes. A MPI compiler can be installed using your Linux distribution's package manager system. It is also available on some online resources, such as [Open MPI](http://www.open-mpi.org/). On Windows, to build and run MPI-CUDA applications one can install [MS-MPI SDK](https://msdn.microsoft.com/en-us/library/bb524831(v=vs.85).aspx).\n\n#### Only 64-Bit\n\nSome samples can only be run on a 64-bit operating system.\n\n#### DirectX\n\nDirectX is a collection of APIs designed to allow development of multimedia applications on Microsoft platforms. For Microsoft platforms, NVIDIA's CUDA Driver supports DirectX. Several CUDA Samples for Windows demonstrates CUDA-DirectX Interoperability, for building such samples one needs to install Microsoft Visual Studio 2012 or higher which provides Microsoft Windows SDK for Windows 8.\n\n#### DirectX12\n\nDirectX 12 is a collection of advanced low-level programming APIs which can reduce driver overhead, designed to allow development of multimedia applications on Microsoft platforms starting with Windows 10 OS onwards. For Microsoft platforms, NVIDIA's CUDA Driver supports DirectX. Few CUDA Samples for Windows demonstrates CUDA-DirectX12 Interoperability, for building such samples one needs to install [Windows 10 SDK or higher](https://developer.microsoft.com/en-us/windows/downloads/windows-10-sdk), with VS 2015 or VS 2017.\n\n#### OpenGL\n\nOpenGL is a graphics library used for 2D and 3D rendering. On systems which support OpenGL, NVIDIA's OpenGL implementation is provided with the CUDA Driver.\n\n#### OpenGL ES\n\nOpenGL ES is an embedded systems graphics library used for 2D and 3D rendering. On systems which support OpenGL ES, NVIDIA's OpenGL ES implementation is provided with the CUDA Driver.\n\n#### Freeglut\n\nFreeglut is an open-source software library that serves as a replacement for the original OpenGL Utility Toolkit (GLUT). Its primary purpose is to make it easier for developers to create and manage windows containing OpenGL contexts, as well as handle input from devices like the mouse, keyboard, and joystick, across a wide range of platforms. To set up Freeglut on a Windowson on ARM system, you need to download the source from [Freeglut website](https://freeglut.sourceforge.net/), build freeglut on your system, and copy the freeglut.lib into the folder `./Common/lib/x64` and copy the freeglut.dll file into the `./bin/win64/${BUILD_TYPE}` execution folder.\n\n#### Vulkan\n\nVulkan is a low-overhead, cross-platform 3D graphics and compute API. Vulkan targets high-performance realtime 3D graphics applications such as video games and interactive media across all platforms. On systems which support Vulkan, NVIDIA's Vulkan implementation is provided with the CUDA Driver. For building and running Vulkan applications one needs to install the [Vulkan SDK](https://www.lunarg.com/vulkan-sdk/).\n\n#### GLEW\n\nGLEW (OpenGL Extension Wrangler Library) is a cross-platform, open-source C/C++ library designed to simplify the process of using modern OpenGL features and extensions. Its main function is to dynamically load OpenGL function pointers at runtime, allowing developers to access both core OpenGL functions and additional features provided by hardware vendors, known as extensions. To set up GLEW on a Windows on ARM system, you need to download the source from [GLEW website](https://glew.sourceforge.net/), build GLEW on your system, and copy the glew32.lib into the folder `./Common/lib/x64` and the glew32.dll into the `./bin/win64/${BUILD_TYPE}` execution folder.\n\n#### GLFW\n\nGLFW is a lightweight, open-source library designed for managing OpenGL, OpenGL ES, and Vulkan contexts. It simplifies the process of creating and managing windows, handling user input (keyboard, mouse, and joystick), and working with multiple monitors in a cross-platform manner.\n\nTo set up GLFW on a Windows system, Download the pre-built binaries from [GLFW website](https://www.glfw.org/download.html) and extract the zip file into the folder, pass the GLFW include header folder as `-DGLFW_INCLUDE_DIR` and lib folder as `-DGLFW_LIB_DIR` for cmake configuring.\n\n#### OpenMP\n\nOpenMP is an API for multiprocessing programming. OpenMP can be installed using your Linux distribution's package manager system. It usually comes preinstalled with GCC. It can also be found at the [OpenMP website](http://openmp.org/). For compilers such as clang, `libomp.so` and other components for LLVM must be installed separated. You will also need to set additional flags in your CMake configuration files, such as: `-DOpenMP_CXX_FLAGS=\"-fopenmp=libomp\" -DOpenMP_CXX_LIB_NAMES=\"omp\" -DOpenMP_omp_LIBRARY=\"/path/to/libomp.so\"`.\n\n#### Screen\n\nScreen is a windowing system found on the QNX operating system. Screen is usually found as part of the root filesystem.\n\n#### X11\n\nX11 is a windowing system commonly found on *-nix style operating systems. X11 can be installed using your Linux distribution's package manager, and comes preinstalled on Mac OS X systems.\n\n#### EGL\n\nEGL is an interface between Khronos rendering APIs (such as OpenGL, OpenGL ES or OpenVG) and the underlying native platform windowing system.\n\n#### EGLOutput\n\nEGLOutput is a set of EGL extensions which allow EGL to render directly to the display.\n\n#### EGLSync\n\nEGLSync is a set of EGL extensions which provides sync objects that are synchronization primitive, representing events whose completion can be tested or waited upon.\n\n#### NVSCI\n\nNvSci is a set of communication interface libraries out of which CUDA interops with NvSciBuf and NvSciSync. NvSciBuf allows applications to allocate and exchange buffers in memory. NvSciSync allows applications to manage synchronization objects which coordinate when sequences of operations begin and end.\n\n#### NvMedia\n\nNvMedia provides powerful processing of multimedia data for true hardware acceleration across NVIDIA Tegra devices. Applications leverage the NvMedia Application Programming Interface (API) to process the image and video data.\n\n### CUDA Features\n\nThese CUDA features are needed by some CUDA samples. They are provided by either the CUDA Toolkit or CUDA Driver. Some features may not be available on your system.\n\n#### CUFFT Callback Routines\n\nCUFFT Callback Routines are user-supplied kernel routines that CUFFT will call when loading or storing data. These callback routines are only available on Linux x86_64 and ppc64le systems.\n\n#### CUDA Dynamic Parallellism\n\nCDP (CUDA Dynamic Parallellism) allows kernels to be launched from threads running on the GPU. CDP is only available on GPUs with SM architecture of 3.5 or above.\n\n#### Multi-block Cooperative Groups\n\nMulti Block Cooperative Groups(MBCG) extends Cooperative Groups and the CUDA programming model to express inter-thread-block synchronization. MBCG is available on GPUs with Pascal and higher architecture.\n\n#### Multi-Device Cooperative Groups\n\n Multi Device Cooperative Groups extends Cooperative Groups and the CUDA programming model enabling thread blocks executing on multiple GPUs to cooperate and synchronize as they execute. This feature is available on GPUs with Pascal and higher architecture.\n\n#### CUBLAS\n\nCUBLAS (CUDA Basic Linear Algebra Subroutines) is a GPU-accelerated version of the BLAS library.\n\n#### CUDA Interprocess Communication\n\nIPC (Interprocess Communication) allows processes to share device pointers.\n\n#### CUFFT\n\nCUFFT (CUDA Fast Fourier Transform) is a GPU-accelerated FFT library.\n\n#### CURAND\n\nCURAND (CUDA Random Number Generation) is a GPU-accelerated RNG library.\n\n#### CUSPARSE\n\nCUSPARSE (CUDA Sparse Matrix) provides linear algebra subroutines used for sparse matrix calculations.\n\n#### CUSOLVER\n\nCUSOLVER library is a high-level package based on the CUBLAS and CUSPARSE libraries. It combines three separate libraries under a single umbrella, each of which can be used independently or in concert with other toolkit libraries. The intent ofCUSOLVER is to provide useful LAPACK-like features, such as common matrix factorization and triangular solve routines for dense matrices, a sparse least-squares solver and an eigenvalue solver. In addition cuSolver provides a new refactorization library useful for solving sequences of matrices with a shared sparsity pattern.\n\n#### NPP\n\nNPP (NVIDIA Performance Primitives) provides GPU-accelerated image, video, and signal processing functions.\n\n#### NVGRAPH\n\nNVGRAPH is a GPU-accelerated graph analytics library.\n\n#### NVJPEG\n\nNVJPEG library provides high-performance, GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications.\n\n#### NVRTC\n\nNVRTC (CUDA RunTime Compilation) is a runtime compilation library for CUDA C++.\n\n#### Stream Priorities\n\nStream Priorities allows the creation of streams with specified priorities. Stream Priorities is only available on GPUs with SM architecture of 3.5 or above.\n\n#### Unified Virtual Memory\n\nUVM (Unified Virtual Memory) enables memory that can be accessed by both the CPU and GPU without explicit copying between the two. UVM is only available on Linux and Windows systems.\n\n#### 16-bit Floating Point\n\nFP16 is a 16-bit floating-point format. One bit is used for the sign, five bits for the exponent, and ten bits for the mantissa.\n\n#### C++11 CUDA\n\nNVCC support of [C++11 features](https://en.wikipedia.org/wiki/C++11).\n\n#### CMake\n\nThe libNVVM samples are built using [CMake](https://cmake.org/) 3.10 or later.\n\n## Contributors Guide\n\nWe welcome your input on issues and suggestions for samples. At this time we are not accepting contributions from the public, check back here as we evolve our contribution model.\n\nWe use Google C++ Style Guide for all the sources https://google.github.io/styleguide/cppguide.html\n\n## Frequently Asked Questions\n\nAnswers to frequently asked questions about CUDA can be found at http://developer.nvidia.com/cuda-faq and in the [CUDA Toolkit Release Notes](http://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html).\n\n## References\n\n*   [CUDA Programming Guide](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)\n*   [Accelerated Computing Blog](https://developer.nvidia.com/blog/?tags=accelerated-computing)\n\n## Attributions\n\n*   Teapot image is obtained from [Wikimedia](https://en.wikipedia.org/wiki/File:Original_Utah_Teapot.jpg) and is licensed under the Creative Commons [Attribution-Share Alike 2.0](https://creativecommons.org/licenses/by-sa/2.0/deed.en) Generic license. The image is modified for samples use cases.\n",
      "stars_today": 9
    },
    {
      "id": 566323731,
      "name": "Easydict",
      "full_name": "tisfeng/Easydict",
      "description": "ä¸€ä¸ªç®€æ´ä¼˜é›…çš„è¯å…¸ç¿»è¯‘ macOS Appã€‚å¼€ç®±å³ç”¨ï¼Œæ”¯æŒç¦»çº¿ OCR è¯†åˆ«ï¼Œæ”¯æŒæœ‰é“è¯å…¸ï¼ŒğŸ è‹¹æœç³»ç»Ÿè¯å…¸ï¼ŒğŸ è‹¹æœç³»ç»Ÿç¿»è¯‘ï¼ŒOpenAIï¼ŒGeminiï¼ŒDeepLï¼ŒGoogleï¼ŒBingï¼Œè…¾è®¯ï¼Œç™¾åº¦ï¼Œé˜¿é‡Œï¼Œå°ç‰›ï¼Œå½©äº‘å’Œç«å±±ç¿»è¯‘ã€‚A concise and elegant Dictionary and Translator macOS App for looking up words and translating text. ",
      "html_url": "https://github.com/tisfeng/Easydict",
      "stars": 11997,
      "forks": 590,
      "language": "Swift",
      "topics": [
        "app",
        "baidu",
        "bing",
        "deepl",
        "dictionary",
        "gemini",
        "google",
        "macos",
        "ocr",
        "openai",
        "shortcuts",
        "tencent",
        "translate",
        "translator",
        "youdao"
      ],
      "created_at": "2022-11-15T12:41:53Z",
      "updated_at": "2026-01-28T02:12:00Z",
      "pushed_at": "2026-01-26T01:44:41Z",
      "open_issues": 126,
      "owner": {
        "login": "tisfeng",
        "avatar_url": "https://avatars.githubusercontent.com/u/25194972?v=4"
      },
      "readme": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/icon_512x512@2x.png\" height=\"256\">\n  <h1 align=\"center\">Easydict</h1>\n  <h4 align=\"center\"> Easy to look up words or translate text</h4>\n<p align=\"center\"> \n<a href=\"https://github.com/tisfeng/easydict/blob/main/LICENSE\">\n<img src=\"https://img.shields.io/github/license/tisfeng/easydict\"\n            alt=\"License\"></a>\n<a href=\"https://github.com/tisfeng/Easydict/releases\">\n<img src=\"https://img.shields.io/github/downloads/tisfeng/easydict/total.svg\"\n            alt=\"Downloads\"></a>\n<a href=\"https://img.shields.io/badge/-macOS-black?&logo=apple&logoColor=white\">\n<img src=\"https://img.shields.io/badge/-macOS-black?&logo=apple&logoColor=white\"\n            alt=\"macOS\"></a>  \n</p>\n\n<div align=\"center\">\n<a href=\"./README_ZH.md\">ä¸­æ–‡</a> &nbsp;&nbsp;|&nbsp;&nbsp; <a href=\"./README.md\">English</a>\n</div>\n\n## Easydict\n\n`Easydict` is a concise and easy-to-use translation dictionary macOS App that allows you to easily and elegantly look up words or translate text.\n\nEasydict is ready to use out of the box, can automatically recognize the language of the input text, supports input translate, select translate, and OCR screenshot translate, and can query multiple translation services results at the same time.\n\n**Supported translation services:** [**ğŸ Apple Dictionary**](./docs/en/How-to-use-macOS-system-dictionary-in-Easydict.md), [ğŸ **Apple Translate**](./docs/en/How-to-use-macOS-system-translation-in-Easydict.md), [OpenAI](https://chat.openai.com/), [Gemini](https://gemini.google.com/), [DeepSeek](https://www.deepseek.com/), [Ollama](https://ollama.com/), [Groq](https://groq.com/), [Zhipu AI](https://open.bigmodel.cn/), [GitHub Models](https://github.com/marketplace/models), [DeepL](https://www.deepl.com/translator), [Google](https://translate.google.com), [Youdao](https://www.youdao.com/), [Tencent](https://fanyi.qq.com/), [Bing](https://www.bing.com/translator), [Baidu](https://fanyi.baidu.com/), [Niutrans](https://niutrans.com/), [Caiyun](https://fanyi.caiyunapp.com/), [Alibaba](https://translate.alibaba.com/), [Volcano](https://translate.volcengine.com/translate) and [Doubao](https://www.volcengine.com/docs/82379/1820188).\n\n![Log](https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/Log-1688378715.png)\n\n<table>\n    <td> <img src=\"https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/iShot_2023-05-28_16.32.18-1685262784.png\">\n    <td> <img src=\"https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/iShot_2023-05-28_16.32.26-1685262803.png\">\n</table>\n\n![immerse-1686534718.gif](https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/immerse-1686534718.gif)\n\n## Features\n\n- ğŸš€ Out of the box, automatic language recognition\n- ğŸ–±ï¸ Auto select with mouse and shortcut key\n- ğŸ“¸ OCR screenshot translation and slient screenshot OCR\n- ğŸ”Š Multiple TTS voice services\n- ğŸ“š Support ğŸ [Apple System Dictionary](./docs/en/How-to-use-macOS-system-dictionary-in-Easydict.md) and [System Translation](./docs/en/How-to-use-macOS-system-translation-in-Easydict.md)\n- ğŸŒ Support 20+ translation services (OpenAI, Gemini, DeepL, Google, Ollama, Groq, etc.)\n- ğŸ—£ï¸ Support for 48 languages\n\n**If you like this app, please consider giving it a [Star](https://github.com/tisfeng/Easydict) â­ï¸, thanks! (^-^)**\n\n## Installation\n\n### Homebrew Installation (Recommended)\n\n```bash\nbrew install --cask easydict\n```\n\n### Manual Installation\n\n[Download](https://github.com/tisfeng/Easydict/releases) the latest release.\n\n> [!NOTE]\n> Latest version supports macOS 13.0+, for older systems please use [2.7.2](https://github.com/tisfeng/Easydict/releases/tag/2.7.2)\n\n---\n\n## Usage\n\n| Ways                      | Description                                                                                                                                  | Preview                                                                                                                                        |\n| ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n| Input Translate           | Press the input translate shortcut key (default `âŒ¥ + A`), enter the text to be translated, and `Enter` key to translate          | ![iShot_2023-01-20_11.28.46-1674185354](https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/iShot_2023-01-20_11.28.46-1674185354.gif) |\n| Mouse Select Translate    | The query icon is automatically displayed after the word is selected, and the mouse hovers over it to query                                  | ![iShot_2023-01-20_11.01.35-1674183779](https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/iShot_2023-01-20_11.01.35-1674183779.gif) |\n| Shortcut Select Translate | After selecting the text to be translated, press the shortcut key (default `âŒ¥ + D`)                                                          | ![iShot_2023-01-20_11.24.37-1674185125](https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/iShot_2023-01-20_11.24.37-1674185125.gif) |\n| Screenshot Translate      | Press the screenshot translate shortcut key (default `âŒ¥ + S`) to capture the area to be translated                                           | ![iShot_2023-01-20_11.26.25-1674185209](https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/iShot_2023-01-20_11.26.25-1674185209.gif) |\n| Silent Screenshot OCR     | Press the Silent Screenshot shortcut keyï¼ˆdefault `âŒ¥ + â‡§ + S`ï¼‰to capture the area, the OCR results will be copied directly to the clipboard | ![å±å¹•å½•åˆ¶ 2023-05-20 22 39 11](https://github.com/Jerry23011/Easydict/assets/89069957/c16f3c20-1748-411e-be04-11d8fe0e61af)                     |\n\n---\n\n## Documentation\n\n- ğŸ“– [Complete Usage Guide](./docs/en/GUIDE.md) - Detailed features, configuration and tips\n- ğŸ”§ [Developer Build Guide](./docs/en/GUIDE.md#developer-build) - Build and run from source code\n- ğŸ [How to use macOS System Dictionary](./docs/en/How-to-use-macOS-system-dictionary-in-Easydict.md)\n- ğŸ [How to use macOS System Translation](./docs/en/How-to-use-macOS-system-translation-in-Easydict.md)\n- ğŸŒ [How to translate Easydict](./docs/How-to-translate-Easydict-en.md)\n\n---\n\n## Acknowledgements\n\n- This project was inspired by [saladict](https://github.com/crimx/ext-saladict) and [Bob](https://github.com/ripperhe/Bob), and the initial version was made based on [Bob (GPL-3.0)](https://github.com/1xiaocainiao/Bob). Easydict has made many improvements and optimizations on the original project, and many features and UI are referenced from Bob.\n- Screenshot feature is based on [isee15](https://github.com/isee15)'s [Capture-Screen-For-Multi-Screens-On-Mac](https://github.com/isee15/Capture-Screen-For-Multi-Screens-On-Mac), and optimized on this project.\n- Select text feature is referenced from [PopClip](https://pilotmoon.com/popclip/).\n\n## Statement\n\nEasydict is licensed under the [GPL-3.0](https://github.com/tisfeng/Easydict/blob/main/LICENSE) open source license, which is for learning and communication only. Anyone can get this product and source code for free. If you believe that your legal rights have been violated, please contact the [author](https://github.com/tisfeng) immediately. You can use the source code freely, but you must attach the corresponding license and copyright.\n\n## Sponsor\n\nEasydict is a free and open source project, currently mainly developed and maintained by the author. If you like this project and find it helpful, you can consider sponsoring this project to support it, so that it can go further.\n\nThanks to [@CanglongCl](https://github.com/CanglongCl) for providing the Apple Developer account, which solved the app [signature issue](https://github.com/tisfeng/Easydict/issues/2), allowing more people to use Easydict conveniently.\n\n<a href=\"https://afdian.com/a/tisfeng\"><img width=\"20%\" src=\"https://pic1.afdiancdn.com/static/img/welcome/button-sponsorme.jpg\" alt=\"\"></a>\n\n<div>\n  <img src=\"https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/IMG_4739-1684680971.JPG\" width=\"30%\">\n</div>\n\nThanks to all sponsors for their generous support. For details, please see the [Sponsor List](./docs/en/SPONSOR_LIST.md).\n\n---\n\n## Star History\n\n<a href=\"https://star-history.com/#tisfeng/easydict&Date\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=tisfeng/easydict&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=tisfeng/easydict&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=tisfeng/easydict&type=Date\" />\n  </picture>\n</a>",
      "stars_today": 9
    },
    {
      "id": 105363726,
      "name": "martin",
      "full_name": "maplibre/martin",
      "description": "Blazing fast and lightweight PostGIS, MBtiles and PMtiles tile server, tile generation, and mbtiles tooling.",
      "html_url": "https://github.com/maplibre/martin",
      "stars": 3307,
      "forks": 321,
      "language": "Rust",
      "topics": [
        "hacktoberfest",
        "leaflet",
        "mapbox-gl",
        "mapbox-gl-js",
        "mapbox-vector-tile",
        "maplibre",
        "maplibre-gl-js",
        "maps",
        "mbtiles",
        "pmtiles",
        "postgis",
        "postgresql",
        "rust",
        "vector-tiles",
        "webserver"
      ],
      "created_at": "2017-09-30T10:53:46Z",
      "updated_at": "2026-01-27T19:39:26Z",
      "pushed_at": "2026-01-27T19:35:27Z",
      "open_issues": 107,
      "owner": {
        "login": "maplibre",
        "avatar_url": "https://avatars.githubusercontent.com/u/75709127?v=4"
      },
      "readme": "[![Martin](https://raw.githubusercontent.com/maplibre/martin/main/logo.png)](https://maplibre.org/martin/)\n\n[![Book](https://img.shields.io/badge/docs-Book-informational)](https://maplibre.org/martin)\n[![docs.rs docs](https://docs.rs/martin/badge.svg)](https://docs.rs/martin)\n[![](https://img.shields.io/badge/Slack-%23maplibre--martin-blueviolet?logo=slack)](https://slack.openstreetmap.us/)\n[![GitHub](https://img.shields.io/badge/github-maplibre/martin-8da0cb?logo=github)](https://github.com/maplibre/martin)\n[![crates.io version](https://img.shields.io/crates/v/martin.svg)](https://crates.io/crates/martin)\n[![Security audit](https://github.com/maplibre/martin/workflows/Security%20audit/badge.svg)](https://github.com/maplibre/martin/security)\n[![CI build](https://github.com/maplibre/martin/actions/workflows/ci.yml/badge.svg)](https://github.com/maplibre/martin/actions)\n[![Codecov](https://img.shields.io/codecov/c/github/maplibre/martin)](https://app.codecov.io/gh/maplibre/martin)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/11613/badge)](https://www.bestpractices.dev/projects/11613)\n\nMartin is a tile server and a set of tools able to generate vector tiles on the fly\nfrom large `PostgreSQL` databases, and serve tiles from `PMTiles` and `MBTiles` files. Martin optimizes for speed and heavy traffic, and is written in [Rust](https://github.com/rust-lang/rust).\n\n## Features\n\n* Serve [vector tiles](https://github.com/mapbox/vector-tile-spec) from\n  * [PostGIS](https://github.com/postgis/postgis) databases, automatically discovering compatible tables and functions\n  * [PMTile](https://protomaps.com/blog/pmtiles-v3-whats-new), both local files and over HTTP\n  * [MBTile](https://github.com/mapbox/mbtiles-spec) files\n* [Combine](https://maplibre.org/martin/sources-composite.html) multiple tile sources into one\n* Serve [styles](https://maplibre.org/martin/sources-styles.html) and generate [sprites](https://maplibre.org/martin/sources-sprites.html) or [font glyphs](https://maplibre.org/martin/sources-fonts.html) on the fly\n* Generate tiles in bulk from any Martin-supported sources into an `MBTiles` file with [martin-cp](https://maplibre.org/martin/martin-cp.html) tool\n* Examine, copy, validate, compare, and apply diffs between `MBTiles` files with [mbtiles](https://maplibre.org/martin/tools.html#mbtiles) tool\n\n## Documentation\n\n* [Quick Start](https://maplibre.org/martin/quick-start.html)\n* [Installation](https://maplibre.org/martin/installation.html)\n* Running with [CLI](https://maplibre.org/martin/run-with-cli.html)\n  or [configuration file](https://maplibre.org/martin/config-file.html)\n* [Usage and API](https://maplibre.org/martin/using.html)\n\n## Getting Involved\n\nJoin the `#maplibre-martin` slack channel at OSMUS -- automatic invite is at <https://slack.openstreetmap.us/>\n\n## Contributing\n\nLike any open source project, Martin welcomes contributions from anyone who wants to help improve it.\n\n* See [Development Guide](https://maplibre.org/martin/development.html) for setup\n* Use `just help` for common commands\n* Check [help wanted](https://github.com/maplibre/martin/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22) issues\n\n## License\n\nLicensed under either of\n\n* Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or <http://www.apache.org/licenses/LICENSE-2.0>)\n* MIT license ([LICENSE-MIT](LICENSE-MIT) or <http://opensource.org/licenses/MIT>)\n  at your option.\n\n### Contribution\n\nUnless you explicitly state otherwise, any contribution intentionally\nsubmitted for inclusion in the work by you, as defined in the\nApache-2.0 license, shall be dual licensed as above, without any\nadditional terms or conditions.\n",
      "stars_today": 9
    },
    {
      "id": 994992847,
      "name": "tempo",
      "full_name": "tempoxyz/tempo",
      "description": "the blockchain for payments",
      "html_url": "https://github.com/tempoxyz/tempo",
      "stars": 722,
      "forks": 192,
      "language": "Rust",
      "topics": [],
      "created_at": "2025-06-02T19:55:49Z",
      "updated_at": "2026-01-28T01:52:31Z",
      "pushed_at": "2026-01-28T02:13:23Z",
      "open_issues": 170,
      "owner": {
        "login": "tempoxyz",
        "avatar_url": "https://avatars.githubusercontent.com/u/211589300?v=4"
      },
      "readme": "<br>\n<br>\n\n<p align=\"center\">\n  <a href=\"https://tempo.xyz\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/tempoxyz/.github/refs/heads/main/assets/combomark-dark.svg\">\n      <img alt=\"tempo combomark\" src=\"https://raw.githubusercontent.com/tempoxyz/.github/refs/heads/main/assets/combomark-bright.svg\" width=\"auto\" height=\"120\">\n    </picture>\n  </a>\n</p>\n\n<br>\n<br>\n\n# Tempo\n\nThe blockchain for payments at scale.\n\n[Tempo](https://docs.tempo.xyz/) is a blockchain designed specifically for stablecoin payments. Its architecture focuses on high throughput, low cost, and features that financial institutions, payment service providers, and fintech platforms expect from modern payment infrastructure.\n\nYou can get started today by integrating with the [Tempo testnet](https://docs.tempo.xyz/quickstart/integrate-tempo), [building on Tempo](https://docs.tempo.xyz/guide/use-accounts), [running a Tempo node](https://docs.tempo.xyz/guide/node), reading the [Tempo protocol specs](https://docs.tempo.xyz/protocol) or by [building with Tempo SDKs](https://docs.tempo.xyz/sdk).\n\n## What makes Tempo different\n\n- [TIPâ€‘20 token standard](https://docs.tempo.xyz/protocol/tip20/overview) (enshrined ERCâ€‘20 extensions)\n\n  - Predictable payment throughput via dedicated payment lanes reserved for TIPâ€‘20 transfers (eliminates noisyâ€‘neighbor contention).\n  - Native reconciliation with onâ€‘transfer memos and commitment patterns (hash/locator) for offâ€‘chain PII and large data.\n  - Builtâ€‘in compliance through [TIPâ€‘403 Policy Registry](https://docs.tempo.xyz/protocol/tip403/overview): single policy shared across multiple tokens, updated once and enforced everywhere.\n\n- Low, predictable fees in [stablecoins](https://docs.tempo.xyz/learn/stablecoins)\n\n  - Users pay gas directly in USD-stablecoins at launch; the [Fee AMM](https://docs.tempo.xyz/protocol/fees/fee-amm#fee-amm-overview) automatically converts to the validatorâ€™s preferred stablecoin.\n  - TIPâ€‘20 transfers target subâ€‘millidollar costs (<$0.001).\n\n- [Tempo Transactions](https://docs.tempo.xyz/guide/tempo-transaction) (native â€œsmart accountsâ€)\n\n  - Batched payments: atomic multiâ€‘operation payouts (payroll, settlements, refunds).\n  - Fee sponsorship: apps can pay users' gas to streamline onboarding and flows.\n  - Scheduled payments: protocolâ€‘level time windows for recurring and timed disbursements.\n  - Modern authentication: passkeys via WebAuthn/P256 (biometric signâ€‘in, secure enclave, crossâ€‘device sync).\n\n- Performance and finality\n\n  - Built on the [Reth SDK](https://github.com/paradigmxyz/reth), the most performant and flexible EVM (Ethereum Virtual Machine) execution client.\n  - Simplex Consensus (via [Commonware](https://commonware.xyz/)): fast, subâ€‘second finality in normal conditions; graceful degradation under adverse networks.\n\n- Coming soon\n\n  - Onâ€‘chain FX and nonâ€‘USD stablecoin support for direct onâ€‘chain liquidity; pay fees in more currencies.\n  - Native private token standard: optâ€‘in privacy for balances/transfers coexisting with issuer compliance and auditability.\n\n## What makes Tempo familiar\n\n- Fully compatible with the Ethereum Virtual Machine (EVM), targeting the Osaka hardfork.\n- Deploy and interact with smart contracts using the same tools, languages, and frameworks used on Ethereum, such as Solidity, Foundry, and Hardhat.\n- All Ethereum JSON-RPC methods work out of the box.\n\nWhile the execution environment mirrors Ethereum's, Tempo introduces some differences optimized for payments, described [here](https://docs.tempo.xyz/quickstart/evm-compatibility).\n\n## Getting Started\n\n### As a user\n\nYou can connect to Tempo's public testnet using the following details:\n\n| Property           | Value                              |\n| ------------------ | ---------------------------------- |\n| **Network Name**   | Tempo Testnet (Moderato)           |\n| **Currency**       | `USD`                              |\n| **Chain ID**       | `42431`                            |\n| **HTTP URL**       | `https://rpc.moderato.tempo.xyz`   |\n| **WebSocket URL**  | `wss://rpc.moderato.tempo.xyz`     |\n| **Block Explorer** | `https://explore.tempo.xyz`        |\n\nNext, grab some stablecoins to test with from Tempo's [Faucet](https://docs.tempo.xyz/quickstart/faucet#faucet).\n\nAlternatively, use [`cast`](https://github.com/tempoxyz/tempo-foundry):\n\n```bash\ncast rpc tempo_fundAddress <ADDRESS> --rpc-url https://rpc.moderato.tempo.xyz\n```\n\n### As an operator\n\nWe provide three different installation paths: installing a pre-built binary, building from source or using our provided Docker image.\n\n- [Pre-built Binary](https://docs.tempo.xyz/guide/node/installation#pre-built-binary)\n- [Build from Source](https://docs.tempo.xyz/guide/node/installation#build-from-source)\n- [Docker](https://docs.tempo.xyz/guide/node/installation#docker)\n\nSee the [Tempo documentation](https://docs.tempo.xyz/guide/node) for instructions on how to install and run Tempo.\n\n### As a developer\n\nTempo has several SDKs to help you get started building on Tempo:\n\n- [TypeScript](https://docs.tempo.xyz/sdk/typescript)\n- [Rust](https://docs.tempo.xyz/sdk/rust)\n- [Go](https://docs.tempo.xyz/sdk/go)\n- [Foundry](https://docs.tempo.xyz/sdk/foundry)\n\nWant to contribute?\n\nFirst, clone the repository:\n\n```\ngit clone https://github.com/tempoxyz/tempo\ncd tempo\n```\n\nNext, install [`just`](https://github.com/casey/just?tab=readme-ov-file#packages).\n\nInstall the dependencies:\n\n```bash\njust\n```\n\nBuild Tempo:\n\n```bash\njust build-all\n```\n\nRun the tests:\n\n```bash\ncargo nextest run\n```\n\nStart a `localnet`:\n\n```bash\njust localnet\n```\n\n## Contributing\n\nOur contributor guidelines can be found in [`CONTRIBUTING.md`](https://github.com/tempoxyz/tempo?tab=contributing-ov-file).\n\n## Security\n\nSee [`SECURITY.md`](https://github.com/tempoxyz/tempo?tab=security-ov-file). Note: Tempo is still undergoing audit and does not have an active bug bounty. Submissions will not be eligible for a bounty until audits have concluded.\n\n## License\n\nLicensed under either of [Apache License](./LICENSE-APACHE), Version\n2.0 or [MIT License](./LICENSE-MIT) at your option.\n\nUnless you explicitly state otherwise, any contribution intentionally submitted\nfor inclusion in these crates by you, as defined in the Apache-2.0 license,\nshall be dual licensed as above, without any additional terms or conditions.\n",
      "stars_today": 9
    },
    {
      "id": 23357588,
      "name": "protobuf",
      "full_name": "protocolbuffers/protobuf",
      "description": "Protocol Buffers - Google's data interchange format",
      "html_url": "https://github.com/protocolbuffers/protobuf",
      "stars": 70389,
      "forks": 16016,
      "language": "C++",
      "topics": [
        "marshalling",
        "protobuf",
        "protobuf-runtime",
        "protoc",
        "protocol-buffers",
        "protocol-compiler",
        "rpc",
        "serialization"
      ],
      "created_at": "2014-08-26T15:52:15Z",
      "updated_at": "2026-01-28T01:34:06Z",
      "pushed_at": "2026-01-28T02:01:03Z",
      "open_issues": 231,
      "owner": {
        "login": "protocolbuffers",
        "avatar_url": "https://avatars.githubusercontent.com/u/26310541?v=4"
      },
      "readme": "Protocol Buffers - Google's data interchange format\n===================================================\n\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/protocolbuffers/protobuf/badge)](https://securityscorecards.dev/viewer/?uri=github.com/protocolbuffers/protobuf)\n\nCopyright 2008 Google LLC\n\nOverview\n--------\n\nProtocol Buffers (a.k.a., protobuf) are Google's language-neutral,\nplatform-neutral, extensible mechanism for serializing structured data. You\ncan learn more about it in [protobuf's documentation](https://protobuf.dev).\n\nThis README file contains protobuf installation instructions. To install\nprotobuf, you need to install the protocol compiler (used to compile .proto\nfiles) and the protobuf runtime for your chosen programming language.\n\nWorking With Protobuf Source Code\n---------------------------------\n\nMost users will find working from\n[supported releases](https://github.com/protocolbuffers/protobuf/releases) to be\nthe easiest path.\n\nIf you choose to work from the head revision of the main branch your build will\noccasionally be broken by source-incompatible changes and insufficiently-tested\n(and therefore broken) behavior.\n\nIf you are using C++ or otherwise need to build protobuf from source as a part\nof your project, you should pin to a release commit on a release branch.\n\nThis is because even release branches can experience some instability in between\nrelease commits.\n\n### Bazel with Bzlmod\n\nProtobuf supports\n[Bzlmod](https://bazel.build/external/module) with Bazel 7 +.\nUsers should specify a dependency on protobuf in their MODULE.bazel file as\nfollows.\n\n```\nbazel_dep(name = \"protobuf\", version = <VERSION>)\n```\n\nUsers can optionally override the repo name, such as for compatibility with\nWORKSPACE.\n\n```\nbazel_dep(name = \"protobuf\", version = <VERSION>, repo_name = \"com_google_protobuf\")\n```\n\n### Bazel with WORKSPACE\n\nUsers can also add the following to their legacy\n[WORKSPACE](https://bazel.build/external/overview#workspace-system) file.\n\nNote that with the release of 30.x there are a few more load statements to\nproperly set up rules_java and rules_python.\n\n```\nhttp_archive(\n    name = \"com_google_protobuf\",\n    strip_prefix = \"protobuf-VERSION\",\n    sha256 = ...,\n    url = ...,\n)\n\nload(\"@com_google_protobuf//:protobuf_deps.bzl\", \"protobuf_deps\")\n\nprotobuf_deps()\n\nload(\"@rules_java//java:rules_java_deps.bzl\", \"rules_java_dependencies\")\n\nrules_java_dependencies()\n\nload(\"@rules_java//java:repositories.bzl\", \"rules_java_toolchains\")\n\nrules_java_toolchains()\n\nload(\"@rules_python//python:repositories.bzl\", \"py_repositories\")\n\npy_repositories()\n```\n\nProtobuf Compiler Installation\n------------------------------\n\nThe protobuf compiler is written in C++. If you are using C++, please follow\nthe [C++ Installation Instructions](src/README.md) to install protoc along\nwith the C++ runtime.\n\nFor non-C++ users, the simplest way to install the protocol compiler is to\ndownload a pre-built binary from our [GitHub release page](https://github.com/protocolbuffers/protobuf/releases).\n\nIn the downloads section of each release, you can find pre-built binaries in\nzip packages: `protoc-$VERSION-$PLATFORM.zip`. It contains the protoc binary\nas well as a set of standard `.proto` files distributed along with protobuf.\n\nIf you are looking for an old version that is not available in the release\npage, check out the [Maven repository](https://repo1.maven.org/maven2/com/google/protobuf/protoc/).\n\nThese pre-built binaries are only provided for released versions. If you want\nto use the github main version at HEAD, or you need to modify protobuf code,\nor you are using C++, it's recommended to build your own protoc binary from\nsource.\n\nIf you would like to build protoc binary from source, see the [C++ Installation Instructions](src/README.md).\n\nProtobuf Runtime Installation\n-----------------------------\n\nProtobuf supports several different programming languages. For each programming\nlanguage, you can find instructions in the corresponding source directory about\nhow to install protobuf runtime for that specific language:\n\n| Language                             | Source                                                      |\n|--------------------------------------|-------------------------------------------------------------|\n| C++ (include C++ runtime and protoc) | [src](src)                                                  |\n| Java                                 | [java](java)                                                |\n| Python                               | [python](python)                                            |\n| Objective-C                          | [objectivec](objectivec)                                    |\n| C#                                   | [csharp](csharp)                                            |\n| Ruby                                 | [ruby](ruby)                                                |\n| Go                                   | [protocolbuffers/protobuf-go](https://github.com/protocolbuffers/protobuf-go)|\n| PHP                                  | [php](php)                                                  |\n| Dart                                 | [dart-lang/protobuf](https://github.com/dart-lang/protobuf) |\n| JavaScript                           | [protocolbuffers/protobuf-javascript](https://github.com/protocolbuffers/protobuf-javascript)|\n\nQuick Start\n-----------\n\nThe best way to learn how to use protobuf is to follow the [tutorials in our\ndeveloper guide](https://protobuf.dev/getting-started).\n\nIf you want to learn from code examples, take a look at the examples in the\n[examples](examples) directory.\n\nDocumentation\n-------------\n\nThe complete documentation is available at the [Protocol Buffers doc site](https://protobuf.dev).\n\nSupport Policy\n--------------\n\nRead about our [version support policy](https://protobuf.dev/version-support/)\nto stay current on support timeframes for the language libraries.\n\nDeveloper Community\n-------------------\n\nTo be alerted to upcoming changes in Protocol Buffers and connect with protobuf developers and users,\n[join the Google Group](https://groups.google.com/g/protobuf).\n",
      "stars_today": 8
    },
    {
      "id": 11171548,
      "name": "json",
      "full_name": "nlohmann/json",
      "description": "JSON for Modern C++",
      "html_url": "https://github.com/nlohmann/json",
      "stars": 48688,
      "forks": 7304,
      "language": "C++",
      "topics": [
        "bson",
        "cbor",
        "header-only",
        "json",
        "json-diff",
        "json-merge-patch",
        "json-parser",
        "json-patch",
        "json-pointer",
        "json-serialization",
        "messagepack",
        "msgpack",
        "rfc-6901",
        "rfc-6902",
        "rfc-7049",
        "rfc-7159",
        "rfc-8259",
        "stl-containers",
        "ubjson"
      ],
      "created_at": "2013-07-04T08:47:49Z",
      "updated_at": "2026-01-28T02:05:42Z",
      "pushed_at": "2026-01-27T16:53:46Z",
      "open_issues": 96,
      "owner": {
        "login": "nlohmann",
        "avatar_url": "https://avatars.githubusercontent.com/u/159488?v=4"
      },
      "readme": "[![JSON for Modern C++](docs/mkdocs/docs/images/json.gif)](https://github.com/nlohmann/json/releases)\n\n[![Build Status](https://ci.appveyor.com/api/projects/status/1acb366xfyg3qybk/branch/develop?svg=true)](https://ci.appveyor.com/project/nlohmann/json)\n[![Ubuntu](https://github.com/nlohmann/json/workflows/Ubuntu/badge.svg)](https://github.com/nlohmann/json/actions?query=workflow%3AUbuntu)\n[![macOS](https://github.com/nlohmann/json/workflows/macOS/badge.svg)](https://github.com/nlohmann/json/actions?query=workflow%3AmacOS)\n[![Windows](https://github.com/nlohmann/json/workflows/Windows/badge.svg)](https://github.com/nlohmann/json/actions?query=workflow%3AWindows)\n[![Coverage Status](https://coveralls.io/repos/github/nlohmann/json/badge.svg?branch=develop)](https://coveralls.io/github/nlohmann/json?branch=develop)\n[![Coverity Scan Build Status](https://scan.coverity.com/projects/5550/badge.svg)](https://scan.coverity.com/projects/nlohmann-json)\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/e0d1a9d5d6fd46fcb655c4cb930bb3e8)](https://app.codacy.com/gh/nlohmann/json/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)\n[![Cirrus CI](https://api.cirrus-ci.com/github/nlohmann/json.svg)](https://cirrus-ci.com/github/nlohmann/json)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/json.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:json)\n[![Try online](https://img.shields.io/badge/try-online-blue.svg)](https://wandbox.org/permlink/1mp10JbaANo6FUc7)\n[![Documentation](https://img.shields.io/badge/docs-mkdocs-blue.svg)](https://json.nlohmann.me)\n[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/nlohmann/json/master/LICENSE.MIT)\n[![GitHub Releases](https://img.shields.io/github/release/nlohmann/json.svg)](https://github.com/nlohmann/json/releases)\n[![Packaging status](https://repology.org/badge/tiny-repos/nlohmann-json.svg)](https://repology.org/project/nlohmann-json/versions)\n[![GitHub Downloads](https://img.shields.io/github/downloads/nlohmann/json/total)](https://github.com/nlohmann/json/releases)\n[![GitHub Issues](https://img.shields.io/github/issues/nlohmann/json.svg)](https://github.com/nlohmann/json/issues)\n[![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/nlohmann/json.svg)](https://isitmaintained.com/project/nlohmann/json \"Average time to resolve an issue\")\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/289/badge)](https://bestpractices.coreinfrastructure.org/projects/289)\n[![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/nlohmann/json/badge)](https://scorecard.dev/viewer/?uri=github.com/nlohmann/json)\n[![Backup Status](https://app.cloudback.it/badge/nlohmann/json)](https://cloudback.it)\n[![GitHub Sponsors](https://img.shields.io/badge/GitHub-Sponsors-ff69b4)](https://github.com/sponsors/nlohmann)\n[![REUSE status](https://api.reuse.software/badge/github.com/nlohmann/json)](https://api.reuse.software/info/github.com/nlohmann/json)\n[![Discord](https://img.shields.io/discord/1003743314341793913)](https://discord.gg/6mrGXKvX7y)\n\n- [Design goals](#design-goals)\n- [Sponsors](#sponsors)\n- [Support](#support) ([documentation](https://json.nlohmann.me), [FAQ](https://json.nlohmann.me/home/faq/), [discussions](https://github.com/nlohmann/json/discussions), [API](https://json.nlohmann.me/api/basic_json/), [bug issues](https://github.com/nlohmann/json/issues))\n- [Quick reference](#quick-reference)\n- [Examples](#examples)\n  - [Read JSON from a file](#read-json-from-a-file)\n  - [Creating `json` objects from JSON literals](#creating-json-objects-from-json-literals)\n  - [JSON as a first-class data type](#json-as-a-first-class-data-type)\n  - [Serialization / Deserialization](#serialization--deserialization)\n  - [STL-like access](#stl-like-access)\n  - [Conversion from STL containers](#conversion-from-stl-containers)\n  - [JSON Pointer and JSON Patch](#json-pointer-and-json-patch)\n  - [JSON Merge Patch](#json-merge-patch)\n  - [Implicit conversions](#implicit-conversions)\n  - [Conversions to/from arbitrary types](#arbitrary-types-conversions)\n  - [Specializing enum conversion](#specializing-enum-conversion)\n  - [Binary formats (BSON, CBOR, MessagePack, UBJSON, and BJData)](#binary-formats-bson-cbor-messagepack-ubjson-and-bjdata)\n- [Customers](#customers)\n- [Supported compilers](#supported-compilers)\n- [Integration](#integration)\n  - [CMake](#cmake)\n  - [Package Managers](#package-managers)\n  - [Pkg-config](#pkg-config)\n- [License](#license)\n- [Contact](#contact)\n- [Thanks](#thanks)\n- [Used third-party tools](#used-third-party-tools)\n- [Notes](#notes)\n- [Execute unit tests](#execute-unit-tests)\n\n## Design goals\n\nThere are myriads of [JSON](https://json.org) libraries out there, and each may even have its reason to exist. Our class had these design goals:\n\n- **Intuitive syntax**. In languages such as Python, JSON feels like a first-class data type. We used all the operator magic of modern C++ to achieve the same feeling in your code. Check out the [examples below](#examples) and you'll know what I mean.\n\n- **Trivial integration**. Our whole code consists of a single header file [`json.hpp`](https://github.com/nlohmann/json/blob/develop/single_include/nlohmann/json.hpp). That's it. No library, no subproject, no dependencies, no complex build system. The class is written in vanilla C++11. All in all, everything should require no adjustment of your compiler flags or project settings. The library is also included in all popular [package managers](https://json.nlohmann.me/integration/package_managers/).\n\n- **Serious testing**. Our code is heavily [unit-tested](https://github.com/nlohmann/json/tree/develop/tests/src) and covers [100%](https://coveralls.io/r/nlohmann/json) of the code, including all exceptional behavior. Furthermore, we checked with [Valgrind](https://valgrind.org) and the [Clang Sanitizers](https://clang.llvm.org/docs/index.html) that there are no memory leaks. [Google OSS-Fuzz](https://github.com/google/oss-fuzz/tree/master/projects/json) additionally runs fuzz tests against all parsers 24/7, effectively executing billions of tests so far. To maintain high quality, the project is following the [Core Infrastructure Initiative (CII) best practices](https://bestpractices.coreinfrastructure.org/projects/289). See the [quality assurance](https://json.nlohmann.me/community/quality_assurance) overview documentation.\n\nOther aspects were not so important to us:\n\n- **Memory efficiency**. Each JSON object has an overhead of one pointer (the maximal size of a union) and one enumeration element (1 byte). The default generalization uses the following C++ data types: `std::string` for strings, `int64_t`, `uint64_t` or `double` for numbers, `std::map` for objects, `std::vector` for arrays, and `bool` for Booleans. However, you can template the generalized class `basic_json` to your needs.\n\n- **Speed**. There are certainly [faster JSON libraries](https://github.com/miloyip/nativejson-benchmark#parsing-time) out there. However, if your goal is to speed up your development by adding JSON support with a single header, then this library is the way to go. If you know how to use a `std::vector` or `std::map`, you are already set.\n\nSee the [contribution guidelines](https://github.com/nlohmann/json/blob/master/.github/CONTRIBUTING.md#please-dont) for more information.\n\n## Sponsors\n\nYou can sponsor this library at [GitHub Sponsors](https://github.com/sponsors/nlohmann).\n\n### :raising_hand: Priority Sponsor\n\n- [Martti Laine](https://github.com/codeclown)\n- [Paul Harrington](https://github.com/phrrngtn)\n- [Mercedes-Benz Group](https://github.com/mercedes-benz)\n\n### :label: Named Sponsors\n\n- [Michael Hartmann](https://github.com/reFX-Mike)\n- [Stefan Hagen](https://github.com/sthagen)\n- [Steve Sperandeo](https://github.com/homer6)\n- [Robert Jefe LindstÃ¤dt](https://github.com/eljefedelrodeodeljefe)\n- [Steve Wagner](https://github.com/ciroque)\n- [Lion Yang](https://github.com/LionNatsu)\n\n### Further support\n\nThe development of the library is further supported by JetBrains by providing free access to their IDE tools.\n\n[![JetBrains logo.](https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.svg)](https://jb.gg/OpenSourceSupport)\n\nThanks everyone!\n\n## Support\n\n:question: If you have a **question**, please check if it is already answered in the [**FAQ**](https://json.nlohmann.me/home/faq/) or the [**Q&A**](https://github.com/nlohmann/json/discussions/categories/q-a) section. If not, please [**ask a new question**](https://github.com/nlohmann/json/discussions/new) there.\n\n:books: If you want to **learn more** about how to use the library, check out the rest of the [**README**](#examples), have a look at [**code examples**](https://github.com/nlohmann/json/tree/develop/docs/mkdocs/docs/examples), or browse through the [**help pages**](https://json.nlohmann.me).\n\n:construction: If you want to understand the **API** better, check out the [**API Reference**](https://json.nlohmann.me/api/basic_json/) or have a look at the [quick reference](#quick-reference) below.\n\n:bug: If you found a **bug**, please check the [**FAQ**](https://json.nlohmann.me/home/faq/) if it is a known issue or the result of a design decision. Please also have a look at the [**issue list**](https://github.com/nlohmann/json/issues) before you [**create a new issue**](https://github.com/nlohmann/json/issues/new/choose). Please provide as much information as possible to help us understand and reproduce your issue.\n\nThere is also a [**docset**](https://github.com/Kapeli/Dash-User-Contributions/tree/master/docsets/JSON_for_Modern_C%2B%2B) for the documentation browsers [Dash](https://kapeli.com/dash), [Velocity](https://velocity.silverlakesoftware.com), and [Zeal](https://zealdocs.org) that contains the full [documentation](https://json.nlohmann.me) as an offline resource.\n\n## Quick reference\n\n- **Constructors** [basic_json](https://json.nlohmann.me/api/basic_json/basic_json), [array](https://json.nlohmann.me/api/basic_json/array), [binary](https://json.nlohmann.me/api/basic_json/binary), [object](https://json.nlohmann.me/api/basic_json/object)\n- **Object inspection**: [type](https://json.nlohmann.me/api/basic_json/type), [operator value_t](https://json.nlohmann.me/api/basic_json/operator_value_t), [type_name](https://json.nlohmann.me/api/basic_json/type_name), [is_primitive](https://json.nlohmann.me/api/basic_json/is_primitive), [is_structured](https://json.nlohmann.me/api/basic_json/is_structured), [is_null](https://json.nlohmann.me/api/basic_json/is_null), [is_boolean](https://json.nlohmann.me/api/basic_json/is_boolean), [is_number](https://json.nlohmann.me/api/basic_json/is_number), [is_number_integer](https://json.nlohmann.me/api/basic_json/is_number_integer), [is_number_unsigned](https://json.nlohmann.me/api/basic_json/is_number_unsigned), [is_number_float](https://json.nlohmann.me/api/basic_json/is_number_float), [is_object](https://json.nlohmann.me/api/basic_json/is_object), [is_array](https://json.nlohmann.me/api/basic_json/is_array), [is_string](https://json.nlohmann.me/api/basic_json/is_string), [is_binary](https://json.nlohmann.me/api/basic_json/is_binary), [is_discarded](https://json.nlohmann.me/api/basic_json/is_discarded)\n- **Value access**; [get](https://json.nlohmann.me/api/basic_json/get), [get_to](https://json.nlohmann.me/api/basic_json/get_to), [get_ptr](https://json.nlohmann.me/api/basic_json/get_ptr), [get_ref](https://json.nlohmann.me/api/basic_json/get_ref), [operator ValueType](https://json.nlohmann.me/api/basic_json/operator_ValueType), [get_binary](https://json.nlohmann.me/api/basic_json/get_binary)\n- **Element access**: [at](https://json.nlohmann.me/api/basic_json/at), [operator[]](https://json.nlohmann.me/api/basic_json/operator[]), [value](https://json.nlohmann.me/api/basic_json/value), [front](https://json.nlohmann.me/api/basic_json/front), [back](https://json.nlohmann.me/api/basic_json/back)\n- **Lookup**: [find](https://json.nlohmann.me/api/basic_json/find), [count](https://json.nlohmann.me/api/basic_json/count), [contains](https://json.nlohmann.me/api/basic_json/contains)\n- **Iterators**: [begin](https://json.nlohmann.me/api/basic_json/begin), [cbegin](https://json.nlohmann.me/api/basic_json/cbegin), [end](https://json.nlohmann.me/api/basic_json/end), [cend](https://json.nlohmann.me/api/basic_json/cend), [rbegin](https://json.nlohmann.me/api/basic_json/rbegin), [rend](https://json.nlohmann.me/api/basic_json/rend), [crbegin](https://json.nlohmann.me/api/basic_json/crbegin), [crend](https://json.nlohmann.me/api/basic_json/crend), [items](https://json.nlohmann.me/api/basic_json/items)\n- **Capacity**: [empty](https://json.nlohmann.me/api/basic_json/empty), [size](https://json.nlohmann.me/api/basic_json/size), [max_size](https://json.nlohmann.me/api/basic_json/max_size)\n- **Modifiers**: [clear](https://json.nlohmann.me/api/basic_json/clear), [push_back](https://json.nlohmann.me/api/basic_json/push_back), [operator+=](https://json.nlohmann.me/api/basic_json/operator+=), [emplace_back](https://json.nlohmann.me/api/basic_json/emplace_back), [emplace](https://json.nlohmann.me/api/basic_json/emplace), [erase](https://json.nlohmann.me/api/basic_json/erase), [insert](https://json.nlohmann.me/api/basic_json/insert), [update](https://json.nlohmann.me/api/basic_json/update), [swap](https://json.nlohmann.me/api/basic_json/swap)\n- **Lexicographical comparison operators**: [operator==](https://json.nlohmann.me/api/basic_json/operator_eq), [operator!=](https://json.nlohmann.me/api/basic_json/operator_ne), [operator<](https://json.nlohmann.me/api/basic_json/operator_lt), [operator>](https://json.nlohmann.me/api/basic_json/operator_gt), [operator<=](https://json.nlohmann.me/api/basic_json/operator_le), [operator>=](https://json.nlohmann.me/api/basic_json/operator_ge), [operator<=>](https://json.nlohmann.me/api/basic_json/operator_spaceship)\n- **Serialization / Dumping**: [dump](https://json.nlohmann.me/api/basic_json/dump)\n- **Deserialization / Parsing**: [parse](https://json.nlohmann.me/api/basic_json/parse), [accept](https://json.nlohmann.me/api/basic_json/accept), [sax_parse](https://json.nlohmann.me/api/basic_json/sax_parse)\n- **JSON Pointer functions**: [flatten](https://json.nlohmann.me/api/basic_json/flatten), [unflatten](https://json.nlohmann.me/api/basic_json/unflatten)\n- **JSON Patch functions**: [patch](https://json.nlohmann.me/api/basic_json/patch), [patch_inplace](https://json.nlohmann.me/api/basic_json/patch_inplace), [diff](https://json.nlohmann.me/api/basic_json/diff), [merge_patch](https://json.nlohmann.me/api/basic_json/merge_patch)\n- **Static functions**: [meta](https://json.nlohmann.me/api/basic_json/meta), [get_allocator](https://json.nlohmann.me/api/basic_json/get_allocator)\n- **Binary formats**: [from_bjdata](https://json.nlohmann.me/api/basic_json/from_bjdata), [from_bson](https://json.nlohmann.me/api/basic_json/from_bson), [from_cbor](https://json.nlohmann.me/api/basic_json/from_cbor), [from_msgpack](https://json.nlohmann.me/api/basic_json/from_msgpack), [from_ubjson](https://json.nlohmann.me/api/basic_json/from_ubjson), [to_bjdata](https://json.nlohmann.me/api/basic_json/to_bjdata), [to_bson](https://json.nlohmann.me/api/basic_json/to_bson), [to_cbor](https://json.nlohmann.me/api/basic_json/to_cbor), [to_msgpack](https://json.nlohmann.me/api/basic_json/to_msgpack), [to_ubjson](https://json.nlohmann.me/api/basic_json/to_ubjson)\n- **Non-member functions**: [operator<<](https://json.nlohmann.me/api/operator_ltlt/), [operator>>](https://json.nlohmann.me/api/operator_gtgt/), [to_string](https://json.nlohmann.me/api/basic_json/to_string)\n- **Literals**: [operator\"\"_json](https://json.nlohmann.me/api/operator_literal_json)\n- **Helper classes**: [std::hash&lt;basic_json&gt;](https://json.nlohmann.me/api/basic_json/std_hash), [std::swap&lt;basic_json&gt;](https://json.nlohmann.me/api/basic_json/std_swap)\n\n[**Full API documentation**](https://json.nlohmann.me/api/basic_json/)\n\n## Examples\n\nHere are some examples to give you an idea how to use the class.\n\nBesides the examples below, you may want to:\n\nâ†’ Check the [documentation](https://json.nlohmann.me/)\\\nâ†’ Browse the [standalone example files](https://github.com/nlohmann/json/tree/develop/docs/mkdocs/docs/examples)\\\nâ†’ Read the full [API Documentation](https://json.nlohmann.me/api/basic_json/) with self-contained examples for every function\n\n### Read JSON from a file\n\nThe `json` class provides an API for manipulating a JSON value. To create a `json` object by reading a JSON file:\n\n```cpp\n#include <fstream>\n#include <nlohmann/json.hpp>\nusing json = nlohmann::json;\n\n// ...\n\nstd::ifstream f(\"example.json\");\njson data = json::parse(f);\n```\n\nIf using modules (enabled with `NLOHMANN_JSON_BUILD_MODULES`), this example becomes:\n```cpp\nimport std;\nimport nlohmann.json;\n\nusing json = nlohmann::json;\n\n// ...\n\nstd::ifstream f(\"example.json\");\njson data = json::parse(f);\n```\n\n### Creating `json` objects from JSON literals\n\nAssume you want to create hard-code this literal JSON value in a file, as a `json` object:\n\n```json\n{\n  \"pi\": 3.141,\n  \"happy\": true\n}\n```\n\nThere are various options:\n\n```cpp\n// Using (raw) string literals and json::parse\njson ex1 = json::parse(R\"(\n  {\n    \"pi\": 3.141,\n    \"happy\": true\n  }\n)\");\n\n// Using user-defined (raw) string literals\nusing namespace nlohmann::literals;\njson ex2 = R\"(\n  {\n    \"pi\": 3.141,\n    \"happy\": true\n  }\n)\"_json;\n\n// Using initializer lists\njson ex3 = {\n  {\"happy\", true},\n  {\"pi\", 3.141},\n};\n```\n\n### JSON as a first-class data type\n\nHere are some examples to give you an idea how to use the class.\n\nAssume you want to create the JSON object\n\n```json\n{\n  \"pi\": 3.141,\n  \"happy\": true,\n  \"name\": \"Niels\",\n  \"nothing\": null,\n  \"answer\": {\n    \"everything\": 42\n  },\n  \"list\": [1, 0, 2],\n  \"object\": {\n    \"currency\": \"USD\",\n    \"value\": 42.99\n  }\n}\n```\n\nWith this library, you could write:\n\n```cpp\n// create an empty structure (null)\njson j;\n\n// add a number stored as double (note the implicit conversion of j to an object)\nj[\"pi\"] = 3.141;\n\n// add a Boolean stored as bool\nj[\"happy\"] = true;\n\n// add a string stored as std::string\nj[\"name\"] = \"Niels\";\n\n// add another null object by passing nullptr\nj[\"nothing\"] = nullptr;\n\n// add an object inside the object\nj[\"answer\"][\"everything\"] = 42;\n\n// add an array stored as std::vector (using an initializer list)\nj[\"list\"] = { 1, 0, 2 };\n\n// add another object (using an initializer list of pairs)\nj[\"object\"] = { {\"currency\", \"USD\"}, {\"value\", 42.99} };\n\n// instead, you could also write (which looks very similar to the JSON above)\njson j2 = {\n  {\"pi\", 3.141},\n  {\"happy\", true},\n  {\"name\", \"Niels\"},\n  {\"nothing\", nullptr},\n  {\"answer\", {\n    {\"everything\", 42}\n  }},\n  {\"list\", {1, 0, 2}},\n  {\"object\", {\n    {\"currency\", \"USD\"},\n    {\"value\", 42.99}\n  }}\n};\n```\n\nNote that in all these cases, you never need to \"tell\" the compiler which JSON value type you want to use. If you want to be explicit or express some edge cases, the functions [`json::array()`](https://json.nlohmann.me/api/basic_json/array/) and [`json::object()`](https://json.nlohmann.me/api/basic_json/object/) will help:\n\n```cpp\n// a way to express the empty array []\njson empty_array_explicit = json::array();\n\n// ways to express the empty object {}\njson empty_object_implicit = json({});\njson empty_object_explicit = json::object();\n\n// a way to express an _array_ of key/value pairs [[\"currency\", \"USD\"], [\"value\", 42.99]]\njson array_not_object = json::array({ {\"currency\", \"USD\"}, {\"value\", 42.99} });\n```\n\n### Serialization / Deserialization\n\n#### To/from strings\n\nYou can create a JSON value (deserialization) by appending `_json` to a string literal:\n\n```cpp\n// create object from string literal\njson j = \"{ \\\"happy\\\": true, \\\"pi\\\": 3.141 }\"_json;\n\n// or even nicer with a raw string literal\nauto j2 = R\"(\n  {\n    \"happy\": true,\n    \"pi\": 3.141\n  }\n)\"_json;\n```\n\nNote that without appending the `_json` suffix, the passed string literal is not parsed, but just used as JSON string\nvalue. That is, `json j = \"{ \\\"happy\\\": true, \\\"pi\\\": 3.141 }\"` would just store the string\n`\"{ \"happy\": true, \"pi\": 3.141 }\"` rather than parsing the actual object.\n\nThe string literal should be brought into scope with `using namespace nlohmann::literals;`\n(see [`json::parse()`](https://json.nlohmann.me/api/operator_literal_json/)).\n\nThe above example can also be expressed explicitly using [`json::parse()`](https://json.nlohmann.me/api/basic_json/parse/):\n\n```cpp\n// parse explicitly\nauto j3 = json::parse(R\"({\"happy\": true, \"pi\": 3.141})\");\n```\n\nYou can also get a string representation of a JSON value (serialize):\n\n```cpp\n// explicit conversion to string\nstd::string s = j.dump();    // {\"happy\":true,\"pi\":3.141}\n\n// serialization with pretty printing\n// pass in the amount of spaces to indent\nstd::cout << j.dump(4) << std::endl;\n// {\n//     \"happy\": true,\n//     \"pi\": 3.141\n// }\n```\n\nNote the difference between serialization and assignment:\n\n```cpp\n// store a string in a JSON value\njson j_string = \"this is a string\";\n\n// retrieve the string value\nauto cpp_string = j_string.get<std::string>();\n// retrieve the string value (alternative when a variable already exists)\nstd::string cpp_string2;\nj_string.get_to(cpp_string2);\n\n// retrieve the serialized value (explicit JSON serialization)\nstd::string serialized_string = j_string.dump();\n\n// output of original string\nstd::cout << cpp_string << \" == \" << cpp_string2 << \" == \" << j_string.get<std::string>() << '\\n';\n// output of serialized value\nstd::cout << j_string << \" == \" << serialized_string << std::endl;\n```\n\n[`.dump()`](https://json.nlohmann.me/api/basic_json/dump/) returns the originally stored string value.\n\nNote the library only supports UTF-8. When you store strings with different encodings in the library, calling [`dump()`](https://json.nlohmann.me/api/basic_json/dump/) may throw an exception unless `json::error_handler_t::replace` or `json::error_handler_t::ignore` are used as error handlers.\n\n#### To/from streams (e.g., files, string streams)\n\nYou can also use streams to serialize and deserialize:\n\n```cpp\n// deserialize from standard input\njson j;\nstd::cin >> j;\n\n// serialize to standard output\nstd::cout << j;\n\n// the setw manipulator was overloaded to set the indentation for pretty printing\nstd::cout << std::setw(4) << j << std::endl;\n```\n\nThese operators work for any subclasses of `std::istream` or `std::ostream`. Here is the same example with files:\n\n```cpp\n// read a JSON file\nstd::ifstream i(\"file.json\");\njson j;\ni >> j;\n\n// write prettified JSON to another file\nstd::ofstream o(\"pretty.json\");\no << std::setw(4) << j << std::endl;\n```\n\nPlease note that setting the exception bit for `failbit` is inappropriate for this use case. It will result in program termination due to the `noexcept` specifier in use.\n\n#### Read from iterator range\n\nYou can also parse JSON from an iterator range; that is, from any container accessible by iterators whose `value_type` is an integral type of 1, 2, or 4 bytes, which will be interpreted as UTF-8, UTF-16, and UTF-32 respectively. For instance, a `std::vector<std::uint8_t>`, or a `std::list<std::uint16_t>`:\n\n```cpp\nstd::vector<std::uint8_t> v = {'t', 'r', 'u', 'e'};\njson j = json::parse(v.begin(), v.end());\n```\n\nYou may leave the iterators for the range [begin, end):\n\n```cpp\nstd::vector<std::uint8_t> v = {'t', 'r', 'u', 'e'};\njson j = json::parse(v);\n```\n\n#### Custom data source\n\nSince the parse function accepts arbitrary iterator ranges, you can provide your own data sources by implementing the `LegacyInputIterator` concept.\n\n```cpp\nstruct MyContainer {\n  void advance();\n  const char& get_current();\n};\n\nstruct MyIterator {\n    using difference_type = std::ptrdiff_t;\n    using value_type = char;\n    using pointer = const char*;\n    using reference = const char&;\n    using iterator_category = std::input_iterator_tag;\n\n    MyIterator& operator++() {\n        target->advance();\n        return *this;\n    }\n\n    bool operator!=(const MyIterator& rhs) const {\n        return rhs.target != target;\n    }\n\n    reference operator*() const {\n        return target->get_current();\n    }\n\n    MyContainer* target = nullptr;\n};\n\nMyIterator begin(MyContainer& tgt) {\n    return MyIterator{&tgt};\n}\n\nMyIterator end(const MyContainer&) {\n    return {};\n}\n\nvoid foo() {\n    MyContainer c;\n    json j = json::parse(c);\n}\n```\n\n#### SAX interface\n\nThe library uses a SAX-like interface with the following functions:\n\n```cpp\n// called when null is parsed\nbool null();\n\n// called when a boolean is parsed; value is passed\nbool boolean(bool val);\n\n// called when a signed or unsigned integer number is parsed; value is passed\nbool number_integer(number_integer_t val);\nbool number_unsigned(number_unsigned_t val);\n\n// called when a floating-point number is parsed; value and original string is passed\nbool number_float(number_float_t val, const string_t& s);\n\n// called when a string is parsed; value is passed and can be safely moved away\nbool string(string_t& val);\n// called when a binary value is parsed; value is passed and can be safely moved away\nbool binary(binary_t& val);\n\n// called when an object or array begins or ends, resp. The number of elements is passed (or -1 if not known)\nbool start_object(std::size_t elements);\nbool end_object();\nbool start_array(std::size_t elements);\nbool end_array();\n// called when an object key is parsed; value is passed and can be safely moved away\nbool key(string_t& val);\n\n// called when a parse error occurs; byte position, the last token, and an exception is passed\nbool parse_error(std::size_t position, const std::string& last_token, const detail::exception& ex);\n```\n\nThe return value of each function determines whether parsing should proceed.\n\nTo implement your own SAX handler, proceed as follows:\n\n1. Implement the SAX interface in a class. You can use class `nlohmann::json_sax<json>` as base class, but you can also use any class where the functions described above are implemented and public.\n2. Create an object of your SAX interface class, e.g. `my_sax`.\n3. Call `bool json::sax_parse(input, &my_sax)`; where the first parameter can be any input like a string or an input stream and the second parameter is a pointer to your SAX interface.\n\nNote the `sax_parse` function only returns a `bool` indicating the result of the last executed SAX event. It does not return a  `json` value - it is up to you to decide what to do with the SAX events. Furthermore, no exceptions are thrown in case of a parse error -- it is up to you what to do with the exception object passed to your `parse_error` implementation. Internally, the SAX interface is used for the DOM parser (class `json_sax_dom_parser`) as well as the acceptor (`json_sax_acceptor`), see file [`json_sax.hpp`](https://github.com/nlohmann/json/blob/develop/include/nlohmann/detail/input/json_sax.hpp).\n\n### STL-like access\n\nWe designed the JSON class to behave just like an STL container. In fact, it satisfies the [**ReversibleContainer**](https://en.cppreference.com/w/cpp/named_req/ReversibleContainer) requirement.\n\n```cpp\n// create an array using push_back\njson j;\nj.push_back(\"foo\");\nj.push_back(1);\nj.push_back(true);\n\n// also use emplace_back\nj.emplace_back(1.78);\n\n// iterate the array\nfor (json::iterator it = j.begin(); it != j.end(); ++it) {\n  std::cout << *it << '\\n';\n}\n\n// range-based for\nfor (auto& element : j) {\n  std::cout << element << '\\n';\n}\n\n// getter/setter\nconst auto tmp = j[0].get<std::string>();\nj[1] = 42;\nbool foo = j.at(2);\n\n// comparison\nj == R\"([\"foo\", 1, true, 1.78])\"_json;  // true\n\n// other stuff\nj.size();     // 4 entries\nj.empty();    // false\nj.type();     // json::value_t::array\nj.clear();    // the array is empty again\n\n// convenience type checkers\nj.is_null();\nj.is_boolean();\nj.is_number();\nj.is_object();\nj.is_array();\nj.is_string();\n\n// create an object\njson o;\no[\"foo\"] = 23;\no[\"bar\"] = false;\no[\"baz\"] = 3.141;\n\n// also use emplace\no.emplace(\"weather\", \"sunny\");\n\n// special iterator member functions for objects\nfor (json::iterator it = o.begin(); it != o.end(); ++it) {\n  std::cout << it.key() << \" : \" << it.value() << \"\\n\";\n}\n\n// the same code as range for\nfor (auto& el : o.items()) {\n  std::cout << el.key() << \" : \" << el.value() << \"\\n\";\n}\n\n// even easier with structured bindings (C++17)\nfor (auto& [key, value] : o.items()) {\n  std::cout << key << \" : \" << value << \"\\n\";\n}\n\n// find an entry\nif (o.contains(\"foo\")) {\n  // there is an entry with key \"foo\"\n}\n\n// or via find and an iterator\nif (o.find(\"foo\") != o.end()) {\n  // there is an entry with key \"foo\"\n}\n\n// or simpler using count()\nint foo_present = o.count(\"foo\"); // 1\nint fob_present = o.count(\"fob\"); // 0\n\n// delete an entry\no.erase(\"foo\");\n```\n\n### Conversion from STL containers\n\nAny sequence container (`std::array`, `std::vector`, `std::deque`, `std::forward_list`, `std::list`) whose values can be used to construct JSON values (e.g., integers, floating point numbers, Booleans, string types, or again STL containers described in this section) can be used to create a JSON array. The same holds for similar associative containers (`std::set`, `std::multiset`, `std::unordered_set`, `std::unordered_multiset`), but in these cases the order of the elements of the array depends on how the elements are ordered in the respective STL container.\n\n```cpp\nstd::vector<int> c_vector {1, 2, 3, 4};\njson j_vec(c_vector);\n// [1, 2, 3, 4]\n\nstd::deque<double> c_deque {1.2, 2.3, 3.4, 5.6};\njson j_deque(c_deque);\n// [1.2, 2.3, 3.4, 5.6]\n\nstd::list<bool> c_list {true, true, false, true};\njson j_list(c_list);\n// [true, true, false, true]\n\nstd::forward_list<int64_t> c_flist {12345678909876, 23456789098765, 34567890987654, 45678909876543};\njson j_flist(c_flist);\n// [12345678909876, 23456789098765, 34567890987654, 45678909876543]\n\nstd::array<unsigned long, 4> c_array {{1, 2, 3, 4}};\njson j_array(c_array);\n// [1, 2, 3, 4]\n\nstd::set<std::string> c_set {\"one\", \"two\", \"three\", \"four\", \"one\"};\njson j_set(c_set); // only one entry for \"one\" is used\n// [\"four\", \"one\", \"three\", \"two\"]\n\nstd::unordered_set<std::string> c_uset {\"one\", \"two\", \"three\", \"four\", \"one\"};\njson j_uset(c_uset); // only one entry for \"one\" is used\n// maybe [\"two\", \"three\", \"four\", \"one\"]\n\nstd::multiset<std::string> c_mset {\"one\", \"two\", \"one\", \"four\"};\njson j_mset(c_mset); // both entries for \"one\" are used\n// maybe [\"one\", \"two\", \"one\", \"four\"]\n\nstd::unordered_multiset<std::string> c_umset {\"one\", \"two\", \"one\", \"four\"};\njson j_umset(c_umset); // both entries for \"one\" are used\n// maybe [\"one\", \"two\", \"one\", \"four\"]\n```\n\nLikewise, any associative key-value containers (`std::map`, `std::multimap`, `std::unordered_map`, `std::unordered_multimap`) whose keys can construct an `std::string` and whose values can be used to construct JSON values (see examples above) can be used to create a JSON object. Note that in case of multimaps, only one key is used in the JSON object and the value depends on the internal order of the STL container.\n\n```cpp\nstd::map<std::string, int> c_map { {\"one\", 1}, {\"two\", 2}, {\"three\", 3} };\njson j_map(c_map);\n// {\"one\": 1, \"three\": 3, \"two\": 2 }\n\nstd::unordered_map<const char*, double> c_umap { {\"one\", 1.2}, {\"two\", 2.3}, {\"three\", 3.4} };\njson j_umap(c_umap);\n// {\"one\": 1.2, \"two\": 2.3, \"three\": 3.4}\n\nstd::multimap<std::string, bool> c_mmap { {\"one\", true}, {\"two\", true}, {\"three\", false}, {\"three\", true} };\njson j_mmap(c_mmap); // only one entry for key \"three\" is used\n// maybe {\"one\": true, \"two\": true, \"three\": true}\n\nstd::unordered_multimap<std::string, bool> c_ummap { {\"one\", true}, {\"two\", true}, {\"three\", false}, {\"three\", true} };\njson j_ummap(c_ummap); // only one entry for key \"three\" is used\n// maybe {\"one\": true, \"two\": true, \"three\": true}\n```\n\n### JSON Pointer and JSON Patch\n\nThe library supports **JSON Pointer** ([RFC 6901](https://tools.ietf.org/html/rfc6901)) as an alternative means to address structured values. On top of this, **JSON Patch** ([RFC 6902](https://tools.ietf.org/html/rfc6902)) allows describing differences between two JSON values -- effectively allowing patch and diff operations known from Unix.\n\n```cpp\n// a JSON value\njson j_original = R\"({\n  \"baz\": [\"one\", \"two\", \"three\"],\n  \"foo\": \"bar\"\n})\"_json;\n\n// access members with a JSON pointer (RFC 6901)\nj_original[\"/baz/1\"_json_pointer];\n// \"two\"\n\n// a JSON patch (RFC 6902)\njson j_patch = R\"([\n  { \"op\": \"replace\", \"path\": \"/baz\", \"value\": \"boo\" },\n  { \"op\": \"add\", \"path\": \"/hello\", \"value\": [\"world\"] },\n  { \"op\": \"remove\", \"path\": \"/foo\"}\n])\"_json;\n\n// apply the patch\njson j_result = j_original.patch(j_patch);\n// {\n//    \"baz\": \"boo\",\n//    \"hello\": [\"world\"]\n// }\n\n// calculate a JSON patch from two JSON values\njson::diff(j_result, j_original);\n// [\n//   { \"op\":\" replace\", \"path\": \"/baz\", \"value\": [\"one\", \"two\", \"three\"] },\n//   { \"op\": \"remove\",\"path\": \"/hello\" },\n//   { \"op\": \"add\", \"path\": \"/foo\", \"value\": \"bar\" }\n// ]\n```\n\n### JSON Merge Patch\n\nThe library supports **JSON Merge Patch** ([RFC 7386](https://tools.ietf.org/html/rfc7386)) as a patch format. Instead of using JSON Pointer (see above) to specify values to be manipulated, it describes the changes using a syntax that closely mimics the document being modified.\n\n```cpp\n// a JSON value\njson j_document = R\"({\n  \"a\": \"b\",\n  \"c\": {\n    \"d\": \"e\",\n    \"f\": \"g\"\n  }\n})\"_json;\n\n// a patch\njson j_patch = R\"({\n  \"a\":\"z\",\n  \"c\": {\n    \"f\": null\n  }\n})\"_json;\n\n// apply the patch\nj_document.merge_patch(j_patch);\n// {\n//  \"a\": \"z\",\n//  \"c\": {\n//    \"d\": \"e\"\n//  }\n// }\n```\n\n### Implicit conversions\n\nSupported types can be implicitly converted to JSON values.\n\nIt is recommended to **NOT USE** implicit conversions **FROM** a JSON value.\nYou can find more details about this recommendation [here](https://www.github.com/nlohmann/json/issues/958).\nYou can switch off implicit conversions by defining `JSON_USE_IMPLICIT_CONVERSIONS` to `0` before including the `json.hpp` header. When using CMake, you can also achieve this by setting the option `JSON_ImplicitConversions` to `OFF`.\n\n```cpp\n// strings\nstd::string s1 = \"Hello, world!\";\njson js = s1;\nauto s2 = js.get<std::string>();\n// NOT RECOMMENDED\nstd::string s3 = js;\nstd::string s4;\ns4 = js;\n\n// Booleans\nbool b1 = true;\njson jb = b1;\nauto b2 = jb.get<bool>();\n// NOT RECOMMENDED\nbool b3 = jb;\nbool b4;\nb4 = jb;\n\n// numbers\nint i = 42;\njson jn = i;\nauto f = jn.get<double>();\n// NOT RECOMMENDED\ndouble f2 = jb;\ndouble f3;\nf3 = jb;\n\n// etc.\n```\n\nNote that `char` types are not automatically converted to JSON strings, but to integer numbers. A conversion to a string must be specified explicitly:\n\n```cpp\nchar ch = 'A';                       // ASCII value 65\njson j_default = ch;                 // stores integer number 65\njson j_string = std::string(1, ch);  // stores string \"A\"\n```\n\n### Arbitrary types conversions\n\nEvery type can be serialized in JSON, not just STL containers and scalar types. Usually, you would do something along those lines:\n\n```cpp\nnamespace ns {\n    // a simple struct to model a person\n    struct person {\n        std::string name;\n        std::string address;\n        int age;\n    };\n}\n\nns::person p = {\"Ned Flanders\", \"744 Evergreen Terrace\", 60};\n\n// convert to JSON: copy each value into the JSON object\njson j;\nj[\"name\"] = p.name;\nj[\"address\"] = p.address;\nj[\"age\"] = p.age;\n\n// ...\n\n// convert from JSON: copy each value from the JSON object\nns::person p {\n    j[\"name\"].get<std::string>(),\n    j[\"address\"].get<std::string>(),\n    j[\"age\"].get<int>()\n};\n```\n\nIt works, but that's quite a lot of boilerplate... Fortunately, there's a better way:\n\n```cpp\n// create a person\nns::person p {\"Ned Flanders\", \"744 Evergreen Terrace\", 60};\n\n// conversion: person -> json\njson j = p;\n\nstd::cout << j << std::endl;\n// {\"address\":\"744 Evergreen Terrace\",\"age\":60,\"name\":\"Ned Flanders\"}\n\n// conversion: json -> person\nauto p2 = j.get<ns::person>();\n\n// that's it\nassert(p == p2);\n```\n\n#### Basic usage\n\nTo make this work with one of your types, you only need to provide two functions:\n\n```cpp\nusing json = nlohmann::json;\n\nnamespace ns {\n    void to_json(json& j, const person& p) {\n        j = json{{\"name\", p.name}, {\"address\", p.address}, {\"age\", p.age}};\n    }\n\n    void from_json(const json& j, person& p) {\n        j.at(\"name\").get_to(p.name);\n        j.at(\"address\").get_to(p.address);\n        j.at(\"age\").get_to(p.age);\n    }\n} // namespace ns\n```\n\nThat's all! When calling the `json` constructor with your type, your custom `to_json` method will be automatically called.\nLikewise, when calling `get<your_type>()` or `get_to(your_type&)`, the `from_json` method will be called.\n\nSome important things:\n\n- Those methods **MUST** be in your type's namespace (which can be the global namespace), or the library will not be able to locate them (in this example, they are in namespace `ns`, where `person` is defined).\n- Those methods **MUST** be available (e.g., proper headers must be included) everywhere you use these conversions. Look at [issue 1108](https://github.com/nlohmann/json/issues/1108) for errors that may occur otherwise.\n- When using `get<your_type>()`, `your_type` **MUST** be [DefaultConstructible](https://en.cppreference.com/w/cpp/named_req/DefaultConstructible). (There is a way to bypass this requirement described later.)\n- In function `from_json`, use function [`at()`](https://json.nlohmann.me/api/basic_json/at/) to access the object values rather than `operator[]`. In case a key does not exist, `at` throws an exception that you can handle, whereas `operator[]` exhibits undefined behavior.\n- You do not need to add serializers or deserializers for STL types like `std::vector`: the library already implements these.\n\n#### Simplify your life with macros\n\nIf you just want to serialize/deserialize some structs, the `to_json`/`from_json` functions can be a lot of boilerplate. There are [**several macros**](https://json.nlohmann.me/features/arbitrary_types/#simplify-your-life-with-macros) to make your life easier as long as you (1) want to use a JSON object as serialization and (2) want to use the member variable names as object keys in that object.\n\nWhich macro to choose depends on whether private member variables need to be accessed, a deserialization is needed, missing values should yield an error or should be replaced by default values, and if derived classes are used. See [this overview to choose the right one for your use case](https://json.nlohmann.me/api/macros/#serializationdeserialization-macros).\n\n##### Example usage of macros\n\nThe `to_json`/`from_json` functions for the `person` struct above can be created with [`NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE`](https://json.nlohmann.me/api/macros/nlohmann_define_type_non_intrusive/). In all macros, the first parameter is the name of the class/struct, and all remaining parameters name the members.\n\n```cpp\nnamespace ns {\n    NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE(person, name, address, age)\n}\n```\n\nHere is another example with private members, where [`NLOHMANN_DEFINE_TYPE_INTRUSIVE`](https://json.nlohmann.me/api/macros/nlohmann_define_type_intrusive/) is needed:\n\n```cpp\nnamespace ns {\n    class address {\n      private:\n        std::string street;\n        int housenumber;\n        int postcode;\n  \n      public:\n        NLOHMANN_DEFINE_TYPE_INTRUSIVE(address, street, housenumber, postcode)\n    };\n}\n```\n\n#### How do I convert third-party types?\n\nThis requires a bit more advanced technique. But first, let's see how this conversion mechanism works:\n\nThe library uses **JSON Serializers** to convert types to JSON.\nThe default serializer for `nlohmann::json` is `nlohmann::adl_serializer` (ADL means [Argument-Dependent Lookup](https://en.cppreference.com/w/cpp/language/adl)).\n\nIt is implemented like this (simplified):\n\n```cpp\ntemplate <typename T>\nstruct adl_serializer {\n    static void to_json(json& j, const T& value) {\n        // calls the \"to_json\" method in T's namespace\n    }\n\n    static void from_json(const json& j, T& value) {\n        // same thing, but with the \"from_json\" method\n    }\n};\n```\n\nThis serializer works fine when you have control over the type's namespace. However, what about `boost::optional` or `std::filesystem::path` (C++17)? Hijacking the `boost` namespace is pretty bad, and it's illegal to add something other than template specializations to `std`...\n\nTo solve this, you need to add a specialization of `adl_serializer` to the `nlohmann` namespace, here's an example:\n\n```cpp\n// partial specialization (full specialization works too)\nnamespace nlohmann {\n    template <typename T>\n    struct adl_serializer<boost::optional<T>> {\n        static void to_json(json& j, const boost::optional<T>& opt) {\n            if (opt == boost::none) {\n                j = nullptr;\n            } else {\n              j = *opt; // this will call adl_serializer<T>::to_json which will\n                        // find the free function to_json in T's namespace!\n            }\n        }\n\n        static void from_json(const json& j, boost::optional<T>& opt) {\n            if (j.is_null()) {\n                opt = boost::none;\n            } else {\n                opt = j.get<T>(); // same as above, but with\n                                  // adl_serializer<T>::from_json\n            }\n        }\n    };\n}\n```\n\n#### How can I use `get()` for non-default constructible/non-copyable types?\n\nThere is a way if your type is [MoveConstructible](https://en.cppreference.com/w/cpp/named_req/MoveConstructible). You will need to specialize the `adl_serializer` as well, but with a special `from_json` overload:\n\n```cpp\nstruct move_only_type {\n    move_only_type() = delete;\n    move_only_type(int ii): i(ii) {}\n    move_only_type(const move_only_type&) = delete;\n    move_only_type(move_only_type&&) = default;\n\n    int i;\n};\n\nnamespace nlohmann {\n    template <>\n    struct adl_serializer<move_only_type> {\n        // note: the return type is no longer 'void', and the method only takes\n        // one argument\n        static move_only_type from_json(const json& j) {\n            return {j.get<int>()};\n        }\n\n        // Here's the catch! You must provide a to_json method! Otherwise, you\n        // will not be able to convert move_only_type to json, since you fully\n        // specialized adl_serializer on that type\n        static void to_json(json& j, move_only_type t) {\n            j = t.i;\n        }\n    };\n}\n```\n\n#### Can I write my own serializer? (Advanced use)\n\nYes. You might want to take a look at [`unit-udt.cpp`](https://github.com/nlohmann/json/blob/develop/tests/src/unit-udt.cpp) in the test suite, to see a few examples.\n\nIf you write your own serializer, you'll need to do a few things:\n\n- use a different `basic_json` alias than `nlohmann::json` (the last template parameter of `basic_json` is the `JSONSerializer`)\n- use your `basic_json` alias (or a template parameter) in all your `to_json`/`from_json` methods\n- use `nlohmann::to_json` and `nlohmann::from_json` when you need ADL\n\nHere is an example, without simplifications, that only accepts types with a size <= 32, and uses ADL.\n\n```cpp\n// You should use void as a second template argument\n// if you don't need compile-time checks on T\ntemplate<typename T, typename SFINAE = typename std::enable_if<sizeof(T) <= 32>::type>\nstruct less_than_32_serializer {\n    template <typename BasicJsonType>\n    static void to_json(BasicJsonType& j, T value) {\n        // we want to use ADL, and call the correct to_json overload\n        using nlohmann::to_json; // this method is called by adl_serializer,\n                                 // this is where the magic happens\n        to_json(j, value);\n    }\n\n    template <typename BasicJsonType>\n    static void from_json(const BasicJsonType& j, T& value) {\n        // same thing here\n        using nlohmann::from_json;\n        from_json(j, value);\n    }\n};\n```\n\nBe **very** careful when reimplementing your serializer, you can stack overflow if you don't pay attention:\n\n```cpp\ntemplate <typename T, void>\nstruct bad_serializer\n{\n    template <typename BasicJsonType>\n    static void to_json(BasicJsonType& j, const T& value) {\n      // this calls BasicJsonType::json_serializer<T>::to_json(j, value)\n      // if BasicJsonType::json_serializer == bad_serializer ... oops!\n      j = value;\n    }\n\n    template <typename BasicJsonType>\n    static void to_json(const BasicJsonType& j, T& value) {\n      // this calls BasicJsonType::json_serializer<T>::from_json(j, value)\n      // if BasicJsonType::json_serializer == bad_serializer ... oops!\n      value = j.get<T>(); // oops!\n    }\n};\n```\n\n### Specializing enum conversion\n\nBy default, enum values are serialized to JSON as integers. In some cases, this could result in undesired behavior. If an enum is modified or re-ordered after data has been serialized to JSON, the later deserialized JSON data may be undefined or a different enum value than was originally intended.\n\nIt is possible to more precisely specify how a given enum is mapped to and from JSON as shown below:\n\n```cpp\n// example enum type declaration\nenum TaskState {\n    TS_STOPPED,\n    TS_RUNNING,\n    TS_COMPLETED,\n    TS_INVALID=-1,\n};\n\n// map TaskState values to JSON as strings\nNLOHMANN_JSON_SERIALIZE_ENUM( TaskState, {\n    {TS_INVALID, nullptr},\n    {TS_STOPPED, \"stopped\"},\n    {TS_RUNNING, \"running\"},\n    {TS_COMPLETED, \"completed\"},\n})\n```\n\nThe `NLOHMANN_JSON_SERIALIZE_ENUM()` macro declares a set of `to_json()` / `from_json()` functions for type `TaskState` while avoiding repetition and boilerplate serialization code.\n\n**Usage:**\n\n```cpp\n// enum to JSON as string\njson j = TS_STOPPED;\nassert(j == \"stopped\");\n\n// json string to enum\njson j3 = \"running\";\nassert(j3.get<TaskState>() == TS_RUNNING);\n\n// undefined json value to enum (where the first map entry above is the default)\njson jPi = 3.14;\nassert(jPi.get<TaskState>() == TS_INVALID);\n```\n\nJust as in [Arbitrary Type Conversions](#arbitrary-types-conversions) above,\n\n- `NLOHMANN_JSON_SERIALIZE_ENUM()` MUST be declared in your enum type's namespace (which can be the global namespace), or the library will not be able to locate it, and it will default to integer serialization.\n- It MUST be available (e.g., proper headers must be included) everywhere you use the conversions.\n\nOther Important points:\n\n- When using `get<ENUM_TYPE>()`, undefined JSON values will default to the first pair specified in your map. Select this default pair carefully.\n- If an enum or JSON value is specified more than once in your map, the first matching occurrence from the top of the map will be returned when converting to or from JSON.\n\n### Binary formats (BSON, CBOR, MessagePack, UBJSON, and BJData)\n\nThough JSON is a ubiquitous data format, it is not a very compact format suitable for data exchange, for instance over a network. Hence, the library supports [BSON](https://bsonspec.org) (Binary JSON), [CBOR](https://cbor.io) (Concise Binary Object Representation), [MessagePack](https://msgpack.org), [UBJSON](https://ubjson.org) (Universal Binary JSON Specification) and [BJData](https://neurojson.org/bjdata) (Binary JData) to efficiently encode JSON values to byte vectors and to decode such vectors.\n\n```cpp\n// create a JSON value\njson j = R\"({\"compact\": true, \"schema\": 0})\"_json;\n\n// serialize to BSON\nstd::vector<std::uint8_t> v_bson = json::to_bson(j);\n\n// 0x1B, 0x00, 0x00, 0x00, 0x08, 0x63, 0x6F, 0x6D, 0x70, 0x61, 0x63, 0x74, 0x00, 0x01, 0x10, 0x73, 0x63, 0x68, 0x65, 0x6D, 0x61, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00\n\n// roundtrip\njson j_from_bson = json::from_bson(v_bson);\n\n// serialize to CBOR\nstd::vector<std::uint8_t> v_cbor = json::to_cbor(j);\n\n// 0xA2, 0x67, 0x63, 0x6F, 0x6D, 0x70, 0x61, 0x63, 0x74, 0xF5, 0x66, 0x73, 0x63, 0x68, 0x65, 0x6D, 0x61, 0x00\n\n// roundtrip\njson j_from_cbor = json::from_cbor(v_cbor);\n\n// serialize to MessagePack\nstd::vector<std::uint8_t> v_msgpack = json::to_msgpack(j);\n\n// 0x82, 0xA7, 0x63, 0x6F, 0x6D, 0x70, 0x61, 0x63, 0x74, 0xC3, 0xA6, 0x73, 0x63, 0x68, 0x65, 0x6D, 0x61, 0x00\n\n// roundtrip\njson j_from_msgpack = json::from_msgpack(v_msgpack);\n\n// serialize to UBJSON\nstd::vector<std::uint8_t> v_ubjson = json::to_ubjson(j);\n\n// 0x7B, 0x69, 0x07, 0x63, 0x6F, 0x6D, 0x70, 0x61, 0x63, 0x74, 0x54, 0x69, 0x06, 0x73, 0x63, 0x68, 0x65, 0x6D, 0x61, 0x69, 0x00, 0x7D\n\n// roundtrip\njson j_from_ubjson = json::from_ubjson(v_ubjson);\n```\n\nThe library also supports binary types from BSON, CBOR (byte strings), and MessagePack (bin, ext, fixext). They are stored by default as `std::vector<std::uint8_t>` to be processed outside the library.\n\n```cpp\n// CBOR byte string with payload 0xCAFE\nstd::vector<std::uint8_t> v = {0x42, 0xCA, 0xFE};\n\n// read value\njson j = json::from_cbor(v);\n\n// the JSON value has type binary\nj.is_binary(); // true\n\n// get reference to stored binary value\nauto& binary = j.get_binary();\n\n// the binary value has no subtype (CBOR has no binary subtypes)\nbinary.has_subtype(); // false\n\n// access std::vector<std::uint8_t> member functions\nbinary.size(); // 2\nbinary[0]; // 0xCA\nbinary[1]; // 0xFE\n\n// set subtype to 0x10\nbinary.set_subtype(0x10);\n\n// serialize to MessagePack\nauto cbor = json::to_msgpack(j); // 0xD5 (fixext2), 0x10, 0xCA, 0xFE\n```\n\n## Customers\n\nThe library is used in multiple projects, applications, operating systems, etc. The list below is not exhaustive, but the result of an internet search. If you know further customers of the library, please let me know, see [contact](#contact).\n\n[![logos of customers using the library](docs/mkdocs/docs/images/customers.png)](https://json.nlohmann.me/home/customers/)\n\n## Supported compilers\n\nThough it's 2026 already, the support for C++11 is still a bit sparse. Currently, the following compilers are known to work:\n\n- GCC 4.8 - 14.2 (and possibly later)\n- Clang 3.4 - 21.0 (and possibly later)\n- Apple Clang 9.1 - 16.0 (and possibly later)\n- Intel C++ Compiler 17.0.2 (and possibly later)\n- Nvidia CUDA Compiler 11.0.221 (and possibly later)\n- Microsoft Visual C++ 2015 / Build Tools 14.0.25123.0 (and possibly later)\n- Microsoft Visual C++ 2017 / Build Tools 15.5.180.51428 (and possibly later)\n- Microsoft Visual C++ 2019 / Build Tools 16.3.1+1def00d3d (and possibly later)\n- Microsoft Visual C++ 2022 / Build Tools 19.30.30709.0 (and possibly later)\n\nI would be happy to learn about other compilers/versions.\n\nPlease note:\n\n- GCC 4.8 has a bug [57824](https://gcc.gnu.org/bugzilla/show_bug.cgi?id=57824): multiline raw strings cannot be the arguments to macros. Don't use multiline raw strings directly in macros with this compiler.\n- Android defaults to using very old compilers and C++ libraries. To fix this, add the following to your `Application.mk`. This will switch to the LLVM C++ library, the Clang compiler, and enable C++11 and other features disabled by default.\n\n    ```makefile\n    APP_STL := c++_shared\n    NDK_TOOLCHAIN_VERSION := clang3.6\n    APP_CPPFLAGS += -frtti -fexceptions\n    ```\n\n    The code compiles successfully with [Android NDK](https://developer.android.com/ndk/index.html?hl=ml), Revision 9 - 11 (and possibly later) and [CrystaX's Android NDK](https://www.crystax.net/en/android/ndk) version 10.\n\n- For GCC running on MinGW or Android SDK, the error `'to_string' is not a member of 'std'` (or similarly, for `strtod` or `strtof`) may occur. Note this is not an issue with the code, but rather with the compiler itself. On Android, see above to build with a newer environment.  For MinGW, please refer to [this site](https://tehsausage.com/mingw-to-string) and [this discussion](https://github.com/nlohmann/json/issues/136) for information on how to fix this bug. For Android NDK using `APP_STL := gnustl_static`, please refer to [this discussion](https://github.com/nlohmann/json/issues/219).\n\n- Unsupported versions of GCC and Clang are rejected by `#error` directives. This can be switched off by defining `JSON_SKIP_UNSUPPORTED_COMPILER_CHECK`. Note that you can expect no support in this case.\n\nSee the page [quality assurance](https://json.nlohmann.me/community/quality_assurance) on the compilers used to check the library in the CI.\n\n## Integration\n\n[`json.hpp`](https://github.com/nlohmann/json/blob/develop/single_include/nlohmann/json.hpp) is the single required file in `single_include/nlohmann` or [released here](https://github.com/nlohmann/json/releases). You need to add\n\n```cpp\n#include <nlohmann/json.hpp>\n\n// for convenience\nusing json = nlohmann::json;\n```\n\nto the files you want to process JSON and set the necessary switches to enable C++11 (e.g., `-std=c++11` for GCC and Clang).\n\nYou can further use file [`include/nlohmann/json_fwd.hpp`](https://github.com/nlohmann/json/blob/develop/include/nlohmann/json_fwd.hpp) for forward-declarations. The installation of `json_fwd.hpp` (as part of cmake's install step) can be achieved by setting `-DJSON_MultipleHeaders=ON`.\n\n### CMake\n\nYou can also use the `nlohmann_json::nlohmann_json` interface target in CMake.  This target populates the appropriate usage requirements for `INTERFACE_INCLUDE_DIRECTORIES` to point to the appropriate include directories and `INTERFACE_COMPILE_FEATURES` for the necessary C++11 flags.\n\n#### External\n\nTo use this library from a CMake project, you can locate it directly with `find_package()` and use the namespaced imported target from the generated package configuration:\n\n```cmake\n# CMakeLists.txt\nfind_package(nlohmann_json 3.12.0 REQUIRED)\n...\nadd_library(foo ...)\n...\ntarget_link_libraries(foo PRIVATE nlohmann_json::nlohmann_json)\n```\n\nThe package configuration file, `nlohmann_jsonConfig.cmake`, can be used either from an install tree or directly out of the build tree.\n\n#### Embedded\n\nTo embed the library directly into an existing CMake project, place the entire source tree in a subdirectory and call `add_subdirectory()` in your `CMakeLists.txt` file:\n\n```cmake\n# Typically you don't care so much for a third party library's tests to be\n# run from your own project's code.\nset(JSON_BuildTests OFF CACHE INTERNAL \"\")\n\n# If you only include this third party in PRIVATE source files, you do not\n# need to install it when your main project gets installed.\n# set(JSON_Install OFF CACHE INTERNAL \"\")\n\n# Don't use include(nlohmann_json/CMakeLists.txt) since that carries with it\n# unintended consequences that will break the build.  It's generally\n# discouraged (although not necessarily well documented as such) to use\n# include(...) for pulling in other CMake projects anyways.\nadd_subdirectory(nlohmann_json)\n...\nadd_library(foo ...)\n...\ntarget_link_libraries(foo PRIVATE nlohmann_json::nlohmann_json)\n```\n\n##### Embedded (FetchContent)\n\nSince CMake v3.11,\n[FetchContent](https://cmake.org/cmake/help/v3.11/module/FetchContent.html) can\nbe used to automatically download a release as a dependency at configure time.\n\nExample:\n\n```cmake\ninclude(FetchContent)\n\nFetchContent_Declare(json URL https://github.com/nlohmann/json/releases/download/v3.12.0/json.tar.xz)\nFetchContent_MakeAvailable(json)\n\ntarget_link_libraries(foo PRIVATE nlohmann_json::nlohmann_json)\n```\n\n**Note**: It is recommended to use the URL approach described above, which is supported as of version 3.10.0. See\n<https://json.nlohmann.me/integration/cmake/#fetchcontent> for more information.\n\n#### Supporting Both\n\nTo allow your project to support either an externally supplied or an embedded JSON library, you can use a pattern akin to the following:\n\n``` cmake\n# Top level CMakeLists.txt\nproject(FOO)\n...\noption(FOO_USE_EXTERNAL_JSON \"Use an external JSON library\" OFF)\n...\nadd_subdirectory(thirdparty)\n...\nadd_library(foo ...)\n...\n# Note that the namespaced target will always be available regardless of the\n# import method\ntarget_link_libraries(foo PRIVATE nlohmann_json::nlohmann_json)\n```\n\n```cmake\n# thirdparty/CMakeLists.txt\n...\nif(FOO_USE_EXTERNAL_JSON)\n  find_package(nlohmann_json 3.12.0 REQUIRED)\nelse()\n  set(JSON_BuildTests OFF CACHE INTERNAL \"\")\n  add_subdirectory(nlohmann_json)\nendif()\n...\n```\n\n`thirdparty/nlohmann_json` is then a complete copy of this source tree.\n\n### Package Managers\n\nUse your favorite [**package manager**](https://json.nlohmann.me/integration/package_managers/) to use the library.\n\n- <img src=\"https://raw.githubusercontent.com/nlohmann/json/refs/heads/develop/docs/mkdocs/docs/images/package_managers/homebrew.svg\" height=\"20\">&nbsp;[**Homebrew**](https://json.nlohmann.me/integration/package_managers/#homebrew) `nlohmann-json`\n- <img src=\"https://raw.githubusercontent.com/nlohmann/json/refs/heads/develop/docs/mkdocs/docs/images/package_managers/meson.svg\" height=\"20\">&nbsp;[**Meson**](https://json.nlohmann.me/integration/package_managers/#meson) `nlohmann_json`\n- <img src=\"https://raw.githubusercontent.com/nlohmann/json/refs/heads/develop/docs/mkdocs/docs/images/package_managers/bazel.svg\" height=\"20\">&nbsp;[**Bazel**](https://json.nlohmann.me/integration/package_managers/#bazel) `nlohmann_json`\n- <img src=\"https://raw.githubusercontent.com/nlohmann/json/refs/heads/develop/docs/mkdocs/docs/images/package_managers/conan.svg\" height=\"20\">&nbsp;[**Conan**](https://json.nlohmann.me/integration/package_managers/#conan) `nlohmann_json`\n- <img src=\"https://raw.githubusercontent.com/nlohmann/json/refs/heads/develop/docs/mkdocs/docs/images/package_managers/spack.svg\" height=\"20\">&nbsp;[**Spack**](https://json.nlohmann.me/integration/package_managers/#spack) `nlohmann-json`\n- [**Hunter**](https://json.nlohmann.me/integration/package_managers/#hunter) `nlohmann_json`\n- <img src=\"https://raw.githubusercontent.com/nlohmann/json/refs/heads/develop/docs/mkdocs/docs/images/package_managers/vcpkg.png\" height=\"20\">&nbsp;[**vcpkg**](https://json.nlohmann.me/integration/package_managers/#vcpkg) `nlohmann-json`\n- [**cget**](https://json.nlohmann.me/integration/package_managers/#cget) `nlohmann/json`\n- <img src=\"https://raw.githubusercontent.com/nlohmann/json/refs/heads/develop/docs/mkdocs/docs/images/package_managers/swift.svg\" height=\"20\">&nbsp;[**Swift Package Manager**](https://json.nlohmann.me/integration/package_managers/#swift-package-manager) `nlohmann/json`\n- <img src=\"https://raw.githubusercontent.com/nlohmann/json/refs/heads/develop/docs/mkdocs/docs/images/package_managers/nuget.svg\" height=\"20\">&nbsp;[**Nuget**](https://json.nlohmann.me/integration/package_managers/#nuget) `nlohmann.json`\n- <img src=\"https://raw.githubusercontent.com/nlohmann/json/refs/heads/develop/docs/mkdocs/docs/images/package_managers/conda.svg\" height=\"20\">&nbsp;[**Conda**](https://json.nlohmann.me/integration/package_managers/#conda) `nlohmann_json`\n- <img src=\"https://raw.githubusercontent.com/nlohmann/json/refs/heads/develop/docs/mkdocs/docs/images/package_managers/macports.svg\" height=\"20\">&nbsp;[**MacPorts**](https://json.nlohmann.me/integration/package_managers/#macports) `nlohmann-json`\n- <img src=\"https://raw.githubusercontent.com/nlohmann/json/refs/heads/develop/docs/mkdocs/docs/images/package_managers/CPM.png\" height=\"20\">&nbsp;[**cpm.cmake**](https://json.nlohmann.me/integration/package_managers/#cpmcmake) `gh:nlohmann/json`\n- <img src=\"https://raw.githubusercontent.com/nlohmann/json/refs/heads/develop/docs/mkdocs/docs/images/package_managers/xmake.svg\" height=\"20\">&nbsp;[**xmake**](https://json.nlohmann.me/integration/package_managers/#xmake) `nlohmann_json`\n\nThe library is part of many package managers. See the [**documentation**](https://json.nlohmann.me/integration/package_managers/) for detailed descriptions and examples.\n\n### Pkg-config\n\nIf you are using bare Makefiles, you can use `pkg-config` to generate the include flags that point to where the library is installed:\n\n```sh\npkg-config nlohmann_json --cflags\n```\n\n## License\n\n<img align=\"right\" src=\"https://149753425.v2.pressablecdn.com/wp-content/uploads/2009/06/OSIApproved_100X125.png\" alt=\"OSI approved license\">\n\nThe class is licensed under the [MIT License](https://opensource.org/licenses/MIT):\n\nCopyright &copy; 2013-2026 [Niels Lohmann](https://nlohmann.me)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the â€œSoftwareâ€), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED â€œAS ISâ€, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n* * *\n\n- The class contains the UTF-8 Decoder from Bjoern Hoehrmann which is licensed under the [MIT License](https://opensource.org/licenses/MIT) (see above). Copyright &copy; 2008-2009 [BjÃ¶rn Hoehrmann](https://bjoern.hoehrmann.de/) <bjoern@hoehrmann.de>\n- The class contains a slightly modified version of the Grisu2 algorithm from Florian Loitsch which is licensed under the [MIT License](https://opensource.org/licenses/MIT) (see above). Copyright &copy; 2009 [Florian Loitsch](https://florian.loitsch.com/)\n- The class contains a copy of [Hedley](https://nemequ.github.io/hedley/) from Evan Nemerson which is licensed as [CC0-1.0](https://creativecommons.org/publicdomain/zero/1.0/).\n- The class contains parts of [Google Abseil](https://github.com/abseil/abseil-cpp) which is licensed under the [Apache 2.0 License](https://opensource.org/licenses/Apache-2.0).\n\n<img align=\"right\" src=\"https://git.fsfe.org/reuse/reuse-ci/raw/branch/master/reuse-horizontal.png\" alt=\"REUSE Software\">\n\nThe library is compliant to version 3.3 of the [**REUSE specification**](https://reuse.software):\n\n- Every source file contains an SPDX copyright header.\n- The full text of all licenses used in the repository can be found in the `LICENSES` folder.\n- File `.reuse/dep5` contains an overview of all files' copyrights and licenses.\n- Run `pipx run reuse lint` to verify the project's REUSE compliance and `pipx run reuse spdx` to generate a SPDX SBOM.\n\n## Contact\n\nIf you have questions regarding the library, I would like to invite you to [open an issue at GitHub](https://github.com/nlohmann/json/issues/new/choose). Please describe your request, problem, or question as detailed as possible, and also mention the version of the library you are using as well as the version of your compiler and operating system. Opening an issue at GitHub allows other users and contributors to this library to collaborate. For instance, I have little experience with MSVC, and most issues in this regard have been solved by a growing community. If you have a look at the [closed issues](https://github.com/nlohmann/json/issues?q=is%3Aissue+is%3Aclosed), you will see that we react quite timely in most cases.\n\nOnly if your request would contain confidential information, please [send me an email](mailto:mail@nlohmann.me). For encrypted messages, please use [this key](https://keybase.io/nlohmann/pgp_keys.asc).\n\n## Security\n\n[Commits by Niels Lohmann](https://github.com/nlohmann/json/commits) and [releases](https://github.com/nlohmann/json/releases) are signed with this [PGP Key](https://keybase.io/nlohmann/pgp_keys.asc?fingerprint=797167ae41c0a6d9232e48457f3cea63ae251b69).\n\n## Thanks\n\nI deeply appreciate the help of the following people.\n\n<img src=\"https://raw.githubusercontent.com/nlohmann/json/develop/docs/avatars.png\" align=\"right\" alt=\"GitHub avatars of the contributors\">\n\n1. [Teemperor](https://github.com/Teemperor) implemented CMake support and lcov integration, realized escape and Unicode handling in the string parser, and fixed the JSON serialization.\n2. [elliotgoodrich](https://github.com/elliotgoodrich) fixed an issue with double deletion in the iterator classes.\n3. [kirkshoop](https://github.com/kirkshoop) made the iterators of the class composable to other libraries.\n4. [wancw](https://github.com/wanwc) fixed a bug that hindered the class to compile with Clang.\n5. Tomas Ã…blad found a bug in the iterator implementation.\n6. [Joshua C. Randall](https://github.com/jrandall) fixed a bug in the floating-point serialization.\n7. [Aaron Burghardt](https://github.com/aburgh) implemented code to parse streams incrementally. Furthermore, he greatly improved the parser class by allowing the definition of a filter function to discard undesired elements while parsing.\n8. [Daniel KopeÄek](https://github.com/dkopecek) fixed a bug in the compilation with GCC 5.0.\n9. [Florian Weber](https://github.com/Florianjw) fixed a bug in and improved the performance of the comparison operators.\n10. [Eric Cornelius](https://github.com/EricMCornelius) pointed out a bug in the handling with NaN and infinity values. He also improved the performance of the string escaping.\n11. [æ˜“æ€é¾™](https://github.com/likebeta) implemented a conversion from anonymous enums.\n12. [kepkin](https://github.com/kepkin) patiently pushed forward the support for Microsoft Visual Studio.\n13. [gregmarr](https://github.com/gregmarr) simplified the implementation of reverse iterators and helped with numerous hints and improvements. In particular, he pushed forward the implementation of user-defined types.\n14. [Caio Luppi](https://github.com/caiovlp) fixed a bug in the Unicode handling.\n15. [dariomt](https://github.com/dariomt) fixed some typos in the examples.\n16. [Daniel Frey](https://github.com/d-frey) cleaned up some pointers and implemented exception-safe memory allocation.\n17. [Colin Hirsch](https://github.com/ColinH) took care of a small namespace issue.\n18. [Huu Nguyen](https://github.com/whoshuu) corrected a variable name in the documentation.\n19. [Silverweed](https://github.com/silverweed) overloaded `parse()` to accept an rvalue reference.\n20. [dariomt](https://github.com/dariomt) fixed a subtlety in MSVC type support and implemented the `get_ref()` function to get a reference to stored values.\n21. [ZahlGraf](https://github.com/ZahlGraf) added a workaround that allows compilation using Android NDK.\n22. [whackashoe](https://github.com/whackashoe) replaced a function that was marked as unsafe by Visual Studio.\n23. [406345](https://github.com/406345) fixed two small warnings.\n24. [Glen Fernandes](https://github.com/glenfe) noted a potential portability problem in the `has_mapped_type` function.\n25. [Corbin Hughes](https://github.com/nibroc) fixed some typos in the contribution guidelines.\n26. [twelsby](https://github.com/twelsby) fixed the array subscript operator, an issue that failed the MSVC build, and floating-point parsing/dumping. He further added support for unsigned integer numbers and implemented better roundtrip support for parsed numbers.\n27. [Volker Diels-Grabsch](https://github.com/vog) fixed a link in the README file.\n28. [msm-](https://github.com/msm-) added support for American Fuzzy Lop.\n29. [Annihil](https://github.com/Annihil) fixed an example in the README file.\n30. [Themercee](https://github.com/Themercee) noted a wrong URL in the README file.\n31. [Lv Zheng](https://github.com/lv-zheng) fixed a namespace issue with `int64_t` and `uint64_t`.\n32. [abc100m](https://github.com/abc100m) analyzed the issues with GCC 4.8 and proposed a [partial solution](https://github.com/nlohmann/json/pull/212).\n33. [zewt](https://github.com/zewt) added useful notes to the README file about Android.\n34. [RÃ³bert MÃ¡rki](https://github.com/robertmrk) added a fix to use move iterators and improved the integration via CMake.\n35. [Chris Kitching](https://github.com/ChrisKitching) cleaned up the CMake files.\n36. [Tom Needham](https://github.com/06needhamt) fixed a subtle bug with MSVC 2015 which was also proposed by [Michael K.](https://github.com/Epidal).\n37. [MÃ¡rio Feroldi](https://github.com/thelostt) fixed a small typo.\n38. [duncanwerner](https://github.com/duncanwerner) found a really embarrassing performance regression in the 2.0.0 release.\n39. [Damien](https://github.com/dtoma) fixed one of the last conversion warnings.\n40. [Thomas Braun](https://github.com/t-b) fixed a warning in a test case and adjusted MSVC calls in the CI.\n41. [ThÃ©o DELRIEU](https://github.com/theodelrieu) patiently and constructively oversaw the long way toward [iterator-range parsing](https://github.com/nlohmann/json/issues/290). He also implemented the magic behind the serialization/deserialization of user-defined types and split the single header file into smaller chunks.\n42. [Stefan](https://github.com/5tefan) fixed a minor issue in the documentation.\n43. [Vasil Dimov](https://github.com/vasild) fixed the documentation regarding conversions from `std::multiset`.\n44. [ChristophJud](https://github.com/ChristophJud) overworked the CMake files to ease project inclusion.\n45. [Vladimir Petrigo](https://github.com/vpetrigo) made a SFINAE hack more readable and added Visual Studio 17 to the build matrix.\n46. [Denis Andrejew](https://github.com/seeekr) fixed a grammar issue in the README file.\n47. [Pierre-Antoine Lacaze](https://github.com/palacaze) found a subtle bug in the `dump()` function.\n48. [TurpentineDistillery](https://github.com/TurpentineDistillery) pointed to [`std::locale::classic()`](https://en.cppreference.com/w/cpp/locale/locale/classic) to avoid too much locale joggling, found some nice performance improvements in the parser, improved the benchmarking code, and realized locale-independent number parsing and printing.\n49. [cgzones](https://github.com/cgzones) had an idea how to fix the Coverity scan.\n50. [Jared Grubb](https://github.com/jaredgrubb) silenced a nasty documentation warning.\n51. [Yixin Zhang](https://github.com/qwename) fixed an integer overflow check.\n52. [Bosswestfalen](https://github.com/Bosswestfalen) merged two iterator classes into a smaller one.\n53. [Daniel599](https://github.com/Daniel599) helped to get Travis to execute the tests with Clang's sanitizers.\n54. [Jonathan Lee](https://github.com/vjon) fixed an example in the README file.\n55. [gnzlbg](https://github.com/gnzlbg) supported the implementation of user-defined types.\n56. [Alexej Harm](https://github.com/qis) helped to get the user-defined types working with Visual Studio.\n57. [Jared Grubb](https://github.com/jaredgrubb) supported the implementation of user-defined types.\n58. [EnricoBilla](https://github.com/EnricoBilla) noted a typo in an example.\n59. [Martin HoÅ™eÅˆovskÃ½](https://github.com/horenmar) found a way for a 2x speedup for the compilation time of the test suite.\n60. [ukhegg](https://github.com/ukhegg) found proposed an improvement for the examples section.\n61. [rswanson-ihi](https://github.com/rswanson-ihi) noted a typo in the README.\n62. [Mihai Stan](https://github.com/stanmihai4) fixed a bug in the comparison with `nullptr`s.\n63. [Tushar Maheshwari](https://github.com/tusharpm) added [cotire](https://github.com/sakra/cotire) support to speed up the compilation.\n64. [TedLyngmo](https://github.com/TedLyngmo) noted a typo in the README, removed unnecessary bit arithmetic, and fixed some `-Weffc++` warnings.\n65. [Krzysztof WoÅ›](https://github.com/krzysztofwos) made exceptions more visible.\n66. [ftillier](https://github.com/ftillier) fixed a compiler warning.\n67. [tinloaf](https://github.com/tinloaf) made sure all pushed warnings are properly popped.\n68. [Fytch](https://github.com/Fytch) found a bug in the documentation.\n69. [Jay Sistar](https://github.com/Type1J) implemented a Meson build description.\n70. [Henry Lee](https://github.com/HenryRLee) fixed a warning in ICC and improved the iterator implementation.\n71. [Vincent Thiery](https://github.com/vthiery) maintains a package for the Conan package manager.\n72. [Steffen](https://github.com/koemeet) fixed a potential issue with MSVC and `std::min`.\n73. [Mike Tzou](https://github.com/Chocobo1) fixed some typos.\n74. [amrcode](https://github.com/amrcode) noted misleading documentation about comparison of floats.\n75. [Oleg Endo](https://github.com/olegendo) reduced the memory consumption by replacing `<iostream>` with `<iosfwd>`.\n76. [dan-42](https://github.com/dan-42) cleaned up the CMake files to simplify including/reusing of the library.\n77. [Nikita Ofitserov](https://github.com/himikof) allowed for moving values from initializer lists.\n78. [Greg Hurrell](https://github.com/wincent) fixed a typo.\n79. [Dmitry Kukovinets](https://github.com/DmitryKuk) fixed a typo.\n80. [kbthomp1](https://github.com/kbthomp1) fixed an issue related to the Intel OSX compiler.\n81. [Markus Werle](https://github.com/daixtrose) fixed a typo.\n82. [WebProdPP](https://github.com/WebProdPP) fixed a subtle error in a precondition check.\n83. [Alex](https://github.com/leha-bot) noted an error in a code sample.\n84. [Tom de Geus](https://github.com/tdegeus) reported some warnings with ICC and helped to fix them.\n85. [Perry Kundert](https://github.com/pjkundert) simplified reading from input streams.\n86. [Sonu Lohani](https://github.com/sonulohani) fixed a small compilation error.\n87. [Jamie Seward](https://github.com/jseward) fixed all MSVC warnings.\n88. [Nate Vargas](https://github.com/eld00d) added a Doxygen tag file.\n89. [pvleuven](https://github.com/pvleuven) helped to fix a warning in ICC.\n90. [Pavel](https://github.com/crea7or) helped to fix some warnings in MSVC.\n91. [Jamie Seward](https://github.com/jseward) avoided unnecessary string copies in `find()` and `count()`.\n92. [Mitja](https://github.com/Itja) fixed some typos.\n93. [Jorrit Wronski](https://github.com/jowr) updated the Hunter package links.\n94. [Matthias MÃ¶ller](https://github.com/TinyTinni) added a `.natvis` for the MSVC debug view.\n95. [bogemic](https://github.com/bogemic) fixed some C++17 deprecation warnings.\n96. [Eren Okka](https://github.com/erengy) fixed some MSVC warnings.\n97. [abolz](https://github.com/abolz) integrated the Grisu2 algorithm for proper floating-point formatting, allowing more roundtrip checks to succeed.\n98. [Vadim Evard](https://github.com/Pipeliner) fixed a Markdown issue in the README.\n99. [zerodefect](https://github.com/zerodefect) fixed a compiler warning.\n100. [Kert](https://github.com/kaidokert) allowed to template the string type in the serialization and added the possibility to override the exceptional behavior.\n101. [mark-99](https://github.com/mark-99) helped fix an ICC error.\n102. [Patrik Huber](https://github.com/patrikhuber) fixed links in the README file.\n103. [johnfb](https://github.com/johnfb) found a bug in the implementation of CBOR's indefinite length strings.\n104. [Paul Fultz II](https://github.com/pfultz2) added a note on the cget package manager.\n105. [Wilson Lin](https://github.com/wla80) made the integration section of the README more concise.\n106. [RalfBielig](https://github.com/ralfbielig) detected and fixed a memory leak in the parser callback.\n107. [agrianius](https://github.com/agrianius) allowed dumping JSON to an alternative string type.\n108. [Kevin Tonon](https://github.com/ktonon) overworked the C++11 compiler checks in CMake.\n109. [Axel Huebl](https://github.com/ax3l) simplified a CMake check and added support for the [Spack package manager](https://spack.io).\n110. [Carlos O'Ryan](https://github.com/coryan) fixed a typo.\n111. [James Upjohn](https://github.com/jammehcow) fixed a version number in the compilers section.\n112. [Chuck Atkins](https://github.com/chuckatkins) adjusted the CMake files to the CMake packaging guidelines and provided documentation for the CMake integration.\n113. [Jan SchÃ¶ppach](https://github.com/dns13) fixed a typo.\n114. [martin-mfg](https://github.com/martin-mfg) fixed a typo.\n115. [Matthias MÃ¶ller](https://github.com/TinyTinni) removed the dependency from `std::stringstream`.\n116. [agrianius](https://github.com/agrianius) added code to use alternative string implementations.\n117. [Daniel599](https://github.com/Daniel599) allowed to use more algorithms with the `items()` function.\n118. [Julius Rakow](https://github.com/jrakow) fixed the Meson include directory and fixed the links to [cppreference.com](https://cppreference.com).\n119. [Sonu Lohani](https://github.com/sonulohani) fixed the compilation with MSVC 2015 in debug mode.\n120. [grembo](https://github.com/grembo) fixed the test suite and re-enabled several test cases.\n121. [Hyeon Kim](https://github.com/simnalamburt) introduced the macro `JSON_INTERNAL_CATCH` to control the exception handling inside the library.\n122. [thyu](https://github.com/thyu) fixed a compiler warning.\n123. [David Guthrie](https://github.com/LEgregius) fixed a subtle compilation error with Clang 3.4.2.\n124. [Dennis Fischer](https://github.com/dennisfischer) allowed to call `find_package` without installing the library.\n125. [Hyeon Kim](https://github.com/simnalamburt) fixed an issue with a double macro definition.\n126. [Ben Berman](https://github.com/rivertam) made some error messages more understandable.\n127. [zakalibit](https://github.com/zakalibit) fixed a compilation problem with the Intel C++ compiler.\n128. [mandreyel](https://github.com/mandreyel) fixed a compilation problem.\n129. [Kostiantyn Ponomarenko](https://github.com/koponomarenko) added version and license information to the Meson build file.\n130. [Henry Schreiner](https://github.com/henryiii) added support for GCC 4.8.\n131. [knilch](https://github.com/knilch0r) made sure the test suite does not stall when run in the wrong directory.\n132. [Antonio Borondo](https://github.com/antonioborondo) fixed an MSVC 2017 warning.\n133. [Dan Gendreau](https://github.com/dgendreau) implemented the `NLOHMANN_JSON_SERIALIZE_ENUM` macro to quickly define an enum/JSON mapping.\n134. [efp](https://github.com/efp) added line and column information to parse errors.\n135. [julian-becker](https://github.com/julian-becker) added BSON support.\n136. [Pratik Chowdhury](https://github.com/pratikpc) added support for structured bindings.\n137. [David Avedissian](https://github.com/davedissian) added support for Clang 5.0.1 (PS4 version).\n138. [Jonathan Dumaresq](https://github.com/dumarjo) implemented an input adapter to read from `FILE*`.\n139. [kjpus](https://github.com/kjpus) fixed a link in the documentation.\n140. [Manvendra Singh](https://github.com/manu-chroma) fixed a typo in the documentation.\n141. [ziggurat29](https://github.com/ziggurat29) fixed an MSVC warning.\n142. [Sylvain Corlay](https://github.com/SylvainCorlay) added code to avoid an issue with MSVC.\n143. [mefyl](https://github.com/mefyl) fixed a bug when JSON was parsed from an input stream.\n144. [Millian Poquet](https://github.com/mpoquet) allowed to install the library via Meson.\n145. [Michael Behrns-Miller](https://github.com/moodboom) found an issue with a missing namespace.\n146. [Nasztanovics Ferenc](https://github.com/naszta) fixed a compilation issue with libc 2.12.\n147. [Andreas Schwab](https://github.com/andreas-schwab) fixed the endian conversion.\n148. [Mark-Dunning](https://github.com/Mark-Dunning) fixed a warning in MSVC.\n149. [Gareth Sylvester-Bradley](https://github.com/garethsb-sony) added `operator/` for JSON Pointers.\n150. [John-Mark](https://github.com/johnmarkwayve) noted a missing header.\n151. [Vitaly Zaitsev](https://github.com/xvitaly) fixed compilation with GCC 9.0.\n152. [Laurent Stacul](https://github.com/stac47) fixed compilation with GCC 9.0.\n153. [Ivor Wanders](https://github.com/iwanders) helped to reduce the CMake requirement to version 3.1.\n154. [njlr](https://github.com/njlr) updated the Buckaroo instructions.\n155. [Lion](https://github.com/lieff) fixed a compilation issue with GCC 7 on CentOS.\n156. [Isaac Nickaein](https://github.com/nickaein) improved the integer serialization performance and implemented the `contains()` function.\n157. [past-due](https://github.com/past-due) suppressed an unfixable warning.\n158. [Elvis Oric](https://github.com/elvisoric) improved Meson support.\n159. [MatÄ›j Plch](https://github.com/Afforix) fixed an example in the README.\n160. [Mark Beckwith](https://github.com/wythe) fixed a typo.\n161. [scinart](https://github.com/scinart) fixed a bug in the serializer.\n162. [Patrick Boettcher](https://github.com/pboettch) implemented `push_back()` and `pop_back()` for JSON Pointers.\n163. [Bruno Oliveira](https://github.com/nicoddemus) added support for Conda.\n164. [Michele Caini](https://github.com/skypjack) fixed links in the README.\n165. [Hani](https://github.com/hnkb) documented how to install the library with NuGet.\n166. [Mark Beckwith](https://github.com/wythe) fixed a typo.\n167. [yann-morin-1998](https://github.com/yann-morin-1998) helped to reduce the CMake requirement to version 3.1.\n168. [Konstantin Podsvirov](https://github.com/podsvirov) maintains a package for the MSYS2 software distro.\n169. [remyabel](https://github.com/remyabel) added GNUInstallDirs to the CMake files.\n170. [Taylor Howard](https://github.com/taylorhoward92) fixed a unit test.\n171. [Gabe Ron](https://github.com/Macr0Nerd) implemented the `to_string` method.\n172. [Watal M. Iwasaki](https://github.com/heavywatal) fixed a Clang warning.\n173. [Viktor Kirilov](https://github.com/onqtam) switched the unit tests from [Catch](https://github.com/philsquared/Catch) to [doctest](https://github.com/onqtam/doctest)\n174. [Juncheng E](https://github.com/ejcjason) fixed a typo.\n175. [tete17](https://github.com/tete17) fixed a bug in the `contains` function.\n176. [Xav83](https://github.com/Xav83) fixed some cppcheck warnings.\n177. [0xflotus](https://github.com/0xflotus) fixed some typos.\n178. [Christian Deneke](https://github.com/chris0x44) added a const version of `json_pointer::back`.\n179. [Julien Hamaide](https://github.com/crazyjul) made the `items()` function work with custom string types.\n180. [Evan Nemerson](https://github.com/nemequ) updated fixed a bug in Hedley and updated this library accordingly.\n181. [Florian Pigorsch](https://github.com/flopp) fixed a lot of typos.\n182. [Camille BÃ©guÃ©](https://github.com/cbegue) fixed an issue in the conversion from  `std::pair` and `std::tuple` to `json`.\n183. [Anthony VH](https://github.com/AnthonyVH) fixed a compile error in an enum deserialization.\n184. [Yuriy Vountesmery](https://github.com/ua-code-dragon) noted a subtle bug in a preprocessor check.\n185. [Chen](https://github.com/dota17) fixed numerous issues in the library.\n186. [Antony Kellermann](https://github.com/aokellermann) added a CI step for GCC 10.1.\n187. [Alex](https://github.com/gistrec) fixed an MSVC warning.\n188. [Rainer](https://github.com/rvjr) proposed an improvement in the floating-point serialization in CBOR.\n189. [Francois Chabot](https://github.com/FrancoisChabot) made performance improvements in the input adapters.\n190. [Arthur Sonzogni](https://github.com/ArthurSonzogni) documented how the library can be included via `FetchContent`.\n191. [Rimas MiseviÄius](https://github.com/rmisev) fixed an error message.\n192. [Alexander Myasnikov](https://github.com/alexandermyasnikov) fixed some examples and a link in the README.\n193. [Hubert Chathi](https://github.com/uhoreg) made CMake's version config file architecture-independent.\n194. [OmnipotentEntity](https://github.com/OmnipotentEntity) implemented the binary values for CBOR, MessagePack, BSON, and UBJSON.\n195. [ArtemSarmini](https://github.com/ArtemSarmini) fixed a compilation issue with GCC 10 and fixed a leak.\n196. [Evgenii Sopov](https://github.com/sea-kg) integrated the library to the wsjcpp package manager.\n197. [Sergey Linev](https://github.com/linev) fixed a compiler warning.\n198. [Miguel MagalhÃ£es](https://github.com/magamig) fixed the year in the copyright.\n199. [Gareth Sylvester-Bradley](https://github.com/garethsb-sony) fixed a compilation issue with MSVC.\n200. [Alexander â€œweejâ€ Jones](https://github.com/alex-weej) fixed an example in the README.\n201. [Antoine CÅ“ur](https://github.com/Coeur) fixed some typos in the documentation.\n202. [jothepro](https://github.com/jothepro) updated links to the Hunter package.\n203. [Dave Lee](https://github.com/kastiglione) fixed a link in the README.\n204. [JoÃ«l Lamotte](https://github.com/Klaim) added instruction for using Build2's package manager.\n205. [Paul Jurczak](https://github.com/pauljurczak) fixed an example in the README.\n206. [Sonu Lohani](https://github.com/sonulohani) fixed a warning.\n207. [Carlos Gomes Martinho](https://github.com/gocarlos) updated the Conan package source.\n208. [Konstantin Podsvirov](https://github.com/podsvirov) fixed the MSYS2 package documentation.\n209. [Tridacnid](https://github.com/Tridacnid) improved the CMake tests.\n210. [Michael](https://github.com/MBalszun) fixed MSVC warnings.\n211. [Quentin Barbarat](https://github.com/quentin-dev) fixed an example in the documentation.\n212. [XyFreak](https://github.com/XyFreak) fixed a compiler warning.\n213. [TotalCaesar659](https://github.com/TotalCaesar659) fixed links in the README.\n214. [Tanuj Garg](https://github.com/tanuj208) improved the fuzzer coverage for UBSAN input.\n215. [AODQ](https://github.com/AODQ) fixed a compiler warning.\n216. [jwittbrodt](https://github.com/jwittbrodt) made `NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE` inline.\n217. [pfeatherstone](https://github.com/pfeatherstone) improved the upper bound of arguments of the `NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE`/`NLOHMANN_DEFINE_TYPE_INTRUSIVE` macros.\n218. [Jan ProchÃ¡zka](https://github.com/jprochazk) fixed a bug in the CBOR parser for binary and string values.\n219. [T0b1-iOS](https://github.com/T0b1-iOS) fixed a bug in the new hash implementation.\n220. [Matthew Bauer](https://github.com/matthewbauer) adjusted the CBOR writer to create tags for binary subtypes.\n221. [gatopeich](https://github.com/gatopeich) implemented an ordered map container for `nlohmann::ordered_json`.\n222. [Ã‰rico Nogueira Rolim](https://github.com/ericonr) added support for pkg-config.\n223. [KonanM](https://github.com/KonanM) proposed an implementation for the `NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE`/`NLOHMANN_DEFINE_TYPE_INTRUSIVE` macros.\n224. [Guillaume Racicot](https://github.com/gracicot) implemented `string_view` support and allowed C++20 support.\n225. [Alex Reinking](https://github.com/alexreinking) improved CMake support for `FetchContent`.\n226. [Hannes Domani](https://github.com/ssbssa) provided a GDB pretty printer.\n227. Lars Wirzenius reviewed the README file.\n228. [Jun Jie](https://github.com/ongjunjie) fixed a compiler path in the CMake scripts.\n229. [Ronak Buch](https://github.com/rbuch) fixed typos in the documentation.\n230. [Alexander Karzhenkov](https://github.com/karzhenkov) fixed a move constructor and the Travis builds.\n231. [Leonardo Lima](https://github.com/leozz37) added CPM.Cmake support.\n232. [Joseph Blackman](https://github.com/jbzdarkid) fixed a warning.\n233. [Yaroslav](https://github.com/YarikTH) updated doctest and implemented unit tests.\n234. [Martin Stump](https://github.com/globberwops) fixed a bug in the CMake files.\n235. [Jaakko Moisio](https://github.com/jasujm) fixed a bug in the input adapters.\n236. [bl-ue](https://github.com/bl-ue) fixed some Markdown issues in the README file.\n237. [William A. Wieselquist](https://github.com/wawiesel) fixed an example from the README.\n238. [abbaswasim](https://github.com/abbaswasim) fixed an example from the README.\n239. [Remy Jette](https://github.com/remyjette) fixed a warning.\n240. [Fraser](https://github.com/frasermarlow) fixed the documentation.\n241. [Ben Beasley](https://github.com/musicinmybrain) updated doctest.\n242. [Doron Behar](https://github.com/doronbehar) fixed pkg-config.pc.\n243. [raduteo](https://github.com/raduteo) fixed a warning.\n244. [David Pfahler](https://github.com/theShmoo) added the possibility to compile the library without I/O support.\n245. [Morten Fyhn Amundsen](https://github.com/mortenfyhn) fixed a typo.\n246. [jpl-mac](https://github.com/jpl-mac) allowed treating the library as a system header in CMake.\n247. [Jason Dsouza](https://github.com/jasmcaus) fixed the indentation of the CMake file.\n248. [offa](https://github.com/offa) added a link to Conan Center to the documentation.\n249. [TotalCaesar659](https://github.com/TotalCaesar659) updated the links in the documentation to use HTTPS.\n250. [Rafail Giavrimis](https://github.com/grafail) fixed the Google Benchmark default branch.\n251. [Louis Dionne](https://github.com/ldionne) fixed a conversion operator.\n252. [justanotheranonymoususer](https://github.com/justanotheranonymoususer) made the examples in the README more consistent.\n253. [Finkman](https://github.com/Finkman) suppressed some `-Wfloat-equal` warnings.\n254. [Ferry Huberts](https://github.com/fhuberts) fixed `-Wswitch-enum` warnings.\n255. [Arseniy Terekhin](https://github.com/senyai) made the GDB pretty-printer robust against unset variable names.\n256. [Amir Masoud Abdol](https://github.com/amirmasoudabdol) updated the Homebrew command as nlohmann/json is now in homebrew-core.\n257. [Hallot](https://github.com/Hallot) fixed some `-Wextra-semi-stmt warnings`.\n258. [Giovanni Cerretani](https://github.com/gcerretani) fixed `-Wunused` warnings on `JSON_DIAGNOSTICS`.\n259. [Bogdan Popescu](https://github.com/Kapeli) hosts the [docset](https://github.com/Kapeli/Dash-User-Contributions/tree/master/docsets/JSON_for_Modern_C%2B%2B) for offline documentation viewers.\n260. [Carl Smedstad](https://github.com/carlsmedstad) fixed an assertion error when using `JSON_DIAGNOSTICS`.\n261. [miikka75](https://github.com/miikka75) provided an important fix to compile C++17 code with Clang 9.\n262. [Maarten Becker](https://github.com/kernie) fixed a warning for shadowed variables.\n263. [Cristi VÃ®jdea](https://github.com/axnsan12) fixed typos in the `operator[]` documentation.\n264. [Alex Beregszaszi](https://github.com/axic) fixed spelling mistakes in comments.\n265. [Dirk Stolle](https://github.com/striezel) fixed typos in documentation.\n266. [Daniel Albuschat](https://github.com/daniel-kun) corrected the parameter name in the `parse` documentation.\n267. [Prince Mendiratta](https://github.com/Prince-Mendiratta) fixed a link to the FAQ.\n268. [Florian Albrechtskirchinger](https://github.com/falbrechtskirchinger) implemented `std::string_view` support for object keys and made dozens of other improvements.\n269. [Qianqian Fang](https://github.com/fangq) implemented the Binary JData (BJData) format.\n270. [pketelsen](https://github.com/pketelsen) added macros `NLOHMANN_DEFINE_TYPE_INTRUSIVE_WITH_DEFAULT` and `NLOHMANN_DEFINE_TYPE_NON_INTRUSIVE_WITH_DEFAULT`.\n271. [DarkZeros](https://github.com/DarkZeros) adjusted to code to not clash with Arduino defines.\n272. [flagarde](https://github.com/flagarde) fixed the output of `meta()` for MSVC.\n273. [Giovanni Cerretani](https://github.com/gcerretani) fixed a check for `std::filesystem`.\n274. [Dimitris Apostolou](https://github.com/rex4539) fixed a typo.\n275. [Ferry Huberts](https://github.com/fhuberts) fixed a typo.\n276. [Michael Nosthoff](https://github.com/heinemml) fixed a typo.\n277. [JungHoon Lee](https://github.com/jhnlee) fixed a typo.\n278. [Faruk D.](https://github.com/fdiblen) fixed the CITATION.CFF file.\n279. [Andrea Cocito](https://github.com/puffetto) added a clarification on macro usage to the documentation.\n280. [Krzysiek Karbowiak](https://github.com/kkarbowiak) refactored the tests to use `CHECK_THROWS_WITH_AS`.\n281. [Chaoqi Zhang](https://github.com/prncoprs) fixed a typo.\n282. [ivanovmp](https://github.com/ivanovmp) fixed a whitespace error.\n283. [KsaNL](https://github.com/KsaNL) fixed a build error when including `<windows.h>`.\n284. [Andrea Pappacoda](https://github.com/Tachi107) moved `.pc` and `.cmake` files to `share` directory.\n285. [Wolf Vollprecht](https://github.com/wolfv) added the `patch_inplace` function.\n286. [Jake Zimmerman](https://github.com/jez) highlighted common usage patterns in the README file.\n287. [NN](https://github.com/NN---) added the Visual Studio output directory to `.gitignore`.\n288. [Romain Reignier](https://github.com/romainreignier) improved the performance of the vector output adapter.\n289. [Mike](https://github.com/Mike-Leo-Smith) fixed the `std::iterator_traits`.\n290. [Richard HozÃ¡k](https://github.com/zxey) added macro `JSON_NO_ENUM` to disable default enum conversions.\n291. [vakokako](https://github.com/vakokako) fixed tests when compiling with C++20.\n292. [Alexander â€œweejâ€ Jones](https://github.com/alexweej) fixed an example in the README.\n293. [Eli Schwartz](https://github.com/eli-schwartz) added more files to the `include.zip` archive.\n294. [Kevin Lu](https://github.com/kevinlul) fixed a compilation issue when typedefs with certain names were present.\n295. [Trevor Hickey](https://github.com/luxe) improved the description of an example.\n296. [Jef LeCompte](https://github.com/jef) updated the year in the README file.\n297. [Alexandre Hamez](https://github.com/ahamez) fixed a warning.\n298. [Maninderpal Badhan](https://github.com/mbadhan) fixed a typo.\n299. [kevin--](https://github.com/kevin--) added a note to an example in the README file.\n300. [I](https://github.com/wx257osn2) fixed a typo.\n301. [Gregorio Litenstein](https://github.com/Lord-Kamina) fixed the Clang detection.\n302. [Andreas Smas](https://github.com/andoma) added a Doozer badge.\n303. [WanCW](https://github.com/wancw) fixed the string conversion with Clang.\n304. [zhaohuaxishi](https://github.com/zhaohuaxishi) fixed a Doxygen error.\n305. [emvivre](https://github.com/emvivre) removed an invalid parameter from CMake.\n306. [Tobias Hermann](https://github.com/Dobiasd) fixed a link in the README file.\n307. [Michael](https://github.com/traits) fixed a warning.\n308. [Ryan Mulder](https://github.com/ryanjmulder) added `ensure_ascii` to the `dump` function.\n309. [Muri Nicanor](https://github.com/murinicanor) fixed the `sed` discovery in the Makefile.\n310. [David Avedissian](https://github.com/dgavedissian) implemented SFINAE-friendly `iterator_traits`.\n311. [AQNOUCH Mohammed](https://github.com/aqnouch) fixed a typo in the README.\n312. [Gareth Sylvester-Bradley](https://github.com/garethsb) added `operator/=` and `operator/` to construct JSON pointers.\n313. [Michael Macnair](https://github.com/mykter) added support for afl-fuzz testing.\n314. [Berkus Decker](https://github.com/berkus) fixed a typo in the README.\n315. [Illia Polishchuk](https://github.com/effolkronium) improved the CMake testing.\n316. [Ikko Ashimine](https://github.com/eltociear) fixed a typo.\n317. [Raphael Grimm](https://github.com/barcode) added the possibility to define a custom base class.\n318. [tocic](https://github.com/tocic) fixed typos in the documentation.\n319. [Vertexwahn](https://github.com/Vertexwahn) added Bazel build support.\n320. [Dirk Stolle](https://github.com/striezel) fixed typos in the documentation.\n321. [DavidKorczynski](https://github.com/DavidKorczynski) added a CIFuzz CI GitHub action.\n322. [Finkman](https://github.com/Finkman) fixed the debug pretty-printer.\n323. [Florian Segginger](https://github.com/floriansegginger) bumped the years in the README.\n324. [haadfida](https://github.com/haadfida) cleaned up the badges of used services.\n325. [Arsen ArsenoviÄ‡](https://github.com/ArsenArsen) fixed a build error.\n326. [theevilone45](https://github.com/theevilone45) fixed a typo in a CMake file.\n327. [Sergei Trofimovich](https://github.com/trofi) fixed the custom allocator support.\n328. [Joyce](https://github.com/joycebrum) fixed some security issues in the GitHub workflows.\n329. [Nicolas Jakob](https://github.com/njakob) add vcpkg version badge.\n330. [Tomerkm](https://github.com/Tomerkm) added tests.\n331. [No.](https://github.com/tusooa) fixed the use of `get<>` calls.\n332. [taro](https://github.com/tarolling) fixed a typo in the `CODEOWNERS` file.\n333. [Ikko Eltociear Ashimine](https://github.com/eltociear) fixed a typo.\n334. [Felix Yan](https://github.com/felixonmars) fixed a typo in the README.\n335. [HO-COOH](https://github.com/HO-COOH) fixed a parenthesis in the documentation.\n336. [Ivor Wanders](https://github.com/iwanders) fixed the examples to catch exception by `const&`.\n337. [miny1233](https://github.com/miny1233) fixed a parenthesis in the documentation.\n338. [tomalakgeretkal](https://github.com/tomalakgeretkal) fixed a compilation error.\n339. [alferov](https://github.com/ALF-ONE) fixed a compilation error.\n340. [Craig Scott](https://github.com/craigscott-crascit) fixed a deprecation warning in CMake.\n341. [Vyacheslav Zhdanovskiy](https://github.com/ZeronSix) added macros for serialization-only types.\n342. [Mathieu Westphal](https://github.com/mwestphal) fixed typos.\n343. [scribam](https://github.com/scribam) fixed the MinGW workflow.\n344. [Aleksei Sapitskii](https://github.com/aleksproger) added support for Apple's Swift Package Manager.\n345. [Benjamin Buch](https://github.com/bebuch) fixed the installation path in CMake.\n346. [Colby Haskell](https://github.com/colbychaskell) clarified the parse error message in case a file cannot be opened.\n347. [Juan Carlos Arevalo Baeza](https://github.com/TheJCAB) fixed the enum conversion.\n348. [alferov](https://github.com/ALF-ONE) fixed a version in the documentation.\n349. [ss](https://github.com/serge-s) fixed the amalgamation call.\n350. [AniketDhemare](https://github.com/AniketDhemare) fixed a version in the documentation.\n351. [Philip MÃ¼ller](https://github.com/philip-paul-mueller) fixed an example.\n352. [Leila Shcheglova](https://github.com/LeilaShcheglova) fixed a warning in a test.\n353. [Alex Prabhat Bara](https://github.com/alexprabhat99) fixed a function name in the documentation.\n354. [laterlaugh](https://github.com/laterlaugh) fixed some typos.\n355. [Yuanhao Jia](https://github.com/MrJia1997) fixed the GDB pretty printer.\n356. [Fallen_Breath](https://github.com/Fallen-Breath) fixed an example for JSON Pointer.\n357. [Nikhil Idiculla](https://github.com/tsnl) fixed some typos.\n358. [Griffin Myers](https://github.com/gmyers18) updated the Natvis file.\n359. [thetimr](https://github.com/thetimr) fixed a typo in the documentation.\n360. [Balazs Erseki](https://github.com/zerocukor287) fixed a URL in the contribution guidelines.\n361. [NiccolÃ² Iardella](https://github.com/rotolof) added `NLOHMANN_DEFINE_DERIVED_TYPE_*` macros.\n362. [Borislav Stanimirov](https://github.com/iboB) allowed overriding the CMake target name.\n363. [Captain Crutches](https://github.com/captaincrutches) made `iterator_proxy_value` a `std::forward_iterator`.\n364. [Fredrik Sandhei](https://github.com/fsandhei) added type conversion support for `std::optional`.\n365. [jh96](https://github.com/jordan-hoang) added exceptions when `nullptr` is passed to `parse`.\n366. [Stuart Gorman](https://github.com/StuartGorman) fixed number parsing when `EINTR` set in `errno`.\n367. [Dylan Baker](https://github.com/dcbaker) generated a pkg-config file that follows the pkg-config conventions.\n368. [Tianyi Chen](https://github.com/TianyiChen) optimized the binary `get_number` implementation.\n369. [peng-wang-cn](https://github.com/peng-wang-cn) added type conversion support for multidimensional arrays.\n370. [Einars Netlis-Galejs](https://github.com/EinarsNG) added `ONLY_SERIALIZE` for `NLOHMANN_DEFINE_DERIVED_TYPE_*` macros.\n371. [Marcel](https://github.com/mering) removed `alwayslink=True` Bazel flag.\n372. [Harinath Nampally](https://github.com/hnampally) added diagnostic positions to exceptions.\n373. [Nissim Armand Ben Danan](https://github.com/NissimBendanan) fixed `NLOHMANN_DEFINE_TYPE_INTRUSIVE_WITH_DEFAULT` with an empty JSON instance.\n374. [Michael Valladolid](https://github.com/codenut) added support for BSON uint64 serialization/deserialization.\n375. [Nikhil](https://github.com/nikhilreddydev) updated the documentation.\n376. [NebojÅ¡a CvetkoviÄ‡](https://github.com/nebkat) added support for BJDATA optimized binary array type.\n377. [Sushrut Shringarputale](https://github.com/sushshring) added support for diagnostic positions. \n378. [kimci86](https://github.com/kimci86) templated to `NLOHMANN_DEFINE_TYPE` macros to also support `ordered_json`.\n379. [Richard Topchii](https://github.com/richardtop) added support for VisionOS in the Swift Package Manager.\n380. [Robert Chisholm](https://github.com/Robadob) fixed a typo.\n381. [zjyhjqs](https://github.com/zjyhjqs) added CPack support.\n382. [bitFiedler](https://github.com/bitFiedler) made GDB pretty printer work with Python 3.8.\n383. [Gianfranco Costamagna](https://github.com/LocutusOfBorg) fixed a compiler warning.\n384. [risa2000](https://github.com/risa2000) made `std::filesystem::path` conversion to/from UTF-8 encoded string explicit.\n\nThanks a lot for helping out! Please [let me know](mailto:mail@nlohmann.me) if I forgot someone.\n\n## Used third-party tools\n\nThe library itself consists of a single header file licensed under the MIT license. However, it is built, tested, documented, and whatnot using a lot of third-party tools and services. Thanks a lot!\n\n- [**amalgamate.py - Amalgamate C source and header files**](https://github.com/edlund/amalgamate) to create a single header file\n- [**American fuzzy lop**](https://lcamtuf.coredump.cx/afl/) for fuzz testing\n- [**AppVeyor**](https://www.appveyor.com) for [continuous integration](https://ci.appveyor.com/project/nlohmann/json) on Windows\n- [**Artistic Style**](http://astyle.sourceforge.net) for automatic source code indentation\n- [**Clang**](https://clang.llvm.org) for compilation with code sanitizers\n- [**CMake**](https://cmake.org) for build automation\n- [**Codacy**](https://www.codacy.com) for further [code analysis](https://app.codacy.com/gh/nlohmann/json/dashboard)\n- [**Coveralls**](https://coveralls.io) to measure [code coverage](https://coveralls.io/github/nlohmann/json)\n- [**Coverity Scan**](https://scan.coverity.com) for [static analysis](https://scan.coverity.com/projects/nlohmann-json)\n- [**cppcheck**](http://cppcheck.sourceforge.net) for static analysis\n- [**doctest**](https://github.com/onqtam/doctest) for the unit tests\n- [**GitHub Changelog Generator**](https://github.com/skywinder/github-changelog-generator) to generate the [ChangeLog](https://github.com/nlohmann/json/blob/develop/ChangeLog.md)\n- [**Google Benchmark**](https://github.com/google/benchmark) to implement the benchmarks\n- [**Hedley**](https://nemequ.github.io/hedley/) to avoid re-inventing several compiler-agnostic feature macros\n- [**lcov**](https://github.com/linux-test-project/lcov) to process coverage information and create an HTML view\n- [**libFuzzer**](https://llvm.org/docs/LibFuzzer.html) to implement fuzz testing for OSS-Fuzz\n- [**Material for MkDocs**](https://squidfunk.github.io/mkdocs-material/) for the style of the documentation site\n- [**MkDocs**](https://www.mkdocs.org) for the documentation site\n- [**OSS-Fuzz**](https://github.com/google/oss-fuzz) for continuous fuzz testing of the library ([project repository](https://github.com/google/oss-fuzz/tree/master/projects/json))\n- [**Probot**](https://probot.github.io) for automating maintainer tasks such as closing stale issues, requesting missing information, or detecting toxic comments.\n- [**Valgrind**](https://valgrind.org) to check for correct memory management\n\n## Notes\n\n### Character encoding\n\nThe library supports **Unicode input** as follows:\n\n- Only **UTF-8** encoded input is supported, which is the default encoding for JSON according to [RFC 8259](https://tools.ietf.org/html/rfc8259.html#section-8.1).\n- `std::u16string` and `std::u32string` can be parsed, assuming UTF-16 and UTF-32 encoding, respectively. These encodings are not supported when reading from files or other input containers.\n- Other encodings such as Latin-1 or ISO 8859-1 are **not** supported and will yield parse or serialization errors.\n- [Unicode noncharacters](https://www.unicode.org/faq/private_use.html#nonchar1) will not be replaced by the library.\n- Invalid surrogates (e.g., incomplete pairs such as `\\uDEAD`) will yield parse errors.\n- The strings stored in the library are UTF-8 encoded. When using the default string type (`std::string`), note that its length/size functions return the number of stored bytes rather than the number of characters or glyphs.\n- When you store strings with different encodings in the library, calling [`dump()`](https://json.nlohmann.me/api/basic_json/dump/) may throw an exception unless `json::error_handler_t::replace` or `json::error_handler_t::ignore` are used as error handlers.\n- To store wide strings (e.g., `std::wstring`), you need to convert them to a UTF-8 encoded `std::string` before, see [an example](https://json.nlohmann.me/home/faq/#wide-string-handling).\n\n### Comments in JSON\n\nThis library does not support comments by default. It does so for three reasons:\n\n1. Comments are not part of the [JSON specification](https://tools.ietf.org/html/rfc8259). You may argue that `//` or `/* */` are allowed in JavaScript, but JSON is not JavaScript.\n2. This was not an oversight: Douglas Crockford [wrote on this](https://plus.google.com/118095276221607585885/posts/RK8qyGVaGSr) in May 2012:\n  \n    > I removed comments from JSON because I saw people were using them to hold parsing directives, a practice which would have destroyed interoperability.  I know that the lack of comments makes some people sad, but it shouldn't.\n    >\n    > Suppose you are using JSON to keep configuration files, which you would like to annotate. Go ahead and insert all the comments you like. Then pipe it through JSMin before handing it to your JSON parser.\n  \n3. It is dangerous for interoperability if some libraries would add comment support while others don't. Please check [The Harmful Consequences of the Robustness Principle](https://tools.ietf.org/html/draft-iab-protocol-maintenance-01) on this.\n\nHowever, you can set set parameter `ignore_comments` to true in the `parse` function to ignore `//` or `/* */` comments. Comments will then be treated as whitespace.\n\n### Trailing commas\n\nThe JSON specification does not allow trailing commas in arrays and objects, and hence this library is treating them as parsing errors by default.\n\nLike comments, you can set parameter `ignore_trailing_commas` to true in the `parse` function to ignore trailing commas in arrays and objects. Note that a single comma as the only content of the array or object (`[,]` or `{,}`) is not allowed, and multiple trailing commas (`[1,,]`) are not allowed either.\n\nThis library does not add trailing commas when serializing JSON data.\n\nFor more information, see [JSON With Commas and Comments (JWCC)](https://nigeltao.github.io/blog/2021/json-with-commas-comments.html).\n\n### Order of object keys\n\nBy default, the library does not preserve the **insertion order of object elements**. This is standards-compliant, as the [JSON standard](https://tools.ietf.org/html/rfc8259.html) defines objects as \"an unordered collection of zero or more name/value pairs\".\n\nIf you do want to preserve the insertion order, you can try the type [`nlohmann::ordered_json`](https://github.com/nlohmann/json/issues/2179). Alternatively, you can use a more sophisticated ordered map like [`tsl::ordered_map`](https://github.com/Tessil/ordered-map) ([integration](https://github.com/nlohmann/json/issues/546#issuecomment-304447518)) or [`nlohmann::fifo_map`](https://github.com/nlohmann/fifo_map) ([integration](https://github.com/nlohmann/json/issues/485#issuecomment-333652309)).\n\nSee the [**documentation on object order**](https://json.nlohmann.me/features/object_order/) for more information.\n\n### Memory Release\n\nWe checked with Valgrind and the Address Sanitizer (ASAN) that there are no memory leaks.\n\nIf you find that a parsing program with this library does not release memory, please consider the following case, and it may be unrelated to this library.\n\n**Your program is compiled with glibc.** There is a tunable threshold that glibc uses to decide whether to actually return memory to the system or whether to cache it for later reuse. If in your program you make lots of small allocations and those small allocations are not a contiguous block and are presumably below the threshold, then they will not get returned to the OS.\nHere is a related issue [#1924](https://github.com/nlohmann/json/issues/1924).\n\n### Further notes\n\n- The code contains numerous debug **assertions** which can be switched off by defining the preprocessor macro `NDEBUG`, see the [documentation of `assert`](https://en.cppreference.com/w/cpp/error/assert). In particular, note [`operator[]`](https://json.nlohmann.me/api/basic_json/operator%5B%5D/) implements **unchecked access** for const objects: If the given key is not present, the behavior is undefined (think of a dereferenced null pointer) and yields an [assertion failure](https://github.com/nlohmann/json/issues/289) if assertions are switched on. If you are not sure whether an element in an object exists, use checked access with the [`at()` function](https://json.nlohmann.me/api/basic_json/at/). Furthermore, you can define `JSON_ASSERT(x)` to replace calls to `assert(x)`. See the [**documentation on runtime assertions**](https://json.nlohmann.me/features/assertions/) for more information.\n- As the exact number type is not defined in the [JSON specification](https://tools.ietf.org/html/rfc8259.html), this library tries to choose the best fitting C++ number type automatically. As a result, the type `double` may be used to store numbers which may yield [**floating-point exceptions**](https://github.com/nlohmann/json/issues/181) in certain rare situations if floating-point exceptions have been unmasked in the calling code. These exceptions are not caused by the library and need to be fixed in the calling code, such as by re-masking the exceptions prior to calling library functions.\n- The code can be compiled without C++ **runtime type identification** features; that is, you can use the `-fno-rtti` compiler flag.\n- **Exceptions** are used widely within the library. They can, however, be switched off with either using the compiler flag `-fno-exceptions` or by defining the symbol `JSON_NOEXCEPTION`. In this case, exceptions are replaced by `abort()` calls. You can further control this behavior by defining `JSON_THROW_USER` (overriding `throw`), `JSON_TRY_USER` (overriding `try`), and `JSON_CATCH_USER` (overriding `catch`). Note that `JSON_THROW_USER` should leave the current scope (e.g., by throwing or aborting), as continuing after it may yield undefined behavior. Note the explanatory [`what()`](https://en.cppreference.com/w/cpp/error/exception/what) string of exceptions is not available for MSVC if exceptions are disabled, see [#2824](https://github.com/nlohmann/json/discussions/2824). See the [**documentation of exceptions**](https://json.nlohmann.me/home/exceptions/) for more information.\n\n## Execute unit tests\n\nTo compile and run the tests, you need to execute\n\n```shell\nmkdir build\ncd build\ncmake .. -DJSON_BuildTests=On\ncmake --build .\nctest --output-on-failure\n```\n\nNote that during the `ctest` stage, several JSON test files are downloaded from an [external repository](https://github.com/nlohmann/json_test_data). If policies forbid downloading artifacts during testing, you can download the files yourself and pass the directory with the test files via `-DJSON_TestDataDirectory=path` to CMake. Then, no Internet connectivity is required. See [issue #2189](https://github.com/nlohmann/json/issues/2189) for more information.\n\nIf the testdata is not found, several test suites will fail like this:\n\n```\n===============================================================================\njson/tests/src/make_test_data_available.hpp:21:\nTEST CASE:  check test suite is downloaded\n\njson/tests/src/make_test_data_available.hpp:23: FATAL ERROR: REQUIRE( utils::check_testsuite_downloaded() ) is NOT correct!\n  values: REQUIRE( false )\n  logged: Test data not found in 'json/cmake-build-debug/json_test_data'.\n          Please execute target 'download_test_data' before running this test suite.\n          See <https://github.com/nlohmann/json#execute-unit-tests> for more information.\n\n===============================================================================\n```\n\nIn case you have downloaded the library rather than checked out the code via Git, test `cmake_fetch_content_configure` will fail. Please execute `ctest -LE git_required` to skip these tests. See [issue #2189](https://github.com/nlohmann/json/issues/2189) for more information.\n\nSome tests are requiring network to be properly execute. They are labeled as `git_required`. Please execute `ctest -LE git_required` to skip these tests. See [issue #4851](https://github.com/nlohmann/json/issues/4851) for more information.\n\nSome tests change the installed files and hence make the whole process not reproducible. Please execute `ctest -LE not_reproducible` to skip these tests. See [issue #2324](https://github.com/nlohmann/json/issues/2324) for more information. Furthermore, assertions must be switched off to ensure reproducible builds (see [discussion 4494](https://github.com/nlohmann/json/discussions/4494)).\n\nNote you need to call `cmake -LE \"not_reproducible|git_required\"` to exclude both labels. See [issue #2596](https://github.com/nlohmann/json/issues/2596) for more information.\n\nAs Intel compilers use unsafe floating point optimization by default, the unit tests may fail. Use flag [`/fp:precise`](https://www.intel.com/content/www/us/en/docs/cpp-compiler/developer-guide-reference/2021-8/fp-model-fp.html) then.\n",
      "stars_today": 8
    },
    {
      "id": 69495170,
      "name": "fastify",
      "full_name": "fastify/fastify",
      "description": "Fast and low overhead web framework, for Node.js",
      "html_url": "https://github.com/fastify/fastify",
      "stars": 35508,
      "forks": 2558,
      "language": "JavaScript",
      "topics": [
        "hacktoberfest",
        "nodejs",
        "performance",
        "speed",
        "webframework"
      ],
      "created_at": "2016-09-28T19:10:14Z",
      "updated_at": "2026-01-28T01:50:55Z",
      "pushed_at": "2026-01-26T12:08:54Z",
      "open_issues": 98,
      "owner": {
        "login": "fastify",
        "avatar_url": "https://avatars.githubusercontent.com/u/24939410?v=4"
      },
      "readme": "<div align=\"center\"> <a href=\"https://fastify.dev/\">\n    <img\n      src=\"https://github.com/fastify/graphics/raw/HEAD/fastify-landscape-outlined.svg\"\n      width=\"650\"\n      height=\"auto\"\n    />\n  </a>\n</div>\n\n<div align=\"center\">\n\n[![CI](https://github.com/fastify/fastify/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/fastify/fastify/actions/workflows/ci.yml)\n[![Package Manager\nCI](https://github.com/fastify/fastify/actions/workflows/package-manager-ci.yml/badge.svg?branch=main)](https://github.com/fastify/fastify/actions/workflows/package-manager-ci.yml)\n[![Web\nsite](https://github.com/fastify/fastify/actions/workflows/website.yml/badge.svg?branch=main)](https://github.com/fastify/fastify/actions/workflows/website.yml)\n[![neostandard javascript style](https://img.shields.io/badge/code_style-neostandard-brightgreen?style=flat)](https://github.com/neostandard/neostandard)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/7585/badge)](https://bestpractices.coreinfrastructure.org/projects/7585)\n\n</div>\n\n<div align=\"center\">\n\n[![NPM\nversion](https://img.shields.io/npm/v/fastify.svg?style=flat)](https://www.npmjs.com/package/fastify)\n[![NPM\ndownloads](https://img.shields.io/npm/dm/fastify.svg?style=flat)](https://www.npmjs.com/package/fastify)\n[![Security Responsible\nDisclosure](https://img.shields.io/badge/Security-Responsible%20Disclosure-yellow.svg)](https://github.com/fastify/fastify/blob/main/SECURITY.md)\n[![Discord](https://img.shields.io/discord/725613461949906985)](https://discord.gg/fastify)\n[![Contribute with Gitpod](https://img.shields.io/badge/Contribute%20with-Gitpod-908a85?logo=gitpod&color=blue)](https://gitpod.io/#https://github.com/fastify/fastify)\n[![Open Collective backers and sponsors](https://img.shields.io/opencollective/all/fastify)](https://github.com/sponsors/fastify#sponsors)\n\n</div>\n\n<br />\n\nAn efficient server implies a lower cost of the infrastructure, better\nresponsiveness under load, and happy users. How can you efficiently handle the\nresources of your server, knowing that you are serving the highest number of\nrequests possible, without sacrificing security validations and handy\ndevelopment?\n\nEnter Fastify. Fastify is a web framework highly focused on providing the best\ndeveloper experience with the least overhead and a powerful plugin architecture.\nIt is inspired by Hapi and Express and as far as we know, it is one of the\nfastest web frameworks in town.\n\nThe `main` branch refers to the Fastify `v5` release.\nCheck out the [`4.x` branch](https://github.com/fastify/fastify/tree/4.x) for `v4`.\n\n### Table of Contents\n\n - [Quick start](#quick-start)\n - [Install](#install)\n - [Example](#example)\n - [Core features](#core-features)\n - [Benchmarks](#benchmarks)\n - [Documentation](#documentation)\n - [Ecosystem](#ecosystem)\n - [Support](#support)\n - [Team](#team)\n - [Hosted by](#hosted-by)\n - [License](#license)\n\n\n### Quick start\n\nCreate a folder and make it your current working directory:\n\n```sh\nmkdir my-app\ncd my-app\n```\n\nGenerate a fastify project with `npm init`:\n\n```sh\nnpm init fastify\n```\n\nInstall dependencies:\n\n```sh\nnpm i\n```\n\nTo start the app in dev mode:\n\n```sh\nnpm run dev\n```\n\nFor production mode:\n\n```sh\nnpm start\n```\n\nUnder the hood `npm init` downloads and runs [Fastify\nCreate](https://github.com/fastify/create-fastify), which in turn uses the\ngenerate functionality of [Fastify CLI](https://github.com/fastify/fastify-cli).\n\n\n### Install\n\nTo install Fastify in an existing project as a dependency:\n\n```sh\nnpm i fastify\n```\n\n### Example\n\n```js\n// Require the framework and instantiate it\n\n// ESM\nimport Fastify from 'fastify'\n\nconst fastify = Fastify({\n  logger: true\n})\n// CommonJs\nconst fastify = require('fastify')({\n  logger: true\n})\n\n// Declare a route\nfastify.get('/', (request, reply) => {\n  reply.send({ hello: 'world' })\n})\n\n// Run the server!\nfastify.listen({ port: 3000 }, (err, address) => {\n  if (err) throw err\n  // Server is now listening on ${address}\n})\n```\n\nWith async-await:\n\n```js\n// ESM\nimport Fastify from 'fastify'\n\nconst fastify = Fastify({\n  logger: true\n})\n// CommonJs\nconst fastify = require('fastify')({\n  logger: true\n})\n\nfastify.get('/', async (request, reply) => {\n  reply.type('application/json').code(200)\n  return { hello: 'world' }\n})\n\nfastify.listen({ port: 3000 }, (err, address) => {\n  if (err) throw err\n  // Server is now listening on ${address}\n})\n```\n\nDo you want to know more? Head to the <a\nhref=\"./docs/Guides/Getting-Started.md\"><code><b>Getting Started</b></code></a>.\nIf you learn best by reading code, explore the official [demo](https://github.com/fastify/demo).\n\n> ## Note\n> `.listen` binds to the local host, `localhost`, interface by default\n> (`127.0.0.1` or `::1`, depending on the operating system configuration). If\n> you are running Fastify in a container (Docker,\n> [GCP](https://cloud.google.com/), etc.), you may need to bind to `0.0.0.0`. Be\n> careful when listening on all interfaces; it comes with inherent\n> [security\n> risks](https://web.archive.org/web/20170711105010/https://snyk.io/blog/mongodb-hack-and-secure-defaults/).\n> See [the documentation](./docs/Reference/Server.md#listen) for more\n> information.\n\n### Core features\n\n- **Highly performant:** as far as we know, Fastify is one of the fastest web\n  frameworks in town, depending on the code complexity we can serve up to 76+\n  thousand requests per second.\n- **Extensible:** Fastify is fully extensible via its hooks, plugins, and\n  decorators.\n- **Schema-based:** even if it is not mandatory we recommend using [JSON\n  Schema](https://json-schema.org/) to validate your routes and serialize your\n  outputs. Internally Fastify compiles the schema in a highly performant\n  function.\n- **Logging:** logs are extremely important but are costly; we chose the best\n  logger to almost remove this cost, [Pino](https://github.com/pinojs/pino)!\n- **Developer friendly:** the framework is built to be very expressive and help\n  developers in their daily use without sacrificing performance and\n  security.\n\n### Benchmarks\n\n__Machine:__ EX41S-SSD, Intel Core i7, 4Ghz, 64GB RAM, 4C/8T, SSD.\n\n__Method:__: `autocannon -c 100 -d 40 -p 10 localhost:3000` * 2, taking the\nsecond average\n\n| Framework          | Version                    | Router?      |  Requests/sec |\n| :----------------- | :------------------------- | :----------: | ------------: |\n| Express            | 4.17.3                     | &#10003;     | 14,200        |\n| hapi               | 20.2.1                     | &#10003;     | 42,284        |\n| Restify            | 8.6.1                      | &#10003;     | 50,363        |\n| Koa                | 2.13.0                     | &#10007;     | 54,272        |\n| **Fastify**        | **4.0.0**                  | **&#10003;** | **77,193**    |\n| -                  |                            |              |               |\n| `http.Server`      | 16.14.2\t                  | &#10007;     | 74,513        |\n\nThese benchmarks taken using https://github.com/fastify/benchmarks. This is a\nsynthetic \"hello world\" benchmark that aims to evaluate the framework overhead.\nThe overhead that each framework has on your application depends on your\napplication. You should __always__ benchmark if performance matters to you.\n\n## Documentation\n* [__`Getting Started`__](./docs/Guides/Getting-Started.md)\n* [__`Guides`__](./docs/Guides/Index.md)\n* [__`Server`__](./docs/Reference/Server.md)\n* [__`Routes`__](./docs/Reference/Routes.md)\n* [__`Encapsulation`__](./docs/Reference/Encapsulation.md)\n* [__`Logging`__](./docs/Reference/Logging.md)\n* [__`Middleware`__](./docs/Reference/Middleware.md)\n* [__`Hooks`__](./docs/Reference/Hooks.md)\n* [__`Decorators`__](./docs/Reference/Decorators.md)\n* [__`Validation and Serialization`__](./docs/Reference/Validation-and-Serialization.md)\n* [__`Fluent Schema`__](./docs/Guides/Fluent-Schema.md)\n* [__`Lifecycle`__](./docs/Reference/Lifecycle.md)\n* [__`Reply`__](./docs/Reference/Reply.md)\n* [__`Request`__](./docs/Reference/Request.md)\n* [__`Errors`__](./docs/Reference/Errors.md)\n* [__`Content Type Parser`__](./docs/Reference/ContentTypeParser.md)\n* [__`Plugins`__](./docs/Reference/Plugins.md)\n* [__`Testing`__](./docs/Guides/Testing.md)\n* [__`Benchmarking`__](./docs/Guides/Benchmarking.md)\n* [__`How to write a good plugin`__](./docs/Guides/Write-Plugin.md)\n* [__`Plugins Guide`__](./docs/Guides/Plugins-Guide.md)\n* [__`HTTP2`__](./docs/Reference/HTTP2.md)\n* [__`Long Term Support`__](./docs/Reference/LTS.md)\n* [__`TypeScript and types support`__](./docs/Reference/TypeScript.md)\n* [__`Serverless`__](./docs/Guides/Serverless.md)\n* [__`Recommendations`__](./docs/Guides/Recommendations.md)\n\n## Ecosystem\n\n- [Core](./docs/Guides/Ecosystem.md#core) - Core plugins maintained by the\n  _Fastify_ [team](#team).\n- [Community](./docs/Guides/Ecosystem.md#community) - Community-supported\n  plugins.\n- [Live Examples](https://github.com/fastify/example) - Multirepo with a broad\n  set of real working examples.\n- [Discord](https://discord.gg/D3FZYPy) - Join our discord server and chat with\n  the maintainers.\n\n## Support\nPlease visit [Fastify help](https://github.com/fastify/help) to view prior\nsupport issues and to ask new support questions.\n\nVersion 3 of Fastify and lower are EOL and will not receive any security or bug\nfixes.\n\nFastify's partner, HeroDevs, provides commercial security fixes for all\nunsupported versions at [https://herodevs.com/support/fastify-nes][hd-link].\nFastify's supported version matrix is available in the\n[Long Term Support][lts-link] documentation.\n\n## Contributing\n\nWhether reporting bugs, discussing improvements and new ideas, or writing code,\nwe welcome contributions from anyone and everyone. Please read the [CONTRIBUTING](./CONTRIBUTING.md)\nguidelines before submitting pull requests.\n\n## Team\n\n_Fastify_ is the result of the work of a great community. Team members are\nlisted in alphabetical order.\n\n**Lead Maintainers:**\n* [__Matteo Collina__](https://github.com/mcollina),\n  <https://x.com/matteocollina>, <https://www.npmjs.com/~matteo.collina>\n* [__Tomas Della Vedova__](https://github.com/delvedor),\n  <https://x.com/delvedor>, <https://www.npmjs.com/~delvedor>\n* [__KaKa Ng__](https://github.com/climba03003),\n  <https://www.npmjs.com/~climba03003>\n* [__Manuel Spigolon__](https://github.com/eomm),\n  <https://x.com/manueomm>, <https://www.npmjs.com/~eomm>\n* [__James Sumners__](https://github.com/jsumners),\n  <https://x.com/jsumners79>, <https://www.npmjs.com/~jsumners>\n\n### Fastify Core team\n* [__Aras Abbasi__](https://github.com/uzlopak),\n  <https://www.npmjs.com/~uzlopak>\n* [__Harry Brundage__](https://github.com/airhorns/),\n  <https://x.com/harrybrundage>, <https://www.npmjs.com/~airhorns>\n* [__Matteo Collina__](https://github.com/mcollina),\n  <https://x.com/matteocollina>, <https://www.npmjs.com/~matteo.collina>\n* [__GÃ¼rgÃ¼n DayÄ±oÄŸlu__](https://github.com/gurgunday),\n  <https://www.npmjs.com/~gurgunday>\n* [__Tomas Della Vedova__](https://github.com/delvedor),\n  <https://x.com/delvedor>, <https://www.npmjs.com/~delvedor>\n* [__Carlos Fuentes__](https://github.com/metcoder95),\n  <https://x.com/metcoder95>, <https://www.npmjs.com/~metcoder95>\n* [__Vincent Le Goff__](https://github.com/zekth)\n* [__Luciano Mammino__](https://github.com/lmammino),\n  <https://x.com/loige>, <https://www.npmjs.com/~lmammino>\n* [__Jean Michelet__](https://github.com/jean-michelet),\n  <https://www.npmjs.com/~jean-michelet>\n* [__KaKa Ng__](https://github.com/climba03003),\n  <https://www.npmjs.com/~climba03003>\n* [__Luis Orbaiceta__](https://github.com/luisorbaiceta),\n  <https://x.com/luisorbai>, <https://www.npmjs.com/~luisorbaiceta>\n* [__Maksim Sinik__](https://github.com/fox1t),\n  <https://x.com/maksimsinik>, <https://www.npmjs.com/~fox1t>\n* [__Manuel Spigolon__](https://github.com/eomm),\n  <https://x.com/manueomm>, <https://www.npmjs.com/~eomm>\n* [__James Sumners__](https://github.com/jsumners),\n  <https://x.com/jsumners79>, <https://www.npmjs.com/~jsumners>\n\n### Fastify Plugins team\n* [__Harry Brundage__](https://github.com/airhorns/),\n  <https://x.com/harrybrundage>, <https://www.npmjs.com/~airhorns>\n* [__Simone Busoli__](https://github.com/simoneb),\n  <https://x.com/simonebu>, <https://www.npmjs.com/~simoneb>\n* [__Dan Castillo__](https://github.com/dancastillo),\n  <https://www.npmjs.com/~dancastillo>\n* [__Matteo Collina__](https://github.com/mcollina),\n  <https://x.com/matteocollina>, <https://www.npmjs.com/~matteo.collina>\n* [__GÃ¼rgÃ¼n DayÄ±oÄŸlu__](https://github.com/gurgunday),\n  <https://www.npmjs.com/~gurgunday>\n* [__Tomas Della Vedova__](https://github.com/delvedor),\n  <https://x.com/delvedor>, <https://www.npmjs.com/~delvedor>\n* [__Carlos Fuentes__](https://github.com/metcoder95),\n  <https://x.com/metcoder95>, <https://www.npmjs.com/~metcoder95>\n* [__Vincent Le Goff__](https://github.com/zekth)\n* [__Jean Michelet__](https://github.com/jean-michelet),\n  <https://www.npmjs.com/~jean-michelet>\n* [__KaKa Ng__](https://github.com/climba03003),\n  <https://www.npmjs.com/~climba03003>\n* [__Maksim Sinik__](https://github.com/fox1t),\n  <https://x.com/maksimsinik>, <https://www.npmjs.com/~fox1t>\n* [__Frazer Smith__](https://github.com/Fdawgs), <https://www.npmjs.com/~fdawgs>\n* [__Manuel Spigolon__](https://github.com/eomm),\n  <https://x.com/manueomm>, <https://www.npmjs.com/~eomm>\n\n### Emeritus Contributors\nGreat contributors to a specific area of the Fastify ecosystem will be invited\nto join this group by Lead Maintainers when they decide to step down from the\nactive contributor's group.\n\n* [__Tommaso Allevi__](https://github.com/allevo),\n  <https://x.com/allevitommaso>, <https://www.npmjs.com/~allevo>\n* [__Ethan Arrowood__](https://github.com/Ethan-Arrowood/),\n  <https://x.com/arrowoodtech>, <https://www.npmjs.com/~ethan_arrowood>\n* [__Ã‡aÄŸatay Ã‡alÄ±__](https://github.com/cagataycali),\n  <https://x.com/cagataycali>, <https://www.npmjs.com/~cagataycali>\n* [__David Mark Clements__](https://github.com/davidmarkclements),\n  <https://x.com/davidmarkclem>,\n  <https://www.npmjs.com/~davidmarkclements>\n* [__dalisoft__](https://github.com/dalisoft), <https://x.com/dalisoft>,\n  <https://www.npmjs.com/~dalisoft>\n* [__Dustin Deus__](https://github.com/StarpTech),\n  <https://x.com/dustindeus>, <https://www.npmjs.com/~starptech>\n* [__Denis FÃ¤cke__](https://github.com/SerayaEryn),\n  <https://x.com/serayaeryn>, <https://www.npmjs.com/~serayaeryn>\n* [__Rafael Gonzaga__](https://github.com/rafaelgss),\n  <https://x.com/_rafaelgss>, <https://www.npmjs.com/~rafaelgss>\n* [__Trivikram Kamat__](https://github.com/trivikr),\n  <https://x.com/trivikram>, <https://www.npmjs.com/~trivikr>\n* [__Ayoub El Khattabi__](https://github.com/AyoubElk),\n  <https://x.com/ayoubelkh>, <https://www.npmjs.com/~ayoubelk>\n* [__Cemre Mengu__](https://github.com/cemremengu),\n  <https://x.com/cemremengu>, <https://www.npmjs.com/~cemremengu>\n* [__Salman Mitha__](https://github.com/salmanm),\n  <https://www.npmjs.com/~salmanm>\n* [__Nathan Woltman__](https://github.com/nwoltman),\n  <https://x.com/NathanWoltman>, <https://www.npmjs.com/~nwoltman>\n\n## Hosted by\n\n[<img\nsrc=\"https://github.com/openjs-foundation/artwork/blob/main/openjs_foundation/openjs_foundation-logo-horizontal-color.png?raw=true\"\nwidth=\"250px;\"/>](https://openjsf.org/projects)\n\nWe are an [At-Large\nProject](https://github.com/openjs-foundation/cross-project-council/blob/HEAD/PROJECT_PROGRESSION.md#at-large-projects)\nin the [OpenJS Foundation](https://openjsf.org/).\n\n## Sponsors\n\nSupport this project by becoming a [SPONSOR](./SPONSORS.md)!\nFastify has an [Open Collective](https://opencollective.com/fastify)\npage where we accept and manage financial contributions.\n\n## Acknowledgments\n\nThis project is kindly sponsored by:\n- [NearForm](https://nearform.com)\n- [Platformatic](https://platformatic.dev)\n\nPast Sponsors:\n- [LetzDoIt](https://www.letzdoitapp.com/)\n\nThis list includes all companies that support one or more team members\nin maintaining this project.\n\n## License\n\nLicensed under [MIT](./LICENSE).\n\nFor your convenience, here is a list of all the licenses of our production\ndependencies:\n- MIT\n- ISC\n- BSD-3-Clause\n- BSD-2-Clause\n\n[hd-link]: https://www.herodevs.com/support/fastify-nes?utm_source=fastify&utm_medium=link&utm_campaign=github_readme\n[lts-link]: https://fastify.dev/docs/latest/Reference/LTS/\n",
      "stars_today": 8
    },
    {
      "id": 204164353,
      "name": "kestra",
      "full_name": "kestra-io/kestra",
      "description": "Event Driven Orchestration & Scheduling Platform for Mission Critical Applications",
      "html_url": "https://github.com/kestra-io/kestra",
      "stars": 26276,
      "forks": 2473,
      "language": "Java",
      "topics": [
        "automation",
        "data-orchestration",
        "devops",
        "hacktoberfest",
        "high-availability",
        "infrastructure-as-code",
        "java",
        "low-code",
        "lowcode",
        "orchestration",
        "pipeline",
        "pipeline-as-code",
        "workflow"
      ],
      "created_at": "2019-08-24T13:56:15Z",
      "updated_at": "2026-01-28T02:04:40Z",
      "pushed_at": "2026-01-27T16:54:26Z",
      "open_issues": 562,
      "owner": {
        "login": "kestra-io",
        "avatar_url": "https://avatars.githubusercontent.com/u/59033362?v=4"
      },
      "readme": "<p align=\"center\">\n  <a href=\"https://www.kestra.io\">\n    <img src=\"https://kestra.io/banner.png\"  alt=\"Kestra workflow orchestrator\" />\n  </a>\n</p>\n\n<h1 align=\"center\" style=\"border-bottom: none\">\n    Event-Driven Declarative Orchestration Platform\n</h1>\n\n<div align=\"center\">\n <a href=\"https://github.com/kestra-io/kestra/releases\"><img src=\"https://img.shields.io/github/tag-pre/kestra-io/kestra.svg?color=blueviolet\" alt=\"Last Version\" /></a>\n  <a href=\"https://github.com/kestra-io/kestra/blob/develop/LICENSE\"><img src=\"https://img.shields.io/github/license/kestra-io/kestra?color=blueviolet\" alt=\"License\" /></a>\n  <a href=\"https://github.com/kestra-io/kestra/stargazers\"><img src=\"https://img.shields.io/github/stars/kestra-io/kestra?color=blueviolet&logo=github\" alt=\"Github star\" /></a> <br>\n<a href=\"https://kestra.io\"><img src=\"https://img.shields.io/badge/Website-kestra.io-192A4E?color=blueviolet\" alt=\"Kestra infinitely scalable orchestration and scheduling platform\"></a>\n<a href=\"https://kestra.io/slack\"><img src=\"https://img.shields.io/badge/Slack-Join%20Community-blueviolet?logo=slack\" alt=\"Slack\"></a>\n</div>\n\n<br />\n\n<p align=\"center\">\n  <a href=\"https://twitter.com/kestra_io\" style=\"margin: 0 10px;\">\n        <img height=\"25\" src=\"https://kestra.io/twitter.svg\" alt=\"twitter\" width=\"35\" height=\"25\" /></a>\n  <a href=\"https://www.linkedin.com/company/kestra/\" style=\"margin: 0 10px;\">\n        <img height=\"25\" src=\"https://kestra.io/linkedin.svg\" alt=\"linkedin\" width=\"35\" height=\"25\" /></a> \n  <a href=\"https://www.youtube.com/@kestra-io\" style=\"margin: 0 10px;\">\n        <img height=\"25\" src=\"https://kestra.io/youtube.svg\" alt=\"youtube\" width=\"35\" height=\"25\" /></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://trendshift.io/repositories/2714\" target=\"_blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/2714\" alt=\"kestra-io%2Fkestra | Trendshift\" width=\"250\" height=\"55\"/>\n  </a>\n  <a href=\"https://www.producthunt.com/posts/kestra?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-kestra\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=612077&theme=light&period=daily&t=1740737506162\" alt=\"Kestra - All&#0045;in&#0045;one&#0032;automation&#0032;&#0038;&#0032;orchestration&#0032;platform | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://go.kestra.io/video/product-overview\" target=\"_blank\">\n        <img src=\"https://kestra.io/startvideo.png\" alt=\"Get started in 3 minutes with Kestra\" width=\"640px\" />\n    </a>\n</p>\n<p align=\"center\" style=\"color:grey;\"><i>Click on the image to learn how to get started with Kestra in 3 minutes.</i></p>\n\n\n## ğŸŒŸ What is Kestra?\n\nKestra is an open-source, event-driven orchestration platform that makes both **scheduled** and **event-driven** workflows easy. By bringing **Infrastructure as Code** best practices to data, process, and microservice orchestration, you can build reliable [workflows](https://kestra.io/docs/getting-started) directly from the UI in just a few lines of YAML.\n\n**Key Features:**\n- **Everything as Code and from the UI:** keep **workflows as code** with a **Git Version Control** integration, even when building them from the UI.\n- **Event-Driven & Scheduled Workflows:** automate both **scheduled** and **real-time** event-driven workflows via a simple `trigger` definition.\n- **Declarative YAML Interface:** define workflows using a simple configuration in the **built-in code editor**.\n- **Rich Plugin Ecosystem:** hundreds of plugins built in to extract data from any database, cloud storage, or API, and **run scripts in any language**.\n- **Intuitive UI & Code Editor:** build and visualize workflows directly from the UI with syntax highlighting, auto-completion and real-time syntax validation.\n- **Scalable:** designed to handle millions of workflows, with high availability and fault tolerance.\n- **Version Control Friendly:** write your workflows from the built-in code Editor and push them to your preferred Git branch directly from Kestra, enabling best practices with CI/CD pipelines and version control systems.\n- **Structure & Resilience**: tame chaos and bring resilience to your workflows with **namespaces**, **labels**, **subflows**, **retries**, **timeout**, **error handling**, **inputs**, **outputs** that generate artifacts in the UI, **variables**, **conditional branching**, **advanced scheduling**, **event triggers**, **backfills**, **dynamic tasks**, **sequential and parallel tasks**, and skip tasks or triggers when needed by setting the flag `disabled` to `true`.\n\n\nğŸ§‘â€ğŸ’» The YAML definition gets automatically adjusted any time you make changes to a workflow from the UI or via an API call. Therefore, the orchestration logic is **always managed declaratively in code**, even if you modify your workflows in other ways (UI, CI/CD, Terraform, API calls).\n\n\n<p align=\"center\">\n  <img src=\"https://kestra.io/adding-tasks.gif\" alt=\"Adding new tasks in the UI\">\n</p>\n\n---\n\n## ğŸš€ Quick Start\n\n### Launch on AWS (CloudFormation)\n\nDeploy Kestra on AWS using our CloudFormation template:\n\n[![Launch Stack](https://cdn.rawgit.com/buildkite/cloudformation-launch-stack-button-svg/master/launch-stack.svg)](https://console.aws.amazon.com/cloudformation/home#/stacks/create/review?templateURL=https://kestra-deployment-templates.s3.eu-west-3.amazonaws.com/aws/cloudformation/ec2-rds-s3/kestra-oss.yaml&stackName=kestra-oss)\n\n### Launch on Google Cloud (Terraform deployment)\n\nDeploy Kestra on Google Cloud Infrastructure Manager using [our Terraform module](https://github.com/kestra-io/deployment-templates/tree/main/gcp/terraform/infrastructure-manager/vm-sql-gcs).\n\n### Get Started Locally in 5 Minutes\n\n#### Launch Kestra in Docker\n\nMake sure that Docker is running. Then, start Kestra in a single command:\n\n```bash\ndocker run --pull=always --rm -it -p 8080:8080 --user=root \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /tmp:/tmp kestra/kestra:latest server local\n```\n\nIf you're on Windows and use PowerShell:\n```powershell\ndocker run --pull=always --rm -it -p 8080:8080 --user=root `\n    -v \"/var/run/docker.sock:/var/run/docker.sock\" `\n    -v \"C:/Temp:/tmp\" kestra/kestra:latest server local\n```\n\nIf you're on Windows and use Command Prompt (CMD):\n```cmd\ndocker run --pull=always --rm -it -p 8080:8080 --user=root ^\n    -v \"/var/run/docker.sock:/var/run/docker.sock\" ^\n    -v \"C:/Temp:/tmp\" kestra/kestra:latest server local\n```\n\nIf you're on Windows and use WSL (Linux-based environment in Windows):\n```bash\ndocker run --pull=always --rm -it -p 8080:8080 --user=root \\\n    -v \"/var/run/docker.sock:/var/run/docker.sock\" \\\n    -v \"/mnt/c/Temp:/tmp\" kestra/kestra:latest server local\n```\n\nCheck our [Installation Guide](https://kestra.io/docs/installation) for other deployment options (Docker Compose, Podman, Kubernetes, AWS, GCP, Azure, and more).\n\nAccess the Kestra UI at [http://localhost:8080](http://localhost:8080) and start building your first flow!\n\n#### Your First Hello World Flow\n\nCreate a new flow with the following content:\n\n```yaml\nid: hello_world\nnamespace: dev\n\ntasks:\n  - id: say_hello\n    type: io.kestra.plugin.core.log.Log\n    message: \"Hello, World!\"\n```\n\n\nRun the flow and see the output in the UI!\n\n---\n\n## ğŸ§© Plugin Ecosystem\n\nKestra's functionality is extended through a rich [ecosystem of plugins](https://kestra.io/plugins) that empower you to run tasks anywhere and code in any language, including Python, Node.js, R, Go, Shell, and more. Here's how Kestra plugins enhance your workflows:\n\n- **Run Anywhere:**\n  - **Local or Remote Execution:** Execute tasks on your local machine, remote servers via SSH, or scale out to serverless containers using [Task Runners](https://kestra.io/docs/task-runners).\n  - **Docker and Kubernetes Support:** Seamlessly run Docker containers within your workflows or launch Kubernetes jobs to handle compute-intensive workloads.\n\n- **Code in Any Language:**\n  - **Scripting Support:** Write scripts in your preferred programming language. Kestra supports Python, Node.js, R, Go, Shell, and others, allowing you to integrate existing codebases and deployment patterns.\n  - **Flexible Automation:** Execute shell commands, run SQL queries against various databases, and make HTTP requests to interact with APIs.\n\n- **Event-Driven and Real-Time Processing:**\n  - **Real-Time Triggers:** React to events from external systems in real-time, such as file arrivals, new messages in message buses (Kafka, Redis, Pulsar, AMQP, MQTT, NATS, AWS SQS, Google Pub/Sub, Azure Event Hubs), and more.\n  - **Custom Events:** Define custom events to trigger flows based on specific conditions or external signals, enabling highly responsive workflows.\n\n- **Cloud Integrations:**\n  - **AWS, Google Cloud, Azure:** Integrate with a variety of cloud services to interact with storage solutions, messaging systems, compute resources, and more.\n  - **Big Data Processing:** Run big data processing tasks using tools like Apache Spark or interact with analytics platforms like Google BigQuery.\n\n- **Monitoring and Notifications:**\n  - **Stay Informed:** Send messages to Slack channels, email notifications, or trigger alerts in PagerDuty to keep your team updated on workflow statuses.\n\nKestra's plugin ecosystem is continually expanding, allowing you to tailor the platform to your specific needs. Whether you're orchestrating complex data pipelines, automating scripts across multiple environments, or integrating with cloud services, there's likely a plugin to assist. And if not, you can always [build your own plugins](https://kestra.io/docs/plugin-developer-guide/) to extend Kestra's capabilities.\n\nğŸ§‘â€ğŸ’» **Note:** This is just a glimpse of what Kestra plugins can do. Explore the full list on our [Plugins Page](https://kestra.io/plugins).\n\n---\n\n## ğŸ“š Key Concepts\n\n- **Flows:** the core unit in Kestra, representing a workflow composed of tasks.\n- **Tasks:** individual units of work, such as running a script, moving data, or calling an API.\n- **Namespaces:** logical grouping of flows for organization and isolation.\n- **Triggers:** schedule or events that initiate the execution of flows.\n- **Inputs & Variables:** parameters and dynamic data passed into flows and tasks.\n\n---\n\n## ğŸ¨ Build Workflows Visually\n\nKestra provides an intuitive UI that allows you to interactively build and visualize your workflows:\n\n- **Drag-and-Drop Interface:** add and rearrange tasks from the Topology Editor.\n- **Real-Time Validation:** instant feedback on your workflow's syntax and structure to catch errors early.\n- **Auto-Completion:** smart suggestions as you type to write flow code quickly and without syntax errors.\n- **Live Topology View:** see your workflow as a Directed Acyclic Graph (DAG) that updates in real-time.\n\n---\n\n\n## ğŸ”§ Extensible and Developer-Friendly\n\n### Plugin Development\n\nCreate custom plugins to extend Kestra's capabilities. Check out our [Plugin Developer Guide](https://kestra.io/docs/plugin-developer-guide/) to get started.\n\n### Infrastructure as Code\n\n- **Version Control:** store your flows in Git repositories.\n- **CI/CD Integration:** automate deployment of flows using CI/CD pipelines.\n- **Terraform Provider:** manage Kestra resources with the [official Terraform provider](https://kestra.io/docs/terraform/).\n\n---\n\n## ğŸŒ Join the Community\n\nStay connected and get support:\n\n- **Slack:** Join our [Slack community](https://kestra.io/slack) to ask questions and share ideas.\n- **LinkedIn:** Follow us on [LinkedIn](https://www.linkedin.com/company/kestra/) â€” next to Slack and GitHub, this is our main channel to share updates and product announcements.\n- **YouTube:** Subscribe to our [YouTube channel](https://www.youtube.com/@kestra-io) for educational video content. We publish new videos every week!\n- **X:** Follow us on [X](https://x.com/kestra_io) if you're still active there.\n\n---\n\n## ğŸ¤ Contributing\n\nWe welcome contributions of all kinds!\n\n- **Report Issues:** Found a bug or have a feature request? Open an [issue on GitHub](https://github.com/kestra-io/kestra/issues).\n- **Contribute Code:** Check out our [Contributor Guide](https://kestra.io/docs/getting-started/contributing) for initial guidelines, and explore our [good first issues](https://go.kestra.io/contributing) for beginner-friendly tasks to tackle first.\n- **Develop Plugins:** Build and share plugins using our [Plugin Developer Guide](https://kestra.io/docs/plugin-developer-guide/).\n- **Contribute to our Docs:** Contribute edits or updates to keep our [documentation](https://github.com/kestra-io/docs) top-notch.\n\n---\n\n## ğŸ“„ License\n\nKestra is licensed under the Apache 2.0 License Â© [Kestra Technologies](https://kestra.io).\n\n---\n\n## â­ï¸ Stay Updated\n\nGive our repository a star to stay informed about the latest features and updates!\n\n[![Star the Repo](https://kestra.io/star.gif)](https://github.com/kestra-io/kestra)\n\n---\n\nThank you for considering Kestra for your workflow orchestration needs. We can't wait to see what you'll build!\n\n",
      "stars_today": 8
    },
    {
      "id": 190729906,
      "name": "budibase",
      "full_name": "Budibase/budibase",
      "description": "Create business apps and automate workflows in minutes. Supports PostgreSQL, MySQL, MariaDB, MSSQL, MongoDB, Rest API, Docker, K8s, and more ğŸš€ No code / Low code platform..",
      "html_url": "https://github.com/Budibase/budibase",
      "stars": 27585,
      "forks": 2069,
      "language": "TypeScript",
      "topics": [
        "ai-app-builder",
        "ai-applications",
        "crud-app",
        "crud-application",
        "data-application",
        "data-apps",
        "internal-tools",
        "it-workflows",
        "low-code",
        "low-code-no-code",
        "low-code-platform",
        "no-code",
        "no-code-platform",
        "open-source",
        "rest-api-framework",
        "sql-gui",
        "workflow-apps",
        "workflow-automation",
        "workflow-engine"
      ],
      "created_at": "2019-06-07T11:03:41Z",
      "updated_at": "2026-01-28T00:53:28Z",
      "pushed_at": "2026-01-27T21:07:16Z",
      "open_issues": 320,
      "owner": {
        "login": "Budibase",
        "avatar_url": "https://avatars.githubusercontent.com/u/45009727?v=4"
      },
      "readme": "<p align=\"center\">\n  <a href=\"https://www.budibase.com\">\n    <img alt=\"Budibase\" src=\"https://res.cloudinary.com/daog6scxm/image/upload/v1696515725/Branding/Assets/Symbol/RGB/Full%20Colour/Budibase_Symbol_RGB_FullColour_cbqvha_1_z5cwq2.svg\" width=\"60\" />\n  </a>\n</p>\n<h1 align=\"center\">\n  Budibase\n</h1>\n\n<h3 align=\"center\">\n  The low code platform you'll enjoy using\n</h3>\n<p align=\"center\">\n  Budibase is an open-source low-code platform that saves engineers 100s of hours building forms, portals, and approval apps, securely.\n</p>\n\n<h3 align=\"center\">\n ğŸ¤– ğŸ¨ ğŸš€\n</h3>\n<br>\n\n<p align=\"center\">\n  <img alt=\"Budibase design ui\" src=\"https://res.cloudinary.com/daog6scxm/image/upload/v1680181644/ui/homepage-design-ui_sizp7b.png\">\n</p>\n\n<p align=\"center\">\n  <a href=\"https://github.com/Budibase/budibase/releases\">\n    <img alt=\"GitHub all releases\" src=\"https://img.shields.io/github/downloads/Budibase/budibase/total\">\n  </a>\n  <a href=\"https://github.com/Budibase/budibase/releases\">\n    <img alt=\"GitHub release (latest by date)\" src=\"https://img.shields.io/github/v/release/Budibase/budibase\">\n  </a>\n  <a href=\"https://twitter.com/intent/follow?screen_name=budibase\">\n    <img src=\"https://img.shields.io/twitter/follow/budibase?style=social\" alt=\"Follow @budibase\" />\n  </a>\n  <img src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\" alt=\"Code of conduct\" />\n  <a href=\"https://codecov.io/gh/Budibase/budibase\">\n    <img src=\"https://codecov.io/gh/Budibase/budibase/graph/badge.svg?token=E8W2ZFXQOH\"/>\n  </a>\n</p>\n\n<h3 align=\"center\">\n  <a href=\"https://account.budibase.app/register\">Get started - we host (Budibase Cloud)</a>\n  <span> Â· </span>\n  <a href=\"https://docs.budibase.com/docs/hosting-methods\">Get started - you host (Docker, K8s, DO)</a>\n  <span> Â· </span>\n  <a href=\"https://docs.budibase.com/docs\">Docs</a>\n  <span> Â· </span>\n  <a href=\"https://github.com/Budibase/budibase/discussions?discussions_q=category%3AIdeas\">Feature request</a>\n  <span> Â· </span>\n  <a href=\"https://github.com/Budibase/budibase/issues\">Report a bug</a>\n  <span> Â· </span>\n  Support: <a href=\"https://github.com/Budibase/budibase/discussions\">Discussions</a>\n</h3>\n\n<br /><br />\n\n## âœ¨ Features\n\n### Build and ship real software\n\nUnlike other platforms, with Budibase you build and ship single page applications. Budibase applications have performance baked in and can be designed responsively, providing users with a great experience.\n<br /><br />\n\n### Open source and extensible\n\nBudibase is open-source - licensed as GPL v3. This should fill you with confidence that Budibase will always be around. You can also code against Budibase or fork it and make changes as you please, providing a developer-friendly experience.\n<br /><br />\n\n### Load data or start from scratch\n\nBudibase pulls data from multiple sources, including MongoDB, CouchDB, PostgreSQL, MariaDB, MySQL, Airtable, S3, DynamoDB, or a REST API. And unlike other platforms, with Budibase you can start from scratch and create business apps with no data sources. [Request new datasources](https://github.com/Budibase/budibase/discussions?discussions_q=category%3AIdeas).\n\n<p align=\"center\">\n  <img alt=\"Budibase data\" src=\"https://res.cloudinary.com/daog6scxm/image/upload/v1680281798/ui/data_klbuna.png\">\n</p>\n<br /><br />\n\n### Design and build apps with powerful pre-made components\n\nBudibase comes out of the box with beautifully designed, powerful components which you can use like building blocks to build your UI. We also expose many of your favourite CSS styling options so you can go that extra creative mile. [Request new component](https://github.com/Budibase/budibase/discussions?discussions_q=category%3AIdeas).\n\n<p align=\"center\">\n  <img alt=\"Budibase design\" src=\"https://res.cloudinary.com/daog6scxm/image/upload/v1675437167/ui/form_2x_mbli8y.png\">\n</p>\n<br /><br />\n\n### Automate processes, integrate with other tools and connect to webhooks\n\nSave time by automating manual processes and workflows. From connecting to webhooks to automating emails, simply tell Budibase what to do and let it work for you. You can easily [create new automations for Budibase here](https://github.com/Budibase/automations) or [Request new automation](https://github.com/Budibase/budibase/discussions?discussions_q=category%3AIdeas).\n<br /><br />\n\n### Integrate with your favorite tools\n\nBudibase integrates with a number of popular tools allowing you to build apps that perfectly fit your stack.\n\n<p align=\"center\">\n  <img alt=\"Budibase integrations\" src=\"https://res.cloudinary.com/daog6scxm/image/upload/v1680195228/ui/automate_fg9z07.png\">\n</p>\n<br /><br />\n\n### Deploy with confidence and security\n\nBudibase is made to scale. With Budibase, you can self-host on your own infrastructure and globally manage users, onboarding, SMTP, apps, groups, theming and more. You can also provide users/groups with an app portal and disseminate user management to the group manager.\n\n- Checkout the promo video: https://youtu.be/xoljVpty_Kw\n\n<br />\n\n---\n\n<br />\n\n## Budibase Public API\n\nAs with anything that we build in Budibase, our new public API is simple to use, flexible, and introduces new extensibility. To summarize, the Budibase API enables:\n\n- Budibase as a backend\n- Interoperability\n\n#### Docs\n\nYou can learn more about the Budibase API at the following places:\n\n- [General documentation](https://docs.budibase.com/docs/public-api): Learn how to get your API key, how to use spec, and how to use Postman\n- [Interactive API documentation](https://docs.budibase.com/reference/appcreate) : Learn how to interact with the API\n\n<br /><br />\n\n## ğŸ Get started\n\nDeploy Budibase using Docker, Kubernetes, and Digital Ocean on your existing infrastructure. Or use Budibase Cloud if you don't need to self-host and would like to get started quickly.\n\n### [Get started with self-hosting Budibase](https://docs.budibase.com/docs/hosting-methods)\n\n- [Docker - single ARM compatible image](https://docs.budibase.com/docs/docker)\n- [Docker Compose](https://docs.budibase.com/docs/docker-compose)\n- [Kubernetes](https://docs.budibase.com/docs/kubernetes-k8s)\n- [Digital Ocean](https://docs.budibase.com/docs/digitalocean)\n- [Portainer](https://docs.budibase.com/docs/portainer)\n\n### [Get started with Budibase Cloud](https://budibase.com)\n\n<br /><br />\n\n## ğŸ“ Learning Budibase\n\nThe Budibase documentation [lives here](https://docs.budibase.com/docs).\n<br />\n\n<br /><br />\n\n## ğŸ’¬ Community\n\nIf you have a question or would like to talk with other Budibase users and join our community, please hop over to [Github discussions](https://github.com/Budibase/budibase/discussions)\n\n<br /><br /><br />\n\n## â— Code of conduct\n\nBudibase is dedicated to providing everyone a welcoming, diverse, and harassment-free experience. We expect everyone in the Budibase community to abide by our [**Code of Conduct**](https://github.com/Budibase/budibase/blob/HEAD/docs/CODE_OF_CONDUCT.md). Please read it.\n<br />\n\n<br /><br />\n\n## ğŸ™Œ Contributing to Budibase\n\nFrom opening a bug report to creating a pull request: every contribution is appreciated and welcomed. If you're planning to implement a new feature or change the API, please create an issue first. This way, we can ensure your work is not in vain.\nEnvironment setup instructions are available [here](https://github.com/Budibase/budibase/tree/HEAD/docs/CONTRIBUTING.md).\n\n### Not Sure Where to Start?\n\nA good place to start contributing is by looking for the [good first issue](https://github.com/Budibase/budibase/labels/good%20first%20issue) tag.\n\n### How the repository is organized\n\nBudibase is a monorepo managed by lerna. Lerna manages the building and publishing of the budibase packages. At a high level, here are the packages that make up Budibase.\n\n- [packages/builder](https://github.com/Budibase/budibase/tree/HEAD/packages/builder) - contains code for the budibase builder client-side svelte application.\n\n- [packages/client](https://github.com/Budibase/budibase/tree/HEAD/packages/client) - A module that runs in the browser responsible for reading JSON definition and creating living, breathing web apps from it.\n\n- [packages/server](https://github.com/Budibase/budibase/tree/HEAD/packages/server) - The budibase server. This Koa app is responsible for serving the JS for the builder and budibase apps, as well as providing the API for interaction with the database and file system.\n\nFor more information, see [CONTRIBUTING.md](https://github.com/Budibase/budibase/blob/HEAD/docs/CONTRIBUTING.md)\n\n<br /><br />\n\n## ğŸ“ License\n\nBudibase is open-source, licensed as [GPL v3](https://www.gnu.org/licenses/gpl-3.0.en.html). The client and component libraries are licensed as [MPL](https://directory.fsf.org/wiki/License:MPL-2.0) - so the apps you build can be licensed however you like.\n\n<br /><br />\n\n## â­ Stargazers over time\n\n[![Stargazers over time](https://starchart.cc/Budibase/budibase.svg)](https://starchart.cc/Budibase/budibase)\n\nIf you are having issues between updates of the builder, please use the guide [here](https://github.com/Budibase/budibase/blob/HEAD/docs/CONTRIBUTING.md#troubleshooting) to clear down your environment.\n\n<br /><br />\n\n## Contributors âœ¨\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<a href=\"https://github.com/Budibase/budibase/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=Budibase/budibase\" />\n</a>\n\nMade with [contrib.rocks](https://contrib.rocks).\n",
      "stars_today": 8
    },
    {
      "id": 488002779,
      "name": "open-gpu-kernel-modules",
      "full_name": "NVIDIA/open-gpu-kernel-modules",
      "description": "NVIDIA Linux open GPU kernel module source",
      "html_url": "https://github.com/NVIDIA/open-gpu-kernel-modules",
      "stars": 16650,
      "forks": 1569,
      "language": "C",
      "topics": [],
      "created_at": "2022-05-02T22:00:04Z",
      "updated_at": "2026-01-28T00:52:12Z",
      "pushed_at": "2026-01-23T18:02:17Z",
      "open_issues": 281,
      "owner": {
        "login": "NVIDIA",
        "avatar_url": "https://avatars.githubusercontent.com/u/1728152?v=4"
      },
      "readme": "# NVIDIA Linux Open GPU Kernel Module Source\n\nThis is the source release of the NVIDIA Linux open GPU kernel modules,\nversion 590.48.01.\n\n\n## How to Build\n\nTo build:\n\n    make modules -j$(nproc)\n\nTo install, first uninstall any existing NVIDIA kernel modules.  Then,\nas root:\n\n    make modules_install -j$(nproc)\n\nNote that the kernel modules built here must be used with GSP\nfirmware and user-space NVIDIA GPU driver components from a corresponding\n590.48.01 driver release.  This can be achieved by installing\nthe NVIDIA GPU driver from the .run file using the `--no-kernel-modules`\noption.  E.g.,\n\n    sh ./NVIDIA-Linux-[...].run --no-kernel-modules\n\n\n## Supported Target CPU Architectures\n\nCurrently, the kernel modules can be built for x86_64 or aarch64.\nIf cross-compiling, set these variables on the make command line:\n\n    TARGET_ARCH=aarch64|x86_64\n    CC\n    LD\n    AR\n    CXX\n    OBJCOPY\n\nE.g.,\n\n    # compile on x86_64 for aarch64\n    make modules -j$(nproc)         \\\n        TARGET_ARCH=aarch64         \\\n        CC=aarch64-linux-gnu-gcc    \\\n        LD=aarch64-linux-gnu-ld     \\\n        AR=aarch64-linux-gnu-ar     \\\n        CXX=aarch64-linux-gnu-g++   \\\n        OBJCOPY=aarch64-linux-gnu-objcopy\n\n\n## Other Build Knobs\n\nNV_VERBOSE - Set this to \"1\" to print each complete command executed;\n    otherwise, a succinct \"CC\" line is printed.\n\nDEBUG - Set this to \"1\" to build the kernel modules as debug.  By default, the\n    build compiles without debugging information.  This also enables\n    various debug log messages in the kernel modules.\n\nThese variables can be set on the make command line.  E.g.,\n\n    make modules -j$(nproc) NV_VERBOSE=1\n\n\n## Supported Toolchains\n\nAny reasonably modern version of GCC or Clang can be used to build the\nkernel modules.  Note that the kernel interface layers of the kernel\nmodules must be built with the toolchain that was used to build the\nkernel.\n\n\n## Supported Linux Kernel Versions\n\nThe NVIDIA open kernel modules support the same range of Linux kernel\nversions that are supported with the proprietary NVIDIA kernel modules.\nThis is currently Linux kernel 4.15 or newer.\n\n\n## How to Contribute\n\nContributions can be made by creating a pull request on\nhttps://github.com/NVIDIA/open-gpu-kernel-modules\nWe'll respond via GitHub.\n\nNote that when submitting a pull request, you will be prompted to accept\na Contributor License Agreement.\n\nThis code base is shared with NVIDIA's proprietary drivers, and various\nprocessing is performed on the shared code to produce the source code that is\npublished here.  This has several implications for the foreseeable future:\n\n* The GitHub repository will function mostly as a snapshot of each driver\n  release.\n\n* We do not expect to be able to provide revision history for individual\n  changes that were made to NVIDIA's shared code base.  There will likely\n  only be one git commit per driver release.\n\n* We may not be able to reflect individual contributions as separate\n  git commits in the GitHub repository.\n\n* Because the code undergoes various processing prior to publishing here,\n  contributions made here require manual merging to be applied to the shared\n  code base.  Therefore, large refactoring changes made here may be difficult to\n  merge and accept back into the shared code base.  If you have large\n  refactoring to suggest, please contact us in advance, so we can coordinate.\n\n\n## How to Report Issues\n\nProblems specific to the Open GPU Kernel Modules can be reported in the\nIssues section of the https://github.com/NVIDIA/open-gpu-kernel-modules\nrepository.\n\nFurther, any of the existing bug reporting venues can be used to communicate\nproblems to NVIDIA, such as our forum:\n\nhttps://forums.developer.nvidia.com/c/gpu-graphics/linux/148\n\nor linux-bugs@nvidia.com.\n\nPlease see the 'NVIDIA Contact Info and Additional Resources' section\nof the NVIDIA GPU Driver README for details.\n\nPlease see the separate [SECURITY.md](SECURITY.md) document if you\nbelieve you have discovered a security vulnerability in this software.\n\n\n## Kernel Interface and OS-Agnostic Components of Kernel Modules\n\nMost of NVIDIA's kernel modules are split into two components:\n\n* An \"OS-agnostic\" component: this is the component of each kernel module\n  that is independent of operating system.\n\n* A \"kernel interface layer\": this is the component of each kernel module\n  that is specific to the Linux kernel version and configuration.\n\nWhen packaged in the NVIDIA .run installation package, the OS-agnostic\ncomponent is provided as a binary: it is large and time-consuming to\ncompile, so pre-built versions are provided so that the user does\nnot have to compile it during every driver installation.  For the\nnvidia.ko kernel module, this component is named \"nv-kernel.o_binary\".\nFor the nvidia-modeset.ko kernel module, this component is named\n\"nv-modeset-kernel.o_binary\".  Neither nvidia-drm.ko nor nvidia-uvm.ko\nhave OS-agnostic components.\n\nThe kernel interface layer component for each kernel module must be built\nfor the target kernel.\n\n\n## Directory Structure Layout\n\n- `kernel-open/`                The kernel interface layer\n- `kernel-open/nvidia/`         The kernel interface layer for nvidia.ko\n- `kernel-open/nvidia-drm/`     The kernel interface layer for nvidia-drm.ko\n- `kernel-open/nvidia-modeset/` The kernel interface layer for nvidia-modeset.ko\n- `kernel-open/nvidia-uvm/`     The kernel interface layer for nvidia-uvm.ko\n\n- `src/`                        The OS-agnostic code\n- `src/nvidia/`                 The OS-agnostic code for nvidia.ko\n- `src/nvidia-modeset/`         The OS-agnostic code for nvidia-modeset.ko\n- `src/common/`                 Utility code used by one or more of nvidia.ko and nvidia-modeset.ko\n- `nouveau/`                    Tools for integration with the Nouveau device driver\n\n\n## Nouveau device driver integration\n\nThe Python script in the 'nouveau' directory is used to extract some of the\nfirmware binary images (and related data) encoded in the source code and\nstore them as distinct files.  These files are used by the Nouveau device\ndriver to load and communicate with the GSP firmware.\n\nThe layout of the binary files is described in nouveau_firmware_layout.ods,\nwhich is an OpenDocument Spreadsheet file, compatible with most spreadsheet\nsoftware applications.\n\n\n## Compatible GPUs\n\nThe NVIDIA open kernel modules can be used on any Turing or later GPU (see the\ntable below).\n\nFor details on feature support and limitations, see the NVIDIA GPU driver\nend user README here:\n\nhttps://us.download.nvidia.com/XFree86/Linux-x86_64/590.48.01/README/kernel_open.html\n\nFor vGPU support, please refer to the README.vgpu packaged in the vGPU Host\nPackage for more details.\n\nIn the below table, if three IDs are listed, the first is the PCI Device \nID, the second is the PCI Subsystem Vendor ID, and the third is the PCI\nSubsystem Device ID.\n\n| Product Name                                            | PCI ID         |\n| ------------------------------------------------------- | -------------- |\n| NVIDIA TITAN RTX                                        | 1E02           |\n| NVIDIA GeForce RTX 2080 Ti                              | 1E04           |\n| NVIDIA GeForce RTX 2080 Ti                              | 1E07           |\n| NVIDIA CMP 50HX                                         | 1E09           |\n| Quadro RTX 6000                                         | 1E30           |\n| Quadro RTX 8000                                         | 1E30 1028 129E |\n| Quadro RTX 8000                                         | 1E30 103C 129E |\n| Quadro RTX 8000                                         | 1E30 10DE 129E |\n| Quadro RTX 6000                                         | 1E36           |\n| Quadro RTX 8000                                         | 1E78 10DE 13D8 |\n| Quadro RTX 6000                                         | 1E78 10DE 13D9 |\n| NVIDIA GeForce RTX 2080 SUPER                           | 1E81           |\n| NVIDIA GeForce RTX 2080                                 | 1E82           |\n| NVIDIA GeForce RTX 2070 SUPER                           | 1E84           |\n| NVIDIA GeForce RTX 2080                                 | 1E87           |\n| NVIDIA GeForce RTX 2060                                 | 1E89           |\n| NVIDIA GeForce RTX 2080                                 | 1E90           |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1025 1375 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1028 08A1 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1028 08A2 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1028 08EA |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1028 08EB |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1028 08EC |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1028 08ED |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1028 08EE |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1028 08EF |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1028 093B |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1028 093C |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 103C 8572 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 103C 8573 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 103C 8602 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 103C 8606 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 103C 86C6 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 103C 86C7 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 103C 87A6 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 103C 87A7 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1043 131F |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1043 137F |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1043 141F |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1043 1751 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1458 1660 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1458 1661 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1458 1662 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1458 75A6 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1458 75A7 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1458 86A6 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1458 86A7 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1462 1274 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1462 1277 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 152D 1220 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1558 95E1 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1558 97E1 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1A58 2002 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1A58 2005 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1A58 2007 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1A58 3000 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1A58 3001 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1E90 1D05 1069 |\n| NVIDIA GeForce RTX 2070 Super                           | 1E91           |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 103C 8607 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 103C 8736 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 103C 8738 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 103C 8772 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 103C 878A |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 103C 878B |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1043 1E61 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1458 1511 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1458 75B3 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1458 75B4 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1458 76B2 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1458 76B3 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1458 78A2 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1458 78A3 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1458 86B2 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1458 86B3 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1462 12AE |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1462 12B0 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1462 12C6 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 17AA 22C3 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 17AA 22C5 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1A58 2009 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1A58 200A |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 1A58 3002 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1E91 8086 3012 |\n| NVIDIA GeForce RTX 2080 Super                           | 1E93           |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1025 1401 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1025 149C |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1028 09D2 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 103C 8607 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 103C 86C7 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 103C 8736 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 103C 8738 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 103C 8772 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 103C 87A6 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 103C 87A7 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1458 75B1 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1458 75B2 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1458 76B0 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1458 76B1 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1458 78A0 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1458 78A1 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1458 86B0 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1458 86B1 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1462 12AE |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1462 12B0 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1462 12B4 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1462 12C6 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1558 50D3 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1558 70D1 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 17AA 22C3 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 17AA 22C5 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1A58 2009 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1A58 200A |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1A58 3002 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1E93 1D05 1089 |\n| Quadro RTX 5000                                         | 1EB0           |\n| Quadro RTX 4000                                         | 1EB1           |\n| Quadro RTX 5000                                         | 1EB5           |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 1025 1375 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 1025 1401 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 1025 149C |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 1028 09C3 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 103C 8736 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 103C 8738 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 103C 8772 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 103C 8780 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 103C 8782 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 103C 8783 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 103C 8785 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 1043 1DD1 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 1462 1274 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 1462 12B0 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 1462 12C6 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 17AA 22B8 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 17AA 22BA |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 1A58 2005 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 1A58 2007 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 1A58 2008 |\n| Quadro RTX 5000 with Max-Q Design                       | 1EB5 1A58 200A |\n| Quadro RTX 4000                                         | 1EB6           |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 1028 09C3 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 103C 8736 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 103C 8738 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 103C 8772 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 103C 8780 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 103C 8782 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 103C 8783 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 103C 8785 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 1462 1274 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 1462 1277 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 1462 12B0 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 1462 12C6 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 17AA 22B8 |\n| Quadro RTX 4000 with Max-Q Design                       | 1EB6 17AA 22BA |\n| Tesla T4                                                | 1EB8 10DE 12A2 |\n| NVIDIA GeForce RTX 2070 SUPER                           | 1EC2           |\n| NVIDIA GeForce RTX 2070 SUPER                           | 1EC7           |\n| NVIDIA GeForce RTX 2080                                 | 1ED0           |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1ED0 1025 132D |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1ED0 1028 08ED |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1ED0 1028 08EE |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1ED0 1028 08EF |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1ED0 103C 8572 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1ED0 103C 8573 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1ED0 103C 8600 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1ED0 103C 8605 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1ED0 1043 138F |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1ED0 1043 15C1 |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1ED0 17AA 3FEE |\n| NVIDIA GeForce RTX 2080 with Max-Q Design               | 1ED0 17AA 3FFE |\n| NVIDIA GeForce RTX 2070 Super                           | 1ED1           |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1ED1 1025 1432 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1ED1 103C 8746 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1ED1 103C 878A |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1ED1 1043 165F |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1ED1 144D C192 |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1ED1 17AA 3FCE |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1ED1 17AA 3FCF |\n| NVIDIA GeForce RTX 2070 Super with Max-Q Design         | 1ED1 17AA 3FD0 |\n| NVIDIA GeForce RTX 2080 Super                           | 1ED3           |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1ED3 1025 1432 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1ED3 1028 09D1 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1ED3 103C 8746 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1ED3 103C 878A |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1ED3 1043 1D61 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1ED3 1043 1E51 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1ED3 1043 1F01 |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1ED3 17AA 3FCE |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1ED3 17AA 3FCF |\n| NVIDIA GeForce RTX 2080 Super with Max-Q Design         | 1ED3 17AA 3FD0 |\n| Quadro RTX 5000                                         | 1EF5           |\n| NVIDIA GeForce RTX 2070                                 | 1F02           |\n| NVIDIA GeForce RTX 2060                                 | 1F03           |\n| NVIDIA GeForce RTX 2060 SUPER                           | 1F06           |\n| NVIDIA GeForce RTX 2070                                 | 1F07           |\n| NVIDIA GeForce RTX 2060                                 | 1F08           |\n| NVIDIA GeForce GTX 1650                                 | 1F0A           |\n| NVIDIA CMP 40HX                                         | 1F0B           |\n| NVIDIA GeForce RTX 2070                                 | 1F10           |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1025 132D |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1025 1342 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1028 08A1 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1028 08A2 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1028 08EA |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1028 08EB |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1028 08EC |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1028 08ED |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1028 08EE |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1028 08EF |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1028 093B |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1028 093C |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 103C 8572 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 103C 8573 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 103C 8602 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 103C 8606 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1043 132F |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1043 136F |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1043 1881 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1043 1E6E |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1458 1658 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1458 1663 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1458 1664 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1458 75A4 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1458 75A5 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1458 86A4 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1458 86A5 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1462 1274 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1462 1277 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1558 95E1 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1558 97E1 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1A58 2002 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1A58 2005 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1A58 2007 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1A58 3000 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1A58 3001 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1D05 105E |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1D05 1070 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 1D05 2087 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F10 8086 2087 |\n| NVIDIA GeForce RTX 2060                                 | 1F11           |\n| NVIDIA GeForce RTX 2060                                 | 1F12           |\n| NVIDIA GeForce RTX 2060 with Max-Q Design               | 1F12 1028 098F |\n| NVIDIA GeForce RTX 2060 with Max-Q Design               | 1F12 103C 8741 |\n| NVIDIA GeForce RTX 2060 with Max-Q Design               | 1F12 103C 8744 |\n| NVIDIA GeForce RTX 2060 with Max-Q Design               | 1F12 103C 878E |\n| NVIDIA GeForce RTX 2060 with Max-Q Design               | 1F12 103C 880E |\n| NVIDIA GeForce RTX 2060 with Max-Q Design               | 1F12 1043 1E11 |\n| NVIDIA GeForce RTX 2060 with Max-Q Design               | 1F12 1043 1F11 |\n| NVIDIA GeForce RTX 2060 with Max-Q Design               | 1F12 1462 12D9 |\n| NVIDIA GeForce RTX 2060 with Max-Q Design               | 1F12 17AA 3801 |\n| NVIDIA GeForce RTX 2060 with Max-Q Design               | 1F12 17AA 3802 |\n| NVIDIA GeForce RTX 2060 with Max-Q Design               | 1F12 17AA 3803 |\n| NVIDIA GeForce RTX 2070                                 | 1F14           |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1025 1401 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1025 1432 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1025 1442 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1025 1446 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1025 147D |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1028 09E2 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1028 09F3 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 103C 8607 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 103C 86C6 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 103C 86C7 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 103C 8736 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 103C 8738 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 103C 8746 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 103C 8772 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 103C 878A |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 103C 878B |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 103C 87A6 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 103C 87A7 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1043 174F |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1458 1512 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1458 75B5 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1458 75B6 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1458 76B4 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1458 76B5 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1458 78A4 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1458 78A5 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1458 86B4 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1458 86B5 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1462 12AE |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1462 12B0 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1462 12C6 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1558 50D3 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1558 70D1 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1A58 200C |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1A58 2011 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F14 1A58 3002 |\n| NVIDIA GeForce RTX 2060                                 | 1F15           |\n| Quadro RTX 3000                                         | 1F36           |\n| Quadro RTX 3000 with Max-Q Design                       | 1F36 1028 0990 |\n| Quadro RTX 3000 with Max-Q Design                       | 1F36 103C 8736 |\n| Quadro RTX 3000 with Max-Q Design                       | 1F36 103C 8738 |\n| Quadro RTX 3000 with Max-Q Design                       | 1F36 103C 8772 |\n| Quadro RTX 3000 with Max-Q Design                       | 1F36 1043 13CF |\n| Quadro RTX 3000 with Max-Q Design                       | 1F36 1414 0032 |\n| NVIDIA GeForce RTX 2060 SUPER                           | 1F42           |\n| NVIDIA GeForce RTX 2060 SUPER                           | 1F47           |\n| NVIDIA GeForce RTX 2070                                 | 1F50           |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F50 1028 08ED |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F50 1028 08EE |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F50 1028 08EF |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F50 103C 8572 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F50 103C 8573 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F50 103C 8574 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F50 103C 8600 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F50 103C 8605 |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F50 17AA 3FEE |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F50 17AA 3FFE |\n| NVIDIA GeForce RTX 2060                                 | 1F51           |\n| NVIDIA GeForce RTX 2070                                 | 1F54           |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F54 103C 878A |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F54 17AA 3FCE |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F54 17AA 3FCF |\n| NVIDIA GeForce RTX 2070 with Max-Q Design               | 1F54 17AA 3FD0 |\n| NVIDIA GeForce RTX 2060                                 | 1F55           |\n| Quadro RTX 3000                                         | 1F76           |\n| Matrox D-Series D2450                                   | 1F76 102B 2800 |\n| Matrox D-Series D2480                                   | 1F76 102B 2900 |\n| NVIDIA GeForce GTX 1650                                 | 1F82           |\n| NVIDIA GeForce GTX 1630                                 | 1F83           |\n| NVIDIA GeForce GTX 1650                                 | 1F91           |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 103C 863E |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 103C 86E7 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 103C 86E8 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 1043 12CF |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 1043 156F |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 1414 0032 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 144D C822 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 1462 127E |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 1462 1281 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 1462 1284 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 1462 1285 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 1462 129C |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 17AA 229F |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 17AA 3802 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 17AA 3806 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 17AA 3F1A |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F91 1A58 1001 |\n| NVIDIA GeForce GTX 1650 Ti                              | 1F95           |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1025 1479 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1025 147A |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1025 147B |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1025 147C |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 103C 86E7 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 103C 86E8 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 103C 8815 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1043 1DFF |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1043 1E1F |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 144D C838 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1462 12BD |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1462 12C5 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1462 12D2 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 17AA 22C0 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 17AA 22C1 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 17AA 3837 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 17AA 3F95 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1A58 1003 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1A58 1006 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1A58 1007 |\n| NVIDIA GeForce GTX 1650 Ti with Max-Q Design            | 1F95 1E83 3E30 |\n| NVIDIA GeForce GTX 1650                                 | 1F96           |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F96 1462 1297 |\n| NVIDIA GeForce MX450                                    | 1F97           |\n| NVIDIA GeForce MX450                                    | 1F98           |\n| NVIDIA GeForce GTX 1650                                 | 1F99           |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 1025 1479 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 1025 147A |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 1025 147B |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 1025 147C |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 103C 8815 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 1043 13B2 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 1043 1402 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 1043 1902 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 1462 12BD |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 1462 12C5 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 1462 12D2 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 17AA 22DA |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 17AA 3F93 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F99 1E83 3E30 |\n| NVIDIA GeForce MX450                                    | 1F9C           |\n| NVIDIA GeForce GTX 1650                                 | 1F9D           |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F9D 1043 128D |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F9D 1043 130D |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F9D 1043 149C |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F9D 1043 185C |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F9D 1043 189C |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F9D 1462 12F4 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F9D 1462 1302 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F9D 1462 131B |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F9D 1462 1326 |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F9D 1462 132A |\n| NVIDIA GeForce GTX 1650 with Max-Q Design               | 1F9D 1462 132E |\n| NVIDIA GeForce MX550                                    | 1F9F           |\n| NVIDIA GeForce MX550                                    | 1FA0           |\n| NVIDIA T1000                                            | 1FB0 1028 12DB |\n| NVIDIA T1000                                            | 1FB0 103C 12DB |\n| NVIDIA T1000                                            | 1FB0 103C 8A80 |\n| NVIDIA T1000                                            | 1FB0 10DE 12DB |\n| NVIDIA DGX Display                                      | 1FB0 10DE 1485 |\n| NVIDIA T1000                                            | 1FB0 17AA 12DB |\n| NVIDIA T600                                             | 1FB1 1028 1488 |\n| NVIDIA T600                                             | 1FB1 103C 1488 |\n| NVIDIA T600                                             | 1FB1 103C 8A80 |\n| NVIDIA T600                                             | 1FB1 10DE 1488 |\n| NVIDIA T600                                             | 1FB1 17AA 1488 |\n| NVIDIA T400                                             | 1FB2 1028 1489 |\n| NVIDIA T400                                             | 1FB2 103C 1489 |\n| NVIDIA T400                                             | 1FB2 103C 8A80 |\n| NVIDIA T400                                             | 1FB2 10DE 1489 |\n| NVIDIA T400                                             | 1FB2 17AA 1489 |\n| NVIDIA T600 Laptop GPU                                  | 1FB6           |\n| NVIDIA T550 Laptop GPU                                  | 1FB7           |\n| Quadro T2000                                            | 1FB8           |\n| Quadro T2000 with Max-Q Design                          | 1FB8 1028 097E |\n| Quadro T2000 with Max-Q Design                          | 1FB8 103C 8736 |\n| Quadro T2000 with Max-Q Design                          | 1FB8 103C 8738 |\n| Quadro T2000 with Max-Q Design                          | 1FB8 103C 8772 |\n| Quadro T2000 with Max-Q Design                          | 1FB8 103C 8780 |\n| Quadro T2000 with Max-Q Design                          | 1FB8 103C 8782 |\n| Quadro T2000 with Max-Q Design                          | 1FB8 103C 8783 |\n| Quadro T2000 with Max-Q Design                          | 1FB8 103C 8785 |\n| Quadro T2000 with Max-Q Design                          | 1FB8 103C 87F0 |\n| Quadro T2000 with Max-Q Design                          | 1FB8 1462 1281 |\n| Quadro T2000 with Max-Q Design                          | 1FB8 1462 12BD |\n| Quadro T2000 with Max-Q Design                          | 1FB8 17AA 22C0 |\n| Quadro T2000 with Max-Q Design                          | 1FB8 17AA 22C1 |\n| Quadro T1000                                            | 1FB9           |\n| Quadro T1000 with Max-Q Design                          | 1FB9 1025 1479 |\n| Quadro T1000 with Max-Q Design                          | 1FB9 1025 147A |\n| Quadro T1000 with Max-Q Design                          | 1FB9 1025 147B |\n| Quadro T1000 with Max-Q Design                          | 1FB9 1025 147C |\n| Quadro T1000 with Max-Q Design                          | 1FB9 103C 8736 |\n| Quadro T1000 with Max-Q Design                          | 1FB9 103C 8738 |\n| Quadro T1000 with Max-Q Design                          | 1FB9 103C 8772 |\n| Quadro T1000 with Max-Q Design                          | 1FB9 103C 8780 |\n| Quadro T1000 with Max-Q Design                          | 1FB9 103C 8782 |\n| Quadro T1000 with Max-Q Design                          | 1FB9 103C 8783 |\n| Quadro T1000 with Max-Q Design                          | 1FB9 103C 8785 |\n| Quadro T1000 with Max-Q Design                          | 1FB9 103C 87F0 |\n| Quadro T1000 with Max-Q Design                          | 1FB9 1462 12BD |\n| Quadro T1000 with Max-Q Design                          | 1FB9 17AA 22C0 |\n| Quadro T1000 with Max-Q Design                          | 1FB9 17AA 22C1 |\n| NVIDIA T600 Laptop GPU                                  | 1FBA           |\n| NVIDIA T500                                             | 1FBB           |\n| NVIDIA T1200 Laptop GPU                                 | 1FBC           |\n| NVIDIA GeForce GTX 1650                                 | 1FDD           |\n| NVIDIA T1000 8GB                                        | 1FF0 1028 1612 |\n| NVIDIA T1000 8GB                                        | 1FF0 103C 1612 |\n| NVIDIA T1000 8GB                                        | 1FF0 103C 8A80 |\n| NVIDIA T1000 8GB                                        | 1FF0 10DE 1612 |\n| NVIDIA T1000 8GB                                        | 1FF0 17AA 1612 |\n| NVIDIA T400 4GB                                         | 1FF2 1028 1613 |\n| NVIDIA T400 4GB                                         | 1FF2 103C 1613 |\n| NVIDIA T400E                                            | 1FF2 103C 18FF |\n| NVIDIA T400 4GB                                         | 1FF2 103C 8A80 |\n| NVIDIA T400 4GB                                         | 1FF2 10DE 1613 |\n| NVIDIA T400E                                            | 1FF2 10DE 18FF |\n| NVIDIA T400 4GB                                         | 1FF2 17AA 1613 |\n| NVIDIA T400E                                            | 1FF2 17AA 18FF |\n| Quadro T1000                                            | 1FF9           |\n| NVIDIA A100-SXM4-40GB                                   | 20B0           |\n| NVIDIA A100-PG509-200                                   | 20B0 10DE 1450 |\n| NVIDIA A100-SXM4-80GB                                   | 20B2 10DE 1463 |\n| NVIDIA A100-SXM4-80GB                                   | 20B2 10DE 147F |\n| NVIDIA A100-SXM4-80GB                                   | 20B2 10DE 1622 |\n| NVIDIA A100-SXM4-80GB                                   | 20B2 10DE 1623 |\n| NVIDIA PG509-210                                        | 20B2 10DE 1625 |\n| NVIDIA A100-SXM-64GB                                    | 20B3 10DE 14A7 |\n| NVIDIA A100-SXM-64GB                                    | 20B3 10DE 14A8 |\n| NVIDIA A100 80GB PCIe                                   | 20B5 10DE 1533 |\n| NVIDIA A100 80GB PCIe                                   | 20B5 10DE 1642 |\n| NVIDIA PG506-232                                        | 20B6 10DE 1492 |\n| NVIDIA A30                                              | 20B7 10DE 1532 |\n| NVIDIA A30                                              | 20B7 10DE 1804 |\n| NVIDIA A30                                              | 20B7 10DE 1852 |\n| NVIDIA A800-SXM4-40GB                                   | 20BD 10DE 17F4 |\n| NVIDIA A100-PCIE-40GB                                   | 20F1 10DE 145F |\n| NVIDIA A800-SXM4-80GB                                   | 20F3 10DE 179B |\n| NVIDIA A800-SXM4-80GB                                   | 20F3 10DE 179C |\n| NVIDIA A800-SXM4-80GB                                   | 20F3 10DE 179D |\n| NVIDIA A800-SXM4-80GB                                   | 20F3 10DE 179E |\n| NVIDIA A800-SXM4-80GB                                   | 20F3 10DE 179F |\n| NVIDIA A800-SXM4-80GB                                   | 20F3 10DE 17A0 |\n| NVIDIA A800-SXM4-80GB                                   | 20F3 10DE 17A1 |\n| NVIDIA A800-SXM4-80GB                                   | 20F3 10DE 17A2 |\n| NVIDIA A800 80GB PCIe                                   | 20F5 10DE 1799 |\n| NVIDIA A800 80GB PCIe LC                                | 20F5 10DE 179A |\n| NVIDIA A800 40GB Active                                 | 20F6 1028 180A |\n| NVIDIA A800 40GB Active                                 | 20F6 103C 180A |\n| NVIDIA A800 40GB Active                                 | 20F6 10DE 180A |\n| NVIDIA A800 40GB Active                                 | 20F6 17AA 180A |\n| NVIDIA AX800                                            | 20FD 10DE 17F8 |\n| NVIDIA GeForce GTX 1660 Ti                              | 2182           |\n| NVIDIA GeForce GTX 1660                                 | 2184           |\n| NVIDIA GeForce GTX 1650 SUPER                           | 2187           |\n| NVIDIA GeForce GTX 1650                                 | 2188           |\n| NVIDIA CMP 30HX                                         | 2189           |\n| NVIDIA GeForce GTX 1660 Ti                              | 2191           |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 1028 0949 |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 103C 85FB |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 103C 85FE |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 103C 86D6 |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 103C 8741 |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 103C 8744 |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 103C 878D |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 103C 87AF |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 103C 87B3 |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 1043 171F |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 1043 17EF |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 1043 18D1 |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 1414 0032 |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 1462 128A |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 1462 128B |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 1462 12C6 |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 1462 12CB |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 1462 12CC |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 1462 12D9 |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 17AA 380C |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 17AA 381D |\n| NVIDIA GeForce GTX 1660 Ti with Max-Q Design            | 2191 17AA 381E |\n| NVIDIA GeForce GTX 1650 Ti                              | 2192           |\n| NVIDIA GeForce GTX 1660 SUPER                           | 21C4           |\n| NVIDIA GeForce GTX 1660 Ti                              | 21D1           |\n| NVIDIA GeForce RTX 3090 Ti                              | 2203           |\n| NVIDIA GeForce RTX 3090                                 | 2204           |\n| NVIDIA GeForce RTX 3080                                 | 2206           |\n| NVIDIA GeForce RTX 3070 Ti                              | 2207           |\n| NVIDIA GeForce RTX 3080 Ti                              | 2208           |\n| NVIDIA GeForce RTX 3080                                 | 220A           |\n| NVIDIA CMP 90HX                                         | 220D           |\n| NVIDIA GeForce RTX 3080                                 | 2216           |\n| NVIDIA RTX A6000                                        | 2230 1028 1459 |\n| NVIDIA RTX A6000                                        | 2230 103C 1459 |\n| NVIDIA RTX A6000                                        | 2230 10DE 1459 |\n| NVIDIA RTX A6000                                        | 2230 17AA 1459 |\n| NVIDIA RTX A5000                                        | 2231 1028 147E |\n| NVIDIA RTX A5000                                        | 2231 103C 147E |\n| NVIDIA RTX A5000                                        | 2231 10DE 147E |\n| NVIDIA RTX A5000                                        | 2231 17AA 147E |\n| NVIDIA RTX A4500                                        | 2232 1028 163C |\n| NVIDIA RTX A4500                                        | 2232 103C 163C |\n| NVIDIA RTX A4500                                        | 2232 10DE 163C |\n| NVIDIA RTX A4500                                        | 2232 17AA 163C |\n| NVIDIA RTX A5500                                        | 2233 1028 165A |\n| NVIDIA RTX A5500                                        | 2233 103C 165A |\n| NVIDIA RTX A5500                                        | 2233 10DE 165A |\n| NVIDIA RTX A5500                                        | 2233 17AA 165A |\n| NVIDIA A40                                              | 2235 10DE 145A |\n| NVIDIA A10                                              | 2236 10DE 1482 |\n| NVIDIA A10G                                             | 2237 10DE 152F |\n| NVIDIA A10M                                             | 2238 10DE 1677 |\n| NVIDIA H20 NVL16                                        | 230E 10DE 20DF |\n| NVIDIA H100 NVL                                         | 2321 10DE 1839 |\n| NVIDIA H800 PCIe                                        | 2322 10DE 17A4 |\n| NVIDIA H800                                             | 2324 10DE 17A6 |\n| NVIDIA H800                                             | 2324 10DE 17A8 |\n| NVIDIA H20                                              | 2329 10DE 198B |\n| NVIDIA H20                                              | 2329 10DE 198C |\n| NVIDIA H20-3e                                           | 232C 10DE 2063 |\n| NVIDIA H100 80GB HBM3                                   | 2330 10DE 16C0 |\n| NVIDIA H100 80GB HBM3                                   | 2330 10DE 16C1 |\n| NVIDIA H100 PCIe                                        | 2331 10DE 1626 |\n| NVIDIA H200                                             | 2335 10DE 18BE |\n| NVIDIA H200                                             | 2335 10DE 18BF |\n| NVIDIA H100                                             | 2339 10DE 17FC |\n| NVIDIA H800 NVL                                         | 233A 10DE 183A |\n| NVIDIA H200 NVL                                         | 233B 10DE 1996 |\n| NVIDIA GH200 120GB                                      | 2342 10DE 16EB |\n| NVIDIA GH200 120GB                                      | 2342 10DE 1805 |\n| NVIDIA GH200 480GB                                      | 2342 10DE 1809 |\n| NVIDIA GH200 144G HBM3e                                 | 2348 10DE 18D2 |\n| NVIDIA GeForce RTX 3060 Ti                              | 2414           |\n| NVIDIA GeForce RTX 3080 Ti Laptop GPU                   | 2420           |\n| NVIDIA RTX A5500 Laptop GPU                             | 2438           |\n| NVIDIA GeForce RTX 3080 Ti Laptop GPU                   | 2460           |\n| NVIDIA GeForce RTX 3070 Ti                              | 2482           |\n| NVIDIA GeForce RTX 3070                                 | 2484           |\n| NVIDIA GeForce RTX 3060 Ti                              | 2486           |\n| NVIDIA GeForce RTX 3060                                 | 2487           |\n| NVIDIA GeForce RTX 3070                                 | 2488           |\n| NVIDIA GeForce RTX 3060 Ti                              | 2489           |\n| NVIDIA CMP 70HX                                         | 248A           |\n| NVIDIA GeForce RTX 3080 Laptop GPU                      | 249C           |\n| NVIDIA GeForce RTX 3060 Laptop GPU                      | 249C 1D05 1194 |\n| NVIDIA GeForce RTX 3070 Laptop GPU                      | 249D           |\n| NVIDIA GeForce RTX 3070 Ti Laptop GPU                   | 24A0           |\n| NVIDIA GeForce RTX 3060 Laptop GPU                      | 24A0 1D05 1192 |\n| NVIDIA RTX A4000                                        | 24B0 1028 14AD |\n| NVIDIA RTX A4000                                        | 24B0 103C 14AD |\n| NVIDIA RTX A4000                                        | 24B0 10DE 14AD |\n| NVIDIA RTX A4000                                        | 24B0 17AA 14AD |\n| NVIDIA RTX A4000H                                       | 24B1 10DE 1658 |\n| NVIDIA RTX A5000 Laptop GPU                             | 24B6           |\n| NVIDIA RTX A4000 Laptop GPU                             | 24B7           |\n| NVIDIA RTX A3000 Laptop GPU                             | 24B8           |\n| NVIDIA RTX A3000 12GB Laptop GPU                        | 24B9           |\n| NVIDIA RTX A4500 Laptop GPU                             | 24BA           |\n| NVIDIA RTX A3000 12GB Laptop GPU                        | 24BB           |\n| NVIDIA GeForce RTX 3060                                 | 24C7           |\n| NVIDIA GeForce RTX 3060 Ti                              | 24C9           |\n| NVIDIA GeForce RTX 3080 Laptop GPU                      | 24DC           |\n| NVIDIA GeForce RTX 3070 Laptop GPU                      | 24DD           |\n| NVIDIA GeForce RTX 3070 Ti Laptop GPU                   | 24E0           |\n| NVIDIA RTX A4500 Embedded GPU                           | 24FA           |\n| NVIDIA GeForce RTX 3060                                 | 2503           |\n| NVIDIA GeForce RTX 3060                                 | 2504           |\n| NVIDIA GeForce RTX 3050                                 | 2507           |\n| NVIDIA GeForce RTX 3050 OEM                             | 2508           |\n| NVIDIA GeForce RTX 3060 Laptop GPU                      | 2520           |\n| NVIDIA GeForce RTX 3060 Laptop GPU                      | 2521           |\n| NVIDIA GeForce RTX 3050 Ti Laptop GPU                   | 2523           |\n| NVIDIA RTX A2000                                        | 2531 1028 151D |\n| NVIDIA RTX A2000                                        | 2531 103C 151D |\n| NVIDIA RTX A2000                                        | 2531 10DE 151D |\n| NVIDIA RTX A2000                                        | 2531 17AA 151D |\n| NVIDIA GeForce RTX 3060                                 | 2544           |\n| NVIDIA GeForce RTX 3060 Laptop GPU                      | 2560           |\n| NVIDIA GeForce RTX 3050 Ti Laptop GPU                   | 2563           |\n| NVIDIA RTX A2000 12GB                                   | 2571 1028 1611 |\n| NVIDIA RTX A2000 12GB                                   | 2571 103C 1611 |\n| NVIDIA RTX A2000 12GB                                   | 2571 10DE 1611 |\n| NVIDIA RTX A2000 12GB                                   | 2571 17AA 1611 |\n| NVIDIA GeForce RTX 3050                                 | 2582           |\n| NVIDIA GeForce RTX 3050                                 | 2584           |\n| NVIDIA GeForce RTX 3050 Ti Laptop GPU                   | 25A0           |\n| NVIDIA GeForce RTX 3050Ti Laptop GPU                    | 25A0 103C 8928 |\n| NVIDIA GeForce RTX 3050Ti Laptop GPU                    | 25A0 103C 89F9 |\n| NVIDIA GeForce RTX 3060 Laptop GPU                      | 25A0 1D05 1196 |\n| NVIDIA GeForce RTX 3050 Laptop GPU                      | 25A2           |\n| NVIDIA GeForce RTX 3050 Ti Laptop GPU                   | 25A2 1028 0BAF |\n| NVIDIA GeForce RTX 3060 Laptop GPU                      | 25A2 1D05 1195 |\n| NVIDIA GeForce RTX 3050 Laptop GPU                      | 25A5           |\n| NVIDIA GeForce MX570                                    | 25A6           |\n| NVIDIA GeForce RTX 2050                                 | 25A7           |\n| NVIDIA GeForce RTX 2050                                 | 25A9           |\n| NVIDIA GeForce MX570 A                                  | 25AA           |\n| NVIDIA GeForce RTX 3050 4GB Laptop GPU                  | 25AB           |\n| NVIDIA GeForce RTX 3050 6GB Laptop GPU                  | 25AC           |\n| NVIDIA GeForce RTX 2050                                 | 25AD           |\n| NVIDIA RTX A1000                                        | 25B0 1028 1878 |\n| NVIDIA RTX A1000                                        | 25B0 103C 1878 |\n| NVIDIA RTX A1000                                        | 25B0 103C 8D96 |\n| NVIDIA RTX A1000                                        | 25B0 10DE 1878 |\n| NVIDIA RTX A1000                                        | 25B0 17AA 1878 |\n| NVIDIA RTX A400                                         | 25B2 1028 1879 |\n| NVIDIA RTX A400                                         | 25B2 103C 1879 |\n| NVIDIA RTX A400                                         | 25B2 103C 8D95 |\n| NVIDIA RTX A400                                         | 25B2 10DE 1879 |\n| NVIDIA RTX A400                                         | 25B2 17AA 1879 |\n| NVIDIA A16                                              | 25B6 10DE 14A9 |\n| NVIDIA A2                                               | 25B6 10DE 157E |\n| NVIDIA RTX A2000 Laptop GPU                             | 25B8           |\n| NVIDIA RTX A1000 Laptop GPU                             | 25B9           |\n| NVIDIA RTX A2000 8GB Laptop GPU                         | 25BA           |\n| NVIDIA RTX A500 Laptop GPU                              | 25BB           |\n| NVIDIA RTX A1000 6GB Laptop GPU                         | 25BC           |\n| NVIDIA RTX A500 Laptop GPU                              | 25BD           |\n| NVIDIA GeForce RTX 3050 Ti Laptop GPU                   | 25E0           |\n| NVIDIA GeForce RTX 3050 Laptop GPU                      | 25E2           |\n| NVIDIA GeForce RTX 3050 Laptop GPU                      | 25E5           |\n| NVIDIA GeForce RTX 3050 6GB Laptop GPU                  | 25EC           |\n| NVIDIA GeForce RTX 2050                                 | 25ED           |\n| NVIDIA RTX A1000 Embedded GPU                           | 25F9           |\n| NVIDIA RTX A2000 Embedded GPU                           | 25FA           |\n| NVIDIA RTX A500 Embedded GPU                            | 25FB           |\n| NVIDIA GeForce RTX 4090                                 | 2684           |\n| NVIDIA GeForce RTX 4090 D                               | 2685           |\n| NVIDIA GeForce RTX 4070 Ti SUPER                        | 2689           |\n| NVIDIA RTX 6000 Ada Generation                          | 26B1 1028 16A1 |\n| NVIDIA RTX 6000 Ada Generation                          | 26B1 103C 16A1 |\n| NVIDIA RTX 6000 Ada Generation                          | 26B1 10DE 16A1 |\n| NVIDIA RTX 6000 Ada Generation                          | 26B1 17AA 16A1 |\n| NVIDIA RTX 5000 Ada Generation                          | 26B2 1028 17FA |\n| NVIDIA RTX 5000 Ada Generation                          | 26B2 103C 17FA |\n| NVIDIA RTX 5000 Ada Generation                          | 26B2 10DE 17FA |\n| NVIDIA RTX 5000 Ada Generation                          | 26B2 17AA 17FA |\n| NVIDIA RTX 5880 Ada Generation                          | 26B3 1028 1934 |\n| NVIDIA RTX 5880 Ada Generation                          | 26B3 103C 1934 |\n| NVIDIA RTX 5880 Ada Generation                          | 26B3 10DE 1934 |\n| NVIDIA RTX 5880 Ada Generation                          | 26B3 17AA 1934 |\n| NVIDIA L40                                              | 26B5 10DE 169D |\n| NVIDIA L40                                              | 26B5 10DE 17DA |\n| NVIDIA L40S                                             | 26B9 10DE 1851 |\n| NVIDIA L40S                                             | 26B9 10DE 18CF |\n| NVIDIA L20                                              | 26BA 10DE 1957 |\n| NVIDIA L20                                              | 26BA 10DE 1990 |\n| NVIDIA GeForce RTX 4080 SUPER                           | 2702           |\n| NVIDIA GeForce RTX 4080                                 | 2704           |\n| NVIDIA GeForce RTX 4070 Ti SUPER                        | 2705           |\n| NVIDIA GeForce RTX 4070                                 | 2709           |\n| NVIDIA GeForce RTX 4090 Laptop GPU                      | 2717           |\n| NVIDIA RTX 5000 Ada Generation Laptop GPU               | 2730           |\n| NVIDIA GeForce RTX 4090 Laptop GPU                      | 2757           |\n| NVIDIA RTX 5000 Ada Generation Embedded GPU             | 2770           |\n| NVIDIA GeForce RTX 4070 Ti                              | 2782           |\n| NVIDIA GeForce RTX 4070 SUPER                           | 2783           |\n| NVIDIA GeForce RTX 4070                                 | 2786           |\n| NVIDIA GeForce RTX 4060 Ti                              | 2788           |\n| NVIDIA GeForce RTX 4080 Laptop GPU                      | 27A0           |\n| NVIDIA RTX 4000 SFF Ada Generation                      | 27B0 1028 16FA |\n| NVIDIA RTX 4000 SFF Ada Generation                      | 27B0 103C 16FA |\n| NVIDIA RTX 4000 SFF Ada Generation                      | 27B0 10DE 16FA |\n| NVIDIA RTX 4000 SFF Ada Generation                      | 27B0 17AA 16FA |\n| NVIDIA RTX 4500 Ada Generation                          | 27B1 1028 180C |\n| NVIDIA RTX 4500 Ada Generation                          | 27B1 103C 180C |\n| NVIDIA RTX 4500 Ada Generation                          | 27B1 10DE 180C |\n| NVIDIA RTX 4500 Ada Generation                          | 27B1 17AA 180C |\n| NVIDIA RTX 4000 Ada Generation                          | 27B2 1028 181B |\n| NVIDIA RTX 4000 Ada Generation                          | 27B2 103C 181B |\n| NVIDIA RTX 4000 Ada Generation                          | 27B2 10DE 181B |\n| NVIDIA RTX 4000 Ada Generation                          | 27B2 17AA 181B |\n| NVIDIA L2                                               | 27B6 10DE 1933 |\n| NVIDIA L4                                               | 27B8 10DE 16CA |\n| NVIDIA L4                                               | 27B8 10DE 16EE |\n| NVIDIA RTX 4000 Ada Generation Laptop GPU               | 27BA           |\n| NVIDIA RTX 3500 Ada Generation Laptop GPU               | 27BB           |\n| NVIDIA GeForce RTX 4080 Laptop GPU                      | 27E0           |\n| NVIDIA RTX 3500 Ada Generation Embedded GPU             | 27FB           |\n| NVIDIA GeForce RTX 4060 Ti                              | 2803           |\n| NVIDIA GeForce RTX 4060 Ti                              | 2805           |\n| NVIDIA GeForce RTX 4060                                 | 2808           |\n| NVIDIA GeForce RTX 4070 Laptop GPU                      | 2820           |\n| NVIDIA GeForce RTX 3050 A Laptop GPU                    | 2822           |\n| NVIDIA RTX 3000 Ada Generation Laptop GPU               | 2838           |\n| NVIDIA GeForce RTX 4070 Laptop GPU                      | 2860           |\n| NVIDIA GeForce RTX 4060                                 | 2882           |\n| NVIDIA GeForce RTX 4060 Laptop GPU                      | 28A0           |\n| NVIDIA GeForce RTX 4050 Laptop GPU                      | 28A1           |\n| NVIDIA GeForce RTX 3050 A Laptop GPU                    | 28A3           |\n| NVIDIA RTX 2000 Ada Generation                          | 28B0 1028 1870 |\n| NVIDIA RTX 2000 Ada Generation                          | 28B0 103C 1870 |\n| NVIDIA RTX 2000E Ada Generation                         | 28B0 103C 1871 |\n| NVIDIA RTX 2000 Ada Generation                          | 28B0 10DE 1870 |\n| NVIDIA RTX 2000E Ada Generation                         | 28B0 10DE 1871 |\n| NVIDIA RTX 2000 Ada Generation                          | 28B0 17AA 1870 |\n| NVIDIA RTX 2000E Ada Generation                         | 28B0 17AA 1871 |\n| NVIDIA RTX 2000 Ada Generation Laptop GPU               | 28B8           |\n| NVIDIA RTX 1000 Ada Generation Laptop GPU               | 28B9           |\n| NVIDIA RTX 500 Ada Generation Laptop GPU                | 28BA           |\n| NVIDIA RTX 500 Ada Generation Laptop GPU                | 28BB           |\n| NVIDIA GeForce RTX 4060 Laptop GPU                      | 28E0           |\n| NVIDIA GeForce RTX 4050 Laptop GPU                      | 28E1           |\n| NVIDIA GeForce RTX 3050 A Laptop GPU                    | 28E3           |\n| NVIDIA RTX 2000 Ada Generation Embedded GPU             | 28F8           |\n| NVIDIA B200                                             | 2901 10DE 1999 |\n| NVIDIA B200                                             | 2901 10DE 199B |\n| NVIDIA B200                                             | 2901 10DE 20DA |\n| NVIDIA GB200                                            | 2941 10DE 2046 |\n| NVIDIA GB200                                            | 2941 10DE 20CA |\n| NVIDIA GB200                                            | 2941 10DE 20D5 |\n| NVIDIA GB200                                            | 2941 10DE 21C9 |\n| NVIDIA GB200                                            | 2941 10DE 21CA |\n| NVIDIA DRIVE P2021                                      | 29BB 10DE 207C |\n| NVIDIA GeForce RTX 5090                                 | 2B85           |\n| NVIDIA GeForce RTX 5090 D                               | 2B87           |\n| NVIDIA GeForce RTX 5090 D v2                            | 2B8C           |\n| NVIDIA RTX PRO 6000 Blackwell Workstation Edition       | 2BB1 1028 204B |\n| NVIDIA RTX PRO 6000 Blackwell Workstation Edition       | 2BB1 103C 204B |\n| NVIDIA RTX PRO 6000 Blackwell Workstation Edition       | 2BB1 10DE 204B |\n| NVIDIA RTX PRO 6000 Blackwell Workstation Edition       | 2BB1 17AA 204B |\n| NVIDIA RTX PRO 5000 Blackwell                           | 2BB3 1028 204D |\n| NVIDIA RTX PRO 5000 72GB Blackwell                      | 2BB3 1028 227A |\n| NVIDIA RTX PRO 5000 Blackwell                           | 2BB3 103C 204D |\n| NVIDIA RTX PRO 5000 72GB Blackwell                      | 2BB3 103C 227A |\n| NVIDIA RTX PRO 5000 Blackwell                           | 2BB3 10DE 204D |\n| NVIDIA RTX PRO 5000 72GB Blackwell                      | 2BB3 10DE 227A |\n| NVIDIA RTX PRO 5000 Blackwell                           | 2BB3 17AA 204D |\n| NVIDIA RTX PRO 5000 72GB Blackwell                      | 2BB3 17AA 227A |\n| NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition | 2BB4 1028 204C |\n| NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition | 2BB4 103C 204C |\n| NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition | 2BB4 10DE 204C |\n| NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition | 2BB4 17AA 204C |\n| NVIDIA RTX PRO 6000 Blackwell Server Edition            | 2BB5 10DE 204E |\n| NVIDIA RTX 6000D                                        | 2BB9 10DE 2091 |\n| NVIDIA GeForce RTX 5080                                 | 2C02           |\n| NVIDIA GeForce RTX 5070 Ti                              | 2C05           |\n| NVIDIA GeForce RTX 5090 Laptop GPU                      | 2C18           |\n| NVIDIA GeForce RTX 5080 Laptop GPU                      | 2C19           |\n| NVIDIA RTX PRO 4500 Blackwell                           | 2C31 1028 2051 |\n| NVIDIA RTX PRO 4500 Blackwell                           | 2C31 103C 2051 |\n| NVIDIA RTX PRO 4500 Blackwell                           | 2C31 10DE 2051 |\n| NVIDIA RTX PRO 4500 Blackwell                           | 2C31 17AA 2051 |\n| NVIDIA RTX PRO 4000 Blackwell SFF Edition               | 2C33 1028 2053 |\n| NVIDIA RTX PRO 4000 Blackwell SFF Edition               | 2C33 103C 2053 |\n| NVIDIA RTX PRO 4000 Blackwell SFF Edition               | 2C33 10DE 2053 |\n| NVIDIA RTX PRO 4000 Blackwell SFF Edition               | 2C33 17AA 2053 |\n| NVIDIA RTX PRO 4000 Blackwell                           | 2C34 1028 2052 |\n| NVIDIA RTX PRO 4000 Blackwell                           | 2C34 103C 2052 |\n| NVIDIA RTX PRO 4000 Blackwell                           | 2C34 10DE 2052 |\n| NVIDIA RTX PRO 4000 Blackwell                           | 2C34 17AA 2052 |\n| NVIDIA RTX PRO 5000 Blackwell Generation Laptop GPU     | 2C38           |\n| NVIDIA RTX PRO 4000 Blackwell Generation Laptop GPU     | 2C39           |\n| NVIDIA GeForce RTX 5090 Laptop GPU                      | 2C58           |\n| NVIDIA GeForce RTX 5080 Laptop GPU                      | 2C59           |\n| NVIDIA RTX PRO 5000 Blackwell Embedded GPU              | 2C77           |\n| NVIDIA RTX PRO 4000 Blackwell Embedded GPU              | 2C79           |\n| NVIDIA GeForce RTX 5060 Ti                              | 2D04           |\n| NVIDIA GeForce RTX 5060                                 | 2D05           |\n| NVIDIA GeForce RTX 5070 Laptop GPU                      | 2D18           |\n| NVIDIA GeForce RTX 5060 Laptop GPU                      | 2D19           |\n| NVIDIA RTX PRO 2000 Blackwell                           | 2D30 1028 2054 |\n| NVIDIA RTX PRO 2000 Blackwell                           | 2D30 103C 2054 |\n| NVIDIA RTX PRO 2000 Blackwell                           | 2D30 10DE 2054 |\n| NVIDIA RTX PRO 2000 Blackwell                           | 2D30 17AA 2054 |\n| NVIDIA RTX PRO 2000 Blackwell Generation Laptop GPU     | 2D39           |\n| NVIDIA GeForce RTX 5070 Laptop GPU                      | 2D58           |\n| NVIDIA GeForce RTX 5060 Laptop GPU                      | 2D59           |\n| NVIDIA RTX PRO 2000 Blackwell Embedded GPU              | 2D79           |\n| NVIDIA GeForce RTX 5050                                 | 2D83           |\n| NVIDIA GeForce RTX 5050 Laptop GPU                      | 2D98           |\n| NVIDIA RTX PRO 1000 Blackwell Generation Laptop GPU     | 2DB8           |\n| NVIDIA RTX PRO 500 Blackwell Generation Laptop GPU      | 2DB9           |\n| NVIDIA GeForce RTX 5050 Laptop GPU                      | 2DD8           |\n| NVIDIA RTX PRO 500 Blackwell Embedded GPU               | 2DF9           |\n| NVIDIA GB10                                             | 2E12 10DE 21EC |\n| NVIDIA GeForce RTX 5070                                 | 2F04           |\n| NVIDIA GeForce RTX 5070 Ti Laptop GPU                   | 2F18           |\n| NVIDIA RTX PRO 3000 Blackwell Generation Laptop GPU     | 2F38           |\n| NVIDIA GeForce RTX 5070 Ti Laptop GPU                   | 2F58           |\n| NVIDIA B300 SXM6 AC                                     | 3182 10DE 20E6 |\n| NVIDIA GB300                                            | 31C2 10DE 21F1 |\n",
      "stars_today": 8
    },
    {
      "id": 571702700,
      "name": "bv",
      "full_name": "aaa1115910/bv",
      "description": "å“”å“©å“”å“© çš„ç¬¬ä¸‰æ–¹ Android åº”ç”¨ã€‚A third-party Android app for Bilibili.",
      "html_url": "https://github.com/aaa1115910/bv",
      "stars": 3686,
      "forks": 459,
      "language": "Kotlin",
      "topics": [],
      "created_at": "2022-11-28T17:50:33Z",
      "updated_at": "2026-01-28T00:54:24Z",
      "pushed_at": "2025-12-08T16:08:45Z",
      "open_issues": 30,
      "owner": {
        "login": "aaa1115910",
        "avatar_url": "https://avatars.githubusercontent.com/u/34527143?v=4"
      },
      "readme": "<div align=\"center\">\n\n<img src=\"app/shared/src/main/res/drawable/ic_banner.webp\" style=\"border-radius: 24px; margin-top: 32px;\"/>\n\n# BV\n\n~~Bug Video~~\n\n[![GitHub Release Release](https://img.shields.io/endpoint?url=https%3A%2F%2Fbadge.versions.bv.aaa1115910.dev%2Fgithub%3Fprerelease%3Dfalse)](https://github.com/aaa1115910/bv/releases?q=prerelease%3Afalse)\n[![GitHub Release Pre-Release](https://img.shields.io/endpoint?url=https%3A%2F%2Fbadge.versions.bv.aaa1115910.dev%2Fgithub%3Fprerelease%3Dtrue)](https://github.com/aaa1115910/bv/releases?q=prerelease%3Atrue)\n\n[![Workflow Release](https://github.com/aaa1115910/bv/actions/workflows/release.yml/badge.svg)](https://github.com/aaa1115910/bv/actions/workflows/release.yml)\n[![Workflow Alpha](https://github.com/aaa1115910/bv/actions/workflows/alpha.yml/badge.svg)](https://github.com/aaa1115910/bv/actions/workflows/alpha.yml)\n[![Android Sdk Require](https://img.shields.io/badge/Android-6.0%2B-informational?logo=android)](https://developer.android.com/jetpack/androidx/versions#version-table)\n[![GitHub](https://img.shields.io/github/license/aaa1115910/bv)](https://github.com/aaa1115910/bv)\n\n**BV æ— æ³•åœ¨ä¸­å›½å¤§é™†åœ°åŒºå†…çš„æ™ºèƒ½ç”µè§†ä¸Šä½¿ç”¨ï¼Œå¦‚æœ‰ç›¸å…³ä½¿ç”¨éœ€æ±‚è¯·ä½¿ç”¨ [äº‘è§†å¬å°ç”µè§†](https://app.bilibili.com)**\n\n**ç¦æ­¢åœ¨ä¸­å›½å¢ƒå†…ä¼ æ’­ã€å®£ä¼ ã€åˆ†å‘ BV**\n\n</div>\n\n---\nBV ~~(Bug Video)~~ æ˜¯ä¸€æ¬¾ [å“”å“©å“”å“©](https://www.bilibili.com) çš„ç¬¬ä¸‰æ–¹åº”ç”¨ï¼Œé€‚é… `Android ç§»åŠ¨ç«¯`\nå’Œ `Android TV`ï¼Œä½¿ç”¨ `Jetpack Compose` å¼€å‘\n\n**éƒ½æ˜¯éšå¿ƒä¹±å†™çš„ä»£ç ï¼Œèƒ½è·‘å°±è¡Œã€‚**\n\n## ç‰¹è‰²\n\n- :bug: ä¸°å¯Œå¤šæ ·çš„ Bug\n- :children_crossing: åäººç±»è®¾è®¡\n- :zap: å¡å¡å¡å¡å¡\n- :art: å¼‚æ ·å®¡ç¾\n- :disappointed: å·¨éš¾ç”¨\n\n## å®‰è£…\n\n### Release\n\n- [Github Release](https://github.com/aaa1115910/bv/releases?q=prerelease%3Afalse)\n\n### Alpha\n\n- [Github Release](https://github.com/aaa1115910/bv/releases?q=prerelease%3Atrue)\n\n## License\n\n[MIT](LICENSE) Â© aaa1115910",
      "stars_today": 8
    },
    {
      "id": 26038648,
      "name": "spdlog",
      "full_name": "gabime/spdlog",
      "description": "Fast C++ logging library.",
      "html_url": "https://github.com/gabime/spdlog",
      "stars": 28197,
      "forks": 5029,
      "language": "C++",
      "topics": [
        "cpp",
        "cpp11",
        "header-only",
        "logging",
        "spdlog"
      ],
      "created_at": "2014-11-01T01:28:53Z",
      "updated_at": "2026-01-27T23:22:02Z",
      "pushed_at": "2026-01-15T20:54:28Z",
      "open_issues": 54,
      "owner": {
        "login": "gabime",
        "avatar_url": "https://avatars.githubusercontent.com/u/6052198?v=4"
      },
      "readme": "# spdlog\r\n\r\n \r\n[![ci](https://github.com/gabime/spdlog/actions/workflows/linux.yml/badge.svg)](https://github.com/gabime/spdlog/actions/workflows/linux.yml)&nbsp;\r\n[![ci](https://github.com/gabime/spdlog/actions/workflows/windows.yml/badge.svg)](https://github.com/gabime/spdlog/actions/workflows/windows.yml)&nbsp;\r\n[![ci](https://github.com/gabime/spdlog/actions/workflows/macos.yml/badge.svg)](https://github.com/gabime/spdlog/actions/workflows/macos.yml)&nbsp;\r\n[![Build status](https://ci.appveyor.com/api/projects/status/d2jnxclg20vd0o50?svg=true&branch=v1.x)](https://ci.appveyor.com/project/gabime/spdlog) [![Release](https://img.shields.io/github/release/gabime/spdlog.svg)](https://github.com/gabime/spdlog/releases/latest)\r\n\r\nFast C++ logging library\r\n\r\n\r\n## Install\r\n#### Header-only version\r\nCopy the include [folder](include/spdlog) to your build tree and use a C++11 compiler.\r\n\r\n#### Compiled version (recommended - much faster compile times)\r\n```console\r\n$ git clone https://github.com/gabime/spdlog.git\r\n$ cd spdlog && mkdir build && cd build\r\n$ cmake .. && cmake --build .\r\n```\r\nsee example [CMakeLists.txt](example/CMakeLists.txt) on how to use.\r\n\r\n## Platforms\r\n* Linux, FreeBSD, OpenBSD, Solaris, AIX\r\n* Windows (msvc 2013+, cygwin)\r\n* macOS (clang 3.5+)\r\n* Android\r\n\r\n## Package managers:\r\n* Debian: `sudo apt install libspdlog-dev`\r\n* Homebrew: `brew install spdlog`\r\n* MacPorts: `sudo port install spdlog`\r\n* FreeBSD:  `pkg install spdlog`\r\n* Fedora: `dnf install spdlog`\r\n* Gentoo: `emerge dev-libs/spdlog`\r\n* Arch Linux: `pacman -S spdlog`\r\n* openSUSE: `sudo zypper in spdlog-devel`\r\n* ALT Linux: `apt-get install libspdlog-devel`\r\n* vcpkg: `vcpkg install spdlog`\r\n* conan: `conan install --requires=spdlog/[*]`\r\n* conda: `conda install -c conda-forge spdlog`\r\n* build2: ```depends: spdlog ^1.8.2```\r\n\r\n\r\n## Features\r\n* Very fast (see [benchmarks](#benchmarks) below).\r\n* Headers only or compiled\r\n* Feature-rich formatting, using the excellent [fmt](https://github.com/fmtlib/fmt) library.\r\n* Asynchronous mode (optional)\r\n* [Custom](https://github.com/gabime/spdlog/wiki/Custom-formatting) formatting.\r\n* Multi/Single threaded loggers.\r\n* Various log targets:\r\n  * Rotating log files.\r\n  * Daily log files.\r\n  * Console logging (colors supported).\r\n  * syslog.\r\n  * Windows event log.\r\n  * Windows debugger (```OutputDebugString(..)```).\r\n  * Log to Qt widgets ([example](#log-to-qt-with-nice-colors)).\r\n  * Easily [extendable](https://github.com/gabime/spdlog/wiki/Sinks#implementing-your-own-sink) with custom log targets.\r\n* Log filtering - log levels can be modified at runtime as well as compile time.\r\n* Support for loading log levels from argv or environment var.\r\n* [Backtrace](#backtrace-support) support - store debug messages in a ring buffer and display them later on demand.\r\n\r\n## Usage samples\r\n\r\n#### Basic usage\r\n```c++\r\n#include \"spdlog/spdlog.h\"\r\n\r\nint main() \r\n{\r\n    spdlog::info(\"Welcome to spdlog!\");\r\n    spdlog::error(\"Some error message with arg: {}\", 1);\r\n    \r\n    spdlog::warn(\"Easy padding in numbers like {:08d}\", 12);\r\n    spdlog::critical(\"Support for int: {0:d};  hex: {0:x};  oct: {0:o}; bin: {0:b}\", 42);\r\n    spdlog::info(\"Support for floats {:03.2f}\", 1.23456);\r\n    spdlog::info(\"Positional args are {1} {0}..\", \"too\", \"supported\");\r\n    spdlog::info(\"{:<30}\", \"left aligned\");\r\n    \r\n    spdlog::set_level(spdlog::level::debug); // Set *global* log level to debug\r\n    spdlog::debug(\"This message should be displayed..\");    \r\n    \r\n    // change log pattern\r\n    spdlog::set_pattern(\"[%H:%M:%S %z] [%n] [%^---%L---%$] [thread %t] %v\");\r\n    \r\n    // Compile time log levels\r\n    // Note that this does not change the current log level, it will only\r\n    // remove (depending on SPDLOG_ACTIVE_LEVEL) the call on the release code.\r\n    SPDLOG_TRACE(\"Some trace message with param {}\", 42);\r\n    SPDLOG_DEBUG(\"Some debug message\");\r\n}\r\n\r\n```\r\n---\r\n#### Create stdout/stderr logger object\r\n```c++\r\n#include \"spdlog/spdlog.h\"\r\n#include \"spdlog/sinks/stdout_color_sinks.h\"\r\nvoid stdout_example()\r\n{\r\n    // create a color multi-threaded logger\r\n    auto console = spdlog::stdout_color_mt(\"console\");    \r\n    auto err_logger = spdlog::stderr_color_mt(\"stderr\");    \r\n    spdlog::get(\"console\")->info(\"loggers can be retrieved from a global registry using the spdlog::get(logger_name)\");\r\n}\r\n```\r\n\r\n---\r\n#### Basic file logger\r\n```c++\r\n#include \"spdlog/sinks/basic_file_sink.h\"\r\nvoid basic_logfile_example()\r\n{\r\n    try \r\n    {\r\n        auto logger = spdlog::basic_logger_mt(\"basic_logger\", \"logs/basic-log.txt\");\r\n    }\r\n    catch (const spdlog::spdlog_ex &ex)\r\n    {\r\n        std::cout << \"Log init failed: \" << ex.what() << std::endl;\r\n    }\r\n}\r\n```\r\n---\r\n#### Rotating files\r\n```c++\r\n#include \"spdlog/sinks/rotating_file_sink.h\"\r\nvoid rotating_example()\r\n{\r\n    // Create a file rotating logger with 5 MB size max and 3 rotated files\r\n    auto max_size = 1048576 * 5;\r\n    auto max_files = 3;\r\n    auto logger = spdlog::rotating_logger_mt(\"some_logger_name\", \"logs/rotating.txt\", max_size, max_files);\r\n}\r\n```\r\n\r\n---\r\n#### Daily files\r\n```c++\r\n\r\n#include \"spdlog/sinks/daily_file_sink.h\"\r\nvoid daily_example()\r\n{\r\n    // Create a daily logger - a new file is created every day at 2:30 am\r\n    auto logger = spdlog::daily_logger_mt(\"daily_logger\", \"logs/daily.txt\", 2, 30);\r\n}\r\n\r\n```\r\n\r\n---\r\n#### Backtrace support\r\n```c++\r\n// Debug messages can be stored in a ring buffer instead of being logged immediately.\r\n// This is useful to display debug logs only when needed (e.g. when an error happens).\r\n// When needed, call dump_backtrace() to dump them to your log.\r\n\r\nspdlog::enable_backtrace(32); // Store the latest 32 messages in a buffer. \r\n// or my_logger->enable_backtrace(32)..\r\nfor(int i = 0; i < 100; i++)\r\n{\r\n  spdlog::debug(\"Backtrace message {}\", i); // not logged yet..\r\n}\r\n// e.g. if some error happened:\r\nspdlog::dump_backtrace(); // log them now! show the last 32 messages\r\n// or my_logger->dump_backtrace(32)..\r\n```\r\n\r\n---\r\n#### Periodic flush\r\n```c++\r\n// periodically flush all *registered* loggers every 3 seconds:\r\n// warning: only use if all your loggers are thread-safe (\"_mt\" loggers)\r\nspdlog::flush_every(std::chrono::seconds(3));\r\n\r\n```\r\n\r\n---\r\n#### Stopwatch\r\n```c++\r\n// Stopwatch support for spdlog\r\n#include \"spdlog/stopwatch.h\"\r\nvoid stopwatch_example()\r\n{\r\n    spdlog::stopwatch sw;    \r\n    spdlog::debug(\"Elapsed {}\", sw);\r\n    spdlog::debug(\"Elapsed {:.3}\", sw);       \r\n}\r\n\r\n```\r\n\r\n---\r\n#### Log binary data in hex\r\n```c++\r\n// many types of std::container<char> types can be used.\r\n// ranges are supported too.\r\n// format flags:\r\n// {:X} - print in uppercase.\r\n// {:s} - don't separate each byte with space.\r\n// {:p} - don't print the position on each line start.\r\n// {:n} - don't split the output into lines.\r\n// {:a} - show ASCII if :n is not set.\r\n\r\n#include \"spdlog/fmt/bin_to_hex.h\"\r\n\r\nvoid binary_example()\r\n{\r\n    auto console = spdlog::get(\"console\");\r\n    std::array<char, 80> buf;\r\n    console->info(\"Binary example: {}\", spdlog::to_hex(buf));\r\n    console->info(\"Another binary example:{:n}\", spdlog::to_hex(std::begin(buf), std::begin(buf) + 10));\r\n    // more examples:\r\n    // logger->info(\"uppercase: {:X}\", spdlog::to_hex(buf));\r\n    // logger->info(\"uppercase, no delimiters: {:Xs}\", spdlog::to_hex(buf));\r\n    // logger->info(\"uppercase, no delimiters, no position info: {:Xsp}\", spdlog::to_hex(buf));\r\n}\r\n\r\n```\r\n\r\n---\r\n#### Logger with multi sinks - each with a different format and log level\r\n```c++\r\n\r\n// create a logger with 2 targets, with different log levels and formats.\r\n// The console will show only warnings or errors, while the file will log all. \r\nvoid multi_sink_example()\r\n{\r\n    auto console_sink = std::make_shared<spdlog::sinks::stdout_color_sink_mt>();\r\n    console_sink->set_level(spdlog::level::warn);\r\n    console_sink->set_pattern(\"[multi_sink_example] [%^%l%$] %v\");\r\n\r\n    auto file_sink = std::make_shared<spdlog::sinks::basic_file_sink_mt>(\"logs/multisink.txt\", true);\r\n    file_sink->set_level(spdlog::level::trace);\r\n\r\n    spdlog::logger logger(\"multi_sink\", {console_sink, file_sink});\r\n    logger.set_level(spdlog::level::debug);\r\n    logger.warn(\"this should appear in both console and file\");\r\n    logger.info(\"this message should not appear in the console, only in the file\");\r\n}\r\n```\r\n\r\n---\r\n#### Register several loggers - change global level\r\n```c++\r\n\r\n// Creation of loggers. Set levels to all registered loggers. \r\nvoid set_level_example()\r\n{\r\n    auto logger1 = spdlog::basic_logger_mt(\"logger1\", \"logs/logger1.txt\");\r\n    auto logger2 = spdlog::basic_logger_mt(\"logger2\", \"logs/logger2.txt\");\r\n\r\n    spdlog::set_default_logger(logger2);\r\n    spdlog::default_logger()->set_level(spdlog::level::trace); // set level for the default logger (logger2) to trace\r\n\r\n    spdlog::trace(\"trace message to the logger2 (specified as default)\");\r\n\r\n    spdlog::set_level(spdlog::level::off) // (sic!) set level for *all* registered loggers to off (disable)\r\n  \r\n    logger1.warn(\"warn message will not appear because the level set to off\");\r\n    logger2.warn(\"warn message will not appear because the level set to off\");\r\n    spdlog::warn(\"warn message will not appear because the level set to off\");\r\n}\r\n```\r\n\r\n---\r\n#### User-defined callbacks about log events\r\n```c++\r\n\r\n// create a logger with a lambda function callback, the callback will be called\r\n// each time something is logged to the logger\r\nvoid callback_example()\r\n{\r\n    auto callback_sink = std::make_shared<spdlog::sinks::callback_sink_mt>([](const spdlog::details::log_msg &msg) {\r\n         // for example you can be notified by sending an email to yourself\r\n    });\r\n    callback_sink->set_level(spdlog::level::err);\r\n\r\n    auto console_sink = std::make_shared<spdlog::sinks::stdout_color_sink_mt>();\r\n    spdlog::logger logger(\"custom_callback_logger\", {console_sink, callback_sink});\r\n\r\n    logger.info(\"some info log\");\r\n    logger.error(\"critical issue\"); // will notify you\r\n}\r\n```\r\n\r\n---\r\n#### Asynchronous logging\r\n```c++\r\n#include \"spdlog/async.h\"\r\n#include \"spdlog/sinks/basic_file_sink.h\"\r\nvoid async_example()\r\n{\r\n    // default thread pool settings can be modified *before* creating the async logger:\r\n    // spdlog::init_thread_pool(8192, 1); // queue with 8k items and 1 backing thread.\r\n    auto async_file = spdlog::basic_logger_mt<spdlog::async_factory>(\"async_file_logger\", \"logs/async_log.txt\");\r\n    // alternatively:\r\n    // auto async_file = spdlog::create_async<spdlog::sinks::basic_file_sink_mt>(\"async_file_logger\", \"logs/async_log.txt\");   \r\n}\r\n\r\n```\r\n\r\n---\r\n#### Asynchronous logger with multi sinks\r\n```c++\r\n#include \"spdlog/async.h\"\r\n#include \"spdlog/sinks/stdout_color_sinks.h\"\r\n#include \"spdlog/sinks/rotating_file_sink.h\"\r\n\r\nvoid multi_sink_example2()\r\n{\r\n    spdlog::init_thread_pool(8192, 1);\r\n    auto stdout_sink = std::make_shared<spdlog::sinks::stdout_color_sink_mt >();\r\n    auto rotating_sink = std::make_shared<spdlog::sinks::rotating_file_sink_mt>(\"mylog.txt\", 1024*1024*10, 3);\r\n    std::vector<spdlog::sink_ptr> sinks {stdout_sink, rotating_sink};\r\n    auto logger = std::make_shared<spdlog::async_logger>(\"loggername\", sinks.begin(), sinks.end(), spdlog::thread_pool(), spdlog::async_overflow_policy::block);\r\n    spdlog::register_logger(logger);\r\n}\r\n```\r\n \r\n---\r\n#### User-defined types\r\n```c++\r\ntemplate<>\r\nstruct fmt::formatter<my_type> : fmt::formatter<std::string>\r\n{\r\n    auto format(my_type my, format_context &ctx) const -> decltype(ctx.out())\r\n    {\r\n        return fmt::format_to(ctx.out(), \"[my_type i={}]\", my.i);\r\n    }\r\n};\r\n\r\nvoid user_defined_example()\r\n{\r\n    spdlog::info(\"user defined type: {}\", my_type(14));\r\n}\r\n\r\n```\r\n\r\n---\r\n#### User-defined flags in the log pattern\r\n```c++ \r\n// Log patterns can contain custom flags.\r\n// the following example will add new flag '%*' - which will be bound to a <my_formatter_flag> instance.\r\n#include \"spdlog/pattern_formatter.h\"\r\nclass my_formatter_flag : public spdlog::custom_flag_formatter\r\n{\r\npublic:\r\n    void format(const spdlog::details::log_msg &, const std::tm &, spdlog::memory_buf_t &dest) override\r\n    {\r\n        std::string some_txt = \"custom-flag\";\r\n        dest.append(some_txt.data(), some_txt.data() + some_txt.size());\r\n    }\r\n\r\n    std::unique_ptr<custom_flag_formatter> clone() const override\r\n    {\r\n        return spdlog::details::make_unique<my_formatter_flag>();\r\n    }\r\n};\r\n\r\nvoid custom_flags_example()\r\n{    \r\n    auto formatter = std::make_unique<spdlog::pattern_formatter>();\r\n    formatter->add_flag<my_formatter_flag>('*').set_pattern(\"[%n] [%*] [%^%l%$] %v\");\r\n    spdlog::set_formatter(std::move(formatter));\r\n}\r\n\r\n```\r\n\r\n---\r\n#### Custom error handler\r\n```c++\r\nvoid err_handler_example()\r\n{\r\n    // can be set globally or per logger(logger->set_error_handler(..))\r\n    spdlog::set_error_handler([](const std::string &msg) { spdlog::get(\"console\")->error(\"*** LOGGER ERROR ***: {}\", msg); });\r\n    spdlog::get(\"console\")->info(\"some invalid message to trigger an error {}{}{}{}\", 3);\r\n}\r\n\r\n```\r\n\r\n---\r\n#### syslog\r\n```c++\r\n#include \"spdlog/sinks/syslog_sink.h\"\r\nvoid syslog_example()\r\n{\r\n    std::string ident = \"spdlog-example\";\r\n    auto syslog_logger = spdlog::syslog_logger_mt(\"syslog\", ident, LOG_PID);\r\n    syslog_logger->warn(\"This is warning that will end up in syslog.\");\r\n}\r\n```\r\n---\r\n#### Android example\r\n```c++\r\n#include \"spdlog/sinks/android_sink.h\"\r\nvoid android_example()\r\n{\r\n    std::string tag = \"spdlog-android\";\r\n    auto android_logger = spdlog::android_logger_mt(\"android\", tag);\r\n    android_logger->critical(\"Use \\\"adb shell logcat\\\" to view this message.\");\r\n}\r\n```\r\n\r\n---\r\n#### Load log levels from the env variable or argv\r\n\r\n```c++\r\n#include \"spdlog/cfg/env.h\"\r\nint main (int argc, char *argv[])\r\n{\r\n    spdlog::cfg::load_env_levels();\r\n    // or specify the env variable name:\r\n    // MYAPP_LEVEL=info,mylogger=trace && ./example\r\n    // spdlog::cfg::load_env_levels(\"MYAPP_LEVEL\");\r\n    // or from the command line:\r\n    // ./example SPDLOG_LEVEL=info,mylogger=trace\r\n    // #include \"spdlog/cfg/argv.h\" // for loading levels from argv\r\n    // spdlog::cfg::load_argv_levels(argc, argv);\r\n}\r\n```\r\nSo then you can:\r\n\r\n```console\r\n$ export SPDLOG_LEVEL=info,mylogger=trace\r\n$ ./example\r\n```\r\n\r\n\r\n---\r\n#### Log file open/close event handlers\r\n```c++\r\n// You can get callbacks from spdlog before/after a log file has been opened or closed. \r\n// This is useful for cleanup procedures or for adding something to the start/end of the log file.\r\nvoid file_events_example()\r\n{\r\n    // pass the spdlog::file_event_handlers to file sinks for open/close log file notifications\r\n    spdlog::file_event_handlers handlers;\r\n    handlers.before_open = [](spdlog::filename_t filename) { spdlog::info(\"Before opening {}\", filename); };\r\n    handlers.after_open = [](spdlog::filename_t filename, std::FILE *fstream) { fputs(\"After opening\\n\", fstream); };\r\n    handlers.before_close = [](spdlog::filename_t filename, std::FILE *fstream) { fputs(\"Before closing\\n\", fstream); };\r\n    handlers.after_close = [](spdlog::filename_t filename) { spdlog::info(\"After closing {}\", filename); };\r\n    auto my_logger = spdlog::basic_logger_st(\"some_logger\", \"logs/events-sample.txt\", true, handlers);        \r\n}\r\n```\r\n\r\n---\r\n#### Replace the Default Logger\r\n```c++\r\nvoid replace_default_logger_example()\r\n{\r\n    auto new_logger = spdlog::basic_logger_mt(\"new_default_logger\", \"logs/new-default-log.txt\", true);\r\n    spdlog::set_default_logger(new_logger);\r\n    spdlog::info(\"new logger log message\");\r\n}\r\n```\r\n\r\n---\r\n#### Log to Qt with nice colors\r\n```c++\r\n#include \"spdlog/spdlog.h\"\r\n#include \"spdlog/sinks/qt_sinks.h\"\r\nMainWindow::MainWindow(QWidget *parent) : QMainWindow(parent)\r\n{\r\n    setMinimumSize(640, 480);\r\n    auto log_widget = new QTextEdit(this);\r\n    setCentralWidget(log_widget);\r\n    int max_lines = 500; // keep the text widget to max 500 lines. remove old lines if needed.\r\n    auto logger = spdlog::qt_color_logger_mt(\"qt_logger\", log_widget, max_lines);\r\n    logger->info(\"Some info message\");\r\n}\r\n```\r\n---\r\n\r\n#### Mapped Diagnostic Context\r\n```c++\r\n// Mapped Diagnostic Context (MDC) is a map that stores key-value pairs (string values) in thread local storage.\r\n// Each thread maintains its own MDC, which loggers use to append diagnostic information to log outputs.\r\n// Note: it is not supported in asynchronous mode due to its reliance on thread-local storage.\r\n#include \"spdlog/mdc.h\"\r\nvoid mdc_example()\r\n{\r\n    spdlog::mdc::put(\"key1\", \"value1\");\r\n    spdlog::mdc::put(\"key2\", \"value2\");\r\n    // if not using the default format, use the %& formatter to print mdc data\r\n    // spdlog::set_pattern(\"[%H:%M:%S %z] [%^%L%$] [%&] %v\");\r\n}\r\n```\r\n---\r\n## Benchmarks\r\n\r\nBelow are some [benchmarks](bench/bench.cpp) done in Ubuntu 64 bit, Intel i7-4770 CPU @ 3.40GHz\r\n\r\n#### Synchronous mode\r\n```\r\n[info] **************************************************************\r\n[info] Single thread, 1,000,000 iterations\r\n[info] **************************************************************\r\n[info] basic_st         Elapsed: 0.17 secs        5,777,626/sec\r\n[info] rotating_st      Elapsed: 0.18 secs        5,475,894/sec\r\n[info] daily_st         Elapsed: 0.20 secs        5,062,659/sec\r\n[info] empty_logger     Elapsed: 0.07 secs       14,127,300/sec\r\n[info] **************************************************************\r\n[info] C-string (400 bytes). Single thread, 1,000,000 iterations\r\n[info] **************************************************************\r\n[info] basic_st         Elapsed: 0.41 secs        2,412,483/sec\r\n[info] rotating_st      Elapsed: 0.72 secs        1,389,196/sec\r\n[info] daily_st         Elapsed: 0.42 secs        2,393,298/sec\r\n[info] null_st          Elapsed: 0.04 secs       27,446,957/sec\r\n[info] **************************************************************\r\n[info] 10 threads, competing over the same logger object, 1,000,000 iterations\r\n[info] **************************************************************\r\n[info] basic_mt         Elapsed: 0.60 secs        1,659,613/sec\r\n[info] rotating_mt      Elapsed: 0.62 secs        1,612,493/sec\r\n[info] daily_mt         Elapsed: 0.61 secs        1,638,305/sec\r\n[info] null_mt          Elapsed: 0.16 secs        6,272,758/sec\r\n```\r\n#### Asynchronous mode\r\n```\r\n[info] -------------------------------------------------\r\n[info] Messages     : 1,000,000\r\n[info] Threads      : 10\r\n[info] Queue        : 8,192 slots\r\n[info] Queue memory : 8,192 x 272 = 2,176 KB \r\n[info] -------------------------------------------------\r\n[info] \r\n[info] *********************************\r\n[info] Queue Overflow Policy: block\r\n[info] *********************************\r\n[info] Elapsed: 1.70784 secs     585,535/sec\r\n[info] Elapsed: 1.69805 secs     588,910/sec\r\n[info] Elapsed: 1.7026 secs      587,337/sec\r\n[info] \r\n[info] *********************************\r\n[info] Queue Overflow Policy: overrun\r\n[info] *********************************\r\n[info] Elapsed: 0.372816 secs    2,682,285/sec\r\n[info] Elapsed: 0.379758 secs    2,633,255/sec\r\n[info] Elapsed: 0.373532 secs    2,677,147/sec\r\n\r\n```\r\n\r\n## Documentation\r\n\r\nDocumentation can be found in the [wiki](https://github.com/gabime/spdlog/wiki) pages.\r\n\r\n---\r\n\r\n### Powered by\r\n<a href=\"https://jb.gg/OpenSource\">\r\n  <img src=\"https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.svg\" alt=\"JetBrains logo\" width=\"200\">\r\n</a>\r\n",
      "stars_today": 7
    },
    {
      "id": 24420506,
      "name": "v8",
      "full_name": "v8/v8",
      "description": "The official mirror of the V8 Git repository",
      "html_url": "https://github.com/v8/v8",
      "stars": 24811,
      "forks": 4210,
      "language": "C++",
      "topics": [
        "compiler",
        "interpreter",
        "javascript",
        "javascript-engine",
        "virtual-machine"
      ],
      "created_at": "2014-09-24T15:24:30Z",
      "updated_at": "2026-01-28T00:02:10Z",
      "pushed_at": "2026-01-27T22:42:04Z",
      "open_issues": 7,
      "owner": {
        "login": "v8",
        "avatar_url": "https://avatars.githubusercontent.com/u/113781?v=4"
      },
      "readme": "V8 JavaScript Engine\n=============\n\nV8 is Google's open source JavaScript engine.\n\nV8 implements ECMAScript as specified in ECMA-262.\n\nV8 is written in C++ and is used in Google Chrome, the open source\nbrowser from Google.\n\nV8 can run standalone, or can be embedded into any C++ application.\n\nV8 Project page: https://v8.dev/docs\n\n\nGetting the Code\n=============\n\nCheckout [depot tools](http://www.chromium.org/developers/how-tos/install-depot-tools), and run\n\n        fetch v8\n\nThis will checkout V8 into the directory `v8` and fetch all of its dependencies.\nTo stay up to date, run\n\n        git pull origin\n        gclient sync\n\nFor fetching all branches, add the following into your remote\nconfiguration in `.git/config`:\n\n        fetch = +refs/branch-heads/*:refs/remotes/branch-heads/*\n        fetch = +refs/tags/*:refs/tags/*\n\n\nContributing\n=============\n\nPlease follow the instructions mentioned at\n[v8.dev/docs/contribute](https://v8.dev/docs/contribute).\n",
      "stars_today": 7
    },
    {
      "id": 488177011,
      "name": "nekoray",
      "full_name": "MatsuriDayo/nekoray",
      "description": "ä¸å†ç»´æŠ¤ï¼Œè‡ªå¯»æ›¿ä»£å“ã€‚ Qt based cross-platform GUI proxy configuration manager (backend: sing-box)",
      "html_url": "https://github.com/MatsuriDayo/nekoray",
      "stars": 15133,
      "forks": 1466,
      "language": "C++",
      "topics": [],
      "created_at": "2022-05-03T11:12:00Z",
      "updated_at": "2026-01-28T01:06:29Z",
      "pushed_at": "2024-12-12T08:25:39Z",
      "open_issues": 218,
      "owner": {
        "login": "MatsuriDayo",
        "avatar_url": "https://avatars.githubusercontent.com/u/95122236?v=4"
      },
      "readme": "# NekoBox For PC\n\nQt based cross-platform GUI proxy configuration manager (backend: sing-box)\n\nSupport Windows / Linux out of the box now.\n\nåŸºäº Qt çš„è·¨å¹³å°ä»£ç†é…ç½®ç®¡ç†å™¨ (åç«¯ sing-box)\n\nç›®å‰æ”¯æŒ Windows / Linux å¼€ç®±å³ç”¨\n\n## ä¸‹è½½ / Download\n\n### GitHub Releases (Portable ZIP)\n\nä¾¿æºæ ¼å¼ï¼Œæ— å®‰è£…å™¨ã€‚è½¬åˆ° Releases ä¸‹è½½é¢„ç¼–è¯‘çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œè§£å‹åå³å¯ä½¿ç”¨ã€‚\n\n[![GitHub All Releases](https://img.shields.io/github/downloads/Matsuridayo/nekoray/total?label=downloads-total&logo=github&style=flat-square)](https://github.com/Matsuridayo/nekoray/releases)\n\n[ä¸‹è½½ / Download](https://github.com/Matsuridayo/nekoray/releases)\n\n[å®‰è£…åŒ…çš„è¯´æ˜ï¼Œå¦‚æœä½ ä¸çŸ¥é“è¦ä¸‹è½½å“ªä¸€ä¸ª](https://github.com/MatsuriDayo/nekoray/wiki/Installation-package-description)\n\n### Package\n\n#### AUR\n\n- [nekoray](https://aur.archlinux.org/packages/nekoray)\n- [nekoray-git](https://aur.archlinux.org/packages/nekoray-git)\n\n#### archlinuxcn\n\n- [nekoray](https://github.com/archlinuxcn/repo/tree/master/archlinuxcn/nekoray)\n- [nekoray-git](https://github.com/archlinuxcn/repo/tree/master/archlinuxcn/nekoray-git)\n\n#### Scoop Extras\n\n`scoop install nekoray`\n\n## æ›´æ”¹è®°å½• & å‘å¸ƒé¢‘é“ / Changelog & Telegram Channel\n\nhttps://t.me/Matsuridayo\n\n## é¡¹ç›®ä¸»é¡µ & æ–‡æ¡£ / Homepage & Documents\n\nhttps://matsuridayo.github.io\n\n## ä»£ç† / Proxy\n\n- SOCKS (4/4a/5)\n- HTTP(S)\n- Shadowsocks\n- VMess\n- VLESS\n- Trojan\n- TUIC ( sing-box )\n- NaÃ¯veProxy ( Custom Core )\n- Hysteria2 ( Custom Core or sing-box )\n- Custom Outbound\n- Custom Config\n- Custom Core\n\n## è®¢é˜… / Subscription\n\n- Raw: some widely used formats (like Shadowsocks, Clash and v2rayN)\n- åŸå§‹æ ¼å¼: ä¸€äº›å¹¿æ³›ä½¿ç”¨çš„æ ¼å¼ (å¦‚ Shadowsocksã€Clash å’Œ v2rayN)\n\n## è¿è¡Œå‚æ•°\n\n[è¿è¡Œå‚æ•°](docs/RunFlags.md)\n\n## Windows è¿è¡Œ\n\nè‹¥æç¤º DLL ç¼ºå¤±ï¼Œæ— æ³•è¿è¡Œï¼Œè¯·ä¸‹è½½ å®‰è£… [å¾®è½¯ C++ è¿è¡Œåº“](https://aka.ms/vs/17/release/vc_redist.x64.exe)\n\n## Linux è¿è¡Œ\n\n[Linux è¿è¡Œæ•™ç¨‹](docs/Run_Linux.md)\n\n## ç¼–è¯‘æ•™ç¨‹ / Compile Tutorial\n\nè¯·çœ‹ [æŠ€æœ¯æ–‡æ¡£ / Technical documentation](https://github.com/MatsuriDayo/nekoray/tree/main/docs)\n\n## æåŠ© / Donate\n\nå¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œå¯ä»¥é€šè¿‡æèµ çš„æ–¹å¼å¸®åŠ©æˆ‘ä»¬ç»´æŒè¿™ä¸ªé¡¹ç›®ã€‚\n\næèµ æ»¡ç­‰é¢ 50 USD å¯ä»¥åœ¨ã€Œ[æèµ æ¦œ](https://mtrdnt.pages.dev/donation_list)ã€æ˜¾ç¤ºå¤´åƒï¼Œå¦‚æœæ‚¨æœªè¢«æ·»åŠ åˆ°è¿™é‡Œï¼Œæ¬¢è¿è”ç³»æˆ‘ä»¬è¡¥å……ã€‚\n\nDonations of 50 USD or more can display your avatar on the [Donation List](https://mtrdnt.pages.dev/donation_list). If you are not added here, please contact us to add it.\n\nUSDT TRC20\n\n`TRhnA7SXE5Sap5gSG3ijxRmdYFiD4KRhPs`\n\nXMR\n\n`49bwESYQjoRL3xmvTcjZKHEKaiGywjLYVQJMUv79bXonGiyDCs8AzE3KiGW2ytTybBCpWJUvov8SjZZEGg66a4e59GXa6k5`\n\n## Credits\n\nCore:\n\n- [v2fly/v2ray-core](https://github.com/v2fly/v2ray-core) ( < 3.10 )\n- [MatsuriDayo/Matsuri](https://github.com/MatsuriDayo/Matsuri) ( < 3.10 )\n- [MatsuriDayo/v2ray-core](https://github.com/MatsuriDayo/v2ray-core) ( < 3.10 )\n- [XTLS/Xray-core](https://github.com/XTLS/Xray-core) ( 3.10 <= Version <= 3.26 )\n- [MatsuriDayo/Xray-core](https://github.com/MatsuriDayo/Xray-core) ( 3.10 <= Version <= 3.26 )\n- [SagerNet/sing-box](https://github.com/SagerNet/sing-box)\n- [Matsuridayo/sing-box-extra](https://github.com/MatsuriDayo/sing-box-extra)\n\nGui:\n\n- [Qv2ray](https://github.com/Qv2ray/Qv2ray)\n- [Qt](https://www.qt.io/)\n- [protobuf](https://github.com/protocolbuffers/protobuf)\n- [yaml-cpp](https://github.com/jbeder/yaml-cpp)\n- [zxing-cpp](https://github.com/nu-book/zxing-cpp)\n- [QHotkey](https://github.com/Skycoder42/QHotkey)\n- [AppImageKit](https://github.com/AppImage/AppImageKit)\n",
      "stars_today": 7
    },
    {
      "id": 368683828,
      "name": "jwt",
      "full_name": "golang-jwt/jwt",
      "description": "Go implementation of JSON Web Tokens (JWT).",
      "html_url": "https://github.com/golang-jwt/jwt",
      "stars": 8864,
      "forks": 426,
      "language": "Go",
      "topics": [
        "auth",
        "ed25519",
        "go",
        "golang",
        "jwt",
        "security"
      ],
      "created_at": "2021-05-18T22:42:37Z",
      "updated_at": "2026-01-27T22:48:51Z",
      "pushed_at": "2026-01-26T13:01:20Z",
      "open_issues": 38,
      "owner": {
        "login": "golang-jwt",
        "avatar_url": "https://avatars.githubusercontent.com/u/84194169?v=4"
      },
      "readme": "# jwt-go\n\n[![build](https://github.com/golang-jwt/jwt/actions/workflows/build.yml/badge.svg)](https://github.com/golang-jwt/jwt/actions/workflows/build.yml)\n[![Go\nReference](https://pkg.go.dev/badge/github.com/golang-jwt/jwt/v5.svg)](https://pkg.go.dev/github.com/golang-jwt/jwt/v5)\n[![Coverage Status](https://coveralls.io/repos/github/golang-jwt/jwt/badge.svg?branch=main)](https://coveralls.io/github/golang-jwt/jwt?branch=main)\n\nA [go](http://www.golang.org) (or 'golang' for search engine friendliness)\nimplementation of [JSON Web\nTokens](https://datatracker.ietf.org/doc/html/rfc7519).\n\nStarting with [v4.0.0](https://github.com/golang-jwt/jwt/releases/tag/v4.0.0)\nthis project adds Go module support, but maintains backward compatibility with\nolder `v3.x.y` tags and upstream `github.com/dgrijalva/jwt-go`. See the\n[`MIGRATION_GUIDE.md`](./MIGRATION_GUIDE.md) for more information. Version\nv5.0.0 introduces major improvements to the validation of tokens, but is not\nentirely backward compatible. \n\n> After the original author of the library suggested migrating the maintenance\n> of `jwt-go`, a dedicated team of open source maintainers decided to clone the\n> existing library into this repository. See\n> [dgrijalva/jwt-go#462](https://github.com/dgrijalva/jwt-go/issues/462) for a\n> detailed discussion on this topic.\n\n\n**SECURITY NOTICE:** Some older versions of Go have a security issue in the\ncrypto/elliptic. The recommendation is to upgrade to at least 1.15 See issue\n[dgrijalva/jwt-go#216](https://github.com/dgrijalva/jwt-go/issues/216) for more\ndetail.\n\n**SECURITY NOTICE:** It's important that you [validate the `alg` presented is\nwhat you\nexpect](https://auth0.com/blog/critical-vulnerabilities-in-json-web-token-libraries/).\nThis library attempts to make it easy to do the right thing by requiring key\ntypes to match the expected alg, but you should take the extra step to verify it in\nyour usage.  See the examples provided.\n\n### Supported Go versions\n\nOur support of Go versions is aligned with Go's [version release\npolicy](https://golang.org/doc/devel/release#policy). So we will support a major\nversion of Go until there are two newer major releases. We no longer support\nbuilding jwt-go with unsupported Go versions, as these contain security\nvulnerabilities that will not be fixed.\n\n## What the heck is a JWT?\n\nJWT.io has [a great introduction](https://jwt.io/introduction) to JSON Web\nTokens.\n\nIn short, it's a signed JSON object that does something useful (for example,\nauthentication).  It's commonly used for `Bearer` tokens in Oauth 2.  A token is\nmade of three parts, separated by `.`'s.  The first two parts are JSON objects,\nthat have been [base64url](https://datatracker.ietf.org/doc/html/rfc4648)\nencoded.  The last part is the signature, encoded the same way.\n\nThe first part is called the header.  It contains the necessary information for\nverifying the last part, the signature.  For example, which encryption method\nwas used for signing and what key was used.\n\nThe part in the middle is the interesting bit.  It's called the Claims and\ncontains the actual stuff you care about.  Refer to [RFC\n7519](https://datatracker.ietf.org/doc/html/rfc7519) for information about\nreserved keys and the proper way to add your own.\n\n## What's in the box?\n\nThis library supports the parsing and verification as well as the generation and\nsigning of JWTs.  Current supported signing algorithms are HMAC SHA, RSA,\nRSA-PSS, and ECDSA, though hooks are present for adding your own.\n\n## Installation Guidelines\n\n1. To install the jwt package, you first need to have\n   [Go](https://go.dev/doc/install) installed, then you can use the command\n   below to add `jwt-go` as a dependency in your Go program.\n\n```sh\ngo get -u github.com/golang-jwt/jwt/v5\n```\n\n2. Import it in your code:\n\n```go\nimport \"github.com/golang-jwt/jwt/v5\"\n```\n\n## Usage\n\nA detailed usage guide, including how to sign and verify tokens can be found on\nour [documentation website](https://golang-jwt.github.io/jwt/usage/create/).\n\n## Examples\n\nSee [the project documentation](https://pkg.go.dev/github.com/golang-jwt/jwt/v5)\nfor examples of usage:\n\n* [Simple example of parsing and validating a\n  token](https://pkg.go.dev/github.com/golang-jwt/jwt/v5#example-Parse-Hmac)\n* [Simple example of building and signing a\n  token](https://pkg.go.dev/github.com/golang-jwt/jwt/v5#example-New-Hmac)\n* [Directory of\n  Examples](https://pkg.go.dev/github.com/golang-jwt/jwt/v5#pkg-examples)\n\n## Compliance\n\nThis library was last reviewed to comply with [RFC\n7519](https://datatracker.ietf.org/doc/html/rfc7519) dated May 2015 with a few\nnotable differences:\n\n* In order to protect against accidental use of [Unsecured\n  JWTs](https://datatracker.ietf.org/doc/html/rfc7519#section-6), tokens using\n  `alg=none` will only be accepted if the constant\n  `jwt.UnsafeAllowNoneSignatureType` is provided as the key.\n\n## Project Status & Versioning\n\nThis library is considered production ready.  Feedback and feature requests are\nappreciated.  The API should be considered stable.  There should be very few\nbackward-incompatible changes outside of major version updates (and only with\ngood reason).\n\nThis project uses [Semantic Versioning 2.0.0](http://semver.org).  Accepted pull\nrequests will land on `main`.  Periodically, versions will be tagged from\n`main`.  You can find all the releases on [the project releases\npage](https://github.com/golang-jwt/jwt/releases).\n\n**BREAKING CHANGES:** A full list of breaking changes is available in\n`VERSION_HISTORY.md`.  See [`MIGRATION_GUIDE.md`](./MIGRATION_GUIDE.md) for more information on updating\nyour code.\n\n## Extensions\n\nThis library publishes all the necessary components for adding your own signing\nmethods or key functions.  Simply implement the `SigningMethod` interface and\nregister a factory method using `RegisterSigningMethod` or provide a\n`jwt.Keyfunc`.\n\nA common use case would be integrating with different 3rd party signature\nproviders, like key management services from various cloud providers or Hardware\nSecurity Modules (HSMs) or to implement additional standards.\n\n| Extension | Purpose                                                                                                  | Repo                                              |\n| --------- | -------------------------------------------------------------------------------------------------------- | ------------------------------------------------- |\n| GCP       | Integrates with multiple Google Cloud Platform signing tools (AppEngine, IAM API, Cloud KMS)             | https://github.com/someone1/gcp-jwt-go            |\n| AWS       | Integrates with AWS Key Management Service, KMS                                                          | https://github.com/matelang/jwt-go-aws-kms        |\n| JWKS      | Provides support for JWKS ([RFC 7517](https://datatracker.ietf.org/doc/html/rfc7517)) as a `jwt.Keyfunc` | https://github.com/MicahParks/keyfunc             |\n| TPM       | Integrates with Trusted Platform Module (TPM)                                                            | https://github.com/salrashid123/golang-jwt-tpm    |\n\n*Disclaimer*: Unless otherwise specified, these integrations are maintained by\nthird parties and should not be considered as a primary offer by any of the\nmentioned cloud providers\n\n## More\n\nGo package documentation can be found [on\npkg.go.dev](https://pkg.go.dev/github.com/golang-jwt/jwt/v5). Additional\ndocumentation can be found on [our project\npage](https://golang-jwt.github.io/jwt/).\n\nThe command line utility included in this project (cmd/jwt) provides a\nstraightforward example of token creation and parsing as well as a useful tool\nfor debugging your own integration. You'll also find several implementation\nexamples in the documentation.\n\n[golang-jwt](https://github.com/orgs/golang-jwt) incorporates a modified version\nof the JWT logo, which is distributed under the terms of the [MIT\nLicense](https://github.com/jsonwebtoken/jsonwebtoken.github.io/blob/master/LICENSE.txt).\n",
      "stars_today": 7
    },
    {
      "id": 94262864,
      "name": "prost",
      "full_name": "tokio-rs/prost",
      "description": "PROST! a Protocol Buffers implementation for the Rust Language",
      "html_url": "https://github.com/tokio-rs/prost",
      "stars": 4575,
      "forks": 592,
      "language": "Rust",
      "topics": [
        "protobuf",
        "rust"
      ],
      "created_at": "2017-06-13T22:13:40Z",
      "updated_at": "2026-01-27T22:00:49Z",
      "pushed_at": "2026-01-10T20:13:41Z",
      "open_issues": 233,
      "owner": {
        "login": "tokio-rs",
        "avatar_url": "https://avatars.githubusercontent.com/u/20248544?v=4"
      },
      "readme": "[![maintenance-status: passively-maintained](https://img.shields.io/badge/maintenance--status-passively--maintained-forestgreen)](https://gist.github.com/rusty-snake/574a91f1df9f97ec77ca308d6d731e29)\n[![continuous integration](https://github.com/tokio-rs/prost/actions/workflows/ci.yml/badge.svg?branch=master)](https://github.com/tokio-rs/prost/actions/workflows/ci.yml?query=branch%3Amaster)\n[![Documentation](https://docs.rs/prost/badge.svg)](https://docs.rs/prost/)\n[![Crate](https://img.shields.io/crates/v/prost.svg)](https://crates.io/crates/prost)\n[![Dependency Status](https://deps.rs/repo/github/tokio-rs/prost/status.svg)](https://deps.rs/repo/github/tokio-rs/prost)\n[![Discord](https://img.shields.io/discord/500028886025895936)](https://discord.gg/tokio)\n\n# *PROST!*\n\n`prost` is a [Protocol Buffers](https://protobuf.dev/)\nimplementation for the [Rust Language](https://www.rust-lang.org/). `prost`\ngenerates simple, idiomatic Rust code from `proto2` and `proto3` files.\n\nCompared to other Protocol Buffers implementations, `prost`\n\n* Generates simple, idiomatic, and readable Rust types by taking advantage of\n  Rust `derive` attributes.\n* Retains comments from `.proto` files in generated Rust code.\n* Allows existing Rust types (not generated from a `.proto`) to be serialized\n  and deserialized by adding attributes.\n* Uses the [`bytes::{Buf, BufMut}`](https://github.com/tokio-rs/bytes)\n  abstractions for serialization instead of `std::io::{Read, Write}`.\n* Respects the Protobuf `package` specifier when organizing generated code\n  into Rust modules.\n* Preserves unknown enum values during deserialization.\n* Does not include support for runtime reflection or message descriptors.\n\n## Using `prost` in a Cargo Project\n\nFirst, add `prost` and its public dependencies to your `Cargo.toml`:\n\n```ignore\n[dependencies]\nprost = \"0.14\"\n# Only necessary if using Protobuf well-known types:\nprost-types = \"0.14\"\n```\n\nThe recommended way to add `.proto` compilation to a Cargo project is to use the\n`prost-build` library. See the [`prost-build` documentation][prost-build] for\nmore details and examples.\n\nSee the [snazzy repository][snazzy] for a simple start-to-finish example.\n\n[prost-build]: https://docs.rs/prost-build/latest/prost_build/\n[snazzy]: https://github.com/danburkert/snazzy\n\n### MSRV\n\n`prost` follows the `tokio-rs` project's MSRV model and supports 1.82. For more\ninformation on the tokio msrv policy you can check it out [here][tokio msrv]\n\n[tokio msrv]: https://github.com/tokio-rs/tokio/#supported-rust-versions\n\n## Generated Code\n\n`prost` generates Rust code from source `.proto` files using the `proto2` or\n`proto3` syntax. `prost`'s goal is to make the generated code as simple as\npossible.\n\n### `protoc`\n\nWith `prost-build` v0.11 release, `protoc` will be required to invoke\n`compile_protos` (unless `skip_protoc` is enabled). Prost will no longer provide\nbundled `protoc` or attempt to compile `protoc` for users. For install\ninstructions for `protoc`, please check out the [protobuf install] instructions.\n\n[protobuf install]: https://github.com/protocolbuffers/protobuf#protobuf-compiler-installation\n\n\n### Packages\n\nProst can now generate code for `.proto` files that don't have a package spec.\n`prost` will translate the Protobuf package into\na Rust module. For example, given the `package` specifier:\n\n[package]: https://protobuf.dev/programming-guides/proto3/#packages\n\n```protobuf,ignore\npackage foo.bar;\n```\n\nAll Rust types generated from the file will be in the `foo::bar` module.\n\n### Messages\n\nGiven a simple message declaration:\n\n```protobuf,ignore\n// Sample message.\nmessage Foo {\n}\n```\n\n`prost` will generate the following Rust struct:\n\n```rust,ignore\n/// Sample message.\n#[derive(Clone, Debug, PartialEq, Message)]\npub struct Foo {\n}\n```\n\n### Fields\n\nFields in Protobuf messages are translated into Rust as public struct fields of the\ncorresponding type.\n\n#### Scalar Values\n\nScalar value types are converted as follows:\n\n| Protobuf Type | Rust Type |\n| --- | --- |\n| `double` | `f64` |\n| `float` | `f32` |\n| `int32` | `i32` |\n| `int64` | `i64` |\n| `uint32` | `u32` |\n| `uint64` | `u64` |\n| `sint32` | `i32` |\n| `sint64` | `i64` |\n| `fixed32` | `u32` |\n| `fixed64` | `u64` |\n| `sfixed32` | `i32` |\n| `sfixed64` | `i64` |\n| `bool` | `bool` |\n| `string` | `String` |\n| `bytes` | `Vec<u8>` |\n\n#### Enumerations\n\nAll `.proto` enumeration types convert to the Rust `i32` type. Additionally,\neach enumeration type gets a corresponding Rust `enum` type. For example, this\n`proto` enum:\n\n```protobuf,ignore\nenum PhoneType {\n  MOBILE = 0;\n  HOME = 1;\n  WORK = 2;\n}\n```\n\ngets this corresponding Rust enum [^1]:\n\n```rust,ignore\npub enum PhoneType {\n    Mobile = 0,\n    Home = 1,\n    Work = 2,\n}\n```\n\n[^1]: Annotations have been elided for clarity. See below for a full example.\n\nYou can convert a `PhoneType` value to an `i32` by doing:\n\n```rust,ignore\nPhoneType::Mobile as i32\n```\n\nThe `#[derive(::prost::Enumeration)]` annotation added to the generated\n`PhoneType` adds these associated functions to the type:\n\n```rust,ignore\nimpl PhoneType {\n    pub fn is_valid(value: i32) -> bool { ... }\n    #[deprecated]\n    pub fn from_i32(value: i32) -> Option<PhoneType> { ... }\n}\n```\n\nIt also adds an `impl TryFrom<i32> for PhoneType`, so you can convert an `i32` to its corresponding `PhoneType` value by doing,\nfor example:\n\n```rust,ignore\nlet phone_type = 2i32;\n\nmatch PhoneType::try_from(phone_type) {\n    Ok(PhoneType::Mobile) => ...,\n    Ok(PhoneType::Home) => ...,\n    Ok(PhoneType::Work) => ...,\n    Err(_) => ...,\n}\n```\n\nAdditionally, wherever a `proto` enum is used as a field in a `Message`, the\nmessage will have 'accessor' methods to get/set the value of the field as the\nRust enum type. For instance, this proto `PhoneNumber` message that has a field\nnamed `type` of type `PhoneType`:\n\n```protobuf,ignore\nmessage PhoneNumber {\n  string number = 1;\n  PhoneType type = 2;\n}\n```\n\nwill become the following Rust type [^2] with methods `type` and `set_type`:\n\n```rust,ignore\npub struct PhoneNumber {\n    pub number: String,\n    pub r#type: i32, // the `r#` is needed because `type` is a Rust keyword\n}\n\nimpl PhoneNumber {\n    pub fn r#type(&self) -> PhoneType { ... }\n    pub fn set_type(&mut self, value: PhoneType) { ... }\n}\n```\n\nNote that the getter methods will return the Rust enum's default value if the\nfield has an invalid `i32` value.\n\nThe `enum` type isn't used directly as a field, because the Protobuf spec\nmandates that enumerations values are 'open', and decoding unrecognized\nenumeration values must be possible.\n\n[^2]: Annotations have been elided for clarity. See below for a full example.\n\n#### Field Modifiers\n\nProtobuf scalar value and enumeration message fields can have a modifier\ndepending on the Protobuf version. Modifiers change the corresponding type of\nthe Rust field:\n\n| `.proto` Version | Modifier | Rust Type |\n| --- | --- | --- |\n| `proto2` | `optional` | `Option<T>` |\n| `proto2` | `required` | `T` |\n| `proto3` | default | `T` for scalar types, `Option<T>` otherwise |\n| `proto3` | `optional` | `Option<T>` |\n| `proto2`/`proto3` | `repeated` | `Vec<T>` |\n\nNote that in `proto3` the default representation for all user-defined message\ntypes is `Option<T>`, and for scalar types just `T` (during decoding, a missing\nvalue is populated by `T::default()`). If you need a witness of the presence of\na scalar type `T`, use the `optional` modifier to enforce an `Option<T>`\nrepresentation in the generated Rust struct.\n\n#### Map Fields\n\nMap fields are converted to a Rust `HashMap` with key and value type converted\nfrom the Protobuf key and value types.\n\n#### Message Fields\n\nMessage fields are converted to the corresponding struct type. The table of\nfield modifiers above applies to message fields, except that `proto3` message\nfields without a modifier (the default) will be wrapped in an `Option`.\nTypically message fields are unboxed. `prost` will automatically box a message\nfield if the field type and the parent type are recursively nested in order to\navoid an infinite sized struct.\n\n#### Oneof Fields\n\nOneof fields convert to a Rust enum. Protobuf `oneof`s types are not named, so\n`prost` uses the name of the `oneof` field for the resulting Rust enum, and\ndefines the enum in a module under the struct. For example, a `proto3` message\nsuch as:\n\n```protobuf,ignore\nmessage Foo {\n  oneof widget {\n    int32 quux = 1;\n    string bar = 2;\n  }\n}\n```\n\ngenerates the following Rust[^3]:\n\n```rust,ignore\npub struct Foo {\n    pub widget: Option<foo::Widget>,\n}\npub mod foo {\n    pub enum Widget {\n        Quux(i32),\n        Bar(String),\n    }\n}\n```\n\n`oneof` fields are always wrapped in an `Option`.\n\n[^3]: Annotations have been elided for clarity. See below for a full example.\n\n### Services\n\n`prost-build` allows a custom code-generator to be used for processing `service`\ndefinitions. This can be used to output Rust traits according to an\napplication's specific needs.\n\n### Generated Code Example\n\nExample `.proto` file:\n\n```protobuf,ignore\nsyntax = \"proto3\";\npackage tutorial;\n\nmessage Person {\n  string name = 1;\n  int32 id = 2;  // Unique ID number for this person.\n  string email = 3;\n\n  enum PhoneType {\n    MOBILE = 0;\n    HOME = 1;\n    WORK = 2;\n  }\n\n  message PhoneNumber {\n    string number = 1;\n    PhoneType type = 2;\n  }\n\n  repeated PhoneNumber phones = 4;\n}\n\n// Our address book file is just one of these.\nmessage AddressBook {\n  repeated Person people = 1;\n}\n```\n\nand the generated Rust code (`tutorial.rs`):\n\n```rust,ignore\n#[derive(Clone, PartialEq, ::prost::Message)]\npub struct Person {\n    #[prost(string, tag=\"1\")]\n    pub name: ::prost::alloc::string::String,\n    /// Unique ID number for this person.\n    #[prost(int32, tag=\"2\")]\n    pub id: i32,\n    #[prost(string, tag=\"3\")]\n    pub email: ::prost::alloc::string::String,\n    #[prost(message, repeated, tag=\"4\")]\n    pub phones: ::prost::alloc::vec::Vec<person::PhoneNumber>,\n}\n/// Nested message and enum types in `Person`.\npub mod person {\n    #[derive(Clone, PartialEq, ::prost::Message)]\n    pub struct PhoneNumber {\n        #[prost(string, tag=\"1\")]\n        pub number: ::prost::alloc::string::String,\n        #[prost(enumeration=\"PhoneType\", tag=\"2\")]\n        pub r#type: i32,\n    }\n    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord, ::prost::Enumeration)]\n    #[repr(i32)]\n    pub enum PhoneType {\n        Mobile = 0,\n        Home = 1,\n        Work = 2,\n    }\n}\n/// Our address book file is just one of these.\n#[derive(Clone, PartialEq, ::prost::Message)]\npub struct AddressBook {\n    #[prost(message, repeated, tag=\"1\")]\n    pub people: ::prost::alloc::vec::Vec<Person>,\n}\n```\n\n## Accessing the `protoc` `FileDescriptorSet`\n\nThe `prost_build::Config::file_descriptor_set_path` option can be used to emit a file descriptor set\nduring the build & code generation step. When used in conjunction with the `std::include_bytes`\nmacro and the `prost_types::FileDescriptorSet` type, applications and libraries using Prost can\nimplement introspection capabilities requiring details from the original `.proto` files.\n\n## Using `prost` in a `no_std` Crate\n\n`prost` is compatible with `no_std` crates. To enable `no_std` support, disable\nthe `std` features in `prost` and `prost-types`:\n\n```ignore\n[dependencies]\nprost = { version = \"0.14.3\", default-features = false, features = [\"derive\"] }\n# Only necessary if using Protobuf well-known types:\nprost-types = { version = \"0.14.3\", default-features = false }\n```\n\nAdditionally, configure `prost-build` to output `BTreeMap`s instead of `HashMap`s\nfor all Protobuf `map` fields in your `build.rs`:\n\n```rust,ignore\nlet mut config = prost_build::Config::new();\nconfig.btree_map(&[\".\"]);\n```\n\nWhen using edition 2015, it may be necessary to add an `extern crate core;`\ndirective to the crate which includes `prost`-generated code.\n\n## Serializing Existing Types\n\n`prost` uses a custom derive macro to handle encoding and decoding types, which\nmeans that if your existing Rust type is compatible with Protobuf types, you can\nserialize and deserialize it by adding the appropriate derive and field\nannotations.\n\nCurrently the best documentation on adding annotations is to look at the\ngenerated code examples above.\n\n### Tag Inference for Existing Types\n\nProst automatically infers tags for the struct.\n\nFields are tagged sequentially in the order they\nare specified, starting with `1`.\n\nYou may skip tags which have been reserved, or where there are gaps between\nsequentially occurring tag values by specifying the tag number to skip to with\nthe `tag` attribute on the first field after the gap. The following fields will\nbe tagged sequentially starting from the next number.\n\n```rust,ignore\nuse prost;\nuse prost::{Enumeration, Message};\n\n#[derive(Clone, PartialEq, Message)]\nstruct Person {\n    #[prost(string, tag = \"1\")]\n    pub id: String, // tag=1\n    // NOTE: Old \"name\" field has been removed\n    // pub name: String, // tag=2 (Removed)\n    #[prost(string, tag = \"6\")]\n    pub given_name: String, // tag=6\n    #[prost(string)]\n    pub family_name: String, // tag=7\n    #[prost(string)]\n    pub formatted_name: String, // tag=8\n    #[prost(uint32, tag = \"3\")]\n    pub age: u32, // tag=3\n    #[prost(uint32)]\n    pub height: u32, // tag=4\n    #[prost(enumeration = \"Gender\")]\n    pub gender: i32, // tag=5\n    // NOTE: Skip to less commonly occurring fields\n    #[prost(string, tag = \"16\")]\n    pub name_prefix: String, // tag=16  (eg. mr/mrs/ms)\n    #[prost(string)]\n    pub name_suffix: String, // tag=17  (eg. jr/esq)\n    #[prost(string)]\n    pub maiden_name: String, // tag=18\n}\n\n#[derive(Clone, Copy, Debug, PartialEq, Eq, Enumeration)]\npub enum Gender {\n    Unknown = 0,\n    Female = 1,\n    Male = 2,\n}\n```\n\n## Nix\n\nThe prost project maintains flakes support for local development. Once you have\nnix and nix flakes setup you can just run `nix develop` to get a shell\nconfigured with the required dependencies to compile the whole project.\n\n## Feature Flags\n- `std`: Enable integration with standard library. Disable this feature for `no_std` support. This feature is enabled by default.\n- `derive`: Enable integration with `prost-derive`. Disable this feature to reduce compile times. This feature is enabled by default.\n- `prost-derive`: Deprecated. Alias for `derive` feature.\n- `no-recursion-limit`: Disable the recursion limit. The recursion limit is 100 and cannot be customized. \n\n## Contributing\n\nThe current maintainer is not contributing new features and doesn't have the time to review new features. Bug fixes and small improvements are welcome. Feel free to contribute small and easily reviewable PRs. \n\nBug fixes are still important, and security fixes will be released as soon as possible. Contact the `#prost` channel in [Tokio discord](https://discord.gg/tokio) if you feel a bug or security fix is not getting enough attention.\n\nThe maintainer expects the official `protobuf` project to release their rust library soon and expects it to be as fully featured as the C++ library. See their [source code](https://github.com/protocolbuffers/protobuf/tree/main/rust) and [crate](https://crates.io/crates/protobuf/4.33.1-release) for more information.\n\n## FAQ\n\n1. **Could `prost` be implemented as a serializer for [Serde](https://serde.rs/)?**\n\n   Probably not, however I would like to hear from a Serde expert on the matter.\n   There are two complications with trying to serialize Protobuf messages with\n   Serde:\n\n   - Protobuf fields require a numbered tag, and currently there appears to be no\n     mechanism suitable for this in `serde`.\n   - The mapping of Protobuf type to Rust type is not 1-to-1. As a result,\n     trait-based approaches to dispatching don't work very well. Example: six\n     different Protobuf field types correspond to a Rust `Vec<i32>`: `repeated\n     int32`, `repeated sint32`, `repeated sfixed32`, and their packed\n     counterparts.\n\n   But it is possible to place `serde` derive tags onto the generated types, so\n   the same structure can support both `prost` and `Serde`.\n\n2. **I get errors when trying to run `cargo test` on MacOS**\n\n   If the errors are about missing `autoreconf` or similar, you can probably fix\n   them by running\n\n   ```ignore\n   brew install automake\n   brew install libtool\n   ```\n\n3. **Why are most fields are wrapped in an `Option`?**\n\n   In the `protobuf` wire protocol all fields are optional. The design of `prost` choose to expose\n   this to the user of the types. `prost` leans toward correctness and safety, aligned with Rustâ€™s \n   philosophy â€” even at the cost of verbosity.\n\n## License\n\n`prost` is distributed under the terms of the Apache License (Version 2.0).\n\nSee [LICENSE](https://github.com/tokio-rs/prost/blob/master/LICENSE) for details.\n\nCopyright 2022 Dan Burkert & Tokio Contributors\n",
      "stars_today": 7
    },
    {
      "id": 5909706,
      "name": "cpp-httplib",
      "full_name": "yhirose/cpp-httplib",
      "description": "A C++ header-only HTTP/HTTPS server and client library",
      "html_url": "https://github.com/yhirose/cpp-httplib",
      "stars": 16058,
      "forks": 2617,
      "language": "C++",
      "topics": [
        "cpp",
        "cpp11",
        "header-only",
        "http",
        "https"
      ],
      "created_at": "2012-09-22T02:38:32Z",
      "updated_at": "2026-01-27T23:53:41Z",
      "pushed_at": "2026-01-23T20:03:11Z",
      "open_issues": 6,
      "owner": {
        "login": "yhirose",
        "avatar_url": "https://avatars.githubusercontent.com/u/357397?v=4"
      },
      "readme": "cpp-httplib\n===========\n\n[![](https://github.com/yhirose/cpp-httplib/workflows/test/badge.svg)](https://github.com/yhirose/cpp-httplib/actions)\n\nA C++11 single-file header-only cross platform HTTP/HTTPS library.\n\nIt's extremely easy to set up. Just include the **httplib.h** file in your code!\n\n> [!IMPORTANT]\n> This library uses 'blocking' socket I/O. If you are looking for a library with 'non-blocking' socket I/O, this is not the one that you want.\n\nSimple examples\n---------------\n\n#### Server (Multi-threaded)\n\n```c++\n#define CPPHTTPLIB_OPENSSL_SUPPORT\n#include \"path/to/httplib.h\"\n\n// HTTP\nhttplib::Server svr;\n\n// HTTPS\nhttplib::SSLServer svr;\n\nsvr.Get(\"/hi\", [](const httplib::Request &, httplib::Response &res) {\n  res.set_content(\"Hello World!\", \"text/plain\");\n});\n\nsvr.listen(\"0.0.0.0\", 8080);\n```\n\n#### Client\n\n```c++\n#define CPPHTTPLIB_OPENSSL_SUPPORT\n#include \"path/to/httplib.h\"\n\n// HTTP\nhttplib::Client cli(\"http://yhirose.github.io\");\n\n// HTTPS\nhttplib::Client cli(\"https://yhirose.github.io\");\n\nif (auto res = cli.Get(\"/hi\")) {\n  res->status;\n  res->body;\n}\n```\n\nSSL Support\n-----------\n\nSSL support is available with `CPPHTTPLIB_OPENSSL_SUPPORT`. `libssl` and `libcrypto` should be linked.\n\n> [!NOTE]\n> cpp-httplib currently supports only version 3.0 or later. Please see [this page](https://www.openssl.org/policies/releasestrat.html) to get more information.\n\n> [!TIP]\n> For macOS: cpp-httplib now can use system certs with `CPPHTTPLIB_USE_CERTS_FROM_MACOSX_KEYCHAIN`. `CoreFoundation` and `Security` should be linked with `-framework`.\n\n```c++\n#define CPPHTTPLIB_OPENSSL_SUPPORT\n#include \"path/to/httplib.h\"\n\n// Server\nhttplib::SSLServer svr(\"./cert.pem\", \"./key.pem\");\n\n// Client\nhttplib::Client cli(\"https://localhost:1234\"); // scheme + host\nhttplib::SSLClient cli(\"localhost:1234\"); // host\nhttplib::SSLClient cli(\"localhost\", 1234); // host, port\n\n// Use your CA bundle\ncli.set_ca_cert_path(\"./ca-bundle.crt\");\n\n// Disable cert verification\ncli.enable_server_certificate_verification(false);\n\n// Disable host verification\ncli.enable_server_hostname_verification(false);\n```\n\n> [!NOTE]\n> When using SSL, it seems impossible to avoid SIGPIPE in all cases, since on some operating systems, SIGPIPE can only be suppressed on a per-message basis, but there is no way to make the OpenSSL library do so for its internal communications. If your program needs to avoid being terminated on SIGPIPE, the only fully general way might be to set up a signal handler for SIGPIPE to handle or ignore it yourself.\n\n### SSL Error Handling\n\nWhen SSL operations fail, cpp-httplib provides detailed error information through two separate error fields:\n\n```c++\n#define CPPHTTPLIB_OPENSSL_SUPPORT\n#include \"path/to/httplib.h\"\n\nhttplib::Client cli(\"https://example.com\");\n\nauto res = cli.Get(\"/\");\nif (!res) {\n  // Check the error type\n  const auto err = res.error();\n\n  switch (err) {\n    case httplib::Error::SSLConnection:\n      std::cout << \"SSL connection failed, SSL error: \"\n                << res.ssl_error() << std::endl;\n      break;\n\n    case httplib::Error::SSLLoadingCerts:\n      std::cout << \"SSL cert loading failed, OpenSSL error: \"\n                << std::hex << res.ssl_openssl_error() << std::endl;\n      break;\n\n    case httplib::Error::SSLServerVerification:\n      std::cout << \"SSL verification failed, X509 error: \"\n                << res.ssl_openssl_error() << std::endl;\n      break;\n\n    case httplib::Error::SSLServerHostnameVerification:\n      std::cout << \"SSL hostname verification failed, X509 error: \"\n                << res.ssl_openssl_error() << std::endl;\n      break;\n\n    default:\n      std::cout << \"HTTP error: \" << httplib::to_string(err) << std::endl;\n  }\n}\n```\n\nServer\n------\n\n```c++\n#include <httplib.h>\n\nint main(void)\n{\n  using namespace httplib;\n\n  Server svr;\n\n  svr.Get(\"/hi\", [](const Request& req, Response& res) {\n    res.set_content(\"Hello World!\", \"text/plain\");\n  });\n\n  // Match the request path against a regular expression\n  // and extract its captures\n  svr.Get(R\"(/numbers/(\\d+))\", [&](const Request& req, Response& res) {\n    auto numbers = req.matches[1];\n    res.set_content(numbers, \"text/plain\");\n  });\n\n  // Capture the second segment of the request path as \"id\" path param\n  svr.Get(\"/users/:id\", [&](const Request& req, Response& res) {\n    auto user_id = req.path_params.at(\"id\");\n    res.set_content(user_id, \"text/plain\");\n  });\n\n  // Extract values from HTTP headers and URL query params\n  svr.Get(\"/body-header-param\", [](const Request& req, Response& res) {\n    if (req.has_header(\"Content-Length\")) {\n      auto val = req.get_header_value(\"Content-Length\");\n    }\n    if (req.has_param(\"key\")) {\n      auto val = req.get_param_value(\"key\");\n    }\n    res.set_content(req.body, \"text/plain\");\n  });\n\n  // If the handler takes time to finish, you can also poll the connection state\n  svr.Get(\"/task\", [&](const Request& req, Response& res) {\n    const char * result = nullptr;\n    process.run(); // for example, starting an external process\n    while (result == nullptr) {\n      sleep(1);\n      if (req.is_connection_closed()) {\n        process.kill(); // kill the process\n        return;\n      }\n      result = process.stdout(); // != nullptr if the process finishes\n    }\n    res.set_content(result, \"text/plain\");\n  });\n\n  svr.Get(\"/stop\", [&](const Request& req, Response& res) {\n    svr.stop();\n  });\n\n  svr.listen(\"localhost\", 1234);\n}\n```\n\n`Post`, `Put`, `Patch`, `Delete` and `Options` methods are also supported.\n\n### Bind a socket to multiple interfaces and any available port\n\n```cpp\nint port = svr.bind_to_any_port(\"0.0.0.0\");\nsvr.listen_after_bind();\n```\n\n### Static File Server\n\n```cpp\n// Mount / to ./www directory\nauto ret = svr.set_mount_point(\"/\", \"./www\");\nif (!ret) {\n  // The specified base directory doesn't exist...\n}\n\n// Mount /public to ./www directory\nret = svr.set_mount_point(\"/public\", \"./www\");\n\n// Mount /public to ./www1 and ./www2 directories\nret = svr.set_mount_point(\"/public\", \"./www1\"); // 1st order to search\nret = svr.set_mount_point(\"/public\", \"./www2\"); // 2nd order to search\n\n// Remove mount /\nret = svr.remove_mount_point(\"/\");\n\n// Remove mount /public\nret = svr.remove_mount_point(\"/public\");\n```\n\n```cpp\n// User defined file extension and MIME type mappings\nsvr.set_file_extension_and_mimetype_mapping(\"cc\", \"text/x-c\");\nsvr.set_file_extension_and_mimetype_mapping(\"cpp\", \"text/x-c\");\nsvr.set_file_extension_and_mimetype_mapping(\"hh\", \"text/x-h\");\n```\n\nThe following are built-in mappings:\n\n| Extension  |          MIME Type          | Extension  |          MIME Type          |\n| :--------- | :-------------------------- | :--------- | :-------------------------- |\n| css        | text/css                    | mpga       | audio/mpeg                  |\n| csv        | text/csv                    | weba       | audio/webm                  |\n| txt        | text/plain                  | wav        | audio/wave                  |\n| vtt        | text/vtt                    | otf        | font/otf                    |\n| html, htm  | text/html                   | ttf        | font/ttf                    |\n| apng       | image/apng                  | woff       | font/woff                   |\n| avif       | image/avif                  | woff2      | font/woff2                  |\n| bmp        | image/bmp                   | 7z         | application/x-7z-compressed |\n| gif        | image/gif                   | atom       | application/atom+xml        |\n| png        | image/png                   | pdf        | application/pdf             |\n| svg        | image/svg+xml               | mjs, js    | text/javascript             |\n| webp       | image/webp                  | json       | application/json            |\n| ico        | image/x-icon                | rss        | application/rss+xml         |\n| tif        | image/tiff                  | tar        | application/x-tar           |\n| tiff       | image/tiff                  | xhtml, xht | application/xhtml+xml       |\n| jpeg, jpg  | image/jpeg                  | xslt       | application/xslt+xml        |\n| mp4        | video/mp4                   | xml        | application/xml             |\n| mpeg       | video/mpeg                  | gz         | application/gzip            |\n| webm       | video/webm                  | zip        | application/zip             |\n| mp3        | audio/mp3                   | wasm       | application/wasm            |\n\n> [!WARNING]\n> These static file server methods are not thread-safe.\n\n### File request handler\n\n```cpp\n// The handler is called right before the response is sent to a client\nsvr.set_file_request_handler([](const Request &req, Response &res) {\n  ...\n});\n```\n\n### Logging\n\ncpp-httplib provides separate logging capabilities for access logs and error logs, similar to web servers like Nginx and Apache.\n\n#### Access Logging\n\nAccess loggers capture successful HTTP requests and responses:\n\n```cpp\nsvr.set_logger([](const httplib::Request& req, const httplib::Response& res) {\n  std::cout << req.method << \" \" << req.path << \" -> \" << res.status << std::endl;\n});\n```\n\n#### Pre-compression Logging\n\nYou can also set a pre-compression logger to capture request/response data before compression is applied:\n\n```cpp\nsvr.set_pre_compression_logger([](const httplib::Request& req, const httplib::Response& res) {\n  // Log before compression - res.body contains uncompressed content\n  // Content-Encoding header is not yet set\n  your_pre_compression_logger(req, res);\n});\n```\n\nThe pre-compression logger is only called when compression would be applied. For responses without compression, only the access logger is called.\n\n#### Error Logging\n\nError loggers capture failed requests and connection issues. Unlike access loggers, error loggers only receive the Error and Request information, as errors typically occur before a meaningful Response can be generated.\n\n```cpp\nsvr.set_error_logger([](const httplib::Error& err, const httplib::Request* req) {\n  std::cerr << httplib::to_string(err) << \" while processing request\";\n  if (req) {\n    std::cerr << \", client: \" << req->get_header_value(\"X-Forwarded-For\")\n              << \", request: '\" << req->method << \" \" << req->path << \" \" << req->version << \"'\"\n              << \", host: \" << req->get_header_value(\"Host\");\n  }\n  std::cerr << std::endl;\n});\n```\n\n### Error handler\n\n```cpp\nsvr.set_error_handler([](const auto& req, auto& res) {\n  auto fmt = \"<p>Error Status: <span style='color:red;'>%d</span></p>\";\n  char buf[BUFSIZ];\n  snprintf(buf, sizeof(buf), fmt, res.status);\n  res.set_content(buf, \"text/html\");\n});\n```\n\n### Exception handler\nThe exception handler gets called if a user routing handler throws an error.\n\n```cpp\nsvr.set_exception_handler([](const auto& req, auto& res, std::exception_ptr ep) {\n  auto fmt = \"<h1>Error 500</h1><p>%s</p>\";\n  char buf[BUFSIZ];\n  try {\n    std::rethrow_exception(ep);\n  } catch (std::exception &e) {\n    snprintf(buf, sizeof(buf), fmt, e.what());\n  } catch (...) { // See the following NOTE\n    snprintf(buf, sizeof(buf), fmt, \"Unknown Exception\");\n  }\n  res.set_content(buf, \"text/html\");\n  res.status = StatusCode::InternalServerError_500;\n});\n```\n\n> [!CAUTION]\n> if you don't provide the `catch (...)` block for a rethrown exception pointer, an uncaught exception will end up causing the server crash. Be careful!\n\n### Pre routing handler\n\n```cpp\nsvr.set_pre_routing_handler([](const auto& req, auto& res) {\n  if (req.path == \"/hello\") {\n    res.set_content(\"world\", \"text/html\");\n    return Server::HandlerResponse::Handled;\n  }\n  return Server::HandlerResponse::Unhandled;\n});\n```\n\n### Post routing handler\n\n```cpp\nsvr.set_post_routing_handler([](const auto& req, auto& res) {\n  res.set_header(\"ADDITIONAL_HEADER\", \"value\");\n});\n```\n\n### Pre request handler\n\n```cpp\nsvr.set_pre_request_handler([](const auto& req, auto& res) {\n  if (req.matched_route == \"/user/:user\") {\n    auto user = req.path_params.at(\"user\");\n    if (user != \"john\") {\n      res.status = StatusCode::Forbidden_403;\n      res.set_content(\"error\", \"text/html\");\n      return Server::HandlerResponse::Handled;\n    }\n  }\n  return Server::HandlerResponse::Unhandled;\n});\n```\n\n### Form data handling\n\n#### URL-encoded form data ('application/x-www-form-urlencoded')\n\n```cpp\nsvr.Post(\"/form\", [&](const auto& req, auto& res) {\n  // URL query parameters and form-encoded data are accessible via req.params\n  std::string username = req.get_param_value(\"username\");\n  std::string password = req.get_param_value(\"password\");\n\n  // Handle multiple values with same name\n  auto interests = req.get_param_values(\"interests\");\n\n  // Check existence\n  if (req.has_param(\"newsletter\")) {\n    // Handle newsletter subscription\n  }\n});\n```\n\n#### 'multipart/form-data' POST data\n\n```cpp\nsvr.Post(\"/multipart\", [&](const Request& req, Response& res) {\n  // Access text fields (from form inputs without files)\n  std::string username = req.form.get_field(\"username\");\n  std::string bio = req.form.get_field(\"bio\");\n\n  // Access uploaded files\n  if (req.form.has_file(\"avatar\")) {\n    const auto& file = req.form.get_file(\"avatar\");\n    std::cout << \"Uploaded file: \" << file.filename\n              << \" (\" << file.content_type << \") - \"\n              << file.content.size() << \" bytes\" << std::endl;\n\n    // Access additional headers if needed\n    for (const auto& header : file.headers) {\n      std::cout << \"Header: \" << header.first << \" = \" << header.second << std::endl;\n    }\n\n    // Save to disk\n    std::ofstream ofs(file.filename, std::ios::binary);\n    ofs << file.content;\n  }\n\n  // Handle multiple values with same name\n  auto tags = req.form.get_fields(\"tags\");  // e.g., multiple checkboxes\n  for (const auto& tag : tags) {\n    std::cout << \"Tag: \" << tag << std::endl;\n  }\n\n  auto documents = req.form.get_files(\"documents\");  // multiple file upload\n  for (const auto& doc : documents) {\n    std::cout << \"Document: \" << doc.filename\n              << \" (\" << doc.content.size() << \" bytes)\" << std::endl;\n  }\n\n  // Check existence before accessing\n  if (req.form.has_field(\"newsletter\")) {\n    std::cout << \"Newsletter subscription: \" << req.form.get_field(\"newsletter\") << std::endl;\n  }\n\n  // Get counts for validation\n  if (req.form.get_field_count(\"tags\") > 5) {\n    res.status = StatusCode::BadRequest_400;\n    res.set_content(\"Too many tags\", \"text/plain\");\n    return;\n  }\n\n  // Summary\n  std::cout << \"Received \" << req.form.fields.size() << \" text fields and \"\n            << req.form.files.size() << \" files\" << std::endl;\n\n  res.set_content(\"Upload successful\", \"text/plain\");\n});\n```\n\n### Receive content with a content receiver\n\n```cpp\nsvr.Post(\"/content_receiver\",\n  [&](const Request &req, Response &res, const ContentReader &content_reader) {\n    if (req.is_multipart_form_data()) {\n      // NOTE: `content_reader` is blocking until every form data field is read\n      // This approach allows streaming processing of large files\n      std::vector<FormData> items;\n      content_reader(\n        [&](const FormData &item) {\n          items.push_back(item);\n          return true;\n        },\n        [&](const char *data, size_t data_length) {\n          items.back().content.append(data, data_length);\n          return true;\n        });\n\n      // Process the received items\n      for (const auto& item : items) {\n        if (item.filename.empty()) {\n          // Text field\n          std::cout << \"Field: \" << item.name << \" = \" << item.content << std::endl;\n        } else {\n          // File\n          std::cout << \"File: \" << item.name << \" (\" << item.filename << \") - \"\n                    << item.content.size() << \" bytes\" << std::endl;\n        }\n      }\n    } else {\n      std::string body;\n      content_reader([&](const char *data, size_t data_length) {\n        body.append(data, data_length);\n        return true;\n      });\n    }\n  });\n```\n\n### Send content with the content provider\n\n```cpp\nconst size_t DATA_CHUNK_SIZE = 4;\n\nsvr.Get(\"/stream\", [&](const Request &req, Response &res) {\n  auto data = new std::string(\"abcdefg\");\n\n  res.set_content_provider(\n    data->size(), // Content length\n    \"text/plain\", // Content type\n    [&, data](size_t offset, size_t length, DataSink &sink) {\n      const auto &d = *data;\n      sink.write(&d[offset], std::min(length, DATA_CHUNK_SIZE));\n      return true; // return 'false' if you want to cancel the process.\n    },\n    [data](bool success) { delete data; });\n});\n```\n\nWithout content length:\n\n```cpp\nsvr.Get(\"/stream\", [&](const Request &req, Response &res) {\n  res.set_content_provider(\n    \"text/plain\", // Content type\n    [&](size_t offset, DataSink &sink) {\n      if (/* there is still data */) {\n        std::vector<char> data;\n        // prepare data...\n        sink.write(data.data(), data.size());\n      } else {\n        sink.done(); // No more data\n      }\n      return true; // return 'false' if you want to cancel the process.\n    });\n});\n```\n\n### Chunked transfer encoding\n\n```cpp\nsvr.Get(\"/chunked\", [&](const Request& req, Response& res) {\n  res.set_chunked_content_provider(\n    \"text/plain\",\n    [](size_t offset, DataSink &sink) {\n      sink.write(\"123\", 3);\n      sink.write(\"345\", 3);\n      sink.write(\"789\", 3);\n      sink.done(); // No more data\n      return true; // return 'false' if you want to cancel the process.\n    }\n  );\n});\n```\n\nWith trailer:\n\n```cpp\nsvr.Get(\"/chunked\", [&](const Request& req, Response& res) {\n  res.set_header(\"Trailer\", \"Dummy1, Dummy2\");\n  res.set_chunked_content_provider(\n    \"text/plain\",\n    [](size_t offset, DataSink &sink) {\n      sink.write(\"123\", 3);\n      sink.write(\"345\", 3);\n      sink.write(\"789\", 3);\n      sink.done_with_trailer({\n        {\"Dummy1\", \"DummyVal1\"},\n        {\"Dummy2\", \"DummyVal2\"}\n      });\n      return true;\n    }\n  );\n});\n```\n\n### Send file content\n\n```cpp\nsvr.Get(\"/content\", [&](const Request &req, Response &res) {\n  res.set_file_content(\"./path/to/content.html\");\n});\n\nsvr.Get(\"/content\", [&](const Request &req, Response &res) {\n  res.set_file_content(\"./path/to/content\", \"text/html\");\n});\n```\n\n### 'Expect: 100-continue' handler\n\nBy default, the server sends a `100 Continue` response for an `Expect: 100-continue` header.\n\n```cpp\n// Send a '417 Expectation Failed' response.\nsvr.set_expect_100_continue_handler([](const Request &req, Response &res) {\n  return StatusCode::ExpectationFailed_417;\n});\n```\n\n```cpp\n// Send a final status without reading the message body.\nsvr.set_expect_100_continue_handler([](const Request &req, Response &res) {\n  return res.status = StatusCode::Unauthorized_401;\n});\n```\n\n### Keep-Alive connection\n\n```cpp\nsvr.set_keep_alive_max_count(2); // Default is 100\nsvr.set_keep_alive_timeout(10);  // Default is 5\n```\n\n### Timeout\n\n```c++\nsvr.set_read_timeout(5, 0); // 5 seconds\nsvr.set_write_timeout(5, 0); // 5 seconds\nsvr.set_idle_interval(0, 100000); // 100 milliseconds\n```\n\n### Set maximum payload length for reading a request body\n\n```c++\nsvr.set_payload_max_length(1024 * 1024 * 512); // 512MB\n```\n\n> [!NOTE]\n> When the request body content type is 'www-form-urlencoded', the actual payload length shouldn't exceed `CPPHTTPLIB_FORM_URL_ENCODED_PAYLOAD_MAX_LENGTH`.\n\n### Server-Sent Events\n\nPlease see [Server example](https://github.com/yhirose/cpp-httplib/blob/master/example/ssesvr.cc) and [Client example](https://github.com/yhirose/cpp-httplib/blob/master/example/ssecli.cc).\n\n### Default thread pool support\n\n`ThreadPool` is used as the **default** task queue, with a default thread count of 8 or `std::thread::hardware_concurrency() - 1`, whichever is greater. You can change it with `CPPHTTPLIB_THREAD_POOL_COUNT`.\n\nIf you want to set the thread count at runtime, there is no convenient way... But here is how.\n\n```cpp\nsvr.new_task_queue = [] { return new ThreadPool(12); };\n```\n\nYou can also provide an optional parameter to limit the maximum number\nof pending requests, i.e. requests `accept()`ed by the listener but\nstill waiting to be serviced by worker threads.\n\n```cpp\nsvr.new_task_queue = [] { return new ThreadPool(/*num_threads=*/12, /*max_queued_requests=*/18); };\n```\n\nDefault limit is 0 (unlimited). Once the limit is reached, the listener\nwill shutdown the client connection.\n\n### Override the default thread pool with yours\n\nYou can supply your own thread pool implementation according to your need.\n\n```cpp\nclass YourThreadPoolTaskQueue : public TaskQueue {\npublic:\n  YourThreadPoolTaskQueue(size_t n) {\n    pool_.start_with_thread_count(n);\n  }\n\n  virtual bool enqueue(std::function<void()> fn) override {\n    /* Return true if the task was actually enqueued, or false\n     * if the caller must drop the corresponding connection. */\n    return pool_.enqueue(fn);\n  }\n\n  virtual void shutdown() override {\n    pool_.shutdown_gracefully();\n  }\n\nprivate:\n  YourThreadPool pool_;\n};\n\nsvr.new_task_queue = [] {\n  return new YourThreadPoolTaskQueue(12);\n};\n```\n\nClient\n------\n\n```c++\n#include <httplib.h>\n#include <iostream>\n\nint main(void)\n{\n  httplib::Client cli(\"localhost\", 1234);\n\n  if (auto res = cli.Get(\"/hi\")) {\n    if (res->status == StatusCode::OK_200) {\n      std::cout << res->body << std::endl;\n    }\n  } else {\n    auto err = res.error();\n    std::cout << \"HTTP error: \" << httplib::to_string(err) << std::endl;\n  }\n}\n```\n\n> [!TIP]\n> Constructor with scheme-host-port string is now supported!\n\n```c++\nhttplib::Client cli(\"localhost\");\nhttplib::Client cli(\"localhost:8080\");\nhttplib::Client cli(\"http://localhost\");\nhttplib::Client cli(\"http://localhost:8080\");\nhttplib::Client cli(\"https://localhost\");\nhttplib::SSLClient cli(\"localhost\");\n```\n\n### Error code\n\nHere is the list of errors from `Result::error()`.\n\n```c++\nenum class Error {\n  Success = 0,\n  Unknown,\n  Connection,\n  BindIPAddress,\n  Read,\n  Write,\n  ExceedRedirectCount,\n  Canceled,\n  SSLConnection,\n  SSLLoadingCerts,\n  SSLServerVerification,\n  SSLServerHostnameVerification,\n  UnsupportedMultipartBoundaryChars,\n  Compression,\n  ConnectionTimeout,\n  ProxyConnection,\n  ConnectionClosed,\n  Timeout,\n  ResourceExhaustion,\n  TooManyFormDataFiles,\n  ExceedMaxPayloadSize,\n  ExceedUriMaxLength,\n  ExceedMaxSocketDescriptorCount,\n  InvalidRequestLine,\n  InvalidHTTPMethod,\n  InvalidHTTPVersion,\n  InvalidHeaders,\n  MultipartParsing,\n  OpenFile,\n  Listen,\n  GetSockName,\n  UnsupportedAddressFamily,\n  HTTPParsing,\n  InvalidRangeHeader,\n};\n```\n\n### Client Logging\n\n#### Access Logging\n\n```cpp\ncli.set_logger([](const httplib::Request& req, const httplib::Response& res) {\n  auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(\n    std::chrono::steady_clock::now() - start_time).count();\n  std::cout << \"âœ“ \" << req.method << \" \" << req.path\n            << \" -> \" << res.status << \" (\" << res.body.size() << \" bytes, \"\n            << duration << \"ms)\" << std::endl;\n});\n```\n\n#### Error Logging\n\n```cpp\ncli.set_error_logger([](const httplib::Error& err, const httplib::Request* req) {\n  std::cerr << \"âœ— \";\n  if (req) {\n    std::cerr << req->method << \" \" << req->path << \" \";\n  }\n  std::cerr << \"failed: \" << httplib::to_string(err);\n\n  // Add specific guidance based on error type\n  switch (err) {\n    case httplib::Error::Connection:\n      std::cerr << \" (verify server is running and reachable)\";\n      break;\n    case httplib::Error::SSLConnection:\n      std::cerr << \" (check SSL certificate and TLS configuration)\";\n      break;\n    case httplib::Error::ConnectionTimeout:\n      std::cerr << \" (increase timeout or check network latency)\";\n      break;\n    case httplib::Error::Read:\n      std::cerr << \" (server may have closed connection prematurely)\";\n      break;\n    default:\n      break;\n  }\n  std::cerr << std::endl;\n});\n```\n\n### GET with HTTP headers\n\n```c++\nhttplib::Headers headers = {\n  { \"Hello\", \"World!\" }\n};\nauto res = cli.Get(\"/hi\", headers);\n```\nor\n```c++\nauto res = cli.Get(\"/hi\", {{\"Hello\", \"World!\"}});\n```\nor\n```c++\ncli.set_default_headers({\n  { \"Hello\", \"World!\" }\n});\nauto res = cli.Get(\"/hi\");\n```\n\n### POST\n\n```c++\nres = cli.Post(\"/post\", \"text\", \"text/plain\");\nres = cli.Post(\"/person\", \"name=john1&note=coder\", \"application/x-www-form-urlencoded\");\n```\n\n### POST with parameters\n\n```c++\nhttplib::Params params;\nparams.emplace(\"name\", \"john\");\nparams.emplace(\"note\", \"coder\");\n\nauto res = cli.Post(\"/post\", params);\n```\n or\n\n```c++\nhttplib::Params params{\n  { \"name\", \"john\" },\n  { \"note\", \"coder\" }\n};\n\nauto res = cli.Post(\"/post\", params);\n```\n\n### POST with Multipart Form Data\n\n```c++\nhttplib::UploadFormDataItems items = {\n  { \"text1\", \"text default\", \"\", \"\" },\n  { \"text2\", \"aÏ‰b\", \"\", \"\" },\n  { \"file1\", \"h\\ne\\n\\nl\\nl\\no\\n\", \"hello.txt\", \"text/plain\" },\n  { \"file2\", \"{\\n  \\\"world\\\", true\\n}\\n\", \"world.json\", \"application/json\" },\n  { \"file3\", \"\", \"\", \"application/octet-stream\" },\n};\n\nauto res = cli.Post(\"/multipart\", items);\n```\n\n### PUT\n\n```c++\nres = cli.Put(\"/resource/foo\", \"text\", \"text/plain\");\n```\n\n### PATCH\n\n```c++\nres = cli.Patch(\"/resource/foo\", \"text\", \"text/plain\");\n```\n\n### DELETE\n\n```c++\nres = cli.Delete(\"/resource/foo\");\n```\n\n### OPTIONS\n\n```c++\nres = cli.Options(\"*\");\nres = cli.Options(\"/resource/foo\");\n```\n\n### Timeout\n\n```c++\ncli.set_connection_timeout(0, 300000); // 300 milliseconds\ncli.set_read_timeout(5, 0); // 5 seconds\ncli.set_write_timeout(5, 0); // 5 seconds\n\n// This method works the same as curl's `--max-time` option\ncli.set_max_timeout(5000); // 5 seconds\n```\n\n### Receive content with a content receiver\n\n```c++\nstd::string body;\n\nauto res = cli.Get(\"/large-data\",\n  [&](const char *data, size_t data_length) {\n    body.append(data, data_length);\n    return true;\n  });\n```\n\n```cpp\nstd::string body;\n\nauto res = cli.Get(\n  \"/stream\", Headers(),\n  [&](const Response &response) {\n    EXPECT_EQ(StatusCode::OK_200, response.status);\n    return true; // return 'false' if you want to cancel the request.\n  },\n  [&](const char *data, size_t data_length) {\n    body.append(data, data_length);\n    return true; // return 'false' if you want to cancel the request.\n  });\n```\n\n### Send content with a content provider\n\n```cpp\nstd::string body = ...;\n\nauto res = cli.Post(\n  \"/stream\", body.size(),\n  [](size_t offset, size_t length, DataSink &sink) {\n    sink.write(body.data() + offset, length);\n    return true; // return 'false' if you want to cancel the request.\n  },\n  \"text/plain\");\n```\n\n### Chunked transfer encoding\n\n```cpp\nauto res = cli.Post(\n  \"/stream\",\n  [](size_t offset, DataSink &sink) {\n    sink.os << \"chunked data 1\";\n    sink.os << \"chunked data 2\";\n    sink.os << \"chunked data 3\";\n    sink.done();\n    return true; // return 'false' if you want to cancel the request.\n  },\n  \"text/plain\");\n```\n\n### With Progress Callback\n\n```cpp\nhttplib::Client cli(url, port);\n\n// prints: 0 / 000 bytes => 50% complete\nauto res = cli.Get(\"/\", [](size_t len, size_t total) {\n  printf(\"%lld / %lld bytes => %d%% complete\\n\",\n    len, total,\n    (int)(len*100/total));\n  return true; // return 'false' if you want to cancel the request.\n}\n);\n```\n\n![progress](https://user-images.githubusercontent.com/236374/33138910-495c4ecc-cf86-11e7-8693-2fc6d09615c4.gif)\n\n### Authentication\n\n```cpp\n// Basic Authentication\ncli.set_basic_auth(\"user\", \"pass\");\n\n// Digest Authentication\ncli.set_digest_auth(\"user\", \"pass\");\n\n// Bearer Token Authentication\ncli.set_bearer_token_auth(\"token\");\n```\n\n> [!NOTE]\n> OpenSSL is required for Digest Authentication.\n\n### Proxy server support\n\n```cpp\ncli.set_proxy(\"host\", port);\n\n// Basic Authentication\ncli.set_proxy_basic_auth(\"user\", \"pass\");\n\n// Digest Authentication\ncli.set_proxy_digest_auth(\"user\", \"pass\");\n\n// Bearer Token Authentication\ncli.set_proxy_bearer_token_auth(\"pass\");\n```\n\n> [!NOTE]\n> OpenSSL is required for Digest Authentication.\n\n### Range\n\n```cpp\nhttplib::Client cli(\"httpcan.org\");\n\nauto res = cli.Get(\"/range/32\", {\n  httplib::make_range_header({{1, 10}}) // 'Range: bytes=1-10'\n});\n// res->status should be 206.\n// res->body should be \"bcdefghijk\".\n```\n\n```cpp\nhttplib::make_range_header({{1, 10}, {20, -1}})      // 'Range: bytes=1-10, 20-'\nhttplib::make_range_header({{100, 199}, {500, 599}}) // 'Range: bytes=100-199, 500-599'\nhttplib::make_range_header({{0, 0}, {-1, 1}})        // 'Range: bytes=0-0, -1'\n```\n\n### Keep-Alive connection\n\n```cpp\nhttplib::Client cli(\"localhost\", 1234);\n\ncli.Get(\"/hello\");         // with \"Connection: close\"\n\ncli.set_keep_alive(true);\ncli.Get(\"/world\");\n\ncli.set_keep_alive(false);\ncli.Get(\"/last-request\");  // with \"Connection: close\"\n```\n\n### Redirect\n\n```cpp\nhttplib::Client cli(\"yahoo.com\");\n\nauto res = cli.Get(\"/\");\nres->status; // 301\n\ncli.set_follow_location(true);\nres = cli.Get(\"/\");\nres->status; // 200\n```\n\n### Use a specific network interface\n\n> [!NOTE]\n> This feature is not available on Windows, yet.\n\n```cpp\ncli.set_interface(\"eth0\"); // Interface name, IP address or host name\n```\n\n### Automatic Path Encoding\n\nThe client automatically encodes special characters in URL paths by default:\n\n```cpp\nhttplib::Client cli(\"https://example.com\");\n\n// Automatic path encoding (default behavior)\ncli.set_path_encode(true);\nauto res = cli.Get(\"/path with spaces/file.txt\"); // Automatically encodes spaces\n\n// Disable automatic path encoding\ncli.set_path_encode(false);\nauto res = cli.Get(\"/already%20encoded/path\"); // Use pre-encoded paths\n```\n\n- `set_path_encode(bool on)` - Controls automatic encoding of special characters in URL paths\n  - `true` (default): Automatically encodes spaces, plus signs, newlines, and other special characters\n  - `false`: Sends paths as-is without encoding (useful for pre-encoded URLs)\n\n### Performance Note for Local Connections\n\n> [!WARNING]\n> On Windows systems with improperly configured IPv6 settings, using \"localhost\" as the hostname may cause significant connection delays (up to 2 seconds per request) due to DNS resolution issues. This affects both client and server operations. For better performance when connecting to local services, use \"127.0.0.1\" instead of \"localhost\".\n> \n> See: https://github.com/yhirose/cpp-httplib/issues/366#issuecomment-593004264\n\n```cpp\n// May be slower on Windows due to DNS resolution delays\nhttplib::Client cli(\"localhost\", 8080);\nhttplib::Server svr;\nsvr.listen(\"localhost\", 8080);\n\n// Faster alternative for local connections\nhttplib::Client cli(\"127.0.0.1\", 8080);\nhttplib::Server svr;\nsvr.listen(\"127.0.0.1\", 8080);\n```\n\nCompression\n-----------\n\nThe server can apply compression to the following MIME type contents:\n\n  * all text types except text/event-stream\n  * image/svg+xml\n  * application/javascript\n  * application/json\n  * application/xml\n  * application/protobuf\n  * application/xhtml+xml\n\n### Zlib Support\n\n'gzip' compression is available with `CPPHTTPLIB_ZLIB_SUPPORT`. `libz` should be linked.\n\n### Brotli Support\n\nBrotli compression is available with `CPPHTTPLIB_BROTLI_SUPPORT`. Necessary libraries should be linked.\nPlease see https://github.com/google/brotli for more detail.\n\n### Zstd Support\n\nZstd compression is available with `CPPHTTPLIB_ZSTD_SUPPORT`. Necessary libraries should be linked.\nPlease see https://github.com/facebook/zstd for more detail.\n\n### Default `Accept-Encoding` value\n\nThe default `Accept-Encoding` value contains all possible compression types. So, the following two examples are same.\n\n```c++\nres = cli.Get(\"/resource/foo\");\nres = cli.Get(\"/resource/foo\", {{\"Accept-Encoding\", \"br, gzip, deflate, zstd\"}});\n```\n\nIf we don't want a response without compression, we have to set `Accept-Encoding` to an empty string. This behavior is similar to curl.\n\n```c++\nres = cli.Get(\"/resource/foo\", {{\"Accept-Encoding\", \"\"}});\n```\n\n### Compress request body on client\n\n```c++\ncli.set_compress(true);\nres = cli.Post(\"/resource/foo\", \"...\", \"text/plain\");\n```\n\n### Compress response body on client\n\n```c++\ncli.set_decompress(false);\nres = cli.Get(\"/resource/foo\");\nres->body; // Compressed data\n\n```\n\nUnix Domain Socket Support\n--------------------------\n\nUnix Domain Socket support is available on Linux and macOS.\n\n```c++\n// Server\nhttplib::Server svr;\nsvr.set_address_family(AF_UNIX).listen(\"./my-socket.sock\", 80);\n\n// Client\nhttplib::Client cli(\"./my-socket.sock\");\ncli.set_address_family(AF_UNIX);\n```\n\n\"my-socket.sock\" can be a relative path or an absolute path. Your application must have the appropriate permissions for the path. You can also use an abstract socket address on Linux. To use an abstract socket address, prepend a null byte ('\\x00') to the path.\n\nThis library automatically sets the Host header to \"localhost\" for Unix socket connections, similar to curl's behavior:\n\n\nURI Encoding/Decoding Utilities\n-------------------------------\n\ncpp-httplib provides utility functions for URI encoding and decoding:\n\n```cpp\n#include <httplib.h>\n\nstd::string url = \"https://example.com/search?q=hello world\";\nstd::string encoded = httplib::encode_uri(url);\nstd::string decoded = httplib::decode_uri(encoded);\n\nstd::string param = \"hello world\";\nstd::string encoded_component = httplib::encode_uri_component(param);\nstd::string decoded_component = httplib::decode_uri_component(encoded_component);\n```\n\n### Functions\n\n- `encode_uri(const std::string &value)` - Encodes a full URI, preserving reserved characters like `://`, `?`, `&`, `=`\n- `decode_uri(const std::string &value)` - Decodes a URI-encoded string\n- `encode_uri_component(const std::string &value)` - Encodes a URI component (query parameter, path segment), encoding all reserved characters\n- `decode_uri_component(const std::string &value)` - Decodes a URI component\n\nUse `encode_uri()` for full URLs and `encode_uri_component()` for individual query parameters or path segments.\n\nStream API\n----------\n\nProcess large responses without loading everything into memory.\n\n```c++\nhttplib::Client cli(\"localhost\", 8080);\ncli.set_follow_location(true);\n...\n\nauto result = httplib::stream::Get(cli, \"/large-file\");\nif (result) {\n  while (result.next()) {\n    process(result.data(), result.size());  // Process each chunk as it arrives\n  }\n}\n\n// Or read the entire body at once\nauto result2 = httplib::stream::Get(cli, \"/file\");\nif (result2) {\n  std::string body = result2.read_all();\n}\n```\n\nAll HTTP methods are supported: `stream::Get`, `Post`, `Put`, `Patch`, `Delete`, `Head`, `Options`.\n\nSee [README-stream.md](README-stream.md) for more details.\n\nSSE Client\n----------\n\n```cpp\n#include <httplib.h>\n\nint main() {\n    httplib::Client cli(\"http://localhost:8080\");\n    httplib::sse::SSEClient sse(cli, \"/events\");\n\n    sse.on_message([](const httplib::sse::SSEMessage &msg) {\n        std::cout << \"Event: \" << msg.event << std::endl;\n        std::cout << \"Data: \" << msg.data << std::endl;\n    });\n\n    sse.start();  // Blocking, with auto-reconnect\n    return 0;\n}\n```\n\nSee [README-sse.md](README-sse.md) for more details.\n\nSplit httplib.h into .h and .cc\n-------------------------------\n\n```console\n$ ./split.py -h\nusage: split.py [-h] [-e EXTENSION] [-o OUT]\n\nThis script splits httplib.h into .h and .cc parts.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -e EXTENSION, --extension EXTENSION\n                        extension of the implementation file (default: cc)\n  -o OUT, --out OUT     where to write the files (default: out)\n\n$ ./split.py\nWrote out/httplib.h and out/httplib.cc\n```\n\nDockerfile for Static HTTP Server\n---------------------------------\n\nDockerfile for static HTTP server is available. Port number of this HTTP server is 80, and it serves static files from `/html` directory in the container.\n\n```bash\n> docker build -t cpp-httplib-server .\n...\n\n> docker run --rm -it -p 8080:80 -v ./docker/html:/html cpp-httplib-server\nServing HTTP on 0.0.0.0 port 80 ...\n192.168.65.1 - - [31/Aug/2024:21:33:56 +0000] \"GET / HTTP/1.1\" 200 599 \"-\" \"curl/8.7.1\"\n192.168.65.1 - - [31/Aug/2024:21:34:26 +0000] \"GET / HTTP/1.1\" 200 599 \"-\" \"Mozilla/5.0 ...\"\n192.168.65.1 - - [31/Aug/2024:21:34:26 +0000] \"GET /favicon.ico HTTP/1.1\" 404 152 \"-\" \"Mozilla/5.0 ...\"\n```\n\nFrom Docker Hub\n\n```bash\n> docker run --rm -it -p 8080:80 -v ./docker/html:/html yhirose4dockerhub/cpp-httplib-server\nServing HTTP on 0.0.0.0 port 80 ...\n192.168.65.1 - - [31/Aug/2024:21:33:56 +0000] \"GET / HTTP/1.1\" 200 599 \"-\" \"curl/8.7.1\"\n192.168.65.1 - - [31/Aug/2024:21:34:26 +0000] \"GET / HTTP/1.1\" 200 599 \"-\" \"Mozilla/5.0 ...\"\n192.168.65.1 - - [31/Aug/2024:21:34:26 +0000] \"GET /favicon.ico HTTP/1.1\" 404 152 \"-\" \"Mozilla/5.0 ...\"\n```\n\nNOTE\n----\n\n### Regular Expression Stack Overflow\n\n> [!CAUTION]\n> When using complex regex patterns in route handlers, be aware that certain patterns may cause stack overflow during pattern matching. This is a known issue with `std::regex` implementations and affects the `dispatch_request()` method.\n> \n> ```cpp\n> // This pattern can cause stack overflow with large input\n> svr.Get(\".*\", handler);\n> ```\n> \n> Consider using simpler patterns or path parameters to avoid this issue:\n> \n> ```cpp\n> // Safer alternatives\n> svr.Get(\"/users/:id\", handler);           // Path parameters\n> svr.Get(R\"(/api/v\\d+/.*)\", handler);     // More specific patterns\n> ```\n\n### g++\n\ng++ 4.8 and below cannot build this library since `<regex>` in the versions are [broken](https://stackoverflow.com/questions/12530406/is-gcc-4-8-or-earlier-buggy-about-regular-expressions).\n\n### Windows\n\nInclude `httplib.h` before `Windows.h` or include `Windows.h` by defining `WIN32_LEAN_AND_MEAN` beforehand.\n\n```cpp\n#include <httplib.h>\n#include <Windows.h>\n```\n\n```cpp\n#define WIN32_LEAN_AND_MEAN\n#include <Windows.h>\n#include <httplib.h>\n```\n\n> [!NOTE]\n> cpp-httplib officially supports only the latest Visual Studio. It might work with former versions of Visual Studio, but I can no longer verify it. Pull requests are always welcome for the older versions of Visual Studio unless they break the C++11 conformance.\n\n> [!NOTE]\n> Windows 8 or lower, Visual Studio 2015 or lower, and Cygwin and MSYS2 including MinGW are neither supported nor tested.\n\nLicense\n-------\n\nMIT license (Â© 2026 Yuji Hirose)\n\nSpecial Thanks To\n-----------------\n\n[These folks](https://github.com/yhirose/cpp-httplib/graphs/contributors) made great contributions to polish this library to totally another level from a simple toy!\n",
      "stars_today": 6
    },
    {
      "id": 53256950,
      "name": "eruda",
      "full_name": "liriliri/eruda",
      "description": "Console for mobile browsers",
      "html_url": "https://github.com/liriliri/eruda",
      "stars": 20643,
      "forks": 1338,
      "language": "JavaScript",
      "topics": [
        "console",
        "debugger",
        "developer-tools",
        "eruda",
        "mobile"
      ],
      "created_at": "2016-03-06T13:45:55Z",
      "updated_at": "2026-01-27T23:52:08Z",
      "pushed_at": "2025-08-01T09:34:30Z",
      "open_issues": 63,
      "owner": {
        "login": "liriliri",
        "avatar_url": "https://avatars.githubusercontent.com/u/13956058?v=4"
      },
      "readme": "<div align=\"center\">\n  <a href=\"https://eruda.liriliri.io/\" target=\"_blank\">\n    <img src=\"https://eruda.liriliri.io/icon.png\" width=\"400\">\n  </a>\n</div>\n\n<h1 align=\"center\">Eruda</h1>\n\n<div align=\"center\">\n\nConsole for Mobile Browsers.\n\n[![NPM version][npm-image]][npm-url]\n[![Build status][ci-image]][ci-url]\n[![Test coverage][codecov-image]][codecov-url]\n[![Downloads][jsdelivr-image]][jsdelivr-url]\n[![License][license-image]][npm-url]\n\n</div>\n\n[npm-image]: https://img.shields.io/npm/v/eruda?style=flat-square\n[npm-url]: https://npmjs.org/package/eruda\n[jsdelivr-image]: https://img.shields.io/jsdelivr/npm/hm/eruda?style=flat-square\n[jsdelivr-url]: https://www.jsdelivr.com/package/npm/eruda\n[ci-image]: https://img.shields.io/github/actions/workflow/status/liriliri/eruda/main.yml?branch=master&style=flat-square\n[ci-url]: https://github.com/liriliri/eruda/actions/workflows/main.yml \n[codecov-image]: https://img.shields.io/codecov/c/github/liriliri/eruda?style=flat-square\n[codecov-url]: https://codecov.io/github/liriliri/eruda?branch=master\n[license-image]: https://img.shields.io/npm/l/eruda?style=flat-square\n[donate-image]: https://img.shields.io/badge/$-donate-0070ba.svg?style=flat-square\n\n<img src=\"https://eruda.liriliri.io/screenshot.jpg\" style=\"width:100%\">\n\n## Demo\n\n![Demo](https://eruda.liriliri.io/qrcode.png)\n\nBrowse it on your phone: [eruda.liriliri.io](https://eruda.liriliri.io/)\n\n## Install\n\nYou can get it on npm.\n\n```bash\nnpm install eruda --save-dev\n```\n\nAdd this script to your page.\n\n```html\n<script src=\"node_modules/eruda/eruda.js\"></script>\n<script>eruda.init();</script>\n```\n\nIt's also available on [jsDelivr](http://www.jsdelivr.com/projects/eruda) and [cdnjs](https://cdnjs.com/libraries/eruda).\n\n```html\n<script src=\"https://cdn.jsdelivr.net/npm/eruda\"></script>\n<script>eruda.init();</script>\n```\n\nFor more detailed usage instructions, please read the documentation at [eruda.liriliri.io](https://eruda.liriliri.io/docs/)!\n\n## Related Projects\n\n* [eruda-android](https://github.com/liriliri/eruda-android): Simple webview with eruda loaded automatically.\n* [chii](https://github.com/liriliri/chii): Remote debugging tool.\n* [chobitsu](https://github.com/liriliri/chobitsu): Chrome devtools protocol JavaScript implementation.\n* [licia](https://github.com/liriliri/licia): Utility library used by eruda.\n* [luna](https://github.com/liriliri/luna): UI components used by eruda.\n* [vivy](https://github.com/liriliri/vivy-docs): Icon image generation.\n\n## Third Party\n\n* [eruda-pixel](https://github.com/Faithree/eruda-pixel): UI pixel restoration tool.\n* [eruda-webpack-plugin](https://github.com/huruji/eruda-webpack-plugin): Eruda webpack plugin.\n* [eruda-vue-devtools](https://github.com/Zippowxk/vue-devtools-plugin): Eruda Vue-devtools plugin.\n\n## Backers\n\n<a rel=\"noreferrer noopener\" href=\"https://opencollective.com/eruda\" target=\"_blank\"><img src=\"https://opencollective.com/eruda/backers.svg?width=890\"></a>\n\n## Contribution\n\nRead [Contributing Guide](https://eruda.liriliri.io/docs/contributing.html) for development setup instructions.\n",
      "stars_today": 6
    },
    {
      "id": 12499251,
      "name": "checkstyle",
      "full_name": "checkstyle/checkstyle",
      "description": "Checkstyle is a development tool to help programmers write Java code that adheres to a coding standard. By default it supports the Google Java Style Guide and Sun Code Conventions, but is highly configurable. It can be invoked with an ANT task and a command line program.",
      "html_url": "https://github.com/checkstyle/checkstyle",
      "stars": 8836,
      "forks": 3972,
      "language": "Java",
      "topics": [
        "code-quality",
        "command-line-tool",
        "hacktoberfest",
        "java",
        "static-analysis",
        "static-code-analysis"
      ],
      "created_at": "2013-08-31T02:05:05Z",
      "updated_at": "2026-01-27T23:03:59Z",
      "pushed_at": "2026-01-27T23:16:57Z",
      "open_issues": 936,
      "owner": {
        "login": "checkstyle",
        "avatar_url": "https://avatars.githubusercontent.com/u/5179750?v=4"
      },
      "readme": "# Checkstyle - Java Code Quality Tool\n\n![](https://raw.githubusercontent.com/checkstyle/resources/master/img/checkstyle-logos/checkstyle-logo-260x99.png)\n\n--------------------------\n\n*Checkstyle is a tool that ensures adherence to a code standard or a set of best practices.*\n\n[![][appveyor img]][appveyor]\n[![][circleci img]][circleci]\n[![][cirrusci img]][cirrusci]\n[![][coverage img]][coverage]\n[![][snyk img]][snyk]\n[![][semaphoreci img]][semaphoreci]\n[![][azure img]][azure]\n[![][error prone img]][error prone]\n[![][pitest img]][pitest]\n[![][checker framework img]][checker framework]\n[![][dependabot img]][dependabot]\n[![][release notes/version img]][release notes/version]\n[![][closed issues img]][closed issues]\n[![][link check img]][link check]\n[![][milestone img]][milestone]\n\n[![][mavenbadge img]][mavenbadge]\n\nThe latest release version can be found at\n[GitHub releases](https://github.com/checkstyle/checkstyle/releases/)\nor at [Maven repo](https://repo1.maven.org/maven2/com/puppycrawl/tools/checkstyle/).\n\nDocumentation is available in HTML format, see https://checkstyle.org/checks.html .\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n- [Contributing](#contributing)\n- [Feedback and Support](#feedback-and-support)\n- [Javadoc](#javadoc)\n- [Sponsor Checkstyle](#sponsor-checkstyle)\n- [Licensing](#licensing)\n\n## Quick Start\n\n- Download our [Latest Release](https://github.com/checkstyle/checkstyle/releases/) from GitHub\n  or Add Checkstyle to your build from [Maven Central](https://mvnrepository.com/artifact/com.puppycrawl.tools/checkstyle).\n- Read our Documentation for [usage](https://checkstyle.org/cmdline.html)\n  and [configuration](https://checkstyle.org/config.html).\n\n```bash\n$ cat config.xml\n<?xml version=\"1.0\"?>\n<!DOCTYPE module PUBLIC\n          \"-//Puppy Crawl//DTD Check Configuration 1.3//EN\"\n          \"https://checkstyle.org/dtds/configuration_1_3.dtd\">\n<module name=\"Checker\">\n  <module name=\"TreeWalker\">\n    <module name=\"FallThrough\"/>\n  </module>\n</module>\n\n$ cat Test.java\nclass Test {\n  public void foo() {\n    int i = 0;\n    while (i >= 0) {\n      switch (i) {\n        case 1:\n        case 2:\n          i++;\n        case 3: // violation 'fall from previous branch of the switch'\n          i++;\n      }\n    }\n  }\n}\n\n$ java -jar checkstyle-10.18.1-all.jar -c config.xml Test.java\nStarting audit...\n[ERROR] Test.java:9:9: Fall through from previous branch of switch statement [FallThrough]\nAudit done.\nCheckstyle ends with 1 errors.\n```\n\n## Contributing\n\nThanks for your interest in contributing to CheckStyle! Please see the\n[Contribution Guidelines](https://github.com/checkstyle/checkstyle/blob/master/.github/CONTRIBUTING.md)\nfor information on how to contribute to the project. This includes creating issues, submitting pull\nrequests, and setting up your development environment.\n\n## Build Instructions\n\nPlease see the [CheckStyle Documentation](https://checkstyle.org/contributing.html#Build) for\ninformation on how to build the project.\n\n## Feedback and Support\n\n- Visit our [Discussions Page](https://github.com/checkstyle/checkstyle/discussions), where you\n  can ask questions and discuss the project with other users and contributors. This is our\n  preferred method of communication for topics\n  like usage and configuration questions, debugging, and other feedback.\n- [Stack Overflow](https://stackoverflow.com/questions/tagged/checkstyle) is another place to\n  ask questions about Checkstyle usage.\n- If you are interested in contributing to the project, you can join our\n  [Discord Contributors Chat](https://discord.com/channels/845645228467159061/1216455699488313554)\n  [with invite link](https://discord.gg/FsUsYC2ura).\n- Our [Google Groups Forum](https://groups.google.com/forum/?hl=en#!forum/checkstyle) is a\n  mailing list for discussion and support; however, we may be slow to respond there.\n\n## Javadoc\n\nTake a look at our [javadoc](https://checkstyle.org/apidocs/index.html) to see\nour API documentation.\n\n## Sponsor Checkstyle\n\nCheckstyle is an open-source project that is developed and maintained by volunteers. If you\nfind Checkstyle useful, please consider sponsoring the project. Your support helps us to\nmaintain and improve Checkstyle.\n\n- [Liberapay](https://liberapay.com/checkstyle/)\n- [OpenCollective](https://opencollective.com/checkstyle/)\n\n[![][backers.opencollective img]][backers.opencollective]\n\n[![][sponsors.opencollective img]][sponsors.opencollective]\n\n## Licensing\n\nCheckstyle is licensed under the [GNU LGPL v2.1 License](LICENSE).\nCheckstyle uses libraries:\n\n- [ANTLR](https://www.antlr.org/)\n- [Apache Commons](https://commons.apache.org/)\n- [Google Guava](https://github.com/google/guava/)\n- [Picocli](https://github.com/remkop/picocli/)\n\n## Development Tools Powered by\n\n[![JetBrains logo.][jetbrains img]][jetbrains]\n\n[![JProfiler logo.][jprofiler img]][jprofiler]\n\n[jetbrains]:https://jb.gg/OpenSource\n[jetbrains img]:https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.svg\n\n[jprofiler]:https://www.ej-technologies.com/jprofiler\n[jprofiler img]:https://www.ej-technologies.com/images/product_banners/jprofiler_medium.png\n\n[appveyor]:https://ci.appveyor.com/project/checkstyle/checkstyle/history\n[appveyor img]:https://ci.appveyor.com/api/projects/status/rw6bw3dl9kph6ucc?svg=true\n\n[coverage]:https://codecov.io/github/checkstyle/checkstyle?branch=master\n[coverage img]:https://codecov.io/github/checkstyle/checkstyle/coverage.svg?branch=master\n\n[mavenbadge]:https://search.maven.org/search?q=g:%22com.puppycrawl.tools%22%20AND%20a:%22checkstyle%22\n[mavenbadge img]:https://img.shields.io/maven-central/v/com.puppycrawl.tools/checkstyle.svg?label=Maven%20Central\n\n[stackoverflow]:https://stackoverflow.com/questions/tagged/checkstyle\n[stackoverflow img]:https://img.shields.io/badge/stackoverflow-CHECKSTYLE-blue.svg\n\n[teamcity]:https://teamcity.jetbrains.com/viewType.html?buildTypeId=Checkstyle_IdeaInspectionsMaster\n[teamcity img]:https://teamcity.jetbrains.com/app/rest/builds/buildType:(id:Checkstyle_IdeaInspectionsMaster)/statusIcon\n\n[circleci]: https://circleci.com/gh/checkstyle/checkstyle/tree/master\n[circleci img]: https://circleci.com/gh/checkstyle/checkstyle/tree/master.svg?style=svg\n\n[cirrusci]: https://cirrus-ci.com/github/checkstyle/checkstyle\n[cirrusci img]: https://api.cirrus-ci.com/github/checkstyle/checkstyle.svg?branch=master\n\n[snyk]: https://snyk.io/test/github/checkstyle/checkstyle?targetFile=pom.xml\n[snyk img]: https://snyk.io/test/github/checkstyle/checkstyle/badge.svg\n\n[semaphoreci]: https://checkstyle.semaphoreci.com/projects/checkstyle\n[semaphoreci img]: https://checkstyle.semaphoreci.com/badges/checkstyle/branches/master.svg?style=shields\n\n[azure]:https://dev.azure.com/romanivanovjr/romanivanovjr/_build/latest?definitionId=1&branchName=master\n[azure img]:https://dev.azure.com/romanivanovjr/romanivanovjr/_apis/build/status/checkstyle.checkstyle?branchName=master\n\n[backers.opencollective]:https://opencollective.com/checkstyle/\n[backers.opencollective img]:https://opencollective.com/checkstyle/backers/badge.svg\n\n[sponsors.opencollective]:https://opencollective.com/checkstyle/\n[sponsors.opencollective img]:https://opencollective.com/checkstyle/sponsors/badge.svg\n\n[dependabot]:https://github.com/dependabot\n[dependabot img]:https://img.shields.io/badge/dependabot-025E8C?style=for-the-badge&logo=dependabot\n\n[closed issues]:https://github.com/checkstyle/checkstyle/actions/workflows/no-old-refs.yml\n[closed issues img]:https://github.com/checkstyle/checkstyle/actions/workflows/no-old-refs.yml/badge.svg\n\n[release notes/version]:https://github.com/checkstyle/checkstyle/actions/workflows/releasenotes-gen.yml\n[release notes/version img]:https://github.com/checkstyle/checkstyle/actions/workflows/releasenotes-gen.yml/badge.svg\n\n[link check]:https://github.com/checkstyle/checkstyle/actions/workflows/run-link-check.yml\n[link check img]:https://github.com/checkstyle/checkstyle/actions/workflows/run-link-check.yml/badge.svg\n\n[error prone]:https://github.com/checkstyle/checkstyle/actions/workflows/error-prone.yml\n[error prone img]:https://github.com/checkstyle/checkstyle/actions/workflows/error-prone.yml/badge.svg\n\n[pitest]:https://github.com/checkstyle/checkstyle/actions/workflows/pitest.yml\n[pitest img]:https://github.com/checkstyle/checkstyle/actions/workflows/pitest.yml/badge.svg\n\n[checker framework]:https://github.com/checkstyle/checkstyle/actions/workflows/checker-framework.yml\n[checker framework img]:https://github.com/checkstyle/checkstyle/actions/workflows/checker-framework.yml/badge.svg\n\n[milestone]:https://github.com/checkstyle/checkstyle/actions/workflows/set-milestone-on-referenced-issue.yml\n[milestone img]:https://github.com/checkstyle/checkstyle/actions/workflows/set-milestone-on-referenced-issue.yml/badge.svg\n",
      "stars_today": 6
    },
    {
      "id": 190845267,
      "name": "sm64",
      "full_name": "n64decomp/sm64",
      "description": "A Super Mario 64 decompilation, brought to you by a bunch of clever folks.",
      "html_url": "https://github.com/n64decomp/sm64",
      "stars": 8456,
      "forks": 1525,
      "language": "C",
      "topics": [],
      "created_at": "2019-06-08T04:30:49Z",
      "updated_at": "2026-01-27T13:10:56Z",
      "pushed_at": "2024-02-04T10:26:16Z",
      "open_issues": 29,
      "owner": {
        "login": "n64decomp",
        "avatar_url": "https://avatars.githubusercontent.com/u/47247020?v=4"
      },
      "readme": "# Super Mario 64\n\n- This repo contains a full decompilation of Super Mario 64 of the following releases: Japan (jp), North America (us), Europe (eu), Shindou (sh) and iQue Player (cn).\n- Naming and documentation of the source code and data structures are in progress.\n\nIt builds the following ROMs:\n\n* sm64.jp.z64 `sha1: 8a20a5c83d6ceb0f0506cfc9fa20d8f438cafe51`\n* sm64.us.z64 `sha1: 9bef1128717f958171a4afac3ed78ee2bb4e86ce`\n* sm64.eu.z64 `sha1: 4ac5721683d0e0b6bbb561b58a71740845dceea9`\n* sm64.sh.z64 `sha1: 3f319ae697533a255a1003d09202379d78d5a2e0`\n* sm64.cn.z64 `sha1: 2e1db2780985a1f068077dc0444b685f39cd90ec`\n\nThis repo does not include all assets necessary for compiling the ROMs.\nA prior copy of the game is required to extract the assets.\n\n## Quick Start (for Ubuntu)\n\n1. Install prerequisites: `sudo apt install -y binutils-mips-linux-gnu build-essential git pkgconf python3`\n2. Clone the repo from within Linux: `git clone https://github.com/n64decomp/sm64.git`\n3. Place a Super Mario 64 ROM called `baserom.<VERSION>.z64` into the project folder for asset extraction, where `VERSION` can be `jp`, `us`, `eu`, `sh`, or `cn`.\n4. Run `make` to build. Specify the version through `make VERSION=<VERSION>`. Add `-j4` to improve build speed (hardware dependent).\n\nEnsure the repo path length does not exceed 255 characters. Long path names result in build errors.\n\n## Installation\n\n### Windows\n\nInstall WSL and a distro of your choice following\n[Windows Subsystem for Linux Installation Guide for Windows 10.](https://docs.microsoft.com/en-us/windows/wsl/install-win10)\nWe recommend either Debian or Ubuntu 18.04 Linux distributions under WSL.\nNote: WSL1 does not currently support Ubuntu 20.04.\n\nNext, clone the SM64 repo from within the Linux shell:\n`git clone https://github.com/n64decomp/sm64.git`\n\nThen continue following the directions in the [Linux](#linux) installation section below.\n\n### Linux\n\nThere are 3 steps to set up a working build.\n\n#### Step 1: Install dependencies\n\nThe build system has the following package requirements:\n * binutils-mips\n * pkgconf\n * python3 >= 3.6\n\nDependency installation instructions for common Linux distros are provided below:\n\n##### Debian / Ubuntu\nTo install build dependencies:\n```\nsudo apt install -y binutils-mips-linux-gnu build-essential git pkgconf python3\n```\n\n##### Arch Linux\nTo install build dependencies:\n```\nsudo pacman -S base-devel python\n```\nInstall the following AUR packages:\n* [mips64-elf-binutils](https://aur.archlinux.org/packages/mips64-elf-binutils) (AUR)\n\n##### Other Linux distributions\n\nMost modern Linux distributions should have equivalent packages to the other two listed above.\nYou may have to use a different version of GNU binutils. Listed below are fully compatible binutils\ndistributions with support in the makefile, and examples of distros that offer them:\n\n* `mips64-elf-` (Arch AUR)\n* `mips-linux-gnu-` (Ubuntu and other Debian-based distros)\n* `mips64-linux-gnu-` (RHEL/CentOS/Fedora)\n\nYou may also use [Docker](#docker-installation) to handle installing an image with minimal dependencies.\n\n#### Step 2: Copy baserom(s) for asset extraction\n\nFor each version (jp/us/eu/sh/cn) for which you want to build a ROM, put an existing ROM at\n`./baserom.<VERSION>.z64` for asset extraction.\n\n##### Step 3: Build the ROM\n\nRun `make` to build the ROM (defaults to `VERSION=us`).\nOther examples:\n```\nmake VERSION=jp -j4       # build (J) version instead with 4 jobs\nmake VERSION=eu COMPARE=0 # build (EU) version but do not compare ROM hashes\n```\n\nResulting artifacts can be found in the `build` directory.\n\nThe full list of configurable variables are listed below, with the default being the first listed:\n\n* ``VERSION``: ``jp``, ``us``, ``eu``, ``sh``, ``cn``\n* ``GRUCODE``: ``f3d_old``, ``f3d_new``, ``f3dex``, ``f3dex2``, ``f3dzex``\n* ``COMPARE``: ``1`` (compare ROM hash), ``0`` (do not compare ROM hash)\n* ``NON_MATCHING``: Use functionally equivalent C implementations for non-matchings. Also will avoid instances of undefined behavior.\n* ``CROSS``: Cross-compiler tool prefix (Example: ``mips64-elf-``).\n\n### macOS\n\nWith macOS, you may either use Homebrew or [Docker](#docker-installation).\n\n#### Homebrew\n\n#### Step 1: Install dependencies\nInstall [Homebrew](https://brew.sh) and the following dependencies:\n```\nbrew update\nbrew install coreutils make pkg-config tehzz/n64-dev/mips64-elf-binutils\n```\n\n#### Step 2: Copy baserom(s) for asset extraction\n\nFor each version (jp/us/eu/sh/cn) for which you want to build a ROM, put an existing ROM at\n`./baserom.<VERSION>.z64` for asset extraction.\n\n##### Step 3: Build the ROM\n\nUse Homebrew's GNU make because the version included with macOS is too old.\n\n```\ngmake VERSION=jp -j4       # build (J) version instead with 4 jobs\n```\n\n### Docker Installation\n\n#### Create Docker image\n\nAfter installing and starting Docker, create the docker image. This only needs to be done once.\n```\ndocker build -t sm64 .\n```\n\n#### Build\n\nTo build, mount the local filesystem into the Docker container and build the ROM with `docker run sm64 make`.\n\n##### macOS example for (U):\n```\ndocker run --rm --mount type=bind,source=\"$(pwd)\",destination=/sm64 sm64 make VERSION=us -j4\n```\n\n##### Linux example for (U):\nFor a Linux host, Docker needs to be instructed which user should own the output files:\n```\ndocker run --rm --mount type=bind,source=\"$(pwd)\",destination=/sm64 --user $UID:$GID sm64 make VERSION=us -j4\n```\n\nResulting artifacts can be found in the `build` directory.\n\n## Project Structure\n\n\tsm64\n\tâ”œâ”€â”€ actors: object behaviors, geo layout, and display lists\n\tâ”œâ”€â”€ asm: handwritten assembly code, rom header\n\tâ”‚   â””â”€â”€ non_matchings: asm for non-matching sections\n\tâ”œâ”€â”€ assets: animation and demo data\n\tâ”‚   â”œâ”€â”€ anims: animation data\n\tâ”‚   â””â”€â”€ demos: demo data\n\tâ”œâ”€â”€ bin: C files for ordering display lists and textures\n\tâ”œâ”€â”€ build: output directory\n\tâ”œâ”€â”€ data: behavior scripts, misc. data\n\tâ”œâ”€â”€ doxygen: documentation infrastructure\n\tâ”œâ”€â”€ enhancements: example source modifications\n\tâ”œâ”€â”€ include: header files\n\tâ”œâ”€â”€ levels: level scripts, geo layout, and display lists\n\tâ”œâ”€â”€ lib: SDK library code\n\tâ”œâ”€â”€ rsp: audio and Fast3D RSP assembly code\n\tâ”œâ”€â”€ sound: sequences, sound samples, and sound banks\n\tâ”œâ”€â”€ src: C source code for game\n\tâ”‚   â”œâ”€â”€ audio: audio code\n\tâ”‚   â”œâ”€â”€ buffers: stacks, heaps, and task buffers\n\tâ”‚   â”œâ”€â”€ engine: script processing engines and utils\n\tâ”‚   â”œâ”€â”€ game: behaviors and rest of game source\n\tâ”‚   â”œâ”€â”€ goddard: Mario intro screen\n\tâ”‚   â””â”€â”€ menu: title screen and file, act, and debug level selection menus\n\tâ”œâ”€â”€ text: dialog, level names, act names\n\tâ”œâ”€â”€ textures: skybox and generic texture data\n\tâ””â”€â”€ tools: build tools\n\n## Contributing\n\nPull requests are welcome. For major changes, please open an issue first to\ndiscuss what you would like to change.\n\nRun `clang-format` on your code to ensure it meets the project's coding standards.\n\nOfficial Discord: [discord.gg/DuYH3Fh](https://discord.gg/DuYH3Fh)\n",
      "stars_today": 6
    },
    {
      "id": 23097173,
      "name": "opus",
      "full_name": "xiph/opus",
      "description": "Modern audio compression for the internet.",
      "html_url": "https://github.com/xiph/opus",
      "stars": 2967,
      "forks": 739,
      "language": "C",
      "topics": [
        "audio",
        "c",
        "codec",
        "compression"
      ],
      "created_at": "2014-08-19T04:45:15Z",
      "updated_at": "2026-01-28T01:27:23Z",
      "pushed_at": "2026-01-21T17:49:20Z",
      "open_issues": 158,
      "owner": {
        "login": "xiph",
        "avatar_url": "https://avatars.githubusercontent.com/u/8365509?v=4"
      },
      "readme": "== Opus audio codec ==\n\nOpus is a codec for interactive speech and audio transmission over the Internet.\n\n  Opus can handle a wide range of interactive audio applications, including\nVoice over IP, videoconferencing, in-game  chat, and even remote live music\nperformances. It can scale from low bit-rate narrowband speech to very high\nquality stereo music.\n\n  Opus, when coupled with an appropriate container format, is also suitable\nfor non-realtime  stored-file applications such as music distribution, game\nsoundtracks, portable music players, jukeboxes, and other applications that\nhave historically used high latency formats such as MP3, AAC, or Vorbis.\n\n                    Opus is specified by IETF RFC 6716:\n                    https://tools.ietf.org/html/rfc6716\n\n  The Opus format and this implementation of it are subject to the royalty-\nfree patent and copyright licenses specified in the file COPYING.\n\nThis package implements a shared library for encoding and decoding raw Opus\nbitstreams. Raw Opus bitstreams should be used over RTP according to\n https://tools.ietf.org/html/rfc7587\n\nThe package also includes a number of test tools used for testing the\ncorrect operation of the library. The bitstreams read/written by these\ntools should not be used for Opus file distribution: They include\nadditional debugging data and cannot support seeking.\n\nOpus stored in files should use the Ogg encapsulation for Opus which is\ndescribed at:\n https://tools.ietf.org/html/rfc7845\n\nAn opus-tools package is available which provides encoding and decoding of\nOgg encapsulated Opus files and includes a number of useful features.\n\nOpus-tools can be found at:\n https://gitlab.xiph.org/xiph/opus-tools.git\nor on the main Opus website:\n https://opus-codec.org/\n\n== Deep Learning and Opus ==\n\nLossy networks continue to be a challenge for real-time communications.\nWhile the original implementation of Opus provides an excellent packet loss\nconcealment mechanism, the team has continued to advance the methodology used\nto improve audio quality in challenge network environments.\n\nIn Opus 1.5, we added a deep learning based redundancy encoder that enhances\naudio in lossy networks by embedding one second of recovery data in the padding\ndata of each packet. The underlying algorithm behind encoding and decoding the\nrecovery data is called the deep redundancy (DRED) algorithm. By leveraging\nthe padding data within the packet, Opus 1.5 is fully backward compatible with\nprior revisions of Opus. Please see the README under the \"dnn\" subdirectory to\nunderstand DRED.\n\nDRED was developed by a team that Amazon Web Services initially sponsored,\nwho open-sourced the implementation as well as began the\nstandardization process at the IETF:\n  https://datatracker.ietf.org/doc/draft-ietf-mlcodec-opus-extension/\nThe license behind Opus or the intellectual property position of Opus does\nnot change with Opus 1.5.\n\n== Compiling libopus ==\n\nTo build from a distribution tarball, you only need to do the following:\n\n    % ./configure\n    % make\n\nTo build from the git repository, the following steps are necessary:\n\n0) Set up a development environment:\n\nOn an Ubuntu or Debian family Linux distribution:\n\n    % sudo apt-get install git autoconf automake libtool gcc make\n\nOn a Fedora/Redhat based Linux:\n\n    % sudo dnf install git autoconf automake libtool gcc make\n\nOr for older Redhat/Centos Linux releases:\n\n    % sudo yum install git autoconf automake libtool gcc make\n\nOn Apple macOS, install Xcode and brew.sh, then in the Terminal enter:\n\n    % brew install autoconf automake libtool\n\n1) Clone the repository:\n\n    % git clone https://gitlab.xiph.org/xiph/opus.git\n    % cd opus\n\n2) Compiling the source\n\n    % ./autogen.sh\n    % ./configure\n    % make\n\nOn x86, it's a good idea to use a -march= option that allows the use of AVX2.\n\n3) Install the codec libraries (optional)\n\n    % sudo make install\n\nOnce you have compiled the codec, there will be a opus_demo executable\nin the top directory.\n\nUsage: opus_demo [-e] <application> <sampling rate (Hz)> <channels (1/2)>\n         <bits per second> [options] <input> <output>\n       opus_demo -d <sampling rate (Hz)> <channels (1/2)> [options]\n         <input> <output>\n\nmode: voip | audio | restricted-lowdelay\noptions:\n  -e                : only runs the encoder (output the bit-stream)\n  -d                : only runs the decoder (reads the bit-stream as input)\n  -cbr              : enable constant bitrate; default: variable bitrate\n  -cvbr             : enable constrained variable bitrate; default:\n                      unconstrained\n  -bandwidth <NB|MB|WB|SWB|FB>\n                    : audio bandwidth (from narrowband to fullband);\n                      default: sampling rate\n  -framesize <2.5|5|10|20|40|60>\n                    : frame size in ms; default: 20\n  -max_payload <bytes>\n                    : maximum payload size in bytes, default: 1024\n  -complexity <comp>\n                    : complexity, 0 (lowest) ... 10 (highest); default: 10\n  -inbandfec        : enable SILK inband FEC\n  -forcemono        : force mono encoding, even for stereo input\n  -dtx              : enable SILK DTX\n  -loss <perc>      : simulate packet loss, in percent (0-100); default: 0\n\ninput and output are little-endian signed 16-bit PCM files or opus\nbitstreams with simple opus_demo proprietary framing.\n\n== Testing ==\n\nThis package includes a collection of automated unit and system tests\nwhich SHOULD be run after compiling the package especially the first\ntime it is run on a new platform.\n\nTo run the integrated tests:\n\n    % make check\n\nThere is also collection of standard test vectors which are not\nincluded in this package for size reasons but can be obtained from:\nhttps://opus-codec.org/docs/opus_testvectors-rfc8251.tar.gz\n\nTo run compare the code to these test vectors:\n\n    % curl -OL https://opus-codec.org/docs/opus_testvectors-rfc8251.tar.gz\n    % tar -zxf opus_testvectors-rfc8251.tar.gz\n    % ./tests/run_vectors.sh ./ opus_newvectors 48000\n\n== Compiling libopus for Windows and alternative build systems ==\n\nSee cmake/README.md or meson/README.md.\n\n== Portability notes ==\n\nThis implementation uses floating-point by default but can be compiled to\nuse only fixed-point arithmetic by setting --enable-fixed-point (if using\nautoconf) or by defining the FIXED_POINT macro (if building manually).\nThe fixed point implementation has somewhat lower audio quality and is\nslower on platforms with fast FPUs, it is normally only used in embedded\nenvironments.\n\nThe implementation can be compiled with either a C89 or a C99 compiler.\nWhile it does not rely on any _undefined behavior_ as defined by C89 or\nC99, it relies on common _implementation-defined behavior_ for two's\ncomplement architectures:\n\no Right shifts of negative values are consistent with two's\n  complement arithmetic, so that a>>b is equivalent to\n  floor(a/(2^b)),\n\no For conversion to a signed integer of N bits, the value is reduced\n  modulo 2^N to be within range of the type,\n\no The result of integer division of a negative value is truncated\n  towards zero, and\n\no The compiler provides a 64-bit integer type (a C99 requirement\n  which is supported by most C89 compilers).\n",
      "stars_today": 6
    },
    {
      "id": 902083208,
      "name": "touchkio",
      "full_name": "leukipp/touchkio",
      "description": "Home Assistant Touch Panel Kiosk application for a Linux device (e.g. Raspberry Pi) with Touch Display.",
      "html_url": "https://github.com/leukipp/touchkio",
      "stars": 1113,
      "forks": 56,
      "language": "JavaScript",
      "topics": [
        "home-assistant",
        "kiosk-touchscreen",
        "raspberry-pi"
      ],
      "created_at": "2024-12-11T21:51:49Z",
      "updated_at": "2026-01-27T22:49:31Z",
      "pushed_at": "2025-12-15T10:13:12Z",
      "open_issues": 21,
      "owner": {
        "login": "leukipp",
        "avatar_url": "https://avatars.githubusercontent.com/u/53117365?v=4"
      },
      "readme": "# TouchKio\n[![build](https://img.shields.io/github/actions/workflow/status/leukipp/touchkio/release.yml?style=flat-square)](https://github.com/leukipp/touchkio/actions)\n[![date](https://img.shields.io/github/release-date/leukipp/touchkio?style=flat-square)](https://github.com/leukipp/touchkio/releases)\n[![platform](https://img.shields.io/badge/platform-%20arm64%20|%20x64%20-teal?style=flat-square)](https://github.com/leukipp/touchkio/releases)\n[![downloads](https://img.shields.io/github/downloads/leukipp/touchkio/total?style=flat-square)](https://github.com/leukipp/touchkio/releases)\n[![sponsor](https://img.shields.io/github/sponsors/leukipp?color=red&logo=github&style=flat-square)](https://github.com/sponsors/leukipp)\n\n**TouchKio** is a Node.js application that utilizes Electron to create a kiosk mode window specifically designed for a Home Assistant dashboard.\nThis tool is packaged as a **.deb** file, making it easy to launch the kiosk application on any Debian based Linux [hardware](https://github.com/leukipp/touchkio/blob/main/HARDWARE.md) (e.g. **Raspberry Pi**) equipped with a **DSI or HDMI** Touch Display.\nAdditional releases for other Linux systems are available as **.zip** file.\n\n[![display](https://raw.githubusercontent.com/leukipp/touchkio/main/img/display.png)](https://github.com/leukipp/touchkio/blob/main/img/display.png)\n\nThis implementation addresses common issues encountered when using the built-in browser running in fullscreen mode on a Linux device with Touch Display.\nMoreover, the device running the **kiosk application** also offers several **Home Assistant MQTT** sensors, enhancing it's functionality for automation purposes.\n\n## Features\n- [x] Fast and easy setup.\n- [x] Remember login credentials.\n- [x] Touch optimized web browsing.\n- [x] Dynamic window status bar layout.\n- [x] Side panel widget for kiosk control.\n- [x] Navigation bar to switch between pages.\n- [x] Adjustable browser zoom and theme support.\n- [x] Extended touch screen wake-up functionality.\n- [x] Remote controllable via MQTT.\n  - [x] Toggle the on-screen keyboard.\n  - [x] Touch display power and brightness.\n  - [x] Manage kiosk window status and zoom.\n  - [x] Screenshot image of the kiosk webview.\n  - [x] Show network interfaces and addresses.\n  - [x] List all available system package upgrades.\n  - [x] Multi-webpage switching and url navigation.\n  - [x] Volume control for connected audio outputs.\n  - [x] Execute system reboot and shutdown commands.\n  - [x] Monitor battery, temperature, processor and memory usage.\n\nThe kiosk application can be executed with command line arguments to load a **Home Assistant dashboard in fullscreen** mode.\nAdditionally, a **MQTT endpoint** can be defined, allowing the application to provide controls and sensors for the Linux device and the connected Touch Display.\n\n## Setup\nBefore you begin, make sure that you have a Linux device configured and operational with a [compatible](https://github.com/leukipp/touchkio/blob/main/HARDWARE.md) Touch Display.\nThis guide assumes that you are using a Raspberry Pi with the latest version of Raspberry Pi OS **(64-bit)**, along with a desktop environment (preferred using **labwc**).\nHowever, the **.deb** setup procedure is also compatible with any other Debian based 64-bit system.\n\n### Optional\nTo utilize the sensor features of your device through Home Assistant, it's essential to have a **MQTT broker running** and the **MQTT integration installed** on your Home Assistant instance.\nThis setup allows seamless communication between your kiosk device and Home Assistant, enabling **real-time data exchange**.\n\n[![mqtt](https://raw.githubusercontent.com/leukipp/touchkio/main/img/mqtt.png)](https://github.com/leukipp/touchkio/blob/main/img/mqtt.png)\n\nFor a comprehensive guide on setting up MQTT with Home Assistant, please refer to the official documentation available here: https://www.home-assistant.io/integrations/mqtt.\n\n## Installation\nOn the first run of the application, you will encounter a **setup procedure (CLI)** and the Home Assistant **login screen (UI)**.\nIt's recommended to create a dedicated Home Assistant user (local access only) for your kiosk device.\n\nYou might also need a physical keyboard or remote VNC access to input these credentials once.\nIf your hardware is [supported](https://github.com/leukipp/touchkio/blob/main/HARDWARE.md) you may be able to activate the on-screen keyboard using the side [widget](https://github.com/leukipp/touchkio/issues/16).\nAfter the first login the Home Assistant credentials are stored inside the `~/.config/touchkio/` folder.\n\n#### Option 1 - The easy way\nRun this command to download and install the latest **.deb** (arm64 or x64) release.\nIt will also create a systemd user file for auto-startup and will guide you through the setup process:\n```bash\nbash <(wget -qO- https://raw.githubusercontent.com/leukipp/touchkio/main/install.sh)\n```\nMake sure that you run this with your **standard user** and not with root (sudo).\nIf you are paranoid, or smart, or both, have a look into the [install.sh](https://github.com/leukipp/touchkio/blob/main/install.sh) script before executing external code on your machine.\n\nThe systemd **user service** is enabled by default and the kiosk application should start automatically the next time you boot.\nIf you need manual control use:\n```bash\nsystemctl --user start|stop|status|restart touchkio.service\n```\n\n<details><summary>Alternatives</summary><div>\n\n#### Option 2 - The standard way\nWhen connected via SSH, it's necessary to export the display variables first, as outlined in the [development](https://github.com/leukipp/touchkio?tab=readme-ov-file#development) section.\nThe [install.sh](https://github.com/leukipp/touchkio/blob/main/install.sh) script mentioned above performs the following tasks (and you just have to do it manually):\n- [Download](https://github.com/leukipp/touchkio/releases/latest) the latest version file that is suitable for your architecture (arm64 or x64).\n  - Debian (**deb**): Open a terminal and execute the following command to install the application, e.g: `sudo apt install ./touchkio_1.x.x_arm64.deb && touchkio --setup`\n  - Others (**zip**): Extract the archive and run the binary, e.g: `unzip touchkio-linux-x64-1.x.x.zip && cd touchkio-linux-x64 && ./touchkio --setup`\n- If you just want to load a Home Assistant dashboard without further control you are good to go, e.g: `touchkio --web-url=https://demo.home-assistant.io`\n  - The `--web-url` doesn't necessarily need to be a Home Assistant url, any kind of website can be shown in kiosk mode.\n  - Only when using the MQTT integration via `--mqtt-*`, a running Home Assistant instance is required.\n- If you need the application to be automatically started on boot, create a systemd file.\n\n#### Option 3 - The hard way\nPre-built release files are available for arm64 and x64 Linux systems.\nIf you are using a different architecture, you can still utilize this repository to build your own application.\n\nFor more information please refer to the [development](https://github.com/leukipp/touchkio?tab=readme-ov-file#development) section, however this will do the job:\n```bash\nyarn build\n```\n\n</div></details>\n\n#### Update\nIf you have already installed TouchKio and want to upgrade to the latest version, simply install the newer version over the existing one.\n\nThe [install.sh](https://github.com/leukipp/touchkio/blob/main/install.sh) script can also be run to update an existing installation to the **latest version**.\nThe setup procedure can be skipped to use the existing default arguments from the configuration file:\n```bash\nbash <(wget -qO- https://raw.githubusercontent.com/leukipp/touchkio/main/install.sh) update\n```\n\n## Configuration\nRunning `touchkio --setup` will prompt you to enter arguments that will be used when the application starts without any specified arguments.\nThese default arguments are stored in `~/.config/touchkio/Arguments.json`, where they can also be modified.\n\n### WEB\nThe available arguments to control the kiosk application via terminal are as follows:\n| Name                      | Default | Description                                                                                                |\n| ------------------------- | ------- | ---------------------------------------------------------------------------------------------------------- |\n| `--web-url` (Required)    | -       | Url of the Home Assistant instance<a id=\"ref1\"></a><sup><a href=\"#foot1\">[1]</a></sup> (HTTP(S)://IP:PORT) |\n| `--web-theme` (Optional)  | `dark`  | Theme settings of the web browser (`light` or `dark`)                                                      |\n| `--web-zoom` (Optional)   | `1.25`  | Zoom settings of the web browser (`1.0` is `100%`)                                                         |\n| `--web-widget` (Optional) | `true`  | Enables the sidebar widget (`true` or `false`)                                                             |\n\nThese arguments allow you to customize the appearance of the web browser view.\n\nFor example:\n```bash\ntouchkio --web-url=http://192.168.1.42:8123 --web-theme=light --web-zoom=1.0\n```\n\n> <a id=\"foot1\"></a><a href=\"#ref1\">[1]</a>: This doesn't necessarily have to be a Home Assistant Url.\n> You can configure multiple pages by separating them with a comma:\n> `touchkio --web-url=https://demo.home-assistant.io,https://demo.immichkiosk.app`.\n\nIn the `~/.config/touchkio/Arguments.json` file:\n```json\n{\n  \"web_url\": [\n    \"https://demo.home-assistant.io\",\n    \"https://demo.immichkiosk.app\"\n  ]\n}\n```\n\n### MQTT\nTo broadcast your local sensor data to Home Assistant, you can use the following arguments, which require a running MQTT broker:\n| Name                          | Default         | Description                                                                              |\n| ----------------------------- | --------------- | ---------------------------------------------------------------------------------------- |\n| `--mqtt-url` (Required)       | -               | Url of the MQTT broker instance (MQTT(S)://IP:PORT)                                      |\n| `--mqtt-user` (Required)      | -               | Username which is available in Home Assistant (e.g. create a user named `kiosk`)         |\n| `--mqtt-password` (Required)  | -               | The password of the user (e.g. use `password`, because it's secure and easy to remember) |\n| `--mqtt-discovery` (Optional) | `homeassistant` | The discovery prefix for MQTT (`homeassistant` works with default setups)                |\n\nWhen you start the application with the MQTT arguments, the Home Assistant auto-discovery feature will automatically add controls and sensors that can be used for further integration.\nYou can find them under **Settings** -> **Devices and Services** -> **Devices** by searching for **TouchKio**.\n\nFor example:\n```bash\ntouchkio --web-url=http://192.168.1.42:8123 --mqtt-url=mqtt://192.168.1.42:1883 --mqtt-user=kiosk --mqtt-password=password\n```\n\n## User Interface\nTouchKio provides a simple webview window designed specifically for Touch Displays. Electron apps are known to be resource intensive due to their architecture and the inclusion of a full web browser environment. If you just run the kiosk application without other heavy loads, everything should run smoothly.\n\n### Touch Controls\nAdditional controls can be found along the right edge of the kiosk application. For more details, you may want to have a closer look here:\n| Name         | Description                                              |\n| ------------ | -------------------------------------------------------- |\n| `Widget`     | [See #16](https://github.com/leukipp/touchkio/issues/16) |\n| `Navigation` | [See #45](https://github.com/leukipp/touchkio/issues/45) |\n| `Pager`      | [See #64](https://github.com/leukipp/touchkio/issues/64) |\n\n\n### Keyboard Shortcuts\nThe application also supports basic shortcuts to enhance navigation and usability for users who prefer or require non-touch input methods:\n| Name                        | Description                  |\n| --------------------------- | ---------------------------- |\n| `Control+Left`              | Navigate to previous page    |\n| `Control+Right`             | Navigate to next page        |\n| `Control+Num_Subtract`      | Decrease the page zoom       |\n| `Control+Num_Add`           | Increase the page zoom       |\n| `Alt+Left`/`Mouse+Back`     | Navigate backward in history |\n| `Alt+Right`/`Mouse+Forward` | Navigate forward in history  |\n\n## Development\nTo create your own local build, you first need to install [Node.js](https://pimylifeup.com/raspberry-pi-nodejs) and [Yarn](https://classic.yarnpkg.com/lang/en/docs/install).\n\nClone this repository and run `yarn install` to install the dependencies.\nThen use `yarn start` to execute the start script located in the [package.json](https://github.com/leukipp/touchkio/blob/main/package.json) file.\nThere you may want to adjust the `--web-url` and other arguments for development runs.\n\nIf you access your device over SSH, make sure to export the display variables so the kiosk application can launch in the desktop environment.\n```bash\nexport DISPLAY=\":0\"\nexport WAYLAND_DISPLAY=\"wayland-0\"\n```\nTo make this permanent, consider adding the export variables into the `~/.bashrc` file.\n\n### Extensions\n\n<details><summary>You probably won't need this.</summary><div></br>\n\nIncorporating custom extensions and external hardware (like motion sensors, ultrasonic sensors, cameras, relays and switches) via **Raspberry Pi's GPIO/USB** involves several steps.\nWhile using external sensors that directly integrate with Home Assistant and by utilizing automation's to interact with **TouchKio via MQTT** is generally easier and **recommended**, here's a rough guide on how to proceed with custom hardware integration:\n\n1. **Install Node.js library**: Use Yarn to add a library that can interact with your hardware (GPIO, USB, etc.):\n    ```bash\n    yarn add [package-name]\n    ```\n    This will update the `package.json` file with the required dependencies.\n\n2. **Import the library**: Open the `hardware.js` file and import the library using:\n    ```javascript\n    const package = require(\"[package-name]\");\n    ```\n    Then implement your custom methods and logic to handle the hardware. Don't forget to export the methods at the end of the file:\n    ```javascript\n    module.exports = { ... };\n    ```\n\n3. **Expose sensors via MQTT**: If you want to publish sensor data through MQTT, implement some init and update methods in the `integration.js` file:\n    ```javascript\n    const initCustomSensor = (client) => { ... }\n    const updateCustomSensor = (client) => { ... }\n    ```\n    To get started have a look at the existing methods. Don't forget to call the custom sensor initialization method inside the global init method, where the MQTT connection is established:\n    ```javascript\n    const init = async (args) => { ... }\n    ```\n    From there, you will need to further refine your code by tinkering with sensor updates. This can be achieved through either periodic update calls or event based triggers.\n\n</div></details>\n\n### The nitty gritty\n\n<details><summary>Don't waste your time reading this.</summary><div></br>\n\nThe Raspberry Pi's **build-in on-screen keyboard** named `squeekboard` (it squeaks because some _Rust_ got inside), is specifically designed for Wayland environments and features a **D-Bus interface** that allows applications to show or hide the keyboard as needed.\nThe kiosk application interacts with squeekboard via the `D-Bus` object path `/sm/puri/OSK0`, enabling the keyboard to be hidden or shown based on **MQTT** user input or system events.\n\nThe Raspberry Pi's **build-in screen blanking** function uses the command `swayidle -w timeout 600 'wlopm --off \\*' resume 'wlopm --on \\*' &` inside `~/.config/labwc/autostart` to blank the screen after **10 minutes**.\nThe `wlopm --off \\*` command changes the `/sys/class/backlight/*/bl_power` value to **4**, when setting the value to **0** the screen will turn on again.\nHowever, `swayidle` still seems to consider the screen to be off and as a result it will not turn off again unless there is some interaction in the meantime.\n\nWhen using the MQTT integration, the kiosk application must be able to **detect changes** made on the **device** itself.\nI managed to achieve this for the `/sys/class/backlight/*/brightness` file by implementing a simple `fs.watch(..)` file listener.\nHowever, I found that it **never triggered** for the `/sys/class/backlight/*/bl_power` or the `/sys/class/drm/*/dpms` file.\nAlthough the file content changes, none of the filesystem listeners where fired.\nThis could be due to `swayidle`/`wlopm` performing write actions at a deeper level that are not detectable by file listeners.\n\nAs a result, I went for a **polling solution**, checking the state of the `/sys/class/backlight/*/brightness` and `/sys/class/drm/*/dpms` file **every second** for any changes.\nIn case `ddcutil` is installed the file `~/.config/touchkio/Cache/Brightness.vcp` will be written/monitored to support brightness support for HDMI screens.\nWhile I understand this is not ideal, it's necessary to ensure proper functionality.\n\nThe display power status and brightness can be adjusted via the MQTT integration.\n**Support** for changing the power status on **DSI and HDMI** displays is achieved by checking for connected screens in `/sys/class/drm/*/status`.\nSupport for changing the brightness of a connected display is implemented by using `sudo tee /sys/class/backlight/*/brightness` or `sudo ddcutil setvcp 10`.\nIn cases where no supported backlight device is found, the Home Assistant light entity will only show an on/off switch without brightness control.\n\nKeep in mind that default arguments are stored as plain text in `~/.config/touchkio/Arguments.json`.\nThis file also includes the **MQTT user password**, which is somewhat obfuscated/encrypted, but in a way that it could be easily reverse engineered.\nImplementing stronger **security measures** would complicate the setup process and could discourage some users from configuring the application properly.\nWhen using the kiosk application without initializing the default arguments, you will need to provide them with every command.\nThis means that the password may be stored as plain text in various files, such as `touchkio.service`, `~/.bash_history`, etc.\n\nTo address the problem where the first **touch** on a **turned-off screen** may trigger a **click event** (which could inadvertently activate Home Assistant actions), a workaround needed to be implemented.\nWhen the screen **turns on/off**, the timestamps of these events are recorded.\nIf a touch event is detected and the **timestamp** of the **last screen-off** is more recent than the **last screen-on** timestamp, the touch event is discarded and the screen is turned on instead.\nFrom that point onward, subsequent touch interactions will function normally as expected.\n\nAdditionally, to address the problem that scrolling only works with the **web browser scrollbar** on the right, the application is configured to **simulate a touch device** using `Emulation.setEmitTouchEventsForMouse`.\nThis adjustment provides a user experience similar to that of a proper mobile device.\n\n</div></details>\n\n## Issues\n> [!NOTE]\n> ### Please read the hardware [FAQ](https://github.com/leukipp/touchkio/blob/main/HARDWARE.md#faq) section first if you encounter any issues.\n\nFor basic debugging **(TouchKio)**, stop the service and launch `touchkio` directly on the terminal to monitor the log output in real-time.\nThis output is also written into `~/.config/touchkio/logs/main.log` for review.\n\nFor extended logging **(Electron)** you can run `touchkio --enable-logging`, which will write an additional log file into `~/.config/touchkio/logs/electron.log`.\nRefer to the [--log-level=[0-3]](https://www.electronjs.org/docs/latest/api/command-line-switches#--log-leveln) and [--v=[0-3]](https://www.electronjs.org/docs/latest/api/command-line-switches#--vlog_level) options to adjust the logging verbosity.\n\nIf you need to debug the webview **(Chrome DevTools)** use `touchkio --app-debug`.\n\nIn case [undocumented problems](https://github.com/leukipp/touchkio/blob/main/HARDWARE.md#faq) arise, please [create an issue](https://github.com/leukipp/touchkio/issues) and include the output of `touchkio --version`, additional information about your system (such as operating system, hardware, etc.), and any relevant log files.\n\n## Credits\n[Inspired by](https://www.jeffgeerling.com/blog/2024/home-assistant-and-carplay-pi-touch-display-2) the one and only Raspberry Pi Master, Jeff Geerling ([@geerlingguy](https://github.com/geerlingguy)).\n\nThanks to Sebastian ([@pdsccode](https://github.com/pdsccode)) for his contributions on issues and [community](https://community.home-assistant.io/t/kiosk-mode-for-raspberry-pi-with-touch-display/821196) discussions.\n\n### Tutorials\nIf you are looking for hardware or a well-designed mounting solution for the Raspberry Pi Display, check out the [blog post](https://www.thestockpot.net/videos/home-assistant-wall-display) from Dillan Stock ([@TheStockPot-AU](https://www.youtube.com/@TheStockPot-AU)):\n\n<a href=\"https://www.youtube.com/watch?v=uTxURzmrVtA\">\n  <img src=\"https://img.youtube.com/vi/uTxURzmrVtA/maxresdefault.jpg\" alt=\"What My Smart Home Was Missing\" title=\"@TheStockPot-AU\" style=\"width:100%\">\n</a>\n\nAlso, have a look at this curated collection of helpful videos contributed by the community:\n\n<p align=\"center\" style=\"width:100%\">\n  <a href=\"https://www.youtube.com/watch?v=_adl1fiXlgk\">\n    <img src=\"https://img.youtube.com/vi/_adl1fiXlgk/mqdefault.jpg\" alt=\"Building a Home Assistant Kiosk - Why Did I Wait So Long?\" title=\"@Jims-Garage\" style=\"width:32%\">\n  </a>\n  <a href=\"https://www.youtube.com/watch?v=uXcjx-zL_UU\">\n    <img src=\"https://img.youtube.com/vi/uXcjx-zL_UU/mqdefault.jpg\" alt=\"Touch Display for Home Assistant with TouchKIO\" title=\"@haus_automation\" style=\"width:32%\">\n  </a>\n  <a href=\"https://www.youtube.com/watch?v=t2YQm7GPmpY\">\n    <img src=\"https://img.youtube.com/vi/t2YQm7GPmpY/mqdefault.jpg\" alt=\"DIY Home Assistant Kiosk Build: Raspberry Pi & 3D Printing Livestream\" title=\"@homeautomatorza\" style=\"width:32%\">\n  </a>\n</p>\n\n## License\n[MIT](https://github.com/leukipp/touchkio/blob/main/LICENSE)\n",
      "stars_today": 6
    },
    {
      "id": 1038218807,
      "name": "opencode-anthropic-auth",
      "full_name": "anomalyco/opencode-anthropic-auth",
      "description": null,
      "html_url": "https://github.com/anomalyco/opencode-anthropic-auth",
      "stars": 191,
      "forks": 45,
      "language": "JavaScript",
      "topics": [],
      "created_at": "2025-08-14T20:17:03Z",
      "updated_at": "2026-01-27T04:25:08Z",
      "pushed_at": "2026-01-26T03:18:48Z",
      "open_issues": 12,
      "owner": {
        "login": "anomalyco",
        "avatar_url": "https://avatars.githubusercontent.com/u/66570915?v=4"
      },
      "readme": null,
      "stars_today": 6
    },
    {
      "id": 507775,
      "name": "elasticsearch",
      "full_name": "elastic/elasticsearch",
      "description": "Free and Open Source, Distributed, RESTful Search Engine",
      "html_url": "https://github.com/elastic/elasticsearch",
      "stars": 75975,
      "forks": 25786,
      "language": "Java",
      "topics": [
        "elasticsearch",
        "java",
        "search-engine"
      ],
      "created_at": "2010-02-08T13:20:56Z",
      "updated_at": "2026-01-28T02:11:21Z",
      "pushed_at": "2026-01-28T02:11:12Z",
      "open_issues": 5370,
      "owner": {
        "login": "elastic",
        "avatar_url": "https://avatars.githubusercontent.com/u/6764390?v=4"
      },
      "readme": "= Elasticsearch\n\nElasticsearch is a distributed search and analytics engine, scalable data store and vector database optimized for speed and relevance on production-scale workloads. Elasticsearch is the foundation of Elastic's open Stack platform. Search in near real-time over massive datasets, perform vector searches, integrate with generative AI applications, and much more.\n\nUse cases enabled by Elasticsearch include:\n\n* https://www.elastic.co/search-labs/blog/articles/retrieval-augmented-generation-rag[Retrieval Augmented Generation (RAG)]\n* https://www.elastic.co/search-labs/blog/categories/vector-search[Vector search]\n* Full-text search\n* Logs\n* Metrics\n* Application performance monitoring (APM)\n* Security logs\n\n\\... and more!\n\nTo learn more about Elasticsearch's features and capabilities, see our\nhttps://www.elastic.co/products/elasticsearch[product page].\n\nTo access information on https://www.elastic.co/search-labs/blog/categories/ml-research[machine learning innovations] and the latest https://www.elastic.co/search-labs/blog/categories/lucene[Lucene contributions from Elastic], more information can be found in https://www.elastic.co/search-labs[Search Labs].\n\n[[get-started]]\n== Get started\n\nThe simplest way to set up Elasticsearch is to create a managed deployment with\nhttps://www.elastic.co/cloud/as-a-service[Elasticsearch Service on Elastic\nCloud].\n\nIf you prefer to install and manage Elasticsearch yourself, you can download\nthe latest version from\nhttps://www.elastic.co/downloads/elasticsearch[elastic.co/downloads/elasticsearch].\n\n=== Run Elasticsearch locally\n\n////\nIMPORTANT: This content is replicated in the Elasticsearch repo. See `run-elasticsearch-locally.asciidoc`.\nEnsure both files are in sync.\n\nhttps://github.com/elastic/start-local is the source of truth.\n////\n\n[WARNING]\n====\nDO NOT USE THESE INSTRUCTIONS FOR PRODUCTION DEPLOYMENTS.\n\nThis setup is intended for local development and testing only.\n====\n\nQuickly set up Elasticsearch and Kibana in Docker for local development or testing, using the https://github.com/elastic/start-local?tab=readme-ov-file#-try-elasticsearch-and-kibana-locally[`start-local` script].\n\nâ„¹ï¸ For more detailed information about the `start-local` setup, refer to the https://github.com/elastic/start-local[README on GitHub].\n\n==== Prerequisites\n\n- If you don't have Docker installed, https://www.docker.com/products/docker-desktop[download and install Docker Desktop] for your operating system.\n- If you're using Microsoft Windows, then install https://learn.microsoft.com/en-us/windows/wsl/install[Windows Subsystem for Linux (WSL)].\n\n==== Trial license\nThis setup comes with a one-month trial license that includes all Elastic features.\n\nAfter the trial period, the license reverts to *Free and open - Basic*.\nRefer to https://www.elastic.co/subscriptions[Elastic subscriptions] for more information.\n\n==== Run `start-local`\n\nTo set up Elasticsearch and Kibana locally, run the `start-local` script:\n\n[source,sh]\n----\ncurl -fsSL https://elastic.co/start-local | sh\n----\n// NOTCONSOLE\n\nThis script creates an `elastic-start-local` folder containing configuration files and starts both Elasticsearch and Kibana using Docker.\n\nAfter running the script, you can access Elastic services at the following endpoints:\n\n* *Elasticsearch*: http://localhost:9200\n* *Kibana*: http://localhost:5601\n\nThe script generates a random password for the `elastic` user, which is displayed at the end of the installation and stored in the `.env` file.\n\n[CAUTION]\n====\nThis setup is for local testing only. HTTPS is disabled, and Basic authentication is used for Elasticsearch. For security, Elasticsearch and Kibana are accessible only through `localhost`.\n====\n\n==== API access\n\nAn API key for Elasticsearch is generated and stored in the `.env` file as `ES_LOCAL_API_KEY`.\nUse this key to connect to Elasticsearch with a https://www.elastic.co/guide/en/elasticsearch/client/index.html[programming language client] or the https://www.elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html[REST API].\n\nFrom the `elastic-start-local` folder, check the connection to Elasticsearch using `curl`:\n\n[source,sh]\n----\nsource .env\ncurl $ES_LOCAL_URL -H \"Authorization: ApiKey ${ES_LOCAL_API_KEY}\"\n----\n\nTo use the password for the `elastic` user, set and export the `ES_LOCAL_PASSWORD` environment variable. For example:\n\n[source,sh]\n----\nsource .env\nexport ES_LOCAL_PASSWORD\n----\n\n// NOTCONSOLE\n\n=== Send requests to Elasticsearch\n\nYou send data and other requests to Elasticsearch through REST APIs.\nYou can interact with Elasticsearch using any client that sends HTTP requests,\nsuch as the https://www.elastic.co/guide/en/elasticsearch/client/index.html[Elasticsearch\nlanguage clients] and https://curl.se[curl].\n\n==== Using curl\n\nHere's an example curl command to create a new Elasticsearch index, using basic auth:\n\n[source,sh]\n----\ncurl -u elastic:$ES_LOCAL_PASSWORD \\\n  -X PUT \\\n  http://localhost:9200/my-new-index \\\n  -H 'Content-Type: application/json'\n----\n\n// NOTCONSOLE\n\n==== Using a language client\n\nTo connect to your local dev Elasticsearch cluster with a language client, you can use basic authentication with the `elastic` username and the password stored in the `ES_LOCAL_PASSWORD` environment variable.\n\nYou'll use the following connection details:\n\n* **Elasticsearch endpoint**: `http://localhost:9200`\n* **Username**: `elastic`\n* **Password**: `$ES_LOCAL_PASSWORD` (Value you set in the environment variable)\n\nFor example, to connect with the Python `elasticsearch` client:\n\n[source,python]\n----\nimport os\nfrom elasticsearch import Elasticsearch\n\nusername = 'elastic'\npassword = os.getenv('ES_LOCAL_PASSWORD') # Value you set in the environment variable\n\nclient = Elasticsearch(\n    \"http://localhost:9200\",\n    basic_auth=(username, password)\n)\n\nprint(client.info())\n----\n\n==== Using the Dev Tools Console\n\nKibana's developer console provides an easy way to experiment and test requests.\nTo access the console, open Kibana, then go to **Management** > **Dev Tools**.\n\n**Add data**\n\nYou index data into Elasticsearch by sending JSON objects (documents) through the REST APIs.\nWhether you have structured or unstructured text, numerical data, or geospatial data,\nElasticsearch efficiently stores and indexes it in a way that supports fast searches.\n\nFor timestamped data such as logs and metrics, you typically add documents to a\ndata stream made up of multiple auto-generated backing indices.\n\nTo add a single document to an index, submit an HTTP post request that targets the index.\n\n----\nPOST /customer/_doc/1\n{\n  \"firstname\": \"Jennifer\",\n  \"lastname\": \"Walters\"\n}\n----\n\nThis request automatically creates the `customer` index if it doesn't exist,\nadds a new document that has an ID of 1, and\nstores and indexes the `firstname` and `lastname` fields.\n\nThe new document is available immediately from any node in the cluster.\nYou can retrieve it with a GET request that specifies its document ID:\n\n----\nGET /customer/_doc/1\n----\n\nTo add multiple documents in one request, use the `_bulk` API.\nBulk data must be newline-delimited JSON (NDJSON).\nEach line must end in a newline character (`\\n`), including the last line.\n\n----\nPUT customer/_bulk\n{ \"create\": { } }\n{ \"firstname\": \"Monica\",\"lastname\":\"Rambeau\"}\n{ \"create\": { } }\n{ \"firstname\": \"Carol\",\"lastname\":\"Danvers\"}\n{ \"create\": { } }\n{ \"firstname\": \"Wanda\",\"lastname\":\"Maximoff\"}\n{ \"create\": { } }\n{ \"firstname\": \"Jennifer\",\"lastname\":\"Takeda\"}\n----\n\n**Search**\n\nIndexed documents are available for search in near real-time.\nThe following search matches all customers with a first name of _Jennifer_\nin the `customer` index.\n\n----\nGET customer/_search\n{\n  \"query\" : {\n    \"match\" : { \"firstname\": \"Jennifer\" }\n  }\n}\n----\n\n**Explore**\n\nYou can use Discover in Kibana to interactively search and filter your data.\nFrom there, you can start creating visualizations and building and sharing dashboards.\n\nTo get started, create a _data view_ that connects to one or more Elasticsearch indices,\ndata streams, or index aliases.\n\n. Go to **Management > Stack Management > Kibana > Data Views**.\n. Select **Create data view**.\n. Enter a name for the data view and a pattern that matches one or more indices,\nsuch as _customer_.\n. Select **Save data view to Kibana**.\n\nTo start exploring, go to **Analytics > Discover**.\n\n[[upgrade]]\n== Upgrade\n\nTo upgrade from an earlier version of Elasticsearch, see the\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/setup-upgrade.html[Elasticsearch upgrade\ndocumentation].\n\n[[build-source]]\n== Build from source\n\nElasticsearch uses https://gradle.org[Gradle] for its build system.\n\nTo build a distribution for your local OS and print its output location upon\ncompletion, run:\n----\n./gradlew localDistro\n----\n\nTo build a distribution for another platform, run the related command:\n----\n./gradlew :distribution:archives:linux-tar:assemble\n./gradlew :distribution:archives:darwin-tar:assemble\n./gradlew :distribution:archives:windows-zip:assemble\n----\n\nDistributions are output to `distribution/archives`.\n\nTo run the test suite, see xref:TESTING.asciidoc[TESTING].\n\n[[docs]]\n== Documentation\n\nFor the complete Elasticsearch documentation visit\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index.html[elastic.co].\n\nFor information about our documentation processes, see the\nxref:https://github.com/elastic/elasticsearch/blob/main/docs/README.md[docs README].\n\n[[examples]]\n== Examples and guides\n\nThe https://github.com/elastic/elasticsearch-labs[`elasticsearch-labs`] repo contains executable Python notebooks, sample apps, and resources to test out Elasticsearch for vector search, hybrid search and generative AI use cases.\n\n\n[[contribute]]\n== Contribute\n\nFor contribution guidelines, see xref:CONTRIBUTING.md[CONTRIBUTING].\n\n[[questions]]\n== Questions? Problems? Suggestions?\n\n* To report a bug or request a feature, create a\nhttps://github.com/elastic/elasticsearch/issues/new/choose[GitHub Issue]. Please\nensure someone else hasn't created an issue for the same topic.\n\n* Need help using Elasticsearch? Reach out on the\nhttps://discuss.elastic.co[Elastic Forum] or https://ela.st/slack[Slack]. A\nfellow community member or Elastic engineer will be happy to help you out.\n",
      "stars_today": 5
    },
    {
      "id": 27729907,
      "name": "grpc-go",
      "full_name": "grpc/grpc-go",
      "description": "The Go language implementation of gRPC. HTTP/2 based RPC",
      "html_url": "https://github.com/grpc/grpc-go",
      "stars": 22744,
      "forks": 4636,
      "language": "Go",
      "topics": [
        "dogs-over-cats",
        "giant-robots",
        "go",
        "golang",
        "grpc",
        "hacktoberfest",
        "microservices",
        "not-nanoservices",
        "proto",
        "rpc"
      ],
      "created_at": "2014-12-08T18:59:34Z",
      "updated_at": "2026-01-28T01:17:19Z",
      "pushed_at": "2026-01-27T08:34:28Z",
      "open_issues": 133,
      "owner": {
        "login": "grpc",
        "avatar_url": "https://avatars.githubusercontent.com/u/7802525?v=4"
      },
      "readme": "# gRPC-Go\n\n[![GoDoc](https://pkg.go.dev/badge/google.golang.org/grpc)][API]\n[![GoReportCard](https://goreportcard.com/badge/grpc/grpc-go)](https://goreportcard.com/report/github.com/grpc/grpc-go)\n[![codecov](https://codecov.io/gh/grpc/grpc-go/graph/badge.svg)](https://codecov.io/gh/grpc/grpc-go)\n\nThe [Go][] implementation of [gRPC][]: A high performance, open source, general\nRPC framework that puts mobile and HTTP/2 first. For more information see the\n[Go gRPC docs][], or jump directly into the [quick start][].\n\n## Prerequisites\n\n- **[Go][]**: any one of the **two latest major** [releases][go-releases].\n\n## Installation\n\nSimply add the following import to your code, and then `go [build|run|test]`\nwill automatically fetch the necessary dependencies:\n\n\n```go\nimport \"google.golang.org/grpc\"\n```\n\n> **Note:** If you are trying to access `grpc-go` from **China**, see the\n> [FAQ](#FAQ) below.\n\n## Learn more\n\n- [Go gRPC docs][], which include a [quick start][] and [API\n  reference][API] among other resources\n- [Low-level technical docs](Documentation) from this repository\n- [Performance benchmark][]\n- [Examples](examples)\n- [Contribution guidelines](CONTRIBUTING.md)\n\n## FAQ\n\n### I/O Timeout Errors\n\nThe `golang.org` domain may be blocked from some countries. `go get` usually\nproduces an error like the following when this happens:\n\n```console\n$ go get -u google.golang.org/grpc\npackage google.golang.org/grpc: unrecognized import path \"google.golang.org/grpc\" (https fetch: Get https://google.golang.org/grpc?go-get=1: dial tcp 216.239.37.1:443: i/o timeout)\n```\n\nTo build Go code, there are several options:\n\n- Set up a VPN and access google.golang.org through that.\n\n- With Go module support: it is possible to use the `replace` feature of `go\n  mod` to create aliases for golang.org packages.  In your project's directory:\n\n  ```sh\n  go mod edit -replace=google.golang.org/grpc=github.com/grpc/grpc-go@latest\n  go mod tidy\n  go mod vendor\n  go build -mod=vendor\n  ```\n\n  Again, this will need to be done for all transitive dependencies hosted on\n  golang.org as well. For details, refer to [golang/go issue\n  #28652](https://github.com/golang/go/issues/28652).\n\n### Compiling error, undefined: grpc.SupportPackageIsVersion\n\nPlease update to the latest version of gRPC-Go using\n`go get google.golang.org/grpc`.\n\n### How to turn on logging\n\nThe default logger is controlled by environment variables. Turn everything on\nlike this:\n\n```console\n$ export GRPC_GO_LOG_VERBOSITY_LEVEL=99\n$ export GRPC_GO_LOG_SEVERITY_LEVEL=info\n```\n\n### The RPC failed with error `\"code = Unavailable desc = transport is closing\"`\n\nThis error means the connection the RPC is using was closed, and there are many\npossible reasons, including:\n 1. mis-configured transport credentials, connection failed on handshaking\n 1. bytes disrupted, possibly by a proxy in between\n 1. server shutdown\n 1. Keepalive parameters caused connection shutdown, for example if you have\n    configured your server to terminate connections regularly to [trigger DNS\n    lookups](https://github.com/grpc/grpc-go/issues/3170#issuecomment-552517779).\n    If this is the case, you may want to increase your\n    [MaxConnectionAgeGrace](https://pkg.go.dev/google.golang.org/grpc/keepalive?tab=doc#ServerParameters),\n    to allow longer RPC calls to finish.\n\nIt can be tricky to debug this because the error happens on the client side but\nthe root cause of the connection being closed is on the server side. Turn on\nlogging on __both client and server__, and see if there are any transport\nerrors.\n\n[API]: https://pkg.go.dev/google.golang.org/grpc\n[Go]: https://golang.org\n[Go module]: https://github.com/golang/go/wiki/Modules\n[gRPC]: https://grpc.io\n[Go gRPC docs]: https://grpc.io/docs/languages/go\n[Performance benchmark]: https://performance-dot-grpc-testing.appspot.com/explore?dashboard=5180705743044608\n[quick start]: https://grpc.io/docs/languages/go/quickstart\n[go-releases]: https://golang.org/doc/devel/release.html\n",
      "stars_today": 5
    },
    {
      "id": 5101141,
      "name": "jq",
      "full_name": "jqlang/jq",
      "description": "Command-line JSON processor",
      "html_url": "https://github.com/jqlang/jq",
      "stars": 33423,
      "forks": 1698,
      "language": "C",
      "topics": [
        "jq"
      ],
      "created_at": "2012-07-18T19:57:25Z",
      "updated_at": "2026-01-28T01:19:20Z",
      "pushed_at": "2026-01-20T12:56:09Z",
      "open_issues": 447,
      "owner": {
        "login": "jqlang",
        "avatar_url": "https://avatars.githubusercontent.com/u/104800540?v=4"
      },
      "readme": "# jq\n\n`jq` is a lightweight and flexible command-line JSON processor akin to `sed`,`awk`,`grep`, and friends for JSON data. It's written in portable C and has zero runtime dependencies, allowing you to easily slice, filter, map, and transform structured data.\n\n## Documentation\n\n- **Official Documentation**: [jqlang.org](https://jqlang.org)\n- **Try jq Online**: [play.jqlang.org](https://play.jqlang.org)\n\n## Installation\n\n### Prebuilt Binaries\n\nDownload the latest releases from the [GitHub release page](https://github.com/jqlang/jq/releases).\n\n### Docker Image\n\nPull the [jq image](https://github.com/jqlang/jq/pkgs/container/jq) to start quickly with Docker.\n\n#### Run with Docker\n\n##### Example: Extracting the version from a `package.json` file\n\n```bash\ndocker run --rm -i ghcr.io/jqlang/jq:latest < package.json '.version'\n```\n\n##### Example: Extracting the version from a `package.json` file with a mounted volume\n\n```bash\ndocker run --rm -i -v \"$PWD:$PWD\" -w \"$PWD\" ghcr.io/jqlang/jq:latest '.version' package.json\n```\n\n### Building from source\n\n#### Dependencies\n\n- libtool\n- make\n- automake\n- autoconf\n\n#### Instructions\n\n```console\ngit submodule update --init    # if building from git to get oniguruma\nautoreconf -i                  # if building from git\n./configure --with-oniguruma=builtin\nmake clean                     # if upgrading from a version previously built from source\nmake -j8\nmake check\nsudo make install\n```\n\nBuild a statically linked version:\n\n```console\nmake LDFLAGS=-all-static\n```\n\nIf you're not using the latest git version but instead building a released tarball (available on the release page), skip the `autoreconf` step, and flex or bison won't be needed.\n\n##### Cross-Compilation\n\nFor details on cross-compilation, check out the [GitHub Actions file](.github/workflows/ci.yml) and the [cross-compilation wiki page](https://github.com/jqlang/jq/wiki/Cross-compilation).\n\n## Community & Support\n\n- Questions & Help: [Stack Overflow (jq tag)](https://stackoverflow.com/questions/tagged/jq)\n- Chat & Community: [Join us on Discord](https://discord.gg/yg6yjNmgAC)\n- Wiki & Advanced Topics: [Explore the Wiki](https://github.com/jqlang/jq/wiki)\n\n## License\n\n`jq` is released under the [MIT License](COPYING). `jq`'s documentation is\nlicensed under the [Creative Commons CC BY 3.0](COPYING).\n`jq` uses parts of the open source C library \"decNumber\", which is distributed\nunder [ICU License](COPYING)\n",
      "stars_today": 5
    },
    {
      "id": 30102273,
      "name": "Mobile-Security-Framework-MobSF",
      "full_name": "MobSF/Mobile-Security-Framework-MobSF",
      "description": "Mobile Security Framework (MobSF) is an automated, all-in-one mobile application (Android/iOS/Windows) pen-testing, malware analysis and security assessment framework capable of performing static and dynamic analysis.",
      "html_url": "https://github.com/MobSF/Mobile-Security-Framework-MobSF",
      "stars": 20269,
      "forks": 3574,
      "language": "JavaScript",
      "topics": [
        "android-security",
        "api-testing",
        "apk",
        "cwe",
        "devsecops",
        "dynamic-analysis",
        "ios-security",
        "malware-analysis",
        "mastg",
        "masvs",
        "mobile-security",
        "mobsf",
        "mstg",
        "owasp",
        "rest",
        "runtime-security",
        "static-analysis",
        "web-security",
        "windows-mobile-security"
      ],
      "created_at": "2015-01-31T04:36:01Z",
      "updated_at": "2026-01-28T01:18:45Z",
      "pushed_at": "2026-01-26T07:01:34Z",
      "open_issues": 18,
      "owner": {
        "login": "MobSF",
        "avatar_url": "https://avatars.githubusercontent.com/u/25052637?v=4"
      },
      "readme": "# Mobile Security Framework (MobSF)\n\n![](https://cloud.githubusercontent.com/assets/4301109/20019521/cc61f7fc-a2f2-11e6-95f3-407030d9fdde.png)\n\nMobile Security Framework (MobSF) is a security research platform for mobile applications in Android, iOS and Windows Mobile. MobSF can be used for a variety of use cases such as mobile application security, penetration testing, malware analysis, and privacy analysis. The Static Analyzer supports popular mobile app binaries like APK, IPA, APPX and source code. Meanwhile, the Dynamic Analyzer supports both Android and iOS applications and offers a platform for interactive instrumented testing, runtime data and network traffic analysis. MobSF seamlessly integrates with your DevSecOps or CI/CD pipeline, facilitated by REST APIs and CLI tools, enhancing your security workflow with ease.\n\nMade with ![Love](https://cloud.githubusercontent.com/assets/4301109/16754758/82e3a63c-4813-11e6-9430-6015d98aeaab.png) in India\n\n[![Docker Pulls](https://img.shields.io/docker/pulls/opensecurity/mobile-security-framework-mobsf?style=social)](https://hub.docker.com/r/opensecurity/mobile-security-framework-mobsf/) [![python](https://img.shields.io/badge/python-3.12+-blue.svg?logo=python&labelColor=yellow)](https://www.python.org/downloads/)\n[![PyPI version](https://badge.fury.io/py/mobsf.svg)](https://badge.fury.io/py/mobsf)\n[![platform](https://img.shields.io/badge/platform-osx%2Flinux%2Fwindows-green.svg)](https://github.com/MobSF/Mobile-Security-Framework-MobSF/)\n[![License](https://img.shields.io/:license-GPL--3.0--only-blue.svg)](https://www.gnu.org/licenses/gpl-3.0.html)\n[![MobSF tests](https://github.com/MobSF/Mobile-Security-Framework-MobSF/workflows/MobSF%20tests/badge.svg?branch=master)](https://github.com/MobSF/Mobile-Security-Framework-MobSF/actions)\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=MobSF_Mobile-Security-Framework-MobSF&metric=alert_status)](https://sonarcloud.io/dashboard?id=MobSF_Mobile-Security-Framework-MobSF)\n![GitHub closed issues](https://img.shields.io/github/issues-closed/MobSF/Mobile-Security-Framework-MobSF)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/6392/badge)](https://bestpractices.coreinfrastructure.org/projects/6392)\n\n\n[![ToolsWatch Best Security Tools 2016](https://img.shields.io/badge/ToolsWatch-Rank%205%20%7C%20Year%202016-red.svg)](http://www.toolswatch.org/2017/02/2016-top-security-tools-as-voted-by-toolswatch-org-readers/)\n[![ToolsWatch Best Security Tools 2017](https://img.shields.io/badge/ToolsWatch-Rank%209%20%7C%20Year%202017-red.svg)](http://www.toolswatch.org/2018/01/black-hat-arsenal-top-10-security-tools/)\n[![Blackhat Arsenal Asia 2015](https://img.shields.io/badge/Black%20Hat%20Arsenal-Asia%202015-blue.svg)](https://www.blackhat.com/asia-15/arsenal.html#yso-mobile-security-framework)\n[![Blackhat Arsenal Asia 2018](https://img.shields.io/badge/Black%20Hat%20Arsenal-Asia%202018-blue.svg)](https://www.blackhat.com/asia-18/arsenal.html#mobile-security-framework-mobsf)\n[![Blackhat Arsenal Europe 2023](https://img.shields.io/badge/Black%20Hat%20Arsenal-Europe%202023-blue.svg)](https://www.blackhat.com/eu-23/arsenal/schedule/index.html#mobile-security-framework---mobsf-35327)\n\n\nMobSF is also bundled with [Android Tamer](https://tamerplatform.com), [BlackArch](https://blackarch.org/mobile.html) and [Pentoo](https://www.pentoo.ch/).\n\n### Support MobSF\n\n[![Donate to MobSF](https://user-images.githubusercontent.com/4301109/117404264-7aab5480-aebe-11eb-9cbd-da82d7346bb3.png)](https://opensecurity.in/donate)\n\n\n> Has MobSF made a difference for you? Show your support and help us innovate with a donation. It's easy to build open source, maintaining one is a different story. \n\n*Long live open source!*\n\n## Documentation\n\nQuick setup with docker\n\n```\ndocker pull opensecurity/mobile-security-framework-mobsf:latest\ndocker run -it --rm -p 8000:8000 opensecurity/mobile-security-framework-mobsf:latest\n\n# Default username and password: mobsf/mobsf\n```\n\n[![See MobSF Documentation](https://user-images.githubusercontent.com/4301109/70686099-3855f780-1c79-11ea-8141-899e39459da2.png)](https://mobsf.github.io/docs)\n\n* Try MobSF Static Analyzer Online: [mobsf.live](https://mobsf.live)\n* MobSF in CI/CD: [mobsfscan](https://github.com/MobSF/mobsfscan)\n* Conference Presentations: [Slides & Videos](https://mobsf.github.io/Mobile-Security-Framework-MobSF/presentations.html)\n* MobSF Online Course: [OpSecX MAS](https://opsecx.com/index.php/product/automated-mobile-application-security-assessment-with-mobsf/)\n* What's New: [See Changelog](https://mobsf.github.io/Mobile-Security-Framework-MobSF/changelog.html)\n\n## Collaborators\n\n[Ajin Abraham](https://in.linkedin.com/in/ajinabraham) ![india](https://user-images.githubusercontent.com/4301109/37564171-6549d678-2ab6-11e8-9b9d-21327c7f5d5b.png)  | [Magaofei](https://github.com/magaofei) ![china](https://user-images.githubusercontent.com/4301109/44515364-00bbe880-a6e0-11e8-944d-5b48a86427da.png) | [Matan Dobrushin](https://github.com/matandobr) ![israel](https://user-images.githubusercontent.com/4301109/37564177-782f1758-2ab6-11e8-91e5-c76bde37b330.png) | [Vincent Nadal](https://github.com/superpoussin22) ![france](https://user-images.githubusercontent.com/4301109/37564175-71d6d92c-2ab6-11e8-89d7-d21f5aa0bda8.png)\n\n## e-Learning Courses & Certifications\n![MobSF Course](https://user-images.githubusercontent.com/4301109/76344880-ad68b580-62d8-11ea-8cde-9e3475fc92f6.png) [Automated Mobile Application Security Assessment with MobSF -MAS](https://opsecx.com/index.php/product/automated-mobile-application-security-assessment-with-mobsf/)\n\n![Android Security Tools Course](https://user-images.githubusercontent.com/4301109/76344939-c709fd00-62d8-11ea-8208-774f1d5a7c52.png) [Android Security Tools Expert -ATX](https://opsecx.com/index.php/product/android-security-tools-expert-atx/)\n\n## MobSF Support\n\n* **Free Support:** Free limited support, questions, help and discussions, join our Slack channel [![Join_MobSF_Slack](https://img.shields.io/badge/mobsf%20slack-join-green?logo=slack&labelColor=4A154B)](https://join.slack.com/t/mobsf/shared_invite/zt-3ephptj6c-OuDMatJ9z9MT0T_Vb~l2pg)\n* **Enterprise Support:** Priority feature requests, live support & onsite training, see [![MobSF Support Packages](https://img.shields.io/badge/enterprise-support%20package-blue?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGMAAABaCAMAAACbkBjCAAAAAXNSR0IB2cksfwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAkNQTFRFAAAA////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////o1yoNQAAAMF0Uk5TAAQ1YH6IXzYFAUqq9P/1rU4CPdHWQ4X+jgOkro+hVWYR6vIajJvz+XN/z9sjLW6lrNPX+PsGHyE5TE9XWV1hW0k6LC7hs7SEhgr2sGRiFBCXJf38sgwOtmpoIiDf68vSGS+Z9yeR70RNvUEVfOa/WgdCR8XuUeeA8Du7Vporrx66SwlcqWV92g2HY+2Ki/GQJjxpM8TVcIGodjJ3ybETJOjUyiop4sxYkkVSGJ/pD6cdwlTOHNnIZ+B13bXkKL7NMOC0/xQAAAPPSURBVHic7dn5PxVRFADwY0sPN0t4Uj0vLQq9lkelRVFRKklZs+RRiiTtSbRoD9GuhdK+Ky3S9qf1Zu68bczcO5rjhz4f5yfnOud9mTszd+Y+gLH4f8PL28fXz893nP/4UQIMAYFBRIoJwSGho0CETSQeER6BLYRGGoksjFGTcIlouSDEZNRpmaJEEDIVkTApE4TEoBHmaWpG7HQsY4YaQchMJMI8S92IQ/pHZqsThMzBMeJZRgIKkcgiCJmLYcSwDQuGMY9tzMcwFrCNhQiEl5VtJCXrNxaxCUIW6zf8ecYS/UYKz1iq31jGM5brN1bwDB/9RirPQFhDuMdqpX5jFc9I0W+k8Yww/UY6z1it3zDzDLN+A9awiTUIBKxlGxkYRibbWIdhLGUbCLcrgPUTWETWegyDPSEbUAjYyDI24RjZWepE7GYcA3LUjSlIBGzJVSO2JmIZsG2UZ1wI1TUEYZ11RJ7KrFvz8Qy1S6QAkYBCZaMI0yhWNrZjGlCiRJSiElCmZJSPGWOGrtiheKFjGhW2yiVVO+XL+i6bz9pqHCB09x77B9YA1O6tcwHGffX7oYGQsnr9+2R5MeHiZx4QkmTTQSocsqUL+WHh5yNHs3UJxVHHpL87RBo5HphkbDwhvS770N9lNZ38Z6Gm2bXGmpyjLa7l1eI8cqdO/8tW7JmNHnutZ6XhtJzA1nOOx7bzbgUXLBdHKLRcKvU8iS4Lo2YL3de4cpXe1Ns8Sto7ro1A6AxMkl8KXQDXbc5tapLbuNsA0CSvumHy0gTkF96Ut9rjFtyWjRwFuDO8ruRuLVfoblLePLwH4bKRmwAFSpXW6E4WYC66rwjYw3fYY8MDgIMqxeWWh8qAobq1Uk0gJBJ6ej0GensAHqmWW1urDQoH6bE6YI82gD53pPeJvecpq+OZt5x4ztkJewEeSG+f0NTObIlL9SReDjtZZWEDd4QSoaqP2jRyX7kTr49wCPJGrKuqE5O34h0R3vGaYqvcjAxeNXlPC+k7biZNPnC7+l1EA7eYBNPKVjG5QxPuLgchH51GP79YetP/JCbxNDnOb5voIE7ya0kjvYboTSCInvsBGvocG5odGmrpkR2QsgF6vmvos0nGZw21X8TKCClrELOvGvq+UYK7SSwEnYIiKesSs0EtjXSl6dZSGi6WOr5ei3I7kTnRrd34LpYOSdmQmL1ANn6IpQlSRr+8mYpsiK9MyY672lbx4acZ2WgXKp84059C+gvZsAqVrt0ycWPsN7JhNHicrIPus6PBMJu0hPAcWOHM/giNKVr6zDAWI4i/nmw15nhs85kAAAAASUVORK5CYII=)](https://opensecurity.in/#support)\n\n\n## Contribution, Feature Requests & Bugs\n\n* Read [CONTRIBUTING.md](https://github.com/MobSF/Mobile-Security-Framework-MobSF/blob/master/.github/CONTRIBUTING.md) before opening bugs, feature requests and pull request.\n* For Project updates and announcements, follow [@ajinabraham](https://twitter.com/ajinabraham) or [@OpenSecurity_IN](https://twitter.com/OpenSecurity_IN).\n* Github Issues are only for tracking bugs and feature requests. Do not post support or help queries there. We have a slack channel for that.\n\n### Static Analysis - Android\n\n![mobsf_android_static_analysis](https://user-images.githubusercontent.com/4301109/95506503-f9b6c980-097d-11eb-803a-f88321e1feb7.gif)\n\n### Static Analysis - iOS\n\n![mobsf_ios_ipa_static_analysis](https://user-images.githubusercontent.com/4301109/95507865-16540100-0980-11eb-9e4d-887668d46969.gif)\n\n### Dynamic Analysis - Android APK\n\n![mobsf_android_dynamic_analysis](https://user-images.githubusercontent.com/4301109/95514697-5e782100-098a-11eb-8390-47bb3822a2d7.gif)\n\n### Web API Viewer\n\n![mobsf_web_api_fuzzing_with_burp](https://user-images.githubusercontent.com/4301109/95516560-69808080-098d-11eb-9e0b-fb5a25e96585.gif)\n\n### Dynamic Analysis - iOS IPA\n\n![mobsf_ios_dynamic_analysis](https://github.com/MobSF/Mobile-Security-Framework-MobSF/assets/4301109/34014c4d-1535-48ad-9944-a4b1b728a030)\n\n## Past Collaborators\n\n* [Dominik Schlecht](https://github.com/sn0b4ll) ![germany](https://user-images.githubusercontent.com/4301109/37564176-743238ba-2ab6-11e8-9666-5d98f0a1d127.png)\n\n## Honorable Contributors & Shoutouts\n\n* Amrutha VC - For the new MobSF logo\n* Dominik Schlecht - For the awesome work on adding Windows Phone App Static Analysis to MobSF\n* Esteban - Better Android Manifest Analysis and Static Analysis Improvement.\n* Matan Dobrushin - For adding Android ARM Emulator support to MobSF - Special thanks goes for cuckoo-droid\n* Shuxin - Android Binary Analysis\n* Abhinav Saxena - (@xandfury) - For Travis CI and Logging integration\n* ![netguru](https://user-images.githubusercontent.com/4301109/76340877-a3dc4f00-62d2-11ea-8631-b4cc8d9e42ed.png) [Netguru](https://www.netguru.com/) (@karolpiateknet, @mtbrzeski) - For iOS Swift support, Rule contributions and SAST refactoring.\n* Maxime Fawe - (@Arenash13) - For Matching Strategy implementation of SAST pattern matching algorithms.\n* Abhinav Sejpal (@Abhinav_Sejpal) - For poking me with bugs, feature requests, and UI & UX suggestions\n* Anant Srivastava (@anantshri) - For Activity Tester Idea\n* Anto Joseph (@antojoseph) - For the help with SuperSU\n* Bharadwaj Machiraju (@tunnelshade) - For writing pyWebProxy from scratch\n* Rahul (@c0dist) - Kali Support\n* MindMac - For writing Android Blue Pill\n* Oscar Alfonso Diaz - (@OscarAkaElvis) - For Dockerfile contributions\n* Thomas Abraham - For JS Hacks on UI\n* Tim Brown (@timb_machine) - For the iOS Binary Analysis Ruleset\n* Shanil Prasad (@Rajuraju14) - For improving iOS ATS Analysis\n* Jovan Petrovic (@JovanPetrovic) - For sponsoring a server to host mobsf.live\n",
      "stars_today": 5
    },
    {
      "id": 105379569,
      "name": "actix-web",
      "full_name": "actix/actix-web",
      "description": "Actix Web is a powerful, pragmatic, and extremely fast web framework for Rust.",
      "html_url": "https://github.com/actix/actix-web",
      "stars": 24255,
      "forks": 1831,
      "language": "Rust",
      "topics": [
        "actix",
        "actix-web",
        "async",
        "rust",
        "web",
        "web-development",
        "websockets"
      ],
      "created_at": "2017-09-30T15:30:02Z",
      "updated_at": "2026-01-28T02:07:09Z",
      "pushed_at": "2026-01-27T11:39:24Z",
      "open_issues": 241,
      "owner": {
        "login": "actix",
        "avatar_url": "https://avatars.githubusercontent.com/u/32776943?v=4"
      },
      "readme": "<div align=\"center\">\n  <h1>Actix Web</h1>\n  <p>\n    <strong>Actix Web is a powerful, pragmatic, and extremely fast web framework for Rust</strong>\n  </p>\n  <p>\n\n<!-- prettier-ignore-start -->\n\n[![crates.io](https://img.shields.io/crates/v/actix-web?label=latest)](https://crates.io/crates/actix-web)\n[![Documentation](https://docs.rs/actix-web/badge.svg?version=4.12.1)](https://docs.rs/actix-web/4.12.1)\n![MSRV](https://img.shields.io/badge/rustc-1.83+-ab6000.svg)\n![MIT or Apache 2.0 licensed](https://img.shields.io/crates/l/actix-web.svg)\n[![Dependency Status](https://deps.rs/crate/actix-web/4.12.1/status.svg)](https://deps.rs/crate/actix-web/4.12.1)\n<br />\n[![CI](https://github.com/actix/actix-web/actions/workflows/ci.yml/badge.svg)](https://github.com/actix/actix-web/actions/workflows/ci.yml)\n[![codecov](https://codecov.io/gh/actix/actix-web/graph/badge.svg?token=dSwOnp9QCv)](https://codecov.io/gh/actix/actix-web)\n![downloads](https://img.shields.io/crates/d/actix-web.svg)\n[![Chat on Discord](https://img.shields.io/discord/771444961383153695?label=chat&logo=discord)](https://discord.gg/NWpN5mmg3x)\n\n<!-- prettier-ignore-end -->\n\n  </p>\n</div>\n\n## Features\n\n- Supports _HTTP/1.x_ and _HTTP/2_\n- Streaming and pipelining\n- Powerful [request routing](https://actix.rs/docs/url-dispatch/) with optional macros\n- Full [Tokio](https://tokio.rs) compatibility\n- Keep-alive and slow requests handling\n- Client/server [WebSockets](https://actix.rs/docs/websockets/) support\n- Transparent content compression/decompression (br, gzip, deflate, zstd)\n- Multipart streams\n- Static assets\n- SSL support using OpenSSL or Rustls\n- Middlewares ([Logger, Session, CORS, etc](https://actix.rs/docs/middleware/))\n- Integrates with the [`awc` HTTP client](https://docs.rs/awc/)\n- Runs on stable Rust 1.72+\n\n## Documentation\n\n- [Website & User Guide](https://actix.rs)\n- [Examples Repository](https://github.com/actix/examples)\n- [API Documentation](https://docs.rs/actix-web)\n- [API Documentation (mainranch)](https://actix.rs/actix-web/actix_web)\n\n## Example\n\nDependencies:\n\n```toml\n[dependencies]\nactix-web = \"4\"\n```\n\nCode:\n\n```rust\nuse actix_web::{get, web, App, HttpServer, Responder};\n\n#[get(\"/hello/{name}\")]\nasync fn greet(name: web::Path<String>) -> impl Responder {\n    format!(\"Hello {name}!\")\n}\n\n#[actix_web::main] // or #[tokio::main]\nasync fn main() -> std::io::Result<()> {\n    HttpServer::new(|| {\n        App::new().service(greet)\n    })\n    .bind((\"127.0.0.1\", 8080))?\n    .run()\n    .await\n}\n```\n\n### More Examples\n\n- [Hello World](https://github.com/actix/examples/tree/main/basics/hello-world)\n- [Basic Setup](https://github.com/actix/examples/tree/main/basics/basics)\n- [Application State](https://github.com/actix/examples/tree/main/basics/state)\n- [JSON Handling](https://github.com/actix/examples/tree/main/json/json)\n- [Multipart Streams](https://github.com/actix/examples/tree/main/forms/multipart)\n- [MongoDB Integration](https://github.com/actix/examples/tree/main/databases/mongodb)\n- [Diesel Integration](https://github.com/actix/examples/tree/main/databases/diesel)\n- [SQLite Integration](https://github.com/actix/examples/tree/main/databases/sqlite)\n- [Postgres Integration](https://github.com/actix/examples/tree/main/databases/postgres)\n- [Tera Templates](https://github.com/actix/examples/tree/main/templating/tera)\n- [Askama Templates](https://github.com/actix/examples/tree/main/templating/askama)\n- [HTTPS using Rustls](https://github.com/actix/examples/tree/main/https-tls/rustls)\n- [HTTPS using OpenSSL](https://github.com/actix/examples/tree/main/https-tls/openssl)\n- [Simple WebSocket](https://github.com/actix/examples/tree/main/websockets)\n- [WebSocket Chat](https://github.com/actix/examples/tree/main/websockets/chat)\n\nYou may consider checking out [this directory](https://github.com/actix/examples/tree/main) for more examples.\n\n## Benchmarks\n\nOne of the fastest web frameworks available according to the [TechEmpower Framework Benchmark](https://www.techempower.com/benchmarks/#section=data-r21&test=composite).\n\n## License\n\nThis project is licensed under either of the following licenses, at your option:\n\n- Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or [http://www.apache.org/licenses/LICENSE-2.0])\n- MIT license ([LICENSE-MIT](LICENSE-MIT) or [http://opensource.org/licenses/MIT])\n\n## Code of Conduct\n\nContribution to the `actix/actix-web` repo is organized under the terms of the Contributor Covenant. The Actix team promises to intervene to uphold that code of conduct.\n",
      "stars_today": 5
    },
    {
      "id": 293498508,
      "name": "compose-multiplatform",
      "full_name": "JetBrains/compose-multiplatform",
      "description": "Compose Multiplatform, a modern UI framework for Kotlin that makes building performant and beautiful user interfaces easy and enjoyable.",
      "html_url": "https://github.com/JetBrains/compose-multiplatform",
      "stars": 18749,
      "forks": 1383,
      "language": "Kotlin",
      "topics": [
        "android",
        "awt",
        "compose",
        "declarative-ui",
        "desktop",
        "gui",
        "ios",
        "javascript",
        "kotlin",
        "multiplatform",
        "reactive",
        "swing",
        "ui",
        "wasm",
        "web",
        "webassembly"
      ],
      "created_at": "2020-09-07T10:40:49Z",
      "updated_at": "2026-01-27T18:48:44Z",
      "pushed_at": "2026-01-27T14:14:35Z",
      "open_issues": 70,
      "owner": {
        "login": "JetBrains",
        "avatar_url": "https://avatars.githubusercontent.com/u/878437?v=4"
      },
      "readme": "[![official project](http://jb.gg/badges/official.svg)](https://confluence.jetbrains.com/display/ALL/JetBrains+on+GitHub)\n[![stable](https://img.shields.io/github/v/release/JetBrains/compose-multiplatform?sort=semver&display_name=release&label=stable&color=brightgreen)](https://github.com/JetBrains/compose-multiplatform/releases/latest)\n[![prerelease](https://img.shields.io/github/v/release/JetBrains/compose-multiplatform?include_prereleases&sort=semver&filter=*-*&display_name=release&label=prerelease&color=blue)](https://github.com/JetBrains/compose-multiplatform/releases)\n[![dev](https://img.shields.io/github/v/tag/JetBrains/compose-multiplatform?include_prereleases&sort=semver&filter=v*%2Bdev*&label=dev&color=orange)](https://github.com/JetBrains/compose-multiplatform/tags)\n\n<a href=\"https://jb.gg/cmp\">\n    <picture>\n        <source srcset=\"artwork/compose-logo-name-white.svg\"  width=\"400\" media=\"(prefers-color-scheme: dark)\">\n        <img src=\"artwork/compose-logo-name-black.svg\" alt=\"Compose Multiplatform logo and name\" width=\"400\">\n    </picture>\n</a>\n\n[Compose Multiplatform](https://jb.gg/cmp) is a declarative framework for sharing UI code across multiple platforms with Kotlin. \nIt is based on [Jetpack Compose](https://developer.android.com/jetpack/compose) and developed by [JetBrains](https://www.jetbrains.com/) and open-source contributors.\n\nYou can choose the platforms across which to share your UI code using Compose Multiplatform:\n\n* [iOS](https://jb.gg/start-cmp)\n* [Android](https://jb.gg/start-cmp) \n* [Desktop](https://jb.gg/start-cmp) (Windows, MacOS, and Linux)\n* [Web](https://jb.gg/start-cmp) (Beta)\n\nFor example, you can share UIs between iOS and Android or Windows and MacOS.\n\n![Shared UIs of the iOS, Android, desktop, and web apps](artwork/readme/apps.png)\n\n## iOS\n\nCompose Multiplatform shares most of its API with Jetpack Compose, the Android UI framework developed by Google. \nYou can use the same APIs to build user interfaces for both Android and iOS.\n\nSince Compose is built on top of [Kotlin Multiplatform](https://jb.gg/kmp), \nyou can easily access native APIs, such as the [Camera API](https://developer.apple.com/documentation/avfoundation/capture_setup/avcam_building_a_camera_app), \nand embed complex native UI views, such as [MKMapView](https://developer.apple.com/documentation/mapkit/mkmapview).\n\n**[Get started with Compose Multiplatform](https://jb.gg/start-cmp)**\n\n## Android\n\nWhen Android is one of your targets, you get the same experience for Android as if you were developing an Android app \nusing [Jetpack Compose](https://developer.android.com/jetpack/compose).\n\n**[Get started with Compose Multiplatform](https://jb.gg/start-cmp)**\n\n## Desktop\n\nCompose Multiplatform targets the JVM and supports high-performance hardware-accelerated UI rendering on all major desktop\nplatforms â€“ macOS, Windows, and Linux.\n\nIt has desktop extensions for menus, keyboard shortcuts, window manipulation, and notification management.\n\n**[Get started with Compose Multiplatform](https://jb.gg/start-cmp)**\n\n## Web\n\n> Web support is in Beta, making it a great time to give it a try. Check out our [blog post](https://blog.jetbrains.com/kotlin/2025/09/compose-multiplatform-1-9-0-compose-for-web-beta/) to learn more about the progress made to reach this milestone.\n> We would appreciate your feedback on it in the public Slack channel [#compose-web](https://kotlinlang.slack.com/archives/C01F2HV7868/p1678887590205449). \n> If you face any issues, please report them on [YouTrack](https://youtrack.jetbrains.com/newIssue?project=CMP).\n\nYou can experiment with sharing your mobile or desktop UIs with the web. Compose Multiplatform for web is based on [Kotlin/Wasm](https://kotl.in/wasm), \nthe newest target for Kotlin Multiplatform projects. It allows Kotlin developers to run their code in the browser with \nall the benefits that WebAssembly has to offer, such as good and predictable performance for your applications.\n\n**[Get started with Compose Multiplatform for web](https://jb.gg/start-cmp)**\n\n## Libraries\n\n### Compose HTML\n\nCompose HTML is a library targeting [Kotlin/JS](https://kotlinlang.org/docs/js-overview.html) that provides Composable building blocks \nfor creating web user interfaces with HTML and CSS.    \n\n> Note that Compose HTML is not a multiplatform library. It can be used only with Kotlin/JS.\n\n## Learn more\n\n* [FAQ](https://jb.gg/cmp-faq)\n* [Samples](https://jb.gg/cmp-samples)\n* [Tutorials](tutorials/README.md)\n* [Compatibility and versioning](https://jb.gg/cmp-versioning)\n* [Changelog](CHANGELOG.md)\n* [Contibution guide](CONTRIBUTING.md)\n\n## Get help\n\nThere are dedicated public Slack channels for [#compose-ios](https://kotlinlang.slack.com/archives/C0346LWVBJ4/p1678888063176359), [#compose-desktop](https://kotlinlang.slack.com/archives/C01D6HTPATV) and [#compose-web](https://kotlinlang.slack.com/archives/C01F2HV7868/p1678887590205449), as well as the general [#compose](https://kotlinlang.slack.com/archives/CJLTWPH7S) channel.\n\nIf you encounter any issues, please report them on [YouTrack](https://youtrack.jetbrains.com/newIssue?project=CMP).\n\n",
      "stars_today": 5
    },
    {
      "id": 206483,
      "name": "maven",
      "full_name": "apache/maven",
      "description": "Apache Maven core",
      "html_url": "https://github.com/apache/maven",
      "stars": 4934,
      "forks": 2818,
      "language": "Java",
      "topics": [
        "apache-maven",
        "build-management",
        "hacktoberfest",
        "java",
        "maven"
      ],
      "created_at": "2009-05-21T03:22:03Z",
      "updated_at": "2026-01-27T23:24:57Z",
      "pushed_at": "2026-01-27T08:04:23Z",
      "open_issues": 723,
      "owner": {
        "login": "apache",
        "avatar_url": "https://avatars.githubusercontent.com/u/47359?v=4"
      },
      "readme": "<!---\n Licensed to the Apache Software Foundation (ASF) under one or more\n contributor license agreements.  See the NOTICE file distributed with\n this work for additional information regarding copyright ownership.\n The ASF licenses this file to You under the Apache License, Version 2.0\n (the \"License\"); you may not use this file except in compliance with\n the License.  You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n-->\nApache Maven\n============\n\n[![Apache License, Version 2.0, January 2004](https://img.shields.io/github/license/apache/maven.svg?label=License)][license]\n[![Reproducible Builds](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/jvm-repo-rebuild/reproducible-central/master/content/org/apache/maven/maven/badge.json)](https://github.com/jvm-repo-rebuild/reproducible-central/blob/master/content/org/apache/maven/maven/README.md)\n\n- [master](https://github.com/apache/maven) = 4.1.x\n[![Jenkins Build](https://img.shields.io/jenkins/build?jobUrl=https%3A%2F%2Fci-maven.apache.org%2Fjob%2FMaven%2Fjob%2Fmaven-box%2Fjob%2Fmaven%2Fjob%2Fmaster%2F)\n  ][build]\n[![Java CI](https://github.com/apache/maven/actions/workflows/maven.yml/badge.svg?branch=master)][gh-build]\n\n- [4.0.x](https://github.com/apache/maven/tree/maven-4.0.x): \n[![Maven Central](https://img.shields.io/maven-central/v/org.apache.maven/apache-maven.svg?label=Maven%20Central&versionPrefix=4.0)](https://central.sonatype.com/artifact/org.apache.maven/apache-maven)\n[![Jenkins Build](https://img.shields.io/jenkins/build?jobUrl=https%3A%2F%2Fci-maven.apache.org%2Fjob%2FMaven%2Fjob%2Fmaven-box%2Fjob%2Fmaven%2Fjob%2Fmaven-4.0.x%2F)][build-4.0]\n[![Java CI](https://github.com/apache/maven/actions/workflows/maven.yml/badge.svg?branch=maven-4.0.x)][gh-build-4.0]\n\n- [3.9.x](https://github.com/apache/maven/tree/maven-3.9.x): \n[![Maven Central](https://img.shields.io/maven-central/v/org.apache.maven/apache-maven.svg?label=Maven%20Central&versionPrefix=3.)](https://central.sonatype.com/artifact/org.apache.maven/apache-maven)\n![Jenkins Build](https://img.shields.io/jenkins/build?jobUrl=https%3A%2F%2Fci-maven.apache.org%2Fjob%2FMaven%2Fjob%2Fmaven-box%2Fjob%2Fmaven%2Fjob%2Fmaven-3.9.x%2F)\n[![Java CI](https://github.com/apache/maven/actions/workflows/maven.yml/badge.svg?branch=maven-3.9.x)][gh-build-3.9]\n\nApache Maven is a software project management and comprehension tool. Based on\nthe concept of a project object model (POM), Maven can manage a project's\nbuild, reporting and documentation from a central piece of information.\n\nIf you think you have found a bug, please file an issue in the [Maven Issue Tracker](https://github.com/apache/maven/issues).\n\nDocumentation\n-------------\n\nMore information can be found on [Apache Maven Homepage][maven-home].\nQuestions related to the usage of Maven should be posted on\nthe [Maven User List][users-list].\n\n\nWhere can I get the latest release?\n-----------------------------------\nYou can download the release source from our [download page][maven-download].\n\nContributing\n------------\n\nIf you are interested in the development of Maven, please consult the\ndocumentation first and afterward you are welcome to join the developers\nmailing list to ask questions or discuss new ideas/features/bugs etc.\n\nTake a look into the [contribution guidelines](CONTRIBUTING.md).\n\nLicense\n-------\nThis code is under the [Apache License, Version 2.0, January 2004][license].\n\nSee the [`NOTICE`](./NOTICE) file for required notices and attributions.\n\nDonations\n---------\nDo you like Apache Maven? Then [donate back to the ASF](https://www.apache.org/foundation/contributing.html) to support the development.\n\nQuick Build\n-------\nIf you want to bootstrap Maven, you'll need:\n- Java 17+\n- Maven 3.6.3 or later\n- Run Maven, specifying a location into which the completed Maven distro should be installed:\n    ```\n    mvn -DdistributionTargetDir=\"$HOME/app/maven/apache-maven-4.1.x-SNAPSHOT\" clean package\n    ```\n\n\n[home]: https://maven.apache.org/\n[license]: https://www.apache.org/licenses/LICENSE-2.0\n[build]: https://ci-maven.apache.org/job/Maven/job/maven-box/job/maven/job/master/\n[build-4.0]: https://ci-maven.apache.org/job/Maven/job/maven-box/job/maven/job/maven-4.0.x/\n[build-3.9]: https://ci-maven.apache.org/job/Maven/job/maven-box/job/maven/job/maven-3.9.x/\n[gh-build]: https://github.com/apache/maven/actions/workflows/maven.yml?query=branch%3Amaster\n[gh-build-4.0]: https://github.com/apache/maven/actions/workflows/maven.yml?query=branch%3Amaven-4.0.x\n[gh-build-3.9]: https://github.com/apache/maven/actions/workflows/maven.yml?query=branch%3Amaven-3.9.x\n[maven-home]: https://maven.apache.org/\n[maven-download]: https://maven.apache.org/download.cgi\n[users-list]: https://maven.apache.org/mailing-lists.html\n[dev-ml-list]: https://www.mail-archive.com/dev@maven.apache.org/\n[code-style]: http://maven.apache.org/developers/conventions/code.html\n[core-it]: https://maven.apache.org/core-its/core-it-suite/\n[building-maven]: https://maven.apache.org/guides/development/guide-building-maven.html\n[cla]: https://www.apache.org/licenses/#clas\n\n",
      "stars_today": 5
    },
    {
      "id": 178079595,
      "name": "opencost",
      "full_name": "opencost/opencost",
      "description": "Cost monitoring for Kubernetes workloads and cloud costs",
      "html_url": "https://github.com/opencost/opencost",
      "stars": 6305,
      "forks": 730,
      "language": "Go",
      "topics": [
        "aws",
        "azure",
        "cncf",
        "cost",
        "cost-optimization",
        "finops",
        "gcp",
        "k8s",
        "kubernetes",
        "monitoring",
        "opencost",
        "prometheus"
      ],
      "created_at": "2019-03-27T21:49:05Z",
      "updated_at": "2026-01-28T01:17:51Z",
      "pushed_at": "2026-01-27T23:24:47Z",
      "open_issues": 167,
      "owner": {
        "login": "opencost",
        "avatar_url": "https://avatars.githubusercontent.com/u/105945214?v=4"
      },
      "readme": "[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/6219/badge)](https://www.bestpractices.dev/projects/6219)\n[![Gurubase](https://img.shields.io/badge/Gurubase-Ask%20OpenCost%20Guru-006BFF)](https://gurubase.io/g/opencost)\n\n![](./opencost-header.png)\n\n# OpenCost â€” your favorite open source cost monitoring tool for Kubernetes and cloud spend\n\nOpenCost give teams visibility into current and historical Kubernetes and cloud spend and resource allocation.\nThese models provide cost transparency in Kubernetes environments that support multiple applications, teams, departments, etc.\nIt also provides visibility into the cloud costs across multiple providers.\n\nOpenCost was originally developed and open sourced by [Kubecost](https://kubecost.com). This project combines a [specification](/spec/) as well as a Golang implementation of these detailed requirements. The web UI is available in the [opencost/opencost-ui](http://github.com/opencost/opencost-ui) repository.\n\n[![OpenCost UI Walkthrough](./ui/src/thumbnail.png)](https://youtu.be/lCP4Ci9Kcdg)\n*OpenCost UI Walkthrough*\n\nTo see the full functionality of OpenCost you can view [OpenCost features](https://opencost.io). Here is a summary of features enabled:\n\n- Real-time cost allocation by Kubernetes cluster, node, namespace, controller kind, controller, service, or pod\n- Multi-cloud cost monitoring for all cloud services on AWS, Azure, GCP\n- Dynamic on-demand k8s asset pricing enabled by integrations with AWS, Azure, and GCP billing APIs\n- Supports on-prem k8s clusters with custom CSV pricing\n- Allocation for in-cluster K8s resources like CPU, GPU, memory, and persistent volumes\n- Easily export pricing data to Prometheus with /metrics endpoint ([learn more](https://www.opencost.io/docs/installation/prometheus))\n- Carbon costs for cloud resources\n- MCP support\n- Support for external costs like Datadog through [OpenCost Plugins](https://github.com/opencost/opencost-plugins)\n- Free and open source distribution ([Apache2 license](LICENSE))\n\n## Getting Started\n\nOpenCost is now installed and managed via the official Helm chart only.\n\nQuick install on any Kubernetes 1.20+ cluster:\n\n```bash\nhelm repo add opencost https://opencost.github.io/opencost-helm-chart\nhelm repo update\nhelm install opencost opencost/opencost\n```\n\nNote: The standalone Kubernetes manifest files have been removed. Please use Helm for all installations and upgrades. See the [Helm installation docs](https://www.opencost.io/docs/installation/install) for details and configuration.\n\n> **Note for sharded Prometheus users:**\n> If you run Prometheus in a sharded (HA) setup, set `PROMETHEUS_SERVER_ENDPOINT` to a global query endpoint (e.g., Thanos Query, Cortex, or Mimir). Pointing to a single Prometheus pod may result in incomplete or intermittent export results. See the [Prometheus integration docs](https://www.opencost.io/docs/installation/prometheus) for details.\n\n## Usage\n\n- [Cost APIs](https://www.opencost.io/docs/integrations/api)\n- [CLI / kubectl cost](https://www.opencost.io/docs/integrations/kubectl-cost)\n- [Prometheus Metrics](https://www.opencost.io/docs/integrations/prometheus)\n- [User Interface](https://www.opencost.io/docs/installation/ui)\n\n## MCP Server\n\nThe OpenCost MCP (Model Context Protocol) server provides AI agents with access to cost allocation and asset data through a standardized interface. The MCP server is **enabled by default** in all OpenCost deployments, runs on port 8081, and is **built into the Helm chart** for easy production deployment. Users have full control to disable it or configure custom ports and settings.\n\n### Features\n\n- **Enabled by Default**: MCP server starts automatically with OpenCost\n- **Full User Control**: Easy to disable or configure port and settings\n- **Allocation Queries**: Retrieve cost allocation data with filtering and aggregation\n- **Asset Queries**: Access detailed asset information including nodes, disks, load balancers, and more\n- **Cloud Cost Queries**: Query cloud cost data with provider, service, and region filtering\n- **HTTP Transport**: Uses HTTP for reliable communication with MCP clients\n- **Zero Configuration**: Works out of the box with default OpenCost deployment\n- **Helm Integration**: Built into the official Helm chart for production deployments\n\n### Quick Start\n\n#### Using Tilt (Development)\n```bash\n# Clone and start OpenCost with MCP server\ngit clone https://github.com/opencost/opencost.git\ncd opencost\ntilt up\n```\n\nTilt configuration notes (cloud costs):\n\nOpenCost's Tilt values (`tilt-values.yaml`) include extra environment variables to enable Cloud Cost ingestion in dev:\n\n```yaml\n# tilt-values.yaml (excerpt)\nopencost:\n  exporter:\n    extraEnv:\n      CLOUD_COST_ENABLED: \"true\"\n      CLOUD_COST_CONFIG_PATH: \"/var/cloud-integration/cloud-integration.json\"\n```\n\n- Set `CLOUD_COST_ENABLED` to \"true\" to turn on cloud cost ingestion.\n- Point `CLOUD_COST_CONFIG_PATH` to the mounted cloud integration file used by Tilt (e.g., `/var/cloud-integration/cloud-integration.json`).\n- Adjust other values in `tilt-values.yaml` as needed during development.\n\n#### Using Helm (Production)\n```bash\n# Add the OpenCost Helm repository\nhelm repo add opencost https://opencost.github.io/opencost-helm-chart\nhelm repo update\n\n# Deploy OpenCost with MCP server (enabled by default)\nhelm install opencost opencost/opencost\n\n# Access MCP server via port forwarding (example)\nkubectl port-forward svc/opencost 8081:8081\n```\n\nThe MCP server is **enabled by default** in the Helm chart. For custom configuration:\n\n```bash\n# Deploy with MCP server disabled\nhelm install opencost opencost/opencost \\\n  --set opencost.mcp.enabled=false\n\n# Deploy with custom MCP port\nhelm install opencost opencost/opencost \\\n  --set opencost.mcp.port=9091\n\n# Deploy with debug logging\nhelm install opencost opencost/opencost \\\n  --set opencost.mcp.extraEnv.MCP_LOG_LEVEL=debug\n```\n\n#### Configuration Summary\n\n| Configuration | Command | Description |\n|---------------|---------|-------------|\n| **Default** | `helm install opencost opencost/opencost` | MCP enabled on port 8081 |\n| **Disable** | `--set opencost.mcp.enabled=false` | Completely disable MCP server |\n| **Custom Port** | `--set opencost.mcp.port=9091` | Use different port |\n| **Debug Mode** | `--set opencost.mcp.extraEnv.MCP_LOG_LEVEL=debug` | Enable debug logging |\n\n### MCP Client Configuration\n\nConfigure your MCP client (e.g., Cursor) to connect to the OpenCost MCP server:\n\n**Default configuration (port 8081):**\n```json\n{\n  \"mcpServers\": {\n    \"opencost\": {\n      \"type\": \"http\",\n      \"url\": \"http://localhost:8081\"\n    }\n  }\n}\n```\n\n**Custom port configuration:**\n```json\n{\n  \"mcpServers\": {\n    \"opencost\": {\n      \"type\": \"http\",\n      \"url\": \"http://localhost:9091\"\n    }\n  }\n}\n```\n\n**For Kubernetes deployments:**\n```json\n{\n  \"mcpServers\": {\n    \"opencost\": {\n      \"type\": \"http\",\n      \"url\": \"http://opencost.opencost.svc.cluster.local:8081\"\n    }\n  }\n}\n```\n\n**For external access (with LoadBalancer/Ingress):**\n```json\n{\n  \"mcpServers\": {\n    \"opencost\": {\n      \"type\": \"http\",\n      \"url\": \"http://your-opencost-domain.com:8081\"\n    }\n  }\n}\n```\n\n### Available MCP Tools\n\nThe MCP server provides these tools for AI agents:\n\n#### `get_allocation_costs`\nRetrieve cost allocation data with filtering and aggregation.\n\n**Parameters:**\n- `window` (required): Time window (e.g., \"7d\", \"1h\", \"30m\")\n- `aggregate` (optional): Aggregation properties (e.g., \"namespace\", \"pod\", \"node\")\n- `step` (optional): Resolution step size\n- `accumulate` (optional): Whether to accumulate over time\n- `share_idle` (optional): Whether to share idle costs\n- `include_idle` (optional): Whether to include idle resources\n\n#### `get_asset_costs`\nRetrieve asset cost data including nodes, disks, load balancers, and more.\n\n**Parameters:**\n- `window` (required): Time window (e.g., \"7d\", \"1h\", \"30m\")\n\n#### `get_cloud_costs`\nRetrieve cloud cost data with provider, service, and region filtering.\n\n**Parameters:**\n- `window` (required): Time window (e.g., \"7d\", \"1h\", \"30m\")\n- `aggregate` (optional): Aggregation properties (e.g., \"provider\", \"service\", \"region\")\n- `accumulate` (optional): Time accumulation (\"day\", \"week\", \"month\")\n- `provider` (optional): Filter by cloud provider (e.g., \"aws\", \"gcp\", \"azure\")\n- `service` (optional): Filter by service (e.g., \"ec2\", \"compute\", \"s3\")\n- `category` (optional): Filter by category (e.g., \"compute\", \"storage\", \"network\")\n- `region` (optional): Filter by region (e.g., \"us-west-1\", \"us-central1\")\n- `accountID` (optional): Filter by account ID\n\n### Supported Asset Types\n\n- **Node**: Compute instances with CPU, RAM, GPU details\n- **Disk**: Storage volumes with usage and cost breakdown\n- **LoadBalancer**: Load balancer instances with IP and private status\n- **Network**: Network-related costs and usage\n- **Cloud**: Cloud service costs with credit information\n- **ClusterManagement**: Kubernetes cluster management costs\n\n### Example Usage\n\nOnce configured, AI agents can query cost data like:\n\n```javascript\n// Get cost allocation for the last 7 days\nconst allocation = await mcpClient.callTool('get_allocation_costs', {\n  window: '7d',\n  aggregate: 'namespace,node'\n});\n\n// Get asset costs for the last 24 hours\nconst assets = await mcpClient.callTool('get_asset_costs', {\n  window: '1d'\n});\n\n// Get cloud costs for AWS EC2 in us-west-1\nconst cloudCosts = await mcpClient.callTool('get_cloud_costs', {\n  window: '7d',\n  aggregate: 'service',\n  provider: 'aws',\n  service: 'ec2',\n  accumulate: 'day',\n  filter: 'regionID:\"us-west-1\"'\n});\n```\n\nFor detailed setup instructions and advanced configuration, see the [Helm chart documentation](https://github.com/opencost/opencost-helm-chart/blob/main/charts/opencost/README.md#mcp-server).\n\n## Contributing\n\nWe :heart: pull requests! See [`CONTRIBUTING.md`](CONTRIBUTING.md) for information on building the project from source and contributing changes.\n\n## Community\n\nIf you need any support or have any questions on contributing to the project, you can reach us on [CNCF Slack](https://slack.cncf.io/) in the [#opencost](https://cloud-native.slack.com/archives/C03D56FPD4G) channel or attend the biweekly [OpenCost Working Group community meeting](https://bit.ly/opencost-meeting) from the [Community Calendar](https://bit.ly/opencost-calendar) to discuss OpenCost development.\n\n## FAQ\n\nYou can view [OpenCost documentation](https://www.opencost.io/docs/FAQ) for a list of commonly asked questions.\n",
      "stars_today": 5
    },
    {
      "id": 1046959217,
      "name": "TEESimulator",
      "full_name": "JingMatrix/TEESimulator",
      "description": "Software simulation for Android hardware-backed key pairs with key attestation",
      "html_url": "https://github.com/JingMatrix/TEESimulator",
      "stars": 596,
      "forks": 42,
      "language": "Kotlin",
      "topics": [
        "android",
        "hardware-simulation",
        "playintegrity",
        "tee",
        "trickystore"
      ],
      "created_at": "2025-08-29T14:02:27Z",
      "updated_at": "2026-01-28T02:00:10Z",
      "pushed_at": "2026-01-27T18:41:43Z",
      "open_issues": 11,
      "owner": {
        "login": "JingMatrix",
        "avatar_url": "https://avatars.githubusercontent.com/u/24476093?v=4"
      },
      "readme": "# TEESimulator â€“ A Full TEE Emulation Framework\n\n**TEESimulator** is a system module designed to create a complete, software-based simulation of a hardware-backed Trusted Execution Environment ([TEE](https://source.android.com/docs/security/features/trusty)) for [Key Attestation](https://developer.android.com/privacy-and-security/security-key-attestation).\n\nThe project's goal is to move beyond simple certificate patching and build a robust framework that can create and manage virtual, self-consistent cryptographic keys.\n\n## âœ¨ Core Principles\n\n*   **Bypass Hardware-Backed Attestation:** The primary goal of this project is to defeat Key Attestation, a security mechanism that allows apps to verify that they are running on a secure, unmodified device. This module provides the tools to bypass these checks on rooted or modified devices.\n*   **Stateful Emulation:** Instead of patching responses from the real TEE, the ultimate goal is to create and manage virtual keys entirely in a simulated software environment. Any request concerning a virtual key will be handled by the simulator, ensuring perfect consistency without ever touching the real hardware.\n*   **Architectural Interception:** By hooking low-level Binder IPC calls to the Keystore, the framework can transparently redirect requests for virtual keys to the software-based simulator, while allowing requests for real keys to pass through to the hardware TEE.\n*   **100% FOSS:** Licensed under GPLv3, ensuring it stays free, auditable, and compliant with open-source laws.\n\n## ğŸ“± Requirements\n- Android 10 or above\n\n## ğŸ“¦ Installation & Configuration\n\n1.  Flash this module via (Magisk / KernelSU / APatch) and reboot. It will replace [TrickyStore](https://github.com/5ec1cff/TrickyStore), [TrickyStoreOSS](https://github.com/beakthoven/TrickyStoreOSS) and their forks.\n2.  (Optional) Place a hardware-backed `keybox.xml` at `/data/adb/tricky_store/keybox.xml`. This provides the cryptographic \"root of trust\" for the simulator.\n3.  (Optional) Customize target packages in `/data/adb/tricky_store/target.txt`.\n4.  (Optional) Customize the simulated security patch level in `/data/adb/tricky_store/security_patch.txt`.\n5.  Enjoy!\n\n**All configuration files are monitored and will take effect immediately upon saving.**\n\n### The `keybox.xml` Root of Trust\n\nThis file provides the master cryptographic identity for the simulator. It contains a private key and a valid, hardware-backed certificate chain from a real device. The simulator uses this to sign the virtual certificates it generates, making them appear legitimate to verifiers.\n\n```xml\n<?xml version=\"1.0\"?>\n<AndroidAttestation>\n    <Keybox DeviceID=\"...\">\n        <Key algorithm=\"ecdsa|rsa\">\n            <PrivateKey format=\"pem\">...</PrivateKey>\n            <CertificateChain>...</CertificateChain>\n        </Key>\n    </Keybox>\n</AndroidAttestation>\n```\n\n### Mode and Keybox Configuration (`target.txt`)\n\nTEESimulator currently operates in two primary modes as it transitions towards full emulation.\nYou can control the simulation mode and the specific keybox.xml file used on a per-package basis.\n\n#### Mode Suffixes\n\n*   **`!` â†’ Force Generation Mode:** Creates a complete, software-based virtual key. This is the foundation of the full TEE simulation.\n*   **`?` â†’ Force Leaf Hacking Mode:** A legacy mode where a real TEE key is generated, but its attestation certificate is intercepted and modified.\n*   **No symbol â†’ Automatic Mode:** The module selects the most appropriate mode for the device.\n\n#### Multi-Keybox Configuration\n\nYou can specify different keybox files for different groups of applications. This is done by adding a line with the filename in square brackets (e.g., [demo_keybox.xml]).\n\nAll applications listed after this line will use the specified keybox file, until a new keybox is declared. Applications listed before any custom keybox declaration will use the default `keybox.xml`.\n\nFor example:\n```\n# These two apps will use the default /data/adb/tricky_store/keybox.xml\ncom.google.android.gms!\nio.github.vvb2060.keyattestation?\n\n# Switch to a different keybox for the following apps.\n# The file must be located at /data/adb/tricky_store/aosp_keybox.xml\n[aosp_keybox.xml]\ncom.google.android.gsf\n\n# Switch again to another keybox.\n# The file must be located at /data/adb/tricky_store/demo_keybox.xml\n[demo_keybox.xml]\norg.matrix.demo\n```\n\n### Security Patch Level (`security_patch.txt`)\n\nThis file allows you to configure the `osPatchLevel`, `vendorPatchLevel`, and `bootPatchLevel` that the simulator will report in its patched or forged attestation certificates.\n\n**Note:** This only affects the Key Attestation data generated by the simulator. It does not change the actual system properties of your device.\n\n#### Global and Per-Package Configuration\n\nYou can set a global patch level that applies to all applications, and you can also override these settings for specific packages. The syntax is hierarchical:\n\n*   Settings defined at the top of the file, before any `[package.name]` line, are **global** and serve as the default for all apps.\n*   To create a specific configuration for an application, add its package name in square brackets (e.g., `[com.google.android.gms]`). All settings following this line will apply *only* to that package until a new package context is declared.\n\n#### Configuration Keys and Values\n\nYou can specify the patch level for the following components using a `key=value` format:\n\n*   `system`: The main OS patch level.\n*   `vendor`: The vendor patch level.\n*   `boot`: The boot/kernel patch level.\n*   `all`: A convenient shorthand to set the same date for `system`, `vendor`, and `boot` simultaneously. Any individual key can still be used to override the value set by `all`.\n\nDates should be provided in `YYYY-MM-DD` format (e.g., `2025-11-05`).\n\n#### Special Keywords\n\nIn addition to static dates, several special keywords provide advanced, dynamic control:\n\n*   **`today`**: Dynamically uses the current date every time an attestation is generated. This ensures the device always appears up-to-date without needing manual edits.\n\n*   **Date Templates**: You can create semi-dynamic dates using `YYYY`, `MM`, and `DD` as placeholders for the current year, month, and day. For example, `YYYY-MM-05` will always resolve to the 5th of the current month and year.\n\n*   **`no`**: This keyword instructs the simulator to **completely omit** the corresponding patch level tag from the generated attestation.\n\n*   **`device_default`**: This keyword forces the simulator to fall back and use the device's **real hardware value** for that specific patch level. This is essential for creating exceptions to a global override or an `all` rule.\n\n#### Example Configuration\n\nThis example demonstrates how to combine global settings, per-package overrides, and special keywords for fine-grained control.\n\n```\n# --- Global Configuration ---\n# This is the default for all apps unless specified otherwise.\n# - Forge a recent system patch level, the 5th of the current month (a common patch date).\n# - Use the device's real vendor patch level.\n# - Do not report a boot patch level at all.\nsystem=YYYY-MM-05\nvendor=device_default\nboot=no\n\n# --- Per-Package Override for Google Play Services ---\n# This app will report an older, specific date for its system patch.\n# It will inherit the global settings for vendor (device_default) and boot (no).\n[com.google.android.gms]\nsystem=2024-10-01\n\n# --- Per-Package Override for a Demo App ---\n# This app gets a completely custom configuration.\n[org.matrix.demo]\n# Set a base date for all patch levels...\nall=2025-09-15\n# ...but make an exception: use the real boot patch level instead of the one from 'all'.\nboot=device_default\n```\n",
      "stars_today": 5
    },
    {
      "id": 39840932,
      "name": "googletest",
      "full_name": "google/googletest",
      "description": "GoogleTest - Google Testing and Mocking Framework",
      "html_url": "https://github.com/google/googletest",
      "stars": 38142,
      "forks": 10679,
      "language": "C++",
      "topics": [],
      "created_at": "2015-07-28T15:07:53Z",
      "updated_at": "2026-01-27T21:58:22Z",
      "pushed_at": "2026-01-27T17:36:04Z",
      "open_issues": 509,
      "owner": {
        "login": "google",
        "avatar_url": "https://avatars.githubusercontent.com/u/1342004?v=4"
      },
      "readme": "# GoogleTest\n\n### Announcements\n\n#### Documentation Updates\n\nOur documentation is now live on GitHub Pages at\nhttps://google.github.io/googletest/. We recommend browsing the documentation on\nGitHub Pages rather than directly in the repository.\n\n#### Release 1.17.0\n\n[Release 1.17.0](https://github.com/google/googletest/releases/tag/v1.17.0) is\nnow available.\n\nThe 1.17.x branch\n[requires at least C++17](https://opensource.google/documentation/policies/cplusplus-support#c_language_standard).\n\n#### Continuous Integration\n\nWe use Google's internal systems for continuous integration.\n\n#### Coming Soon\n\n*   We are planning to take a dependency on\n    [Abseil](https://github.com/abseil/abseil-cpp).\n\n## Welcome to **GoogleTest**, Google's C++ test framework!\n\nThis repository is a merger of the formerly separate GoogleTest and GoogleMock\nprojects. These were so closely related that it makes sense to maintain and\nrelease them together.\n\n### Getting Started\n\nSee the [GoogleTest User's Guide](https://google.github.io/googletest/) for\ndocumentation. We recommend starting with the\n[GoogleTest Primer](https://google.github.io/googletest/primer.html).\n\nMore information about building GoogleTest can be found at\n[googletest/README.md](googletest/README.md).\n\n## Features\n\n*   xUnit test framework: \\\n    Googletest is based on the [xUnit](https://en.wikipedia.org/wiki/XUnit)\n    testing framework, a popular architecture for unit testing\n*   Test discovery: \\\n    Googletest automatically discovers and runs your tests, eliminating the need\n    to manually register your tests\n*   Rich set of assertions: \\\n    Googletest provides a variety of assertions, such as equality, inequality,\n    exceptions, and more, making it easy to test your code\n*   User-defined assertions: \\\n    You can define your own assertions with Googletest, making it simple to\n    write tests that are specific to your code\n*   Death tests: \\\n    Googletest supports death tests, which verify that your code exits in a\n    certain way, making it useful for testing error-handling code\n*   Fatal and non-fatal failures: \\\n    You can specify whether a test failure should be treated as fatal or\n    non-fatal with Googletest, allowing tests to continue running even if a\n    failure occurs\n*   Value-parameterized tests: \\\n    Googletest supports value-parameterized tests, which run multiple times with\n    different input values, making it useful for testing functions that take\n    different inputs\n*   Type-parameterized tests: \\\n    Googletest also supports type-parameterized tests, which run with different\n    data types, making it useful for testing functions that work with different\n    data types\n*   Various options for running tests: \\\n    Googletest provides many options for running tests including running\n    individual tests, running tests in a specific order and running tests in\n    parallel\n\n## Supported Platforms\n\nGoogleTest follows Google's\n[Foundational C++ Support Policy](https://opensource.google/documentation/policies/cplusplus-support).\nSee\n[this table](https://github.com/google/oss-policies-info/blob/main/foundational-cxx-support-matrix.md)\nfor a list of currently supported versions of compilers, platforms, and build\ntools.\n\n## Who Is Using GoogleTest?\n\nIn addition to many internal projects at Google, GoogleTest is also used by the\nfollowing notable projects:\n\n*   The [Chromium projects](https://www.chromium.org/) (behind the Chrome\n    browser and Chrome OS).\n*   The [LLVM](https://llvm.org/) compiler.\n*   [Protocol Buffers](https://github.com/google/protobuf), Google's data\n    interchange format.\n*   The [OpenCV](https://opencv.org/) computer vision library.\n\n## Related Open Source Projects\n\n[GTest Runner](https://github.com/nholthaus/gtest-runner) is a Qt5 based\nautomated test-runner and Graphical User Interface with powerful features for\nWindows and Linux platforms.\n\n[GoogleTest UI](https://github.com/ospector/gtest-gbar) is a test runner that\nruns your test binary, allows you to track its progress via a progress bar, and\ndisplays a list of test failures. Clicking on one shows failure text. GoogleTest\nUI is written in C#.\n\n[GTest TAP Listener](https://github.com/kinow/gtest-tap-listener) is an event\nlistener for GoogleTest that implements the\n[TAP protocol](https://en.wikipedia.org/wiki/Test_Anything_Protocol) for test\nresult output. If your test runner understands TAP, you may find it useful.\n\n[gtest-parallel](https://github.com/google/gtest-parallel) is a test runner that\nruns tests from your binary in parallel to provide significant speed-up.\n\n[GoogleTest Adapter](https://marketplace.visualstudio.com/items?itemName=DavidSchuldenfrei.gtest-adapter)\nis a VS Code extension allowing to view GoogleTest in a tree view and run/debug\nyour tests.\n\n[C++ TestMate](https://github.com/matepek/vscode-catch2-test-adapter) is a VS\nCode extension allowing to view GoogleTest in a tree view and run/debug your\ntests.\n\n[Cornichon](https://pypi.org/project/cornichon/) is a small Gherkin DSL parser\nthat generates stub code for GoogleTest.\n\n## Contributing Changes\n\nPlease read\n[`CONTRIBUTING.md`](https://github.com/google/googletest/blob/main/CONTRIBUTING.md)\nfor details on how to contribute to this project.\n\nHappy testing!\n",
      "stars_today": 4
    },
    {
      "id": 8205602,
      "name": "mybatis-3",
      "full_name": "mybatis/mybatis-3",
      "description": "MyBatis SQL mapper framework for Java",
      "html_url": "https://github.com/mybatis/mybatis-3",
      "stars": 20363,
      "forks": 12971,
      "language": "Java",
      "topics": [
        "java",
        "mybatis",
        "sql"
      ],
      "created_at": "2013-02-14T19:03:32Z",
      "updated_at": "2026-01-28T00:53:50Z",
      "pushed_at": "2026-01-27T03:23:54Z",
      "open_issues": 204,
      "owner": {
        "login": "mybatis",
        "avatar_url": "https://avatars.githubusercontent.com/u/1483254?v=4"
      },
      "readme": "MyBatis SQL Mapper Framework for Java\n=====================================\n\n[![build](https://github.com/mybatis/mybatis-3/actions/workflows/ci.yaml/badge.svg)](https://github.com/mybatis/mybatis-3/actions?query=workflow%3A%22Java+CI%22)\n[![Coverage Status](https://coveralls.io/repos/mybatis/mybatis-3/badge.svg?branch=master&service=github)](https://coveralls.io/github/mybatis/mybatis-3?branch=master)\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=mybatis_mybatis-3&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=mybatis_mybatis-3)\n[![Maven central](https://maven-badges.herokuapp.com/maven-central/org.mybatis/mybatis/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.mybatis/mybatis)\n[![Sonatype Nexus (Snapshots)](https://img.shields.io/nexus/s/https/oss.sonatype.org/org.mybatis/mybatis.svg)](https://oss.sonatype.org/content/repositories/snapshots/org/mybatis/mybatis/)\n[![License](https://img.shields.io/:license-apache-brightgreen.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)\n[![Stack Overflow](https://img.shields.io/:stack%20overflow-mybatis-brightgreen.svg)](https://stackoverflow.com/questions/tagged/mybatis)\n[![Project Stats](https://www.openhub.net/p/mybatis/widgets/project_thin_badge.gif)](https://www.openhub.net/p/mybatis)\n\n![mybatis](https://mybatis.org/images/mybatis-logo.png)\n\nThe MyBatis SQL mapper framework makes it easier to use a relational database with object-oriented applications.\nMyBatis couples objects with stored procedures or SQL statements using an XML descriptor or annotations.\nSimplicity is the biggest advantage of the MyBatis data mapper over object relational mapping tools.\n\nEssentials\n----------\n\n* [See the docs](https://mybatis.org/mybatis-3)\n* [Download Latest](https://github.com/mybatis/mybatis-3/releases)\n* [Download Snapshot](https://oss.sonatype.org/content/repositories/snapshots/org/mybatis/mybatis/)\n\nContributions\n-------------\n\nSee [here](CONTRIBUTING.md)\n\nTests\n-----\n\nMybatis-3 code runs more expressive testing depending on jdk usage and platform.\n\nBy default, we set ```<excludedGroups>TestcontainersTests,RequireIllegalAccess</excludedGroups>``` which will exclude a subset of tests with @Tag('TestcontainersTests') and @Tag('RequireIllegalAccess').\n\nWhen we run on ci platform, we further make adjustments as needed.  See [here](.github/workflows/ci.yaml) for details.\n\nAs of 12/28/2024, using combined system + jdk will result in given number of tests ran.  This will change as tests are added or removed over time.\n\nwithout adjusting settings (ie use as is, platform does not matter)\n\n- any OS + jdk 17 = 1899 tests\n- any OS + jdk 21 = 1899 tests\n- any OS + jdk 23 = 1899 tests\n- any OS + jdk 24 = 1899 tests\n- any OS + jdk 25 = 1899 tests\n\nour adjustments for GH actions where platform does matter\n\n- windows + jdk 17 = 1899 tests\n- windows + jdk 21 = 1899 tests\n- windows + jdk 23 = 1899 tests\n- windows + jdk 24 = 1899 tests\n- windows + jdk 25 = 1899 tests\n\n- linux + jdk 17 = 1934 tests\n- linux + jdk 21 = 1934 tests\n- linux + jdk 23 = 1934 tests\n- linux + jdk 24 = 1934 tests\n- linux + jdk 25 = 1934 tests\n\n- mac + jdk 17 = 1899 tests\n- mac + jdk 21 = 1899 tests\n- mac + jdk 23 = 1899 tests\n- mac + jdk 24 = 1899 tests\n- mac + jdk 25 = 1899 tests\n",
      "stars_today": 4
    },
    {
      "id": 104231541,
      "name": "abseil-cpp",
      "full_name": "abseil/abseil-cpp",
      "description": "Abseil Common Libraries (C++)",
      "html_url": "https://github.com/abseil/abseil-cpp",
      "stars": 16980,
      "forks": 2956,
      "language": "C++",
      "topics": [],
      "created_at": "2017-09-20T15:10:30Z",
      "updated_at": "2026-01-27T21:20:15Z",
      "pushed_at": "2026-01-22T20:16:23Z",
      "open_issues": 210,
      "owner": {
        "login": "abseil",
        "avatar_url": "https://avatars.githubusercontent.com/u/26718316?v=4"
      },
      "readme": "# Abseil - C++ Common Libraries\n\nThe repository contains the Abseil C++ library code. Abseil is an open-source\ncollection of C++ code (compliant to C++17) designed to augment the C++\nstandard library.\n\n## Table of Contents\n\n- [About Abseil](#about)\n- [Quickstart](#quickstart)\n- [Building Abseil](#build)\n- [Support](#support)\n- [Codemap](#codemap)\n- [Releases](#releases)\n- [License](#license)\n- [Links](#links)\n\n<a name=\"about\"></a>\n## About Abseil\n\nAbseil is an open-source collection of C++ library code designed to augment\nthe C++ standard library. The Abseil library code is collected from Google's\nown C++ code base, has been extensively tested and used in production, and\nis the same code we depend on in our daily coding lives.\n\nIn some cases, Abseil provides pieces missing from the C++ standard; in\nothers, Abseil provides alternatives to the standard for special needs\nwe've found through usage in the Google code base. We denote those cases\nclearly within the library code we provide you.\n\nAbseil is not meant to be a competitor to the standard library; we've\njust found that many of these utilities serve a purpose within our code\nbase, and we now want to provide those resources to the C++ community as\na whole.\n\n<a name=\"quickstart\"></a>\n## Quickstart\n\nIf you want to just get started, make sure you at least run through the\n[Abseil Quickstart](https://abseil.io/docs/cpp/quickstart). The Quickstart\ncontains information about setting up your development environment, downloading\nthe Abseil code, running tests, and getting a simple binary working.\n\n<a name=\"build\"></a>\n## Building Abseil\n\n[Bazel](https://bazel.build) and [CMake](https://cmake.org/) are the official\nbuild systems for Abseil.\nSee the [quickstart](https://abseil.io/docs/cpp/quickstart) for more information\non building Abseil using the Bazel build system.\nIf you require CMake support, please check the [CMake build\ninstructions](CMake/README.md) and [CMake\nQuickstart](https://abseil.io/docs/cpp/quickstart-cmake).\n\n<a name=\"support\"></a>\n## Support\n\nAbseil follows Google's [Foundational C++ Support\nPolicy](https://opensource.google/documentation/policies/cplusplus-support). See\n[this\ntable](https://github.com/google/oss-policies-info/blob/main/foundational-cxx-support-matrix.md)\nfor a list of currently supported versions compilers, platforms, and build\ntools.\n\n<a name=\"codemap\"></a>\n## Codemap\n\nAbseil contains the following C++ library components:\n\n* [`base`](absl/base/)\n  <br /> The `base` library contains initialization code and other code which\n  all other Abseil code depends on. Code within `base` may not depend on any\n  other code (other than the C++ standard library).\n* [`algorithm`](absl/algorithm/)\n  <br /> The `algorithm` library contains additions to the C++ `<algorithm>`\n  library and container-based versions of such algorithms.\n* [`cleanup`](absl/cleanup/)\n  <br /> The `cleanup` library contains the control-flow-construct-like type\n  `absl::Cleanup` which is used for executing a callback on scope exit.\n* [`container`](absl/container/)\n  <br /> The `container` library contains additional STL-style containers,\n  including Abseil's unordered \"Swiss table\" containers.\n* [`crc`](absl/crc/) The `crc` library contains code for\n  computing error-detecting cyclic redundancy checks on data.\n* [`debugging`](absl/debugging/)\n  <br /> The `debugging` library contains code useful for enabling leak\n  checks, and stacktrace and symbolization utilities.\n* [`flags`](absl/flags/)\n  <br /> The `flags` library contains code for handling command line flags for\n  libraries and binaries built with Abseil.\n* [`hash`](absl/hash/)\n  <br /> The `hash` library contains the hashing framework and default hash\n  functor implementations for hashable types in Abseil.\n* [`log`](absl/log/)\n  <br /> The `log` library contains `LOG` and `CHECK` macros and facilities\n  for writing logged messages out to disk, `stderr`, or user-extensible\n  destinations.\n* [`memory`](absl/memory/)\n  <br /> The `memory` library contains memory management facilities that augment\n  C++'s `<memory>` library.\n* [`meta`](absl/meta/)\n  <br /> The `meta` library contains type checks\n  similar to those available in the C++ `<type_traits>` library.\n* [`numeric`](absl/numeric/)\n  <br /> The `numeric` library contains 128-bit integer types as well as\n  implementations of C++20's bitwise math functions.\n* [`profiling`](absl/profiling/)\n  <br /> The `profiling` library contains utility code for profiling C++\n  entities.  It is currently a private dependency of other Abseil libraries.\n* [`random`](absl/random/)\n  <br /> The `random` library contains functions for generating pseudorandom\n  values.\n* [`status`](absl/status/)\n  <br /> The `status` library contains abstractions for error handling,\n  specifically `absl::Status` and `absl::StatusOr<T>`.\n* [`strings`](absl/strings/)\n  <br /> The `strings` library contains a variety of strings routines and\n  utilities.\n* [`synchronization`](absl/synchronization/)\n  <br /> The `synchronization` library contains concurrency primitives (Abseil's\n  `absl::Mutex` class, an alternative to `std::mutex`) and a variety of\n  synchronization abstractions.\n* [`time`](absl/time/)\n  <br /> The `time` library contains abstractions for computing with absolute\n  points in time, durations of time, and formatting and parsing time within\n  time zones.\n* [`types`](absl/types/)\n  <br /> The `types` library contains non-container utility types.\n* [`utility`](absl/utility/)\n  <br /> The `utility` library contains utility and helper code.\n\n<a name=\"releases\"></a>\n## Releases\n\nAbseil recommends users \"live-at-head\" (update to the latest commit from the\nmaster branch as often as possible). However, we realize this philosophy doesn't\nwork for every project, so we also provide [Long Term Support\nReleases](https://github.com/abseil/abseil-cpp/releases) to which we backport\nfixes for severe bugs. See our [release\nmanagement](https://abseil.io/about/releases) document for more details.\n\n<a name=\"license\"></a>\n## License\n\nThe Abseil C++ library is licensed under the terms of the Apache\nlicense. See [LICENSE](LICENSE) for more information.\n\n<a name=\"links\"></a>\n## Links\n\nFor more information about Abseil:\n\n* Consult our [Abseil Introduction](https://abseil.io/about/intro)\n* Read [Why Adopt Abseil](https://abseil.io/about/philosophy) to understand our\n  design philosophy.\n* Peruse our\n  [Abseil Compatibility Guarantees](https://abseil.io/about/compatibility) to\n  understand both what we promise to you, and what we expect of you in return.\n",
      "stars_today": 4
    },
    {
      "id": 59676171,
      "name": "klipper",
      "full_name": "Klipper3d/klipper",
      "description": "Klipper is a 3d-printer firmware",
      "html_url": "https://github.com/Klipper3d/klipper",
      "stars": 11226,
      "forks": 5761,
      "language": "C",
      "topics": [],
      "created_at": "2016-05-25T15:42:06Z",
      "updated_at": "2026-01-27T16:33:04Z",
      "pushed_at": "2026-01-27T00:03:55Z",
      "open_issues": 208,
      "owner": {
        "login": "Klipper3d",
        "avatar_url": "https://avatars.githubusercontent.com/u/90982958?v=4"
      },
      "readme": "Welcome to the Klipper project!\n\n[![Klipper](docs/img/klipper-logo-small.png)](https://www.klipper3d.org/)\n\nhttps://www.klipper3d.org/\n\nThe Klipper firmware controls 3d-Printers. It combines the power of a\ngeneral purpose computer with one or more micro-controllers. See the\n[features document](https://www.klipper3d.org/Features.html) for more\ninformation on why you should use the Klipper software.\n\nStart by [installing Klipper software](https://www.klipper3d.org/Installation.html).\n\nKlipper software is Free Software. See the [license](COPYING) or read\nthe [documentation](https://www.klipper3d.org/Overview.html). We\ndepend on the generous support from our\n[sponsors](https://www.klipper3d.org/Sponsors.html).\n",
      "stars_today": 4
    },
    {
      "id": 7170058,
      "name": "flyway",
      "full_name": "flyway/flyway",
      "description": "Flyway by Redgate â€¢ Database Migrations Made Easy.",
      "html_url": "https://github.com/flyway/flyway",
      "stars": 9469,
      "forks": 1587,
      "language": "Java",
      "topics": [
        "aurora",
        "continuous-delivery",
        "continuous-deployment",
        "database",
        "database-administration",
        "database-deployment",
        "database-management",
        "database-migrations",
        "db2",
        "devops",
        "flyway",
        "java",
        "java-library",
        "mariadb",
        "mysql",
        "postgresql",
        "redgate",
        "sql",
        "sqlserver"
      ],
      "created_at": "2012-12-14T18:43:22Z",
      "updated_at": "2026-01-28T01:38:39Z",
      "pushed_at": "2026-01-26T10:39:52Z",
      "open_issues": 236,
      "owner": {
        "login": "flyway",
        "avatar_url": "https://avatars.githubusercontent.com/u/2109532?v=4"
      },
      "readme": "# [Flyway](https://github.com/flyway/flyway) by [Redgate](https://www.red-gate.com/) [![Build Release Tags](https://github.com/flyway/flyway/actions/workflows/build-release.yml/badge.svg)](https://github.com/flyway/flyway/actions/workflows/build-release.yml) [![Maven Central](https://img.shields.io/maven-central/v/org.flywaydb/flyway-core?logo=apachemaven&logoColor=red)](https://search.maven.org/artifact/org.flywaydb/flyway-core) [![GitHub license](https://img.shields.io/badge/license-Apache%20License%202.0-blue.svg?style=flat)](http://www.apache.org/licenses/LICENSE-2.0)\n\n### Database Migrations Made Easy.\n\n![Flyway](https://documentation.red-gate.com/download/attachments/138346876/FD?version=3&modificationDate=1633982869952&api=v2 \"Flyway\")\n\n\n#### Evolve your database schema easily and reliably across all your instances.\nSimple, focused and powerful.\n\n#### Works on\nWindows, macOS, Linux, Docker and Java\n\n#### Supported build tools\nMaven and Gradle\n\n#### Supported databases\nAurora MySQL, Aurora PostgreSQL, Azure Synapse, Clickhouse, CockroachDB, Databricks, DB2, Derby, Firebird, Google BigQuery, Google Cloud Spanner, H2, HSQLDB, Informix, MariaDB, MongoDB, MySQL, Oracle, Percona XtraDB Cluster, PostgreSQL, Redshift, SAP HANA (Including SAP HANA Cloud), SingleStoreDB, Snowflake, SQLite, SQL Server, Sybase ASE, TiDB, TimescaleDB, YugabyteDB\n\n#### Third party plugins\nSBT, Ant, Spring Boot, Grails, Play!, DropWizard, Grunt, Griffon, Ninja, ...\n\n## Documentation\n- [Getting started guides](https://documentation.red-gate.com/flyway/getting-started-with-flyway)\n- [Reference documentation](https://documentation.red-gate.com/flyway/reference)\n- [Contributing to the project](https://flyway.github.io/flyway/)\n\n## Download\nYou can download Flyway from [here](https://documentation.red-gate.com/flyway/reference/usage/flyway-open-source)\n\n## About\nFlyway is brought to you by [Redgate](https://www.red-gate.com/) with the help of many contributors.\n\n- [Posts on changes and updates to Flyway](https://documentation.red-gate.com/fd/flyway-blog-205226034.html)\n- [Release notes](https://documentation.red-gate.com/fd/release-notes-for-flyway-engine-179732572.html)\n\n## How to contribute\nPlease visit our [contribution page](https://flyway.github.io/flyway/) to find out how you can contribute in various ways to the project.\n\n## License\nCopyright Â© [Red Gate Software Ltd](http://www.red-gate.com) 2010-2025\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n## Trademark\nFlyway is a registered trademark of Boxfuse GmbH, owned by  [Red Gate Software Ltd](https://www.red-gate.com/).\n",
      "stars_today": 4
    },
    {
      "id": 224908244,
      "name": "WasmEdge",
      "full_name": "WasmEdge/WasmEdge",
      "description": "WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications. It powers serverless apps, embedded functions, microservices, smart contracts, and IoT devices.",
      "html_url": "https://github.com/WasmEdge/WasmEdge",
      "stars": 10379,
      "forks": 968,
      "language": "C++",
      "topics": [
        "artificial-intelligence",
        "cloud",
        "cloud-native",
        "cncf",
        "container",
        "docker",
        "edge-computing",
        "ewasm",
        "hacktoberfest",
        "hacktoberfest2023",
        "kubernetes",
        "rust-lang",
        "serverless",
        "wasm",
        "webassembly"
      ],
      "created_at": "2019-11-29T19:00:17Z",
      "updated_at": "2026-01-27T23:32:05Z",
      "pushed_at": "2026-01-27T16:44:01Z",
      "open_issues": 201,
      "owner": {
        "login": "WasmEdge",
        "avatar_url": "https://avatars.githubusercontent.com/u/83329692?v=4"
      },
      "readme": "<div align=\"right\">\n\n  [ä¸­æ–‡](README-zh.md) | [æ­£é«”ä¸­æ–‡](README-zh-TW.md) | [æ—¥æœ¬èªã§èª­ã‚€](README-ja.md)\n\n</div>\n\n<div align=\"center\">\n\n![WasmEdge Logo](/docs/wasmedge-runtime-logo.png)\n\n# [ğŸ¤© WasmEdge is the easiest and fastest way to run LLMs on your own devices. ğŸ¤©](https://llamaedge.com/docs/intro)\n\n<a href=\"https://trendshift.io/repositories/2481\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/2481\" alt=\"WasmEdge%2FWasmEdge | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\nWasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime. It is [the fastest Wasm VM](https://ieeexplore.ieee.org/document/9214403). WasmEdge is an official sandbox project hosted by the [CNCF](https://www.cncf.io/). [LlamaEdge](https://github.com/LlamaEdge/LlamaEdge) is an application framework built on top of WasmEdge to run GenAI models (e.g., [LLM](https://llamaedge.com/docs/intro), [speech-to-text](https://llamaedge.com/docs/user-guide/speech-to-text/quick-start-whisper), [text-to-image](https://llamaedge.com/docs/user-guide/text-to-image/quick-start-sd), and [TTS](https://github.com/LlamaEdge/whisper-api-server)) across GPUs on servers, personal computers, and edge devices. Additional [use cases](https://wasmedge.org/docs/start/usage/use-cases/) include microservices on the edge cloud, serverless SaaS APIs, embedded functions, smart contracts, and smart devices.\n\n[![build](https://github.com/WasmEdge/WasmEdge/actions/workflows/build.yml/badge.svg)](https://github.com/WasmEdge/WasmEdge/actions/workflows/build.yml?query=event%3Apush++branch%3Amaster)\n[![codecov](https://codecov.io/gh/WasmEdge/WasmEdge/branch/master/graph/badge.svg)](https://codecov.io/gh/WasmEdge/WasmEdge)\n[![CodeQL](https://github.com/WasmEdge/WasmEdge/actions/workflows/codeql-analysis.yml/badge.svg)](https://github.com/WasmEdge/WasmEdge/actions/workflows/codeql-analysis.yml?query=event%3Apush++branch%3Amaster)\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2FWasmEdge%2FWasmEdge.svg?type=shield)](https://app.fossa.com/projects/git%2Bgithub.com%2FWasmEdge%2FWasmEdge?ref=badge_shield)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5059/badge)](https://bestpractices.coreinfrastructure.org/projects/5059)\n\n</div>\n\n# Quick start guides\n\nğŸš€ [Install](https://wasmedge.org/docs/start/install) WasmEdge \\\nğŸ‘·ğŸ»â€â™‚ï¸ [Build](https://wasmedge.org/docs/category/build-wasmedge-from-source) and [contribute to](https://wasmedge.org/docs/contribute/) WasmEdge \\\nâŒ¨ï¸ [Run](https://wasmedge.org/docs/category/running-with-wasmedge) a standalone Wasm program or a [JavaScript program](https://wasmedge.org/docs/category/develop-wasm-apps-in-javascript) from CLI or [Docker](https://wasmedge.org/docs/start/getting-started/quick_start_docker) \\\nğŸ¤– [Chat](https://llamaedge.com/docs/intro) with an open source LLM via [LlamaEdge](https://github.com/LlamaEdge/LlamaEdge) \\\nğŸ”Œ Embed a Wasm function in your [Go](https://wasmedge.org/docs/category/go-sdk-for-embedding-wasmedge), [Rust](https://wasmedge.org/docs/category/rust-sdk-for-embedding-wasmedge), or [C](https://wasmedge.org/docs/category/c-sdk-for-embedding-wasmedge) app \\\nğŸ›  Manage and orchestrate Wasm runtimes using [Kubernetes](https://wasmedge.org/docs/category/deploy-wasmedge-apps-in-kubernetes), [data streaming frameworks](https://wasmedge.org/docs/embed/use-case/yomo), and [blockchains](https://medium.com/ethereum-on-steroids/running-ethereum-smart-contracts-in-a-substrate-blockchain-56fbc27fc95a) \\\nğŸ“š **[Check out our official documentation](https://wasmedge.org/docs/)**\n\n# Introduction\n\nThe WasmEdge Runtime provides a well-defined execution sandbox for its contained WebAssembly bytecode program. The runtime offers isolation and protection for operating system resources (e.g., file system, sockets, environment variables, processes) and memory space. The most important use case for WasmEdge is to safely execute user-defined or community-contributed code as plug-ins in a software product (e.g., SaaS, software-defined vehicles, edge nodes, or even blockchain nodes). It enables third-party developers, vendors, suppliers, and community members to extend and customize the software product. **[Learn more here](https://wasmedge.org/docs/contribute/users)**\n\n## Performance\n\n* [A Lightweight Design for High-performance Serverless Computing](https://arxiv.org/abs/2010.07115), published on IEEE Software, Jan 2021. [https://arxiv.org/abs/2010.07115](https://arxiv.org/abs/2010.07115)\n* [Performance Analysis for Arm vs. x86 CPUs in the Cloud](https://www.infoq.com/articles/arm-vs-x86-cloud-performance/), published on infoQ.com, Jan 2021. [https://www.infoq.com/articles/arm-vs-x86-cloud-performance/](https://www.infoq.com/articles/arm-vs-x86-cloud-performance/)\n\n## Features\n\nWasmEdge can run standard WebAssembly bytecode programs compiled from C/C++, Rust, Swift, AssemblyScript, or Kotlin source code. It [runs JavaScript](https://wasmedge.org/docs/category/develop-wasm-apps-in-javascript), including 3rd party ES6, CJS, and NPM modules, in a secure, fast, lightweight, portable, and containerized sandbox. It also supports mixing of those languages (e.g., to [use Rust to implement a JavaScript API](https://wasmedge.org/docs/develop/javascript/rust)), the [Fetch](https://wasmedge.org/docs/develop/javascript/networking#fetch-client) API, and [Server-side Rendering (SSR)](https://wasmedge.org/docs/develop/javascript/ssr) functions on edge servers.\n\nWasmEdge supports [all standard WebAssembly features and many proposed extensions](https://wasmedge.org/docs/start/wasmedge/extensions/proposals). It also supports a number of extensions tailored for cloud-native and edge computing uses (e.g., the [WasmEdge network sockets](https://wasmedge.org/docs/category/socket-networking),[Postgres and MySQL-based database driver](https://wasmedge.org/docs/category/database-drivers), and the [WasmEdge AI extension](https://wasmedge.org/docs/category/ai-inference)).\n\n **Learn more about [technical highlights](https://wasmedge.org/docs/start/wasmedge/features) of WasmEdge.**\n\n## Integrations and management\n\nWasmEdge and its contained wasm program can be started from the [CLI](https://wasmedge.org/docs/category/running-with-wasmedge) as a new process, or from an existing process. If started from an existing process (e.g., from a running [Go](https://wasmedge.org/docs/category/go-sdk-for-embedding-wasmedge) or [Rust](https://wasmedge.org/docs/category/rust-sdk-for-embedding-wasmedge) program), WasmEdge will simply run inside the process as a function. Currently, WasmEdge is not yet thread-safe. In order to use WasmEdge in your own application or cloud-native frameworks, please refer to the guides below.\n\n* [Embed WasmEdge into a host application](https://wasmedge.org/docs/embed/overview)\n* [Orchestrate and manage WasmEdge instances using container tools](https://wasmedge.org/docs/category/deploy-wasmedge-apps-in-kubernetes)\n* [Run a WasmEdge app as a Dapr microservice](https://wasmedge.org/docs/develop/rust/dapr)\n\n# Community\n\n## Contributing\n\nWe welcome contributions from the community! Please check out our:\n- [Contributing Guide](./docs/CONTRIBUTING.md) for how to get started\n- [Governance documentation](./docs/GOVERNANCE.md) for project decision-making processes\n- [Code of Conduct](./docs/CODE_OF_CONDUCT.md) for community standards\n\nWant to become a maintainer? See our [Contributor Ladder](./docs/CONTRIBUTOR_LADDER.md).\n\n## Roadmap\n\nCheck out our [project roadmap](https://github.com/WasmEdge/WasmEdge/blob/master/docs/ROADMAP.md) to see the upcoming features and plans for WasmEdge.\n\n## Contact\n\nIf you have any questions, feel free to open a GitHub issue on a related project or to join the following channels:\n\n* Mailing list: Send an email to [WasmEdge@googlegroups.com](https://groups.google.com/g/wasmedge/)\n* Discord: Join the [WasmEdge Discord server](https://discord.gg/h4KDyB8XTt)!\n* Slack: Join the #WasmEdge channel on the [CNCF Slack](https://slack.cncf.io/)\n* X (formerly Twitter): Follow @realwasmedge on [X](https://x.com/realwasmedge)\n\n## Adopters\n\nCheck out our [list of Adopters](https://wasmedge.org/docs/contribute/users/) who are using WasmEdge in their projects.\n\n## Community Meeting\n\nWe host a monthly community meeting to showcase new features, demo new use cases, and a Q&A part. Everyone is welcome!\n\nTime: The first Tuesday of each month at 11PM Hong Kong Time/ 7AM PST.\n\n[Public meeting agenda/notes](https://docs.google.com/document/d/1iFlVl7R97Lze4RDykzElJGDjjWYDlkI8Rhf8g4dQ5Rk/edit#) | [Zoom link](https://us06web.zoom.us/j/82221747919?pwd=3MORhaxDk15rACk7mNDvyz9KtaEbWy.1)\n\n# License\n\n[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2FWasmEdge%2FWasmEdge.svg?type=large)](https://app.fossa.com/projects/git%2Bgithub.com%2FWasmEdge%2FWasmEdge?ref=badge_large)\n",
      "stars_today": 4
    },
    {
      "id": 19438,
      "name": "ggplot2",
      "full_name": "tidyverse/ggplot2",
      "description": "An implementation of the Grammar of Graphics in R",
      "html_url": "https://github.com/tidyverse/ggplot2",
      "stars": 6873,
      "forks": 2116,
      "language": "R",
      "topics": [
        "data-visualisation",
        "r",
        "visualisation"
      ],
      "created_at": "2008-05-25T01:21:32Z",
      "updated_at": "2026-01-28T01:51:09Z",
      "pushed_at": "2026-01-26T08:15:34Z",
      "open_issues": 97,
      "owner": {
        "login": "tidyverse",
        "avatar_url": "https://avatars.githubusercontent.com/u/22032646?v=4"
      },
      "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# ggplot2 <a href=\"https://ggplot2.tidyverse.org\"><img src=\"man/figures/logo.png\" align=\"right\" height=\"138\" alt=\"ggplot2 website\" /></a>\n\n<!-- badges: start -->\n\n[![R-CMD-check](https://github.com/tidyverse/ggplot2/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/tidyverse/ggplot2/actions/workflows/R-CMD-check.yaml)\n[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/ggplot2)](https://cran.r-project.org/package=ggplot2)\n[![Codecov test\ncoverage](https://codecov.io/gh/tidyverse/ggplot2/graph/badge.svg)](https://app.codecov.io/gh/tidyverse/ggplot2)\n<!-- badges: end -->\n\n## Overview\n\nggplot2 is a system for declaratively creating graphics, based on [The\nGrammar of\nGraphics](https://link.springer.com/book/10.1007/0-387-28695-0). You\nprovide the data, tell ggplot2 how to map variables to aesthetics, what\ngraphical primitives to use, and it takes care of the details.\n\n## Installation\n\n``` r\n# The easiest way to get ggplot2 is to install the whole tidyverse:\ninstall.packages(\"tidyverse\")\n\n# Alternatively, install just ggplot2:\ninstall.packages(\"ggplot2\")\n\n# Or the development version from GitHub:\n# install.packages(\"pak\")\npak::pak(\"tidyverse/ggplot2\")\n```\n\n## Cheatsheet\n\n<a href=\"https://github.com/rstudio/cheatsheets/blob/main/data-visualization.pdf\"><img src=\"https://raw.githubusercontent.com/rstudio/cheatsheets/main/pngs/thumbnails/data-visualization-cheatsheet-thumbs.png\" width=\"630\" height=\"252\" alt=\"ggplot2 cheatsheet\" /></a>\n\n## Usage\n\nItâ€™s hard to succinctly describe how ggplot2 works because it embodies a\ndeep philosophy of visualisation. However, in most cases you start with\n`ggplot()`, supply a dataset and aesthetic mapping (with `aes()`). You\nthen add on layers (like `geom_point()` or `geom_histogram()`), scales\n(like `scale_colour_brewer()`), faceting specifications (like\n`facet_wrap()`) and coordinate systems (like `coord_flip()`).\n\n``` r\nlibrary(ggplot2)\n\nggplot(mpg, aes(displ, hwy, colour = class)) +\n  geom_point()\n```\n\n<img src=\"man/figures/README-example-1.png\" alt=\"Scatterplot of engine displacement versus highway miles per gallon, for 234 cars coloured by 7 'types' of car. The displacement and miles per gallon are inversely correlated.\"  />\n\n## Lifecycle\n\n[![lifecycle](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n\nggplot2 is now 18 years old and is used by hundreds of thousands of\npeople to make millions of plots. That means, by-and-large, ggplot2\nitself changes relatively little. When we do make changes, they will be\ngenerally to add new functions or arguments rather than changing the\nbehaviour of existing functions, and if we do make changes to existing\nbehaviour we will do them for compelling reasons.\n\nIf you are looking for innovation, look to ggplot2â€™s rich ecosystem of\nextensions. See a community maintained list at\n<https://exts.ggplot2.tidyverse.org/gallery/>.\n\n## Learning ggplot2\n\nIf you are new to ggplot2 you are better off starting with a systematic\nintroduction, rather than trying to learn from reading individual\ndocumentation pages. Currently, there are several good places to start:\n\n1.  The [Data Visualization](https://r4ds.hadley.nz/data-visualize) and\n    [Communication](https://r4ds.hadley.nz/communication) chapters in [R\n    for Data Science](https://r4ds.hadley.nz). R for Data Science is\n    designed to give you a comprehensive introduction to the\n    [tidyverse](https://tidyverse.org/), and these two chapters will get\n    you up to speed with the essentials of ggplot2 as quickly as\n    possible.\n\n2.  If youâ€™d like to take an online course, try [Data Visualization in R\n    With\n    ggplot2](https://learning.oreilly.com/videos/data-visualization-in/9781491963661/)\n    by Kara Woo.\n\n3.  If youâ€™d like to follow a webinar, try [Plotting Anything with\n    ggplot2](https://youtu.be/h29g21z0a68) by Thomas Lin Pedersen.\n\n4.  If you want to dive into making common graphics as quickly as\n    possible, I recommend [The R Graphics\n    Cookbook](https://r-graphics.org) by Winston Chang. It provides a\n    set of recipes to solve common graphics problems.\n\n5.  If youâ€™ve mastered the basics and want to learn more, read [ggplot2:\n    Elegant Graphics for Data Analysis](https://ggplot2-book.org). It\n    describes the theoretical underpinnings of ggplot2 and shows you how\n    all the pieces fit together. This book helps you understand the\n    theory that underpins ggplot2, and will help you create new types of\n    graphics specifically tailored to your needs.\n\n6.  For articles about announcements and deep-dives you can visit the\n    [tidyverse blog](https://tidyverse.org/tags/ggplot2/).\n\n## Getting help\n\nThere are two main places to get help with ggplot2:\n\n1.  The [Posit Community](https://forum.posit.co/) (formerly RStudio\n    Community) is a friendly place to ask any questions about ggplot2.\n\n2.  [Stack\n    Overflow](https://stackoverflow.com/questions/tagged/ggplot2?sort=frequent&pageSize=50)\n    is a great source of answers to common ggplot2 questions. It is also\n    a great place to get help, once you have created a reproducible\n    example that illustrates your problem.\n",
      "stars_today": 4
    },
    {
      "id": 312830970,
      "name": "wvp-GB28181-pro",
      "full_name": "648540858/wvp-GB28181-pro",
      "description": "åŸºäºGB28181-2016ã€éƒ¨æ ‡808ã€éƒ¨æ ‡1078æ ‡å‡†å®ç°çš„å¼€ç®±å³ç”¨çš„ç½‘ç»œè§†é¢‘å¹³å°ã€‚è‡ªå¸¦ç®¡ç†é¡µé¢ï¼Œæ”¯æŒNATç©¿é€ï¼Œæ”¯æŒæµ·åº·ã€å¤§åã€å®‡è§†ç­‰å“ç‰Œçš„IPCã€NVRæ¥å…¥ã€‚æ”¯æŒå›½æ ‡çº§è”ï¼Œæ”¯æŒå°†æ™®é€šæ‘„åƒæœº/ç›´æ’­æµ/ç›´æ’­æ¨æµè½¬å›½æ ‡å…±äº«åˆ°å›½æ ‡å¹³å°ã€‚",
      "html_url": "https://github.com/648540858/wvp-GB28181-pro",
      "stars": 6617,
      "forks": 1872,
      "language": "Java",
      "topics": [
        "1078",
        "28181",
        "28181web",
        "808",
        "gb28181",
        "gb28181server",
        "wvp"
      ],
      "created_at": "2020-11-14T14:16:37Z",
      "updated_at": "2026-01-27T13:13:49Z",
      "pushed_at": "2026-01-27T14:26:53Z",
      "open_issues": 29,
      "owner": {
        "login": "648540858",
        "avatar_url": "https://avatars.githubusercontent.com/u/18274453?v=4"
      },
      "readme": "![logo](doc/_media/logo.png)\n# å¼€ç®±å³ç”¨çš„å›½æ ‡28181å’Œéƒ¨æ ‡808+1078åè®®è§†é¢‘å¹³å°\n\n[![Build Status](https://travis-ci.org/xia-chu/ZLMediaKit.svg?branch=master)](https://travis-ci.org/xia-chu/ZLMediaKit)\n[![license](http://img.shields.io/badge/license-MIT-green.svg)](https://github.com/xia-chu/ZLMediaKit/blob/master/LICENSE)\n[![JAVA](https://img.shields.io/badge/language-java-red.svg)](https://en.cppreference.com/)\n[![platform](https://img.shields.io/badge/platform-linux%20|%20macos%20|%20windows-blue.svg)](https://github.com/xia-chu/ZLMediaKit)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-yellow.svg)](https://github.com/xia-chu/ZLMediaKit/pulls)\n\n\nWEB VIDEO PLATFORMæ˜¯ä¸€ä¸ªåŸºäºGB28181-2016ã€éƒ¨æ ‡808ã€éƒ¨æ ‡1078æ ‡å‡†å®ç°çš„å¼€ç®±å³ç”¨çš„ç½‘ç»œè§†é¢‘å¹³å°ï¼Œè´Ÿè´£å®ç°æ ¸å¿ƒä¿¡ä»¤ä¸è®¾å¤‡ç®¡ç†åå°éƒ¨åˆ†ï¼Œæ”¯æŒNATç©¿é€ï¼Œæ”¯æŒæµ·åº·ã€å¤§åã€å®‡è§†ç­‰å“ç‰Œçš„IPCã€NVRæ¥å…¥ã€‚æ”¯æŒå›½æ ‡çº§è”ï¼Œæ”¯æŒå°†ä¸å¸¦å›½æ ‡åŠŸèƒ½çš„æ‘„åƒæœº/ç›´æ’­æµ/ç›´æ’­æ¨æµè½¬å‘åˆ°å…¶ä»–å›½æ ‡å¹³å°ã€‚\n\næµåª’ä½“æœåŠ¡åŸºäº@å¤æ¥š ZLMediaKit [https://github.com/ZLMediaKit/ZLMediaKit](https://github.com/ZLMediaKit/ZLMediaKit)   \næ’­æ”¾å™¨ä½¿ç”¨@dexter jessibuca [https://github.com/langhuihui/jessibuca/tree/v3](https://github.com/langhuihui/jessibuca/tree/v3)  \næ’­æ”¾å™¨ä½¿ç”¨@Numberwolf-Yanlong h265web.js [https://github.com/numberwolf/h265web.js](https://github.com/numberwolf/h265web.js)  \nå‰ç«¯é¡µé¢åŸºäºvue-admin-templateæ„å»º [https://github.com/PanJiaChen/vue-admin-template?tab=readme-ov-file](https://github.com/PanJiaChen/vue-admin-template?tab=readme-ov-file)  \n\n# åº”ç”¨åœºæ™¯ï¼š\n- æ”¯æŒæµè§ˆå™¨æ— æ’ä»¶æ’­æ”¾æ‘„åƒå¤´è§†é¢‘ã€‚\n- æ”¯æŒå›½æ ‡è®¾å¤‡(æ‘„åƒæœºã€å¹³å°ã€NVRç­‰)è®¾å¤‡æ¥å…¥\n- æ”¯æŒrtsp, rtmpï¼Œç›´æ’­è®¾å¤‡è®¾å¤‡æ¥å…¥ï¼Œå……åˆ†åˆ©æ—§ã€‚\n- æ”¯æŒå›½æ ‡çº§è”ã€‚å¤šå¹³å°çº§è”ã€‚è·¨ç½‘è§†é¢‘é¢„è§ˆã€‚\n- æ”¯æŒè·¨ç½‘ç½‘é—¸å¹³å°äº’è”ã€‚\n\n\n# æ–‡æ¡£\nwvpä½¿ç”¨æ–‡æ¡£ [https://doc.wvp-pro.cn](https://doc.wvp-pro.cn)  \nZLMä½¿ç”¨æ–‡æ¡£ [https://github.com/ZLMediaKit/ZLMediaKit](https://github.com/ZLMediaKit/ZLMediaKit)\n\n# giteeä»“åº“\nhttps://gitee.com/pan648540858/wvp-GB28181-pro.git\n\n# æˆªå›¾\n<table>\n    <tr>\n        <td ><center><img src=\"doc/_media/1.png\" >ç™»å½•é¡µé¢ </center></td>\n        <td ><center><img src=\"doc/_media/2.png\" >é¦–é¡µ</center></td>\n    </tr>\n    <tr>\n        <td ><center><img src=\"doc/_media/3.png\" >åˆ†å±æ’­æ”¾ </center></td>\n        <td ><center><img src=\"doc/_media/4.png\" >å›½æ ‡è®¾å¤‡åˆ—è¡¨</center></td>\n    </tr>\n    <tr>\n        <td ><center><img src=\"doc/_media/5.png\" >è¡Œæ”¿åŒºåˆ’ç®¡ç† </center></td>\n        <td ><center><img src=\"doc/_media/8.png\" >ä¸šåŠ¡åˆ†ç»„ç®¡ç†</center></td>\n    </tr>\n    <tr>\n        <td ><center><img src=\"doc/_media/6.png\" >å½•åˆ¶è®¡åˆ’</center></td>\n        <td ><center><img src=\"doc/_media/7.png\" >å¹³å°ä¿¡æ¯</center></td>\n    </tr>\n</table>\n\n# åŠŸèƒ½ç‰¹æ€§\n- [X] é›†æˆwebç•Œé¢\n- [X] å…¼å®¹æ€§è‰¯å¥½\n- [X] è·¨å¹³å°æœåŠ¡ï¼Œä¸€æ¬¡ç¼–è¯‘å¤šç«¯éƒ¨ç½²ï¼Œ å¯ä»¥åŒæ—¶ç”¨äºx86å’Œarmæ¶æ„\n- [X] æ¥å…¥è®¾å¤‡\n  - [X] è§†é¢‘é¢„è§ˆ\n  - [X] æ”¯æŒä¸»ç æµå­ç æµåˆ‡æ¢\n  - [X] æ— é™åˆ¶æ¥å…¥è·¯æ•°ï¼Œèƒ½æ¥å…¥å¤šå°‘è®¾å¤‡åªå–å†³äºä½ çš„æœåŠ¡å™¨æ€§èƒ½\n  - [X] äº‘å°æ§åˆ¶ï¼Œæ§åˆ¶è®¾å¤‡è½¬å‘ï¼Œæ‹‰è¿‘ï¼Œæ‹‰è¿œ\n  - [X] é¢„ç½®ä½æŸ¥è¯¢ï¼Œä½¿ç”¨ä¸è®¾ç½®\n  - [X] æŸ¥è¯¢NVR/IPCä¸Šçš„å½•åƒä¸æ’­æ”¾ï¼Œæ”¯æŒæŒ‡å®šæ—¶é—´æ’­æ”¾ä¸ä¸‹è½½\n  - [X] æ— äººè§‚çœ‹è‡ªåŠ¨æ–­æµï¼ŒèŠ‚çœæµé‡\n  - [X] è§†é¢‘è®¾å¤‡ä¿¡æ¯åŒæ­¥\n  - [X] ç¦»åœ¨çº¿ç›‘æ§\n  - [X] æ”¯æŒç›´æ¥è¾“å‡ºRTSPã€RTMPã€HTTP-FLVã€Websocket-FLVã€HLSå¤šç§åè®®æµåœ°å€\n  - [X] æ”¯æŒé€šè¿‡ä¸€ä¸ªæµåœ°å€ç›´æ¥è§‚çœ‹æ‘„åƒå¤´ï¼Œæ— éœ€ç™»å½•ä»¥åŠè°ƒç”¨ä»»ä½•æ¥å£\n  - [X] æ”¯æŒUDPå’ŒTCPä¸¤ç§å›½æ ‡ä¿¡ä»¤ä¼ è¾“æ¨¡å¼\n  - [X] æ”¯æŒUDPå’ŒTCPä¸¤ç§å›½æ ‡æµä¼ è¾“æ¨¡å¼\n  - [X] æ”¯æŒæ£€ç´¢,é€šé“ç­›é€‰\n  - [X] æ”¯æŒé€šé“å­ç›®å½•æŸ¥è¯¢\n  - [X] æ”¯æŒè¿‡æ»¤éŸ³é¢‘ï¼Œé˜²æ­¢æ‚éŸ³å½±å“è§‚çœ‹\n  - [X] æ”¯æŒå›½æ ‡ç½‘ç»œæ ¡æ—¶\n  - [X] æ”¯æŒæ’­æ”¾H264å’ŒH265\n  - [X] æŠ¥è­¦ä¿¡æ¯å¤„ç†ï¼Œæ”¯æŒå‘å‰ç«¯æ¨é€æŠ¥è­¦ä¿¡æ¯\n  - [X] è¯­éŸ³å¯¹è®²\n  - [X] æ”¯æŒä¸šåŠ¡åˆ†ç»„å’Œè¡Œæ”¿åŒºåˆ’æ ‘è‡ªå®šä¹‰å±•ç¤ºä»¥åŠçº§è”æ¨é€\n  - [X] æ”¯æŒè®¢é˜…ä¸é€šçŸ¥æ–¹æ³•\n    - [X] ç§»åŠ¨ä½ç½®è®¢é˜…\n    - [X] ç§»åŠ¨ä½ç½®é€šçŸ¥å¤„ç†\n    - [X] æŠ¥è­¦äº‹ä»¶è®¢é˜…\n    - [X] æŠ¥è­¦äº‹ä»¶é€šçŸ¥å¤„ç†\n    - [X] è®¾å¤‡ç›®å½•è®¢é˜…\n    - [X] è®¾å¤‡ç›®å½•é€šçŸ¥å¤„ç†\n  -  [X] ç§»åŠ¨ä½ç½®æŸ¥è¯¢å’Œæ˜¾ç¤º\n  - [X] æ”¯æŒæ‰‹åŠ¨æ·»åŠ è®¾å¤‡å’Œç»™è®¾å¤‡è®¾ç½®å•ç‹¬çš„å¯†ç \n-  [X] æ”¯æŒå¹³å°å¯¹æ¥æ¥å…¥\n-  [X] æ”¯æŒå›½æ ‡çº§è”\n  - [X] å›½æ ‡é€šé“å‘ä¸Šçº§è”\n    - [X] WEBæ·»åŠ ä¸Šçº§å¹³å°\n    - [X] æ³¨å†Œ\n    - [X] å¿ƒè·³ä¿æ´»\n    - [X] é€šé“é€‰æ‹©\n    - [X] æ”¯æŒé€šé“ç¼–å·è‡ªå®šä¹‰, æ”¯æŒæ¯ä¸ªå¹³å°ä½¿ç”¨ä¸åŒçš„é€šé“ç¼–å·\n    - [X] é€šé“æ¨é€\n    - [X] ç‚¹æ’­\n    - [X] äº‘å°æ§åˆ¶\n    - [X] å¹³å°çŠ¶æ€æŸ¥è¯¢\n    - [X] å¹³å°ä¿¡æ¯æŸ¥è¯¢\n    - [X] å¹³å°è¿œç¨‹å¯åŠ¨\n    - [X] æ¯ä¸ªçº§è”å¹³å°å¯è‡ªå®šä¹‰çš„è™šæ‹Ÿç›®å½•\n    - [X] ç›®å½•è®¢é˜…ä¸é€šçŸ¥\n    - [X] å½•åƒæŸ¥çœ‹ä¸æ’­æ”¾\n    - [X] GPSè®¢é˜…ä¸é€šçŸ¥ï¼ˆç›´æ’­æ¨æµï¼‰\n    - [X] è¯­éŸ³å¯¹è®²\n  - [X] æ”¯æŒåŒæ—¶çº§è”åˆ°å¤šä¸ªä¸Šçº§å¹³å°\n- [X] æ”¯æŒè‡ªåŠ¨é…ç½®ZLMåª’ä½“æœåŠ¡, å‡å°‘å› é…ç½®é—®é¢˜æ‰€å‡ºç°çš„é—®é¢˜;\n- [X] æ”¯æŒæµåª’ä½“èŠ‚ç‚¹é›†ç¾¤ï¼Œè´Ÿè½½å‡è¡¡ã€‚\n- [X] æ”¯æŒå¯ç”¨udpå¤šç«¯å£æ¨¡å¼, æé«˜udpæ¨¡å¼ä¸‹åª’ä½“ä¼ è¾“æ€§èƒ½;\n- [X] æ”¯æŒå…¬ç½‘éƒ¨ç½²ï¼›\n- [X] æ”¯æŒwvpä¸zlmåˆ†å¼€éƒ¨ç½²ï¼Œæå‡å¹³å°å¹¶å‘èƒ½åŠ›\n- [X] æ”¯æŒæ‹‰æµRTSP/RTMPï¼Œåˆ†å‘ä¸ºå„ç§æµæ ¼å¼ï¼Œæˆ–è€…æ¨é€åˆ°å…¶ä»–å›½æ ‡å¹³å°\n- [X] æ”¯æŒæ¨æµRTSP/RTMPï¼Œåˆ†å‘ä¸ºå„ç§æµæ ¼å¼ï¼Œæˆ–è€…æ¨é€åˆ°å…¶ä»–å›½æ ‡å¹³å°\n- [X] æ”¯æŒæ¨æµé‰´æƒ\n- [X] æ”¯æŒæ¥å£é‰´æƒ\n- [X] äº‘ç«¯å½•åƒï¼Œæ¨æµ/ä»£ç†/å›½æ ‡è§†é¢‘å‡å¯ä»¥å½•åˆ¶åœ¨äº‘ç«¯æœåŠ¡å™¨ï¼Œæ”¯æŒé¢„è§ˆå’Œä¸‹è½½\n- [X] æ”¯æŒæ‰“åŒ…å¯æ‰§è¡Œjarå’Œwar\n- [X] æ”¯æŒè·¨åŸŸè¯·æ±‚ï¼Œæ”¯æŒå‰åç«¯åˆ†ç¦»éƒ¨ç½²\n- [X] æ”¯æŒMysqlï¼ŒPostgresqlï¼Œé‡‘ä»“ç­‰æ•°æ®åº“\n- [X] æ”¯æŒå½•åˆ¶è®¡åˆ’, æ ¹æ®è®¾å®šçš„æ—¶é—´å¯¹é€šé“è¿›è¡Œå½•åˆ¶. æš‚ä¸æ”¯æŒå°†å½•åˆ¶çš„å†…å®¹è½¬å‘åˆ°å›½æ ‡ä¸Šçº§\n- [X] æ”¯æŒå›½æ ‡ä¿¡ä»¤é›†ç¾¤\n- [X] æ–°å¢æ”¯æŒéƒ¨æ ‡808å’Œéƒ¨æ ‡1078ï¼Œå¤§é‡æ–°ç‰¹æ€§ä¸ä¸€ä¸€åˆ—è¡¨äº†ã€‚æ”¯æŒä½œä¸ºç½‘å…³è¢«å›½æ ‡ä¸Šçº§è°ƒç”¨éƒ¨æ ‡è®¾å¤‡\n- [X] æ”¯æŒç”µå­åœ°å›¾ã€‚æ”¯æŒå±•ç¤ºé€šé“ä½ç½®ï¼Œæ”¯æŒåœ¨åœ°å›¾ä¸Šä¿®æ”¹é€šé“ä½ç½®ã€‚æ”¯æŒäº†æ•°æ®åˆ†å±‚æŠ½ç¨€æ•°æ®èƒ½åŠ›ï¼Œç™¾ä¸‡çº§æ•°æ®ä¹Ÿå¯ä»¥è½»æ¾å±•ç¤ºã€‚æä¾›æ ‡å‡†çš„çŸ¢é‡ç“¦ç‰‡å›¾å±‚ï¼Œå¸¸è§åœ°å›¾å¼•æ“éƒ½å¯ä»¥ç›´æ¥å±•ç¤ºã€‚\n- [X] å€Ÿç”¨zlmé—­æºç‰ˆæœ¬æ–°èƒ½åŠ›ï¼Œå¯ä»¥æ”¯æŒå½•åƒä¿å­˜è‡³s3å­˜å‚¨ï¼Œæ”¯æŒminioã€‚\n\n# é—­æºå†…å®¹\n- [X] å›½æ ‡å¢å¼ºç‰ˆ: æ”¯æŒå›½æ ‡28181-2022åè®®ï¼Œæ”¯æŒå·¡èˆªè½¨è¿¹æŸ¥è¯¢ï¼ŒPTZç²¾å‡†æ§åˆ¶ï¼Œå­˜å‚¨å¡æ ¼å¼åŒ–ï¼Œè®¾å¤‡è½¯ä»¶å‡çº§ï¼ŒOSDé…ç½®ï¼Œh265+aacï¼Œæ”¯æŒè¾…ç æµï¼Œå½•åƒå€’æ”¾ç­‰ã€‚\n- [X] å…¨åŠŸèƒ½ç‰ˆï¼š \n  - [X] æ”¯æŒå¼€æºæ‰€æœ‰åŠŸèƒ½\n  - [X] ONVIFåè®®\n    - è®¾å¤‡æ£€ç´¢\n    - å®æ—¶å›¾åƒé¢„è§ˆ\n    - å½•åƒå›æ”¾ã€å›æ”¾å€é€Ÿæ§åˆ¶\n    - äº‘å°æ§åˆ¶ã€é¢„ç½®ä½æ§åˆ¶ã€äº‘å°ç»å¯¹å®šä½ã€çœ‹å®ˆä½\n    - èšç„¦æ§åˆ¶\n    - è®¾å¤‡é‡å¯\n    - è®¾å¤‡æ—¶é—´è®¾ç½®ä»¥åŠè·Ÿç³»ç»Ÿæ—¶é—´çš„å·®å€¼æ¯”è¾ƒ\n    - æ¢å¤å‡ºå‚è®¾ç½®\n    - è‡ªåŠ¨è·å–è®¾å¤‡å“ç‰Œç­‰ä¿¡æ¯ã€æ”¯æŒå±•ç¤ºDNSä¿¡æ¯ã€æ”¯æŒåè®®çš„å±•ç¤º\n    - å›½æ ‡çº§è”ç‚¹æ’­ã€è‡ªåŠ¨ç‚¹æ’­ç­‰ã€‚\n  - [X] å›½ç½‘Bæ¥å£åè®®\n    - è®¾å¤‡æ³¨å†Œ\n    - èµ„æºè·å–\n    - é¢„è§ˆ\n    - äº‘å°æ§åˆ¶\n    - é¢„ç½®ä½æ§åˆ¶ç­‰ï¼Œ\n    - å¯å…è´¹å®šåˆ¶æ”¯æŒè¯­éŸ³å¯¹è®²ã€å½•åƒå›æ”¾å’ŒæŠ“æ‹å›¾åƒã€‚\n  - [X] æ”¯æŒæŒ‰æƒé™åˆ†é…å¯ä»¥ä½¿ç”¨çš„é€šé“\n  - [X] æ”¯æŒè¡¨æ ¼å¯¼å‡º\n  - [X] æ‹‰æµä»£ç†æ”¯æŒæŒ‰ç…§å“ç‰Œæ‹¼æ¥urlã€‚\n  - [X] æ’­æ”¾é‰´æƒï¼Œæ›´åŠ å®‰å…¨ã€‚\n\n\n# æˆæƒåè®®\næœ¬é¡¹ç›®è‡ªæœ‰ä»£ç ä½¿ç”¨å®½æ¾çš„MITåè®®ï¼Œåœ¨ä¿ç•™ç‰ˆæƒä¿¡æ¯çš„æƒ…å†µä¸‹å¯ä»¥è‡ªç”±åº”ç”¨äºå„è‡ªå•†ç”¨ã€éå•†ä¸šçš„é¡¹ç›®ã€‚ ä½†æ˜¯æœ¬é¡¹ç›®ä¹Ÿé›¶ç¢çš„ä½¿ç”¨äº†ä¸€äº›å…¶ä»–çš„å¼€æºä»£ç ï¼Œåœ¨å•†ç”¨çš„æƒ…å†µä¸‹è¯·è‡ªè¡Œæ›¿ä»£æˆ–å‰”é™¤ï¼› ç”±äºä½¿ç”¨æœ¬é¡¹ç›®è€Œäº§ç”Ÿçš„å•†ä¸šçº çº·æˆ–ä¾µæƒè¡Œä¸ºä¸€æ¦‚ä¸æœ¬é¡¹ç›®åŠå¼€å‘è€…æ— å…³ï¼Œè¯·è‡ªè¡Œæ‰¿æ‹…æ³•å¾‹é£é™©ã€‚ åœ¨ä½¿ç”¨æœ¬é¡¹ç›®ä»£ç æ—¶ï¼Œä¹Ÿåº”è¯¥åœ¨æˆæƒåè®®ä¸­åŒæ—¶è¡¨æ˜æœ¬é¡¹ç›®ä¾èµ–çš„ç¬¬ä¸‰æ–¹åº“çš„åè®®\n\n# æŠ€æœ¯æ”¯æŒ\n\n# ä»˜è´¹ç¤¾ç¾¤\n<img src=\"doc/_media/shequ.png\" width=\"50%\" height=\"50%\">\n\n> åŠ å…¥ä¸‰å¤©å†…ä¸æ»¡æ„å¯ä»¥ç›´æ¥è‡ªè¡Œæ¨å‡º,æ˜Ÿçƒä¼šç›´æ¥é€€æ¬¾ç»™å¤§å®¶ã€‚éœ€è¦å‘ç¥¨å¯ä»¥åœ¨æ˜Ÿçƒappä¸­ç›´æ¥å’¨è¯¢æ˜Ÿçƒå®¢æœè·å–ã€‚\n\n> æ˜Ÿçƒè¿˜æä¾›äº†åŒ…æ‹¬é—­æºçš„å…¨åŠŸèƒ½è¯•ç”¨åŒ…, ä¼šéšæ—¶æ›´æ–°ã€‚\n\n> ä»˜è´¹ç¤¾ç¾¤å³å¯ä»¥å¯¹ä½œè€…æä¾›æ”¯æŒï¼Œä¹Ÿå¯ä»¥ä¸ºå¤§å®¶æ›´åŠ å¿«é€Ÿçš„è§£å†³é—®é¢˜ã€‚å¦‚æœæš‚æ—¶æ— æ³•åŠ å…¥ï¼Œç»™é¡¹ç›®ç‚¹ä¸ªæ˜Ÿä¹Ÿæ˜¯æå¤§çš„é¼“åŠ±ã€‚\n\n\n[çŸ¥è¯†æ˜Ÿçƒ](https://t.zsxq.com/0d8VAD3Dm)ä¸“æ åˆ—è¡¨ï¼šï¼Œ\n- [ä½¿ç”¨å…¥é—¨ç³»åˆ—ä¸€ï¼šWVP-PROèƒ½åšä»€ä¹ˆ](https://t.zsxq.com/0dLguVoSp)\n\næœ‰å¿æŠ€æœ¯æ”¯æŒï¼Œä¸€å¯¹ä¸€å¼€å‘è¾…å¯¼ï¼Œé—­æºå†…å®¹åˆä½œè¯·å‘é€é‚®ä»¶åˆ°648540858@qq.comå’¨è¯¢\n\n# è‡´è°¢\næ„Ÿè°¢ä½œè€…[å¤æ¥š](https://github.com/xia-chu) æä¾›è¿™ä¹ˆæ£’çš„å¼€æºæµåª’ä½“æœåŠ¡æ¡†æ¶,å¹¶åœ¨å¼€å‘è¿‡ç¨‹ä¸­ç»™äºˆæ”¯æŒä¸å¸®åŠ©ã€‚     \næ„Ÿè°¢ä½œè€…[dexter langhuihui](https://github.com/langhuihui)å’Œ[Numberwolf-Yanlong](https://github.com/numberwolf/h265web.js) å¼€æºè¿™ä¹ˆå¥½ç”¨çš„WEBæ’­æ”¾å™¨ã€‚      \næ„Ÿè°¢å„ä½å¤§ä½¬çš„èµåŠ©ä»¥åŠå¯¹é¡¹ç›®çš„æŒ‡æ­£ä¸å¸®åŠ©ã€‚åŒ…æ‹¬ä½†ä¸é™äºä»£ç è´¡çŒ®ã€é—®é¢˜åé¦ˆã€èµ„é‡‘æèµ ç­‰å„ç§æ–¹å¼çš„æ”¯æŒï¼ä»¥ä¸‹æ’åä¸åˆ†å…ˆåï¼š  \n[lawrencehj](https://github.com/lawrencehj) [Smallwhitepig](https://github.com/Smallwhitepig) [swwhaha](https://github.com/swwheihei)\n[hotcoffie](https://github.com/hotcoffie) [xiaomu](https://github.com/nikmu) [TristingChen](https://github.com/TristingChen)\n[chenparty](https://github.com/chenparty) [Hotleave](https://github.com/hotleave) [ydwxb](https://github.com/ydwxb)\n[ydpd](https://github.com/ydpd) [szy833](https://github.com/szy833) [ydwxb](https://github.com/ydwxb) [Albertzhu666](https://github.com/Albertzhu666)\n[mk1990](https://github.com/mk1990) [SaltFish001](https://github.com/SaltFish001)\n",
      "stars_today": 4
    },
    {
      "id": 38304949,
      "name": "GRDB.swift",
      "full_name": "groue/GRDB.swift",
      "description": "A toolkit for SQLite databases, with a focus on application development",
      "html_url": "https://github.com/groue/GRDB.swift",
      "stars": 8140,
      "forks": 836,
      "language": "Swift",
      "topics": [
        "database",
        "database-observation",
        "grdb",
        "spm",
        "sql",
        "sql-builder",
        "sqlite",
        "sqlite-databases"
      ],
      "created_at": "2015-06-30T11:17:06Z",
      "updated_at": "2026-01-27T13:52:02Z",
      "pushed_at": "2026-01-16T12:18:54Z",
      "open_issues": 12,
      "owner": {
        "login": "groue",
        "avatar_url": "https://avatars.githubusercontent.com/u/54219?v=4"
      },
      "readme": "<picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/groue/GRDB.swift/master/GRDB~dark.png\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/groue/GRDB.swift/master/GRDB.png\">\n    <img alt=\"GRDB: A toolkit for SQLite databases, with a focus on application development.\" src=\"https://raw.githubusercontent.com/groue/GRDB.swift/master/GRDB.png\">\n</picture>\n\n<p align=\"center\">\n    <strong>A toolkit for SQLite databases, with a focus on application development</strong><br>\n    Proudly serving the community since 2015\n</p>\n\n<p align=\"center\">\n    <a href=\"https://developer.apple.com/swift/\"><img alt=\"Swift 6.1\" src=\"https://img.shields.io/badge/swift-6.1-orange.svg?style=flat\"></a>\n    <a href=\"https://github.com/groue/GRDB.swift/blob/master/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/github/license/groue/GRDB.swift.svg?maxAge=2592000\"></a>\n    <a href=\"https://github.com/groue/GRDB.swift/actions/workflows/CI.yml\"><img alt=\"CI Status\" src=\"https://github.com/groue/GRDB.swift/actions/workflows/CI.yml/badge.svg?branch=master\"></a>\n</p>\n\n**Latest release**: December 13, 2025 â€¢ [version 7.9.0](https://github.com/groue/GRDB.swift/tree/v7.9.0) â€¢ [CHANGELOG](CHANGELOG.md) â€¢ [Migrating From GRDB 6 to GRDB 7](Documentation/GRDB7MigrationGuide.md)\n\n**Requirements**: iOS 13.0+ / macOS 10.15+ / tvOS 13.0+ / watchOS 7.0+ &bull; SQLite 3.20.0+ &bull; Swift 6.1+ / Xcode 16.3+\n\n**Contact**:\n\n- Release announcements and usage tips: follow [@groue@hachyderm.io](https://hachyderm.io/@groue) on Mastodon.\n- Report bugs in a [Github issue](https://github.com/groue/GRDB.swift/issues/new). Make sure you check the [existing issues](https://github.com/groue/GRDB.swift/issues?q=is%3Aopen) first.\n- A question? Looking for advice? Do you wonder how to contribute? Fancy a chat? Go to the [GitHub discussions](https://github.com/groue/GRDB.swift/discussions), or the [GRDB forums](https://forums.swift.org/c/related-projects/grdb).\n\n\n## What is GRDB?\n\nUse this library to save your applicationâ€™s permanent data into SQLite databases. It comes with built-in tools that address common needs:\n\n- **SQL Generation**\n    \n    Enhance your application models with persistence and fetching methods, so that you don't have to deal with SQL and raw database rows when you don't want to.\n\n- **Database Observation**\n    \n    Get notifications when database values are modified. \n\n- **Robust Concurrency**\n    \n    Multi-threaded applications can efficiently use their databases, including WAL databases that support concurrent reads and writes. \n\n- **Migrations**\n    \n    Evolve the schema of your database as you ship new versions of your application.\n    \n- **Leverage your SQLite skills**\n\n    Not all developers need advanced SQLite features. But when you do, GRDB is as sharp as you want it to be. Come with your SQL and SQLite skills, or learn new ones as you go!\n\n---\n\n<p align=\"center\">\n    <a href=\"#usage\">Usage</a> &bull;\n    <a href=\"#documentation\">Documentation</a> &bull;\n    <a href=\"#installation\">Installation</a> &bull;\n    <a href=\"#faq\">FAQ</a>\n</p>\n\n---\n\n## Usage\n\n<details open>\n  <summary>Start using the database in four steps</summary>\n\n```swift\nimport GRDB\n\n// 1. Open a database connection\nlet dbQueue = try DatabaseQueue(path: \"/path/to/database.sqlite\")\n\n// 2. Define the database schema\ntry dbQueue.write { db in\n    try db.create(table: \"player\") { t in\n        t.primaryKey(\"id\", .text)\n        t.column(\"name\", .text).notNull()\n        t.column(\"score\", .integer).notNull()\n    }\n}\n\n// 3. Define a record type\nstruct Player: Codable, Identifiable, FetchableRecord, PersistableRecord {\n    var id: String\n    var name: String\n    var score: Int\n    \n    enum Columns {\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n}\n\n// 4. Write and read in the database\ntry dbQueue.write { db in\n    try Player(id: \"1\", name: \"Arthur\", score: 100).insert(db)\n    try Player(id: \"2\", name: \"Barbara\", score: 1000).insert(db)\n}\n\ntry dbQueue.read { db in\n    let player = try Player.find(db, id: \"1\"))\n    \n    let bestPlayers = try Player\n        .order(\\.score.desc)\n        .limit(10)\n        .fetchAll(db)\n}\n```\n\n</details>\n\n<details>\n    <summary>Access to raw SQL</summary>\n\n```swift\ntry dbQueue.write { db in\n    try db.execute(sql: \"\"\"\n        CREATE TABLE player (\n          id TEXT PRIMARY KEY,\n          name TEXT NOT NULL,\n          score INT NOT NULL)\n        \"\"\")\n    \n    try db.execute(sql: \"\"\"\n        INSERT INTO player (id, name, score)\n        VALUES (?, ?, ?)\n        \"\"\", arguments: [\"1\", \"Arthur\", 100])\n    \n    // Avoid SQL injection with SQL interpolation\n    let id = \"2\"\n    let name = \"O'Brien\"\n    let score = 1000\n    try db.execute(literal: \"\"\"\n        INSERT INTO player (id, name, score)\n        VALUES (\\(id), \\(name), \\(score))\n        \"\"\")\n}\n```\n\nSee [Executing Updates](#executing-updates)\n\n</details>\n\n<details>\n    <summary>Access to raw database rows and values</summary>\n\n```swift\ntry dbQueue.read { db in\n    // Fetch database rows\n    let rows = try Row.fetchCursor(db, sql: \"SELECT * FROM player\")\n    while let row = try rows.next() {\n        let id: String = row[\"id\"]\n        let name: String = row[\"name\"]\n        let score: Int = row[\"score\"]\n    }\n    \n    // Fetch values\n    let playerCount = try Int.fetchOne(db, sql: \"SELECT COUNT(*) FROM player\")! // Int\n    let playerNames = try String.fetchAll(db, sql: \"SELECT name FROM player\") // [String]\n}\n\nlet playerCount = try dbQueue.read { db in\n    try Int.fetchOne(db, sql: \"SELECT COUNT(*) FROM player\")!\n}\n```\n\nSee [Fetch Queries](#fetch-queries)\n\n</details>\n\n<details>\n    <summary>Database model types aka \"records\"</summary>\n\n```swift\nstruct Player: Codable, Identifiable, FetchableRecord, PersistableRecord {\n    var id: String\n    var name: String\n    var score: Int\n    \n    enum Columns {\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n}\n\ntry dbQueue.write { db in\n    // Create database table\n    try db.create(table: \"player\") { t in\n        t.primaryKey(\"id\", .text)\n        t.column(\"name\", .text).notNull()\n        t.column(\"score\", .integer).notNull()\n    }\n    \n    // Insert a record\n    var player = Player(id: \"1\", name: \"Arthur\", score: 100)\n    try player.insert(db)\n    \n    // Update a record\n    player.score += 10\n    try score.update(db)\n    \n    try player.updateChanges { $0.score += 10 }\n    \n    // Delete a record\n    try player.delete(db)\n}\n```\n\nSee [Records](#records)\n\n</details>\n\n<details>\n    <summary>Query the database with the Swift query interface</summary>\n\n```swift\ntry dbQueue.read { db in\n    // Player\n    let player = try Player.find(db, id: \"1\")\n    \n    // Player?\n    let arthur = try Player.filter { $0.name == \"Arthur\" }.fetchOne(db)\n    \n    // [Player]\n    let bestPlayers = try Player.order(\\.score.desc).limit(10).fetchAll(db)\n    \n    // Int\n    let playerCount = try Player.fetchCount(db)\n    \n    // SQL is always welcome\n    let players = try Player.fetchAll(db, sql: \"SELECT * FROM player\")\n}\n```\n\nSee the [Query Interface](#the-query-interface)\n\n</details>\n\n<details>\n    <summary>Database changes notifications</summary>\n\n```swift\n// Define the observed value\nlet observation = ValueObservation.tracking { db in\n    try Player.fetchAll(db)\n}\n\n// Start observation\nlet cancellable = observation.start(\n    in: dbQueue,\n    onError: { error in ... },\n    onChange: { (players: [Player]) in print(\"Fresh players: \\(players)\") })\n```\n\nReady-made support for Combine and RxSwift:\n\n```swift\n// Swift concurrency\nfor try await players in observation.values(in: dbQueue) {\n    print(\"Fresh players: \\(players)\")\n}\n\n// Combine\nlet cancellable = observation.publisher(in: dbQueue).sink(\n    receiveCompletion: { completion in ... },\n    receiveValue: { (players: [Player]) in print(\"Fresh players: \\(players)\") })\n\n// RxSwift\nlet disposable = observation.rx.observe(in: dbQueue).subscribe(\n    onNext: { (players: [Player]) in print(\"Fresh players: \\(players)\") },\n    onError: { error in ... })\n```\n\nSee [Database Observation], [Combine Support], [RxGRDB].\n\n</details>\n\nDocumentation\n=============\n\n**GRDB runs on top of SQLite**: you should get familiar with the [SQLite FAQ](http://www.sqlite.org/faq.html). For general and detailed information, jump to the [SQLite Documentation](http://www.sqlite.org/docs.html).\n\n\n#### Demo Applications & Frequently Asked Questions\n\n- [Demo Applications]\n- [FAQ]\n\n#### Reference\n\n- ğŸ“– [GRDB Reference](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/)\n\n#### Getting Started\n\n- [Installation](#installation)\n- [Database Connections]: Connect to SQLite databases\n\n#### SQLite and SQL\n\n- [SQLite API](#sqlite-api): The low-level SQLite API &bull; [executing updates](#executing-updates) &bull; [fetch queries](#fetch-queries) &bull; [SQL Interpolation]\n\n#### Records and the Query Interface\n\n- [Records](#records): Fetching and persistence methods for your custom structs and class hierarchies\n- [Query Interface](#the-query-interface): A swift way to generate SQL &bull; [create tables, indexes, etc](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseschema) &bull; [requests](#requests) â€¢ [associations between record types](Documentation/AssociationsBasics.md)\n\n#### Application Tools\n\n- [Migrations]: Transform your database as your application evolves.\n- [Full-Text Search]: Perform efficient and customizable full-text searches.\n- [Database Observation]: Observe database changes and transactions.\n- [Encryption](#encryption): Encrypt your database with SQLCipher.\n- [Backup](#backup): Dump the content of a database to another.\n- [Interrupt a Database](#interrupt-a-database): Abort any pending database operation.\n- [Sharing a Database]: How to share an SQLite database between multiple processes - recommendations for App Group containers, App Extensions, App Sandbox, and file coordination.\n\n#### Good to Know\n\n- [Concurrency]: How to access databases in a multi-threaded application.\n- [Combine](Documentation/Combine.md): Access and observe the database with Combine publishers.\n- [Avoiding SQL Injection](#avoiding-sql-injection)\n- [Error Handling](#error-handling)\n- [Unicode](#unicode)\n- [Memory Management](#memory-management)\n- [Data Protection](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseconnections)\n- :bulb: [Migrating From GRDB 6 to GRDB 7](Documentation/GRDB7MigrationGuide.md)\n- :bulb: [Why Adopt GRDB?](Documentation/WhyAdoptGRDB.md)\n- :bulb: [Recommended Practices for Designing Record Types](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/recordrecommendedpractices)\n\n#### Companion Libraries\n\n- [GRDBQuery](https://github.com/groue/GRDBQuery): Access and observe the database from your SwiftUI views.\n- [GRDBSnapshotTesting](https://github.com/groue/GRDBSnapshotTesting): Test your database. \n\n**[FAQ]**\n\n**[Sample Code](#sample-code)**\n\n\nInstallation\n============\n\n**The installation procedures below have GRDB use the version of SQLite that ships with the target operating system.**\n\nSee [Encryption](#encryption) for the installation procedure of GRDB with SQLCipher.\n\nSee [Custom SQLite builds](Documentation/CustomSQLiteBuilds.md) for the installation procedure of GRDB with a customized build of SQLite.\n\n\n## Swift Package Manager\n\nThe [Swift Package Manager](https://swift.org/package-manager/) automates the distribution of Swift code. To use GRDB with SPM, add a dependency to `https://github.com/groue/GRDB.swift.git`\n\nGRDB offers two libraries, `GRDB` and `GRDB-dynamic`. Pick only one. When in doubt, prefer `GRDB`. The `GRDB-dynamic` library can reveal useful if you are going to link it with multiple targets within your app and only wish to link to a shared, dynamic framework once. See [How to link a Swift Package as dynamic](https://forums.swift.org/t/how-to-link-a-swift-package-as-dynamic/32062) for more information.\n\n> **Note**: Linux support is provided by contributors. It is not automatically tested, and not officially maintained. If you notice a build or runtime failure on Linux, please open a pull request with the necessary fix, thank you!\n\n\n## CocoaPods\n\n[CocoaPods](http://cocoapods.org/) is a dependency manager for Xcode projects. To use GRDB with CocoaPods (version 1.2 or higher), specify in your `Podfile`:\n\n```ruby\npod 'GRDB.swift'\n```\n\nGRDB can be installed as a framework, or a static library.\n\n**Important Note for CocoaPods installation**\n\nDue to an [issue](https://github.com/CocoaPods/CocoaPods/issues/11839) in CocoaPods, it is currently not possible to deploy new versions of GRDB to CocoaPods. The last version available on CocoaPods is 6.24.1. To install later versions of GRDB using CocoaPods, use one of the following workarounds:\n\n- Depend on the `GRDB7` branch. This is more or less equivalent to what `pod 'GRDB.swift', '~> 7.0'` would normally do, if CocoaPods would accept new GRDB versions to be published:\n\n    ```ruby\n    # Can't use semantic versioning due to https://github.com/CocoaPods/CocoaPods/issues/11839\n    pod 'GRDB.swift', git: 'https://github.com/groue/GRDB.swift.git', branch: 'GRDB7'\n    ```\n\n- Depend on a specific version explicitly (Replace the tag with the version you want to use):\n\n    ```ruby\n    # Can't use semantic versioning due to https://github.com/CocoaPods/CocoaPods/issues/11839\n    # Replace the tag with the tag that you want to use.\n    pod 'GRDB.swift', git: 'https://github.com/groue/GRDB.swift.git', tag: 'v6.29.0' \n    ```\n\n## Carthage\n\n[Carthage](https://github.com/Carthage/Carthage) is **unsupported**. For some context about this decision, see [#433](https://github.com/groue/GRDB.swift/issues/433).\n\n\n## Manually\n\n1. [Download](https://github.com/groue/GRDB.swift/releases) a copy of GRDB, or clone its repository and make sure you checkout the latest tagged version.\n\n2. Embed the `GRDB.xcodeproj` project in your own project.\n\n3. Add the `GRDB` target in the **Target Dependencies** section of the **Build Phases** tab of your application target (extension target for WatchOS).\n\n4. Add the `GRDB.framework` to the **Embedded Binaries** section of the **General**  tab of your application target (extension target for WatchOS).\n\n\nDatabase Connections\n====================\n\nGRDB provides two classes for accessing SQLite databases: [`DatabaseQueue`] and [`DatabasePool`]:\n\n```swift\nimport GRDB\n\n// Pick one:\nlet dbQueue = try DatabaseQueue(path: \"/path/to/database.sqlite\")\nlet dbPool = try DatabasePool(path: \"/path/to/database.sqlite\")\n```\n\nThe differences are:\n\n- Database pools allow concurrent database accesses (this can improve the performance of multithreaded applications).\n- Database pools open your SQLite database in the [WAL mode](https://www.sqlite.org/wal.html) (unless read-only).\n- Database queues support [in-memory databases](https://www.sqlite.org/inmemorydb.html).\n\n**If you are not sure, choose [`DatabaseQueue`].** You will always be able to switch to [`DatabasePool`] later.\n\nFor more information and tips when opening connections, see [Database Connections](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseconnections).\n\n\nSQLite API\n==========\n\n**In this section of the documentation, we will talk SQL.** Jump to the [query interface](#the-query-interface) if SQL is not your cup of tea.\n\n- [Executing Updates](#executing-updates)\n- [Fetch Queries](#fetch-queries)\n    - [Fetching Methods](#fetching-methods)\n    - [Row Queries](#row-queries)\n    - [Value Queries](#value-queries)\n- [Values](#values)\n    - [Data](#data-and-memory-savings)\n    - [Date and DateComponents](#date-and-datecomponents)\n    - [NSNumber, NSDecimalNumber, and Decimal](#nsnumber-nsdecimalnumber-and-decimal)\n    - [Swift enums](#swift-enums)\n    - [`DatabaseValueConvertible`]: the protocol for custom value types\n- [Transactions and Savepoints]\n- [SQL Interpolation]\n\nAdvanced topics:\n\n- [Prepared Statements]\n- [Custom SQL Functions and Aggregates](#custom-sql-functions-and-aggregates)\n- [Database Schema Introspection](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseschemaintrospection)\n- [Row Adapters](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/rowadapter)\n- [Raw SQLite Pointers](#raw-sqlite-pointers)\n\n\n## Executing Updates\n\nOnce granted with a [database connection], the [`execute(sql:arguments:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/database/execute(sql:arguments:)) method executes the SQL statements that do not return any database row, such as `CREATE TABLE`, `INSERT`, `DELETE`, `ALTER`, etc.\n\nFor example:\n\n```swift\ntry dbQueue.write { db in\n    try db.execute(sql: \"\"\"\n        CREATE TABLE player (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            name TEXT NOT NULL,\n            score INT)\n        \"\"\")\n    \n    try db.execute(\n        sql: \"INSERT INTO player (name, score) VALUES (?, ?)\",\n        arguments: [\"Barbara\", 1000])\n    \n    try db.execute(\n        sql: \"UPDATE player SET score = :score WHERE id = :id\",\n        arguments: [\"score\": 1000, \"id\": 1])\n    }\n}\n```\n\nThe `?` and colon-prefixed keys like `:score` in the SQL query are the **statements arguments**. You pass arguments with arrays or dictionaries, as in the example above. See [Values](#values) for more information on supported arguments types (Bool, Int, String, Date, Swift enums, etc.), and [`StatementArguments`] for a detailed documentation of SQLite arguments.\n\nYou can also embed query arguments right into your SQL queries, with [`execute(literal:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/database/execute(literal:)), as in the example below. See [SQL Interpolation] for more details.\n\n```swift\ntry dbQueue.write { db in\n    let name = \"O'Brien\"\n    let score = 550\n    try db.execute(literal: \"\"\"\n        INSERT INTO player (name, score) VALUES (\\(name), \\(score))\n        \"\"\")\n}\n```\n\n**Never ever embed values directly in your raw SQL strings**. See [Avoiding SQL Injection](#avoiding-sql-injection) for more information:\n\n```swift\n// WRONG: don't embed values in raw SQL strings\nlet id = 123\nlet name = textField.text\ntry db.execute(\n    sql: \"UPDATE player SET name = '\\(name)' WHERE id = \\(id)\")\n\n// CORRECT: use arguments dictionary\ntry db.execute(\n    sql: \"UPDATE player SET name = :name WHERE id = :id\",\n    arguments: [\"name\": name, \"id\": id])\n\n// CORRECT: use arguments array\ntry db.execute(\n    sql: \"UPDATE player SET name = ? WHERE id = ?\",\n    arguments: [name, id])\n\n// CORRECT: use SQL Interpolation\ntry db.execute(\n    literal: \"UPDATE player SET name = \\(name) WHERE id = \\(id)\")\n```\n\n**Join multiple statements with a semicolon**:\n\n```swift\ntry db.execute(sql: \"\"\"\n    INSERT INTO player (name, score) VALUES (?, ?);\n    INSERT INTO player (name, score) VALUES (?, ?);\n    \"\"\", arguments: [\"Arthur\", 750, \"Barbara\", 1000])\n\ntry db.execute(literal: \"\"\"\n    INSERT INTO player (name, score) VALUES (\\(\"Arthur\"), \\(750));\n    INSERT INTO player (name, score) VALUES (\\(\"Barbara\"), \\(1000));\n    \"\"\")\n```\n\nWhen you want to make sure that a single statement is executed, use a prepared [`Statement`].\n\n**After an INSERT statement**, you can get the row ID of the inserted row with [`lastInsertedRowID`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/database/lastinsertedrowid):\n\n```swift\ntry db.execute(\n    sql: \"INSERT INTO player (name, score) VALUES (?, ?)\",\n    arguments: [\"Arthur\", 1000])\nlet playerId = db.lastInsertedRowID\n```\n\nDon't miss [Records](#records), that provide classic **persistence methods**:\n\n```swift\nvar player = Player(name: \"Arthur\", score: 1000)\ntry player.insert(db)\nlet playerId = player.id\n```\n\n\n## Fetch Queries\n\n[Database connections] let you fetch database rows, plain values, and custom models aka \"records\".\n\n**Rows** are the raw results of SQL queries:\n\n```swift\ntry dbQueue.read { db in\n    if let row = try Row.fetchOne(db, sql: \"SELECT * FROM wine WHERE id = ?\", arguments: [1]) {\n        let name: String = row[\"name\"]\n        let color: Color = row[\"color\"]\n        print(name, color)\n    }\n}\n```\n\n\n**Values** are the Bool, Int, String, Date, Swift enums, etc. stored in row columns:\n\n```swift\ntry dbQueue.read { db in\n    let urls = try URL.fetchCursor(db, sql: \"SELECT url FROM wine\")\n    while let url = try urls.next() {\n        print(url)\n    }\n}\n```\n\n\n**Records** are your application objects that can initialize themselves from rows:\n\n```swift\nlet wines = try dbQueue.read { db in\n    try Wine.fetchAll(db, sql: \"SELECT * FROM wine\")\n}\n```\n\n- [Fetching Methods](#fetching-methods) and [Cursors](#cursors)\n- [Row Queries](#row-queries)\n- [Value Queries](#value-queries)\n- [Records](#records)\n\n\n### Fetching Methods\n\n**Throughout GRDB**, you can always fetch *cursors*, *arrays*, *sets*, or *single values* of any fetchable type (database [row](#row-queries), simple [value](#value-queries), or custom [record](#records)):\n\n```swift\ntry Row.fetchCursor(...) // A Cursor of Row\ntry Row.fetchAll(...)    // [Row]\ntry Row.fetchSet(...)    // Set<Row>\ntry Row.fetchOne(...)    // Row?\n```\n\n- `fetchCursor` returns a **[cursor](#cursors)** over fetched values:\n    \n    ```swift\n    let rows = try Row.fetchCursor(db, sql: \"SELECT ...\") // A Cursor of Row\n    ```\n    \n- `fetchAll` returns an **array**:\n    \n    ```swift\n    let players = try Player.fetchAll(db, sql: \"SELECT ...\") // [Player]\n    ```\n\n- `fetchSet` returns a **set**:\n    \n    ```swift\n    let names = try String.fetchSet(db, sql: \"SELECT ...\") // Set<String>\n    ```\n\n- `fetchOne` returns a **single optional value**, and consumes a single database row (if any).\n    \n    ```swift\n    let count = try Int.fetchOne(db, sql: \"SELECT COUNT(*) ...\") // Int?\n    ```\n\n**All those fetching methods require an SQL string that contains a single SQL statement.** When you want to fetch from multiple statements joined with a semicolon, iterate the multiple [prepared statements] found in the SQL string.\n\n### Cursors\n\nğŸ“– [`Cursor`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/cursor)\n\n**Whenever you consume several rows from the database, you can fetch an Array, a Set, or a Cursor**.\n\nThe `fetchAll()` and `fetchSet()` methods return regular Swift array and sets, that you iterate like all other arrays and sets:\n\n```swift\ntry dbQueue.read { db in\n    // [Player]\n    let players = try Player.fetchAll(db, sql: \"SELECT ...\")\n    for player in players {\n        // use player\n    }\n}\n```\n\nUnlike arrays and sets, cursors returned by `fetchCursor()` load their results step after step:\n\n```swift\ntry dbQueue.read { db in\n    // Cursor of Player\n    let players = try Player.fetchCursor(db, sql: \"SELECT ...\")\n    while let player = try players.next() {\n        // use player\n    }\n}\n```\n\n- **Cursors can not be used on any thread**: you must consume a cursor on the dispatch queue it was created in. Particularly, don't extract a cursor out of a database access method:\n    \n    ```swift\n    // Wrong\n    let cursor = try dbQueue.read { db in\n        try Player.fetchCursor(db, ...)\n    }\n    while let player = try cursor.next() { ... }\n    ```\n    \n    Conversely, arrays and sets may be consumed on any thread:\n    \n    ```swift\n    // OK\n    let array = try dbQueue.read { db in\n        try Player.fetchAll(db, ...)\n    }\n    for player in array { ... }\n    ```\n    \n- **Cursors can be iterated only one time.** Arrays and sets can be iterated many times.\n\n- **Cursors iterate database results in a lazy fashion**, and don't consume much memory. Arrays and sets contain copies of database values, and may take a lot of memory when there are many fetched results.\n\n- **Cursors are granted with direct access to SQLite,** unlike arrays and sets that have to take the time to copy database values. If you look after extra performance, you may prefer cursors.\n\n- **Cursors can feed Swift collections.**\n    \n    You will most of the time use `fetchAll` or `fetchSet` when you want an array or a set. For more specific needs, you may prefer one of the initializers below. All of them accept an extra optional `minimumCapacity` argument which helps optimizing your app when you have an idea of the number of elements in a cursor (the built-in `fetchAll` and `fetchSet` do not perform such an optimization).\n    \n    **Arrays** and all types conforming to `RangeReplaceableCollection`:\n    \n    ```swift\n    // [String]\n    let cursor = try String.fetchCursor(db, ...)\n    let array = try Array(cursor)\n    ```\n    \n    **Sets**:\n    \n    ```swift\n    // Set<Int>\n    let cursor = try Int.fetchCursor(db, ...)\n    let set = try Set(cursor)\n    ```\n    \n    **Dictionaries**:\n    \n    ```swift\n    // [Int64: [Player]]\n    let cursor = try Player.fetchCursor(db)\n    let dictionary = try Dictionary(grouping: cursor, by: { $0.teamID })\n    \n    // [Int64: Player]\n    let cursor = try Player.fetchCursor(db).map { ($0.id, $0) }\n    let dictionary = try Dictionary(uniqueKeysWithValues: cursor)\n    ```\n\n- **Cursors adopt the [Cursor](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/cursor) protocol, which looks a lot like standard [lazy sequences](https://developer.apple.com/reference/swift/lazysequenceprotocol) of Swift.** As such, cursors come with many convenience methods: `compactMap`, `contains`, `dropFirst`, `dropLast`, `drop(while:)`, `enumerated`, `filter`, `first`, `flatMap`, `forEach`, `joined`, `joined(separator:)`, `max`, `max(by:)`, `min`, `min(by:)`, `map`, `prefix`, `prefix(while:)`, `reduce`, `reduce(into:)`, `suffix`:\n    \n    ```swift\n    // Prints all Github links\n    try URL\n        .fetchCursor(db, sql: \"SELECT url FROM link\")\n        .filter { url in url.host == \"github.com\" }\n        .forEach { url in print(url) }\n    \n    // An efficient cursor of coordinates:\n    let locations = try Row.\n        .fetchCursor(db, sql: \"SELECT latitude, longitude FROM place\")\n        .map { row in\n            CLLocationCoordinate2D(latitude: row[0], longitude: row[1])\n        }\n    ```\n\n- **Cursors are not Swift sequences.** That's because Swift sequences can't handle iteration errors, when reading SQLite results may fail at any time.\n\n- **Cursors require a little care**:\n    \n    - Don't modify the results during a cursor iteration:\n        \n        ```swift\n        // Undefined behavior\n        while let player = try players.next() {\n            try db.execute(sql: \"DELETE ...\")\n        }\n        ```\n    \n    - Don't turn a cursor of `Row` into an array or a set. You would not get the distinct rows you expect. To get a array of rows, use `Row.fetchAll(...)`. To get a set of rows, use `Row.fetchSet(...)`. Generally speaking, make sure you copy a row whenever you extract it from a cursor for later use: `row.copy()`.\n\nIf you don't see, or don't care about the difference, use arrays. If you care about memory and performance, use cursors when appropriate.\n\n\n### Row Queries\n\n- [Fetching Rows](#fetching-rows)\n- [Column Values](#column-values)\n- [DatabaseValue](#databasevalue)\n- [Rows as Dictionaries](#rows-as-dictionaries)\n- ğŸ“– [`Row`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/row)\n\n\n#### Fetching Rows\n\nFetch **cursors** of rows, **arrays**, **sets**, or **single** rows (see [fetching methods](#fetching-methods)):\n\n```swift\ntry dbQueue.read { db in\n    try Row.fetchCursor(db, sql: \"SELECT ...\", arguments: ...) // A Cursor of Row\n    try Row.fetchAll(db, sql: \"SELECT ...\", arguments: ...)    // [Row]\n    try Row.fetchSet(db, sql: \"SELECT ...\", arguments: ...)    // Set<Row>\n    try Row.fetchOne(db, sql: \"SELECT ...\", arguments: ...)    // Row?\n    \n    let rows = try Row.fetchCursor(db, sql: \"SELECT * FROM wine\")\n    while let row = try rows.next() {\n        let name: String = row[\"name\"]\n        let color: Color = row[\"color\"]\n        print(name, color)\n    }\n}\n\nlet rows = try dbQueue.read { db in\n    try Row.fetchAll(db, sql: \"SELECT * FROM player\")\n}\n```\n\nArguments are optional arrays or dictionaries that fill the positional `?` and colon-prefixed keys like `:name` in the query:\n\n```swift\nlet rows = try Row.fetchAll(db,\n    sql: \"SELECT * FROM player WHERE name = ?\",\n    arguments: [\"Arthur\"])\n\nlet rows = try Row.fetchAll(db,\n    sql: \"SELECT * FROM player WHERE name = :name\",\n    arguments: [\"name\": \"Arthur\"])\n```\n\nSee [Values](#values) for more information on supported arguments types (Bool, Int, String, Date, Swift enums, etc.), and [`StatementArguments`] for a detailed documentation of SQLite arguments.\n\nUnlike row arrays that contain copies of the database rows, row cursors are close to the SQLite metal, and require a little care:\n\n> **Note**: **Don't turn a cursor of `Row` into an array or a set**. You would not get the distinct rows you expect. To get a array of rows, use `Row.fetchAll(...)`. To get a set of rows, use `Row.fetchSet(...)`. Generally speaking, make sure you copy a row whenever you extract it from a cursor for later use: `row.copy()`.\n\n\n#### Column Values\n\n**Read column values** by index or column name:\n\n```swift\nlet name: String = row[0]      // 0 is the leftmost column\nlet name: String = row[\"name\"] // Leftmost matching column - lookup is case-insensitive\nlet name: String = row[Column(\"name\")] // Using query interface's Column\n```\n\nMake sure to ask for an optional when the value may be NULL:\n\n```swift\nlet name: String? = row[\"name\"]\n```\n\nThe `row[]` subscript returns the type you ask for. See [Values](#values) for more information on supported value types:\n\n```swift\nlet bookCount: Int     = row[\"bookCount\"]\nlet bookCount64: Int64 = row[\"bookCount\"]\nlet hasBooks: Bool     = row[\"bookCount\"] // false when 0\n\nlet string: String     = row[\"date\"]      // \"2015-09-11 18:14:15.123\"\nlet date: Date         = row[\"date\"]      // Date\nself.date = row[\"date\"] // Depends on the type of the property.\n```\n\nYou can also use the `as` type casting operator:\n\n```swift\nrow[...] as Int\nrow[...] as Int?\n```\n\nThrowing accessors exist as well. Their use is not encouraged, because a database decoding error is a programming error. If an application stores invalid data in the database file, that is a bug that needs to be fixed:\n\n```swift\nlet name = try row.decode(String.self, atIndex: 0)\nlet bookCount = try row.decode(Int.self, forColumn: \"bookCount\")\n```\n\n> **Warning**: avoid the `as!` and `as?` operators:\n> \n> ```swift\n> if let int = row[...] as? Int { ... } // BAD - doesn't work\n> if let int = row[...] as Int? { ... } // GOOD\n> ```\n\n> **Warning**: avoid nil-coalescing row values, and prefer the `coalesce` method instead:\n>\n> ```swift\n> let name: String? = row[\"nickname\"] ?? row[\"name\"]     // BAD - doesn't work\n> let name: String? = row.coalesce([\"nickname\", \"name\"]) // GOOD\n> ```\n\nGenerally speaking, you can extract the type you need, provided it can be converted from the underlying SQLite value:\n\n- **Successful conversions include:**\n    \n    - All numeric SQLite values to all numeric Swift types, and Bool (zero is the only false boolean).\n    - Text SQLite values to Swift String.\n    - Blob SQLite values to Foundation Data.\n    \n    See [Values](#values) for more information on supported types (Bool, Int, String, Date, Swift enums, etc.)\n    \n- **NULL returns nil.**\n    \n    ```swift\n    let row = try Row.fetchOne(db, sql: \"SELECT NULL\")!\n    row[0] as Int? // nil\n    row[0] as Int  // fatal error: could not convert NULL to Int.\n    ```\n    \n    There is one exception, though: the [DatabaseValue](#databasevalue) type:\n    \n    ```swift\n    row[0] as DatabaseValue // DatabaseValue.null\n    ```\n    \n- **Missing columns return nil.**\n    \n    ```swift\n    let row = try Row.fetchOne(db, sql: \"SELECT 'foo' AS foo\")!\n    row[\"missing\"] as String? // nil\n    row[\"missing\"] as String  // fatal error: no such column: missing\n    ```\n    \n    You can explicitly check for a column presence with the `hasColumn` method.\n\n- **Invalid conversions throw a fatal error.**\n    \n    ```swift\n    let row = try Row.fetchOne(db, sql: \"SELECT 'Momâ€™s birthday'\")!\n    row[0] as String // \"Momâ€™s birthday\"\n    row[0] as Date?  // fatal error: could not convert \"Momâ€™s birthday\" to Date.\n    row[0] as Date   // fatal error: could not convert \"Momâ€™s birthday\" to Date.\n    \n    let row = try Row.fetchOne(db, sql: \"SELECT 256\")!\n    row[0] as Int    // 256\n    row[0] as UInt8? // fatal error: could not convert 256 to UInt8.\n    row[0] as UInt8  // fatal error: could not convert 256 to UInt8.\n    ```\n    \n    Those conversion fatal errors can be avoided with the [DatabaseValue](#databasevalue) type:\n    \n    ```swift\n    let row = try Row.fetchOne(db, sql: \"SELECT 'Momâ€™s birthday'\")!\n    let dbValue: DatabaseValue = row[0]\n    if dbValue.isNull {\n        // Handle NULL\n    } else if let date = Date.fromDatabaseValue(dbValue) {\n        // Handle valid date\n    } else {\n        // Handle invalid date\n    }\n    ```\n    \n    This extra verbosity is the consequence of having to deal with an untrusted database: you may consider fixing the content of your database instead. See [Fatal Errors](#fatal-errors) for more information.\n    \n- **SQLite has a weak type system, and provides [convenience conversions](https://www.sqlite.org/c3ref/column_blob.html) that can turn String to Int, Double to Blob, etc.**\n    \n    GRDB will sometimes let those conversions go through:\n    \n    ```swift\n    let rows = try Row.fetchCursor(db, sql: \"SELECT '20 small cigars'\")\n    while let row = try rows.next() {\n        row[0] as Int   // 20\n    }\n    ```\n    \n    Don't freak out: those conversions did not prevent SQLite from becoming the immensely successful database engine you want to use. And GRDB adds safety checks described just above. You can also prevent those convenience conversions altogether by using the [DatabaseValue](#databasevalue) type.\n\n\n#### DatabaseValue\n\nğŸ“– [`DatabaseValue`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasevalue)\n\n**`DatabaseValue` is an intermediate type between SQLite and your values, which gives information about the raw value stored in the database.**\n\nYou get `DatabaseValue` just like other value types:\n\n```swift\nlet dbValue: DatabaseValue = row[0]\nlet dbValue: DatabaseValue? = row[\"name\"] // nil if and only if column does not exist\n\n// Check for NULL:\ndbValue.isNull // Bool\n\n// The stored value:\ndbValue.storage.value // Int64, Double, String, Data, or nil\n\n// All the five storage classes supported by SQLite:\nswitch dbValue.storage {\ncase .null:                 print(\"NULL\")\ncase .int64(let int64):     print(\"Int64: \\(int64)\")\ncase .double(let double):   print(\"Double: \\(double)\")\ncase .string(let string):   print(\"String: \\(string)\")\ncase .blob(let data):       print(\"Data: \\(data)\")\n}\n```\n\nYou can extract regular [values](#values) (Bool, Int, String, Date, Swift enums, etc.) from `DatabaseValue` with the [fromDatabaseValue()](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasevalueconvertible/fromdatabasevalue(_:)-21zzv) method:\n\n```swift\nlet dbValue: DatabaseValue = row[\"bookCount\"]\nlet bookCount   = Int.fromDatabaseValue(dbValue)   // Int?\nlet bookCount64 = Int64.fromDatabaseValue(dbValue) // Int64?\nlet hasBooks    = Bool.fromDatabaseValue(dbValue)  // Bool?, false when 0\n\nlet dbValue: DatabaseValue = row[\"date\"]\nlet string = String.fromDatabaseValue(dbValue)     // \"2015-09-11 18:14:15.123\"\nlet date   = Date.fromDatabaseValue(dbValue)       // Date?\n```\n\n`fromDatabaseValue` returns nil for invalid conversions:\n\n```swift\nlet row = try Row.fetchOne(db, sql: \"SELECT 'Momâ€™s birthday'\")!\nlet dbValue: DatabaseValue = row[0]\nlet string = String.fromDatabaseValue(dbValue) // \"Momâ€™s birthday\"\nlet int    = Int.fromDatabaseValue(dbValue)    // nil\nlet date   = Date.fromDatabaseValue(dbValue)   // nil\n```\n\n\n#### Rows as Dictionaries\n\nRow adopts the standard [RandomAccessCollection](https://developer.apple.com/documentation/swift/randomaccesscollection) protocol, and can be seen as a dictionary of [DatabaseValue](#databasevalue):\n\n```swift\n// All the (columnName, dbValue) tuples, from left to right:\nfor (columnName, dbValue) in row {\n    ...\n}\n```\n\n**You can build rows from dictionaries** (standard Swift dictionaries and NSDictionary). See [Values](#values) for more information on supported types:\n\n```swift\nlet row: Row = [\"name\": \"foo\", \"date\": nil]\nlet row = Row([\"name\": \"foo\", \"date\": nil])\nlet row = Row(/* [AnyHashable: Any] */) // nil if invalid dictionary\n```\n\nYet rows are not real dictionaries: they may contain duplicate columns:\n\n```swift\nlet row = try Row.fetchOne(db, sql: \"SELECT 1 AS foo, 2 AS foo\")!\nrow.columnNames    // [\"foo\", \"foo\"]\nrow.databaseValues // [1, 2]\nrow[\"foo\"]         // 1 (leftmost matching column)\nfor (columnName, dbValue) in row { ... } // (\"foo\", 1), (\"foo\", 2)\n```\n\n**When you build a dictionary from a row**, you have to disambiguate identical columns, and choose how to present database values. For example:\n\n- A `[String: DatabaseValue]` dictionary that keeps leftmost value in case of duplicated column name:\n\n    ```swift\n    let dict = Dictionary(row, uniquingKeysWith: { (left, _) in left })\n    ```\n\n- A `[String: AnyObject]` dictionary which keeps rightmost value in case of duplicated column name. This dictionary is identical to FMResultSet's resultDictionary from FMDB. It contains NSNull values for null columns, and can be shared with Objective-C:\n\n    ```swift\n    let dict = Dictionary(\n        row.map { (column, dbValue) in\n            (column, dbValue.storage.value as AnyObject)\n        },\n        uniquingKeysWith: { (_, right) in right })\n    ```\n\n- A `[String: Any]` dictionary that can feed, for example, JSONSerialization:\n    \n    ```swift\n    let dict = Dictionary(\n        row.map { (column, dbValue) in\n            (column, dbValue.storage.value)\n        },\n        uniquingKeysWith: { (left, _) in left })\n    ```\n\nSee the documentation of [`Dictionary.init(_:uniquingKeysWith:)`](https://developer.apple.com/documentation/swift/dictionary/2892961-init) for more information.\n\n\n### Value Queries\n\nğŸ“– [`DatabaseValueConvertible`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasevalueconvertible)\n\n**Instead of rows, you can directly fetch values.** There are many supported [value types](#values) (Bool, Int, String, Date, Swift enums, etc.).\n\nLike rows, fetch values as **cursors**, **arrays**, **sets**, or **single** values (see [fetching methods](#fetching-methods)). Values are extracted from the leftmost column of the SQL queries:\n\n```swift\ntry dbQueue.read { db in\n    try Int.fetchCursor(db, sql: \"SELECT ...\", arguments: ...) // A Cursor of Int\n    try Int.fetchAll(db, sql: \"SELECT ...\", arguments: ...)    // [Int]\n    try Int.fetchSet(db, sql: \"SELECT ...\", arguments: ...)    // Set<Int>\n    try Int.fetchOne(db, sql: \"SELECT ...\", arguments: ...)    // Int?\n    \n    let maxScore = try Int.fetchOne(db, sql: \"SELECT MAX(score) FROM player\") // Int?\n    let names = try String.fetchAll(db, sql: \"SELECT name FROM player\")       // [String]\n}\n```\n\n`Int.fetchOne` returns nil in two cases: either the SELECT statement yielded no row, or one row with a NULL value:\n\n```swift\n// No row:\ntry Int.fetchOne(db, sql: \"SELECT 42 WHERE FALSE\") // nil\n\n// One row with a NULL value:\ntry Int.fetchOne(db, sql: \"SELECT NULL\")           // nil\n\n// One row with a non-NULL value:\ntry Int.fetchOne(db, sql: \"SELECT 42\")             // 42\n```\n\nFor requests which may contain NULL, fetch optionals:\n\n```swift\ntry dbQueue.read { db in\n    try Optional<Int>.fetchCursor(db, sql: \"SELECT ...\", arguments: ...) // A Cursor of Int?\n    try Optional<Int>.fetchAll(db, sql: \"SELECT ...\", arguments: ...)    // [Int?]\n    try Optional<Int>.fetchSet(db, sql: \"SELECT ...\", arguments: ...)    // Set<Int?>\n}\n```\n\n> :bulb: **Tip**: One advanced use case, when you fetch one value, is to distinguish the cases of a statement that yields no row, or one row with a NULL value. To do so, use `Optional<Int>.fetchOne`, which returns a double optional `Int??`:\n> \n> ```swift\n> // No row:\n> try Optional<Int>.fetchOne(db, sql: \"SELECT 42 WHERE FALSE\") // .none\n> // One row with a NULL value:\n> try Optional<Int>.fetchOne(db, sql: \"SELECT NULL\")           // .some(.none)\n> // One row with a non-NULL value:\n> try Optional<Int>.fetchOne(db, sql: \"SELECT 42\")             // .some(.some(42))\n> ```\n\nThere are many supported value types (Bool, Int, String, Date, Swift enums, etc.). See [Values](#values) for more information.\n\n\n## Values\n\nGRDB ships with built-in support for the following value types:\n\n- **Swift Standard Library**: Bool, Double, Float, all signed and unsigned integer types, String, [Swift enums](#swift-enums).\n    \n- **Foundation**: [Data](#data-and-memory-savings), [Date](#date-and-datecomponents), [DateComponents](#date-and-datecomponents), [Decimal](#nsnumber-nsdecimalnumber-and-decimal), NSNull, [NSNumber](#nsnumber-nsdecimalnumber-and-decimal), NSString, URL, [UUID](#uuid).\n    \n- **CoreGraphics**: CGFloat.\n\n- **[DatabaseValue](#databasevalue)**, the type which gives information about the raw value stored in the database.\n\n- **Full-Text Patterns**: [FTS3Pattern](Documentation/FullTextSearch.md#fts3pattern) and [FTS5Pattern](Documentation/FullTextSearch.md#fts5pattern).\n\n- Generally speaking, all types that adopt the [`DatabaseValueConvertible`] protocol.\n\nValues can be used as [statement arguments](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statementarguments):\n\n```swift\nlet url: URL = ...\nlet verified: Bool = ...\ntry db.execute(\n    sql: \"INSERT INTO link (url, verified) VALUES (?, ?)\",\n    arguments: [url, verified])\n```\n\nValues can be [extracted from rows](#column-values):\n\n```swift\nlet rows = try Row.fetchCursor(db, sql: \"SELECT * FROM link\")\nwhile let row = try rows.next() {\n    let url: URL = row[\"url\"]\n    let verified: Bool = row[\"verified\"]\n}\n```\n\nValues can be [directly fetched](#value-queries):\n\n```swift\nlet urls = try URL.fetchAll(db, sql: \"SELECT url FROM link\")  // [URL]\n```\n\nUse values in [Records](#records):\n\n```swift\nstruct Link: FetchableRecord {\n    var url: URL\n    var isVerified: Bool\n    \n    init(row: Row) {\n        url = row[\"url\"]\n        isVerified = row[\"verified\"]\n    }\n}\n```\n\nUse values in the [query interface](#the-query-interface):\n\n```swift\nlet url: URL = ...\nlet link = try Link.filter { $0.url == url }.fetchOne(db)\n```\n\n\n### Data (and Memory Savings)\n\n**Data** suits the BLOB SQLite columns. It can be stored and fetched from the database just like other [values](#values):\n\n```swift\nlet rows = try Row.fetchCursor(db, sql: \"SELECT data, ...\")\nwhile let row = try rows.next() {\n    let data: Data = row[\"data\"]\n}\n```\n\nAt each step of the request iteration, the `row[]` subscript creates *two copies* of the database bytes: one fetched by SQLite, and another, stored in the Swift Data value.\n\n**You have the opportunity to save memory** by not copying the data fetched by SQLite:\n\n```swift\nwhile let row = try rows.next() {\n    try row.withUnsafeData(name: \"data\") { (data: Data?) in\n        ...\n    }\n}\n```\n\nThe non-copied data does not live longer than the iteration step: make sure that you do not use it past this point.\n\n\n### Date and DateComponents\n\n[**Date**](#date) and [**DateComponents**](#datecomponents) can be stored and fetched from the database.\n\nHere is how GRDB supports the various [date formats](https://www.sqlite.org/lang_datefunc.html) supported by SQLite:\n\n| SQLite format                | Date               | DateComponents |\n|:---------------------------- |:------------------:|:--------------:|\n| YYYY-MM-DD                   |       Read Â¹       | Read / Write   |\n| YYYY-MM-DD HH:MM             |       Read Â¹ Â²     | Read Â² / Write |\n| YYYY-MM-DD HH:MM:SS          |       Read Â¹ Â²     | Read Â² / Write |\n| YYYY-MM-DD HH:MM:SS.SSS      | Read Â¹ Â² / Write Â¹ | Read Â² / Write |\n| YYYY-MM-DD**T**HH:MM         |       Read Â¹ Â²     |      Read Â²    |\n| YYYY-MM-DD**T**HH:MM:SS      |       Read Â¹ Â²     |      Read Â²    |\n| YYYY-MM-DD**T**HH:MM:SS.SSS  |       Read Â¹ Â²     |      Read Â²    |\n| HH:MM                        |                    | Read Â² / Write |\n| HH:MM:SS                     |                    | Read Â² / Write |\n| HH:MM:SS.SSS                 |                    | Read Â² / Write |\n| Timestamps since unix epoch  |       Read Â³       |                |\n| `now`                        |                    |                |\n\nÂ¹ Missing components are assumed to be zero. Dates are stored and read in the UTC time zone, unless the format is followed by a timezone indicator â½Â²â¾.\n\nÂ² This format may be optionally followed by a timezone indicator of the form `[+-]HH:MM` or just `Z`.\n\nÂ³ GRDB 2+ interprets numerical values as timestamps that fuel `Date(timeIntervalSince1970:)`. Previous GRDB versions used to interpret numbers as [julian days](https://en.wikipedia.org/wiki/Julian_day). Julian days are still supported, with the `Date(julianDay:)` initializer.\n\n> **Warning**: the range of valid years in the SQLite date formats is 0000-9999. You will need to pick another date format when your application needs to process years outside of this range. See the following chapters.\n\n\n#### Date\n\n**Date** can be stored and fetched from the database just like other [values](#values):\n\n```swift\ntry db.execute(\n    sql: \"INSERT INTO player (creationDate, ...) VALUES (?, ...)\",\n    arguments: [Date(), ...])\n\nlet row = try Row.fetchOne(db, ...)!\nlet creationDate: Date = row[\"creationDate\"]\n```\n\nDates are stored using the format \"YYYY-MM-DD HH:MM:SS.SSS\" in the UTC time zone. It is precise to the millisecond.\n\n> **Note**: this format was chosen because it is the only format that is:\n> \n> - Comparable (`ORDER BY date` works)\n> - Comparable with the SQLite keyword CURRENT_TIMESTAMP (`WHERE date > CURRENT_TIMESTAMP` works)\n> - Able to feed [SQLite date & time functions](https://www.sqlite.org/lang_datefunc.html)\n> - Precise enough\n>\n> **Warning**: the range of valid years in the SQLite date format is 0000-9999. You will experience problems with years outside of this range, such as decoding errors, or invalid date computations with [SQLite date & time functions](https://www.sqlite.org/lang_datefunc.html).\n\nSome applications may prefer another date format:\n\n- Some may prefer ISO-8601, with a `T` separator.\n- Some may prefer ISO-8601, with a time zone.\n- Some may need to store years beyond the 0000-9999 range.\n- Some may need sub-millisecond precision.\n- Some may need exact `Date` roundtrip.\n- Etc.\n\n**You should think twice before choosing a different date format:**\n\n- ISO-8601 is about *exchange and communication*, when SQLite is about *storage and data manipulation*. Sharing the same representation in your database and in JSON files only provides a superficial convenience, and should be the least of your priorities. Don't store dates as ISO-8601 without understanding what you lose. For example, ISO-8601 time zones forbid database-level date comparison. \n- Sub-millisecond precision and exact `Date` roundtrip are not as obvious needs as it seems at first sight. Dates generally don't precisely roundtrip as soon as they leave your application anyway, because the other systems your app communicates with use their own date representation (the Android version of your app, the server your application is talking to, etc.) On top of that, `Date` comparison is at least as hard and nasty as [floating point comparison](https://www.google.com/search?q=floating+point+comparison+is+hard).\n\nThe customization of date format is explicit. For example:\n\n```swift\nlet date = Date()\nlet timeInterval = date.timeIntervalSinceReferenceDate\ntry db.execute(\n    sql: \"INSERT INTO player (creationDate, ...) VALUES (?, ...)\",\n    arguments: [timeInterval, ...])\n\nif let row = try Row.fetchOne(db, ...) {\n    let timeInterval: TimeInterval = row[\"creationDate\"]\n    let creationDate = Date(timeIntervalSinceReferenceDate: timeInterval)\n}\n```\n\nSee also [Codable Records] for more date customization options, and [`DatabaseValueConvertible`] if you want to define a Date-wrapping type with customized database representation.\n\n\n#### DateComponents\n\nDateComponents is indirectly supported, through the **DatabaseDateComponents** helper type.\n\nDatabaseDateComponents reads date components from all [date formats supported by SQLite](https://www.sqlite.org/lang_datefunc.html), and stores them in the format of your choice, from HH:MM to YYYY-MM-DD HH:MM:SS.SSS.\n\n> **Warning**: the range of valid years is 0000-9999. You will experience problems with years outside of this range, such as decoding errors, or invalid date computations with [SQLite date & time functions](https://www.sqlite.org/lang_datefunc.html). See [Date](#date) for more information.\n\nDatabaseDateComponents can be stored and fetched from the database just like other [values](#values):\n\n```swift\nlet components = DateComponents()\ncomponents.year = 1973\ncomponents.month = 9\ncomponents.day = 18\n\n// Store \"1973-09-18\"\nlet dbComponents = DatabaseDateComponents(components, format: .YMD)\ntry db.execute(\n    sql: \"INSERT INTO player (birthDate, ...) VALUES (?, ...)\",\n    arguments: [dbComponents, ...])\n\n// Read \"1973-09-18\"\nlet row = try Row.fetchOne(db, sql: \"SELECT birthDate ...\")!\nlet dbComponents: DatabaseDateComponents = row[\"birthDate\"]\ndbComponents.format         // .YMD (the actual format found in the database)\ndbComponents.dateComponents // DateComponents\n```\n\n\n### NSNumber, NSDecimalNumber, and Decimal\n\n**NSNumber** and **Decimal** can be stored and fetched from the database just like other [values](#values).\n\nHere is how GRDB supports the various data types supported by SQLite:\n\n|                 |    Integer   |     Double   |    String    |\n|:--------------- |:------------:|:------------:|:------------:|\n| NSNumber        | Read / Write | Read / Write |     Read     |\n| NSDecimalNumber | Read / Write | Read / Write |     Read     |\n| Decimal         |     Read     |     Read     | Read / Write |\n\n- All three types can decode database integers and doubles:\n\n    ```swift\n    let number = try NSNumber.fetchOne(db, sql: \"SELECT 10\")            // NSNumber\n    let number = try NSDecimalNumber.fetchOne(db, sql: \"SELECT 1.23\")   // NSDecimalNumber\n    let number = try Decimal.fetchOne(db, sql: \"SELECT -100\")           // Decimal\n    ```\n    \n- All three types decode database strings as decimal numbers:\n\n    ```swift\n    let number = try NSNumber.fetchOne(db, sql: \"SELECT '10'\")          // NSDecimalNumber (sic)\n    let number = try NSDecimalNumber.fetchOne(db, sql: \"SELECT '1.23'\") // NSDecimalNumber\n    let number = try Decimal.fetchOne(db, sql: \"SELECT '-100'\")         // Decimal\n    ```\n\n- `NSNumber` and `NSDecimalNumber` send 64-bit signed integers and doubles in the database:\n\n    ```swift\n    // INSERT INTO transfer VALUES (10)\n    try db.execute(sql: \"INSERT INTO transfer VALUES (?)\", arguments: [NSNumber(value: 10)])\n    \n    // INSERT INTO transfer VALUES (10.0)\n    try db.execute(sql: \"INSERT INTO transfer VALUES (?)\", arguments: [NSNumber(value: 10.0)])\n    \n    // INSERT INTO transfer VALUES (10)\n    try db.execute(sql: \"INSERT INTO transfer VALUES (?)\", arguments: [NSDecimalNumber(string: \"10.0\")])\n    \n    // INSERT INTO transfer VALUES (10.5)\n    try db.execute(sql: \"INSERT INTO transfer VALUES (?)\", arguments: [NSDecimalNumber(string: \"10.5\")])\n    ```\n    \n    > **Warning**: since SQLite does not support decimal numbers, sending a non-integer `NSDecimalNumber` can result in a loss of precision during the conversion to double.\n    >\n    > Instead of sending non-integer `NSDecimalNumber` to the database, you may prefer:\n    >\n    > - Send `Decimal` instead (those store decimal strings in the database).\n    > - Send integers instead (for example, store amounts of cents instead of amounts of Euros).\n\n- `Decimal` sends decimal strings in the database:\n\n    ```swift\n    // INSERT INTO transfer VALUES ('10')\n    try db.execute(sql: \"INSERT INTO transfer VALUES (?)\", arguments: [Decimal(10)])\n    \n    // INSERT INTO transfer VALUES ('10.5')\n    try db.execute(sql: \"INSERT INTO transfer VALUES (?)\", arguments: [Decimal(string: \"10.5\")!])\n    ```\n\n\n### UUID\n\n**UUID** can be stored and fetched from the database just like other [values](#values).\n\nGRDB stores uuids as 16-bytes data blobs, and decodes them from both 16-bytes data blobs and strings such as \"E621E1F8-C36C-495A-93FC-0C247A3E6E5F\".\n\n\n### Swift Enums\n\n**Swift enums** and generally all types that adopt the [RawRepresentable](https://developer.apple.com/library/tvos/documentation/Swift/Reference/Swift_RawRepresentable_Protocol/index.html) protocol can be stored and fetched from the database just like their raw [values](#values):\n\n```swift\nenum Color : Int {\n    case red, white, rose\n}\n\nenum Grape : String {\n    case chardonnay, merlot, riesling\n}\n\n// Declare empty DatabaseValueConvertible adoption\nextension Color : DatabaseValueConvertible { }\nextension Grape : DatabaseValueConvertible { }\n\n// Store\ntry db.execute(\n    sql: \"INSERT INTO wine (grape, color) VALUES (?, ?)\",\n    arguments: [Grape.merlot, Color.red])\n\n// Read\nlet rows = try Row.fetchCursor(db, sql: \"SELECT * FROM wine\")\nwhile let row = try rows.next() {\n    let grape: Grape = row[\"grape\"]\n    let color: Color = row[\"color\"]\n}\n```\n\n**When a database value does not match any enum case**, you get a fatal error. This fatal error can be avoided with the [DatabaseValue](#databasevalue) type:\n\n```swift\nlet row = try Row.fetchOne(db, sql: \"SELECT 'syrah'\")!\n\nrow[0] as String  // \"syrah\"\nrow[0] as Grape?  // fatal error: could not convert \"syrah\" to Grape.\nrow[0] as Grape   // fatal error: could not convert \"syrah\" to Grape.\n\nlet dbValue: DatabaseValue = row[0]\nif dbValue.isNull {\n    // Handle NULL\n} else if let grape = Grape.fromDatabaseValue(dbValue) {\n    // Handle valid grape\n} else {\n    // Handle unknown grape\n}\n```\n\n\n## Custom SQL Functions and Aggregates\n\n**SQLite lets you define SQL functions and aggregates.**\n\nA custom SQL function or aggregate extends SQLite:\n\n```sql\nSELECT reverse(name) FROM player;   -- custom function\nSELECT maxLength(name) FROM player; -- custom aggregate\n```\n\n- [Custom SQL Functions](#custom-sql-functions)\n- [Custom Aggregates](#custom-aggregates)\n\n\n### Custom SQL Functions\n\nğŸ“– [`DatabaseFunction`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasefunction)\n\nA *function* argument takes an array of [DatabaseValue](#databasevalue), and returns any valid [value](#values) (Bool, Int, String, Date, Swift enums, etc.) The number of database values is guaranteed to be *argumentCount*.\n\nSQLite has the opportunity to perform additional optimizations when functions are \"pure\", which means that their result only depends on their arguments. So make sure to set the *pure* argument to true when possible.\n\n```swift\nlet reverse = DatabaseFunction(\"reverse\", argumentCount: 1, pure: true) { (values: [DatabaseValue]) in\n    // Extract string value, if any...\n    guard let string = String.fromDatabaseValue(values[0]) else {\n        return nil\n    }\n    // ... and return reversed string:\n    return String(string.reversed())\n}\n```\n\nYou make a function available to a database connection through its configuration:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    db.add(function: reverse)\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n\ntry dbQueue.read { db in\n    // \"oof\"\n    try String.fetchOne(db, sql: \"SELECT reverse('foo')\")!\n}\n```\n\n\n**Functions can take a variable number of arguments:**\n\nWhen you don't provide any explicit *argumentCount*, the function can take any number of arguments:\n\n```swift\nlet averageOf = DatabaseFunction(\"averageOf\", pure: true) { (values: [DatabaseValue]) in\n    let doubles = values.compactMap { Double.fromDatabaseValue($0) }\n    return doubles.reduce(0, +) / Double(doubles.count)\n}\ndb.add(function: averageOf)\n\n// 2.0\ntry Double.fetchOne(db, sql: \"SELECT averageOf(1, 2, 3)\")!\n```\n\n\n**Functions can throw:**\n\n```swift\nlet sqrt = DatabaseFunction(\"sqrt\", argumentCount: 1, pure: true) { (values: [DatabaseValue]) in\n    guard let double = Double.fromDatabaseValue(values[0]) else {\n        return nil\n    }\n    guard double >= 0 else {\n        throw DatabaseError(message: \"invalid negative number\")\n    }\n    return sqrt(double)\n}\ndb.add(function: sqrt)\n\n// SQLite error 1 with statement `SELECT sqrt(-1)`: invalid negative number\ntry Double.fetchOne(db, sql: \"SELECT sqrt(-1)\")!\n```\n\n\n**Use custom functions in the [query interface](#the-query-interface):**\n\n```swift\n// SELECT reverseString(\"name\") FROM player\nPlayer.select { reverseString($0.name) }\n```\n\n\n**GRDB ships with built-in SQL functions that perform unicode-aware string transformations.** See [Unicode](#unicode).\n\n\n### Custom Aggregates\n\nğŸ“– [`DatabaseFunction`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasefunction), [`DatabaseAggregate`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseaggregate)\n\nBefore registering a custom aggregate, you need to define a type that adopts the `DatabaseAggregate` protocol:\n\n```swift\nprotocol DatabaseAggregate {\n    // Initializes an aggregate\n    init()\n    \n    // Called at each step of the aggregation\n    mutating func step(_ dbValues: [DatabaseValue]) throws\n    \n    // Returns the final result\n    func finalize() throws -> DatabaseValueConvertible?\n}\n```\n\nFor example:\n\n```swift\nstruct MaxLength : DatabaseAggregate {\n    var maxLength: Int = 0\n    \n    mutating func step(_ dbValues: [DatabaseValue]) {\n        // At each step, extract string value, if any...\n        guard let string = String.fromDatabaseValue(dbValues[0]) else {\n            return\n        }\n        // ... and update the result\n        let length = string.count\n        if length > maxLength {\n            maxLength = length\n        }\n    }\n    \n    func finalize() -> DatabaseValueConvertible? {\n        maxLength\n    }\n}\n\nlet maxLength = DatabaseFunction(\n    \"maxLength\",\n    argumentCount: 1,\n    pure: true,\n    aggregate: MaxLength.self)\n```\n\nLike [custom SQL Functions](#custom-sql-functions), you make an aggregate function available to a database connection through its configuration:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    db.add(function: maxLength)\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n\ntry dbQueue.read { db in\n    // Some Int\n    try Int.fetchOne(db, sql: \"SELECT maxLength(name) FROM player\")!\n}\n```\n\nThe `step` method of the aggregate takes an array of [DatabaseValue](#databasevalue). This array contains as many values as the *argumentCount* parameter (or any number of values, when *argumentCount* is omitted).\n\nThe `finalize` method of the aggregate returns the final aggregated [value](#values) (Bool, Int, String, Date, Swift enums, etc.).\n\nSQLite has the opportunity to perform additional optimizations when aggregates are \"pure\", which means that their result only depends on their inputs. So make sure to set the *pure* argument to true when possible.\n\n\n**Use custom aggregates in the [query interface](#the-query-interface):**\n\n```swift\n// SELECT maxLength(\"name\") FROM player\nlet request = Player.select { maxLength($0.name) }\ntry Int.fetchOne(db, request) // Int?\n```\n\n\n## Raw SQLite Pointers\n\n**If not all SQLite APIs are exposed in GRDB, you can still use the [SQLite C Interface](https://www.sqlite.org/c3ref/intro.html) and call [SQLite C functions](https://www.sqlite.org/c3ref/funclist.html).**\n\nTo access the C SQLite functions from SQLCipher or the system SQLite, you need to perform an extra import:\n\n```swift\nimport SQLite3   // System SQLite\nimport SQLCipher // SQLCipher\n\nlet sqliteVersion = String(cString: sqlite3_libversion())\n```\n\nRaw pointers to database connections and statements are available through the `Database.sqliteConnection` and `Statement.sqliteStatement` properties:\n\n```swift\ntry dbQueue.read { db in\n    // The raw pointer to a database connection:\n    let sqliteConnection = db.sqliteConnection\n\n    // The raw pointer to a statement:\n    let statement = try db.makeStatement(sql: \"SELECT ...\")\n    let sqliteStatement = statement.sqliteStatement\n}\n```\n\n> **Note**\n>\n> - Those pointers are owned by GRDB: don't close connections or finalize statements created by GRDB.\n> - GRDB opens SQLite connections in the \"[multi-thread mode](https://www.sqlite.org/threadsafe.html)\", which (oddly) means that **they are not thread-safe**. Make sure you touch raw databases and statements inside their dedicated dispatch queues.\n> - Use the raw SQLite C Interface at your own risk. GRDB won't prevent you from shooting yourself in the foot.\n\n\nRecords\n=======\n\n**On top of the [SQLite API](#sqlite-api), GRDB provides protocols** that help manipulating database rows as regular objects named \"records\":\n\n```swift\ntry dbQueue.write { db in\n    if var place = try Player.fetchOne(db, id: 1) {\n        player.score += 10\n        try player.update(db)\n    }\n}\n```\n\nOf course, you need to open a [database connection], and [create database tables](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseschema) first.\n\nTo define a record type, define a type and extend it with database protocols:\n\n- `FetchableRecord` makes it possible to fetch instances from the database.\n- `PersistableRecord` makes it possible to save instances into the database.\n- `Codable` (not mandatory) provides ready-made serialization to and from database rows.\n- `Identifiable` (not mandatory) provides extra convenience database methods.\n\nTo make it easier to customize database requests, also nest a `Columns` enum: \n\n```swift\nstruct Player: Codable, Identifiable {\n    var id: Int64\n    var name: String\n    var score: Int\n    var team: String?\n}\n\n// Add database support\nextension Player: FetchableRecord, PersistableRecord {\n    enum Columns {\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n        static let team = Column(CodingKeys.team)\n    }\n}\n```\n\nSee more [examples of record definitions](#examples-of-record-definitions) below.\n\n> Note: if you are familiar with Core Data's NSManagedObject or Realm's Object, you may experience a cultural shock: GRDB records are not uniqued, do not auto-update, and do not lazy-load. This is both a purpose, and a consequence of protocol-oriented programming.\n>\n> Tip: The [Recommended Practices for Designing Record Types](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/recordrecommendedpractices) guide provides general guidance..\n>\n> Tip: See the [Demo Applications] for sample apps that uses records.\n\n**Overview**\n\n- [Inserting Records](#inserting-records)\n- [Fetching Records](#fetching-records)\n- [Updating Records](#updating-records)\n- [Deleting Records](#deleting-records)\n- [Counting Records](#counting-records)\n\n**Protocols and the Record Class**\n\n- [Record Protocols Overview](#record-protocols-overview)\n- [FetchableRecord Protocol](#fetchablerecord-protocol)\n- [TableRecord Protocol](#tablerecord-protocol)\n- [PersistableRecord Protocol](#persistablerecord-protocol)\n    - [Persistence Methods]\n    - [Persistence Methods and the `RETURNING` clause]\n    - [Persistence Callbacks]\n- [Identifiable Records]\n- [Codable Records]\n- [Record Comparison]\n- [Record Customization Options]\n- [Record Timestamps and Transaction Date]\n\n\n### Inserting Records\n\nTo insert a record in the database, call the `insert` method:\n\n```swift\nlet player = Player(id: 1, name: \"Arthur\", score: 1000)\ntry player.insert(db)\n```\n\n:point_right: `insert` is available for types that adopt the [PersistableRecord] protocol.\n\n\n### Fetching Records\n\nTo fetch records from the database, call a [fetching method](#fetching-methods):\n\n```swift\nlet arthur = try Player.fetchOne(db,            // Player?\n    sql: \"SELECT * FROM players WHERE name = ?\",\n    arguments: [\"Arthur\"])\n\nlet bestPlayers = try Player                    // [Player]\n    .order(\\.score.desc)\n    .limit(10)\n    .fetchAll(db)\n    \nlet spain = try Country.fetchOne(db, id: \"ES\")  // Country?\nlet italy = try Country.find(db, id: \"IT\")      // Country\n```\n\n:point_right: Fetching from raw SQL is available for types that adopt the [FetchableRecord] protocol.\n\n:point_right: Fetching without SQL, using the [query interface](#the-query-interface), is available for types that adopt both [FetchableRecord] and [TableRecord] protocol.\n\n\n### Updating Records\n\nTo update a record in the database, call the `update` method:\n\n```swift\nvar player: Player = ...\nplayer.score = 1000\ntry player.update(db)\n```\n\nIt is possible to [avoid useless updates](#record-comparison):\n\n```swift\n// does not hit the database if score has not changed\ntry player.updateChanges(db) {\n    $0.score = 1000\n}\n```\n\nSee the [query interface](#the-query-interface) for batch updates:\n\n```swift\ntry Player\n    .filter { $0.team == \"red\" }\n    .updateAll(db) { $0.score += 1 }\n```\n\n:point_right: update methods are available for types that adopt the [PersistableRecord] protocol. Batch updates are available on the [TableRecord] protocol.\n\n\n### Deleting Records\n\nTo delete a record in the database, call the `delete` method:\n\n```swift\nlet player: Player = ...\ntry player.delete(db)\n```\n\nYou can also delete by primary key, unique key, or perform batch deletes (see [Delete Requests](#delete-requests)):\n\n```swift\ntry Player.deleteOne(db, id: 1)\ntry Player.deleteOne(db, key: [\"email\": \"arthur@example.com\"])\ntry Country.deleteAll(db, ids: [\"FR\", \"US\"])\ntry Player\n    .filter { $0.email == nil }\n    .deleteAll(db)\n```\n\n:point_right: delete methods are available for types that adopt the [PersistableRecord] protocol. Batch deletes are available on the [TableRecord] protocol.\n\n\n### Counting Records\n\nTo count records, call the `fetchCount` method:\n\n```swift\nlet playerCount: Int = try Player.fetchCount(db)\n\nlet playerWithEmailCount: Int = try Player\n    .filter { $0.email == nil }\n    .fetchCount(db)\n```\n\n:point_right: `fetchCount` is available for types that adopt the [TableRecord] protocol.\n\n\nDetails follow:\n\n- [Record Protocols Overview](#record-protocols-overview)\n- [FetchableRecord Protocol](#fetchablerecord-protocol)\n- [TableRecord Protocol](#tablerecord-protocol)\n- [PersistableRecord Protocol](#persistablerecord-protocol)\n- [Identifiable Records]\n- [Codable Records]\n- [Record Comparison]\n- [Record Customization Options]\n- [Examples of Record Definitions](#examples-of-record-definitions)\n\n\n## Record Protocols Overview\n\n**GRDB ships with three record protocols**. Your own types will adopt one or several of them, according to the abilities you want to extend your types with.\n\n- [FetchableRecord] is able to **decode database rows**.\n    \n    ```swift\n    struct Place: FetchableRecord { ... }\n    \n    let places = try dbQueue.read { db in\n        try Place.fetchAll(db, sql: \"SELECT * FROM place\")\n    }\n    ```\n    \n    > :bulb: **Tip**: `FetchableRecord` can derive its implementation from the standard `Decodable` protocol. See [Codable Records] for more information.\n    \n    `FetchableRecord` can decode database rows, but it is not able to build SQL requests for you. For that, you also need `TableRecord`:\n    \n- [TableRecord] is able to **generate SQL queries**:\n    \n    ```swift\n    struct Place: TableRecord { ... }\n    \n    let placeCount = try dbQueue.read { db in\n        // Generates and runs `SELECT COUNT(*) FROM place`\n        try Place.fetchCount(db)\n    }\n    ```\n    \n    When a type adopts both `TableRecord` and `FetchableRecord`, it can load from those requests:\n    \n    ```swift\n    struct Place: TableRecord, FetchableRecord { ... }\n    \n    try dbQueue.read { db in\n        let places = try Place.order(\\.title).fetchAll(db)\n        let paris = try Place.fetchOne(id: 1)\n    }\n    ```\n\n- [PersistableRecord] is able to **write**: it can create, update, and delete rows in the database:\n    \n    ```swift\n    struct Place : PersistableRecord { ... }\n    \n    try dbQueue.write { db in\n        try Place.delete(db, id: 1)\n        try Place(...).insert(db)\n    }\n    ```\n    \n    A persistable record can also [compare](#record-comparison) itself against other records, and avoid useless database updates.\n    \n    > :bulb: **Tip**: `PersistableRecord` can derive its implementation from the standard `Encodable` protocol. See [Codable Records] for more information.\n\n\n## FetchableRecord Protocol\n\nğŸ“– [`FetchableRecord`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/fetchablerecord)\n\n**The FetchableRecord protocol grants fetching methods to any type** that can be built from a database row:\n\n```swift\nprotocol FetchableRecord {\n    /// Row initializer\n    init(row: Row) throws\n}\n```\n\nFor example:\n\n```swift\nstruct Place {\n    var id: Int64?\n    var title: String\n    var coordinate: CLLocationCoordinate2D\n}\n\nextension Place: FetchableRecord {\n    enum Columns {\n        static let id = Column(\"id\")\n        static let title = Column(\"title\")\n        static let latitude = Column(\"latitude\")\n        static let longitude = Column(\"longitude\")\n    }\n    \n    init(row: Row) {\n        id = row[Columns.id]\n        title = row[Columns.title]\n        coordinate = CLLocationCoordinate2D(\n            latitude: row[Columns.latitude],\n            longitude: row[Columns.longitude])\n    }\n}\n```\n\nSee [column values](#column-values) for more information about the `row[]` subscript.\n\nWhen your record type adopts the standard Decodable protocol, you don't have to provide the implementation for `init(row:)`. See [Codable Records] for more information:\n\n```swift\n// That's all\nstruct Player: Decodable, FetchableRecord {\n    var id: Int64\n    var name: String\n    var score: Int\n    \n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n}\n```\n\nFetchableRecord allows adopting types to be fetched from SQL queries:\n\n```swift\ntry Place.fetchCursor(db, sql: \"SELECT ...\", arguments:...) // A Cursor of Place\ntry Place.fetchAll(db, sql: \"SELECT ...\", arguments:...)    // [Place]\ntry Place.fetchSet(db, sql: \"SELECT ...\", arguments:...)    // Set<Place>\ntry Place.fetchOne(db, sql: \"SELECT ...\", arguments:...)    // Place?\n```\n\nSee [fetching methods](#fetching-methods) for information about the `fetchCursor`, `fetchAll`, `fetchSet` and `fetchOne` methods. See [`StatementArguments`] for more information about the query arguments.\n\n> **Note**: for performance reasons, the same row argument to `init(row:)` is reused during the iteration of a fetch query. If you want to keep the row for later use, make sure to store a copy: `self.row = row.copy()`.\n\n> **Note**: The `FetchableRecord.init(row:)` initializer fits the needs of most applications. But some application are more demanding than others. When FetchableRecord does not exactly provide the support you need, have a look at the [Beyond FetchableRecord] chapter.\n\n\n## TableRecord Protocol\n\nğŸ“– [`TableRecord`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerecord)\n\n**The TableRecord protocol** generates SQL for you:\n\n```swift\nprotocol TableRecord {\n    static var databaseTableName: String { get }\n    static var databaseSelection: [any SQLSelectable] { get }\n}\n```\n\nThe `databaseSelection` type property is optional, and documented in the [Columns Selected by a Request] chapter.\n\nThe `databaseTableName` type property is the name of a database table. By default, it is derived from the type name:\n\n```swift\nstruct Place: TableRecord { }\n\nprint(Place.databaseTableName) // prints \"place\"\n```\n\nFor example:\n\n- Place: `place`\n- Country: `country`\n- PostalAddress: `postalAddress`\n- HTTPRequest: `httpRequest`\n- TOEFL: `toefl`\n\nYou can still provide a custom table name:\n\n```swift\nstruct Place: TableRecord {\n    static let databaseTableName = \"location\"\n}\n\nprint(Place.databaseTableName) // prints \"location\"\n```\n\nWhen a type adopts both TableRecord and [FetchableRecord](#fetchablerecord-protocol), it can be fetched using the [query interface](#the-query-interface):\n\n```swift\n// SELECT * FROM place WHERE name = 'Paris'\nlet paris = try Place.filter { $0.name == \"Paris\" }.fetchOne(db)\n```\n\nTableRecord can also fetch deal with primary and unique keys: see [Fetching by Key](#fetching-by-key) and [Testing for Record Existence](#testing-for-record-existence).\n\n\n## PersistableRecord Protocol\n\nğŸ“– [`EncodableRecord`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/encodablerecord), [`MutablePersistableRecord`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/mutablepersistablerecord), [`PersistableRecord`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/persistablerecord)\n\n**GRDB record types can create, update, and delete rows in the database.**\n\nThose abilities are granted by three protocols:\n\n```swift\n// Defines how a record encodes itself into the database\nprotocol EncodableRecord {\n    /// Defines the values persisted in the database\n    func encode(to container: inout PersistenceContainer) throws\n}\n\n// Adds persistence methods\nprotocol MutablePersistableRecord: TableRecord, EncodableRecord {\n    /// Optional method that lets your adopting type store its rowID upon\n    /// successful insertion. Don't call it directly: it is called for you.\n    mutating func didInsert(_ inserted: InsertionSuccess)\n}\n\n// Adds immutability\nprotocol PersistableRecord: MutablePersistableRecord {\n    /// Non-mutating version of the optional didInsert(_:)\n    func didInsert(_ inserted: InsertionSuccess)\n}\n```\n\nYes, three protocols instead of one. Here is how you pick one or the other:\n\n- **If your type is a class**, choose `PersistableRecord`. On top of that, implement `didInsert(_:)` if the database table has an auto-incremented primary key.\n\n- **If your type is a struct, and the database table has an auto-incremented primary key**, choose `MutablePersistableRecord`, and implement `didInsert(_:)`.\n\n- **Otherwise**, choose `PersistableRecord`, and ignore `didInsert(_:)`.\n\nThe `encode(to:)` method defines which [values](#values) (Bool, Int, String, Date, Swift enums, etc.) are assigned to database columns.\n\nThe optional `didInsert` method lets the adopting type store its rowID after successful insertion, and is only useful for tables that have an auto-incremented primary key. It is called from a protected dispatch queue, and serialized with all database updates.\n\nFor example:\n\n```swift\nextension Place: MutablePersistableRecord {\n    enum Columns {\n        static let id = Column(\"id\")\n        static let title = Column(\"title\")\n        static let latitude = Column(\"latitude\")\n        static let longitude = Column(\"longitude\")\n    }\n    \n    /// The values persisted in the database\n    func encode(to container: inout PersistenceContainer) {\n        container[Columns.id] = id\n        container[Columns.title] = title\n        container[Columns.latitude] = coordinate.latitude\n        container[Columns.longitude] = coordinate.longitude\n    }\n    \n    // Update auto-incremented id upon successful insertion\n    mutating func didInsert(_ inserted: InsertionSuccess) {\n        id = inserted.rowID\n    }\n}\n\nvar paris = Place(\n    id: nil,\n    title: \"Paris\",\n    coordinate: CLLocationCoordinate2D(latitude: 48.8534100, longitude: 2.3488000))\n\ntry paris.insert(db)\nparis.id   // some value\n```\n\nWhen your record type adopts the standard Encodable protocol, you don't have to provide the implementation for `encode(to:)`. See [Codable Records] for more information:\n\n```swift\n// That's all\nstruct Player: Encodable, MutablePersistableRecord {\n    var id: Int64?\n    var name: String\n    var score: Int\n    \n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n    \n    // Update auto-incremented id upon successful insertion\n    mutating func didInsert(_ inserted: InsertionSuccess) {\n        id = inserted.rowID\n    }\n}\n```\n\n\n### Persistence Methods\n\nTypes that adopt the [PersistableRecord] protocol are given methods that insert, update, and delete:\n\n```swift\n// INSERT\ntry place.insert(db)\nlet insertedPlace = try place.inserted(db) // non-mutating\n\n// UPDATE\ntry place.update(db)\ntry place.update(db, columns: [\"title\"])\n\n// Maybe UPDATE\ntry place.updateChanges(db, from: otherPlace)\ntry place.updateChanges(db) { $0.isFavorite = true }\n\n// INSERT or UPDATE\ntry place.save(db)\nlet savedPlace = place.saved(db) // non-mutating\n\n// UPSERT\ntry place.upsert(db)\nlet insertedPlace = place.upsertAndFetch(db)\n\n// DELETE\ntry place.delete(db)\n\n// EXISTENCE CHECK\nlet exists = try place.exists(db)\n```\n\nSee [Upsert](#upsert) below for more information about upserts.\n\n**The [TableRecord] protocol comes with batch operations**:\n\n```swift\n// UPDATE\ntry Place.updateAll(db, ...)\n\n// DELETE\ntry Place.deleteAll(db)\ntry Place.deleteAll(db, ids:...)\ntry Place.deleteAll(db, keys:...)\ntry Place.deleteOne(db, id:...)\ntry Place.deleteOne(db, key:...)\n```\n\nFor more information about batch updates, see [Update Requests](#update-requests).\n\n- All persistence methods can throw a [DatabaseError](#error-handling).\n\n- `update` and `updateChanges` throw [RecordError] if the database does not contain any row for the primary key of the record.\n\n- `save` makes sure your values are stored in the database. It performs an UPDATE if the record has a non-null primary key, and then, if no row was modified, an INSERT. It directly performs an INSERT if the record has no primary key, or a null primary key.\n\n- `delete` and `deleteOne` returns whether a database row was deleted or not. `deleteAll` returns the number of deleted rows. `updateAll` returns the number of updated rows. `updateChanges` returns whether a database row was updated or not.\n\n**All primary keys are supported**, including composite primary keys that span several columns, and the [hidden `rowid` column](https://www.sqlite.org/rowidtable.html).\n\n**To customize persistence methods**, you provide [Persistence Callbacks], described below. Do not attempt at overriding the ready-made persistence methods.\n\n### Upsert\n\n[UPSERT](https://www.sqlite.org/lang_UPSERT.html) is an SQLite feature that causes an INSERT to behave as an UPDATE or a no-op if the INSERT would violate a uniqueness constraint (primary key or unique index).\n\n> **Note**: Upsert apis are available from SQLite 3.35.0+: iOS 15.0+, macOS 12.0+, tvOS 15.0+, watchOS 8.0+, or with a [custom SQLite build] or [SQLCipher](#encryption).\n>\n> **Note**: With regard to [persistence callbacks](#available-callbacks), an upsert behaves exactly like an insert. In particular: the `aroundInsert(_:)` and `didInsert(_:)` callbacks reports the rowid of the inserted or updated row; `willUpdate`, `aroundUpdate`, `didUpdate` are not called.\n\n[PersistableRecord] provides three upsert methods:\n\n- `upsert(_:)`\n    \n    Inserts or updates a record.\n    \n    The upsert behavior is triggered by a violation of any uniqueness constraint on the table (primary key or unique index). In case of conflict, all columns but the primary key are overwritten with the inserted values:\n    \n    ```swift\n    struct Player: Encodable, PersistableRecord {\n        var id: Int64\n        var name: String\n        var score: Int\n    }\n    \n    // INSERT INTO player (id, name, score)\n    // VALUES (1, 'Arthur', 1000)\n    // ON CONFLICT DO UPDATE SET\n    //   name = excluded.name,\n    //   score = excluded.score\n    let player = Player(id: 1, name: \"Arthur\", score: 1000)\n    try player.upsert(db)\n    ```\n\n- `upsertAndFetch(_:onConflict:doUpdate:)` (requires [FetchableRecord] conformance)\n\n    Inserts or updates a record, and returns the upserted record.\n    \n    The `onConflict` and `doUpdate` arguments let you further control the upsert behavior. Make sure you check the [SQLite UPSERT documentation](https://www.sqlite.org/lang_UPSERT.html) for detailed information.\n    \n    - `onConflict`: the \"conflict target\" is the array of columns in the uniqueness constraint (primary key or unique index) that triggers the upsert.\n        \n        If empty (the default), all uniqueness constraint are considered.\n    \n    - `doUpdate`: a closure that returns columns assignments to perform in case of conflict. Other columns are overwritten with the inserted values.\n        \n        By default, all inserted columns but the primary key and the conflict target are overwritten.\n    \n    In the example below, we upsert the new vocabulary word \"jovial\". It is inserted if that word is not already in the dictionary. Otherwise, `count` is incremented, `isTainted` is not overwritten, and `kind` is overwritten:\n    \n    ```swift\n    // CREATE TABLE vocabulary(\n    //   word TEXT NOT NULL PRIMARY KEY,\n    //   kind TEXT NOT NULL,\n    //   isTainted BOOLEAN DEFAULT 0,\n    //   count INT DEFAULT 1))\n    struct Vocabulary: Encodable, PersistableRecord {\n        var word: String\n        var kind: String\n        var isTainted: Bool\n    }\n    \n    // INSERT INTO vocabulary(word, kind, isTainted)\n    // VALUES('jovial', 'adjective', 0)\n    // ON CONFLICT(word) DO UPDATE SET \\\n    //   count = count + 1,   -- on conflict, count is incremented\n    //   kind = excluded.kind -- on conflict, kind is overwritten\n    // RETURNING *\n    let vocabulary = Vocabulary(word: \"jovial\", kind: \"adjective\", isTainted: false)\n    let upserted = try vocabulary.upsertAndFetch(\n        db, onConflict: [\"word\"],\n        doUpdate: { _ in\n            [Column(\"count\") += 1,            // on conflict, count is incremented\n             Column(\"isTainted\").noOverwrite] // on conflict, isTainted is NOT overwritten\n        })\n    ```\n    \n    The `doUpdate` closure accepts an `excluded` TableAlias argument that refers to the inserted values that trigger the conflict. You can use it to specify an explicit overwrite, or to perform a computation. In the next example, the upsert keeps the maximum date in case of conflict:\n    \n    ```swift\n    // INSERT INTO message(id, text, date)\n    // VALUES(...)\n    // ON CONFLICT DO UPDATE SET \\\n    //   text = excluded.text,\n    //   date = MAX(date, excluded.date)\n    // RETURNING *\n    let upserted = try message.upsertAndFetch(doUpdate: { excluded in\n        // keep the maximum date in case of conflict\n        [Column(\"date\").set(to: max(Column(\"date\"), excluded[\"date\"]))]\n    })\n    ```\n\n- `upsertAndFetch(_:as:onConflict:doUpdate:)` (does not require [FetchableRecord] conformance)\n\n    This method is identical to `upsertAndFetch(_:onConflict:doUpdate:)` described above, but you can provide a distinct [FetchableRecord] record type as a result, in order to specify the returned columns.\n\n### Persistence Methods and the `RETURNING` clause\n\nSQLite is able to return values from a inserted, updated, or deleted row, with the [`RETURNING` clause](https://www.sqlite.org/lang_returning.html).\n\n> **Note**: Support for the `RETURNING` clause is available from SQLite 3.35.0+: iOS 15.0+, macOS 12.0+, tvOS 15.0+, watchOS 8.0+, or with a [custom SQLite build] or [SQLCipher](#encryption).\n\nThe `RETURNING` clause helps dealing with database features such as auto-incremented ids, default values, and [generated columns](https://sqlite.org/gencol.html). You can, for example, insert a few columns and fetch the default or generated ones in one step.\n\nGRDB uses the `RETURNING` clause in all persistence methods that contain `AndFetch` in their name.\n\nFor example, given a database table with an auto-incremented primary key and a default score:\n\n```swift\ntry dbQueue.write { db in\n    try db.execute(sql: \"\"\"\n        CREATE TABLE player(\n          id INTEGER PRIMARY KEY AUTOINCREMENT,\n          name TEXT NOT NULL,\n          score INTEGER NOT NULL DEFAULT 1000)\n        \"\"\")\n}\n```\n\nYou can define a record type with full database information, and another partial record type that deals with a subset of columns:\n\n```swift\n// A player with full database information\nstruct Player: Codable, PersistableRecord, FetchableRecord {\n    var id: Int64\n    var name: String\n    var score: Int\n    \n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n}\n\n// A partial player\nstruct PartialPlayer: Encodable, PersistableRecord {\n    static let databaseTableName = \"player\"\n    var name: String\n    \n    typealias Columns = Player.Columns\n}\n```\n\nAnd now you can get a full player by inserting a partial one:\n\n```swift\ntry dbQueue.write { db in\n    let partialPlayer = PartialPlayer(name: \"Alice\")\n    \n    // INSERT INTO player (name) VALUES ('Alice') RETURNING *\n    let player = try partialPlayer.insertAndFetch(db, as: Player.self)\n    print(player.id)    // The inserted id\n    print(player.name)  // The inserted name\n    print(player.score) // The default score\n}\n```\n\nFor extra precision, you can select only the columns you need, and fetch the desired value from the provided prepared [`Statement`]:\n\n```swift\ntry dbQueue.write { db in\n    let partialPlayer = PartialPlayer(name: \"Alice\")\n    \n    // INSERT INTO player (name) VALUES ('Alice') RETURNING score\n    let score = try partialPlayer.insertAndFetch(db) { statement in\n        try Int.fetchOne(statement)\n    } select: {\n        [$0.score]\n    }\n    print(score) // Prints 1000, the default score\n}\n```\n\nThere are other similar persistence methods, such as `upsertAndFetch`, `saveAndFetch`, `updateAndFetch`, `updateChangesAndFetch`, etc. They all behave like `upsert`, `save`, `update`, `updateChanges`, except that they return saved values. For example:\n\n```swift\n// Save and return the saved player\nlet savedPlayer = try player.saveAndFetch(db)\n```\n\nSee [Persistence Methods], [Upsert](#upsert), and [`updateChanges` methods](#the-updatechanges-methods) for more information.\n\n**Batch operations** can return updated or deleted values:\n\n> **Warning**: Make sure you check the [documentation of the `RETURNING` clause](https://www.sqlite.org/lang_returning.html#limitations_and_caveats), which describes important limitations and caveats for batch operations.\n\n```swift\nlet request = Player.filter(...)...\n\n// Fetch all deleted players\n// DELETE FROM player RETURNING *\nlet deletedPlayers = try request.deleteAndFetchAll(db) // [Player]\n\n// Fetch a selection of columns from the deleted rows\n// DELETE FROM player RETURNING name\nlet statement = try request.deleteAndFetchStatement(db) { [$0.name] }\nlet deletedNames = try String.fetchSet(statement)\n\n// Fetch all updated players\n// UPDATE player SET score = score + 10 RETURNING *\nlet updatedPlayers = try request.updateAndFetchAll(db) { [$0.score += 10] } // [Player]\n\n// Fetch a selection of columns from the updated rows\n// UPDATE player SET score = score + 10 RETURNING score\nlet statement = try request.updateAndFetchStatement(db) {\n    [$0.score += 10]\n} select: {\n    [$0.score]\n}\nlet updatedScores = try Int.fetchAll(statement)\n```\n\n\n### Persistence Callbacks\n\nYour custom type may want to perform extra work when the persistence methods are invoked.\n\nTo this end, your record type can implement **persistence callbacks**. Callbacks are methods that get called at certain moments of a record's life cycle. With callbacks it is possible to write code that will run whenever an record is inserted, updated, or deleted.\n\nIn order to use a callback method, you need to provide its implementation. For example, a frequently used callback is `didInsert`, in the case of auto-incremented database ids:\n\n```swift\nstruct Player: MutablePersistableRecord {\n    var id: Int64?\n    \n    // Update auto-incremented id upon successful insertion\n    mutating func didInsert(_ inserted: InsertionSuccess) {\n        id = inserted.rowID\n    }\n}\n\ntry dbQueue.write { db in\n    var player = Player(id: nil, ...)\n    try player.insert(db)\n    print(player.id) // didInsert was called: prints some non-nil id\n}\n```\n\nCallbacks can also help implementing record validation:\n\n```swift\nstruct Link: PersistableRecord {\n    var url: URL\n    \n    func willSave(_ db: Database) throws {\n        if url.host == nil {\n            throw ValidationError(\"url must be absolute.\")\n        }\n    }\n}\n\ntry link.insert(db) // Calls the willSave callback\ntry link.update(db) // Calls the willSave callback\ntry link.save(db)   // Calls the willSave callback\ntry link.upsert(db) // Calls the willSave callback\n```\n\n#### Available Callbacks\n\nHere is a list with all the available [persistence callbacks], listed in the same order in which they will get called during the respective operations:\n\n- Inserting a record (all `record.insert` and `record.upsert` methods)\n    - `willSave`\n    - `aroundSave`\n    - `willInsert`\n    - `aroundInsert`\n    - `didInsert`\n    - `didSave`\n    \n- Updating a record (all `record.update` methods)\n    - `willSave`\n    - `aroundSave`\n    - `willUpdate`\n    - `aroundUpdate`\n    - `didUpdate`\n    - `didSave`\n    \n- Deleting a record (only the `record.delete(_:)` method)\n    - `willDelete`\n    - `aroundDelete`\n    - `didDelete`\n\nFor detailed information about each callback, check the [reference](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/mutablepersistablerecord/).\n\nIn the `MutablePersistableRecord` protocol, `willInsert` and `didInsert` are mutating methods. In `PersistableRecord`, they are not mutating.\n\n> **Note**: The `record.save(_:)` method performs an UPDATE if the record has a non-null primary key, and then, if no row was modified, an INSERT. It directly performs an INSERT if the record has no primary key, or a null primary key. It triggers update and/or insert callbacks accordingly.\n>\n> **Warning**: Callbacks are only invoked from persistence methods called on record instances. Callbacks are not invoked when you call a type method, perform a batch operations, or execute raw SQL.\n>\n> **Warning**: When a `did***` callback is invoked, do not assume that the change is actually persisted on disk, because the database may still be inside an uncommitted transaction. When you need to handle transaction completions, use the [afterNextTransaction(onCommit:onRollback:)](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/database/afternexttransaction(oncommit:onrollback:)). For example:\n>\n> ```swift\n> struct PictureFile: PersistableRecord {\n>     var path: String\n>     \n>     func willDelete(_ db: Database) {\n>         db.afterNextTransaction { _ in\n>             try? deleteFileOnDisk()\n>         }\n>     }\n> }\n> ```\n\n\n## Identifiable Records\n\n**When a record type maps a table with a single-column primary key, it is recommended to have it adopt the standard [Identifiable] protocol.**\n\n```swift\nstruct Player: Identifiable, FetchableRecord, PersistableRecord {\n    var id: Int64 // fulfills the Identifiable requirement\n    var name: String\n    var score: Int\n}\n```\n\nWhen `id` has a [database-compatible type](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasevalueconvertible) (Int64, Int, String, UUID, ...), the `Identifiable` conformance unlocks type-safe record and request methods:\n\n```swift\nlet player = try Player.find(db, id: 1)               // Player\nlet player = try Player.fetchOne(db, id: 1)           // Player?\nlet players = try Player.fetchAll(db, ids: [1, 2, 3]) // [Player]\nlet players = try Player.fetchSet(db, ids: [1, 2, 3]) // Set<Player>\n\nlet request = Player.filter(id: 1)\nlet request = Player.filter(ids: [1, 2, 3])\n\ntry Player.deleteOne(db, id: 1)\ntry Player.deleteAll(db, ids: [1, 2, 3])\n```\n\n> **Note**: Not all record types can be made `Identifiable`, and not all tables have a single-column primary key. GRDB provides other methods that deal with primary and unique keys, but they won't check the type of their arguments:\n> \n> ```swift\n> // Available on non-Identifiable types\n> try Player.fetchOne(db, key: 1)\n> try Player.fetchOne(db, key: [\"email\": \"arthur@example.com\"])\n> try Country.fetchAll(db, keys: [\"FR\", \"US\"])\n> try Citizenship.fetchOne(db, key: [\"citizenId\": 1, \"countryCode\": \"FR\"])\n> \n> let request = Player.filter(key: 1)\n> let request = Player.filter(keys: [1, 2, 3])\n> \n> try Player.deleteOne(db, key: 1)\n> try Player.deleteAll(db, keys: [1, 2, 3])\n> ```\n\n> **Note**: It is not recommended to use `Identifiable` on record types that use an auto-incremented primary key:\n>\n> ```swift\n> // AVOID declaring Identifiable conformance when key is auto-incremented\n> struct Player {\n>     var id: Int64? // Not an id suitable for Identifiable\n>     var name: String\n>     var score: Int\n> }\n> \n> extension Player: FetchableRecord, MutablePersistableRecord {\n>     // Update auto-incremented id upon successful insertion\n>     mutating func didInsert(_ inserted: InsertionSuccess) {\n>         id = inserted.rowID\n>     }\n> }\n> ```\n>\n> For a detailed rationale, please see [issue #1435](https://github.com/groue/GRDB.swift/issues/1435#issuecomment-1740857712).\n\nSome database tables have a single-column primary key which is not called \"id\":\n\n```swift\ntry db.create(table: \"country\") { t in\n    t.primaryKey(\"isoCode\", .text)\n    t.column(\"name\", .text).notNull()\n    t.column(\"population\", .integer).notNull()\n}\n```\n\nIn this case, `Identifiable` conformance can be achieved, for example, by returning the primary key column from the `id` property:\n\n```swift\nstruct Country: Identifiable, FetchableRecord, PersistableRecord {\n    var isoCode: String\n    var name: String\n    var population: Int\n    \n    // Fulfill the Identifiable requirement\n    var id: String { isoCode }\n}\n\nlet france = try dbQueue.read { db in\n    try Country.fetchOne(db, id: \"FR\")\n}\n```\n\n\n## Codable Records\n\nRecord types that adopt an archival protocol ([Codable, Encodable or Decodable](https://developer.apple.com/documentation/foundation/archives_and_serialization/encoding_and_decoding_custom_types)) get free database support just by declaring conformance to the desired [record protocols](#record-protocols-overview):\n\n```swift\n// Declare a record...\nstruct Player: Codable, FetchableRecord, PersistableRecord {\n    var id: Int64\n    var name: String\n    var score: Int\n    \n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n}\n\n// ...and there you go:\ntry dbQueue.write { db in\n    try Player(id: 1, name: \"Arthur\", score: 100).insert(db)\n    let players = try Player.order(\\.score.desc).fetchAll(db)\n}\n```\n\nCodable records encode and decode their properties according to their own implementation of the Encodable and Decodable protocols. Yet databases have specific requirements:\n\n- Properties are always coded according to their preferred database representation, when they have one (all [values](#values) that adopt the [`DatabaseValueConvertible`] protocol).\n- You can customize the encoding and decoding of dates and uuids.\n- Complex properties (arrays, dictionaries, nested structs, etc.) are stored as JSON.\n\nFor more information about Codable records, see:\n\n- [JSON Columns]\n- [Column Names Coding Strategies]\n- [Data, Date, and UUID Coding Strategies]\n- [The userInfo Dictionary]\n- [Tip: Derive Columns from Coding Keys](#tip-derive-columns-from-coding-keys)\n\n> :bulb: **Tip**: see the [Demo Applications] for sample code that uses Codable records.\n\n\n### JSON Columns\n\nWhen a [Codable record](#codable-records) contains a property that is not a simple [value](#values) (Bool, Int, String, Date, Swift enums, etc.), that value is encoded and decoded as a **JSON string**. For example:\n\n```swift\nenum AchievementColor: String, Codable {\n    case bronze, silver, gold\n}\n\nstruct Achievement: Codable {\n    var name: String\n    var color: AchievementColor\n}\n\nstruct Player: Codable, FetchableRecord, PersistableRecord {\n    var name: String\n    var score: Int\n    var achievements: [Achievement] // stored in a JSON column\n}\n\ntry dbQueue.write { db in\n    // INSERT INTO player (name, score, achievements)\n    // VALUES (\n    //   'Arthur',\n    //   100,\n    //   '[{\"color\":\"gold\",\"name\":\"Use Codable Records\"}]')\n    let achievement = Achievement(name: \"Use Codable Records\", color: .gold)\n    let player = Player(name: \"Arthur\", score: 100, achievements: [achievement])\n    try player.insert(db)\n}\n```\n\nGRDB uses the standard [JSONDecoder](https://developer.apple.com/documentation/foundation/jsondecoder) and [JSONEncoder](https://developer.apple.com/documentation/foundation/jsonencoder) from Foundation. By default, Data values are handled with the `.base64` strategy, Date with the `.millisecondsSince1970` strategy, and non conforming floats with the `.throw` strategy.\n\nYou can customize the JSON format by implementing those methods:\n\n```swift\nprotocol FetchableRecord {\n    static func databaseJSONDecoder(for column: String) -> JSONDecoder\n}\n\nprotocol EncodableRecord {\n    static func databaseJSONEncoder(for column: String) -> JSONEncoder\n}\n```\n\n> :bulb: **Tip**: Make sure you set the JSONEncoder `sortedKeys` option. This option makes sure that the JSON output is stable. This stability is required for [Record Comparison] to work as expected, and database observation tools such as [ValueObservation] to accurately recognize changed records.\n\n\n### Column Names Coding Strategies\n\nBy default, [Codable Records] store their values into database columns that match their coding keys: the `teamID` property is stored into the `teamID` column.\n\nThis behavior can be overridden, so that you can, for example, store the `teamID` property into the `team_id` column:\n\n```swift\nprotocol FetchableRecord {\n    static var databaseColumnDecodingStrategy: DatabaseColumnDecodingStrategy { get }\n}\n\nprotocol EncodableRecord {\n    static var databaseColumnEncodingStrategy: DatabaseColumnEncodingStrategy { get }\n}\n```\n\nSee [DatabaseColumnDecodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasecolumndecodingstrategy) and [DatabaseColumnEncodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasecolumnencodingstrategy/) to learn about all available strategies.\n\n\n### Data, Date, and UUID Coding Strategies\n\nBy default, [Codable Records] encode and decode their Data properties as blobs, and Date and UUID properties as described in the general [Date and DateComponents](#date-and-datecomponents) and [UUID](#uuid) chapters.\n\nTo sum up: dates encode themselves in the \"YYYY-MM-DD HH:MM:SS.SSS\" format, in the UTC time zone, and decode a variety of date formats and timestamps. UUIDs encode themselves as 16-bytes data blobs, and decode both 16-bytes data blobs and strings such as \"E621E1F8-C36C-495A-93FC-0C247A3E6E5F\".\n\nThose behaviors can be overridden:\n\n```swift\nprotocol FetchableRecord {\n    static func databaseDataDecodingStrategy(for column: String) -> DatabaseDataDecodingStrategy\n    static func databaseDateDecodingStrategy(for column: String) -> DatabaseDateDecodingStrategy\n}\n\nprotocol EncodableRecord {\n    static func databaseDataEncodingStrategy(for column: String) -> DatabaseDataEncodingStrategy\n    static func databaseDateEncodingStrategy(for column: String) -> DatabaseDateEncodingStrategy\n    static func databaseUUIDEncodingStrategy(for column: String) -> DatabaseUUIDEncodingStrategy\n}\n```\n\nSee [DatabaseDataDecodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasedatadecodingstrategy/), [DatabaseDateDecodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasedatedecodingstrategy/), [DatabaseDataEncodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasedataencodingstrategy/), [DatabaseDateEncodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasedateencodingstrategy/), and [DatabaseUUIDEncodingStrategy](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseuuidencodingstrategy/) to learn about all available strategies.\n\nThere is no customization of uuid decoding, because UUID can already decode all its encoded variants (16-bytes blobs and uuid strings, both uppercase and lowercase).\n\nCustomized coding strategies apply:\n\n- When encoding and decoding database rows to and from records (fetching and persistence methods).\n- In requests by single-column primary key: `fetchOne(_:id:)`, `filter(id:)`, `deleteAll(_:keys:)`, etc.\n\n*They do not apply* in other requests based on data, date, or uuid values.\n\nSo make sure that those are properly encoded in your requests. For example:\n\n```swift\nstruct Player: Codable, FetchableRecord, PersistableRecord, Identifiable {\n    // UUIDs are stored as strings\n    static func databaseUUIDEncodingStrategy(for column: String) -> DatabaseUUIDEncodingStrategy {\n        .uppercaseString\n    }\n    \n    var id: UUID\n    ...\n}\n\ntry dbQueue.write { db in\n    let uuid = UUID()\n    let player = Player(id: uuid, ...)\n    \n    // OK: inserts a player in the database, with a string uuid\n    try player.insert(db)\n    \n    // OK: performs a string-based query, finds the inserted player\n    _ = try Player.filter(id: uuid).fetchOne(db)\n\n    // NOT OK: performs a blob-based query, fails to find the inserted player\n    _ = try Player.filter { $0.id == uuid }.fetchOne(db)\n    \n    // OK: performs a string-based query, finds the inserted player\n    _ = try Player.filter { $0.id == uuid.uuidString }.fetchOne(db)\n}\n```\n\n\n### The userInfo Dictionary\n\nYour [Codable Records] can be stored in the database, but they may also have other purposes. In this case, you may need to customize their implementations of `Decodable.init(from:)` and `Encodable.encode(to:)`, depending on the context.\n\nThe standard way to provide such context is the `userInfo` dictionary. Implement those properties:\n\n```swift\nprotocol FetchableRecord {\n    static var databaseDecodingUserInfo: [CodingUserInfoKey: Any] { get }\n}\n\nprotocol EncodableRecord {\n    static var databaseEncodingUserInfo: [CodingUserInfoKey: Any] { get }\n}\n```\n\nFor example, here is a Player type that customizes its decoding:\n\n```swift\n// A key that holds a decoder's name\nlet decoderName = CodingUserInfoKey(rawValue: \"decoderName\")!\n\nstruct Player: FetchableRecord, Decodable {\n    init(from decoder: Decoder) throws {\n        // Print the decoder name\n        let decoderName = decoder.userInfo[decoderName] as? String\n        print(\"Decoded from \\(decoderName ?? \"unknown decoder\")\")\n        ...\n    }\n}\n```\n\nYou can have a specific decoding from JSON...\n\n```swift\n// prints \"Decoded from JSON\"\nlet decoder = JSONDecoder()\ndecoder.userInfo = [decoderName: \"JSON\"]\nlet player = try decoder.decode(Player.self, from: jsonData)\n```\n\n... and another one from database rows:\n\n```swift\nextension Player: FetchableRecord {\n    static var databaseDecodingUserInfo: [CodingUserInfoKey: Any] {\n        [decoderName: \"database row\"]\n    }\n}\n\n// prints \"Decoded from database row\"\nlet player = try Player.fetchOne(db, ...)\n```\n\n> **Note**: make sure the `databaseDecodingUserInfo` and `databaseEncodingUserInfo` properties are explicitly declared as `[CodingUserInfoKey: Any]`. If they are not, the Swift compiler may silently miss the protocol requirement, resulting in sticky empty userInfo.\n\n\n### Tip: Derive Columns from Coding Keys\n\nCodable types are granted with a [CodingKeys](https://developer.apple.com/documentation/foundation/archives_and_serialization/encoding_and_decoding_custom_types) enum. You can use them to safely define database columns:\n\n```swift\nstruct Player: Codable {\n    var id: Int64\n    var name: String\n    var score: Int\n}\n\nextension Player: FetchableRecord, PersistableRecord {\n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let name = Column(CodingKeys.name)\n        static let score = Column(CodingKeys.score)\n    }\n}\n```\n\nSee the [query interface](#the-query-interface) and [Recommended Practices for Designing Record Types](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/recordrecommendedpractices) for further information.\n\n\n## Record Comparison\n\n**Records that adopt the [EncodableRecord] protocol can compare against other records, or against previous versions of themselves.**\n\nThis helps avoiding costly UPDATE statements when a record has not been edited.\n\n- [The `updateChanges` Methods](#the-updatechanges-methods)\n- [The `databaseEquals` Method](#the-databaseequals-method)\n- [The `databaseChanges` and `hasDatabaseChanges` Methods](#the-databasechanges-and-hasdatabasechanges-methods)\n\n\n### The `updateChanges` Methods\n\nThe `updateChanges` methods perform a database update of the changed columns only (and does nothing if record has no change).\n\n- `updateChanges(_:from:)`\n\n    This method lets you compare two records:\n\n    ```swift\n    if let oldPlayer = try Player.fetchOne(db, id: 42) {\n        var newPlayer = oldPlayer\n        newPlayer.score = 100\n        if try newPlayer.updateChanges(db, from: oldPlayer) {\n            print(\"player was modified, and updated in the database\")\n        } else {\n            print(\"player was not modified, and database was not hit\")\n        }\n    }\n    ```\n\n- `updateChanges(_:modify:)`\n    \n    This method lets you update a record in place:\n    \n    ```swift\n    if var player = try Player.fetchOne(db, id: 42) {\n        let modified = try player.updateChanges(db) {\n            $0.score = 100\n        }\n        if modified {\n            print(\"player was modified, and updated in the database\")\n        } else {\n            print(\"player was not modified, and database was not hit\")\n        }\n    }\n    ```\n\n### The `databaseEquals` Method\n\nThis method returns whether two records have the same database representation:\n\n```swift\nlet oldPlayer: Player = ...\nvar newPlayer: Player = ...\nif newPlayer.databaseEquals(oldPlayer) == false {\n    try newPlayer.save(db)\n}\n```\n\n> **Note**: The comparison is performed on the database representation of records. As long as your record type adopts the EncodableRecord protocol, you don't need to care about Equatable.\n\n\n### The `databaseChanges` and `hasDatabaseChanges` Methods\n\n`databaseChanges(from:)` returns a dictionary of differences between two records:\n\n```swift\nlet oldPlayer = Player(id: 1, name: \"Arthur\", score: 100)\nlet newPlayer = Player(id: 1, name: \"Arthur\", score: 1000)\nfor (column, oldValue) in try newPlayer.databaseChanges(from: oldPlayer) {\n    print(\"\\(column) was \\(oldValue)\")\n}\n// prints \"score was 100\"\n```\n\nFor an efficient algorithm which synchronizes the content of a database table with a JSON payload, check [groue/SortedDifference](https://github.com/groue/SortedDifference).\n\n\n## Record Customization Options\n\nGRDB records come with many default behaviors, that are designed to fit most situations. Many of those defaults can be customized for your specific needs:\n\n- [Persistence Callbacks]: define what happens when you call a persistence method such as `player.insert(db)`\n- [Conflict Resolution]: Run `INSERT OR REPLACE` queries, and generally define what happens when a persistence method violates a unique index.\n- [Columns Selected by a Request]: define which columns are selected by requests such as `Player.fetchAll(db)`.\n- [Beyond FetchableRecord]: the FetchableRecord protocol is not the end of the story.\n\n[Codable Records] have a few extra options:\n\n- [JSON Columns]: control the format of JSON columns.\n- [Column Names Coding Strategies]: control how coding keys are turned into column names\n- [Date and UUID Coding Strategies]: control the format of Date and UUID properties in your Codable records.\n- [The userInfo Dictionary]: adapt your Codable implementation for the database.\n\n\n### Conflict Resolution\n\n**Insertions and updates can create conflicts**: for example, a query may attempt to insert a duplicate row that violates a unique index.\n\nThose conflicts normally end with an error. Yet SQLite let you alter the default behavior, and handle conflicts with specific policies. For example, the `INSERT OR REPLACE` statement handles conflicts with the \"replace\" policy which replaces the conflicting row instead of throwing an error.\n\nThe [five different policies](https://www.sqlite.org/lang_conflict.html) are: abort (the default), replace, rollback, fail, and ignore.\n\n**SQLite let you specify conflict policies at two different places:**\n\n- In the definition of the database table:\n    \n    ```swift\n    // CREATE TABLE player (\n    //     id INTEGER PRIMARY KEY AUTOINCREMENT,\n    //     email TEXT UNIQUE ON CONFLICT REPLACE\n    // )\n    try db.create(table: \"player\") { t in\n        t.autoIncrementedPrimaryKey(\"id\")\n        t.column(\"email\", .text).unique(onConflict: .replace) // <--\n    }\n    \n    // Despite the unique index on email, both inserts succeed.\n    // The second insert replaces the first row:\n    try db.execute(sql: \"INSERT INTO player (email) VALUES (?)\", arguments: [\"arthur@example.com\"])\n    try db.execute(sql: \"INSERT INTO player (email) VALUES (?)\", arguments: [\"arthur@example.com\"])\n    ```\n    \n- In each modification query:\n    \n    ```swift\n    // CREATE TABLE player (\n    //     id INTEGER PRIMARY KEY AUTOINCREMENT,\n    //     email TEXT UNIQUE\n    // )\n    try db.create(table: \"player\") { t in\n        t.autoIncrementedPrimaryKey(\"id\")\n        t.column(\"email\", .text).unique()\n    }\n    \n    // Again, despite the unique index on email, both inserts succeed.\n    try db.execute(sql: \"INSERT OR REPLACE INTO player (email) VALUES (?)\", arguments: [\"arthur@example.com\"])\n    try db.execute(sql: \"INSERT OR REPLACE INTO player (email) VALUES (?)\", arguments: [\"arthur@example.com\"])\n    ```\n\nWhen you want to handle conflicts at the query level, specify a custom `persistenceConflictPolicy` in your type that adopts the PersistableRecord protocol. It will alter the INSERT and UPDATE queries run by the `insert`, `update` and `save` [persistence methods]:\n\n```swift\nprotocol MutablePersistableRecord {\n    /// The policy that handles SQLite conflicts when records are\n    /// inserted or updated.\n    ///\n    /// This property is optional: its default value uses the ABORT\n    /// policy for both insertions and updates, so that GRDB generate\n    /// regular INSERT and UPDATE queries.\n    static var persistenceConflictPolicy: PersistenceConflictPolicy { get }\n}\n\nstruct Player : MutablePersistableRecord {\n    static let persistenceConflictPolicy = PersistenceConflictPolicy(\n        insert: .replace,\n        update: .replace)\n}\n\n// INSERT OR REPLACE INTO player (...) VALUES (...)\ntry player.insert(db)\n```\n\n> **Note**: If you specify the `ignore` policy for inserts, the [`didInsert` callback](#persistence-callbacks) will be called with some random id in case of failed insert. You can detect failed insertions with `insertAndFetch`:\n>     \n> ```swift\n> // How to detect failed `INSERT OR IGNORE`:\n> // INSERT OR IGNORE INTO player ... RETURNING *\n> do {\n>     let insertedPlayer = try player.insertAndFetch(db) {\n>     // Successful insertion\n> catch RecordError.recordNotFound {\n>     // Failed insertion due to IGNORE policy\n> }\n> ```\n>\n> **Note**: The `replace` policy may have to delete rows so that inserts and updates can succeed. Those deletions are not reported to [transaction observers](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/transactionobserver) (this might change in a future release of SQLite).\n\n### Beyond FetchableRecord\n\n**Some GRDB users eventually discover that the [FetchableRecord] protocol does not fit all situations.** Use cases that are not well handled by FetchableRecord include:\n\n- Your application needs polymorphic row decoding: it decodes some type or another, depending on the values contained in a database row.\n\n- Your application needs to decode rows with a context: each decoded value should be initialized with some extra value that does not come from the database.\n\nSince those use cases are not well handled by FetchableRecord, don't try to implement them on top of this protocol: you'll just fight the framework.\n\n\n## Examples of Record Definitions\n\nWe will show below how to declare a record type for the following database table:\n\n```swift\ntry dbQueue.write { db in\n    try db.create(table: \"place\") { t in\n        t.autoIncrementedPrimaryKey(\"id\")\n        t.column(\"title\", .text).notNull()\n        t.column(\"isFavorite\", .boolean).notNull().defaults(to: false)\n        t.column(\"longitude\", .double).notNull()\n        t.column(\"latitude\", .double).notNull()\n    }\n}\n```\n\nEach one of the three examples below is correct. You will pick one or the other depending on your personal preferences and the requirements of your application:\n\n<details>\n  <summary>Define a Codable struct, and adopt the record protocols you need</summary>\n\nThis is the shortest way to define a record type.\n\nSee the [Record Protocols Overview](#record-protocols-overview), and [Codable Records] for more information.\n\n```swift\nstruct Place: Codable {\n    var id: Int64?\n    var title: String\n    var isFavorite: Bool\n    private var latitude: CLLocationDegrees\n    private var longitude: CLLocationDegrees\n    \n    var coordinate: CLLocationCoordinate2D {\n        get {\n            CLLocationCoordinate2D(\n                latitude: latitude,\n                longitude: longitude)\n        }\n        set {\n            latitude = newValue.latitude\n            longitude = newValue.longitude\n        }\n    }\n}\n\n// SQL generation\nextension Place: TableRecord {\n    /// The table columns\n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let title = Column(CodingKeys.title)\n        static let isFavorite = Column(CodingKeys.isFavorite)\n        static let latitude = Column(CodingKeys.latitude)\n        static let longitude = Column(CodingKeys.longitude)\n    }\n}\n\n// Fetching methods\nextension Place: FetchableRecord { }\n\n// Persistence methods\nextension Place: MutablePersistableRecord {\n    // Update auto-incremented id upon successful insertion\n    mutating func didInsert(_ inserted: InsertionSuccess) {\n        id = inserted.rowID\n    }\n}\n```\n\n</details>\n\n<details>\n  <summary>Define a plain struct, and adopt the record protocols you need</summary>\n\nSee the [Record Protocols Overview](#record-protocols-overview) for more information.\n    \n```swift\nstruct Place {\n    var id: Int64?\n    var title: String\n    var isFavorite: Bool\n    var coordinate: CLLocationCoordinate2D\n}\n\n// SQL generation\nextension Place: TableRecord {\n    /// The table columns\n    enum Columns {\n        static let id = Column(\"id\")\n        static let title = Column(\"title\")\n        static let isFavorite = Column(\"isFavorite\")\n        static let latitude = Column(\"latitude\")\n        static let longitude = Column(\"longitude\")\n    }\n}\n\n// Fetching methods\nextension Place: FetchableRecord {\n    /// Creates a record from a database row\n    init(row: Row) {\n        id = row[Columns.id]\n        title = row[Columns.title]\n        isFavorite = row[Columns.isFavorite]\n        coordinate = CLLocationCoordinate2D(\n            latitude: row[Columns.latitude],\n            longitude: row[Columns.longitude])\n    }\n}\n\n// Persistence methods\nextension Place: MutablePersistableRecord {\n    /// The values persisted in the database\n    func encode(to container: inout PersistenceContainer) {\n        container[Columns.id] = id\n        container[Columns.title] = title\n        container[Columns.isFavorite] = isFavorite\n        container[Columns.latitude] = coordinate.latitude\n        container[Columns.longitude] = coordinate.longitude\n    }\n    \n    // Update auto-incremented id upon successful insertion\n    mutating func didInsert(_ inserted: InsertionSuccess) {\n        id = inserted.rowID\n    }\n}\n```\n\n</details>\n\n<details>\n  <summary>Define a plain struct optimized for fetching performance</summary>\n\nThis struct derives its persistence methods from the standard Encodable protocol (see [Codable Records]), but performs optimized row decoding by accessing database columns with numeric indexes.\n\nSee the [Record Protocols Overview](#record-protocols-overview) for more information.\n    \n```swift\nstruct Place: Encodable {\n    var id: Int64?\n    var title: String\n    var isFavorite: Bool\n    private var latitude: CLLocationDegrees\n    private var longitude: CLLocationDegrees\n    \n    var coordinate: CLLocationCoordinate2D {\n        get {\n            CLLocationCoordinate2D(\n                latitude: latitude,\n                longitude: longitude)\n        }\n        set {\n            latitude = newValue.latitude\n            longitude = newValue.longitude\n        }\n    }\n}\n\n// SQL generation\nextension Place: TableRecord {\n    /// The table columns\n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let title = Column(CodingKeys.title)\n        static let isFavorite = Column(CodingKeys.isFavorite)\n        static let latitude = Column(CodingKeys.latitude)\n        static let longitude = Column(CodingKeys.longitude)\n    }\n    \n    /// Arrange the selected columns and lock their order\n    static var databaseSelection: [any SQLSelectable] {\n        [\n            Columns.id,\n            Columns.title,\n            Columns.favorite,\n            Columns.latitude,\n            Columns.longitude,\n        ]\n    }\n}\n\n// Fetching methods\nextension Place: FetchableRecord {\n    /// Creates a record from a database row\n    init(row: Row) {\n        // For high performance, use numeric indexes that match the\n        // order of Place.databaseSelection\n        id = row[0]\n        title = row[1]\n        isFavorite = row[2]\n        coordinate = CLLocationCoordinate2D(\n            latitude: row[3],\n            longitude: row[4])\n    }\n}\n\n// Persistence methods\nextension Place: MutablePersistableRecord {\n    // Update auto-incremented id upon successful insertion\n    mutating func didInsert(_ inserted: InsertionSuccess) {\n        id = inserted.rowID\n    }\n}\n```\n\n</details>\n\n\nThe Query Interface\n===================\n\n**The query interface lets you write pure Swift instead of SQL:**\n\n```swift\ntry dbQueue.write { db in\n    // Update database schema\n    try db.create(table: \"player\") { t in ... }\n    \n    // Fetch records\n    let bestPlayers = try Player\n        .order(\\.score.desc)\n        .limit(10)\n        .fetchAll(db)\n    \n    // Count\n    let count = try Player\n        .filter { $0.score >= 1000 }\n        .fetchCount(db)\n    \n    // Batch update\n    try Player\n        .filter { $0.team == \"Reds\" }\n        .updateAll(db) { $0.score += 100 }\n    \n    // Batch delete\n    try Player\n        .filter { $0.score == 0 }\n        .deleteAll(db)\n}\n```\n\nYou need to open a [database connection] before you can query the database.\n\nPlease bear in mind that the query interface can not generate all possible SQL queries. You may also *prefer* writing SQL, and this is just OK. From little snippets to full queries, your SQL skills are welcome:\n\n```swift\ntry dbQueue.write { db in\n    // Update database schema (with SQL)\n    try db.execute(sql: \"CREATE TABLE player (...)\")\n    \n    // Fetch records (with SQL)\n    let bestPlayers = try Player.fetchAll(db, sql: \"\"\"\n        SELECT * FROM player ORDER BY score DESC LIMIT 10\n        \"\"\")\n    \n    // Count (with an SQL snippet)\n    let minScore = 1000\n    let count = try Player\n        .filter(sql: \"score >= ?\", arguments: [minScore])\n        .fetchCount(db)\n    \n    // Update (with SQL)\n    try db.execute(sql: \"UPDATE player SET score = score + 100 WHERE team = 'Reds'\")\n    \n    // Delete (with SQL)\n    try db.execute(sql: \"DELETE FROM player WHERE score = 0\")\n}\n```\n\nSo don't miss the [SQL API](#sqlite-api).\n\n> **Note**: the generated SQL may change between GRDB releases, without notice: don't have your application rely on any specific SQL output.\n\n- [The Database Schema](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseschema)\n- [Requests](#requests)\n- [Expressions](#expressions)\n    - [SQL Operators](#sql-operators)\n    - [SQL Functions](#sql-functions)\n- [Embedding SQL in Query Interface Requests]\n- [Fetching from Requests]\n- [Fetching by Key](#fetching-by-key)\n- [Testing for Record Existence](#testing-for-record-existence)\n- [Fetching Aggregated Values](#fetching-aggregated-values)\n- [Delete Requests](#delete-requests)\n- [Update Requests](#update-requests)\n- [Custom Requests](#custom-requests)\n- :blue_book: [Associations and Joins](Documentation/AssociationsBasics.md)\n- :blue_book: [Common Table Expressions]\n- :blue_book: [Query Interface Organization]\n\n## Requests\n\nğŸ“– [`QueryInterfaceRequest`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterfacerequest), [`Table`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/table)\n\n**The query interface requests** let you fetch values from the database:\n\n```swift\nlet request = Player.filter { $0.email != nil }.order(\\.name)\nlet players = try request.fetchAll(db)  // [Player]\nlet count = try request.fetchCount(db)  // Int\n```\n\nQuery interface requests usually start from **a type** that adopts the `TableRecord` protocol:\n\n```swift\nstruct Player: TableRecord { ... }\n\n// The request for all players:\nlet request = Player.all()\nlet players = try request.fetchAll(db) // [Player]\n```\n\nWhen you can not use a record type, use `Table`:\n\n```swift\n// The request for all rows from the player table:\nlet table = Table(\"player\")\nlet request = table.all()\nlet rows = try request.fetchAll(db)    // [Row]\n\n// The request for all players from the player table:\nlet table = Table<Player>(\"player\")\nlet request = table.all()\nlet players = try request.fetchAll(db) // [Player]\n```\n\n> **Note**: all examples in the documentation below use a record type, but you can always substitute a `Table` instead.\n\nNext, declare the table **columns** that you want to use for filtering, or sorting, in a nested type named `Columns`:\n\n```swift\nextension Player {\n    enum Columns {\n        static let id = Column(\"id\")\n        static let name = Column(\"name\")\n    }\n}\n```\n\nWhen `Player` is `Codable`, you'll prefer defining columns from coding keys:\n\n```swift\nextension Player {\n    enum Columns {\n        static let id = Column(CodingKeys.id)\n        static let name = Column(CodingKeys.name)\n    }\n}\n```\n\nYou can now build requests with the following methods: `all`, `none`, `select`, `distinct`, `filter`, `matching`, `group`, `having`, `order`, `reversed`, `limit`, `joining`, `including`, `with`. All those methods return another request, which you can further refine by applying another method: `Player.select(...).filter(...).order(...)`.\n\n- [`all()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerecord/all()), [`none()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerecord/none()): the requests for all rows, or no row.\n\n    ```swift\n    // SELECT * FROM player\n    Player.all()\n    ```\n    \n    By default, all columns are selected. See [Columns Selected by a Request].\n\n- [`select(...)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/selectionrequest/select(_:)-ruzy) and [`select(..., as:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterfacerequest/select(_:as:)-58954) define the selected columns. See [Columns Selected by a Request].\n    \n    ```swift\n    // SELECT name FROM player\n    Player.select(\\.name, as: String.self)\n    ```\n\n- [`selectID()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterfacerequest/selectID()) is available on [Identifiable Records]. It supports all tables that have a single-column primary key:\n\n    ```swift\n    // SELECT id FROM player\n    Player.selectID()\n    \n    // SELECT id FROM player WHERE name IS NOT NULL\n    Player.filter { $0.name != nil }.selectID()\n    ```\n\n- [`annotated(with: expression...)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/selectionrequest/annotated(with:)-1satx) extends the selection.\n\n    ```swift\n    // SELECT *, (score + bonus) AS total FROM player\n    Player.annotated { ($0.score + $0.bonus).forKey(\"total\") }\n    ```\n\n- [`annotated(with: aggregate)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/derivablerequest/annotated(with:)-74xfs) extends the selection with [association aggregates](Documentation/AssociationsBasics.md#association-aggregates).\n    \n    ```swift\n    // SELECT team.*, COUNT(DISTINCT player.id) AS playerCount\n    // FROM team\n    // LEFT JOIN player ON player.teamId = team.id\n    // GROUP BY team.id\n    Team.annotated(with: Team.players.count)\n    ```\n\n- [`annotated(withRequired: association)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/annotated(withrequired:)) and [`annotated(withOptional: association)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/annotated(withoptional:)) extends the selection with [Associations].\n    \n    ```swift\n    // SELECT player.*, team.color\n    // FROM player\n    // JOIN team ON team.id = player.teamId\n    Player.annotated(withRequired: Player.team.select(\\.color))\n    ```\n\n- [`distinct()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/derivablerequest/distinct()) performs uniquing.\n    \n    ```swift\n    // SELECT DISTINCT name FROM player\n    Player.select(\\.name, as: String.self).distinct()\n    ```\n\n- [`filter(expression)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/filteredrequest/filter(_:)-6xr3d) applies conditions.\n    \n    ```swift\n    // SELECT * FROM player WHERE id IN (1, 2, 3)\n    Player.filter { [1,2,3].contains($0.id) }\n    \n    // SELECT * FROM player WHERE (name IS NOT NULL) AND (height > 1.75)\n    Player.filter { $0.name != nil && $0.height > 1.75 }\n    ```\n\n- [`filter(id:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/filter(id:)) and [`filter(ids:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/filter(ids:)) are type-safe methods available on [Identifiable Records]:\n    \n    ```swift\n    // SELECT * FROM player WHERE id = 1\n    Player.filter(id: 1)\n    \n    // SELECT * FROM country WHERE isoCode IN ('FR', 'US')\n    Country.filter(ids: [\"FR\", \"US\"])\n    ```\n    \n- [`filter(key:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/filter(key:)-1p9sq) and [`filter(keys:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/filter(keys:)-6ggt1) apply conditions on primary and unique keys:\n    \n    ```swift\n    // SELECT * FROM player WHERE id = 1\n    Player.filter(key: 1)\n    \n    // SELECT * FROM country WHERE isoCode IN ('FR', 'US')\n    Country.filter(keys: [\"FR\", \"US\"])\n    \n    // SELECT * FROM citizenship WHERE citizenId = 1 AND countryCode = 'FR'\n    Citizenship.filter(key: [\"citizenId\": 1, \"countryCode\": \"FR\"])\n    \n    // SELECT * FROM player WHERE email = 'arthur@example.com'\n    Player.filter(key: [\"email\": \"arthur@example.com\"])\n    ```\n\n- `matching(pattern)` ([FTS3](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/matching(_:)-3s3zr), [FTS5](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/matching(_:)-7c1e8)) performs [full-text search](Documentation/FullTextSearch.md).\n    \n    ```swift\n    // SELECT * FROM document WHERE document MATCH 'sqlite database'\n    let pattern = FTS3Pattern(matchingAllTokensIn: \"SQLite database\")\n    Document.matching(pattern)\n    ```\n    \n    When the pattern is nil, no row will match.\n\n- [`group(expression, ...)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/aggregatingrequest/group(_:)-2g7br) groups rows.\n    \n    ```swift\n    // SELECT name, MAX(score) FROM player GROUP BY name\n    Player\n        .select { [$0.name, max($0.score)] }\n        .group(\\.name)\n    ```\n\n- [`having(expression)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/aggregatingrequest/having(_:)-2oggh) applies conditions on grouped rows.\n    \n    ```swift\n    // SELECT team, MAX(score) FROM player GROUP BY team HAVING MIN(score) >= 1000\n    Player\n        .select { [$0.team, max($0.score)] }\n        .group(\\.team)\n        .having { min($0.score) >= 1000 }\n    ```\n\n- [`having(aggregate)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/derivablerequest/having(_:)) applies conditions on grouped rows, according to an [association aggregate](Documentation/AssociationsBasics.md#association-aggregates).\n    \n    ```swift\n    // SELECT team.*\n    // FROM team\n    // LEFT JOIN player ON player.teamId = team.id\n    // GROUP BY team.id\n    // HAVING COUNT(DISTINCT player.id) >= 5\n    Team.having(Team.players.count >= 5)\n    ```\n\n- [`order(ordering, ...)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/orderedrequest/order(_:)-9d0hr) sorts.\n    \n    ```swift\n    // SELECT * FROM player ORDER BY name\n    Player.order(\\.name)\n    \n    // SELECT * FROM player ORDER BY score DESC\n    Player.order(\\.score.desc)\n    \n    // SELECT * FROM player ORDER BY score DESC, name\n    Player.order { [$0.score.desc, $0.name] }\n    ```\n    \n    SQLite considers NULL values to be smaller than any other values for sorting purposes. Hence, NULLs naturally appear at the beginning of an ascending ordering and at the end of a descending ordering. With a [custom SQLite build], this can be changed using `.ascNullsLast` and `.descNullsFirst`:\n    \n    ```swift\n    // SELECT * FROM player ORDER BY score ASC NULLS LAST\n    Player.order(\\.name.ascNullsLast)\n    ```\n    \n    Each `order` call clears any previous ordering:\n    \n    ```swift\n    // SELECT * FROM player ORDER BY name\n    Player.order(\\.score).order(\\.name)\n    ```\n\n- [`reversed()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/orderedrequest/reversed()) reverses the eventual orderings.\n    \n    ```swift\n    // SELECT * FROM player ORDER BY score ASC, name DESC\n    Player.order { [$0.score.desc, $0.name] }.reversed()\n    ```\n    \n    If no ordering was already specified, this method has no effect:\n    \n    ```swift\n    // SELECT * FROM player\n    Player.all().reversed()\n    ```\n\n- [`limit(limit, offset: offset)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterfacerequest/limit(_:offset:)) limits and pages results.\n    \n    ```swift\n    // SELECT * FROM player LIMIT 5\n    Player.limit(5)\n    \n    // SELECT * FROM player LIMIT 5 OFFSET 10\n    Player.limit(5, offset: 10)\n    ```\n\n- [`joining(required:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/joining(required:)), [`joining(optional:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/joining(optional:)), [`including(required:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/including(required:)), [`including(optional:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/including(optional:)), and [`including(all:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/joinablerequest/including(all:)) fetch and join records through [Associations].\n    \n    ```swift\n    // SELECT player.*, team.*\n    // FROM player\n    // JOIN team ON team.id = player.teamId\n    Player.including(required: Player.team)\n    ```\n\n- [`with(cte)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/derivablerequest/with(_:)) embeds a [common table expression]:\n    \n    ```swift\n    // WITH ... SELECT * FROM player\n    let cte = CommonTableExpression(...)\n    Player.with(cte)\n    ```\n\n- Other requests that involve the primary key:\n    \n    - [`selectPrimaryKey(as:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterfacerequest/selectprimarykey(as:)) selects the primary key.\n    \n        ```swift\n        // SELECT id FROM player\n        Player.selectPrimaryKey(as: Int64.self)    // QueryInterfaceRequest<Int64>\n        \n        // SELECT code FROM country\n        Country.selectPrimaryKey(as: String.self)  // QueryInterfaceRequest<String>\n        \n        // SELECT citizenId, countryCode FROM citizenship\n        Citizenship.selectPrimaryKey(as: Row.self) // QueryInterfaceRequest<Row>\n        ```\n        \n    - [`orderByPrimaryKey()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/orderbyprimarykey()) sorts by primary key.\n        \n        ```swift\n        // SELECT * FROM player ORDER BY id\n        Player.orderByPrimaryKey()\n        \n        // SELECT * FROM country ORDER BY code\n        Country.orderByPrimaryKey()\n        \n        // SELECT * FROM citizenship ORDER BY citizenId, countryCode\n        Citizenship.orderByPrimaryKey()\n        ```\n    \n    - [`groupByPrimaryKey()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/tablerequest/groupbyprimarykey()) groups rows by primary key.\n\n\nYou can refine requests by chaining those methods:\n\n```swift\n// SELECT * FROM player WHERE (email IS NOT NULL) ORDER BY name\nPlayer.order(\\.name).filter { $0.email != nil }\n```\n\nThe `select`, `order`, `group`, and `limit` methods ignore and replace previously applied selection, orderings, grouping, and limits. On the opposite, `filter`, `matching`, and `having` methods extend the query:\n\n```swift\nPlayer                          // SELECT * FROM player\n    .filter { $0.name != nil }  // WHERE (name IS NOT NULL)\n    .filter { $0.email != nil } //        AND (email IS NOT NULL)\n    .order(\\.name)              // - ignored -\n    .reversed()                 // - ignored -\n    .order(\\.score)             // ORDER BY score\n    .limit(20, offset: 40)      // - ignored -\n    .limit(10)                  // LIMIT 10\n```\n\n\nRaw SQL snippets are also accepted, with eventual [arguments](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statementarguments):\n\n```swift\n// SELECT DATE(creationDate), COUNT(*) FROM player WHERE name = 'Arthur' GROUP BY date(creationDate)\nPlayer\n    .select(sql: \"DATE(creationDate), COUNT(*)\")\n    .filter(sql: \"name = ?\", arguments: [\"Arthur\"])\n    .group(sql: \"DATE(creationDate)\")\n```\n\n\n### Columns Selected by a Request\n\nBy default, query interface requests select all columns:\n\n```swift\n// SELECT * FROM player\nstruct Player: TableRecord { ... }\nlet request = Player.all()\n\n// SELECT * FROM player\nlet table = Table(\"player\")\nlet request = table.all()\n```\n\n**The selection can be changed for each individual requests, or in the case of record-based requests, for all requests built from this record type.**\n\nThe `select(...)` and `select(..., as:)` methods change the selection of a single request (see [Fetching from Requests] for detailed information):\n\n```swift\nlet request = Player.select { max($0.score) }\nlet maxScore = try Int.fetchOne(db, request) // Int?\n\nlet request = Player.select({ max($0.score) }, as: Int.self)\nlet maxScore = try request.fetchOne(db)      // Int?\n```\n\nThe default selection for a record type is controlled by the `databaseSelection` property. For example:\n\n```swift\n// Select a limited set of columns\nstruct RestrictedPlayer: TableRecord {\n    static let databaseTableName = \"player\"\n    \n    enum Columns {\n        static let id = Column(\"id\")\n        static let name = Column(\"name\")\n    }\n    \n    static var databaseSelection: [any SQLSelectable] {\n        [Columns.id, Columns.name]\n    }\n}\n\n// SELECT id, name FROM player\nlet request = RestrictedPlayer.all()\n```\n\n```swift\n// Select all but a few columns\nstruct Player : TableRecord {\n    static var databaseSelection: [any SQLSelectable] { \n        [.allColumns(excluding: [\"generatedColumn\"])]\n    }\n}\n\n// SELECT id, name FROM player\nlet request = RestrictedPlayer.all()\n```\n\n```swift\n// Select all columns and more\nstruct ExtendedPlayer : TableRecord {\n    static let databaseTableName = \"player\"\n    static var databaseSelection: [any SQLSelectable] {\n        [.allColumns, .rowID]\n    }\n}\n\n// SELECT *, rowid FROM player\nlet request = ExtendedPlayer.all()\n```\n\n> **Note**: make sure the `databaseSelection` property is explicitly declared as `[any SQLSelectable]`. If it is not, the Swift compiler may silently miss the protocol requirement, resulting in sticky `SELECT *` requests. To verify your setup, see the [How do I print a request as SQL?](#how-do-i-print-a-request-as-sql) FAQ.\n\n\n## Expressions\n\nFeed [requests](#requests) with SQL expressions built from your Swift code:\n\n\n### SQL Operators\n\nğŸ“– [`SQLSpecificExpressible`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/sqlspecificexpressible)\n\nGRDB comes with a Swift version of many SQLite [built-in operators](https://sqlite.org/lang_expr.html#operators), listed below. But not all: see [Embedding SQL in Query Interface Requests] for a way to add support for missing SQL operators.\n\n- `=`, `<>`, `<`, `<=`, `>`, `>=`, `IS`, `IS NOT`\n    \n    Comparison operators are based on the Swift operators `==`, `!=`, `===`, `!==`, `<`, `<=`, `>`, `>=`:\n    \n    ```swift\n    // SELECT * FROM player WHERE (name = 'Arthur')\n    Player.filter { $0.name == \"Arthur\" }\n    \n    // SELECT * FROM player WHERE (name IS NULL)\n    Player.filter { $0.name == nil }\n    \n    // SELECT * FROM player WHERE (score IS 1000)\n    Player.filter { $0.score === 1000 }\n    \n    // SELECT * FROM rectangle WHERE width < height\n    Rectangle.filter { $0.width < $0.height }\n    ```\n    \n    Subqueries are supported:\n    \n    ```swift\n    // SELECT * FROM player WHERE score = (SELECT max(score) FROM player)\n    let maximumScore = Player.select { max($0.score) }\n    Player.filter { $0.score == maximumScore }\n    \n    // SELECT * FROM player WHERE score = (SELECT max(score) FROM player)\n    let maximumScore = SQLRequest(\"SELECT max(score) FROM player\")\n    Player.filter { $0.score == maximumScore }\n    ```\n    \n    > **Note**: SQLite string comparison, by default, is case-sensitive and not Unicode-aware. See [string comparison](#string-comparison) if you need more control.\n\n- `*`, `/`, `+`, `-`\n    \n    SQLite arithmetic operators are derived from their Swift equivalent:\n    \n    ```swift\n    // SELECT ((temperature * 1.8) + 32) AS fahrenheit FROM planet\n    Planet.select { ($0.temperature * 1.8 + 32).forKey(\"fahrenheit\") }\n    ```\n    \n    > **Note**: an expression like `nameColumn + \"rrr\"` will be interpreted by SQLite as a numerical addition (with funny results), not as a string concatenation. See the `concat` operator below.\n    \n    When you want to join a sequence of expressions with the `+` or `*` operator, use `joined(operator:)`:\n    \n    ```swift\n    // SELECT score + bonus + 1000 FROM player\n    Player.select {\n        [$0.score, $0.bonus, 1000.databaseValue].joined(operator: .add)\n    }\n    ```\n    \n    Note in the example above how you concatenate raw values: `1000.databaseValue`. A plain `1000` would not compile.\n    \n    When the sequence is empty, `joined(operator: .add)` returns 0, and `joined(operator: .multiply)` returns 1.\n\n- `&`, `|`, `~`, `<<`, `>>`\n    \n    Bitwise operations (bitwise and, or, not, left shift, right shift) are derived from their Swift equivalent:\n    \n    ```swift\n    // SELECT mask & 2 AS isRocky FROM planet\n    Planet.select { ($0.mask & 2).forKey(\"isRocky\") }\n    ```\n\n- `||`\n    \n    Concatenate several strings:\n    \n    ```swift\n    // SELECT firstName || ' ' || lastName FROM player\n    Player.select {\n        [$0.firstName, \" \".databaseValue, $0.lastName].joined(operator: .concat)\n    }\n    ```\n    \n    Note in the example above how you concatenate raw strings: `\" \".databaseValue`. A plain `\" \"` would not compile.\n    \n    When the sequence is empty, `joined(operator: .concat)` returns the empty string.\n\n- `AND`, `OR`, `NOT`\n    \n    The SQL logical operators are derived from the Swift `&&`, `||` and `!`:\n    \n    ```swift\n    // SELECT * FROM player WHERE ((NOT isVerified) OR (score < 1000))\n    Player.filter { !$0.isVerified || $0.score < 1000 }\n    ```\n    \n    When you want to join a sequence of expressions with the `AND` or `OR` operator, use `joined(operator:)`:\n    \n    ```swift\n    // SELECT * FROM player WHERE (isVerified AND (score >= 1000) AND (name IS NOT NULL))\n    Player.filter {\n        [$0.isVerified, $0.score >= 1000, $0.name != nil].joined(operator: .and)\n    }\n    ```\n    \n    When the sequence is empty, `joined(operator: .and)` returns true, and `joined(operator: .or)` returns false:\n\n- `BETWEEN`, `IN`, `NOT IN`\n    \n    To check inclusion in a Swift sequence (array, set, rangeâ€¦), call the `contains` method:\n    \n    ```swift\n    // SELECT * FROM player WHERE id IN (1, 2, 3)\n    Player.filter { [1, 2, 3].contains($0.id) }\n    \n    // SELECT * FROM player WHERE id NOT IN (1, 2, 3)\n    Player.filter { ![1, 2, 3].contains($0.id) }\n    \n    // SELECT * FROM player WHERE score BETWEEN 0 AND 1000\n    Player.filter { (0...1000).contains($0.score) }\n    \n    // SELECT * FROM player WHERE (score >= 0) AND (score < 1000)\n    Player.filter { (0..<1000).contains($0.score) }\n    \n    // SELECT * FROM player WHERE initial BETWEEN 'A' AND 'N'\n    Player.filter { (\"A\"...\"N\").contains($0.initial) }\n    \n    // SELECT * FROM player WHERE (initial >= 'A') AND (initial < 'N')\n    Player.filter { (\"A\"..<\"N\").contains($0.initial) }\n    ```\n    \n    To check inclusion inside a subquery, call the `contains` method as well:\n    \n    ```swift\n    // SELECT * FROM player WHERE id IN (SELECT playerId FROM playerSelection)\n    let selectedPlayerIds = PlayerSelection.select(\\.playerId)\n    Player.filter { selectedPlayerIds.contains($0.id) }\n    \n    // SELECT * FROM player WHERE id IN (SELECT playerId FROM playerSelection)\n    let selectedPlayerIds = SQLRequest(\"SELECT playerId FROM playerSelection\")\n    Player.filter { selectedPlayerIds.contains($0.id) }\n    ```\n    \n    To check inclusion inside a [common table expression], call the `contains` method as well:\n    \n    ```swift\n    // WITH selectedName AS (...)\n    // SELECT * FROM player WHERE name IN selectedName\n    let cte = CommonTableExpression(named: \"selectedName\", ...)\n    Player\n        .with(cte)\n        .filter { cte.contains($0.name) }\n    ```\n    \n    > **Note**: SQLite string comparison, by default, is case-sensitive and not Unicode-aware. See [string comparison](#string-comparison) if you need more control.\n\n- `EXISTS`, `NOT EXISTS`\n    \n    To check if a subquery would return rows, call the `exists` method:\n    \n    ```swift\n    // Teams that have at least one other player\n    //\n    //  SELECT * FROM team\n    //  WHERE EXISTS (SELECT * FROM player WHERE teamId = team.id)\n    let teamAlias = TableAlias<Team>()\n    let player = Player.filter { $0.teamId == teamAlias.id }\n    let teams = Team.aliased(teamAlias).filter(player.exists())\n    \n    // Teams that have no player\n    //\n    //  SELECT * FROM team\n    //  WHERE NOT EXISTS (SELECT * FROM player WHERE teamId = team.id)\n    let teams = Team.aliased(teamAlias).filter(!player.exists())\n    ```\n    \n    In the above example, you use a `TableAlias` in order to let a subquery refer to a column from another table.\n    \n    In the next example, which involves the same table twice, the table alias requires an explicit disambiguation with `TableAlias(name:)`:\n    \n    ```swift    \n    // Players who coach at least one other player\n    //\n    //  SELECT coach.* FROM player coach\n    //  WHERE EXISTS (SELECT * FROM player WHERE coachId = coach.id)\n    let coachAlias = TableAlias<Player>(name: \"coach\")\n    let coachedPlayer = Player.filter { $0.coachId == coachAlias.id }\n    let coaches = Player.aliased(coachAlias).filter(coachedPlayer.exists())\n    ```\n    \n    Finally, subqueries can also be expressed as SQL, with [SQL Interpolation]:\n    \n    ```swift\n    // SELECT coach.* FROM player coach\n    // WHERE EXISTS (SELECT * FROM player WHERE coachId = coach.id)\n    let coachedPlayer = SQLRequest(\"SELECT * FROM player WHERE coachId = \\(coachAlias.id)\")\n    let coaches = Player.aliased(coachAlias).filter(coachedPlayer.exists())\n    ```\n    \n- `LIKE`\n    \n    The SQLite LIKE operator is available as the `like` method:\n    \n    ```swift\n    // SELECT * FROM player WHERE (email LIKE '%@example.com')\n    Player.filter { $0.email.like(\"%@example.com\") }\n    \n    // SELECT * FROM book WHERE (title LIKE '%10\\%%' ESCAPE '\\')\n    Player.filter { $0.email.like(\"%10\\\\%%\", escape: \"\\\\\") }\n    ```\n    \n    > **Note**: the SQLite LIKE operator is case-insensitive but not Unicode-aware. For example, the expression `'a' LIKE 'A'` is true but `'Ã¦' LIKE 'Ã†'` is false.\n\n- `MATCH`\n    \n    The full-text MATCH operator is available through [FTS3Pattern](Documentation/FullTextSearch.md#fts3pattern) (for FTS3 and FTS4 tables) and [FTS5Pattern](Documentation/FullTextSearch.md#fts5pattern) (for FTS5):\n    \n    FTS3 and FTS4:\n    \n    ```swift\n    let pattern = FTS3Pattern(matchingAllTokensIn: \"SQLite database\")\n    \n    // SELECT * FROM document WHERE document MATCH 'sqlite database'\n    Document.matching(pattern)\n    \n    // SELECT * FROM document WHERE content MATCH 'sqlite database'\n    Document.filter { $0.content.match(pattern) }\n    ```\n    \n    FTS5:\n    \n    ```swift\n    let pattern = FTS5Pattern(matchingAllTokensIn: \"SQLite database\")\n    \n    // SELECT * FROM document WHERE document MATCH 'sqlite database'\n    Document.matching(pattern)\n    ```\n- `AS`\n    \n    To give an alias to an expression, use the `forKey` method:\n    \n    ```swift\n    // SELECT (score + bonus) AS total\n    // FROM player\n    Player.select { ($0.score + $0.bonus).forKey(\"total\") }\n    ```\n    \n    If you need to refer to this aliased column in another place of the request, use a detached column:\n    \n    ```swift\n    // SELECT (score + bonus) AS total\n    // FROM player \n    // ORDER BY total\n    Player\n        .select { ($0.score + $0.bonus).forKey(\"total\") }\n        .order(Column(\"total\").detached)\n    ```\n    \n    The detached column `Column(\"total\").detached` is not considered as a part of the \"player\" table, so it is always rendered as `total` in the generated SQL, even when the request involves other tables via an [association](Documentation/AssociationsBasics.md) or a [common table expression].\n\n\n### SQL Functions\n\nğŸ“– [`SQLSpecificExpressible`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/sqlspecificexpressible)\n\nGRDB comes with a Swift version of many SQLite [built-in functions](https://sqlite.org/lang_corefunc.html), listed below. But not all: see [Embedding SQL in Query Interface Requests] for a way to add support for missing SQL functions.\n\n- `ABS`, `AVG`, `COALESCE`, `COUNT`, `DATETIME`, `JULIANDAY`, `LENGTH`, `MAX`, `MIN`, `SUM`, `TOTAL`:\n    \n    Those are based on the `abs`, `average`, `coalesce`, `count`, `dateTime`, `julianDay`, `length`, `max`, `min`, `sum`, and `total` Swift functions:\n    \n    ```swift\n    // SELECT MIN(score), MAX(score) FROM player\n    Player.select { [min($0.score), max($0.score)] }\n    \n    // SELECT COUNT(name) FROM player\n    Player.select { count($0.name) }\n    \n    // SELECT COUNT(DISTINCT name) FROM player\n    Player.select { count(distinct: $0.name) }\n    \n    // SELECT JULIANDAY(date, 'start of year') FROM game\n    Game.select { julianDay($0.date, .startOfYear) }\n    ```\n    \n    For more information about the functions `dateTime` and `julianDay`, see [Date And Time Functions](https://www.sqlite.org/lang_datefunc.html).\n\n- `CAST`\n\n    Use the `cast` Swift function:\n    \n    ```swift\n    // SELECT (CAST(wins AS REAL) / games) AS successRate FROM player\n    Player.select { (cast($0.wins, as: .real) / $0.games).forKey(\"successRate\") }\n    ```\n    \n    See [CAST expressions](https://www.sqlite.org/lang_expr.html#castexpr) for more information about SQLite conversions.\n\n- `IFNULL`\n    \n    Use the Swift `??` operator:\n    \n    ```swift\n    // SELECT IFNULL(name, 'Anonymous') FROM player\n    Player.select { $0.name ?? \"Anonymous\" }\n    \n    // SELECT IFNULL(name, email) FROM player\n    Player.select { $0.name ?? $0.email }\n    ```\n\n- `LOWER`, `UPPER`\n    \n    The query interface does not give access to those SQLite functions. Nothing against them, but they are not unicode aware.\n    \n    Instead, GRDB extends SQLite with SQL functions that call the Swift built-in string functions `capitalized`, `lowercased`, `uppercased`, `localizedCapitalized`, `localizedLowercased` and `localizedUppercased`:\n    \n    ```swift\n    Player.select { $0.name.uppercased() }\n    ```\n    \n    > **Note**: When *comparing* strings, you'd rather use a [collation](#string-comparison):\n    >\n    > ```swift\n    > let name: String = ...\n    >\n    > // Not recommended\n    > Player.filter { $0.name.uppercased() == name.uppercased() }\n    >\n    > // Better\n    > Player.filter { $0.name.collating(.caseInsensitiveCompare) == name }\n    > ```\n\n- Custom SQL functions and aggregates\n    \n    You can apply your own [custom SQL functions and aggregates](#custom-functions-):\n    \n    ```swift\n    let myFunction = DatabaseFunction(\"myFunction\", ...)\n    \n    // SELECT myFunction(name) FROM player\n    Player.select { myFunction($0.name) }\n    ```\n\n## Embedding SQL in Query Interface Requests\n\nYou will sometimes want to extend your query interface requests with SQL snippets. This can happen because GRDB does not provide a Swift interface for some SQL function or operator, or because you want to use an SQLite construct that GRDB does not support.\n\nSupport for extensibility is large, but not unlimited. All the SQL queries built by the query interface request have the shape below. _If you need something else, you'll have to use [raw SQL requests](#sqlite-api)._\n\n```sql\nWITH ...     -- 1\nSELECT ...   -- 2\nFROM ...     -- 3\nJOIN ...     -- 4\nWHERE ...    -- 5\nGROUP BY ... -- 6\nHAVING ...   -- 7\nORDER BY ... -- 8\nLIMIT ...    -- 9\n```\n\n1. `WITH ...`: see [Common Table Expressions].\n\n2. `SELECT ...`\n\n    The selection can be provided as raw SQL:\n    \n    ```swift\n    // SELECT IFNULL(name, 'O''Brien'), score FROM player\n    let request = Player.select(sql: \"IFNULL(name, 'O''Brien'), score\")\n    \n    // SELECT IFNULL(name, 'O''Brien'), score FROM player\n    let defaultName = \"O'Brien\"\n    let request = Player.select(sql: \"IFNULL(name, ?), score\", arguments: [suffix])\n    ```\n\n    The selection can be provided with [SQL Interpolation]:\n    \n    ```swift\n    // SELECT IFNULL(name, 'O''Brien'), score FROM player\n    let defaultName = \"O'Brien\"\n    let request = Player.select(literal: \"IFNULL(name, \\(defaultName)), score\")\n    ```\n    \n    The selection can be provided with a mix of Swift and [SQL Interpolation]:\n    \n    ```swift\n    // SELECT IFNULL(name, 'O''Brien') AS displayName, score FROM player\n    let defaultName = \"O'Brien\"\n    let request = Player.select {\n        let displayName: SQL = \"IFNULL(\\($0.name), \\(defaultName)) AS displayName\"\n        return [displayName, $0.score]\n    }\n    ```\n    \n    When the custom SQL snippet should behave as a full-fledged expression, with support for the `+` Swift operator, the `forKey` aliasing method, and all other [SQL Operators](#sql-operators), build an _expression literal_ with the `SQL.sqlExpression` method:\n    \n    ```swift\n    // SELECT IFNULL(name, 'O''Brien') AS displayName, score FROM player\n    let defaultName = \"O'Brien\"\n    let request = Player.select {\n        let displayName = SQL(\"IFNULL(\\($0.name), \\(defaultName))\").sqlExpression\n        return [displayName.forKey(\"displayName\"), $0.score]\n    }\n    ```\n    \n    Such expression literals allow you to build a reusable support library of SQL functions or operators that are missing from the query interface. For example, you can define a Swift `date` function:\n    \n    ```swift\n    func date(_ value: some SQLSpecificExpressible) -> SQLExpression {\n        SQL(\"DATE(\\(value))\").sqlExpression\n    }\n    \n    // SELECT * FROM \"player\" WHERE DATE(\"createdAt\") = '2020-01-23'\n    let request = Player.filter { date($0.createdAt) == \"2020-01-23\" }\n    ```\n    \n    See the [Query Interface Organization] for more information about `SQLSpecificExpressible` and `SQLExpression`.\n    \n3. `FROM ...`: only one table is supported here. You can not customize this SQL part.\n\n4. `JOIN ...`: joins are fully controlled by [Associations]. You can not customize this SQL part.\n\n5. `WHERE ...`\n    \n    The WHERE clause can be provided as raw SQL:\n    \n    ```swift\n    // SELECT * FROM player WHERE score >= 1000\n    let request = Player.filter(sql: \"score >= 1000\")\n    \n    // SELECT * FROM player WHERE score >= 1000\n    let minScore = 1000\n    let request = Player.filter(sql: \"score >= ?\", arguments: [minScore])\n    ```\n\n    The WHERE clause can be provided with [SQL Interpolation]:\n    \n    ```swift\n    // SELECT * FROM player WHERE score >= 1000\n    let minScore = 1000\n    let request = Player.filter(literal: \"score >= \\(minScore)\")\n    ```\n    \n    The WHERE clause can be provided with a mix of Swift and [SQL Interpolation]:\n    \n    ```swift\n    // SELECT * FROM player WHERE (score >= 1000) AND (team = 'red')\n    let minScore = 1000\n    let request = Player.filter { \n        let scoreCondition: SQL = \"\\($0.score) >= \\(minScore)\"\n        return scoreCondition && $0.team == \"red\"\n    }\n    ```\n    \n    See `SELECT ...` above for more SQL Interpolation examples.\n    \n6. `GROUP BY ...`\n\n    The GROUP BY clause can be provided as raw SQL, SQL Interpolation, or a mix of Swift and SQL Interpolation, just as the selection and the WHERE clause (see above).\n    \n7. `HAVING ...`\n\n    The HAVING clause can be provided as raw SQL, SQL Interpolation, or a mix of Swift and SQL Interpolation, just as the selection and the WHERE clause (see above).\n    \n8. `ORDER BY ...`\n\n    The ORDER BY clause can be provided as raw SQL, SQL Interpolation, or a mix of Swift and SQL Interpolation, just as the selection and the WHERE clause (see above).\n    \n    In order to support the `desc` and `asc` query interface operators, and the `reversed()` query interface method, you must provide your orderings as _expression literals_ with the `SQL.sqlExpression` method:\n    \n    ```swift\n    // SELECT * FROM \"player\" \n    // ORDER BY (score + bonus) ASC, name DESC\n    let request = Player\n        .order {\n            let total = SQL(\"(\\($0.score) + \\($0.bonus))\").sqlExpression\n            return [total.desc, $0.name]\n        }\n        .reversed()\n    ```\n    \n9. `LIMIT ...`: use the `limit(_:offset:)` method. You can not customize this SQL part.\n\n\n## Fetching from Requests\n\nOnce you have a request, you can fetch the records at the origin of the request:\n\n```swift\n// Some request based on `Player`\nlet request = Player.filter { ... }... // QueryInterfaceRequest<Player>\n\n// Fetch players:\ntry request.fetchCursor(db) // A Cursor of Player\ntry request.fetchAll(db)    // [Player]\ntry request.fetchSet(db)    // Set<Player>\ntry request.fetchOne(db)    // Player?\n```\n\nFor example:\n\n```swift\nlet allPlayers = try Player.fetchAll(db)                            // [Player]\nlet arthur = try Player.filter { $0.name == \"Arthur\" }.fetchOne(db) // Player?\n```\n\nSee [fetching methods](#fetching-methods) for information about the `fetchCursor`, `fetchAll`, `fetchSet` and `fetchOne` methods.\n\n**You sometimes want to fetch other values**.\n\nThe simplest way is to use the request as an argument to a fetching method of the desired type:\n\n```swift\n// Fetch an Int\nlet request = Player.select { max($0.score) }\nlet maxScore = try Int.fetchOne(db, request) // Int?\n\n// Fetch a Row\nlet request = Player.select { [min($0.score), max($0.score)] }\nlet row = try Row.fetchOne(db, request)!     // Row\nlet minScore = row[0] as Int?\nlet maxScore = row[1] as Int?\n```\n\nYou can also change the request so that it knows the type it has to fetch:\n\n- With `asRequest(of:)`, useful when you use [Associations]:\n    \n    ```swift\n    struct BookInfo: FetchableRecord, Decodable {\n        var book: Book\n        var author: Author\n    }\n    \n    // A request of BookInfo\n    let request = Book\n        .including(required: Book.author)\n        .asRequest(of: BookInfo.self)\n    \n    let bookInfos = try dbQueue.read { db in\n        try request.fetchAll(db) // [BookInfo]\n    }\n    ```\n    \n- With `select(..., as:)`, which is handy when you change the selection:\n    \n    ```swift\n    // A request of Int\n    let request = Player.select({ max($0.score) }, as: Int.self)\n    \n    let maxScore = try dbQueue.read { db in\n        try request.fetchOne(db) // Int?\n    }\n    ```\n\n\n## Fetching by Key\n\n**Fetching records according to their primary key** is a common task.\n\n[Identifiable Records] can use the type-safe methods `find(_:id:)`, `fetchOne(_:id:)`, `fetchAll(_:ids:)` and `fetchSet(_:ids:)`:\n\n```swift\ntry Player.find(db, id: 1)                   // Player\ntry Player.fetchOne(db, id: 1)               // Player?\ntry Country.fetchAll(db, ids: [\"FR\", \"US\"])  // [Countries]\n```\n\nAll record types can use `find(_:key:)`, `fetchOne(_:key:)`, `fetchAll(_:keys:)` and `fetchSet(_:keys:)` that apply conditions on primary and unique keys:\n\n```swift\ntry Player.find(db, key: 1)                  // Player\ntry Player.fetchOne(db, key: 1)              // Player?\ntry Country.fetchAll(db, keys: [\"FR\", \"US\"]) // [Country]\ntry Player.fetchOne(db, key: [\"email\": \"arthur@example.com\"])            // Player?\ntry Citizenship.fetchOne(db, key: [\"citizenId\": 1, \"countryCode\": \"FR\"]) // Citizenship?\n```\n\nWhen the table has no explicit primary key, GRDB uses the [hidden `rowid` column](https://www.sqlite.org/rowidtable.html):\n\n```swift\n// SELECT * FROM document WHERE rowid = 1\ntry Document.fetchOne(db, key: 1)            // Document?\n```\n\n**When you want to build a request and plan to fetch from it later**, use a `filter` method:\n\n```swift\nlet request = Player.filter(id: 1)\nlet request = Country.filter(ids: [\"FR\", \"US\"])\nlet request = Player.filter(key: [\"email\": \"arthur@example.com\"])\nlet request = Citizenship.filter(key: [\"citizenId\": 1, \"countryCode\": \"FR\"])\n```\n\n\n## Testing for Record Existence\n\n**You can check if a request has matching rows in the database.**\n\n```swift\n// Some request based on `Player`\nlet request = Player.filter { ... }...\n\n// Check for player existence:\nlet noSuchPlayer = try request.isEmpty(db) // Bool\n```\n\nYou should check for emptiness instead of counting:\n\n```swift\n// Correct\nlet noSuchPlayer = try request.fetchCount(db) == 0\n// Even better\nlet noSuchPlayer = try request.isEmpty(db)\n```\n\n**You can also check if a given primary or unique key exists in the database.**\n\n[Identifiable Records] can use the type-safe method `exists(_:id:)`:\n\n```swift\ntry Player.exists(db, id: 1)\ntry Country.exists(db, id: \"FR\")\n```\n\nAll record types can use `exists(_:key:)` that can check primary and unique keys:\n\n```swift\ntry Player.exists(db, key: 1)\ntry Country.exists(db, key: \"FR\")\ntry Player.exists(db, key: [\"email\": \"arthur@example.com\"])\ntry Citizenship.exists(db, key: [\"citizenId\": 1, \"countryCode\": \"FR\"])\n```\n\nYou should check for key existence instead of fetching a record and checking for nil:\n\n```swift\n// Correct\nlet playerExists = try Player.fetchOne(db, id: 1) != nil\n// Even better\nlet playerExists = try Player.exists(db, id: 1)\n```\n\n\n## Fetching Aggregated Values\n\n**Requests can count.** The `fetchCount()` method returns the number of rows that would be returned by a fetch request:\n\n```swift\n// SELECT COUNT(*) FROM player\nlet count = try Player.fetchCount(db) // Int\n\n// SELECT COUNT(*) FROM player WHERE email IS NOT NULL\nlet count = try Player.filter { $0.email != nil }.fetchCount(db)\n\n// SELECT COUNT(DISTINCT name) FROM player\nlet count = try Player.select(\\.name).distinct().fetchCount(db)\n\n// SELECT COUNT(*) FROM (SELECT DISTINCT name, score FROM player)\nlet count = try Player.select { [$0.name, $0.score] }.distinct().fetchCount(db)\n```\n\n\n**Other aggregated values** can also be selected and fetched (see [SQL Functions](#sql-functions)):\n\n```swift\nlet request = Player.select { max($0.score) }\nlet maxScore = try Int.fetchOne(db, request) // Int?\n\nlet request = Player.select { [min($0.score), max($0.score)] }\nlet row = try Row.fetchOne(db, request)!     // Row\nlet minScore = row[0] as Int?\nlet maxScore = row[1] as Int?\n```\n\n\n## Delete Requests\n\n**Requests can delete records**, with the `deleteAll()` method:\n\n```swift\n// DELETE FROM player\ntry Player.deleteAll(db)\n\n// DELETE FROM player WHERE team = 'Reds'\ntry Player\n    .filter { $0.team == \"Reds\" }\n    .deleteAll(db)\n\n// DELETE FROM player ORDER BY score LIMIT 10\ntry Player\n    .order(\\.score)\n    .limit(10)\n    .deleteAll(db)\n```\n\n> **Note** Deletion methods are available on types that adopt the [TableRecord] protocol, and `Table`:\n>\n> ```swift\n> struct Player: TableRecord { ... }\n> try Player.deleteAll(db)          // Fine\n> try Table(\"player\").deleteAll(db) // Just as fine\n> ```\n\n**Deleting records according to their primary key** is a common task.\n\n[Identifiable Records] can use the type-safe methods `deleteOne(_:id:)` and `deleteAll(_:ids:)`:\n\n```swift\ntry Player.deleteOne(db, id: 1)\ntry Country.deleteAll(db, ids: [\"FR\", \"US\"])\n```\n\nAll record types can use `deleteOne(_:key:)` and `deleteAll(_:keys:)` that apply conditions on primary and unique keys:\n\n```swift\ntry Player.deleteOne(db, key: 1)\ntry Country.deleteAll(db, keys: [\"FR\", \"US\"])\ntry Player.deleteOne(db, key: [\"email\": \"arthur@example.com\"])\ntry Citizenship.deleteOne(db, key: [\"citizenId\": 1, \"countryCode\": \"FR\"])\n```\n\nWhen the table has no explicit primary key, GRDB uses the [hidden `rowid` column](https://www.sqlite.org/rowidtable.html):\n\n```swift\n// DELETE FROM document WHERE rowid = 1\ntry Document.deleteOne(db, id: 1)             // Document?\n```\n\n\n## Update Requests\n\n**Requests can batch update records**. The `updateAll()` method accepts *column assignments* defined with the `set(to:)` method:\n\n```swift\n// UPDATE player SET score = 0, isHealthy = 1, bonus = NULL\ntry Player.updateAll(db) { [\n    $0.score.set(to: 0), \n    $0.isHealthy.set(to: true), \n    $0.bonus.set(to: nil),\n] }\n\n// UPDATE player SET score = 0 WHERE team = 'Reds'\ntry Player\n    .filter { $0.team == \"Reds\" }\n    .updateAll(db) { $0.score.set(to: 0) }\n\n// UPDATE player SET isGreat = 1 ORDER BY score DESC LIMIT 10\ntry Player\n    .order(\\.score.desc)\n    .limit(10)\n    .updateAll(db) { $0.isGreat.set(to: true) }\n\n// UPDATE country SET population = 67848156 WHERE id = 'FR'\ntry Country\n    .filter(id: \"FR\")\n    .updateAll(db) { $0.population.set(to: 67_848_156) }\n```\n\nColumn assignments accept any expression:\n\n```swift\n// UPDATE player SET score = score + (bonus * 2)\ntry Player.updateAll(db) {\n    $0.score.set(to: $0.score + $0.bonus * 2)\n}\n```\n\nAs a convenience, you can also use the `+=`, `-=`, `*=`, or `/=` operators:\n\n```swift\n// UPDATE player SET score = score + (bonus * 2)\ntry Player.updateAll(db) { $0.score += $0.bonus * 2 }\n```\n\nDefault [Conflict Resolution] rules apply, and you may also provide a specific one:\n\n```swift\n// UPDATE OR IGNORE player SET ...\ntry Player.updateAll(db, onConflict: .ignore) { /* assignments... */ }\n```\n\n> **Note** The `updateAll` method is available on types that adopt the [TableRecord] protocol, and `Table`:\n>\n> ```swift\n> struct Player: TableRecord { ... }\n> try Player.updateAll(db, ...)          // Fine\n> try Table(\"player\").updateAll(db, ...) // Just as fine\n> ```\n\n\n## Custom Requests\n\nUntil now, we have seen [requests](#requests) created from any type that adopts the [TableRecord] protocol:\n\n```swift\nlet request = Player.all()  // QueryInterfaceRequest<Player>\n```\n\nThose requests of type `QueryInterfaceRequest` can fetch and count:\n\n```swift\ntry request.fetchCursor(db) // A Cursor of Player\ntry request.fetchAll(db)    // [Player]\ntry request.fetchSet(db)    // Set<Player>\ntry request.fetchOne(db)    // Player?\ntry request.fetchCount(db)  // Int\n```\n\n**When the query interface can not generate the SQL you need**, you can still fallback to [raw SQL](#fetch-queries):\n\n```swift\n// Custom SQL is always welcome\ntry Player.fetchAll(db, sql: \"SELECT ...\")   // [Player]\n```\n\nBut you may prefer to bring some elegance back in, and build custom requests:\n\n```swift\n// No custom SQL in sight\ntry Player.customRequest().fetchAll(db) // [Player]\n```\n\n**To build custom requests**, you can use one of the built-in requests or derive requests from other requests.\n\n- [SQLRequest] is a fetch request built from raw SQL. For example:\n    \n    ```swift\n    extension Player {\n        static func filter(color: Color) -> SQLRequest<Player> {\n            SQLRequest<Player>(\n                sql: \"SELECT * FROM player WHERE color = ?\"\n                arguments: [color])\n        }\n    }\n    \n    // [Player]\n    try Player.filter(color: .red).fetchAll(db)\n    ```\n    \n    SQLRequest supports [SQL Interpolation]:\n    \n    ```swift\n    extension Player {\n        static func filter(color: Color) -> SQLRequest<Player> {\n            \"SELECT * FROM player WHERE color = \\(color)\"\n        }\n    }\n    ```\n    \n- The [`asRequest(of:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterfacerequest/asrequest(of:)) method changes the type fetched by the request. It is useful, for example, when you use [Associations]:\n\n    ```swift\n    struct BookInfo: FetchableRecord, Decodable {\n        var book: Book\n        var author: Author\n    }\n    \n    let request = Book\n        .including(required: Book.author)\n        .asRequest(of: BookInfo.self)\n    \n    // [BookInfo]\n    try request.fetchAll(db)\n    ```\n\n- The [`adapted(_:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/fetchrequest/adapted(_:)) method eases the consumption of complex rows with row adapters. See [`RowAdapter`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/rowadapter) and [`splittingRowAdapters(columnCounts:)`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/splittingrowadapters(columncounts:)) for a sample code that uses `adapted(_:)`.\n\n\nEncryption\n==========\n\n**GRDB can encrypt your database with [SQLCipher](http://sqlcipher.net) v3.4+.**\n\nUse [CocoaPods](http://cocoapods.org/), and specify in your `Podfile`:\n\n```ruby\n# GRDB with SQLCipher 4\npod 'GRDB.swift/SQLCipher'\npod 'SQLCipher', '~> 4.0'\n\n# GRDB with SQLCipher 3\npod 'GRDB.swift/SQLCipher'\npod 'SQLCipher', '~> 3.4'\n```\n\nMake sure you remove any existing `pod 'GRDB.swift'` from your Podfile. `GRDB.swift/SQLCipher` must be the only active GRDB pod in your whole project, or you will face linker or runtime errors, due to the conflicts between SQLCipher and the system SQLite.\n\n- [Creating or Opening an Encrypted Database](#creating-or-opening-an-encrypted-database)\n- [Changing the Passphrase of an Encrypted Database](#changing-the-passphrase-of-an-encrypted-database)\n- [Exporting a Database to an Encrypted Database](#exporting-a-database-to-an-encrypted-database)\n- [Security Considerations](#security-considerations)\n\n\n### Creating or Opening an Encrypted Database\n\n**You create and open an encrypted database** by providing a passphrase to your [database connection]:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    try db.usePassphrase(\"secret\")\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\nIt is also in `prepareDatabase` that you perform other [SQLCipher configuration steps](https://www.zetetic.net/sqlcipher/sqlcipher-api/) that must happen early in the lifetime of a SQLCipher connection. For example:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    try db.usePassphrase(\"secret\")\n    try db.execute(sql: \"PRAGMA cipher_page_size = ...\")\n    try db.execute(sql: \"PRAGMA kdf_iter = ...\")\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\nWhen you want to open an existing SQLCipher 3 database with SQLCipher 4, you may want to run the `cipher_compatibility` pragma:\n\n```swift\n// Open an SQLCipher 3 database with SQLCipher 4\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    try db.usePassphrase(\"secret\")\n    try db.execute(sql: \"PRAGMA cipher_compatibility = 3\")\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\nSee [SQLCipher 4.0.0 Release](https://www.zetetic.net/blog/2018/11/30/sqlcipher-400-release/) and [Upgrading to SQLCipher 4](https://discuss.zetetic.net/t/upgrading-to-sqlcipher-4/3283) for more information.\n\n\n### Changing the Passphrase of an Encrypted Database\n\n**You can change the passphrase** of an already encrypted database.\n\nWhen you use a [database queue](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasequeue), open the database with the old passphrase, and then apply the new passphrase:\n\n```swift\ntry dbQueue.write { db in\n    try db.changePassphrase(\"newSecret\")\n}\n```\n\nWhen you use a [database pool](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasepool), make sure that no concurrent read can happen by changing the passphrase within the `barrierWriteWithoutTransaction` block. You must also ensure all future reads open a new database connection by calling the `invalidateReadOnlyConnections` method:\n\n```swift\ntry dbPool.barrierWriteWithoutTransaction { db in\n    try db.changePassphrase(\"newSecret\")\n    dbPool.invalidateReadOnlyConnections()\n}\n```\n\n> **Note**: When an application wants to keep on using a database queue or pool after the passphrase has changed, it is responsible for providing the correct passphrase to the `usePassphrase` method called in the database preparation function. Consider:\n>\n> ```swift\n> // WRONG: this won't work across a passphrase change\n> let passphrase = try getPassphrase()\n> var config = Configuration()\n> config.prepareDatabase { db in\n>     try db.usePassphrase(passphrase)\n> }\n>\n> // CORRECT: get the latest passphrase when it is needed\n> var config = Configuration()\n> config.prepareDatabase { db in\n>     let passphrase = try getPassphrase()\n>     try db.usePassphrase(passphrase)\n> }\n> ```\n\n> **Note**: The `DatabasePool.barrierWriteWithoutTransaction` method does not prevent [database snapshots](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasesnapshot) from accessing the database during the passphrase change, or after the new passphrase has been applied to the database. Those database accesses may throw errors. Applications should provide their own mechanism for invalidating open snapshots before the passphrase is changed.\n\n> **Note**: Instead of changing the passphrase \"in place\" as described here, you can also export the database in a new encrypted database that uses the new passphrase. See [Exporting a Database to an Encrypted Database](#exporting-a-database-to-an-encrypted-database).\n\n\n### Exporting a Database to an Encrypted Database\n\nProviding a passphrase won't encrypt a clear-text database that already exists, though. SQLCipher can't do that, and you will get an error instead: `SQLite error 26: file is encrypted or is not a database`.\n\nInstead, create a new encrypted database, at a distinct location, and export the content of the existing database. This can both encrypt a clear-text database, or change the passphrase of an encrypted database.\n\nThe technique to do that is [documented](https://discuss.zetetic.net/t/how-to-encrypt-a-plaintext-sqlite-database-to-use-sqlcipher-and-avoid-file-is-encrypted-or-is-not-a-database-errors/868/1) by SQLCipher.\n\nWith GRDB, it gives:\n\n```swift\n// The existing database\nlet existingDBQueue = try DatabaseQueue(path: \"/path/to/existing.db\")\n\n// The new encrypted database, at some distinct location:\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    try db.usePassphrase(\"secret\")\n}\nlet newDBQueue = try DatabaseQueue(path: \"/path/to/new.db\", configuration: config)\n\ntry existingDBQueue.inDatabase { db in\n    try db.execute(\n        sql: \"\"\"\n            ATTACH DATABASE ? AS encrypted KEY ?;\n            SELECT sqlcipher_export('encrypted');\n            DETACH DATABASE encrypted;\n            \"\"\",\n        arguments: [newDBQueue.path, \"secret\"])\n}\n\n// Now the export is completed, and the existing database can be deleted.\n```\n\n\n### Security Considerations\n\n#### Managing the lifetime of the passphrase string\n\nIt is recommended to avoid keeping the passphrase in memory longer than necessary. To do this, make sure you load the passphrase from the `prepareDatabase` method:\n\n```swift\n// NOT RECOMMENDED: this keeps the passphrase in memory longer than necessary\nlet passphrase = try getPassphrase()\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    try db.usePassphrase(passphrase)\n}\n\n// RECOMMENDED: only load the passphrase when it is needed\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    let passphrase = try getPassphrase()\n    try db.usePassphrase(passphrase)\n}\n```\n\nThis technique helps manages the lifetime of the passphrase, although keep in mind that the content of a String may remain intact in memory long after the object has been released.\n\nFor even better control over the lifetime of the passphrase in memory, use a Data object which natively provides the `resetBytes` function.\n\n```swift\n// RECOMMENDED: only load the passphrase when it is needed and reset its content immediately after use\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    var passphraseData = try getPassphraseData() // Data\n    defer {\n        passphraseData.resetBytes(in: 0..<passphraseData.count)\n    }\n    try db.usePassphrase(passphraseData)\n}\n```\n\nSome demanding users will want to go further, and manage the lifetime of the raw passphrase bytes. See below.\n\n\n#### Managing the lifetime of the passphrase bytes\n\nGRDB offers convenience methods for providing the database passphrases as Swift strings: `usePassphrase(_:)` and `changePassphrase(_:)`. Those methods don't keep the passphrase String in memory longer than necessary. But they are as secure as the standard String type: the lifetime of actual passphrase bytes in memory is not under control.\n\nWhen you want to precisely manage the passphrase bytes, talk directly to SQLCipher, using its raw C functions.\n\nFor example:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    ... // Carefully load passphrase bytes\n    let code = sqlite3_key(db.sqliteConnection, /* passphrase bytes */)\n    ... // Carefully dispose passphrase bytes\n    guard code == SQLITE_OK else {\n        throw DatabaseError(\n            resultCode: ResultCode(rawValue: code), \n            message: db.lastErrorMessage)\n    }\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\n#### Passphrase availability vs. Database availability\n\nWhen the passphrase is securely stored in the system keychain, your application can protect it using the [`kSecAttrAccessible`](https://developer.apple.com/documentation/security/ksecattraccessible) attribute.\n\nSuch protection prevents GRDB from creating SQLite connections when the passphrase is not available:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    let passphrase = try loadPassphraseFromSystemKeychain()\n    try db.usePassphrase(passphrase)\n}\n\n// Success if and only if the passphrase is available\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\nFor the same reason, [database pools], which open SQLite connections on demand, may fail at any time as soon as the passphrase becomes unavailable:\n\n```swift\n// Success if and only if the passphrase is available\nlet dbPool = try DatabasePool(path: dbPath, configuration: config)\n\n// May fail if passphrase has turned unavailable\ntry dbPool.read { ... }\n\n// May trigger value observation failure if passphrase has turned unavailable\ntry dbPool.write { ... }\n```\n\nBecause DatabasePool maintains a pool of long-lived SQLite connections, some database accesses will use an existing connection, and succeed. And some other database accesses will fail, as soon as the pool wants to open a new connection. It is impossible to predict which accesses will succeed or fail.\n\nFor the same reason, a database queue, which also maintains a long-lived SQLite connection, will remain available even after the passphrase has turned unavailable.\n\nApplications are thus responsible for protecting database accesses when the passphrase is unavailable. To this end, they can use [Data Protection](https://developer.apple.com/documentation/uikit/protecting_the_user_s_privacy/encrypting_your_app_s_files). They can also destroy their instances of database queue or pool when the passphrase becomes unavailable.\n\n\n## Backup\n\n**You can backup (copy) a database into another.**\n\nBackups can for example help you copying an in-memory database to and from a database file when you implement NSDocument subclasses.\n\n```swift\nlet source: DatabaseQueue = ...      // or DatabasePool\nlet destination: DatabaseQueue = ... // or DatabasePool\ntry source.backup(to: destination)\n```\n\nThe `backup` method blocks the current thread until the destination database contains the same contents as the source database.\n\nWhen the source is a [database pool](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasepool), concurrent writes can happen during the backup. Those writes may, or may not, be reflected in the backup, but they won't trigger any error.\n\n`Database` has an analogous `backup` method.\n\n```swift\nlet source: DatabaseQueue = ...      // or DatabasePool\nlet destination: DatabaseQueue = ... // or DatabasePool\ntry source.write { sourceDb in\n    try destination.barrierWriteWithoutTransaction { destDb in\n        try sourceDb.backup(to: destDb)\n    }\n}\n```\n\nThis method allows for the choice of source and destination `Database` handles with which to backup the database.\n\n### Backup Progress Reporting\n\nThe `backup` methods take optional `pagesPerStep` and `progress` parameters. Together these parameters can be used to track a database backup in progress and abort an incomplete backup.\n\nWhen `pagesPerStep` is provided, the database backup is performed in _steps_. At each step, no more than `pagesPerStep` database pages are copied from the source to the destination. The backup proceeds one step at a time until all pages have been copied.\n\nWhen a `progress` callback is provided, `progress` is called after every backup step, including the last. Even if a non-default `pagesPerStep` is specified or the backup is otherwise completed in a single step, the `progress` callback will be called.\n\n```swift\ntry source.backup(\n    to: destination,\n    pagesPerStep: ...)\n    { backupProgress in\n       print(\"Database backup progress:\", backupProgress)\n    }\n```\n\n### Aborting an Incomplete Backup\n\nIf a call to `progress` throws when `backupProgress.isComplete == false`, the backup will be aborted and the error rethrown. However, if a call to `progress` throws when `backupProgress.isComplete == true`, the backup is unaffected and the error is silently ignored.\n\n> **Warning**: Passing non-default values of `pagesPerStep` or `progress` to the backup methods is an advanced API intended to provide additional capabilities to expert users. GRDB's backup API provides a faithful, low-level wrapper to the underlying SQLite online backup API. GRDB's documentation is not a comprehensive substitute for the official SQLite [documentation of their backup API](https://www.sqlite.org/c3ref/backup_finish.html).\n\n## Interrupt a Database\n\n**The `interrupt()` method** causes any pending database operation to abort and return at its earliest opportunity.\n\nIt can be called from any thread.\n\n```swift\ndbQueue.interrupt()\ndbPool.interrupt()\n```\n\nA call to `interrupt()` that occurs when there are no running SQL statements is a no-op and has no effect on SQL statements that are started after `interrupt()` returns.\n\nA database operation that is interrupted will throw a DatabaseError with code `SQLITE_INTERRUPT`. If the interrupted SQL operation is an INSERT, UPDATE, or DELETE that is inside an explicit transaction, then the entire transaction will be rolled back automatically. If the rolled back transaction was started by a transaction-wrapping method such as `DatabaseWriter.write` or `Database.inTransaction`, then all database accesses will throw a DatabaseError with code `SQLITE_ABORT` until the wrapping method returns.\n\nFor example:\n\n```swift\ntry dbQueue.write { db in\n    try Player(...).insert(db)     // throws SQLITE_INTERRUPT\n    try Player(...).insert(db)     // not executed\n}                                  // throws SQLITE_INTERRUPT\n\ntry dbQueue.write { db in\n    do {\n        try Player(...).insert(db) // throws SQLITE_INTERRUPT\n    } catch { }\n}                                  // throws SQLITE_ABORT\n\ntry dbQueue.write { db in\n    do {\n        try Player(...).insert(db) // throws SQLITE_INTERRUPT\n    } catch { }\n    try Player(...).insert(db)     // throws SQLITE_ABORT\n}                                  // throws SQLITE_ABORT\n```\n\nYou can catch both `SQLITE_INTERRUPT` and `SQLITE_ABORT` errors:\n\n```swift\ndo {\n    try dbPool.write { db in ... }\n} catch DatabaseError.SQLITE_INTERRUPT, DatabaseError.SQLITE_ABORT {\n    // Oops, the database was interrupted.\n}\n```\n\nFor more information, see [Interrupt A Long-Running Query](https://www.sqlite.org/c3ref/interrupt.html).\n\n\n## Avoiding SQL Injection\n\nSQL injection is a technique that lets an attacker nuke your database.\n\n> ![XKCD: Exploits of a Mom](https://imgs.xkcd.com/comics/exploits_of_a_mom.png)\n>\n> https://xkcd.com/327/\n\nHere is an example of code that is vulnerable to SQL injection:\n\n```swift\n// BAD BAD BAD\nlet id = 1\nlet name = textField.text\ntry dbQueue.write { db in\n    try db.execute(sql: \"UPDATE students SET name = '\\(name)' WHERE id = \\(id)\")\n}\n```\n\nIf the user enters a funny string like `Robert'; DROP TABLE students; --`, SQLite will see the following SQL, and drop your database table instead of updating a name as intended:\n\n```sql\nUPDATE students SET name = 'Robert';\nDROP TABLE students;\n--' WHERE id = 1\n```\n\nTo avoid those problems, **never embed raw values in your SQL queries**. The only correct technique is to provide [arguments](#executing-updates) to your raw SQL queries:\n\n```swift\nlet name = textField.text\ntry dbQueue.write { db in\n    // Good\n    try db.execute(\n        sql: \"UPDATE students SET name = ? WHERE id = ?\",\n        arguments: [name, id])\n    \n    // Just as good\n    try db.execute(\n        sql: \"UPDATE students SET name = :name WHERE id = :id\",\n        arguments: [\"name\": name, \"id\": id])\n}\n```\n\nWhen you use [records](#records) and the [query interface](#the-query-interface), GRDB always prevents SQL injection for you:\n\n```swift\nlet id = 1\nlet name = textField.text\ntry dbQueue.write { db in\n    if var student = try Student.fetchOne(db, id: id) {\n        student.name = name\n        try student.update(db)\n    }\n}\n```\n\n\n## Error Handling\n\nGRDB can throw [DatabaseError](#databaseerror), [RecordError], [RowDecodingError], or crash your program with a [fatal error](#fatal-errors).\n\nConsidering that a local database is not some JSON loaded from a remote server, GRDB focuses on **trusted databases**. Dealing with [untrusted databases](#how-to-deal-with-untrusted-inputs) requires extra care.\n\n- [DatabaseError](#databaseerror)\n- [RecordError]\n- [RowDecodingError]\n- [Fatal Errors](#fatal-errors)\n- [How to Deal with Untrusted Inputs](#how-to-deal-with-untrusted-inputs)\n- [Error Log](#error-log)\n\n\n### DatabaseError\n\nğŸ“– [`DatabaseError`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseerror)\n\n**DatabaseError** are thrown on SQLite errors:\n\n```swift\ndo {\n    try Pet(masterId: 1, name: \"Bobby\").insert(db)\n} catch let error as DatabaseError {\n    // The SQLite error code: 19 (SQLITE_CONSTRAINT)\n    error.resultCode\n    \n    // The extended error code: 787 (SQLITE_CONSTRAINT_FOREIGNKEY)\n    error.extendedResultCode\n    \n    // The eventual SQLite message: FOREIGN KEY constraint failed\n    error.message\n    \n    // The eventual erroneous SQL query\n    // \"INSERT INTO pet (masterId, name) VALUES (?, ?)\"\n    error.sql\n    \n    // The eventual SQL arguments\n    // [1, \"Bobby\"]\n    error.arguments\n    \n    // Full error description\n    // > SQLite error 19: FOREIGN KEY constraint failed -\n    // > while executing `INSERT INTO pet (masterId, name) VALUES (?, ?)`\n    error.description\n}\n```\n\nIf you want to see statement arguments in the error description, [make statement arguments public](https://swiftpackageindex.com/groue/GRDB.swift/configuration/publicstatementarguments).\n\n**SQLite uses [results codes](https://www.sqlite.org/rescode.html) to distinguish between various errors**.\n\nYou can catch a DatabaseError and match on result codes:\n\n```swift\ndo {\n    try ...\n} catch let error as DatabaseError {\n    switch error {\n    case DatabaseError.SQLITE_CONSTRAINT_FOREIGNKEY:\n        // foreign key constraint error\n    case DatabaseError.SQLITE_CONSTRAINT:\n        // any other constraint error\n    default:\n        // any other database error\n    }\n}\n```\n\nYou can also directly match errors on result codes:\n\n```swift\ndo {\n    try ...\n} catch DatabaseError.SQLITE_CONSTRAINT_FOREIGNKEY {\n    // foreign key constraint error\n} catch DatabaseError.SQLITE_CONSTRAINT {\n    // any other constraint error\n} catch {\n    // any other database error\n}\n```\n\nEach DatabaseError has two codes: an `extendedResultCode` (see [extended result code](https://www.sqlite.org/rescode.html#extended_result_code_list)), and a less precise `resultCode` (see [primary result code](https://www.sqlite.org/rescode.html#primary_result_code_list)). Extended result codes are refinements of primary result codes, as `SQLITE_CONSTRAINT_FOREIGNKEY` is to `SQLITE_CONSTRAINT`, for example.\n\n> **Warning**: SQLite has progressively introduced extended result codes across its versions. The [SQLite release notes](http://www.sqlite.org/changes.html) are unfortunately not quite clear about that: write your handling of extended result codes with care.\n\n\n### RecordError\n\nğŸ“– [`RecordError`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/recorderror)\n\n**RecordError** is thrown by the [PersistableRecord] protocol when the `update` method could not find any row to update:\n\n```swift\ndo {\n    try player.update(db)\n} catch let RecordError.recordNotFound(databaseTableName: table, key: key) {\n    print(\"Key \\(key) was not found in table \\(table).\")\n}\n```\n\n**RecordError** is also thrown by the [FetchableRecord] protocol when the `find` method does not find any record:\n\n```swift\ndo {\n    let player = try Player.find(db, id: 42)\n} catch let RecordError.recordNotFound(databaseTableName: table, key: key) {\n    print(\"Key \\(key) was not found in table \\(table).\")\n}\n```\n\n\n### RowDecodingError\n\nğŸ“– [`RowDecodingError`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/rowdecodingerror)\n\n**RowDecodingError** is thrown when the application can not decode a value from a database row. For example:\n\n```swift\nlet row = try Row.fetchOne(db, sql: \"SELECT NULL AS name\")!\n// RowDecodingError: could not decode String from database value NULL.\nlet name = try row.decode(String.self, forColumn: \"name\")\n```\n\n### Fatal Errors\n\n**Fatal errors notify that the program, or the database, has to be changed.**\n\nThey uncover programmer errors, false assumptions, and prevent misuses. Here are a few examples:\n\n- **The code asks for a non-optional value, when the database contains NULL:**\n    \n    ```swift\n    // fatal error: could not convert NULL to String.\n    let name: String = row[\"name\"]\n    ```\n    \n    Solution: fix the contents of the database, use [NOT NULL constraints](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/columndefinition/notnull(onconflict:)), or load an optional:\n    \n    ```swift\n    let name: String? = row[\"name\"]\n    ```\n\n- **Conversion from database value to Swift type fails:**\n    \n    ```swift\n    // fatal error: could not convert \"Momâ€™s birthday\" to Date.\n    let date: Date = row[\"date\"]\n    \n    // fatal error: could not convert \"\" to URL.\n    let url: URL = row[\"url\"]\n    ```\n    \n    Solution: fix the contents of the database, or use [DatabaseValue](#databasevalue) to handle all possible cases:\n    \n    ```swift\n    let dbValue: DatabaseValue = row[\"date\"]\n    if dbValue.isNull {\n        // Handle NULL\n    } else if let date = Date.fromDatabaseValue(dbValue) {\n        // Handle valid date\n    } else {\n        // Handle invalid date\n    }\n    ```\n\n- **The database can't guarantee that the code does what it says:**\n\n    ```swift\n    // fatal error: table player has no unique index on column email\n    try Player.deleteOne(db, key: [\"email\": \"arthur@example.com\"])\n    ```\n    \n    Solution: add a unique index to the player.email column, or use the `deleteAll` method to make it clear that you may delete more than one row:\n    \n    ```swift\n    try Player.filter { $0.email == \"arthur@example.com\" }.deleteAll(db)\n    ```\n\n- **Database connections are not reentrant:**\n    \n    ```swift\n    // fatal error: Database methods are not reentrant.\n    dbQueue.write { db in\n        dbQueue.write { db in\n            ...\n        }\n    }\n    ```\n    \n    Solution: avoid reentrancy, and instead pass a database connection along.\n\n\n### How to Deal with Untrusted Inputs\n\nLet's consider the code below:\n\n```swift\nlet sql = \"SELECT ...\"\n\n// Some untrusted arguments for the query\nlet arguments: [String: Any] = ...\nlet rows = try Row.fetchCursor(db, sql: sql, arguments: StatementArguments(arguments))\n\nwhile let row = try rows.next() {\n    // Some untrusted database value:\n    let date: Date? = row[0]\n}\n```\n\nIt has two opportunities to throw fatal errors:\n\n- **Untrusted arguments**: The dictionary may contain values that do not conform to the [DatabaseValueConvertible protocol](#values), or may miss keys required by the statement.\n- **Untrusted database content**: The row may contain a non-null value that can't be turned into a date.\n\nIn such a situation, you can still avoid fatal errors by exposing and handling each failure point, one level down in the GRDB API:\n\n```swift\n// Untrusted arguments\nif let arguments = StatementArguments(arguments) {\n    let statement = try db.makeStatement(sql: sql)\n    try statement.setArguments(arguments)\n    \n    var cursor = try Row.fetchCursor(statement)\n    while let row = try iterator.next() {\n        // Untrusted database content\n        let dbValue: DatabaseValue = row[0]\n        if dbValue.isNull {\n            // Handle NULL\n        if let date = Date.fromDatabaseValue(dbValue) {\n            // Handle valid date\n        } else {\n            // Handle invalid date\n        }\n    }\n}\n```\n\nSee [`Statement`] and [DatabaseValue](#databasevalue) for more information.\n\n\n### Error Log\n\n**SQLite can be configured to invoke a callback function containing an error code and a terse error message whenever anomalies occur.**\n\nThis global error callback must be configured early in the lifetime of your application:\n\n```swift\nDatabase.logError = { (resultCode, message) in\n    NSLog(\"%@\", \"SQLite error \\(resultCode): \\(message)\")\n}\n```\n\n> **Warning**: Database.logError must be set before any database connection is opened. This includes the connections that your application opens with GRDB, but also connections opened by other tools, such as third-party libraries. Setting it after a connection has been opened is an SQLite misuse, and has no effect.\n\nSee [The Error And Warning Log](https://sqlite.org/errlog.html) for more information.\n\n\n## Unicode\n\nSQLite lets you store unicode strings in the database.\n\nHowever, SQLite does not provide any unicode-aware string transformations or comparisons.\n\n\n### Unicode functions\n\nThe `UPPER` and `LOWER` built-in SQLite functions are not unicode-aware:\n\n```swift\n// \"JÃ©RÃ´ME\"\ntry String.fetchOne(db, sql: \"SELECT UPPER('JÃ©rÃ´me')\")\n```\n\nGRDB extends SQLite with [SQL functions](#custom-sql-functions-and-aggregates) that call the Swift built-in string functions `capitalized`, `lowercased`, `uppercased`, `localizedCapitalized`, `localizedLowercased` and `localizedUppercased`:\n\n```swift\n// \"JÃ‰RÃ”ME\"\nlet uppercased = DatabaseFunction.uppercase\ntry String.fetchOne(db, sql: \"SELECT \\(uppercased.name)('JÃ©rÃ´me')\")\n```\n\nThose unicode-aware string functions are also readily available in the [query interface](#sql-functions):\n\n```swift\nPlayer.select { $0.name.uppercased }\n```\n\n\n### String Comparison\n\nSQLite compares strings in many occasions: when you sort rows according to a string column, or when you use a comparison operator such as `=` and `<=`.\n\nThe comparison result comes from a *collating function*, or *collation*. SQLite comes with three built-in collations that do not support Unicode: [binary, nocase, and rtrim](https://www.sqlite.org/datatype3.html#collation).\n\nGRDB comes with five extra collations that leverage unicode-aware comparisons based on the standard Swift String comparison functions and operators:\n\n- `unicodeCompare` (uses the built-in `<=` and `==` Swift operators)\n- `caseInsensitiveCompare`\n- `localizedCaseInsensitiveCompare`\n- `localizedCompare`\n- `localizedStandardCompare`\n\nA collation can be applied to a table column. All comparisons involving this column will then automatically trigger the comparison function:\n    \n```swift\ntry db.create(table: \"player\") { t in\n    // Guarantees case-insensitive email unicity\n    t.column(\"email\", .text).unique().collate(.nocase)\n    \n    // Sort names in a localized case insensitive way\n    t.column(\"name\", .text).collate(.localizedCaseInsensitiveCompare)\n}\n\n// Players are sorted in a localized case insensitive way:\nlet players = try Player.order(\\.name).fetchAll(db)\n```\n\n> **Warning**: SQLite *requires* host applications to provide the definition of any collation other than binary, nocase and rtrim. When a database file has to be shared or migrated to another SQLite library of platform (such as the Android version of your application), make sure you provide a compatible collation.\n\nIf you can't or don't want to define the comparison behavior of a column (see warning above), you can still use an explicit collation in SQL requests and in the [query interface](#the-query-interface):\n\n```swift\nlet collation = DatabaseCollation.localizedCaseInsensitiveCompare\nlet players = try Player.fetchAll(db,\n    sql: \"SELECT * FROM player ORDER BY name COLLATE \\(collation.name))\")\nlet players = try Player.order { $0.name.collating(collation) }.fetchAll(db)\n```\n\n\n**You can also define your own collations**:\n\n```swift\nlet collation = DatabaseCollation(\"customCollation\") { (lhs, rhs) -> NSComparisonResult in\n    // return the comparison of lhs and rhs strings.\n}\n\n// Make the collation available to a database connection\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    db.add(collation: collation)\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\n\n\n## Memory Management\n\nBoth SQLite and GRDB use non-essential memory that help them perform better.\n\nYou can reclaim this memory with the `releaseMemory` method:\n\n```swift\n// Release as much memory as possible.\ndbQueue.releaseMemory()\ndbPool.releaseMemory()\n```\n\nThis method blocks the current thread until all current database accesses are completed, and the memory collected.\n\n> **Warning**: If `DatabasePool.releaseMemory()` is called while a long read is performed concurrently, then no other read access will be possible until this long read has completed, and the memory has been released. If this does not suit your application needs, look for the asynchronous options below:\n\nYou can release memory in an asynchronous way as well:\n\n```swift\n// On a DatabaseQueue\ndbQueue.asyncWriteWithoutTransaction { db in\n    db.releaseMemory()\n}\n\n// On a DatabasePool\ndbPool.releaseMemoryEventually()\n```\n\n`DatabasePool.releaseMemoryEventually()` does not block the current thread, and does not prevent concurrent database accesses. In exchange for this convenience, you don't know when memory has been freed.\n\n\n### Memory Management on iOS\n\n**The iOS operating system likes applications that do not consume much memory.**\n\n[Database queues] and [pools](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasepool) automatically free non-essential memory when the application receives a memory warning, and when the application enters background.\n\nYou can opt out of this automatic memory management:\n\n```swift\nvar config = Configuration()\nconfig.automaticMemoryManagement = false\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config) // or DatabasePool\n```\n\nFAQ\n===\n\n**[FAQ: Opening Connections](#faq-opening-connections)**\n\n- [How do I create a database in my application?](#how-do-i-create-a-database-in-my-application)\n- [How do I open a database stored as a resource of my application?](#how-do-i-open-a-database-stored-as-a-resource-of-my-application)\n- [How do I close a database connection?](#how-do-i-close-a-database-connection)\n\n**[FAQ: SQL](#faq-sql)**\n\n- [How do I print a request as SQL?](#how-do-i-print-a-request-as-sql)\n\n**[FAQ: General](#faq-general)**\n\n- [How do I monitor the duration of database statements execution?](#how-do-i-monitor-the-duration-of-database-statements-execution)\n- [What Are Experimental Features?](#what-are-experimental-features)\n- [Does GRDB support library evolution and ABI stability?](#does-grdb-support-library-evolution-and-abi-stability)\n\n**[FAQ: Associations](#faq-associations)**\n\n- [How do I filter records and only keep those that are associated to another record?](#how-do-i-filter-records-and-only-keep-those-that-are-associated-to-another-record)\n- [How do I filter records and only keep those that are NOT associated to another record?](#how-do-i-filter-records-and-only-keep-those-that-are-not-associated-to-another-record)\n- [How do I select only one column of an associated record?](#how-do-i-select-only-one-column-of-an-associated-record)\n\n**[FAQ: ValueObservation](#faq-valueobservation)**\n\n- [Why is ValueObservation not publishing value changes?](#why-is-valueobservation-not-publishing-value-changes)\n\n**[FAQ: Errors](#faq-errors)**\n\n- [Generic parameter 'T' could not be inferred](#generic-parameter-t-could-not-be-inferred)\n- [Mutation of captured var in concurrently-executing code](#mutation-of-captured-var-in-concurrently-executing-code)\n- [SQLite error 1 \"no such column\"](#sqlite-error-1-no-such-column)\n- [SQLite error 10 \"disk I/O error\", SQLite error 23 \"not authorized\"](#sqlite-error-10-disk-io-error-sqlite-error-23-not-authorized)\n- [SQLite error 21 \"wrong number of statement arguments\" with LIKE queries](#sqlite-error-21-wrong-number-of-statement-arguments-with-like-queries)\n\n\n## FAQ: Opening Connections\n\n- :arrow_up: [FAQ]\n- [How do I create a database in my application?](#how-do-i-create-a-database-in-my-application)\n- [How do I open a database stored as a resource of my application?](#how-do-i-open-a-database-stored-as-a-resource-of-my-application)\n- [How do I close a database connection?](#how-do-i-close-a-database-connection)\n\n### How do I create a database in my application?\n\nFirst choose a proper location for the database file. Document-based applications will let the user pick a location. Apps that use the database as a global storage will prefer the Application Support directory.\n\nThe sample code below creates or opens a database file inside its dedicated directory (a [recommended practice](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseconnections)). On the first run, a new empty database file is created. On subsequent runs, the database file already exists, so it just opens a connection:\n\n```swift\n// HOW TO create an empty database, or open an existing database file\n\n// Create the \"Application Support/MyDatabase\" directory\nlet fileManager = FileManager.default\nlet appSupportURL = try fileManager.url(\n    for: .applicationSupportDirectory, in: .userDomainMask,\n    appropriateFor: nil, create: true) \nlet directoryURL = appSupportURL.appendingPathComponent(\"MyDatabase\", isDirectory: true)\ntry fileManager.createDirectory(at: directoryURL, withIntermediateDirectories: true)\n\n// Open or create the database\nlet databaseURL = directoryURL.appendingPathComponent(\"db.sqlite\")\nlet dbQueue = try DatabaseQueue(path: databaseURL.path)\n```\n\n### How do I open a database stored as a resource of my application?\n\nOpen a read-only connection to your resource:\n\n```swift\n// HOW TO open a read-only connection to a database resource\n\n// Get the path to the database resource.\nif let dbPath = Bundle.main.path(forResource: \"db\", ofType: \"sqlite\") {\n    // If the resource exists, open a read-only connection.\n    // Writes are disallowed because resources can not be modified. \n    var config = Configuration()\n    config.readonly = true\n    let dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n} else {\n    // The database resource can not be found.\n    // Fix your setup, or report the problem to the user. \n}\n```\n\n### How do I close a database connection?\n\nDatabase connections are automatically closed when `DatabaseQueue` or `DatabasePool` instances are deinitialized.\n\nIf the correct execution of your program depends on precise database closing, perform an explicit call to [`close()`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasereader/close()). This method may fail and create zombie connections, so please check its detailed documentation.\n\n## FAQ: SQL\n\n- :arrow_up: [FAQ]\n- [How do I print a request as SQL?](#how-do-i-print-a-request-as-sql)\n\n### How do I print a request as SQL?\n\nWhen you want to debug a request that does not deliver the expected results, you may want to print the SQL that is actually executed.\n\nYou can compile the request into a prepared [`Statement`]:\n\n```swift\ntry dbQueue.read { db in\n    let request = Player.filter { $0.email == \"arthur@example.com\" }\n    let statement = try request.makePreparedRequest(db).statement\n    print(statement) // SELECT * FROM player WHERE email = ?\n    print(statement.arguments) // [\"arthur@example.com\"]\n}\n```\n\nAnother option is to setup a tracing function that prints out the executed SQL requests. For example, provide a tracing function when you connect to the database:\n\n```swift\n// Prints all SQL statements\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    db.trace { print($0) }\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n\ntry dbQueue.read { db in\n    // Prints \"SELECT * FROM player WHERE email = ?\"\n    let players = try Player.filter { $0.email == \"arthur@example.com\" }.fetchAll(db)\n}\n```\n\nIf you want to see statement arguments such as `'arthur@example.com'` in the logged statements, [make statement arguments public](https://swiftpackageindex.com/groue/GRDB.swift/configuration/publicstatementarguments).\n\n> **Note**: the generated SQL may change between GRDB releases, without notice: don't have your application rely on any specific SQL output.\n\n\n## FAQ: General\n\n- :arrow_up: [FAQ]\n- [How do I monitor the duration of database statements execution?](#how-do-i-monitor-the-duration-of-database-statements-execution)\n- [What Are Experimental Features?](#what-are-experimental-features)\n- [Does GRDB support library evolution and ABI stability?](#does-grdb-support-library-evolution-and-abi-stability)\n\n### How do I monitor the duration of database statements execution?\n\nUse the `trace(options:_:)` method, with the `.profile` option:\n\n```swift\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    db.trace(options: .profile) { event in\n        // Prints all SQL statements with their duration\n        print(event)\n        \n        // Access to detailed profiling information\n        if case let .profile(statement, duration) = event, duration > 0.5 {\n            print(\"Slow query: \\(statement.sql)\")\n        }\n    }\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n\ntry dbQueue.read { db in\n    let players = try Player.filter { $0.email == \"arthur@example.com\" }.fetchAll(db)\n    // Prints \"0.003s SELECT * FROM player WHERE email = ?\"\n}\n```\n\nIf you want to see statement arguments such as `'arthur@example.com'` in the logged statements, [make statement arguments public](https://swiftpackageindex.com/groue/GRDB.swift/configuration/publicstatementarguments).\n\n### What Are Experimental Features?\n\nSince GRDB 1.0, all backwards compatibility guarantees of [semantic versioning](http://semver.org) apply: no breaking change will happen until the next major version of the library.\n\nThere is an exception, though: *experimental features*, marked with the \"**:fire: EXPERIMENTAL**\" badge. Those are advanced features that are too young, or lack user feedback. They are not stabilized yet.\n\nThose experimental features are not protected by semantic versioning, and may break between two minor releases of the library. To help them becoming stable, [your feedback](https://github.com/groue/GRDB.swift/issues) is greatly appreciated.\n\n### Does GRDB support library evolution and ABI stability?\n\nNo, GRDB does not support library evolution and ABI stability. The only promise is API stability according to [semantic versioning](http://semver.org), with an exception for [experimental features](#what-are-experimental-features).\n\nYet, GRDB can be built with the \"Build Libraries for Distribution\" Xcode option (`BUILD_LIBRARY_FOR_DISTRIBUTION`), so that you can build binary frameworks at your convenience.\n\n## FAQ: Associations\n\n- :arrow_up: [FAQ]\n- [How do I filter records and only keep those that are associated to another record?](#how-do-i-filter-records-and-only-keep-those-that-are-associated-to-another-record)\n- [How do I filter records and only keep those that are NOT associated to another record?](#how-do-i-filter-records-and-only-keep-those-that-are-not-associated-to-another-record)\n- [How do I select only one column of an associated record?](#how-do-i-select-only-one-column-of-an-associated-record)\n\n### How do I filter records and only keep those that are associated to another record?\n\nLet's say you have two record types, `Book` and `Author`, and you want to only fetch books that have an author, and discard anonymous books.\n\nWe start by defining the association between books and authors:\n\n```swift\nstruct Book: TableRecord {\n    ...\n    static let author = belongsTo(Author.self)\n}\n\nstruct Author: TableRecord {\n    ...\n}\n```\n\nAnd then we can write our request and only fetch books that have an author, discarding anonymous ones:\n\n```swift\nlet books: [Book] = try dbQueue.read { db in\n    // SELECT book.* FROM book \n    // JOIN author ON author.id = book.authorID\n    let request = Book.joining(required: Book.author)\n    return try request.fetchAll(db)\n}\n```\n\nNote how this request does not use the `filter` method. Indeed, we don't have any condition to express on any column. Instead, we just need to \"require that a book can be joined to its author\".\n\nSee [How do I filter records and only keep those that are NOT associated to another record?](#how-do-i-filter-records-and-only-keep-those-that-are-not-associated-to-another-record) below for the opposite question.\n\n\n### How do I filter records and only keep those that are NOT associated to another record?\n\nLet's say you have two record types, `Book` and `Author`, and you want to only fetch anonymous books that do not have any author.\n\nWe start by defining the association between books and authors:\n\n```swift\nstruct Book: TableRecord {\n    ...\n    static let author = belongsTo(Author.self)\n}\n\nstruct Author: TableRecord {\n    ...\n}\n```\n\nAnd then we can write our request and only fetch anonymous books that don't have any author:\n\n```swift\nlet books: [Book] = try dbQueue.read { db in\n    // SELECT book.* FROM book\n    // LEFT JOIN author ON author.id = book.authorID\n    // WHERE author.id IS NULL\n    let authorAlias = TableAlias<Author>()\n    let request = Book\n        .joining(optional: Book.author.aliased(authorAlias))\n        .filter(!authorAlias.exists)\n    return try request.fetchAll(db)\n}\n```\n\nThis request uses a TableAlias in order to be able to filter on the eventual associated author. We make sure that the `Author.primaryKey` is nil, which is another way to say it does not exist: the book has no author.\n\nSee [How do I filter records and only keep those that are associated to another record?](#how-do-i-filter-records-and-only-keep-those-that-are-associated-to-another-record) above for the opposite question.\n\n\n### How do I select only one column of an associated record?\n\nLet's say you have two record types, `Book` and `Author`, and you want to fetch all books with their author name, but not the full associated author records.\n\nWe start by defining the association between books and authors:\n\n```swift\nstruct Book: Decodable, TableRecord {\n    ...\n    static let author = belongsTo(Author.self)\n}\n\nstruct Author: Decodable, TableRecord {\n    ...\n    enum Columns {\n        static let name = Column(CodingKeys.name)\n    }\n}\n```\n\nAnd then we can write our request and the ad-hoc record that decodes it:\n\n```swift\nstruct BookInfo: Decodable, FetchableRecord {\n    var book: Book\n    var authorName: String? // nil when the book is anonymous\n    \n    static func all() -> QueryInterfaceRequest<BookInfo> {\n        // SELECT book.*, author.name AS authorName\n        // FROM book\n        // LEFT JOIN author ON author.id = book.authorID\n        return Book\n            .annotated(withOptional: Book.author.select { \n                $0.name.forKey(CodingKeys.authorName)\n            })\n            .asRequest(of: BookInfo.self)\n    }\n}\n\nlet bookInfos: [BookInfo] = try dbQueue.read { db in\n    BookInfo.all().fetchAll(db)\n}\n```\n\nBy defining the request as a static method of BookInfo, you have access to the private `CodingKeys.authorName`, and a compiler-checked SQL column name.\n\nBy using the `annotated(withOptional:)` method, you append the author name to the top-level selection that can be decoded by the ad-hoc record.\n\nBy using `asRequest(of:)`, you enhance the type-safety of your request.\n\n\n## FAQ: ValueObservation\n\n- :arrow_up: [FAQ]\n- [Why is ValueObservation not publishing value changes?](#why-is-valueobservation-not-publishing-value-changes)\n\n### Why is ValueObservation not publishing value changes?\n\nSometimes it looks that a [ValueObservation] does not notify the changes you expect.\n\nThere may be four possible reasons for this:\n\n1. The expected changes were not committed into the database.\n2. The expected changes were committed into the database, but were quickly overwritten.\n3. The observation was stopped.\n4. The observation does not track the expected database region.\n\nTo answer the first two questions, look at SQL statements executed by the database. This is done when you open the database connection:\n\n```swift\n// Prints all SQL statements\nvar config = Configuration()\nconfig.prepareDatabase { db in\n    db.trace { print(\"SQL: \\($0)\") }\n}\nlet dbQueue = try DatabaseQueue(path: dbPath, configuration: config)\n```\n\nIf, after that, you are convinced that the expected changes were committed into the database, and not overwritten soon after, trace observation events:\n\n```swift\nlet observation = ValueObservation\n    .tracking { db in ... }\n    .print() // <- trace observation events\nlet cancellable = observation.start(...)\n```\n\nLook at the observation logs which start with `cancel` or `failure`: maybe the observation was cancelled by your app, or did fail with an error.\n\nLook at the observation logs which start with `value`: make sure, again, that the expected value was not actually notified, then overwritten.\n\nFinally, look at the observation logs which start with `tracked region`. Does the printed database region cover the expected changes?\n\nFor example:\n\n- `empty`: The empty region, which tracks nothing and never triggers the observation.\n- `player(*)`: The full `player` table\n- `player(id,name)`: The `id` and `name` columns of the `player` table\n- `player(id,name)[1]`: The `id` and `name` columns of the row with id 1 in the `player` table\n- `player(*),team(*)`: Both the full `player` and `team` tables\n\nIf you happen to use the `ValueObservation.trackingConstantRegion(_:)` method and see a mismatch between the tracked region and your expectation, then change the definition of your observation by using `tracking(_:)`. You should witness that the logs which start with `tracked region` now evolve in order to include the expected changes, and that you get the expected notifications.\n\nIf after all those steps (thanks you!), your observation is still failing you, please [open an issue](https://github.com/groue/GRDB.swift/issues/new) and provide a [minimal reproducible example](https://stackoverflow.com/help/minimal-reproducible-example)!\n\n\n## FAQ: Errors\n\n- :arrow_up: [FAQ]\n- [Generic parameter 'T' could not be inferred](#generic-parameter-t-could-not-be-inferred)\n- [Mutation of captured var in concurrently-executing code](#mutation-of-captured-var-in-concurrently-executing-code)\n- [SQLite error 1 \"no such column\"](#sqlite-error-1-no-such-column)\n- [SQLite error 10 \"disk I/O error\", SQLite error 23 \"not authorized\"](#sqlite-error-10-disk-io-error-sqlite-error-23-not-authorized)\n- [SQLite error 21 \"wrong number of statement arguments\" with LIKE queries](#sqlite-error-21-wrong-number-of-statement-arguments-with-like-queries)\n\n### Generic parameter 'T' could not be inferred\n    \nYou may get this error when using the `read` and `write` methods of database queues and pools:\n\n```swift\n// Generic parameter 'T' could not be inferred\nlet string = try dbQueue.read { db in\n    let result = try String.fetchOne(db, ...)\n    return result\n}\n```\n\nThis is a limitation of the Swift compiler.\n\nThe general workaround is to explicitly declare the type of the closure result:\n\n```swift\n// General Workaround\nlet string = try dbQueue.read { db -> String? in\n    let result = try String.fetchOne(db, ...)\n    return result\n}\n```\n\nYou can also, when possible, write a single-line closure:\n\n```swift\n// Single-line closure workaround:\nlet string = try dbQueue.read { db in\n    try String.fetchOne(db, ...)\n}\n```\n\n\n### Mutation of captured var in concurrently-executing code\n\nThe `insert` and `save` [persistence methods](#persistablerecord-protocol) can trigger a compiler error in async contexts:\n\n```swift\nvar player = Player(id: nil, name: \"Arthur\")\ntry await dbWriter.write { db in\n    // Error: Mutation of captured var 'player' in concurrently-executing code\n    try player.insert(db)\n}\nprint(player.id) // A non-nil id\n```\n\nWhen this happens, prefer the `inserted` and `saved` methods instead:\n\n```swift\n// OK\nvar player = Player(id: nil, name: \"Arthur\")\nplayer = try await dbWriter.write { [player] db in\n    return try player.inserted(db)\n}\nprint(player.id) // A non-nil id\n```\n\n\n### SQLite error 1 \"no such column\"\n\nThis error message is self-explanatory: do check for misspelled or non-existing column names.\n\nHowever, sometimes this error only happens when an app runs on a recent operating system (iOS 14+, Big Sur+, etc.) The error does not happen with previous ones.\n\nWhen this is the case, there are two possible explanations:\n\n1. Maybe a column name is *really* misspelled or missing from the database schema.\n    \n    To find it, check the SQL statement that comes with the [DatabaseError](#databaseerror).\n\n2. Maybe the application is using the character `\"` instead of the single quote `'` as the delimiter for string literals in raw SQL queries. Recent versions of SQLite have learned to tell about this deviation from the SQL standard, and this is why you are seeing this error. \n    \n    For example: this is not standard SQL: `UPDATE player SET name = \"Arthur\"`.\n    \n    The standard version is: `UPDATE player SET name = 'Arthur'`.\n    \n    It just happens that old versions of SQLite used to accept the former, non-standard version. Newer versions are able to reject it with an error.\n    \n    The fix is to change the SQL statements run by the application: replace `\"` with `'` in your string literals.\n    \n    It may also be time to learn about statement arguments and [SQL injection](#avoiding-sql-injection):\n    \n    ```swift\n    let name: String = ...\n    \n    // NOT STANDARD (double quote)\n    try db.execute(sql: \"\"\"\n        UPDATE player SET name = \"\\(name)\"\n        \"\"\")\n    \n    // STANDARD, BUT STILL NOT RECOMMENDED (single quote)\n    try db.execute(sql: \"UPDATE player SET name = '\\(name)'\")\n    \n    // STANDARD, AND RECOMMENDED (statement arguments)\n    try db.execute(sql: \"UPDATE player SET name = ?\", arguments: [name])\n    \n    // STANDARD, AND RECOMMENDED (SQL interpolation)\n    try db.execute(literal: \"UPDATE player SET name = \\(name)\")\n    ```\n    \nFor more information, see [Double-quoted String Literals Are Accepted](https://sqlite.org/quirks.html#double_quoted_string_literals_are_accepted), and [Configuration.acceptsDoubleQuotedStringLiterals](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/configuration/acceptsdoublequotedstringliterals).\n    \n\n\n### SQLite error 10 \"disk I/O error\", SQLite error 23 \"not authorized\"\n\nThose errors may be the sign that SQLite can't access the database due to [data protection](https://developer.apple.com/documentation/uikit/protecting_the_user_s_privacy/encrypting_your_app_s_files).\n\nWhen your application should be able to run in the background on a locked device, it has to catch this error, and, for example, wait for [UIApplicationDelegate.applicationProtectedDataDidBecomeAvailable(_:)](https://developer.apple.com/reference/uikit/uiapplicationdelegate/1623044-applicationprotecteddatadidbecom) or [UIApplicationProtectedDataDidBecomeAvailable](https://developer.apple.com/reference/uikit/uiapplicationprotecteddatadidbecomeavailable) notification and retry the failed database operation.\n\n```swift\ndo {\n    try ...\n} catch DatabaseError.SQLITE_IOERR, DatabaseError.SQLITE_AUTH {\n    // Handle possible data protection error\n}\n```\n\nThis error can also be prevented altogether by using a more relaxed [file protection](https://developer.apple.com/reference/foundation/filemanager/1653059-file_protection_values).\n\n\n### SQLite error 21 \"wrong number of statement arguments\" with LIKE queries\n\nYou may get the error \"wrong number of statement arguments\" when executing a LIKE query similar to:\n\n```swift\nlet name = textField.text\nlet players = try dbQueue.read { db in\n    try Player.fetchAll(db, sql: \"SELECT * FROM player WHERE name LIKE '%?%'\", arguments: [name])\n}\n```\n\nThe problem lies in the `'%?%'` pattern.\n\nSQLite only interprets `?` as a parameter when it is a placeholder for a whole value (int, double, string, blob, null). In this incorrect query, `?` is just a character in the `'%?%'` string: it is not a query parameter, and is not processed in any way. See [https://www.sqlite.org/lang_expr.html#varparam](https://www.sqlite.org/lang_expr.html#varparam) for more information about SQLite parameters.\n\nTo fix the error, you can feed the request with the pattern itself, instead of the name:\n\n```swift\nlet name = textField.text\nlet players: [Player] = try dbQueue.read { db in\n    let pattern = \"%\\(name)%\"\n    return try Player.fetchAll(db, sql: \"SELECT * FROM player WHERE name LIKE ?\", arguments: [pattern])\n}\n```\n\n\nSample Code\n===========\n\n- The [Documentation](#documentation) is full of GRDB snippets.\n- [Demo Applications]\n- Open `GRDB.xcworkspace`: it contains GRDB-enabled playgrounds to play with.\n- [groue/SortedDifference](https://github.com/groue/SortedDifference): How to synchronize a database table with a JSON payload\n\n\n---\n\n**Thanks**\n\n- [Pierlis](http://pierlis.com), where we write great software.\n- [@alextrob](https://github.com/alextrob), [@alexwlchan](https://github.com/alexwlchan), [@bellebethcooper](https://github.com/bellebethcooper), [@bfad](https://github.com/bfad), [@cfilipov](https://github.com/cfilipov), [@charlesmchen-signal](https://github.com/charlesmchen-signal), [@Chiliec](https://github.com/Chiliec), [@chrisballinger](https://github.com/chrisballinger), [@darrenclark](https://github.com/darrenclark), [@davidkraus](https://github.com/davidkraus), [@eburns-vmware](https://github.com/eburns-vmware), [@felixscheinost](https://github.com/felixscheinost), [@fpillet](https://github.com/fpillet), [@gcox](https://github.com/gcox), [@GetToSet](https://github.com/GetToSet), [@gjeck](https://github.com/gjeck), [@guidedways](https://github.com/guidedways), [@gusrota](https://github.com/gusrota), [@haikusw](https://github.com/haikusw), [@hartbit](https://github.com/hartbit), [@holsety](https://github.com/holsety), [@jroselightricks](https://github.com/jroselightricks), [@kdubb](https://github.com/kdubb), [@kluufger](https://github.com/kluufger), [@KyleLeneau](https://github.com/KyleLeneau), [@layoutSubviews](https://github.com/layoutSubviews), [@mallman](https://github.com/mallman), [@MartinP7r](https://github.com/MartinP7r), [@Marus](https://github.com/Marus), [@mattgallagher](https://github.com/mattgallagher), [@MaxDesiatov](https://github.com/MaxDesiatov), [@michaelkirk-signal](https://github.com/michaelkirk-signal), [@mtancock](https://github.com/mtancock), [@pakko972](https://github.com/pakko972), [@peter-ss](https://github.com/peter-ss), [@pierlo](https://github.com/pierlo), [@pocketpixels](https://github.com/pocketpixels), [@pp5x](https://github.com/pp5x), [@professordeng](https://github.com/professordeng), [@robcas3](https://github.com/robcas3), [@runhum](https://github.com/runhum), [@sberrevoets](https://github.com/sberrevoets), [@schveiguy](https://github.com/schveiguy), [@SD10](https://github.com/SD10), [@sobri909](https://github.com/sobri909), [@sroddy](https://github.com/sroddy), [@steipete](https://github.com/steipete), [@swiftlyfalling](https://github.com/swiftlyfalling), [@Timac](https://github.com/Timac), [@tternes](https://github.com/tternes), [@valexa](https://github.com/valexa), [@wuyuehyang](https://github.com/wuyuehyang), [@ZevEisenberg](https://github.com/ZevEisenberg), and [@zmeyc](https://github.com/zmeyc) for their contributions, help, and feedback on GRDB.\n- [@aymerick](https://github.com/aymerick) and [@kali](https://github.com/kali) because SQL.\n- [ccgus/fmdb](https://github.com/ccgus/fmdb) for its excellency.\n\n---\n\n[URIs don't change: people change them.](https://www.w3.org/Provider/Style/URI)\n\n#### Adding support for missing SQL functions or operators\n\nThis chapter was renamed to [Embedding SQL in Query Interface Requests].\n\n#### Advanced DatabasePool\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### After Commit Hook\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/database/afternexttransaction(oncommit:onrollback:)).\n\n#### Asynchronous APIs\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### Changes Tracking\n\nThis chapter has been renamed [Record Comparison].\n\n#### Concurrency\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### Custom Value Types\n\nCustom Value Types conform to the [`DatabaseValueConvertible`] protocol.\n\n#### Customized Decoding of Database Rows\n\nThis chapter has been renamed [Beyond FetchableRecord].\n\n#### Customizing the Persistence Methods\n\nThis chapter was replaced with [Persistence Callbacks].\n\n#### Database Changes Observation\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseobservation).\n\n#### Database Configuration\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/configuration).\n\n#### Database Queues\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasequeue).\n\n#### Database Pools\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasepool).\n\n#### Database Snapshots\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### DatabaseWriter and DatabaseReader Protocols\n\nThis chapter was removed. See the references of [DatabaseReader](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasereader) and [DatabaseWriter](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasewriter).\n\n#### Date and UUID Coding Strategies\n\nThis chapter has been renamed [Data, Date, and UUID Coding Strategies].\n\n#### Dealing with External Connections\n\nThis chapter has been superseded by the [Sharing a Database] guide.\n\n#### Differences between Database Queues and Pools\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### Enabling FTS5 Support\n\nFTS5 is enabled by default since GRDB 6.7.0.\n\n#### FetchedRecordsController\n\nFetchedRecordsController has been removed in GRDB 5.\n\nThe [Database Observation] chapter describes the other ways to observe the database.\n\n#### Full-Text Search\n\nThis chapter has [moved](Documentation/FullTextSearch.md).\n\n#### Guarantees and Rules\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### Joined Queries Support\n\nThis chapter was replaced with the documentation of [splittingRowAdapters(columnCounts:)](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/splittingrowadapters(columncounts:)).\n\n#### List of Record Methods\n\nSee [Records and the Query Interface](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/queryinterface).\n\n#### Migrations\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/migrations).\n\n#### NSNumber and NSDecimalNumber\n\nThis chapter has [moved](#nsnumber-nsdecimalnumber-and-decimal).\n\n#### Persistable Protocol\n\nThis protocol has been renamed [PersistableRecord] in GRDB 3.0.\n\n#### PersistenceError\n\nThis error was renamed to [RecordError].\n\n#### Prepared Statements\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statement).\n\n#### Record Class\n\nThe [`Record`](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/record) class is a legacy GRDB type. Since GRDB 7, it is not recommended to define record types by subclassing the `Record` class.\n\n#### Row Adapters\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/rowadapter).\n\n#### RowConvertible Protocol\n\nThis protocol has been renamed [FetchableRecord] in GRDB 3.0.\n\n#### TableMapping Protocol\n\nThis protocol has been renamed [TableRecord] in GRDB 3.0.\n\n#### Transactions and Savepoints\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/transactions).\n\n#### Transaction Hook\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/database/afternexttransaction(oncommit:onrollback:)).\n\n#### TransactionObserver Protocol\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/transactionobserver).\n\n#### Unsafe Concurrency APIs\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency).\n\n#### ValueObservation\n\nThis chapter has [moved](https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/valueobservation).\n\n#### ValueObservation and DatabaseRegionObservation\n\nThis chapter has been superseded by [ValueObservation] and [DatabaseRegionObservation].\n\n[Associations]: Documentation/AssociationsBasics.md\n[Beyond FetchableRecord]: #beyond-fetchablerecord\n[Identifiable Records]: #identifiable-records\n[Codable Records]: #codable-records\n[Columns Selected by a Request]: #columns-selected-by-a-request\n[common table expression]: Documentation/CommonTableExpressions.md\n[Common Table Expressions]: Documentation/CommonTableExpressions.md\n[Conflict Resolution]: #conflict-resolution\n[Column Names Coding Strategies]: #column-names-coding-strategies\n[Data, Date, and UUID Coding Strategies]: #data-date-and-uuid-coding-strategies\n[Fetching from Requests]: #fetching-from-requests\n[Embedding SQL in Query Interface Requests]: #embedding-sql-in-query-interface-requests\n[Full-Text Search]: Documentation/FullTextSearch.md\n[Migrations]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/migrations\n[The userInfo Dictionary]: #the-userinfo-dictionary\n[JSON Columns]: #json-columns\n[FetchableRecord]: #fetchablerecord-protocol\n[EncodableRecord]: #persistablerecord-protocol\n[PersistableRecord]: #persistablerecord-protocol\n[Record Comparison]: #record-comparison\n[Record Customization Options]: #record-customization-options\n[Persistence Callbacks]: #persistence-callbacks\n[persistence callbacks]: #persistence-callbacks\n[Record Timestamps and Transaction Date]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/recordtimestamps\n[TableRecord]: #tablerecord-protocol\n[ValueObservation]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/valueobservation\n[DatabaseRegionObservation]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseregionobservation\n[RxGRDB]: https://github.com/RxSwiftCommunity/RxGRDB\n[DatabaseRegion]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseregion\n[SQL Interpolation]: Documentation/SQLInterpolation.md\n[custom SQLite build]: Documentation/CustomSQLiteBuilds.md\n[Combine]: https://developer.apple.com/documentation/combine\n[Combine Support]: Documentation/Combine.md\n[Concurrency]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/concurrency\n[Demo Applications]: Documentation/DemoApps\n[Sharing a Database]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasesharing\n[FAQ]: #faq\n[Database Observation]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databaseobservation\n[SQLRequest]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/sqlrequest\n[SQL literal]: Documentation/SQLInterpolation.md#sql-literal\n[Identifiable]: https://developer.apple.com/documentation/swift/identifiable\n[Query Interface Organization]: Documentation/QueryInterfaceOrganization.md\n[Database Configuration]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/configuration\n[Persistence Methods]: #persistence-methods\n[persistence methods]: #persistence-methods\n[Persistence Methods and the `RETURNING` clause]: #persistence-methods-and-the-returning-clause\n[RecordError]: #recorderror\n[RowDecodingError]: #rowdecodingerror\n[Transactions and Savepoints]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/transactions\n[`DatabaseQueue`]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasequeue\n[Database queues]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasequeue\n[`DatabasePool`]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasepool\n[database pools]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasepool\n[`DatabaseValueConvertible`]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/databasevalueconvertible\n[`StatementArguments`]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statementarguments\n[Prepared Statements]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statement\n[prepared statements]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statement\n[`Statement`]: https://swiftpackageindex.com/groue/GRDB.swift/documentation/grdb/statement\n[Database Connections]: #database-connections\n[Database connections]: #database-connections\n[database connection]: #database-connections\n",
      "stars_today": 4
    },
    {
      "id": 13405986,
      "name": "UEFITool",
      "full_name": "LongSoft/UEFITool",
      "description": "UEFI firmware image viewer and editor",
      "html_url": "https://github.com/LongSoft/UEFITool",
      "stars": 5279,
      "forks": 715,
      "language": "C",
      "topics": [],
      "created_at": "2013-10-08T07:05:51Z",
      "updated_at": "2026-01-27T19:17:11Z",
      "pushed_at": "2026-01-26T04:40:08Z",
      "open_issues": 22,
      "owner": {
        "login": "LongSoft",
        "avatar_url": "https://avatars.githubusercontent.com/u/7540921?v=4"
      },
      "readme": "# UEFITool\n\nUEFITool is a viewer and editor of firmware images conforming to UEFI Platform Interface (PI) Specifications.\n\n![UEFITool icon](https://raw.githubusercontent.com/LongSoft/UEFITool/new_engine/UEFITool/icons/uefitool_64x64.png \"UEFITool icon\")  \n![CI Status](https://github.com/LongSoft/UEFITool/actions/workflows/main.yml/badge.svg?branch=new_engine) [![Scan Status](https://scan.coverity.com/projects/17209/badge.svg?flat=1)](https://scan.coverity.com/projects/17209) [![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=LongSoft_UEFITool&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=LongSoft_UEFITool)\n\n\n## Very Brief Introduction to UEFI\n\nUnified Extensible Firmware Interface or UEFI is a post-BIOS firmware specification originally written by Intel for Itanium architecture and than adapted for X86 systems.  \nThe first EFI-compatible x86 firmwares were used on Apple Macintosh systems in 2006 and PC motherboard vendors started putting UEFI-compatible firmwares on their boards in 2011.  \nIn 2015 there are numerous systems using UEFI-compatible firmware including PCs, Macs, Tablets and Smartphones on x86, x86-64 and ARM architectures.  \nMore information on UEFI is available on [UEFI Forum official site](http://www.uefi.org/faq) and in [Wikipedia](http://en.wikipedia.org/wiki/Unified_Extensible_Firmware_Interface).  \n  \n## Very Brief Introduction to UEFITool\n\nUEFITool is a cross-platform open source application written in C++/Qt, that parses UEFI-compatible firmware image into a tree structure, verifies image's integrity and provides a GUI to manipulate image's elements.  \nProject development started in the middle of 2013 because of the lack of cross-platform open source utilities for tinkering with UEFI images.  \n\nIn the beginning of 2015 the major refactoring round was started to make the program compatible with newer UEFI features including FFSv3 volumes and fixed image elements. \nIt's in development right now with the following features still missing:\n* Editor part, i.e image reconstruction routines\n* Console UI\n\nThe missing parts are in development and the version with a new engine will be made as soon as image reconstruction works again.\n\n## Derived projects\n\nThere are some other projects that use UEFITool's engine:\n* UEFIExtract, which uses ffsParser to parse supplied firmware image into a tree structure and dumps the parsed structure recursively on the FS. Jethro Beekman's [tree](https://github.com/jethrogb/uefireverse) utility can be used to work with the extracted tree.\n* UEFIFind, which uses ffsParser to find image elements containing a specified pattern. It was developed for [UBU](https://winraid.level1techs.com/t/tool-guide-news-uefi-bios-updater-ubu/30357) project.\n\n## Alternatives\n\nRight now there are some alternatives to UEFITool that you could find useful too:\n* **[FMMT](https://github.com/tianocore/edk2/tree/master/BaseTools/Source/Python/FMMT)** by TianoCore. Python-based open source toolset for modifying EDK2-based UEFI firmware images. Does not support any IBV customizations, but is _official_, and lives in EDK2 repository.\n* **[Fiano](https://github.com/linuxboot/fiano)** by Google and Facebook. Go-based cross-platform open source toolset for modifying UEFI firmware images.\n* **[PhoenixTool](https://forums.mydigitallife.net/threads/tool-to-insert-replace-slic-in-phoenix-insyde-dell-efi-bioses.13194)** by [AndyP](https://forums.mydigitallife.net/members/andyp.39295). Windows-only freeware GUI application written in C#. Used mostly for SLIC-related modifications, but it not limited to this task. Requires Microsoft .NET 3.5 to work properly. Supports unpacking firmware images from various vendor-specific formats like encrypted HP update files and Dell installers.\n* **[uefi-firmware-parser](https://github.com/theopolis/uefi-firmware-parser)** by [Teddy Reed](https://github.com/theopolis). Cross-platform open source console application written in Python. Very tinker-friendly due to use of Python. Can be used in scripts to automate firmware patching.\n* **[Chipsec](https://github.com/chipsec/chipsec)** by Intel. Cross-platform partially open source console application written in Python and C. Can be used to test Intel-based platforms for various security-related misconfigurations, but also has NVRAM parser and other components aimed to firmware modification.\n\n## Installation\n\nYou can either use [pre-built binaries](https://github.com/LongSoft/UEFITool/releases) or build a binary yourself.  \n* To build a binary that uses Qt library (UEFITool) you need a C++ compiler and an instance of [Qt5 or Qt6](https://www.qt.io) library. Install both of them, get the sources, generate makefiles using qmake (`qmake ./UEFITool/uefitool.pro`) and use your system's make command on that generated files (i.e. `nmake release`, `make release` and so on). Qt6-based builds can also use CMAKE as an altearnative build system.\n* To build a binary that doesn't use Qt (UEFIExtract, UEFIFind), you need a C++ compiler and [CMAKE](https://cmake.org) utility to generate a makefile for your OS and build environment. Install both of them, get the sources, generate makefiles using cmake (`cmake UEFIExtract`) and use your system's make command on that generated files (i.e. `nmake release`, `make release` and so on). Non-Qt builds can also use Meson as an alternative build system.\n\n## Known issues\n\n* Image editing is currently only possible using an outdated and unsupported UEFITool 0.28 (`old_engine` branch) and the tools based on it (`UEFIReplace`, `UEFIPatch`). This is the top priority [issue #67](https://github.com/LongSoft/UEFITool/issues/67), which is being worked on, albeit slowly (due to the amount of coding and testing required to implement it correctly). \n* Some vendor-specific firmware update files can be opened incorrectly or can't be opened at all. This includes encrypted HP update files, Dell HDR and EXE files, some InsydeFlash FD files and so on. Enabling support for such files will require massive amount of reverse-engineering which is almost pointless because the updated image can be obtained from BIOS chip where it's already decrypted and unpacked.\n* Intel Firmware Interface Table (FIT) editing is not supported right now. FIT contains pointers to various image components that must be loaded before executing the first CPU instruction from the BIOS chip. Those components include CPU microcode updates, binaries and settings used by BIOS Guard and Boot Guard technologies and some other stuff. More information on FIT can be obtained [here](https://edc.intel.com/content/www/us/en/design/products-and-solutions/software-and-services/firmware-and-bios/firmware-interface-table/firmware-interface-table/).\n* Windows builds of `UEFIExtract` and `UEFIFind` might encouter an issue with folder paths being longer than 260 bytes (`MAX_PATH`) on some input files (see [issue #363](https://github.com/LongSoft/UEFITool/issues/363)). This is a [known Windows limitation](https://learn.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation?tabs=registry), that can be fixed by enabling long paths support via Windows Registry and adding a manifest to the executable file that requires such support. `UEFIExtract` has the required manifest additions since version `A67`, and the required registry file is provided by Microsoft on the page linked above, but this workaround is only awailable starting with Windows 10 build 1067.   \n\n## Bug repellents\n\n* [Coverity Scan](https://scan.coverity.com/projects/17209) - static analyzer for C, C++, C#, JavaScript, Ruby, or Python code.\n\n## GUID Database\n\nEvery new release includes an update to the database of known UEFI-related GUIDs build with help of [Linux Vendor Firmware Service](https://fwupd.org).\n\nYou can download the up-to-date version of that database using [this link](https://fwupd.org/lvfs/shards/export/csv).\n",
      "stars_today": 4
    },
    {
      "id": 36422720,
      "name": "trime",
      "full_name": "osfans/trime",
      "description": "åŒæ–‡å®‰å“è¼¸å…¥æ³•å¹³è‡º3.x/Android-rime/Rime Input Method Engine for Android",
      "html_url": "https://github.com/osfans/trime",
      "stars": 4012,
      "forks": 438,
      "language": "Kotlin",
      "topics": [
        "android",
        "chinese",
        "ime",
        "jni",
        "opencc",
        "rime"
      ],
      "created_at": "2015-05-28T07:43:37Z",
      "updated_at": "2026-01-28T01:49:24Z",
      "pushed_at": "2026-01-25T16:10:15Z",
      "open_issues": 100,
      "owner": {
        "login": "osfans",
        "avatar_url": "https://avatars.githubusercontent.com/u/7677088?v=4"
      },
      "readme": "<!--\nSPDX-FileCopyrightText: 2015 - 2024 Rime community\n\nSPDX-License-Identifier: GPL-3.0-or-later\n-->\n\n# Trime\n\nRime IME for Android\n\n![build](https://github.com/osfans/trime/actions/workflows/commit-ci.yml/badge.svg?branch=develop)\n[![License: GPL v3](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n[![GitHub release](https://img.shields.io/github/release/osfans/trime.svg)](https://github.com/osfans/trime/releases)\n[![F-Droid release](https://img.shields.io/f-droid/v/com.osfans.trime.svg)](https://f-droid.org/packages/com.osfans.trime)\n[![Latest build](https://img.shields.io/github/last-commit/osfans/trime.svg)](http://osfans.github.io/trime/)\n\nEnglish | [ç®€ä½“ä¸­æ–‡](README_sc.md) | [ç¹é«”ä¸­æ–‡](README_tc.md)\n\n## About\n\nTrime is originally a frontend of open-source [Android Traditional Chinese IME], based on [RIME] input method framework and written in Java/Kotlin with JNI. It is designed to protect the native language of various local dialects of Chinese and is a universal shape-based and phonetic-based input method platform.\n\n[Documentation](https://github.com/osfans/trime/wiki)\n\n## Download\n\n- Stable Channel <br>\n  [<img alt='Get it on F-Droid' src='https://fdroid.gitlab.io/artwork/badge/get-it-on.png' height='80px'/>](https://f-droid.org/packages/com.osfans.trime)\n  [<img alt='Google Play Download Now' src='https://play.google.com/intl/en_us/badges/images/generic/en_badge_web_generic.png' height='80px'/>](https://play.google.com/store/apps/details?id=com.osfans.trime)\n\n- Nightly Channel [Download](https://github.com/osfans/trime/releases/tag/nightly)\n\n- Canary Channel [Download](https://github.com/osfans/trime/actions)\n\n- Configurations [rimerc](https://github.com/Bambooin/rimerc)\n\n## History\n\nTRIME is the abbreviation of _Tongwen RIME_ or _ThaeRv Input Method_.\n\nFrom the beginning, TRIME was written for TaeRv Pinyin, and named _TaeRv Input Method (æ³°å¦‚è¾“å…¥æ³•)_.\n\nThen, we created an input method platform with some code tables, such as Wu dialect (å´è¯­). We renamed it to _Chinese Character Dialect Input Method (æ±‰å­—æ–¹è¨€è¾“å…¥æ³•)_.\n\nLater, it supports Wubi and Liangbi and other shape-based input method, we branded it [_Tongwen Input Method Platform 2.0 (åŒæ–‡è¾“å…¥æ³•å¹³å° 2.0)_](https://github.com/osfans/trime-legacy), which implies that the phonetic-based and shape-based input method on one platform, while dialects and Mandrain share one kind of characters.\n\nBenefit from the [librime](https://github.com/rime/librime) project by JNI, we are now in version 3.0 of TRIME aka _Tongwen Input Method (åŒæ–‡è¾“å…¥æ³•)_.\n\nYour are now welcome to [contribution](CONTRIBUTING.md) ~ !\n\n## Getting Started for developer\n\n### Prepare\n\n#### Requirements:\n\n- Android SDK and Android NDK\n  * If you are new to Android development, please install [Android Studio](https://developer.android.com/studio).\n\n- JDK (OpenJDK) 17\n- Python 3 (required by OpenCC to generate dictionary text files)\n\n#### Prerequisites for Windows\n\nSymbolic links will be created according to current build configurations, developers need:\n\n- Enable [Developer Mode](https://learn.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development) so that symlinks can be created without administrator privilege.\n\n- Enable symlink support for `git`:\n\n  ```powershell\n  git config --global core.symlinks true\n  ```\n\nIf you cannot or wouldn't like to enable anything, it doesn't matter. Copying will be used instead when error on creating symbolic links.\n\n### Build\n\n#### 1. Clone this project and fetch all submodules:\n\n```sh\ngit clone git@github.com:osfans/trime.git\ngit submodule update --init --recursive\n# use partial clone to save time\ngit submodule update --init --recursive --filter=blob:none\n```\n\n#### 2. Debug version without signature:\n\n```sh\n# On Linux or macOS\nmake debug\n\n# On Windows\n.\\gradlew assembleDebug\n```\n\n#### 3. Release version with signture:\n\nCreate `keystore.properties` file which contains following contents for [signing information](https://developer.android.com/studio/publish/app-signing.html):\n\n```gradle.properties\nstorePassword=myStorePassword\nkeyPassword=mykeyPassword\nkeyAlias=myKeyAlias\nstoreFile=myStoreFileLocation\n```\n\nThen, you may run:\n\n```sh\n# On Linux or macOS\nmake release\n\n# On Windows\n.\\gradlew assembleRelease\n```\n\n### Troubleshooting\n\n```\nTarget \"boost_log_setup\" links to target \"Boost::coroutine\" but the target was not found.\n```\n\nRun `make clean` on Linux or macOS, or run `.\\gradlew clean` on Windows.\n\nOther issues:\n\n1. Try `make clean`\n2. Make sure your repo is up-to-date. If one or more submodules are modified, also make sure they are compatible with the current version.\n3. If the problem still exists(very unlikely), try to make a new clone.\n4. Check if this is there is an issue/PR related to your problem. If yes, try their solutions.\n5. If none of them works, you may make an issue to ask for help.(optional)\n\n## Acknowledgments\n\n- Developer: [osfans](https://github.com/osfans)\n- Contributors: [boboIqiqi](https://github.com/boboIqiqi)ã€[Bambooin](https://github.com/Bambooin)ã€[senchi96](https://github.com/senchi96)ã€[heiher](https://github.com/heiher)ã€[abay](https://github.com/a342191555)ã€[iovxw](https://github.com/iovxw)ã€[huyz-git](https://github.com/huyz-git)ã€[tumuyan](https://github.com/tumuyan)ã€[WhiredPlanck](https://github.com/WhiredPlanck)ã€[nopdan](https://github.com/nopdan)...\n- [Wiki Editors](https://github.com/osfans/trime/wiki): [xiaoqun2016](https://github.com/xiaoqun2016)ã€[boboIqiqi](https://github.com/boboIqiqi)...\n- Translators: å¤©çœŸå¯çˆ±çš„æ»¡æ»¡ (Chinese Traditional), ç‚¹è§£ (English) ...\n- Keyboard Designers: å¤©çœŸå¯çˆ±çš„æ»¡æ»¡ã€çš›ç­±æ™“å°ç¬¨é±¼ã€å´ç› 11ã€ç†ŠçŒ«é˜¿ Boã€é»˜é»˜ã„‡ã„› Ë‹...\n- Donations: See QR Code in [Releases](https://github.com/osfans/trime/releases)\n- Community: Netizens who feedback in [Issues](https://github.com/osfans/trime/issues)ã€[QQ Group (811142286)](https://jq.qq.com/?_wv=1027&k=AXdR80HN)ã€[QQ Group (224230445)](http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&k=pg_q7UVumWYLq1Rk8kIAqkK1xGt64VnX&authKey=04m9l7OBO5H5vgrEL8IbpsmtnptWM60xy%2FUwYCfyvw9VcRhe8zRzAS1ezoemZdFr&noverify=0&group_code=224230445)ã€[Tieba](http://tieba.baidu.com/f?kw=rime)ã€[Google Play](https://play.google.com/store/apps/details?id=com.osfans.trime)ã€[Telegram](https://t.me/trime_dev)...\n- Projects: [RIME]ã€[OpenCC]ã€[Android Traditional Chinese IME] and so on.\n\n## Third Party Libraries\n\n- [Boost C++ Libraries](https://www.boost.org/) (Boost Software License)\n- [darts-clone](https://github.com/s-yata/darts-clone) (New BSD License)\n- [LevelDB](https://github.com/google/leveldb) (New BSD License)\n- [libiconv](https://www.gnu.org/software/libiconv/) (LGPL License)\n- [marisa-trie](https://github.com/s-yata/marisa-trie) (BSD License)\n- [glog](https://github.com/google/glog) (New BSD License)\n- [OpenCC](https://github.com/BYVoid/OpenCC) (Apache License 2.0)\n- [RIME](https://rime.im) (BSD License)\n- [snappy](https://github.com/google/snappy)(BSD License)\n- [utfcpp](https://github.com/nemtrif/utfcpp) (Boost Software License)\n- [yaml-cpp](https://github.com/jbeder/yaml-cpp) (MIT License)\n- [Android Traditional Chinese IME](https://code.google.com/p/android-traditional-chinese-ime/) (Apache License 2.0)\n\n[Android Traditional Chinese IME]: https://code.google.com/p/android-traditional-chinese-ime/\n[RIME]: http://rime.im\n[OpenCC]: https://github.com/BYVoid/OpenCC\n",
      "stars_today": 4
    },
    {
      "id": 192480120,
      "name": "koanf",
      "full_name": "knadh/koanf",
      "description": "Simple, extremely lightweight, extensible, configuration management library for Go. Supports JSON, TOML, YAML, env, command line, file, S3 etc. Alternative to viper.",
      "html_url": "https://github.com/knadh/koanf",
      "stars": 3799,
      "forks": 184,
      "language": "Go",
      "topics": [
        "config",
        "config-loader",
        "configuration",
        "configuration-file",
        "configuration-management",
        "etcd-client",
        "go",
        "golang",
        "golang-package",
        "s3-bucket",
        "toml",
        "viper",
        "yaml"
      ],
      "created_at": "2019-06-18T06:34:05Z",
      "updated_at": "2026-01-28T01:24:22Z",
      "pushed_at": "2026-01-25T06:13:10Z",
      "open_issues": 9,
      "owner": {
        "login": "knadh",
        "avatar_url": "https://avatars.githubusercontent.com/u/547147?v=4"
      },
      "readme": "<a href=\"https://zerodha.tech\"><img src=\"https://zerodha.tech/static/images/github-badge.svg\" align=\"right\" /></a>\n\n![koanf](https://user-images.githubusercontent.com/547147/72681838-6981dd00-3aed-11ea-8f5d-310816c70c08.png)\n\n**koanf** is a library for reading configuration from different sources in different formats in Go applications. It is a cleaner, lighter [alternative to spf13/viper](#alternative-to-viper) with better abstractions and extensibility and far fewer dependencies.\n\nkoanf v2 has modules (Providers) for reading configuration from a variety of sources such as files, command line flags, environment variables, Vault, and S3 and for parsing (Parsers) formats such as JSON, YAML, TOML, HUML, Hashicorp HCL. It is easy to plug in custom parsers and providers.\n\nAll external dependencies in providers and parsers are detached from the core and can be installed separately as necessary.\n\n[![Run Tests](https://github.com/knadh/koanf/actions/workflows/test.yml/badge.svg)](https://github.com/knadh/koanf/actions/workflows/test.yml) [![GoDoc](https://pkg.go.dev/badge/github.com/knadh/koanf?utm_source=godoc)](https://pkg.go.dev/github.com/knadh/koanf/v2) \n\n### Installation\n\n```shell\n# Install the core.\ngo get -u github.com/knadh/koanf/v2\n\n# Install the necessary Provider(s).\n# Available: file, env/v2, posflag, basicflag, confmap, rawbytes,\n#            structs, fs, s3, appconfig/v2, consul/v2, etcd/v2, vault/v2, parameterstore/v2\n# eg: go get -u github.com/knadh/koanf/providers/s3\n# eg: go get -u github.com/knadh/koanf/providers/consul/v2\n\ngo get -u github.com/knadh/koanf/providers/file\n\n\n# Install the necessary Parser(s).\n# Available: toml, toml/v2, json, yaml, huml, dotenv, hcl, hjson, nestedtext\n# go get -u github.com/knadh/koanf/parsers/$parser\n\ngo get -u github.com/knadh/koanf/parsers/toml\n```\n\n[See the list](#api) of all bundled Providers and Parsers.\n\n### Contents\n\n- [Concepts](#concepts)\n- [Reading config from files](#reading-config-from-files)\n- [Watching file for changes](#watching-file-for-changes)\n- [Reading from command line](#reading-from-command-line)\n- [Reading environment variables](#reading-environment-variables)\n- [Reading from an S3 bucket](#reading-from-an-s3-bucket)\n- [Reading raw bytes](#reading-raw-bytes)\n- [Reading from maps and structs](#reading-from-nested-maps)\n- [Unmarshalling and marshalling](#unmarshalling-and-marshalling)\n- [Order of merge and key case sensitivity](#order-of-merge-and-key-case-sensitivity)\n- [Custom Providers and Parsers](#custom-providers-and-parsers)\n- [Custom merge strategies](#custom-merge-strategies)\n- [List of installable Providers and Parsers](#api)\n\n### Concepts\n\n- `koanf.Provider` is a generic interface that provides configuration, for example, from files, environment variables, HTTP sources, or anywhere. The configuration can either be raw bytes that a parser can parse, or it can be a nested `map[string]any` that can be directly loaded.\n- `koanf.Parser` is a generic interface that takes raw bytes, parses, and returns a nested `map[string]any`. For example, JSON and YAML parsers.\n- Once loaded into koanf, configuration are values queried by a delimited key path syntax. eg: `app.server.port`. Any delimiter can be chosen.\n- Configuration from multiple sources can be loaded and merged into a koanf instance, for example, load from a file first and override certain values with flags from the command line.\n\nWith these two interface implementations, koanf can obtain configuration in any format from any source, parse it, and make it available to an application.\n\n### Reading config from files\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/knadh/koanf/v2\"\n\t\"github.com/knadh/koanf/parsers/json\"\n\t\"github.com/knadh/koanf/parsers/yaml\"\n\t\"github.com/knadh/koanf/providers/file\"\n)\n\n// Global koanf instance. Use \".\" as the key path delimiter. This can be \"/\" or any character.\nvar k = koanf.New(\".\")\n\nfunc main() {\n\t// Load JSON config.\n\tif err := k.Load(file.Provider(\"mock/mock.json\"), json.Parser()); err != nil {\n\t\tlog.Fatalf(\"error loading config: %v\", err)\n\t}\n\n\t// Load YAML config and merge into the previously loaded config (because we can).\n\tk.Load(file.Provider(\"mock/mock.yml\"), yaml.Parser())\n\n\tfmt.Println(\"parent's name is = \", k.String(\"parent1.name\"))\n\tfmt.Println(\"parent's ID is = \", k.Int(\"parent1.id\"))\n}\n\n```\n\n### Watching file for changes\nSome providers expose a `Watch()` method that makes the provider watch for changes\nin configuration and trigger a callback to reload the configuration.\nThis is not goroutine safe if there are concurrent `*Get()` calls happening on the\nkoanf object while it is doing a `Load()`. Such scenarios will need mutex locking.\n\n`file, appconfig, vault, consul` providers have a `Watch()` method.\n\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/knadh/koanf/v2\"\n\t\"github.com/knadh/koanf/parsers/json\"\n\t\"github.com/knadh/koanf/parsers/yaml\"\n\t\"github.com/knadh/koanf/providers/file\"\n)\n\n// Global koanf instance. Use \".\" as the key path delimiter. This can be \"/\" or any character.\nvar k = koanf.New(\".\")\n\nfunc main() {\n\t// Load JSON config.\n\tf := file.Provider(\"mock/mock.json\")\n\tif err := k.Load(f, json.Parser()); err != nil {\n\t\tlog.Fatalf(\"error loading config: %v\", err)\n\t}\n\n\t// Load YAML config and merge into the previously loaded config (because we can).\n\tk.Load(file.Provider(\"mock/mock.yml\"), yaml.Parser())\n\n\tfmt.Println(\"parent's name is = \", k.String(\"parent1.name\"))\n\tfmt.Println(\"parent's ID is = \", k.Int(\"parent1.id\"))\n\n\t// Watch the file and get a callback on change. The callback can do whatever,\n\t// like re-load the configuration.\n\t// File provider always returns a nil `event`.\n\tf.Watch(func(event any, err error) {\n\t\tif err != nil {\n\t\t\tlog.Printf(\"watch error: %v\", err)\n\t\t\treturn\n\t\t}\n\n\t\t// Throw away the old config and load a fresh copy.\n\t\tlog.Println(\"config changed. Reloading ...\")\n\t\tk = koanf.New(\".\")\n\t\tk.Load(f, json.Parser())\n\t\tk.Print()\n\t})\n\n\t// To stop a file watcher, call:\n\t// f.Unwatch()\n\n\t// Block forever (and manually make a change to mock/mock.json) to\n\t// reload the config.\n\tlog.Println(\"waiting forever. Try making a change to mock/mock.json to live reload\")\n\t<-make(chan bool)\n}\n```\n\n\n### Reading from command line\n\nThe following example shows the use of `posflag.Provider`, a wrapper over the [spf13/pflag](https://github.com/spf13/pflag) library, an advanced commandline lib. For Go's built in `flag` package, use `basicflag.Provider`.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/knadh/koanf/v2\"\n\t\"github.com/knadh/koanf/parsers/toml\"\n\n\t// TOML version 2 is available at:\n\t// \"github.com/knadh/koanf/parsers/toml/v2\"\n\n\t\"github.com/knadh/koanf/providers/file\"\n\t\"github.com/knadh/koanf/providers/posflag\"\n\tflag \"github.com/spf13/pflag\"\n)\n\n// Global koanf instance. Use \".\" as the key path delimiter. This can be \"/\" or any character.\nvar k = koanf.New(\".\")\n\nfunc main() {\n\t// Use the POSIX compliant pflag lib instead of Go's flag lib.\n\tf := flag.NewFlagSet(\"config\", flag.ContinueOnError)\n\tf.Usage = func() {\n\t\tfmt.Println(f.FlagUsages())\n\t\tos.Exit(0)\n\t}\n\t// Path to one or more config files to load into koanf along with some config params.\n\tf.StringSlice(\"conf\", []string{\"mock/mock.toml\"}, \"path to one or more .toml config files\")\n\tf.String(\"time\", \"2020-01-01\", \"a time string\")\n\tf.String(\"type\", \"xxx\", \"type of the app\")\n\tf.Parse(os.Args[1:])\n\n\t// Load the config files provided in the commandline.\n\tcFiles, _ := f.GetStringSlice(\"conf\")\n\tfor _, c := range cFiles {\n\t\tif err := k.Load(file.Provider(c), toml.Parser()); err != nil {\n\t\t\tlog.Fatalf(\"error loading file: %v\", err)\n\t\t}\n\t}\n\n\t// \"time\" and \"type\" may have been loaded from the config file, but\n\t// they can still be overridden with the values from the command line.\n\t// The bundled posflag.Provider takes a flagset from the spf13/pflag lib.\n\t// Passing the Koanf instance to posflag helps it deal with default command\n\t// line flag values that are not present in conf maps from previously loaded\n\t// providers.\n\tif err := k.Load(posflag.Provider(f, \".\", k), nil); err != nil {\n\t\tlog.Fatalf(\"error loading config: %v\", err)\n\t}\n\n\tfmt.Println(\"time is = \", k.String(\"time\"))\n}\n```\n\n### Reading environment variables\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"strings\"\n\n\t\"github.com/knadh/koanf/v2\"\n\t\"github.com/knadh/koanf/parsers/json\"\n\t\"github.com/knadh/koanf/providers/env/v2\"\n\t\"github.com/knadh/koanf/providers/file\"\n)\n\n// Global koanf instance. Use . as the key path delimiter. This can be / or anything.\nvar k = koanf.New(\".\")\n\nfunc main() {\n\t// Load JSON config.\n\tif err := k.Load(file.Provider(\"mock/mock.json\"), json.Parser()); err != nil {\n\t\tlog.Fatalf(\"error loading config: %v\", err)\n\t}\n\n\t// Load only environment variables with prefix \"MYVAR_\" and merge into config.\n\t// Transform var names by:\n\t// 1. Converting to lowercase\n\t// 2. Removing \"MYVAR_\" prefix  \n\t// 3. Replacing \"_\" with \".\" to representing nesting using the . delimiter.\n\t// Example: MYVAR_PARENT1_CHILD1_NAME becomes \"parent1.child1.name\"\n\tk.Load(env.Provider(\".\", env.Opt{\n\t\tPrefix: \"MYVAR_\",\n\t\tTransformFunc: func(k, v string) (string, any) {\n\t\t\t// Transform the key.\n\t\t\tk = strings.ReplaceAll(strings.ToLower(strings.TrimPrefix(k, \"MYVAR_\")), \"_\", \".\")\n\n\t\t\t// Transform the value into slices, if they contain spaces.\n\t\t\t// Eg: MYVAR_TAGS=\"foo bar baz\" -> tags: [\"foo\", \"bar\", \"baz\"]\n\t\t\t// This is to demonstrate that string values can be transformed to any type\n\t\t\t// where necessary.\n\t\t\tif strings.Contains(v, \" \") {\n\t\t\t\treturn k, strings.Split(v, \" \")\n\t\t\t}\n\n\t\t\treturn k, v\n\t\t},\n\t}), nil)\n\n\tfmt.Println(\"name is =\", k.String(\"parent1.child1.name\"))\n\tfmt.Println(\"time is =\", k.Time(\"time\", time.DateOnly))\n\tfmt.Println(\"ids are =\", k.Strings(\"parent1.child1.grandchild1.ids\"))\n}\n```\n\n### Reading from an S3 bucket\n\n```go\n// Load JSON config from s3.\nif err := k.Load(s3.Provider(s3.Config{\n\tAccessKey: os.Getenv(\"AWS_S3_ACCESS_KEY\"),\n\tSecretKey: os.Getenv(\"AWS_S3_SECRET_KEY\"),\n\tRegion:    os.Getenv(\"AWS_S3_REGION\"),\n\tBucket:    os.Getenv(\"AWS_S3_BUCKET\"),\n\tObjectKey: \"dir/config.json\",\n}), json.Parser()); err != nil {\n\tlog.Fatalf(\"error loading config: %v\", err)\n}\n```\n\n### Reading raw bytes\n\nThe bundled `rawbytes` Provider can be used to read arbitrary bytes from a source, like a database or an HTTP call.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/knadh/koanf/v2\"\n\t\"github.com/knadh/koanf/parsers/json\"\n\t\"github.com/knadh/koanf/providers/rawbytes\"\n)\n\n// Global koanf instance. Use . as the key path delimiter. This can be / or anything.\nvar k = koanf.New(\".\")\n\nfunc main() {\n\tb := []byte(`{\"type\": \"rawbytes\", \"parent1\": {\"child1\": {\"type\": \"rawbytes\"}}}`)\n\tk.Load(rawbytes.Provider(b), json.Parser())\n\tfmt.Println(\"type is = \", k.String(\"parent1.child1.type\"))\n}\n```\n\n### Unmarshalling and marshalling\n`Parser`s can be used to unmarshal and scan the values in a Koanf instance into a struct based on the field tags, and to marshal a Koanf instance back into serialized bytes, for example to JSON or YAML files\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/knadh/koanf/v2\"\n\t\"github.com/knadh/koanf/parsers/json\"\n\t\"github.com/knadh/koanf/providers/file\"\n)\n\n// Global koanf instance. Use . as the key path delimiter. This can be / or anything.\nvar (\n\tk      = koanf.New(\".\")\n\tparser = json.Parser()\n)\n\nfunc main() {\n\t// Load JSON config.\n\tif err := k.Load(file.Provider(\"mock/mock.json\"), parser); err != nil {\n\t\tlog.Fatalf(\"error loading config: %v\", err)\n\t}\n\n\t// Structure to unmarshal nested conf to.\n\ttype childStruct struct {\n\t\tName       string            `koanf:\"name\"`\n\t\tType       string            `koanf:\"type\"`\n\t\tEmpty      map[string]string `koanf:\"empty\"`\n\t\tGrandChild struct {\n\t\t\tIds []int `koanf:\"ids\"`\n\t\t\tOn  bool  `koanf:\"on\"`\n\t\t} `koanf:\"grandchild1\"`\n\t}\n\n\tvar out childStruct\n\n\t// Quick unmarshal.\n\tk.Unmarshal(\"parent1.child1\", &out)\n\tfmt.Println(out)\n\n\t// Unmarshal with advanced config.\n\tout = childStruct{}\n\tk.UnmarshalWithConf(\"parent1.child1\", &out, koanf.UnmarshalConf{Tag: \"koanf\"})\n\tfmt.Println(out)\n\n\t// Marshal the instance back to JSON.\n\t// The parser instance can be anything, eg: json.Parser(), yaml.Parser() etc.\n\tb, _ := k.Marshal(parser)\n\tfmt.Println(string(b))\n}\n```\n\n### Unmarshalling with flat paths\n\nSometimes it is necessary to unmarshal an assortment of keys from various nested structures into a flat target structure. This is possible with the `UnmarshalConf.FlatPaths` flag.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/knadh/koanf/v2\"\n\t\"github.com/knadh/koanf/parsers/json\"\n\t\"github.com/knadh/koanf/providers/file\"\n)\n\n// Global koanf instance. Use . as the key path delimiter. This can be / or anything.\nvar k = koanf.New(\".\")\n\nfunc main() {\n\t// Load JSON config.\n\tif err := k.Load(file.Provider(\"mock/mock.json\"), json.Parser()); err != nil {\n\t\tlog.Fatalf(\"error loading config: %v\", err)\n\t}\n\n\ttype rootFlat struct {\n\t\tType                        string            `koanf:\"type\"`\n\t\tEmpty                       map[string]string `koanf:\"empty\"`\n\t\tParent1Name                 string            `koanf:\"parent1.name\"`\n\t\tParent1ID                   int               `koanf:\"parent1.id\"`\n\t\tParent1Child1Name           string            `koanf:\"parent1.child1.name\"`\n\t\tParent1Child1Type           string            `koanf:\"parent1.child1.type\"`\n\t\tParent1Child1Empty          map[string]string `koanf:\"parent1.child1.empty\"`\n\t\tParent1Child1Grandchild1IDs []int             `koanf:\"parent1.child1.grandchild1.ids\"`\n\t\tParent1Child1Grandchild1On  bool              `koanf:\"parent1.child1.grandchild1.on\"`\n\t}\n\n\t// Unmarshal the whole root with FlatPaths: True.\n\tvar o1 rootFlat\n\tk.UnmarshalWithConf(\"\", &o1, koanf.UnmarshalConf{Tag: \"koanf\", FlatPaths: true})\n\tfmt.Println(o1)\n\n\t// Unmarshal a child structure of \"parent1\".\n\ttype subFlat struct {\n\t\tName                 string            `koanf:\"name\"`\n\t\tID                   int               `koanf:\"id\"`\n\t\tChild1Name           string            `koanf:\"child1.name\"`\n\t\tChild1Type           string            `koanf:\"child1.type\"`\n\t\tChild1Empty          map[string]string `koanf:\"child1.empty\"`\n\t\tChild1Grandchild1IDs []int             `koanf:\"child1.grandchild1.ids\"`\n\t\tChild1Grandchild1On  bool              `koanf:\"child1.grandchild1.on\"`\n\t}\n\n\tvar o2 subFlat\n\tk.UnmarshalWithConf(\"parent1\", &o2, koanf.UnmarshalConf{Tag: \"koanf\", FlatPaths: true})\n\tfmt.Println(o2)\n}\n```\n\n#### Reading from nested maps\n\nThe bundled `confmap` provider takes a `map[string]any` that can be loaded into a koanf instance. \n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/knadh/koanf/v2\"\n\t\"github.com/knadh/koanf/providers/confmap\"\n\t\"github.com/knadh/koanf/providers/file\"\n\t\"github.com/knadh/koanf/parsers/json\"\n\t\"github.com/knadh/koanf/parsers/yaml\"\n)\n\n// Global koanf instance. Use \".\" as the key path delimiter. This can be \"/\" or any character.\nvar k = koanf.New(\".\")\n\nfunc main() {\n\t// Load default values using the confmap provider.\n\t// We provide a flat map with the \".\" delimiter.\n\t// A nested map can be loaded by setting the delimiter to an empty string \"\".\n\tk.Load(confmap.Provider(map[string]any{\n\t\t\"parent1.name\": \"Default Name\",\n\t\t\"parent3.name\": \"New name here\",\n\t}, \".\"), nil)\n\n\t// Load JSON config on top of the default values.\n\tif err := k.Load(file.Provider(\"mock/mock.json\"), json.Parser()); err != nil {\n\t\tlog.Fatalf(\"error loading config: %v\", err)\n\t}\n\n\t// Load YAML config and merge into the previously loaded config (because we can).\n\tk.Load(file.Provider(\"mock/mock.yml\"), yaml.Parser())\n\n\tfmt.Println(\"parent's name is = \", k.String(\"parent1.name\"))\n\tfmt.Println(\"parent's ID is = \", k.Int(\"parent1.id\"))\n}\n```\n\n#### Reading from struct \n\nThe bundled `structs` provider can be used to read data from a struct to load into a koanf instance.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/knadh/koanf/v2\"\n\t\"github.com/knadh/koanf/providers/structs\"\n)\n\n// Global koanf instance. Use \".\" as the key path delimiter. This can be \"/\" or any character.\nvar k = koanf.New(\".\")\n\ntype parentStruct struct {\n\tName   string      `koanf:\"name\"`\n\tID     int         `koanf:\"id\"`\n\tChild1 childStruct `koanf:\"child1\"`\n}\ntype childStruct struct {\n\tName        string            `koanf:\"name\"`\n\tType        string            `koanf:\"type\"`\n\tEmpty       map[string]string `koanf:\"empty\"`\n\tGrandchild1 grandchildStruct  `koanf:\"grandchild1\"`\n}\ntype grandchildStruct struct {\n\tIds []int `koanf:\"ids\"`\n\tOn  bool  `koanf:\"on\"`\n}\ntype sampleStruct struct {\n\tType    string            `koanf:\"type\"`\n\tEmpty   map[string]string `koanf:\"empty\"`\n\tParent1 parentStruct      `koanf:\"parent1\"`\n}\n\nfunc main() {\n\t// Load default values using the structs provider.\n\t// We provide a struct along with the struct tag `koanf` to the\n\t// provider.\n\tk.Load(structs.Provider(sampleStruct{\n\t\tType:  \"json\",\n\t\tEmpty: make(map[string]string),\n\t\tParent1: parentStruct{\n\t\t\tName: \"parent1\",\n\t\t\tID:   1234,\n\t\t\tChild1: childStruct{\n\t\t\t\tName:  \"child1\",\n\t\t\t\tType:  \"json\",\n\t\t\t\tEmpty: make(map[string]string),\n\t\t\t\tGrandchild1: grandchildStruct{\n\t\t\t\t\tIds: []int{1, 2, 3},\n\t\t\t\t\tOn:  true,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}, \"koanf\"), nil)\n\n\tfmt.Printf(\"name is = `%s`\\n\", k.String(\"parent1.child1.name\"))\n}\n```\n### Merge behavior\n#### Default behavior\nThe default behavior when you create Koanf this way is: `koanf.New(delim)` that the latest loaded configuration will\nmerge with the previous one.\n\nFor example:\n`first.yml`\n```yaml\nkey: [1,2,3]\n```\n`second.yml`\n```yaml\nkey: 'string'\n```\nWhen `second.yml` is loaded it will override the type of the `first.yml`.\n\nIf this behavior is not desired, you can merge 'strictly'. In the same scenario, `Load` will return an error.\n\n```go\npackage main\n\nimport (\n\t\"errors\"\n\t\"log\"\n\n\t\"github.com/knadh/koanf/v2\"\n\t\"github.com/knadh/koanf/maps\"\n\t\"github.com/knadh/koanf/parsers/json\"\n\t\"github.com/knadh/koanf/parsers/yaml\"\n\t\"github.com/knadh/koanf/providers/file\"\n)\n\nvar conf = koanf.Conf{\n\tDelim:       \".\",\n\tStrictMerge: true,\n}\nvar k = koanf.NewWithConf(conf)\n\nfunc main() {\n\tyamlPath := \"mock/mock.yml\"\n\tif err := k.Load(file.Provider(yamlPath), yaml.Parser()); err != nil {\n\t\tlog.Fatalf(\"error loading config: %v\", err)\n\t}\n\n\tjsonPath := \"mock/mock.json\"\n\tif err := k.Load(file.Provider(jsonPath), json.Parser()); err != nil {\n\t\tlog.Fatalf(\"error loading config: %v\", err)\n\t}\n}\n```\n**Note:** When merging different extensions, each parser can treat his types differently,\n meaning even though you the load same types there is a probability that it will fail with `StrictMerge: true`.\n\nFor example: merging JSON and YAML will most likely fail because JSON treats integers as float64 and YAML treats them as integers.\n\n### Order of merge and key case sensitivity\n\n- Config keys are case-sensitive in koanf. For example, `app.server.port` and `APP.SERVER.port` are not the same.\n- koanf does not impose any ordering on loading config from various providers. Every successive `Load()` or `Merge()` merges new config into the existing config. That is, it is possible to load environment variables first, then files on top of it, and then command line variables on top of it, or any such order.\n\n### Custom Providers and Parsers\n\nA Provider returns a nested `map[string]any` config that can be loaded directly into koanf with `koanf.Load()` or it can return raw bytes that can be parsed with a Parser (again, loaded using `koanf.Load()`. Writing Providers and Parsers are easy. See the bundled implementations in the [providers](https://github.com/knadh/koanf/tree/master/providers) and [parsers](https://github.com/knadh/koanf/tree/master/parsers) directories.\n\n### Custom merge strategies\n\nBy default, when merging two config sources using `Load()`, koanf recursively merges keys of nested maps (`map[string]any`),\nwhile static values are overwritten (slices, strings, etc). This behaviour can be changed by providing a custom merge function with the `WithMergeFunc` option.\n\n```go\npackage main\n\nimport (\n\t\"errors\"\n\t\"log\"\n\n\t\"github.com/knadh/koanf/v2\"\n\t\"github.com/knadh/koanf/maps\"\n\t\"github.com/knadh/koanf/parsers/json\"\n\t\"github.com/knadh/koanf/parsers/yaml\"\n\t\"github.com/knadh/koanf/providers/file\"\n)\n\nvar conf = koanf.Conf{\n\tDelim:       \".\",\n\tStrictMerge: true,\n}\nvar k = koanf.NewWithConf(conf)\n\nfunc main() {\n\tyamlPath := \"mock/mock.yml\"\n\tif err := k.Load(file.Provider(yamlPath), yaml.Parser()); err != nil {\n\t\tlog.Fatalf(\"error loading config: %v\", err)\n\t}\n\n\tjsonPath := \"mock/mock.json\"\n\tif err := k.Load(file.Provider(jsonPath), json.Parser(), koanf.WithMergeFunc(func(src, dest map[string]any) error {\n     // Your custom logic, copying values from src into dst\n     return nil\n    })); err != nil {\n\t\tlog.Fatalf(\"error loading config: %v\", err)\n\t}\n}\n```\n\n## API\n\nSee the full API documentation of all available methods at https://pkg.go.dev/github.com/knadh/koanf/v2#section-documentation\n\n### Bundled Providers\n\nInstall with `go get -u github.com/knadh/koanf/providers/$provider`\n\n| Package             | Provider                                                      | Description                                                                                                                                                                           |\n| ------------------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| file      | `file.Provider(filepath string)`                              | Reads a file and returns the raw bytes to be parsed.                                                                                                                                  |\n| fs      | `fs.Provider(f fs.FS, filepath string)`                              | (**Experimental**) Reads a file from fs.FS and returns the raw bytes to be parsed. The provider requires `go v1.16` or higher.                                            |\n| basicflag | `basicflag.Provider(f *flag.FlagSet, delim string)`           | Takes a stdlib `flag.FlagSet`                                                                                                                                                        |\n| posflag   | `posflag.Provider(f *pflag.FlagSet, delim string)`            | Takes an `spf13/pflag.FlagSet` (advanced POSIX compatible flags with multiple types) and provides a nested config map based on delim.                                                 |\n| env/v2       | `env.Provider(prefix, delim string, f func(s string) string)` | Takes an optional prefix to filter env variables by, an optional function that takes and returns a string to transform env variables, and returns a nested config map based on delim. |\n| confmap   | `confmap.Provider(mp map[string]any, delim string)`   | Takes a premade `map[string]any` conf map. If delim is provided, the keys are assumed to be flattened, thus unflattened using delim.                                          |\n| structs   | `structs.Provider(s any, tag string)`                 | Takes a struct and struct tag.                                                                                                                                                        |\n| s3        | `s3.Provider(s3.S3Config{})`                                  | Takes a s3 config struct.                                                                                                                                                             |\n| rawbytes  | `rawbytes.Provider(b []byte)`                                 | Takes a raw `[]byte` slice to be parsed with a koanf.Parser                                                                                                                           |\n| vault/v2     | `vault.Provider(vault.Config{})`                              | Hashicorp Vault provider                                                                                                                           |\n| appconfig/v2     | `vault.AppConfig(appconfig.Config{})`                              | AWS AppConfig provider                                                                                                                           |\n| etcd/v2     | `etcd.Provider(etcd.Config{})`                              | CNCF etcd provider                                                                                                                           |\n| consul/v2     | `consul.Provider(consul.Config{})`                              | Hashicorp Consul provider                                                                                                                           |\n| parameterstore/v2 | `parameterstore.Provider(parameterstore.Config{})` | AWS Systems Manager Parameter Store provider |\n| cliflagv2  |  `cliflagv2.Provider(ctx *cli.Context, delimiter string)` |  Reads commands and flags from urfave/cli/v2 context including global flags and nested command flags and provides a nested config map based on delim. |\n| cliflagv3  |  `cliflagv3.Provider(ctx *cli.Context, delimiter string)` |  Reads commands and flags from urfave/cli/v3 and provides a nested config map based on delim. |\n| kiln   |  `kiln.Provider(configPath, keyPath, file string)` | Takes an optional prefix to filter environment variables keys by, an optional function that takes and returns a string to transform environment variables, and returns a nested config map. |\n\n\n### Bundled Parsers\n\nInstall with `go get -u github.com/knadh/koanf/parsers/$parser`\n\n| Package      | Parser                           | Description                                                                                                                                               |\n| ------------ | -------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| json       | `json.Parser()`                  | Parses JSON bytes into a nested map                                                                                                                       |\n| yaml       | `yaml.Parser()`                  | Parses YAML bytes into a nested map                                                                                                                       |\n| toml       | `toml.Parser()`                  | Parses TOML bytes into a nested map                                                                                                                       |\n| toml/v2    | `toml.Parser()`                  | Parses TOML bytes into a nested map (using go-toml v2)                                                                                                    |\n| dotenv     | `dotenv.Parser()`              | Parses DotEnv bytes into a flat map                                                                                                                       |\n| hcl        | `hcl.Parser(flattenSlices bool)` | Parses Hashicorp HCL bytes into a nested map. `flattenSlices` is recommended to be set to true. [Read more](https://github.com/hashicorp/hcl/issues/162). |\n| hjson\t\t | `hjson.Parser()`\t\t\t\t\t| Parses HJSON bytes into a nested map                                                                                                                     |\n| huml       | `huml.Parser()`                   | Parses HUML (Human-Oriented Markup Language) bytes into a nested map                                                                                     |\n| nestedtext | `nestedtext.Parser()`              | Parses NestedText bytes into a flat map                                                                                                                 |\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t|\n\n\n### Third-party Providers\n| Package             | Provider                                                      | Description                                                                                                                                                                           |\n| ------------------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| github.com/defensestation/koanf/providers/secretsmanager     | `vault.SecretsManager(secretsmanager.Config{}, f func(s string) string)`                              | AWS Secrets Manager provider, takes map or string as a value from store                                                \t\t\t\t\t\t  |\n| github.com/defensestation/koanf/providers/parameterstore     | `vault.ParameterStore(parameterstore.Config{}, f func(s string) string)`                              | AWS ParameterStore provider, an optional function that takes and returns a string to transform env variables                                                 \t\t\t\t\t\t  |\n\n\n### Alternative to viper\n\nkoanf is a [lightweight](https://github.com/knadh/koanf/blob/master/go.mod) alternative to the popular [spf13/viper](https://github.com/spf13/viper). It was written as a result of multiple stumbling blocks encountered with some of viper's fundamental flaws.\n\n- viper breaks JSON, YAML, TOML, HCL language specs by [forcibly lowercasing keys](https://github.com/spf13/viper/pull/635).\n- Significantly bloats [build sizes](https://github.com/knadh/koanf/wiki/Comparison-with-spf13-viper).\n- Tightly couples config parsing with file extensions.\n- Has poor semantics and abstractions. Commandline, env, file etc. and various parses are hardcoded in the core. There are no primitives that can be extended.\n- Pulls a large number of [third party dependencies](https://github.com/spf13/viper/issues/707) into the core package. For instance, even if you do not use YAML or flags, the dependencies are still pulled as a result of the coupling.\n- Imposes arbitrary ordering conventions (eg: flag -> env -> config etc.)\n- `Get()` returns references to slices and maps. Mutations made outside change the underlying values inside the conf map.\n- Does non-idiomatic things such as [throwing away O(1) on flat maps](https://github.com/spf13/viper/blob/3b4aca75714a37276c4b1883630bd98c02498b73/viper.go#L1524).\n- Viper treats keys that contain an empty map (eg: `my_key: {}`) as if they were not set (ie: `IsSet(\"my_key\") == false`).\n- There are a large number of [open issues](https://github.com/spf13/viper/issues).\n",
      "stars_today": 4
    },
    {
      "id": 1043515641,
      "name": "CordysCRM",
      "full_name": "1Panel-dev/CordysCRM",
      "description": "ğŸ”¥ æ–°ä¸€ä»£çš„å¼€æº AI CRM ç³»ç»Ÿã€‚An open-source AI CRM alternative to Salesforce.",
      "html_url": "https://github.com/1Panel-dev/CordysCRM",
      "stars": 1728,
      "forks": 304,
      "language": "Java",
      "topics": [
        "ai-crm",
        "cordys",
        "crm",
        "crm-system",
        "dataease",
        "maxkb",
        "salesforce",
        "sqlbot"
      ],
      "created_at": "2025-08-24T02:54:30Z",
      "updated_at": "2026-01-28T02:08:14Z",
      "pushed_at": "2026-01-28T02:02:55Z",
      "open_issues": 142,
      "owner": {
        "login": "1Panel-dev",
        "avatar_url": "https://avatars.githubusercontent.com/u/109613420?v=4"
      },
      "readme": "<h1 align=\"center\">Cordys CRM</h1>\n<h3 align=\"center\">æ–°ä¸€ä»£çš„å¼€æº AI CRM ç³»ç»Ÿ</h3>\n<p align=\"center\">\n  <a href=\"https://trendshift.io/repositories/15469\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/15469\" alt=\"1Panel-dev%2FCordysCRM | Trendshift\" style=\"width: 240px; height: auto;\" /></a>\n</p>\n<p align=\"center\">\n  <a href=\"https://github.com/1Panel-dev/CordysCRM/releases\"><img src=\"https://img.shields.io/github/v/release/1Panel-dev/CordysCRM\" alt=\"Latest release\"></a>\n  <a href=\"https://github.com/1Panel-dev/CordysCRM\"><img src=\"https://img.shields.io/github/stars/1Panel-dev/CordysCRM?color=%231890FF&style=flat-square\" alt=\"Stars\"></a>    \n  <a href=\"https://hub.docker.com/r/1panel/cordys-crm\"><img src=\"https://img.shields.io/docker/pulls/1panel/cordys-crm?label=downloads\" alt=\"Download\"></a><br/>\n</p>\n\n<hr/>\n\n**Cordys CRM** æ˜¯æ–°ä¸€ä»£çš„å¼€æº AI CRM ç³»ç»Ÿï¼Œæ˜¯é›†ä¿¡æ¯åŒ–ã€æ•°å­—åŒ–ã€æ™ºèƒ½åŒ–äºä¸€ä½“çš„ã€Œå®¢æˆ·å…³ç³»ç®¡ç†ç³»ç»Ÿã€ï¼Œç”± [é£è‡´äº‘](https://fit2cloud.com/) åŒ å¿ƒå‡ºå“ã€‚Cordys CRM èƒ½å¤Ÿå¸®åŠ©ä¼ä¸šå®ç°ä»çº¿ç´¢åˆ°å›æ¬¾ï¼ˆL2Cï¼‰çš„å…¨æµç¨‹ç²¾ç»†åŒ–ç®¡ç†ï¼Œè¦†ç›–çº¿ç´¢è·å–ã€æ™ºèƒ½åˆ†é…ã€å®¢æˆ·ä¸è”ç³»äººç®¡ç†ã€å•†æœºè·Ÿè¿›ã€åˆåŒç­¾çº¦åŠå›æ¬¾æ‰§è¡Œï¼Œæ„å»ºç«¯åˆ°ç«¯çš„é”€å”®è¿è¥é—­ç¯ã€‚\n\nCordys [/ËˆkÉ”ËrdÉªs/] ç”±â€œCordâ€ï¼ˆè¿æ¥ä¹‹ç»³ï¼‰ä¸â€œSystemâ€ï¼ˆç³»ç»Ÿï¼‰èåˆè€Œæˆï¼Œå¯“æ„â€œå…³ç³»çš„çº½å¸¦ç³»ç»Ÿâ€ï¼Œè¯ é‡Šäº† CRM çš„æœ¬è´¨ï¼šè¿æ¥å®¢æˆ·ï¼Œç¼”é€ é•¿æœŸä»·å€¼ã€‚\n\n[![Watch the video](https://resource.fit2cloud.com/1panel/cordys-crm/img/overview-video-20251110.png)](https://www.bilibili.com/video/BV1Wm1sB4ExL/)\n\n**Cordys CRM** çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯ï¼š\n\n- **çµæ´»æ˜“ç”¨**ï¼šåŸºäºç°ä»£åŒ–æŠ€æœ¯æ ˆæ„å»ºï¼Œä½¿ç”¨ä½“éªŒå¥½ï¼›å¹³å°æ”¯æŒåŸºäºè§’è‰²çš„æƒé™ç®¡æ§ï¼Œæä¾›çµæ´»çš„æ¨¡å—åŒ–é…ç½®ï¼Œå¯æ— ç¼é›†æˆä¼ä¸šå¾®ä¿¡ã€é’‰é’‰ã€é£ä¹¦ç­‰ä¸»æµåŠå…¬å¹³å°ï¼Œå®ç°é«˜æ•ˆååŒï¼›\n- **å®‰å…¨å¯æ§**ï¼šç§æœ‰åŒ–éƒ¨ç½²ï¼Œæ‰€æœ‰å®¢æˆ·æ•°æ®ä¸ä¸šåŠ¡ä¿¡æ¯å‡å­˜å‚¨äºä¼ä¸šè‡ªæœ‰æœåŠ¡å™¨ï¼Œæ•°æ®ä¸»æƒå®Œå…¨è‡ªä¸»ï¼ŒåŒæ—¶ä¾¿äºæ·±åº¦é›†æˆä¸äºŒæ¬¡å¼€å‘ï¼›\n- **AI åŠ æŒ**ï¼šå¼€æ”¾ MCP Serverï¼Œå€ŸåŠ© [MaxKB](https://github.com/1Panel-dev/MaxKB) å¼ºå¤§çš„æ™ºèƒ½ä½“å¼€å‘èƒ½åŠ›ï¼Œè½»æ¾æ„å»ºæ™ºèƒ½åˆ›å»ºã€æ™ºèƒ½è·Ÿè¿›ã€æ™ºèƒ½æŠ¥ä»·ç­‰å¤šæ ·åŒ–æ™ºèƒ½ä½“ï¼›\n- **BI åŠ æŒ**ï¼šèåˆ [DataEase](https://github.com/dataease/dataease) ä¸ [SQLBot](https://github.com/dataease/SQLBot) çš„å¼ºå¤§èƒ½åŠ›ï¼Œå®ç°é”€å”®æ•°æ®å¯è§†åŒ–å‘ˆç°ã€è‡ªåŠ©åˆ†æï¼Œä»¥åŠåŸºäºè‡ªç„¶è¯­è¨€çš„æ™ºèƒ½æŸ¥è¯¢ä¸å½’å› åˆ†æã€‚\n\n## å¿«é€Ÿå¼€å§‹\n\n### å®‰è£…éƒ¨ç½²\n\nå‡†å¤‡ä¸€å° Linux æœåŠ¡å™¨ï¼Œå®‰è£…å¥½ [Docker](https://docs.docker.com/get-docker/) åï¼Œæ‰§è¡Œä»¥ä¸‹ä¸€é”®å®‰è£…è„šæœ¬ã€‚\n\n```bash\ndocker run -d \\\n  --name cordys-crm \\\n  --restart unless-stopped \\\n  -p 8081:8081 \\\n  -p 8082:8082 \\\n  -v ~/cordys:/opt/cordys \\\n  1panel/cordys-crm\n```\n\nä½ ä¹Ÿå¯ä»¥é€šè¿‡ [1Panel åº”ç”¨å•†åº—](https://cordys.cn/docs/installation/1panel_installtion/) æ¥å®‰è£…éƒ¨ç½² Cordys CRMã€‚\n\nåœ¨æ— æ³•è”ç½‘çš„ç¯å¢ƒä¸­ï¼Œè¿˜å¯ä»¥é€šè¿‡ [ç¦»çº¿å®‰è£…åŒ…](https://cordys.cn/docs/installation/offline_installtion/) æ¥å®‰è£…éƒ¨ç½² Cordys CRMã€‚\n\n### è®¿é—®æ–¹å¼\n\n- åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: http://<ä½ çš„æœåŠ¡å™¨IP>:8081/\n- ç”¨æˆ·å: `admin`\n- å¯†ç : `CordysCRM`\n\n### è”ç³»æˆ‘ä»¬\n\nå®‰è£…å®Œæˆåï¼Œå¯ä»¥å‚è€ƒ [åœ¨çº¿æ–‡æ¡£](https://cordys.cn/docs/) æ¥ä½¿ç”¨ Cordys CRMã€‚\n\nä½ å¯ä»¥é€šè¿‡ä¸‹æ–¹çš„å¾®ä¿¡äº¤æµç¾¤ï¼Œä¸ Cordys CRM å¼€æºé¡¹ç›®ç»„è¿›è¡Œäº¤æµå’Œåé¦ˆã€‚\n\n<image height=\"150px\" width=\"150px\" alt=\"Cordys CRM QRCode\" src=\"https://resource.fit2cloud.com/1panel/cordys-crm/img/wechat.png?v=20250904\" />\n\n## UI å±•ç¤º\n\n<table style=\"border-collapse: collapse; border: 1px solid black;\">\n  <tr>\n    <td style=\"padding: 5px;background-color:#fff;\"><img src= \"https://resource.fit2cloud.com/1panel/cordys-crm/img/setting.png\" alt=\"Settings\" /></td>\n    <td style=\"padding: 5px;background-color:#fff;\"><img src= \"https://resource.fit2cloud.com/1panel/cordys-crm/img/rbac.png\" alt=\"RBAC\" /></td>\n  </tr>\n  <tr>\n    <td style=\"padding: 5px;background-color:#fff;\"><img src= \"https://resource.fit2cloud.com/1panel/cordys-crm/img/opportunity.png\" alt=\"Opportunity List\" /></td>\n    <td style=\"padding: 5px;background-color:#fff;\"><img src= \"https://resource.fit2cloud.com/1panel/cordys-crm/img/opportunity-detail.png\" alt=\"Opportunity Detail\" /></td>\n  </tr>\n  <tr>\n    <td style=\"padding: 5px;background-color:#fff;\"><img src= \"https://resource.fit2cloud.com/1panel/cordys-crm/img/bi.png\" alt=\"BI\" /></td>\n    <td style=\"padding: 5px;background-color:#fff;\"><img src= \"https://resource.fit2cloud.com/1panel/cordys-crm/img/ai.png\" alt=\"AI\" /></td>\n  </tr>  \n</table>\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=1Panel-dev/CordysCRM&type=date&legend=top-left)](https://www.star-history.com/#1Panel-dev/CordysCRM&type=date&legend=top-left)\n\n## Roadmap\n\n- [x] 2024.09ï¼šå†™ä¸‹ç¬¬ä¸€è¡Œä»£ç \n- [x] 2025.06ï¼šv1.0 å¼€å‘å®Œæˆ\n- [x] 2025.07ï¼šåƒè‡ªå·±çš„ç‹—ç²®ï¼ŒæˆåŠŸæ›¿æ¢é£è‡´äº‘ä½¿ç”¨ 7 å¹´çš„ Salesforce CRM\n- [x] 2025.08ï¼šå®Œæˆä¸ SQLBot å’Œ DataEase çš„å¯¹æ¥\n- [x] 2025.08.27ï¼š[v1.1.5](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.1.5) å‘å¸ƒï¼Œå¼€å§‹å…¬æµ‹\n- [x] 2025.08.27ï¼š[v1.1.6](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.1.6) å‘å¸ƒ\n- [x] 2025.09.01ï¼š[v1.1.7](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.1.7) å‘å¸ƒ\n- [x] 2025.09.05ï¼š[v1.1.8](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.1.8) å‘å¸ƒ\n- [x] 2025.09.12ï¼š[v1.1.9](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.1.9) å‘å¸ƒ\n- [x] 2025.09.19ï¼š[v1.2.0](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.2.0) å‘å¸ƒï¼Œå¼€æ”¾ MCP Serverï¼Œå¹¶å®Œæˆå’Œ MaxKB çš„å¯¹æ¥\n- [x] 2025.09.26ï¼š[v1.2.1](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.2.1) å‘å¸ƒ\n- [x] 2025.10.11ï¼š[v1.2.2](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.2.2) å‘å¸ƒ\n- [x] 2025.10.17ï¼š[v1.2.3](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.2.3) å‘å¸ƒ\n- [x] 2025.11.03ï¼š[v1.3.0](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.3.0) å‘å¸ƒï¼Œä»£ç æ­£å¼å¼€æº\n- [x] 2025.11.05ï¼š[v1.3.1](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.3.1) å‘å¸ƒ\n- [x] 2025.11.14ï¼š[v1.3.2](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.3.2) å‘å¸ƒ\n- [x] 2025.11.21ï¼š[v1.3.3](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.3.3) å‘å¸ƒ\n- [x] 2025.11.28ï¼š[v1.3.4](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.3.4) å‘å¸ƒ\n- [x] 2025.12.04ï¼š[v1.3.5](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.3.5) å‘å¸ƒ\n- [x] 2025.12.12ï¼š[v1.3.6](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.3.6) å‘å¸ƒ\n- [x] 2025.12.18ï¼š[v1.4.0 æ ‡è®¯ï¼ˆé›†æˆå¤§å•ç½‘ï¼‰æ¨¡å—ä¸Šçº¿ã€åˆåŒæ¨¡å—ä¸Šçº¿](https://github.com/1Panel-dev/CordysCRM/releases/tag/v1.4.0)\n\n## æŠ€æœ¯æ ˆ\n\n-  AI æ™ºèƒ½ä½“èƒ½åŠ›ï¼š[MaxKB](https://github.com/1Panel-dev/MaxKB)\n-  AI æ™ºèƒ½é—®æ•°èƒ½åŠ›ï¼š[SQLBot](https://github.com/dataease/SQLBot)\n-  æ•°æ®å¯è§†åŒ–èƒ½åŠ›ï¼š[DataEase](https://github.com/dataease/dataease)\n-  åç«¯ï¼š[Spring Boot](https://spring.io/projects/spring-boot)\n-  å‰ç«¯ï¼š[Vue.js](https://vuejs.org/) ã€[Naive-UI](https://www.naiveui.com/) ã€[Vant-UI](https://vant-ui.github.io/)\n-  ä¸­é—´ä»¶ï¼š[MySQL](https://www.mysql.com/) , [Redis](https://redis.com/)\n-  åŸºç¡€è®¾æ–½ï¼š[Docker](https://www.docker.com/)\n\n## é£è‡´äº‘æ——ä¸‹çš„å…¶ä»–æ˜æ˜Ÿé¡¹ç›®\n\n- [JumpServer](https://github.com/jumpserver/jumpserver/) - å¹¿å—æ¬¢è¿çš„å¼€æºå ¡å’æœº\n- [1Panel](https://github.com/1panel-dev/1panel/) - ç°ä»£åŒ–ã€å¼€æºçš„ Linux æœåŠ¡å™¨è¿ç»´ç®¡ç†é¢æ¿\n- [MaxKB](https://github.com/1panel-dev/MaxKB/) - å¼ºå¤§æ˜“ç”¨çš„ä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°\n- [DataEase](https://github.com/dataease/dataease/) - äººäººå¯ç”¨çš„å¼€æº BI å·¥å…·\n- [SQLBot](https://github.com/dataease/SQLBot/) - åŸºäºå¤§æ¨¡å‹å’Œ RAG çš„æ™ºèƒ½é—®æ•°ç³»ç»Ÿ\n- [MeterSphere](https://github.com/metersphere/metersphere/) - æ–°ä¸€ä»£çš„å¼€æºæŒç»­æµ‹è¯•å·¥å…·\n- [Halo](https://github.com/halo-dev/halo/) - å¼ºå¤§æ˜“ç”¨çš„å¼€æºå»ºç«™å·¥å…·\n\n## License\n\næœ¬ä»“åº“éµå¾ª [FIT2CLOUD Open Source License](LICENSE) å¼€æºåè®®ï¼Œè¯¥è®¸å¯è¯æœ¬è´¨ä¸Šæ˜¯ GPLv3ï¼Œä½†æœ‰ä¸€äº›é¢å¤–çš„é™åˆ¶ã€‚\n\nä½ å¯ä»¥åŸºäº Cordys CRM çš„æºä»£ç è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œä½†æ˜¯éœ€è¦éµå®ˆä»¥ä¸‹è§„å®šï¼š\n\n- ä¸èƒ½æ›¿æ¢å’Œä¿®æ”¹ Cordys CRM çš„ Logo å’Œç‰ˆæƒä¿¡æ¯ï¼›\n- äºŒæ¬¡å¼€å‘åçš„è¡ç”Ÿä½œå“å¿…é¡»éµå®ˆ GPL V3 çš„å¼€æºä¹‰åŠ¡ã€‚\n\nå¦‚éœ€å•†ä¸šæˆæƒï¼Œè¯·è”ç³»ï¼š`support@fit2cloud.com`ã€‚\n",
      "stars_today": 4
    },
    {
      "id": 591430510,
      "name": "CFScanner",
      "full_name": "MortezaBashsiz/CFScanner",
      "description": "Cloudflare scanner",
      "html_url": "https://github.com/MortezaBashsiz/CFScanner",
      "stars": 1574,
      "forks": 276,
      "language": "Kotlin",
      "topics": [],
      "created_at": "2023-01-20T18:34:08Z",
      "updated_at": "2026-01-27T13:03:23Z",
      "pushed_at": "2025-08-07T13:49:01Z",
      "open_issues": 33,
      "owner": {
        "login": "MortezaBashsiz",
        "avatar_url": "https://avatars.githubusercontent.com/u/19620677?v=4"
      },
      "readme": "# CloudFlare Scanner\nThis script scans Millions of Cloudflare IP addresses and generates a result file containing the IPs which are work with CDN\n\nThis script uses v2ray+vmess+websocket+tls by default and if you want to use it behind your Cloudflare proxy then you have to set up a vmess account, otherwise, it will use the default configuration.\n\n\n\n\n\n# DONATE\n\nYou can donate us with\n\nBuy me a coffee [here](https://buymeacoffee.com/Bashsiz \"here\")\n\nOr with crypto\n```shell\nTether: TRasnfQrKdZ2dNZsPxJ2oyxSw9Mj1z3XVS\n```\n```shell\nDogecoin: D9eKyR4c2vymXaF1pfZqgSE4meBj4JGfbk\n```\n\n## Bash\n\nFor Bash find out [here](https://github.com/MortezaBashsiz/CFScanner/tree/main/bash \"here\").\n\n## Docker\n\nThis script also exists in docker, find out [here](https://github.com/MortezaBashsiz/CFScanner/tree/main/docker \"here\").\n\n## Windows\n\nThis script is also available for windows [here](https://github.com/MortezaBashsiz/CFScanner/tree/main/windows \"here\").\n\n## Python\n\nThis script is also available for python3 [here](https://github.com/MortezaBashsiz/CFScanner/tree/main/python \"here\").\n\n## Golang\n\nThis script is also available for golang [here](https://github.com/MortezaBashsiz/CFScanner/tree/main/golang \"here\").\n\n## Android\n\nFind for android [here](https://github.com/MortezaBashsiz/CFScanner/tree/main/android \"here\").\n\n## Video Guide\nYou can find a video guide for this script on [youtube](https://youtu.be/BKLRAHolhvM \"youtube\") and [youtybe](https://youtu.be/4xJvWYdGuV8 \"youtube\") and [youtube](https://youtu.be/cgV6uPBty90 \"youtube\").\n",
      "stars_today": 4
    },
    {
      "id": 938856534,
      "name": "LLD",
      "full_name": "adityatandon15/LLD",
      "description": "All Code of LLD Playlist",
      "html_url": "https://github.com/adityatandon15/LLD",
      "stars": 612,
      "forks": 423,
      "language": "Java",
      "topics": [],
      "created_at": "2025-02-25T15:59:36Z",
      "updated_at": "2026-01-27T18:45:31Z",
      "pushed_at": "2025-07-23T05:02:23Z",
      "open_issues": 7,
      "owner": {
        "login": "adityatandon15",
        "avatar_url": "https://avatars.githubusercontent.com/u/40351467?v=4"
      },
      "readme": "# LLD\nAll Codes and Notes for System Design (LLD) Playlist of Coder Army.\nAbsolutely Free on Coder Army Youtube Channel. \n\nPlaylist Link : https://youtube.com/playlist?list=PLQEaRBV9gAFvzp6XhcNFpk1WdOcyVo9qT&si=Ihevlxfa3nczoXp0\n",
      "stars_today": 4
    },
    {
      "id": 962090208,
      "name": "rocm-systems",
      "full_name": "ROCm/rocm-systems",
      "description": "super repo for rocm systems projects",
      "html_url": "https://github.com/ROCm/rocm-systems",
      "stars": 226,
      "forks": 118,
      "language": "C++",
      "topics": [],
      "created_at": "2025-04-07T16:23:38Z",
      "updated_at": "2026-01-28T00:45:21Z",
      "pushed_at": "2026-01-28T02:08:13Z",
      "open_issues": 921,
      "owner": {
        "login": "ROCm",
        "avatar_url": "https://avatars.githubusercontent.com/u/21157610?v=4"
      },
      "readme": "# ROCm Systems\n\nWelcome to the ROCm Systems super-repo. This repository consolidates multiple ROCm systems projects into a single repository to streamline development, CI, and integration. The first set of projects focuses on requirements for building PyTorch.\n\n# Super-repo Status and CI Health\n\nThis table provides the current status of the migration of specific ROCm systems projects as well as a pointer to their current CI health.\n\n**Key:**\n- **Completed**: Fully migrated and integrated. This super-repo should be considered the source of truth for this project. The old repo may still be used for release activities.\n- **In Progress**: Ongoing migration, tests, or integration. Please refrain from submitting new pull requests on the individual repo of the project, and develop on the super-repo.\n- **Pending**: Not yet started or in the early planning stages. The individual repo should be considered the source of truth for this project.\n\n| Component              | Source of Truth | Migration Status | Azure CI Status                       | Component CI Status                   |\n|------------------------|-----------------|------------------|---------------------------------------|---------------------------------------|\n| `amdsmi`               | EMU             | Pending          |                                       |                                       |\n| `aqlprofile`           | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Faqlprofile?repoName=ROCm%2Frocm-systems&branchName=refs%2Fpull%2F368%2Fmerge)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=365&repoName=ROCm%2Frocm-systems&branchName=refs%2Fpull%2F368%2Fmerge) | [![CodeQL](https://github.com/ROCm/rocm-systems/actions/workflows/aqlprofile-codeql.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/aqlprofile-codeql.yml) <br> [![Continuous Integration](https://github.com/ROCm/rocm-systems/actions/workflows/aqlprofile-continuous_integration.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/aqlprofile-continuous_integration.yml) |\n| `clr`                  | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Fhip-clr?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=335&repoName=ROCm%2Frocm-systems&branchName=develop) |                                       |\n| `hip`                  | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Fhip-clr?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=335&repoName=ROCm%2Frocm-systems&branchName=develop) |                                       |\n| `hipother`             | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Fhip-clr?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=335&repoName=ROCm%2Frocm-systems&branchName=develop) |                                       |\n| `hip-tests`            | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Fhip-tests?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=362&repoName=ROCm%2Frocm-systems&branchName=develop) |                                       |\n| `rdc`                  | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Frdc?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=360&repoName=ROCm%2Frocm-systems&branchName=develop) |                                       |\n| `rocm-core`            | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Frocm-core?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=349&repoName=ROCm%2Frocm-systems&branchName=develop) |                                       |\n| `rocminfo`             | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Frocminfo?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=356&repoName=ROCm%2Frocm-systems&branchName=develop) |                                       |\n| `rocm-smi-lib`         | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Frocm-smi-lib?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=358&repoName=ROCm%2Frocm-systems&branchName=develop) |                                       |\n| `rocprofiler`          | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Frocprofiler?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=329&repoName=ROCm%2Frocm-systems&branchName=develop) |                                       |\n| `rocprofiler-compute`  | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Frocprofiler-compute?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=344&repoName=ROCm%2Frocm-systems&branchName=develop) | [![Formatting](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-compute-formatting.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-compute-formatting.yml) <br> [![ rhel-8](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-compute-rhel-8.yml/badge.svg)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-compute-rhel-8.yml) <br> [![tarball](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-compute-tarball.yml/badge.svg)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-compute-tarball.yml) <br> [![ubuntu jammy](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-compute-ubuntu-jammy.yml/badge.svg)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-compute-ubuntu-jammy.yml) |\n| `rocprofiler-register` | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Frocprofiler-register?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=327&repoName=ROCm%2Frocm-systems&branchName=develop) | [![Continuous Integration](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-register-continuous-integration.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-register-continuous-integration.yml) |\n| `rocprofiler-sdk`      | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Frocprofiler-sdk?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=347&repoName=ROCm%2Frocm-systems&branchName=develop) | [![Code Coverage Integration](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-code_coverage.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-code_coverage.yml) <br> [![CodeQL](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-codeql.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-codeql.yml) <br> [![Continuous Integration](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-continuous_integration.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-continuous_integration.yml) <br> [![Documentation](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-docs.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-docs.yml) <br> [![Formatting](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-formatting.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-formatting.yml) <br> [![Python Linting](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-python.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-python.yml) <br> [![Restrictions](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-restrictions.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-restrictions.yml) <br> [![Release Compatibility](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-rocm_release_compatibility.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-sdk-rocm_release_compatibility.yml) |\n| `rocprofiler-systems`  | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Frocprofiler-systems?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=345&repoName=ROCm%2Frocm-systems&branchName=develop) | [![Containers](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-containers.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-containers.yml) <br> [![rocprofiler-systems GHCR Packages for CI Images](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-ghcr.yml/badge.svg)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-ghcr.yml) <br> [![CPack](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-cpack.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-cpack.yml) <br> [![Formatting](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-formatting.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-formatting.yml) <br> [![OpenSUSE](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-opensuse.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-opensuse.yml) <br> [![Python Linting](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-python.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-python.yml) <br> [![RedHat Linux](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-redhat.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-redhat.yml) <br> [![Ubuntu Jammy](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-ubuntu-jammy.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-ubuntu-jammy.yml) <br> [![Ubuntu Noble](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-ubuntu-noble.yml/badge.svg?branch=develop)](https://github.com/ROCm/rocm-systems/actions/workflows/rocprofiler-systems-ubuntu-noble.yml) |\n| `rocr-runtime`         | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Frocr-runtime?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=354&repoName=ROCm%2Frocm-systems&branchName=develop) |                                       |\n| `roctracer`            | Public          | Completed        | [![Azure Pipelines](https://dev.azure.com/ROCm-CI/ROCm-CI/_apis/build/status%2Fmonorepo%2Froctracer?repoName=ROCm%2Frocm-systems&branchName=develop)](https://dev.azure.com/ROCm-CI/ROCm-CI/_build/latest?definitionId=331&repoName=ROCm%2Frocm-systems&branchName=develop) |                                       |\n\n\n## Tentative migration schedule\n\n| Component              | Tentative Date |\n|------------------------|----------------|\n\n\n*Remaining schedule to be determined.\n\n# TheRock CI Status\n\nNote TheRock CI performs multi-component testing on top of builds leveraging [TheRock](https://github.com/ROCm/TheRock) build system.\n\n[![The Rock CI](https://github.com/ROCm/rocm-systems/actions/workflows/therock-ci.yml/badge.svg?branch%3Adevelop+event%3Apush)](https://github.com/ROCm/rocm-systems/actions/workflows/therock-ci.yml?query=branch%3Adevelop+event%3Apush)\n\n---\n\n## Nomenclature\n\nProject names have been standardized to match the casing and punctuation of released packages. This removes inconsistent camel-casing and underscores used in legacy repositories.\n\n## Structure\n\nThe repository is organized as follows:\n\n```\nprojects/\n  amdsmi/\n  aqlprofile/\n  clr/\n  hip/\n  hipother/\n  hip-tests/\n  rccl/\n  rdc/\n  rocm-core\n  rocminfo/\n  rocmsmilib/\n  rocprofiler/\n  rocprofiler-compute/\n  rocprofiler-register/\n  rocprofiler-sdk/\n  rocprofiler-systems/\n  rocrruntime/\n  rocshmem/\n  roctracer/\n```\n\n- Each folder under `projects/` corresponds to a ROCm systems project that was previously maintained in a standalone GitHub repository and released as distinct packages.\n- Each folder under `shared/` contains code that existed in its own repository and is used as a dependency by multiple projects, but does not produce its own distinct packages in previous ROCm releases.\n\n## Goals\n\n- Enable unified build and test workflows across ROCm libraries.\n- Facilitate shared tooling, CI, and contributor experience.\n- Improve integration, visibility, and collaboration across ROCm library teams.\n\n## Getting Started\n\nTo begin contributing or building, see the [CONTRIBUTING.md](./CONTRIBUTING.md) guide. It includes setup instructions, sparse-checkout configuration, development workflow, and pull request guidelines.\n\n## License\n\nThis super-repo contains multiple subprojects, each of which retains the license under which it was originally published.\n\nğŸ“ Refer to the `LICENSE`, `LICENSE.md`, or `LICENSE.txt` file within each `projects/` or `shared/` directory for specific license terms.\nğŸ“„ Refer to the header notice in individual files outside `projects/` or `shared/` folders for their specific license terms.\n\n> **Note**: The root of this repository does not define a unified license across all components.\n\n## Questions or Feedback?\n\n- ğŸ’¬ [Start a discussion](https://github.com/ROCm/rocm-systems/discussions)\n- ğŸ [Open an issue](https://github.com/ROCm/rocm-systems/issues)\n\nWe're happy to help!\n",
      "stars_today": 4
    },
    {
      "id": 33486016,
      "name": "Kingfisher",
      "full_name": "onevcat/Kingfisher",
      "description": "A lightweight, pure-Swift library for downloading and caching images from the web.",
      "html_url": "https://github.com/onevcat/Kingfisher",
      "stars": 24247,
      "forks": 2746,
      "language": "Swift",
      "topics": [
        "cache",
        "filters",
        "image",
        "image-processor",
        "ios",
        "kingfisher",
        "macos",
        "swift",
        "xcode"
      ],
      "created_at": "2015-04-06T14:26:21Z",
      "updated_at": "2026-01-27T16:07:16Z",
      "pushed_at": "2026-01-25T13:35:34Z",
      "open_issues": 175,
      "owner": {
        "login": "onevcat",
        "avatar_url": "https://avatars.githubusercontent.com/u/1019875?v=4"
      },
      "readme": "<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/onevcat/Kingfisher/master/images/logo.png\" alt=\"Kingfisher\" title=\"Kingfisher\" width=\"557\"/>\n</p>\n\n<p align=\"center\">\n<a href=\"https://github.com/onevcat/Kingfisher/actions?query=workflow%3Abuild\"><img src=\"https://github.com/onevcat/kingfisher/workflows/build/badge.svg?branch=master\"></a>\n<a href=\"https://swiftpackageindex.com/onevcat/Kingfisher/master/documentation/kingfisher\"><img src=\"https://img.shields.io/badge/Swift-Doc-DE5C43.svg?style=flat\"></a>\n<a href=\"https://cocoapods.org/pods/Kingfisher\"><img src=\"https://img.shields.io/github/v/tag/onevcat/Kingfisher.svg?color=blue&include_prereleases=&sort=semver\"></a>\n<a href=\"https://swift.org/package-manager/\"><img src=\"https://img.shields.io/badge/SPM-supported-DE5C43.svg?style=flat\"></a>\n<a href=\"https://raw.githubusercontent.com/onevcat/Kingfisher/master/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-black\"></a>\n</p>\n\nKingfisher is a powerful, pure-Swift library for downloading and caching images from the web. It provides you a chance to use a pure-Swift way to work with remote images in your next app.\n\n## Features\n\n- [x] Asynchronous image downloading and caching.\n- [x] Loading image from either `URLSession`-based networking or local provided data.\n- [x] Useful image processors and filters provided.\n- [x] Multiple-layer hybrid cache for both memory and disk.\n- [x] Fine control on cache behavior. Customizable expiration date and size limit.\n- [x] Cancelable downloading and auto-reusing previous downloaded content to improve performance.\n- [x] Independent components. Use the downloader, caching system, and image processors separately as you need.\n- [x] Prefetching images and showing them from the cache to boost your app.\n- [x] Extensions for `UIImageView`, `NSImageView`, `NSButton`, `UIButton`, `NSTextAttachment`, `WKInterfaceImage`, `TVMonogramView` and `CPListItem` to directly set an image from a URL.\n- [x] Built-in transition animation when setting images.\n- [x] Customizable placeholder and indicator while loading images.\n- [x] Extensible image processing and image format easily.\n- [x] Low Data Mode support.\n- [x] SwiftUI support.\n- [x] Swift 6 & Swift Concurrency (strict mode) prepared.\n- [x] Load & cache for Live Photo.\n\n### Kingfisher 101\n\nThe simplest use-case is setting an image to an image view with the `UIImageView` extension:\n\n```swift\nimport Kingfisher\n\nlet url = URL(string: \"https://example.com/image.png\")\nimageView.kf.setImage(with: url)\n```\n\nKingfisher will download the image from `url`, send it to both memory cache and disk cache, and display it in `imageView`. \nWhen you set it with the same URL later, the image will be retrieved from the cache and shown immediately.\n\nIt also works if you use SwiftUI:\n\n```swift\nvar body: some View {\n    KFImage(URL(string: \"https://example.com/image.png\")!)\n}\n```\n\n### A More Advanced Example\n\nWith the powerful options, you can do hard tasks with Kingfisher in a simple way. For example, the code below: \n\n1. Downloads a high-resolution image.\n2. Downsamples it to match the image view size.\n3. Makes it round cornered with a given radius.\n4. Shows a system indicator and a placeholder image while downloading.\n5. When prepared, it animates the small thumbnail image with a \"fade in\" effect. \n6. The original large image is also cached to disk for later use, to get rid of downloading it again in a detail view.\n7. A console log is printed when the task finishes, either for success or failure.\n\n```swift\nlet url = URL(string: \"https://example.com/high_resolution_image.png\")\nlet processor = DownsamplingImageProcessor(size: imageView.bounds.size)\n             |> RoundCornerImageProcessor(cornerRadius: 20)\nimageView.kf.indicatorType = .activity\nimageView.kf.setImage(\n    with: url,\n    placeholder: UIImage(named: \"placeholderImage\"),\n    options: [\n        .processor(processor),\n        .scaleFactor(UIScreen.main.scale),\n        .transition(.fade(1)),\n        .cacheOriginalImage\n    ])\n{\n    result in\n    switch result {\n    case .success(let value):\n        print(\"Task done for: \\(value.source.url?.absoluteString ?? \"\")\")\n    case .failure(let error):\n        print(\"Job failed: \\(error.localizedDescription)\")\n    }\n}\n```\n\nIt is a common situation I can meet in my daily work. Think about how many lines you need to write without\nKingfisher!\n\n### Method Chaining\n\nIf you are not a fan of the `kf` extension, you can also prefer to use the `KF` builder and chained the method \ninvocations. The code below is doing the same thing:\n\n```swift\n// Use `kf` extension\nimageView.kf.setImage(\n    with: url,\n    placeholder: placeholderImage,\n    options: [\n        .processor(processor),\n        .loadDiskFileSynchronously,\n        .cacheOriginalImage,\n        .transition(.fade(0.25)),\n        .lowDataMode(.network(lowResolutionURL))\n    ],\n    progressBlock: { receivedSize, totalSize in\n        // Progress updated\n    },\n    completionHandler: { result in\n        // Done\n    }\n)\n\n// Use `KF` builder\nKF.url(url)\n  .placeholder(placeholderImage)\n  .setProcessor(processor)\n  .loadDiskFileSynchronously()\n  .cacheMemoryOnly()\n  .fade(duration: 0.25)\n  .lowDataModeSource(.network(lowResolutionURL))\n  .onProgress { receivedSize, totalSize in  }\n  .onSuccess { result in  }\n  .onFailure { error in }\n  .set(to: imageView)\n```\n\nAnd even better, if later you want to switch to SwiftUI, just change the `KF` above to `KFImage`, and you've done:\n\n```swift\nstruct ContentView: View {\n    var body: some View {\n        KFImage.url(url)\n          .placeholder(placeholderImage)\n          .setProcessor(processor)\n          .loadDiskFileSynchronously()\n          .cacheMemoryOnly()\n          .fade(duration: 0.25)\n          .lowDataModeSource(.network(lowResolutionURL))\n          .onProgress { receivedSize, totalSize in  }\n          .onSuccess { result in  }\n          .onFailure { error in }\n    }\n}\n```\n\n## Requirements\n\n### Kingfisher 8.0\n\n- (UIKit/AppKit) iOS 13.0+ / macOS 10.15+ / tvOS 13.0+ / watchOS 6.0+ / visionOS 1.0+\n- (SwiftUI) iOS 14.0+ / macOS 11.0+ / tvOS 14.0+ / watchOS 7.0+ / visionOS 1.0+\n- Swift 5.9+\n\n### Kingfisher 7.0\n\n- (UIKit/AppKit) iOS 12.0+ / macOS 10.14+ / tvOS 12.0+ / watchOS 5.0+ / visionOS 1.0+\n- (SwiftUI) iOS 14.0+ / macOS 11.0+ / tvOS 14.0+ / watchOS 7.0+ / visionOS 1.0+\n- Swift 5.0+\n\n### Installation\n\nRefer to one of the following tutorials to install and use the framework:\n\n- [UIKit Tutorial](https://swiftpackageindex.com/onevcat/kingfisher/master/tutorials/kingfisher/gettingstarteduikit)\n- [SwiftUI Tutorial](https://swiftpackageindex.com/onevcat/kingfisher/master/tutorials/kingfisher/gettingstartedswiftui)\n\nAlternatively, you can follow either of the methods below.\n\n#### Swift Package Manager\n\n- File > Swift Packages > Add Package Dependency\n- Add `https://github.com/onevcat/Kingfisher.git`\n- Select \"Up to Next Major\" with \"8.0.0\"\n\n#### CocoaPods\n\n```ruby\nsource 'https://github.com/CocoaPods/Specs.git'\nplatform :ios, '13.0'\nuse_frameworks!\n\ntarget 'MyApp' do\n  pod 'Kingfisher', '~> 8.0'\nend\n```\n\n#### Pre-built Framework\n\n1. Open the release page, download the latest version of Kingfisher from the assets section. \n2. Drag the `Kingfisher.xcframework` into your project and add it to the target (usually the app target).\n3. Select your target, in the \"General\" Tab, find the \"Frameworks, Libraries, and Embedded Content\" section, set the `Embed Without Signing` to Kingfisher.\n\n## Documentation\n\nCheck the documentation and tutorials:\n\n- [Documentation Home](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher)\n- [Getting Started](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher/gettingstarted)\n    - [UIKit Tutorial](https://swiftpackageindex.com/onevcat/kingfisher/master/tutorials/kingfisher/gettingstarteduikit)\n    - [SwiftUI Tutorial](https://swiftpackageindex.com/onevcat/kingfisher/master/tutorials/kingfisher/gettingstartedswiftui)\n- [Common Tasks - General](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher/commontasks)\n    - [Common Tasks - Cache](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher/commontasks_cache)\n    - [Common Tasks - Downloader](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher/commontasks_downloader)\n    - [Common tasks - Processor](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher/commontasks_processor)\n\n### Migrating\n\n- [Kingfisher 8.0 Migration](https://swiftpackageindex.com/onevcat/kingfisher/master/documentation/kingfisher/migration-to-8)\n- [Kingfisher 7.0 Migration](https://github.com/onevcat/Kingfisher/wiki/Kingfisher-7.0-Migration-Guide)\n\nIf you are using an even earlier version, see the guides below to know the steps for migrating.\n\n## Other\n\n### Future of Kingfisher\n\nI want to keep Kingfisher lightweight. This framework focuses on providing a simple solution for downloading and caching images. This doesnâ€™t mean the framework canâ€™t be improved. Kingfisher is far from perfect, so necessary and useful updates will be made to make it better.\n\n### Developments and Tests\n\nAny contributing and pull requests are warmly welcome. However, before you plan to implement some features or try to fix an uncertain issue, it is recommended to open a discussion first. It would be appreciated if your pull requests could build with all tests green. :)\n\n### About the logo\n\nThe logo of Kingfisher is inspired by [Tangram (ä¸ƒå·§æ¿)](http://en.wikipedia.org/wiki/Tangram), a dissection puzzle consisting of seven flat shapes from China. I believe she's a kingfisher bird instead of a swift, but someone insists that she is a pigeon. I guess I should give her a name. Hi, guys, do you have any suggestions?\n\n### Contact\n\nFollow and contact me on [Twitter](http://twitter.com/onevcat) or [Sina Weibo](http://weibo.com/onevcat). If you find an issue, [open a ticket](https://github.com/onevcat/Kingfisher/issues/new). Pull requests are warmly welcome as well.\n\n## Backers & Sponsors\n\nOpen-source projects cannot live long without your help. If you find Kingfisher to be useful, please consider supporting this \nproject by becoming a sponsor. Your user icon or company logo shows up [on my blog](https://onevcat.com/tabs/about/) with a link to your home page. \n\nBecome a sponsor through [GitHub Sponsors](https://github.com/sponsors/onevcat). :heart:\n\nSpecial thanks to:\n\n[![imgly](https://user-images.githubusercontent.com/1812216/106253726-271ed000-6218-11eb-98e0-c9c681925770.png)](https://img.ly/)\n\n[![emergetools](https://github-production-user-asset-6210df.s3.amazonaws.com/1019875/254794187-d44f6f50-993f-42e3-b79c-960f69c4adc1.png)](https://www.emergetools.com)\n\n\n\n### License\n\nKingfisher is released under the MIT license. See LICENSE for details.\n",
      "stars_today": 3
    },
    {
      "id": 38581626,
      "name": "pybind11",
      "full_name": "pybind/pybind11",
      "description": "Seamless operability between C++11 and Python",
      "html_url": "https://github.com/pybind/pybind11",
      "stars": 17666,
      "forks": 2263,
      "language": "C++",
      "topics": [
        "bindings",
        "python"
      ],
      "created_at": "2015-07-05T19:46:48Z",
      "updated_at": "2026-01-28T01:29:14Z",
      "pushed_at": "2026-01-21T22:58:09Z",
      "open_issues": 723,
      "owner": {
        "login": "pybind",
        "avatar_url": "https://avatars.githubusercontent.com/u/17565521?v=4"
      },
      "readme": ".. figure:: https://github.com/pybind/pybind11/raw/master/docs/pybind11-logo.png\n   :alt: pybind11 logo\n\n**pybind11 (v3)  â€” Seamless interoperability between C++ and Python**\n\n|Latest Documentation Status| |Stable Documentation Status| |Gitter chat| |GitHub Discussions|\n\n|CI| |Build status| |SPEC 4 â€” Using and Creating Nightly Wheels|\n\n|Repology| |PyPI package| |Conda-forge| |Python Versions|\n\n`Setuptools example <https://github.com/pybind/python_example>`_\nâ€¢ `Scikit-build example <https://github.com/pybind/scikit_build_example>`_\nâ€¢ `CMake example <https://github.com/pybind/cmake_example>`_\n\n.. start\n\n\n**pybind11** is a lightweight header-only library that exposes C++ types\nin Python and vice versa, mainly to create Python bindings of existing\nC++ code. Its goals and syntax are similar to the excellent\n`Boost.Python <http://www.boost.org/doc/libs/1_58_0/libs/python/doc/>`_\nlibrary by David Abrahams: to minimize boilerplate code in traditional\nextension modules by inferring type information using compile-time\nintrospection.\n\nThe main issue with Boost.Pythonâ€”and the reason for creating such a\nsimilar projectâ€”is Boost. Boost is an enormously large and complex suite\nof utility libraries that works with almost every C++ compiler in\nexistence. This compatibility has its cost: arcane template tricks and\nworkarounds are necessary to support the oldest and buggiest of compiler\nspecimens. Now that C++11-compatible compilers are widely available,\nthis heavy machinery has become an excessively large and unnecessary\ndependency.\n\nThink of this library as a tiny self-contained version of Boost.Python\nwith everything stripped away that isn't relevant for binding\ngeneration. Without comments, the core header files only require ~4K\nlines of code and depend on Python (CPython 3.8+, PyPy, or GraalPy) and the C++\nstandard library. This compact implementation was possible thanks to some C++11\nlanguage features (specifically: tuples, lambda functions and variadic\ntemplates). Since its creation, this library has grown beyond Boost.Python in\nmany ways, leading to dramatically simpler binding code in many common\nsituations.\n\nTutorial and reference documentation is provided at\n`pybind11.readthedocs.io <https://pybind11.readthedocs.io/en/latest>`_.\nA PDF version of the manual is available\n`here <https://pybind11.readthedocs.io/_/downloads/en/latest/pdf/>`_.\nAnd the source code is always available at\n`github.com/pybind/pybind11 <https://github.com/pybind/pybind11>`_.\n\n\nCore features\n-------------\n\n\npybind11 can map the following core C++ features to Python:\n\n- Functions accepting and returning custom data structures per value,\n  reference, or pointer\n- Instance methods and static methods\n- Overloaded functions\n- Instance attributes and static attributes\n- Arbitrary exception types\n- Enumerations\n- Callbacks\n- Iterators and ranges\n- Custom operators\n- Single and multiple inheritance\n- STL data structures\n- Smart pointers with reference counting like ``std::shared_ptr``\n- Internal references with correct reference counting\n- C++ classes with virtual (and pure virtual) methods can be extended\n  in Python\n- Integrated NumPy support (NumPy 2 requires pybind11 2.12+)\n\nGoodies\n-------\n\nIn addition to the core functionality, pybind11 provides some extra\ngoodies:\n\n- CPython 3.8+, PyPy3 7.3.17+, and GraalPy 24.1+ are supported with an\n  implementation-agnostic interface (see older versions for older CPython\n  and PyPy versions).\n\n- It is possible to bind C++11 lambda functions with captured\n  variables. The lambda capture data is stored inside the resulting\n  Python function object.\n\n- pybind11 uses C++11 move constructors and move assignment operators\n  whenever possible to efficiently transfer custom data types.\n\n- It's easy to expose the internal storage of custom data types through\n  Pythons' buffer protocols. This is handy e.g.Â for fast conversion\n  between C++ matrix classes like Eigen and NumPy without expensive\n  copy operations.\n\n- pybind11 can automatically vectorize functions so that they are\n  transparently applied to all entries of one or more NumPy array\n  arguments.\n\n- Python's slice-based access and assignment operations can be\n  supported with just a few lines of code.\n\n- Everything is contained in just a few header files; there is no need\n  to link against any additional libraries.\n\n- Binaries are generally smaller by a factor of at least 2 compared to\n  equivalent bindings generated by Boost.Python. A recent pybind11\n  conversion of PyRosetta, an enormous Boost.Python binding project,\n  `reported <https://graylab.jhu.edu/Sergey/2016.RosettaCon/PyRosetta-4.pdf>`_\n  a binary size reduction of **5.4x** and compile time reduction by\n  **5.8x**.\n\n- Function signatures are precomputed at compile time (using\n  ``constexpr``), leading to smaller binaries.\n\n- With little extra effort, C++ types can be pickled and unpickled\n  similar to regular Python objects.\n\nSupported platforms & compilers\n-------------------------------\n\npybind11 is exercised in continuous integration across a range of operating\nsystems, Python versions, C++ standards, and toolchains. For an up-to-date\nview of the combinations we currently test, please see the\n`pybind11 GitHub Actions <https://github.com/pybind/pybind11/actions?query=branch%3Amaster>`_\nlogs.\n\nThe test matrix naturally evolves over time as older platforms and compilers\nfall out of use and new ones are added by the community. Closely related\nversions of a tested compiler or platform will often work as well in practice,\nbut we cannot promise to validate every possible combination. If a\nconfiguration you rely on is missing from the matrix or regresses, issues and\npull requests to extend coverage are very welcome. At the same time, we need\nto balance the size of the test matrix with the available CI resources,\nsuch as GitHub's limits on concurrent jobs under the free tier.\n\nAbout\n-----\n\nThis project was created by `Wenzel\nJakob <http://rgl.epfl.ch/people/wjakob>`_. Significant features and/or\nimprovements to the code were contributed by\nJonas Adler,\nLori A. Burns,\nSylvain Corlay,\nEric Cousineau,\nAaron Gokaslan,\nRalf Grosse-Kunstleve,\nTrent Houliston,\nAxel Huebl,\n@hulucc,\nYannick Jadoul,\nSergey Lyskov,\nJohan Mabille,\nTomasz MiÄ…sko,\nDean Moldovan,\nBen Pritchard,\nJason Rhinelander,\nBoris SchÃ¤ling,\nPim Schellart,\nHenry Schreiner,\nIvan Smirnov,\nDustin Spicuzza,\nBoris Staletic,\nEthan Steinberg,\nPatrick Stewart,\nIvor Wanders,\nand\nXiaofei Wang.\n\nWe thank Google for a generous financial contribution to the continuous\nintegration infrastructure used by this project.\n\n\nContributing\n~~~~~~~~~~~~\n\nSee the `contributing\nguide <https://github.com/pybind/pybind11/blob/master/.github/CONTRIBUTING.md>`_\nfor information on building and contributing to pybind11.\n\nLicense\n~~~~~~~\n\npybind11 is provided under a BSD-style license that can be found in the\n`LICENSE <https://github.com/pybind/pybind11/blob/master/LICENSE>`_\nfile. By using, distributing, or contributing to this project, you agree\nto the terms and conditions of this license.\n\n.. |Latest Documentation Status| image:: https://readthedocs.org/projects/pybind11/badge?version=latest\n   :target: http://pybind11.readthedocs.org/en/latest\n.. |Stable Documentation Status| image:: https://img.shields.io/badge/docs-stable-blue.svg\n   :target: http://pybind11.readthedocs.org/en/stable\n.. |Gitter chat| image:: https://img.shields.io/gitter/room/gitterHQ/gitter.svg\n   :target: https://gitter.im/pybind/Lobby\n.. |CI| image:: https://github.com/pybind/pybind11/workflows/CI/badge.svg\n   :target: https://github.com/pybind/pybind11/actions\n.. |Build status| image:: https://ci.appveyor.com/api/projects/status/riaj54pn4h08xy40?svg=true\n   :target: https://ci.appveyor.com/project/wjakob/pybind11\n.. |PyPI package| image:: https://img.shields.io/pypi/v/pybind11.svg\n   :target: https://pypi.org/project/pybind11/\n.. |Conda-forge| image:: https://img.shields.io/conda/vn/conda-forge/pybind11.svg\n   :target: https://github.com/conda-forge/pybind11-feedstock\n.. |Repology| image:: https://repology.org/badge/latest-versions/python:pybind11.svg\n   :target: https://repology.org/project/python:pybind11/versions\n.. |Python Versions| image:: https://img.shields.io/pypi/pyversions/pybind11.svg\n   :target: https://pypi.org/project/pybind11/\n.. |GitHub Discussions| image:: https://img.shields.io/static/v1?label=Discussions&message=Ask&color=blue&logo=github\n   :target: https://github.com/pybind/pybind11/discussions\n.. |SPEC 4 â€” Using and Creating Nightly Wheels| image:: https://img.shields.io/badge/SPEC-4-green?labelColor=%23004811&color=%235CA038\n   :target: https://scientific-python.org/specs/spec-0004/\n",
      "stars_today": 3
    },
    {
      "id": 7083242,
      "name": "mysql",
      "full_name": "go-sql-driver/mysql",
      "description": "Go MySQL Driver is a MySQL driver for Go's (golang) database/sql package",
      "html_url": "https://github.com/go-sql-driver/mysql",
      "stars": 15403,
      "forks": 2341,
      "language": "Go",
      "topics": [
        "database",
        "go",
        "mariadb",
        "mysql",
        "mysql-driver",
        "sql"
      ],
      "created_at": "2012-12-09T20:33:55Z",
      "updated_at": "2026-01-28T01:58:02Z",
      "pushed_at": "2025-06-13T06:21:14Z",
      "open_issues": 71,
      "owner": {
        "login": "go-sql-driver",
        "avatar_url": "https://avatars.githubusercontent.com/u/3002971?v=4"
      },
      "readme": "# Go-MySQL-Driver\n\n[![DeepWiki](https://img.shields.io/badge/DeepWiki-go--sql--driver%2Fmysql-blue.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAyCAYAAAAnWDnqAAAAAXNSR0IArs4c6QAAA05JREFUaEPtmUtyEzEQhtWTQyQLHNak2AB7ZnyXZMEjXMGeK/AIi+QuHrMnbChYY7MIh8g01fJoopFb0uhhEqqcbWTp06/uv1saEDv4O3n3dV60RfP947Mm9/SQc0ICFQgzfc4CYZoTPAswgSJCCUJUnAAoRHOAUOcATwbmVLWdGoH//PB8mnKqScAhsD0kYP3j/Yt5LPQe2KvcXmGvRHcDnpxfL2zOYJ1mFwrryWTz0advv1Ut4CJgf5uhDuDj5eUcAUoahrdY/56ebRWeraTjMt/00Sh3UDtjgHtQNHwcRGOC98BJEAEymycmYcWwOprTgcB6VZ5JK5TAJ+fXGLBm3FDAmn6oPPjR4rKCAoJCal2eAiQp2x0vxTPB3ALO2CRkwmDy5WohzBDwSEFKRwPbknEggCPB/imwrycgxX2NzoMCHhPkDwqYMr9tRcP5qNrMZHkVnOjRMWwLCcr8ohBVb1OMjxLwGCvjTikrsBOiA6fNyCrm8V1rP93iVPpwaE+gO0SsWmPiXB+jikdf6SizrT5qKasx5j8ABbHpFTx+vFXp9EnYQmLx02h1QTTrl6eDqxLnGjporxl3NL3agEvXdT0WmEost648sQOYAeJS9Q7bfUVoMGnjo4AZdUMQku50McDcMWcBPvr0SzbTAFDfvJqwLzgxwATnCgnp4wDl6Aa+Ax283gghmj+vj7feE2KBBRMW3FzOpLOADl0Isb5587h/U4gGvkt5v60Z1VLG8BhYjbzRwyQZemwAd6cCR5/XFWLYZRIMpX39AR0tjaGGiGzLVyhse5C9RKC6ai42ppWPKiBagOvaYk8lO7DajerabOZP46Lby5wKjw1HCRx7p9sVMOWGzb/vA1hwiWc6jm3MvQDTogQkiqIhJV0nBQBTU+3okKCFDy9WwferkHjtxib7t3xIUQtHxnIwtx4mpg26/HfwVNVDb4oI9RHmx5WGelRVlrtiw43zboCLaxv46AZeB3IlTkwouebTr1y2NjSpHz68WNFjHvupy3q8TFn3Hos2IAk4Ju5dCo8B3wP7VPr/FGaKiG+T+v+TQqIrOqMTL1VdWV1DdmcbO8KXBz6esmYWYKPwDL5b5FA1a0hwapHiom0r/cKaoqr+27/XcrS5UwSMbQAAAABJRU5ErkJggg==)](https://deepwiki.com/go-sql-driver/mysql)\n\n\nA MySQL-Driver for Go's [database/sql](https://golang.org/pkg/database/sql/) package\n\n![Go-MySQL-Driver logo](https://raw.github.com/wiki/go-sql-driver/mysql/gomysql_m.png \"Golang Gopher holding the MySQL Dolphin\")\n\n---------------------------------------\n  * [Features](#features)\n  * [Requirements](#requirements)\n  * [Installation](#installation)\n  * [Usage](#usage)\n    * [DSN (Data Source Name)](#dsn-data-source-name)\n      * [Password](#password)\n      * [Protocol](#protocol)\n      * [Address](#address)\n      * [Parameters](#parameters)\n      * [Examples](#examples)\n    * [Connection pool and timeouts](#connection-pool-and-timeouts)\n    * [context.Context Support](#contextcontext-support)\n    * [ColumnType Support](#columntype-support)\n    * [LOAD DATA LOCAL INFILE support](#load-data-local-infile-support)\n    * [time.Time support](#timetime-support)\n    * [Unicode support](#unicode-support)\n  * [Testing / Development](#testing--development)\n  * [License](#license)\n\n---------------------------------------\n\n## Features\n  * Lightweight and [fast](https://github.com/go-sql-driver/sql-benchmark \"golang MySQL-Driver performance\")\n  * Native Go implementation. No C-bindings, just pure Go\n  * Connections over TCP/IPv4, TCP/IPv6, Unix domain sockets or [custom protocols](https://godoc.org/github.com/go-sql-driver/mysql#DialFunc)\n  * Automatic handling of broken connections\n  * Automatic Connection Pooling *(by database/sql package)*\n  * Supports queries larger than 16MB\n  * Full [`sql.RawBytes`](https://golang.org/pkg/database/sql/#RawBytes) support.\n  * Intelligent `LONG DATA` handling in prepared statements\n  * Secure `LOAD DATA LOCAL INFILE` support with file allowlisting and `io.Reader` support\n  * Optional `time.Time` parsing\n  * Optional placeholder interpolation\n  * Supports zlib compression.\n\n## Requirements\n\n* Go 1.22 or higher. We aim to support the 3 latest versions of Go.\n* MySQL (5.7+) and MariaDB (10.5+) are supported.\n* [TiDB](https://github.com/pingcap/tidb) is supported by PingCAP.\n  * Do not ask questions about TiDB in our issue tracker or forum.\n  * [Document](https://docs.pingcap.com/tidb/v6.1/dev-guide-sample-application-golang)\n  * [Forum](https://ask.pingcap.com/)\n* go-mysql would work with Percona Server, Google CloudSQL or Sphinx (2.2.3+).\n  * Maintainers won't support them. Do not expect issues are investigated and resolved by maintainers.\n  * Investigate issues yourself and please send a pull request to fix it.\n\n---------------------------------------\n\n## Installation\nSimple install the package to your [$GOPATH](https://github.com/golang/go/wiki/GOPATH \"GOPATH\") with the [go tool](https://golang.org/cmd/go/ \"go command\") from shell:\n```bash\ngo get -u github.com/go-sql-driver/mysql\n```\nMake sure [Git is installed](https://git-scm.com/downloads) on your machine and in your system's `PATH`.\n\n## Usage\n_Go MySQL Driver_ is an implementation of Go's `database/sql/driver` interface. You only need to import the driver and can use the full [`database/sql`](https://golang.org/pkg/database/sql/) API then.\n\nUse `mysql` as `driverName` and a valid [DSN](#dsn-data-source-name)  as `dataSourceName`:\n\n```go\nimport (\n\t\"database/sql\"\n\t\"time\"\n\n\t_ \"github.com/go-sql-driver/mysql\"\n)\n\n// ...\n\ndb, err := sql.Open(\"mysql\", \"user:password@/dbname\")\nif err != nil {\n\tpanic(err)\n}\n// See \"Important settings\" section.\ndb.SetConnMaxLifetime(time.Minute * 3)\ndb.SetMaxOpenConns(10)\ndb.SetMaxIdleConns(10)\n```\n\n[Examples are available in our Wiki](https://github.com/go-sql-driver/mysql/wiki/Examples \"Go-MySQL-Driver Examples\").\n\n### Important settings\n\n`db.SetConnMaxLifetime()` is required to ensure connections are closed by the driver safely before connection is closed by MySQL server, OS, or other middlewares. Since some middlewares close idle connections by 5 minutes, we recommend timeout shorter than 5 minutes. This setting helps load balancing and changing system variables too.\n\n`db.SetMaxOpenConns()` is highly recommended to limit the number of connection used by the application. There is no recommended limit number because it depends on application and MySQL server.\n\n`db.SetMaxIdleConns()` is recommended to be set same to `db.SetMaxOpenConns()`. When it is smaller than `SetMaxOpenConns()`, connections can be opened and closed much more frequently than you expect. Idle connections can be closed by the `db.SetConnMaxLifetime()`. If you want to close idle connections more rapidly, you can use `db.SetConnMaxIdleTime()` since Go 1.15.\n\n\n### DSN (Data Source Name)\n\nThe Data Source Name has a common format, like e.g. [PEAR DB](http://pear.php.net/manual/en/package.database.db.intro-dsn.php) uses it, but without type-prefix (optional parts marked by squared brackets):\n```\n[username[:password]@][protocol[(address)]]/dbname[?param1=value1&...&paramN=valueN]\n```\n\nA DSN in its fullest form:\n```\nusername:password@protocol(address)/dbname?param=value\n```\n\nExcept for the databasename, all values are optional. So the minimal DSN is:\n```\n/dbname\n```\n\nIf you do not want to preselect a database, leave `dbname` empty:\n```\n/\n```\nThis has the same effect as an empty DSN string:\n```\n\n```\n\n`dbname` is escaped by [PathEscape()](https://pkg.go.dev/net/url#PathEscape) since v1.8.0. If your database name is `dbname/withslash`, it becomes:\n\n```\n/dbname%2Fwithslash\n```\n\nAlternatively, [Config.FormatDSN](https://godoc.org/github.com/go-sql-driver/mysql#Config.FormatDSN) can be used to create a DSN string by filling a struct.\n\n#### Password\nPasswords can consist of any character. Escaping is **not** necessary.\n\n#### Protocol\nSee [net.Dial](https://golang.org/pkg/net/#Dial) for more information which networks are available.\nIn general you should use a Unix domain socket if available and TCP otherwise for best performance.\n\n#### Address\nFor TCP and UDP networks, addresses have the form `host[:port]`.\nIf `port` is omitted, the default port will be used.\nIf `host` is a literal IPv6 address, it must be enclosed in square brackets.\nThe functions [net.JoinHostPort](https://golang.org/pkg/net/#JoinHostPort) and [net.SplitHostPort](https://golang.org/pkg/net/#SplitHostPort) manipulate addresses in this form.\n\nFor Unix domain sockets the address is the absolute path to the MySQL-Server-socket, e.g. `/var/run/mysqld/mysqld.sock` or `/tmp/mysql.sock`.\n\n#### Parameters\n*Parameters are case-sensitive!*\n\nNotice that any of `true`, `TRUE`, `True` or `1` is accepted to stand for a true boolean value. Not surprisingly, false can be specified as any of: `false`, `FALSE`, `False` or `0`.\n\n##### `allowAllFiles`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        false\n```\n\n`allowAllFiles=true` disables the file allowlist for `LOAD DATA LOCAL INFILE` and allows *all* files.\n[*Might be insecure!*](https://dev.mysql.com/doc/refman/8.0/en/load-data.html#load-data-local)\n\n##### `allowCleartextPasswords`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        false\n```\n\n`allowCleartextPasswords=true` allows using the [cleartext client side plugin](https://dev.mysql.com/doc/en/cleartext-pluggable-authentication.html) if required by an account, such as one defined with the [PAM authentication plugin](http://dev.mysql.com/doc/en/pam-authentication-plugin.html). Sending passwords in clear text may be a security problem in some configurations. To avoid problems if there is any possibility that the password would be intercepted, clients should connect to MySQL Server using a method that protects the password. Possibilities include [TLS / SSL](#tls), IPsec, or a private network.\n\n\n##### `allowFallbackToPlaintext`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        false\n```\n\n`allowFallbackToPlaintext=true` acts like a `--ssl-mode=PREFERRED` MySQL client as described in [Command Options for Connecting to the Server](https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode)\n\n##### `allowNativePasswords`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        true\n```\n`allowNativePasswords=false` disallows the usage of MySQL native password method.\n\n##### `allowOldPasswords`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        false\n```\n`allowOldPasswords=true` allows the usage of the insecure old password method. This should be avoided, but is necessary in some cases. See also [the old_passwords wiki page](https://github.com/go-sql-driver/mysql/wiki/old_passwords).\n\n##### `charset`\n\n```\nType:           string\nValid Values:   <name>\nDefault:        none\n```\n\nSets the charset used for client-server interaction (`\"SET NAMES <value>\"`). If multiple charsets are set (separated by a comma), the following charset is used if setting the charset fails. This enables for example support for `utf8mb4` ([introduced in MySQL 5.5.3](http://dev.mysql.com/doc/refman/5.5/en/charset-unicode-utf8mb4.html)) with fallback to `utf8` for older servers (`charset=utf8mb4,utf8`).\n\nSee also [Unicode Support](#unicode-support).\n\n##### `checkConnLiveness`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        true\n```\n\nOn supported platforms connections retrieved from the connection pool are checked for liveness before using them. If the check fails, the respective connection is marked as bad and the query retried with another connection.\n`checkConnLiveness=false` disables this liveness check of connections.\n\n##### `collation`\n\n```\nType:           string\nValid Values:   <name>\nDefault:        utf8mb4_general_ci\n```\n\nSets the collation used for client-server interaction on connection. In contrast to `charset`, `collation` does not issue additional queries. If the specified collation is unavailable on the target server, the connection will fail.\n\nA list of valid charsets for a server is retrievable with `SHOW COLLATION`.\n\nThe default collation (`utf8mb4_general_ci`) is supported from MySQL 5.5.  You should use an older collation (e.g. `utf8_general_ci`) for older MySQL.\n\nCollations for charset \"ucs2\", \"utf16\", \"utf16le\", and \"utf32\" can not be used ([ref](https://dev.mysql.com/doc/refman/5.7/en/charset-connection.html#charset-connection-impermissible-client-charset)).\n\nSee also [Unicode Support](#unicode-support).\n\n##### `clientFoundRows`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        false\n```\n\n`clientFoundRows=true` causes an UPDATE to return the number of matching rows instead of the number of rows changed.\n\n##### `columnsWithAlias`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        false\n```\n\nWhen `columnsWithAlias` is true, calls to `sql.Rows.Columns()` will return the table alias and the column name separated by a dot. For example:\n\n```\nSELECT u.id FROM users as u\n```\n\nwill return `u.id` instead of just `id` if `columnsWithAlias=true`.\n\n##### `compress`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        false\n```\n\nToggles zlib compression. false by default.\n\n##### `interpolateParams`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        false\n```\n\nIf `interpolateParams` is true, placeholders (`?`) in calls to `db.Query()` and `db.Exec()` are interpolated into a single query string with given parameters. This reduces the number of roundtrips, since the driver has to prepare a statement, execute it with given parameters and close the statement again with `interpolateParams=false`.\n\n*This can not be used together with the multibyte encodings BIG5, CP932, GB2312, GBK or SJIS. These are rejected as they may [introduce a SQL injection vulnerability](http://stackoverflow.com/a/12118602/3430118)!*\n\n##### `loc`\n\n```\nType:           string\nValid Values:   <escaped name>\nDefault:        UTC\n```\n\nSets the location for time.Time values (when using `parseTime=true`). *\"Local\"* sets the system's location. See [time.LoadLocation](https://golang.org/pkg/time/#LoadLocation) for details.\n\nNote that this sets the location for time.Time values but does not change MySQL's [time_zone setting](https://dev.mysql.com/doc/refman/5.5/en/time-zone-support.html). For that see the [time_zone system variable](#system-variables), which can also be set as a DSN parameter.\n\nPlease keep in mind, that param values must be [url.QueryEscape](https://golang.org/pkg/net/url/#QueryEscape)'ed. Alternatively you can manually replace the `/` with `%2F`. For example `US/Pacific` would be `loc=US%2FPacific`.\n\n##### `timeTruncate`\n\n```\nType:           duration\nDefault:        0\n```\n\n[Truncate time values](https://pkg.go.dev/time#Duration.Truncate) to the specified duration. The value must be a decimal number with a unit suffix (*\"ms\"*, *\"s\"*, *\"m\"*, *\"h\"*), such as *\"30s\"*, *\"0.5m\"* or *\"1m30s\"*.\n\n##### `maxAllowedPacket`\n```\nType:          decimal number\nDefault:       64*1024*1024\n```\n\nMax packet size allowed in bytes. The default value is 64 MiB and should be adjusted to match the server settings. `maxAllowedPacket=0` can be used to automatically fetch the `max_allowed_packet` variable from server *on every connection*.\n\n##### `multiStatements`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        false\n```\n\nAllow multiple statements in one query. This can be used to bach multiple queries. Use [Rows.NextResultSet()](https://pkg.go.dev/database/sql#Rows.NextResultSet) to get result of the second and subsequent queries.\n\nWhen `multiStatements` is used, `?` parameters must only be used in the first statement. [interpolateParams](#interpolateparams) can be used to avoid this limitation unless prepared statement is used explicitly.\n\nIt's possible to access the last inserted ID and number of affected rows for multiple statements by using `sql.Conn.Raw()` and the `mysql.Result`. For example:\n\n```go\nconn, _ := db.Conn(ctx)\nconn.Raw(func(conn any) error {\n  ex := conn.(driver.Execer)\n  res, err := ex.Exec(`\n  UPDATE point SET x = 1 WHERE y = 2;\n  UPDATE point SET x = 2 WHERE y = 3;\n  `, nil)\n  // Both slices have 2 elements.\n  log.Print(res.(mysql.Result).AllRowsAffected())\n  log.Print(res.(mysql.Result).AllLastInsertIds())\n})\n```\n\n##### `parseTime`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        false\n```\n\n`parseTime=true` changes the output type of `DATE` and `DATETIME` values to `time.Time` instead of `[]byte` / `string`\nThe date or datetime like `0000-00-00 00:00:00` is converted into zero value of `time.Time`.\n\n\n##### `readTimeout`\n\n```\nType:           duration\nDefault:        0\n```\n\nI/O read timeout. The value must be a decimal number with a unit suffix (*\"ms\"*, *\"s\"*, *\"m\"*, *\"h\"*), such as *\"30s\"*, *\"0.5m\"* or *\"1m30s\"*.\n\n##### `rejectReadOnly`\n\n```\nType:           bool\nValid Values:   true, false\nDefault:        false\n```\n\n\n`rejectReadOnly=true` causes the driver to reject read-only connections. This\nis for a possible race condition during an automatic failover, where the mysql\nclient gets connected to a read-only replica after the failover.\n\nNote that this should be a fairly rare case, as an automatic failover normally\nhappens when the primary is down, and the race condition shouldn't happen\nunless it comes back up online as soon as the failover is kicked off. On the\nother hand, when this happens, a MySQL application can get stuck on a\nread-only connection until restarted. It is however fairly easy to reproduce,\nfor example, using a manual failover on AWS Aurora's MySQL-compatible cluster.\n\nIf you are not relying on read-only transactions to reject writes that aren't\nsupposed to happen, setting this on some MySQL providers (such as AWS Aurora)\nis safer for failovers.\n\nNote that ERROR 1290 can be returned for a `read-only` server and this option will\ncause a retry for that error. However the same error number is used for some\nother cases. You should ensure your application will never cause an ERROR 1290\nexcept for `read-only` mode when enabling this option.\n\n\n##### `serverPubKey`\n\n```\nType:           string\nValid Values:   <name>\nDefault:        none\n```\n\nServer public keys can be registered with [`mysql.RegisterServerPubKey`](https://godoc.org/github.com/go-sql-driver/mysql#RegisterServerPubKey), which can then be used by the assigned name in the DSN.\nPublic keys are used to transmit encrypted data, e.g. for authentication.\nIf the server's public key is known, it should be set manually to avoid expensive and potentially insecure transmissions of the public key from the server to the client each time it is required.\n\n\n##### `timeout`\n\n```\nType:           duration\nDefault:        OS default\n```\n\nTimeout for establishing connections, aka dial timeout. The value must be a decimal number with a unit suffix (*\"ms\"*, *\"s\"*, *\"m\"*, *\"h\"*), such as *\"30s\"*, *\"0.5m\"* or *\"1m30s\"*.\n\n\n##### `tls`\n\n```\nType:           bool / string\nValid Values:   true, false, skip-verify, preferred, <name>\nDefault:        false\n```\n\n`tls=true` enables TLS / SSL encrypted connection to the server. Use `skip-verify` if you want to use a self-signed or invalid certificate (server side) or use `preferred` to use TLS only when advertised by the server. This is similar to `skip-verify`, but additionally allows a fallback to a connection which is not encrypted. Neither `skip-verify` nor `preferred` add any reliable security. You can use a custom TLS config after registering it with [`mysql.RegisterTLSConfig`](https://godoc.org/github.com/go-sql-driver/mysql#RegisterTLSConfig).\n\n\n##### `writeTimeout`\n\n```\nType:           duration\nDefault:        0\n```\n\nI/O write timeout. The value must be a decimal number with a unit suffix (*\"ms\"*, *\"s\"*, *\"m\"*, *\"h\"*), such as *\"30s\"*, *\"0.5m\"* or *\"1m30s\"*.\n\n##### `connectionAttributes`\n\n```\nType:           comma-delimited string of user-defined \"key:value\" pairs\nValid Values:   (<name1>:<value1>,<name2>:<value2>,...)\nDefault:        none\n```\n\n[Connection attributes](https://dev.mysql.com/doc/refman/8.0/en/performance-schema-connection-attribute-tables.html) are key-value pairs that application programs can pass to the server at connect time.\n\n##### System Variables\n\nAny other parameters are interpreted as system variables:\n  * `<boolean_var>=<value>`: `SET <boolean_var>=<value>`\n  * `<enum_var>=<value>`: `SET <enum_var>=<value>`\n  * `<string_var>=%27<value>%27`: `SET <string_var>='<value>'`\n\nRules:\n* The values for string variables must be quoted with `'`.\n* The values must also be [url.QueryEscape](http://golang.org/pkg/net/url/#QueryEscape)'ed!\n (which implies values of string variables must be wrapped with `%27`).\n\nExamples:\n  * `autocommit=1`: `SET autocommit=1`\n  * [`time_zone=%27Europe%2FParis%27`](https://dev.mysql.com/doc/refman/5.5/en/time-zone-support.html): `SET time_zone='Europe/Paris'`\n  * [`transaction_isolation=%27REPEATABLE-READ%27`](https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_transaction_isolation): `SET transaction_isolation='REPEATABLE-READ'`\n\n\n#### Examples\n```\nuser@unix(/path/to/socket)/dbname\n```\n\n```\nroot:pw@unix(/tmp/mysql.sock)/myDatabase?loc=Local\n```\n\n```\nuser:password@tcp(localhost:5555)/dbname?tls=skip-verify&autocommit=true\n```\n\nTreat warnings as errors by setting the system variable [`sql_mode`](https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html):\n```\nuser:password@/dbname?sql_mode=TRADITIONAL\n```\n\nTCP via IPv6:\n```\nuser:password@tcp([de:ad:be:ef::ca:fe]:80)/dbname?timeout=90s&collation=utf8mb4_unicode_ci\n```\n\nTCP on a remote host, e.g. Amazon RDS:\n```\nid:password@tcp(your-amazonaws-uri.com:3306)/dbname\n```\n\nGoogle Cloud SQL on App Engine:\n```\nuser:password@unix(/cloudsql/project-id:region-name:instance-name)/dbname\n```\n\nTCP using default port (3306) on localhost:\n```\nuser:password@tcp/dbname?charset=utf8mb4,utf8&sys_var=esc%40ped\n```\n\nUse the default protocol (tcp) and host (localhost:3306):\n```\nuser:password@/dbname\n```\n\nNo Database preselected:\n```\nuser:password@/\n```\n\n\n### Connection pool and timeouts\nThe connection pool is managed by Go's database/sql package. For details on how to configure the size of the pool and how long connections stay in the pool see `*DB.SetMaxOpenConns`, `*DB.SetMaxIdleConns`, and `*DB.SetConnMaxLifetime` in the [database/sql documentation](https://golang.org/pkg/database/sql/). The read, write, and dial timeouts for each individual connection are configured with the DSN parameters [`readTimeout`](#readtimeout), [`writeTimeout`](#writetimeout), and [`timeout`](#timeout), respectively.\n\n## `ColumnType` Support\nThis driver supports the [`ColumnType` interface](https://golang.org/pkg/database/sql/#ColumnType) introduced in Go 1.8, with the exception of [`ColumnType.Length()`](https://golang.org/pkg/database/sql/#ColumnType.Length), which is currently not supported. All Unsigned database type names will be returned `UNSIGNED ` with `INT`, `TINYINT`, `SMALLINT`, `MEDIUMINT`, `BIGINT`.\n\n## `context.Context` Support\nGo 1.8 added `database/sql` support for `context.Context`. This driver supports query timeouts and cancellation via contexts.\nSee [context support in the database/sql package](https://golang.org/doc/go1.8#database_sql) for more details.\n\n> [!IMPORTANT]\n> The `QueryContext`, `ExecContext`, etc. variants provided by `database/sql` will cause the connection to be closed if the provided context is cancelled or timed out before the result is received by the driver.\n\n\n### `LOAD DATA LOCAL INFILE` support\nFor this feature you need direct access to the package. Therefore you must change the import path (no `_`):\n```go\nimport \"github.com/go-sql-driver/mysql\"\n```\n\nFiles must be explicitly allowed by registering them with `mysql.RegisterLocalFile(filepath)` (recommended) or the allowlist check must be deactivated by using the DSN parameter `allowAllFiles=true` ([*Might be insecure!*](https://dev.mysql.com/doc/refman/8.0/en/load-data.html#load-data-local)).\n\nTo use a `io.Reader` a handler function must be registered with `mysql.RegisterReaderHandler(name, handler)` which returns a `io.Reader` or `io.ReadCloser`. The Reader is available with the filepath `Reader::<name>` then. Choose different names for different handlers and `DeregisterReaderHandler` when you don't need it anymore.\n\nSee the [godoc of Go-MySQL-Driver](https://godoc.org/github.com/go-sql-driver/mysql \"golang mysql driver documentation\") for details.\n\n\n### `time.Time` support\nThe default internal output type of MySQL `DATE` and `DATETIME` values is `[]byte` which allows you to scan the value into a `[]byte`, `string` or `sql.RawBytes` variable in your program.\n\nHowever, many want to scan MySQL `DATE` and `DATETIME` values into `time.Time` variables, which is the logical equivalent in Go to `DATE` and `DATETIME` in MySQL. You can do that by changing the internal output type from `[]byte` to `time.Time` with the DSN parameter `parseTime=true`. You can set the default [`time.Time` location](https://golang.org/pkg/time/#Location) with the `loc` DSN parameter.\n\n**Caution:** As of Go 1.1, this makes `time.Time` the only variable type you can scan `DATE` and `DATETIME` values into. This breaks for example [`sql.RawBytes` support](https://github.com/go-sql-driver/mysql/wiki/Examples#rawbytes).\n\n\n### Unicode support\nSince version 1.5 Go-MySQL-Driver automatically uses the collation ` utf8mb4_general_ci` by default.\n\nOther charsets / collations can be set using the [`charset`](#charset) or [`collation`](#collation) DSN parameter.\n\n- When only the `charset` is specified, the `SET NAMES <charset>` query is sent and the server's default collation is used.\n- When both the `charset` and `collation` are specified, the `SET NAMES <charset> COLLATE <collation>` query is sent.\n- When only the `collation` is specified, the collation is specified in the protocol handshake and the `SET NAMES` query is not sent. This can save one roundtrip, but note that the server may ignore the specified collation silently and use the server's default charset/collation instead.\n\nSee http://dev.mysql.com/doc/refman/8.0/en/charset-unicode.html for more details on MySQL's Unicode support.\n\n## Testing / Development\nTo run the driver tests you may need to adjust the configuration. See the [Testing Wiki-Page](https://github.com/go-sql-driver/mysql/wiki/Testing \"Testing\") for details.\n\nGo-MySQL-Driver is not feature-complete yet. Your help is very appreciated.\nIf you want to contribute, you can work on an [open issue](https://github.com/go-sql-driver/mysql/issues?state=open) or review a [pull request](https://github.com/go-sql-driver/mysql/pulls).\n\nSee the [Contribution Guidelines](https://github.com/go-sql-driver/mysql/blob/master/.github/CONTRIBUTING.md) for details.\n\n---------------------------------------\n\n## License\nGo-MySQL-Driver is licensed under the [Mozilla Public License Version 2.0](https://raw.github.com/go-sql-driver/mysql/master/LICENSE)\n\nMozilla summarizes the license scope as follows:\n> MPL: The copyleft applies to any files containing MPLed code.\n\n\nThat means:\n  * You can **use** the **unchanged** source code both in private and commercially.\n  * When distributing, you **must publish** the source code of any **changed files** licensed under the MPL 2.0 under a) the MPL 2.0 itself or b) a compatible license (e.g. GPL 3.0 or Apache License 2.0).\n  * You **needn't publish** the source code of your library as long as the files licensed under the MPL 2.0 are **unchanged**.\n\nPlease read the [MPL 2.0 FAQ](https://www.mozilla.org/en-US/MPL/2.0/FAQ/) if you have further questions regarding the license.\n\nYou can read the full terms here: [LICENSE](https://raw.github.com/go-sql-driver/mysql/master/LICENSE).\n\n![Go Gopher and MySQL Dolphin](https://raw.github.com/wiki/go-sql-driver/mysql/go-mysql-driver_m.jpg \"Golang Gopher transporting the MySQL Dolphin in a wheelbarrow\")\n",
      "stars_today": 3
    },
    {
      "id": 61231339,
      "name": "raygui",
      "full_name": "raysan5/raygui",
      "description": "A simple and easy-to-use immediate-mode gui library",
      "html_url": "https://github.com/raysan5/raygui",
      "stars": 4632,
      "forks": 358,
      "language": "C",
      "topics": [
        "buttons",
        "c",
        "gui",
        "imgui",
        "immediate-mode-gui",
        "open-source",
        "raylib",
        "ui-design",
        "ui-library"
      ],
      "created_at": "2016-06-15T18:33:19Z",
      "updated_at": "2026-01-28T00:57:06Z",
      "pushed_at": "2026-01-20T20:02:15Z",
      "open_issues": 26,
      "owner": {
        "login": "raysan5",
        "avatar_url": "https://avatars.githubusercontent.com/u/5766837?v=4"
      },
      "readme": "<img align=\"left\" src=\"logo/raygui_256x256.png\" width=256>\n\n**raygui is a simple and easy-to-use immediate-mode-gui library.**\n\n`raygui` was originally inspired by [Unity IMGUI](https://docs.unity3d.com/Manual/GUIScriptingGuide.html) (immediate mode GUI API).\n\n`raygui` was designed as an auxiliary module for [raylib](https://github.com/raysan5/raylib) to create simple GUI interfaces using raylib graphic style (simple colors, plain rectangular shapes, wide borders...) but it can be adapted to other engines/frameworks.\n\n`raygui` is intended for **tools development**; it has already been used to develop [multiple published tools](https://raylibtech.itch.io).\n\n<br>\n\n**WARNING: Latest `raygui` from master branch is always aligned with latest `raylib` from master branch. Make sure to use the appropriate versions.**\n\n**WARNING: Latest `raygui 4.0` is an API-BREAKING redesign from previous versions (3.x), now all functions are more consistent and coherent, you can read the details about this breaking change in issue [283](https://github.com/raysan5/raygui/issues/283)**\n\n*NOTE: raygui is a single-file header-only library (despite its internal dependency on raylib), so, functions definition AND implementation reside in the same file `raygui.h`, when including `raygui.h` in a module, `RAYGUI_IMPLEMENTATION` must be previously defined to include the implementation part of `raygui.h` BUT only in one compilation unit, other modules could also include `raygui.h` but `RAYGUI_IMPLEMENTATION` must not be defined again.*\n\n## features\n\n - **Immediate-mode gui, no retained data**\n - **+25** controls provided (basic and advanced)\n - Powerful **styling system** for colors, font and metrics\n - Standalone usage mode supported (for other graphic libs)\n - **Icons support**, embedding a complete 1-bit icons pack\n - Multiple **tools** provided for raygui development\n\n## code sample\n```c\n#include \"raylib.h\"\n\n#define RAYGUI_IMPLEMENTATION\n#include \"raygui.h\"\n\nint main()\n{\n    InitWindow(400, 200, \"raygui - controls test suite\");\n    SetTargetFPS(60);\n\n    bool showMessageBox = false;\n\n    while (!WindowShouldClose())\n    {\n        // Draw\n        //----------------------------------------------------------------------------------\n        BeginDrawing();\n            ClearBackground(GetColor(GuiGetStyle(DEFAULT, BACKGROUND_COLOR)));\n\n            if (GuiButton((Rectangle){ 24, 24, 120, 30 }, \"#191#Show Message\")) showMessageBox = true;\n\n            if (showMessageBox)\n            {\n                int result = GuiMessageBox((Rectangle){ 85, 70, 250, 100 },\n                    \"#191#Message Box\", \"Hi! This is a message!\", \"Nice;Cool\");\n\n                if (result >= 0) showMessageBox = false;\n            }\n\n        EndDrawing();\n    }\n\n    CloseWindow();\n    return 0;\n}\n```\n![screenshot000](https://github.com/raysan5/raygui/assets/5766837/170e2bce-b7ca-49dc-a263-32b673376546)\n\n## raygui controls\n\n### basic controls\n```\nLabel       |  Button      |  LabelButton |  Toggle      |  ToggleGroup  |  ToggleSlider\nCheckBox    |  ComboBox    |  DropdownBox |  TextBox     |  ValueBox     |  Spinner\nSlider      |  SliderBar   |  ProgressBar |  StatusBar   |  DummyRec     |  Grid\n```\n### container/separator controls\n```\nWindowBox   |  GroupBox    |  Line        |  Panel       |  ScrollPanel  | TabBar\n```\n### advanced controls\n```\nListView    |  ColorPicker |  MessageBox  |  TextInputBox\n```\n\n\n## raygui styles\n\n`raygui` comes with a [default](styles/default) style automatically loaded at runtime:\n\n![raygui default style](styles/default/style_default.png)\n\nSome additional styles are also provided for convenience, just check [styles directory](styles) for details:\n\n![raygui additional styles](images/raygui_style_table_multi.png)\n\nCustom styles can also be created very easily using [rGuiStyler](https://raylibtech.itch.io/rguistyler) tool.\n\nStyles can be loaded at runtime using raygui `GuiLoadStyle()` function. Simple and easy-to-use.\n\n## raygui icons\n\n`raygui` supports custom icons, by default, a predefined set of icons is provided inside `raygui` as an array of binary data; it contains **256 possible icons** defined as **16x16 pixels** each; each pixel is codified using **1-bit**. The total size of the array is `2048 bytes`.\n\n<img align=\"right\" src=\"images/raygui_ricons.png\">\n\nTo use any of those icons just prefix the *#iconId#* number to **any text** written within `raygui` controls:\n```c\nif (GuiButton(rec, \"#05#Open Image\")) { /* ACTION */ }\n```\nIt's also possible to use the provided `GuiIconText()` function to prefix it automatically, using a clearer identifier (defined in `raygui.h`).\n```c\nif (GuiButton(rec, GuiIconText(RICON_FILE_OPEN, \"Open Image\"))) { /* ACTION */ }\n```\nProvided set of icons can be reviewed and customized using [rGuiIcons](https://raylibtech.itch.io/rguiicons) tool.\n\n## raygui support tools\n\n - [**rGuiStyler**](https://raylibtech.itch.io/rguistyler) - A simple and easy-to-use raygui styles editor.\n\n   ![rGuiStyler v3.1](images/rguistyler_v300.png)\n\n - [**rGuiIcons**](https://raylibtech.itch.io/rguiicons) - A simple and easy-to-use raygui icons editor.\n\n   ![rGuiIcons v1.0](images/rguiicons_v100.png)\n\n - [**rGuiLayout**](https://raylibtech.itch.io/rguilayout) - A simple and easy-to-use raygui layouts editor.\n\n   ![rGuiLayout v2.2](images/rguilayout_v220.png)\n\n## building\n\n`raygui` is intended to be used as a portable single-file header-only library, to be directly integrated into any C/C++ codebase but some users could require a shared/dynamic version of the library, for example, to create bindings:\n\n - **Windows (MinGW, GCC)**\n```\ncopy src/raygui.h src/raygui.c\ngcc -o src/raygui.dll src/raygui.c -shared -DRAYGUI_IMPLEMENTATION -DBUILD_LIBTYPE_SHARED -static-libgcc -lopengl32 -lgdi32 -lwinmm -Wl,--out-implib,src/librayguidll.a\n```\n\n - **Windows (MSVC)**\n```\ncopy src\\raygui.h src\\raygui.c\ncl /O2 /I../raylib/src/ /D_USRDLL /D_WINDLL /DRAYGUI_IMPLEMENTATION /DBUILD_LIBTYPE_SHARED src/raygui.c /LD /Feraygui.dll /link /LIBPATH ../raylib/build/raylib/Release/raylib.lib /subsystem:windows /machine:x64\n```\n\n - **Linux (GCC)**\n```\nmv src/raygui.h src/raygui.c\ngcc -o raygui.so src/raygui.c -shared -fpic -DRAYGUI_IMPLEMENTATION -lraylib -lGL -lm -lpthread -ldl -lrt -lX11\nmv src/raygui.c src/raygui.h\n```\n\n- **Mac (clang, homebrew installed raylib)**\n```\ncp src/raygui.h src/raygui.c\nbrew install raylib\ngcc -o raygui.dynlib src/raygui.c -shared -fpic -DRAYGUI_IMPLEMENTATION -framework OpenGL -lm -lpthread -ldl $(pkg-config --libs --cflags raylib)\n```\n\n\n## license\n\nraygui is licensed under an unmodified zlib/libpng license, which is an OSI-certified, BSD-like license that allows static linking with closed source software. Check [LICENSE](LICENSE) for further details.\n",
      "stars_today": 3
    },
    {
      "id": 278638069,
      "name": "Olauncher",
      "full_name": "tanujnotes/Olauncher",
      "description": "Minimal AF Launcher for Android. Reduce your screen time. Daily wallpapers.",
      "html_url": "https://github.com/tanujnotes/Olauncher",
      "stars": 3230,
      "forks": 373,
      "language": "Kotlin",
      "topics": [
        "android",
        "android-launcher",
        "digital-wellbeing",
        "gplv3",
        "kotlin",
        "launcher",
        "minimal",
        "mvvm",
        "olauncher",
        "viewmodel",
        "wallpaper"
      ],
      "created_at": "2020-07-10T13:16:58Z",
      "updated_at": "2026-01-27T07:23:53Z",
      "pushed_at": "2026-01-14T11:15:43Z",
      "open_issues": 67,
      "owner": {
        "login": "tanujnotes",
        "avatar_url": "https://avatars.githubusercontent.com/u/11060605?v=4"
      },
      "readme": "![Olauncher](https://repository-images.githubusercontent.com/278638069/db0acb80-661b-11eb-803e-926cae5dccb4)\n\n\n# Olauncher | Minimal AF Launcher\nAF stands for Ad-Free\n\n[<img src=\"https://fdroid.gitlab.io/artwork/badge/get-it-on.png\"\n    alt=\"Get it on F-Droid\"\n    height=\"80\">](https://f-droid.org/packages/app.olauncher)\n[<img src=\"https://play.google.com/intl/en_us/badges/static/images/badges/en_badge_web_generic.png\"\n    alt=\"Get it on Play Store\"\n    height=\"80\">](https://play.google.com/store/apps/details?id=app.olauncher)\n\n### Install using [F-Droid](https://f-droid.org/packages/app.olauncher), [Play Store](https://play.google.com/store/apps/details?id=app.olauncher) or the [latest APK](https://github.com/tanujnotes/Olauncher/releases/).\n\n- To maintain the simplicity of the launcher, a few niche features are available but hidden.\n\n- Please check out the **[About](https://tanujnotes.substack.com/p/olauncher-minimal-af-launcher?utm_source=github)** page in the Olauncher settings for a complete list of features and **FAQs**.\n\n##\n\nLicense: [GNU GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html)\n\nDev: [X/twitter](https://x.com/tanujnotes) â€¢ [Bluesky](https://bsky.app/profile/tanujnotes.bsky.social)\n\n##\n\n### My other apps:\n\n- [Pro Launcher](https://play.google.com/store/apps/details?id=app.prolauncher) - Pro version of Olauncher with extra features like widgets, weather, folders, etc.\n\n- [Note to Self](https://play.google.com/store/apps/details?id=com.makenotetoself) - Free and [open source](https://github.com/jeerovan/ntsapp) notes app with chat like interface and end-to-end encryption.\n\n- [Pentastic](https://play.google.com/store/apps/details?id=app.pentastic) - Minimal todo lists. Free and [open source](https://github.com/tanujnotes/Pentastic).\n",
      "stars_today": 3
    },
    {
      "id": 303360020,
      "name": "copymanga",
      "full_name": "fumiama/copymanga",
      "description": "æ‹·è´æ¼«ç”»çš„ç¬¬ä¸‰æ–¹APPï¼Œä»…æä¾›åŸºç¡€åŠŸèƒ½ï¼Œæ›´å¤šä¸°å¯ŒåŠŸèƒ½è¯·ç§»æ­¥å®˜æ–¹ç‰ˆæœ¬",
      "html_url": "https://github.com/fumiama/copymanga",
      "stars": 4094,
      "forks": 97,
      "language": "Kotlin",
      "topics": [
        "android",
        "android-app",
        "android-application",
        "android-studio",
        "copymanga",
        "kotlin",
        "kotlin-android",
        "manga",
        "manga-api",
        "manga-downloader",
        "manga-reader",
        "mangareader",
        "material-design",
        "material-ui"
      ],
      "created_at": "2020-10-12T10:43:58Z",
      "updated_at": "2026-01-27T17:45:38Z",
      "pushed_at": "2025-12-04T14:05:05Z",
      "open_issues": 46,
      "owner": {
        "login": "fumiama",
        "avatar_url": "https://avatars.githubusercontent.com/u/41315874?v=4"
      },
      "readme": "<div align=\"center\">\n  <img src=\"app/src/main/res/drawable-nodpi/kohima.webp\" width = \"256\" height = \"256\" alt=\"Kohima\"><br>\n  <h1>copymanga æ‹·è´æ¼«ç”»</h1>\n  æ‹·è´æ¼«ç”»çš„ç¬¬ä¸‰æ–¹APPï¼Œä»…æä¾›åŸºç¡€åŠŸèƒ½ï¼Œæ›´å¤šä¸°å¯ŒåŠŸèƒ½è¯·ç§»æ­¥å®˜æ–¹ç‰ˆæœ¬<br><br>\n\n  [<img src=\"https://fdroid.gitlab.io/artwork/badge/get-it-on.png\"\n    alt=\"Get it on F-Droid\"\n    height=\"80\">](https://f-droid.org/packages/top.fumiama.copymanga)\n\n  [![qgroup-559748702](https://img.shields.io/badge/-1ç¾¤-red?style=for-the-badge&labelColor=white&logo=qq)](https://qm.qq.com/q/yYPB2Xn1cs)\n  [![qgroup-941632512](https://img.shields.io/badge/-2ç¾¤-orange?style=for-the-badge&labelColor=white&logo=qq)](https://qm.qq.com/q/iVp4xKJctO)\n  [![qguild-copymanga3p](https://img.shields.io/badge/-é¢‘é“-green?style=for-the-badge&labelColor=white&logo=qq)](https://pd.qq.com/s/h8255c7t8)\n  [![t.me/cpmanga3p](https://img.shields.io/badge/-cpmanga3p-blue?style=for-the-badge&labelColor=white&logo=telegram)](https://t.me/cpmanga3p)\n\n</div>\n\n\n<div align=center> <a href=\"#\"> <img src=\"https://counter.seku.su/cmoe?name=copymanga&theme=r34\" /> </a> </div>\n\n\n## æç¤º\næ¼«ç”»æ•°æ®å‡æ¥æºäºæ‹·è´æ¼«ç”»å®˜æ–¹ï¼Œä½œè€…ä¸å¯¹å…¶ä¸­å‘ˆç°çš„ä»»ä½•å†…å®¹è´Ÿè´£ã€‚\n\n## è¯´æ˜\n1. ä¸‹è½½æ–‡ä»¶ä¸º`webp`æ ¼å¼å›¾ç‰‡ï¼ŒæŒ‰ç« èŠ‚æ‰“åŒ…ä¸º`zip`ï¼Œå¯ä½¿ç”¨æœ¬åº”ç”¨æˆ–å…¶ä»–æ¼«ç”»é˜…è¯»åº”ç”¨æ‰“å¼€ã€‚\n2. è‹¥æƒ³æŸ¥çœ‹ä¸‹è½½çš„æ¼«ç”»æ˜¯å¦æœ‰é”™è¯¯ï¼Œå¯ä»¥é•¿æŒ‰è¯¥æ¼«ç”»ç›®å½•æ‰§è¡ŒæŸ¥é”™ã€‚\n3. ä¸‹è½½æ–‡ä»¶ä½äº`./Android/data/top.fumiama.copymanga/files`ç›®å½•ã€‚\n4. å®˜æ–¹åœ¨æŸæ®µæ—¶é—´å…³é—­`H5`åï¼ˆç°å·²é‡æ–°æ‰“å¼€ï¼‰ï¼Œä»`2.0`ç‰ˆæœ¬å¼€å§‹ï¼Œæœ¬åº”ç”¨è¿›è¡Œäº†å…¨æ–°å‡çº§ã€‚\n5. æ–°ç‰ˆä½¿ç”¨`Material Design`é…åˆå®˜æ–¹`APP`çš„`API`ã€‚\n6. æœ¬åº”ç”¨é»˜è®¤ä½¿ç”¨å¤§é™†çº¿è·¯ã€‚\n\n### åŠŸèƒ½\n1. æµè§ˆä¸»é¡µã€åˆ†ç±»ã€æ’è¡Œã€æˆ‘çš„ä¸‹è½½ã€æˆ‘çš„è®¢é˜…ã€æµè§ˆå†å²ã€æ ‡ç­¾ã€ä½œè€…ã€‚\n2. æŸ¥çœ‹ã€æœç´¢æ¼«ç”»å¹¶ç›´æ¥é˜…è¯»ï¼›**åœ¨æœ¬åœ°å’Œäº‘ç«¯**è®°å½•æ¼«ç”»ä¸ç« èŠ‚çš„é˜…è¯»è¿›åº¦(äº‘ç«¯ä¸èƒ½ç²¾ç¡®åˆ°é¡µ)ã€‚\n3. ä¸‹è½½æ¼«ç”»ã€‚ä½†æ˜¯ç”±äºä¸å¯æŠ—åŠ›ï¼Œä¸‹è½½é€Ÿåº¦è¾ƒæ…¢ä¸”å®¹æ˜“å‡ºé”™ï¼Œè¿™ç»å¯¹ä¸æ˜¯ä¼˜åŒ–çš„åŸå› ï¼Œç»å¯¹ä¸æ˜¯ã€‚\n4. é˜…è¯»ã€åˆ é™¤ä¸‹è½½çš„æ¼«ç”»ï¼Œæˆ–ä»æˆ‘çš„ä¸‹è½½é¡µé¢ç›´æ¥å¯¼èˆªåˆ°æ¼«ç”»è¯¦æƒ…é¡µã€‚\n5. æ£€æŸ¥æ›´æ–°ã€‚\n6. ç™»å½•ï¼Œæ³¨é”€ã€‚\n7. è®¢é˜…ã€å–æ¶ˆè®¢é˜…ã€‚\n\n### æœªå®ç°åŠŸèƒ½\næœªåœ¨ä¸Šè¡¨åˆ—å‡ºçš„å®˜æ–¹`APP`çš„å…¶ä»–åŠŸèƒ½ã€‚\n\n### é¢„è§ˆ\n#### æµ…è‰²æ¨¡å¼\n\n<table>\n\t<tr>\n\t\t<td align=\"center\"><img src=\"https://user-images.githubusercontent.com/41315874/196217391-7f617392-4ad4-47cf-b903-fa445db6fcfc.png\"></td>\n\t\t<td align=\"center\"><img src=\"https://github.com/fumiama/copymanga/assets/41315874/a72a839e-d093-4f60-b22c-a65b0fe7c32a\"></td>\n\t\t<td align=\"center\"><img src=\"https://user-images.githubusercontent.com/41315874/196217414-198fd7d2-ed80-4c0e-a40c-c83ac9ff091d.png\"></td>\n\t\t<td align=\"center\"><img src=\"https://github.com/fumiama/copymanga/assets/41315874/60269c74-a719-4a3b-be4d-c6c2a85989f1\"></td>\n\t</tr>\n    <tr>\n\t\t<td align=\"center\">ä¸»é¡µ</td>\n\t\t<td align=\"center\">è¯¦æƒ…</td>\n\t\t<td align=\"center\">é˜…è¯»</td>\n\t\t<td align=\"center\">æ ‡ç­¾</td>\n\t</tr>\n</table>\n<table>\n\t<tr>\n\t\t<td align=\"center\"><img src=\"https://github.com/fumiama/copymanga/assets/41315874/ae60ee32-01bc-44f7-93e3-f79b937e66a8\"></td>\n\t\t<td align=\"center\"><img src=\"https://user-images.githubusercontent.com/41315874/196217462-3f25eee2-d356-420a-b129-754725201f36.png\"></td>\n\t\t<td align=\"center\"><img src=\"https://user-images.githubusercontent.com/41315874/196217475-3f4b1c5b-d885-4338-9312-26330a1fabd5.png\"></td>\n\t\t<td align=\"center\"><img src=\"https://github.com/fumiama/copymanga/assets/41315874/e513c809-4e60-42e5-9bbd-98a1c29d7335\"></td>\n\t</tr>\n    <tr>\n\t\t<td align=\"center\">åˆ†ç±»</td>\n\t\t<td align=\"center\">ä¸‹è½½</td>\n\t\t<td align=\"center\">æ­£åœ¨ä¸‹è½½</td>\n\t\t<td align=\"center\">æŠ½å±‰</td>\n\t</tr>\n</table>\n\n#### æ·±è‰²æ¨¡å¼\n\n<table>\n\t<tr>\n\t\t<td align=\"center\"><img src=\"https://user-images.githubusercontent.com/41315874/196217254-5fc9b56b-2800-4cb8-bbeb-5020e2b0387d.png\"></td>\n\t\t<td align=\"center\"><img src=\"https://github.com/fumiama/copymanga/assets/41315874/0edccff4-a6b1-4ee8-9d0d-61f01b1edbac\"></td>\n\t\t<td align=\"center\"><img src=\"https://user-images.githubusercontent.com/41315874/196217310-c245eddc-1698-454d-96ad-456b81f469cb.png\"></td>\n\t\t<td align=\"center\"><img src=\"https://github.com/fumiama/copymanga/assets/41315874/05f6a89a-63b3-4350-89ef-44b8310ae744\"></td>\n\t</tr>\n    <tr>\n\t\t<td align=\"center\">ä¸»é¡µ</td>\n\t\t<td align=\"center\">è¯¦æƒ…</td>\n\t\t<td align=\"center\">é˜…è¯»</td>\n\t\t<td align=\"center\">æ ‡ç­¾</td>\n\t</tr>\n</table>\n<table>\n\t<tr>\n\t\t<td align=\"center\"><img src=\"https://github.com/fumiama/copymanga/assets/41315874/c8fc2b09-902e-4b3b-b8c2-8cf5ebf8d759\"></td>\n\t\t<td align=\"center\"><img src=\"https://user-images.githubusercontent.com/41315874/196217365-be6278f8-684c-44e8-be81-f8a14ced9ac0.png\"></td>\n\t\t<td align=\"center\"><img src=\"https://user-images.githubusercontent.com/41315874/196217372-7ca3a1be-ebd9-4a9c-8371-666f91c415db.png\"></td>\n\t\t<td align=\"center\"><img src=\"https://github.com/fumiama/copymanga/assets/41315874/532143a9-e9d8-419b-8b1b-4f7978dc1ef9\"></td>\n\t</tr>\n    <tr>\n\t\t<td align=\"center\">åˆ†ç±»</td>\n\t\t<td align=\"center\">ä¸‹è½½</td>\n\t\t<td align=\"center\">æ­£åœ¨ä¸‹è½½</td>\n\t\t<td align=\"center\">æŠ½å±‰</td>\n\t</tr>\n</table>\n",
      "stars_today": 3
    },
    {
      "id": 14125254,
      "name": "consul",
      "full_name": "hashicorp/consul",
      "description": "Consul is a distributed, highly available, and data center aware solution to connect and configure applications across dynamic, distributed infrastructure.",
      "html_url": "https://github.com/hashicorp/consul",
      "stars": 29690,
      "forks": 4556,
      "language": "Go",
      "topics": [
        "api-gateway",
        "consul",
        "ecs",
        "kubernetes",
        "service-discovery",
        "service-mesh",
        "vault"
      ],
      "created_at": "2013-11-04T22:15:27Z",
      "updated_at": "2026-01-27T14:05:58Z",
      "pushed_at": "2026-01-27T16:01:51Z",
      "open_issues": 1349,
      "owner": {
        "login": "hashicorp",
        "avatar_url": "https://avatars.githubusercontent.com/u/761456?v=4"
      },
      "readme": "<h1>\n  <img src=\"./website/public/img/logo.svg\" align=\"left\" height=\"46px\" alt=\"Consul logo\"/>\n  <span>Consul</span>\n</h1>\n\n[![License: BUSL-1.1](https://img.shields.io/badge/License-BUSL--1.1-yellow.svg)](LICENSE)\n[![Docker Pulls](https://img.shields.io/docker/pulls/hashicorp/consul.svg)](https://hub.docker.com/r/hashicorp/consul)\n[![Go Report Card](https://goreportcard.com/badge/github.com/hashicorp/consul)](https://goreportcard.com/report/github.com/hashicorp/consul)\n\nConsul is a distributed, highly available, and data center aware solution to connect and configure applications across dynamic, distributed infrastructure.\n\n* Documentation and Tutorials: [https://developer.hashicorp.com/consul]\n* Forum: [Discuss](https://discuss.hashicorp.com/c/consul)\n\nConsul provides several key features:\n\n* **Multi-Datacenter** - Consul is built to be datacenter aware, and can\n  support any number of regions without complex configuration.\n\n* **Service Mesh** - Consul Service Mesh enables secure service-to-service\n  communication with automatic TLS encryption and identity-based authorization. Applications\n  can use sidecar proxies in a service mesh configuration to establish TLS\n  connections for inbound and outbound connections with Transparent Proxy.\n\n* **API Gateway** - Consul API Gateway manages access to services within Consul Service Mesh, \n  allow users to define traffic and authorization policies to services deployed within the mesh.  \n\n* **Service Discovery** - Consul makes it simple for services to register\n  themselves and to discover other services via a DNS or HTTP interface.\n  External services such as SaaS providers can be registered as well.\n\n* **Health Checking** - Health Checking enables Consul to quickly alert\n  operators about any issues in a cluster. The integration with service\n  discovery prevents routing traffic to unhealthy hosts and enables service\n  level circuit breakers.\n\n* **Dynamic App Configuration** - An HTTP API that allows users to store indexed objects within Consul,\n  for storing configuration parameters and application metadata.\n\nConsul runs on Linux, macOS, FreeBSD, Solaris, and Windows and includes an\noptional [browser based UI](https://demo.consul.io). A commercial version\ncalled [Consul Enterprise](https://developer.hashicorp.com/consul/docs/enterprise) is also\navailable.\n\n**Please note**: We take Consul's security and our users' trust very seriously. If you\nbelieve you have found a security issue in Consul, please [responsibly disclose](https://www.hashicorp.com/security#vulnerability-reporting)\nby contacting us at security@hashicorp.com.\n\n## Quick Start\n\nA few quick start guides are available on the Consul website:\n\n* **Standalone binary install:** https://learn.hashicorp.com/collections/consul/get-started-vms\n* **Minikube install:** https://learn.hashicorp.com/tutorials/consul/kubernetes-minikube\n* **Kind install:** https://learn.hashicorp.com/tutorials/consul/kubernetes-kind\n* **Kubernetes install:** https://learn.hashicorp.com/tutorials/consul/kubernetes-deployment-guide\n* **Deploy HCP Consul:** https://learn.hashicorp.com/tutorials/consul/hcp-gs-deploy \n\n## Documentation\n\nFull, comprehensive documentation is available on the Consul website: https://developer.hashicorp.com/consul/docs\n\n## Contributing\n\nThank you for your interest in contributing! Please refer to [CONTRIBUTING.md](https://github.com/hashicorp/consul/blob/main/.github/CONTRIBUTING.md)\nfor guidance. For contributions specifically to the browser based UI, please\nrefer to the UI's [README.md](https://github.com/hashicorp/consul/blob/main/ui/packages/consul-ui/README.md)\nfor guidance.\n",
      "stars_today": 2
    },
    {
      "id": 166515022,
      "name": "trino",
      "full_name": "trinodb/trino",
      "description": "Official repository of Trino, the distributed SQL query engine for big data, formerly known as PrestoSQL (https://trino.io)",
      "html_url": "https://github.com/trinodb/trino",
      "stars": 12470,
      "forks": 3453,
      "language": "Java",
      "topics": [
        "analytics",
        "big-data",
        "data-science",
        "database",
        "databases",
        "datalake",
        "delta-lake",
        "distributed-database",
        "distributed-systems",
        "hadoop",
        "hive",
        "iceberg",
        "java",
        "jdbc",
        "presto",
        "prestodb",
        "query-engine",
        "sql",
        "trino"
      ],
      "created_at": "2019-01-19T06:38:14Z",
      "updated_at": "2026-01-28T01:56:28Z",
      "pushed_at": "2026-01-27T15:47:17Z",
      "open_issues": 2518,
      "owner": {
        "login": "trinodb",
        "avatar_url": "https://avatars.githubusercontent.com/u/34147222?v=4"
      },
      "readme": "<p align=\"center\">\n    <a href=\"https://trino.io/\"><img alt=\"Trino Logo\" src=\".github/homepage.png\" /></a>\n</p>\n<p align=\"center\">\n    <b>Trino is a fast distributed SQL query engine for big data analytics.</b>\n</p>\n<p align=\"center\">\n    See the <a href=\"https://trino.io/docs/current/\">User Manual</a> for deployment instructions and end user documentation.\n</p>\n<p align=\"center\">\n  <a href=\"https://trino.io/download.html\" style=\"text-decoration: none\"><img\n    src=\"https://img.shields.io/github/v/release/trinodb/trino\"\n    alt=\"Trino download\"\n  /></a>\n  <a href=\"https://github.com/jvm-repo-rebuild/reproducible-central/blob/master/content/io/trino/README.md\" style=\"text-decoration: none\"><img\n    src=\"https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/jvm-repo-rebuild/reproducible-central/master/content/io/trino/badge.json\"\n    alt=\"Reproducible builds supported\"\n  /></a>\n  <a href=\"https://trino.io/slack.html\" style=\"text-decoration: none\"><img\n    src=\"https://img.shields.io/static/v1?logo=slack&logoColor=959DA5&label=Slack&labelColor=333a41&message=join%20conversation&color=3AC358\"\n    alt=\"Trino Slack\"\n  /></a>\n  <a href=\"https://trino.io/trino-the-definitive-guide.html\" style=\"text-decoration: none\"><img\n    src=\"https://img.shields.io/badge/Trino%3A%20The%20Definitive%20Guide-download-brightgreen\"\n    alt=\"Trino: The Definitive Guide book download\"\n  /></a>\n</p>\n\n## Development\n\nSee [DEVELOPMENT](.github/DEVELOPMENT.md) for information about development and release process,\ncode style and guidelines for implementors of Trino plugins.\n\nSee [CONTRIBUTING](.github/CONTRIBUTING.md) for contribution requirements.\n\n## Security\n\nSee the project [security policy](.github/SECURITY.md) for\ninformation about reporting vulnerabilities.\n\nTrino supports [reproducible builds](https://reproducible-builds.org) as of version 449.\n\n## Build requirements\n\n* Mac OS X or Linux\n  * Note that some npm packages used to build the web UI are only available\n    for x86 architectures, so if you're building on Apple Silicon, you need \n    to have Rosetta 2 installed\n* Java 25.0.1+, 64-bit\n* Docker\n  * Turn SELinux or other systems disabling write access to the local checkout\n    off, to allow containers to mount parts of the Trino source tree\n\n## Building Trino\n\nTrino is a standard Maven project. Simply run the following command from the\nproject root directory:\n\n    ./mvnw clean install -DskipTests\n\nOn the first build, Maven downloads all the dependencies from the internet\nand caches them in the local repository (`~/.m2/repository`), which can take a\nwhile, depending on your connection speed. Subsequent builds are faster.\n\nTrino has a comprehensive set of tests that take a considerable amount of time\nto run, and are thus disabled by the above command. These tests are run by the\nCI system when you submit a pull request. We recommend only running tests\nlocally for the areas of code that you change.\n\n## Running Trino in your IDE\n\n### Overview\n\nAfter building Trino for the first time, you can load the project into your IDE\nand run the server.  We recommend using\n[IntelliJ IDEA](http://www.jetbrains.com/idea/). Because Trino is a standard\nMaven project, you easily can import it into your IDE.  In IntelliJ, choose\n*Open Project* from the *Quick Start* box or choose *Open*\nfrom the *File* menu and select the root `pom.xml` file.\n\nAfter opening the project in IntelliJ, double check that the Java SDK is\nproperly configured for the project:\n\n* Open the File menu and select Project Structure\n* In the SDKs section, ensure that JDK 25 is selected (create one if none exist)\n* In the Project section, ensure the Project language level is set to 25\n\n### Running a testing server\n\nThe simplest way to run Trino for development is to run the `TpchQueryRunner`\nclass. It will start a development version of the server that is configured with\nthe TPCH connector. You can then use the CLI to execute queries against this\nserver. Many other connectors have their own `*QueryRunner` class that you can\nuse when working on a specific connector. The generally required VM option \nhere is `--add-modules jdk.incubator.vector` but various `*QueryRunner` classes\nmight require additional options (if necessary, check the `air.test.jvm.additional-arguments` \nproperty in the `pom.xml` file of the module from which the runner comes).\n\n### Running the full server\n\nTrino comes with sample configuration that should work out-of-the-box for\ndevelopment. Use the following options to create a run configuration:\n\n* Main Class: `io.trino.server.DevelopmentServer`\n* VM Options: `-ea -Dconfig=etc/config.properties -Dlog.levels-file=etc/log.properties -Djdk.attach.allowAttachSelf=true --sun-misc-unsafe-memory-access=allow --add-modules jdk.incubator.vector`\n* Working directory: `$MODULE_DIR$`\n* Use classpath of module: `trino-server-dev`\n\nThe working directory should be the `trino-server-dev` subdirectory. In\nIntelliJ, using `$MODULE_DIR$` accomplishes this automatically.\n\nIf `VM options` doesn't exist in the dialog, you need to select `Modify options`\nand enable `Add VM options`.\n\nTo adjust which plugins are enabled for the development server, adjust the value of\n`plugin.bundles` in `config.properties`. Each entry in this list must represent a plugin\nspecified by one of the following options:\n* A path to a `pom.xml` or `*.pom` file describing a Maven project that produces a plugin.\n* Maven coordinates, in the form `<groupId>:<artifactId>[:<extension>[:<classifier>]]:<version>`. The plugin will be loaded via Maven and therefore must be available in your local repository or a remote repository.\n* A path to a plugin directory containing JAR files. See [Deploying a custom plugin](https://trino.io/docs/current/develop/spi-overview.html#deploying-a-custom-plugin) for more details.\n\nIf you want to use a plugin in a catalog, you must add a corresponding\n`<catalog_name>.properties` file to `testing/trino-server-dev/etc/catalog`.\n\n### Running the CLI\n\nStart the CLI to connect to the server and run SQL queries:\n\n    client/trino-cli/target/trino-cli-*-executable.jar\n\nRun a query to see the nodes in the cluster:\n\n    SELECT * FROM system.runtime.nodes;\n\nRun a query against the TPCH connector:\n\n    SELECT * FROM tpch.tiny.region;\n",
      "stars_today": 2
    },
    {
      "id": 54280778,
      "name": "cJSON",
      "full_name": "DaveGamble/cJSON",
      "description": "Ultralightweight JSON parser in ANSI C",
      "html_url": "https://github.com/DaveGamble/cJSON",
      "stars": 12392,
      "forks": 3417,
      "language": "C",
      "topics": [],
      "created_at": "2016-03-19T18:22:54Z",
      "updated_at": "2026-01-27T17:21:18Z",
      "pushed_at": "2025-09-09T13:58:05Z",
      "open_issues": 292,
      "owner": {
        "login": "DaveGamble",
        "avatar_url": "https://avatars.githubusercontent.com/u/118247?v=4"
      },
      "readme": "# cJSON\n\nUltralightweight JSON parser in ANSI C.\n\n## Table of contents\n* [License](#license)\n* [Usage](#usage)\n  * [Welcome to cJSON](#welcome-to-cjson)\n  * [Building](#building)\n    * [Copying the source](#copying-the-source)\n    * [CMake](#cmake)\n    * [Makefile](#makefile)\n    * [Meson](#meson)\n    * [Vcpkg](#Vcpkg)\n  * [Including cJSON](#including-cjson)\n  * [Data Structure](#data-structure)\n  * [Working with the data structure](#working-with-the-data-structure)\n    * [Basic types](#basic-types)\n    * [Arrays](#arrays)\n    * [Objects](#objects)\n  * [Parsing JSON](#parsing-json)\n  * [Printing JSON](#printing-json)\n  * [Example](#example)\n    * [Printing](#printing)\n    * [Parsing](#parsing)\n  * [Caveats](#caveats)\n    * [Zero Character](#zero-character)\n    * [Character Encoding](#character-encoding)\n    * [C Standard](#c-standard)\n    * [Floating Point Numbers](#floating-point-numbers)\n    * [Deep Nesting Of Arrays And Objects](#deep-nesting-of-arrays-and-objects)\n    * [Thread Safety](#thread-safety)\n    * [Case Sensitivity](#case-sensitivity)\n    * [Duplicate Object Members](#duplicate-object-members)\n  * [Enjoy cJSON!](#enjoy-cjson)\n\n## License\n\nMIT License\n\n>  Copyright (c) 2009-2017 Dave Gamble and cJSON contributors\n>\n>  Permission is hereby granted, free of charge, to any person obtaining a copy\n>  of this software and associated documentation files (the \"Software\"), to deal\n>  in the Software without restriction, including without limitation the rights\n>  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n>  copies of the Software, and to permit persons to whom the Software is\n>  furnished to do so, subject to the following conditions:\n>\n>  The above copyright notice and this permission notice shall be included in\n>  all copies or substantial portions of the Software.\n>\n>  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n>  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n>  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n>  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n>  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n>  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n>  THE SOFTWARE.\n\n## Usage\n\n### Welcome to cJSON.\n\ncJSON aims to be the dumbest possible parser that you can get your job done with.\nIt's a single file of C, and a single header file.\n\nJSON is described best here: http://www.json.org/\nIt's like XML, but fat-free. You use it to move data around, store things, or just\ngenerally represent your program's state.\n\nAs a library, cJSON exists to take away as much legwork as it can, but not get in your way.\nAs a point of pragmatism (i.e. ignoring the truth), I'm going to say that you can use it\nin one of two modes: Auto and Manual. Let's have a quick run-through.\n\nI lifted some JSON from this page: http://www.json.org/fatfree.html\nThat page inspired me to write cJSON, which is a parser that tries to share the same\nphilosophy as JSON itself. Simple, dumb, out of the way.\n\n### Building\n\nThere are several ways to incorporate cJSON into your project.\n\n#### copying the source\n\nBecause the entire library is only one C file and one header file, you can just copy `cJSON.h` and `cJSON.c` to your projects source and start using it.\n\ncJSON is written in ANSI C (C89) in order to support as many platforms and compilers as possible.\n\n#### CMake\n\nWith CMake, cJSON supports a full blown build system. This way you get the most features. CMake with an equal or higher version than 2.8.5 is supported. With CMake it is recommended to do an out of tree build, meaning the compiled files are put in a directory separate from the source files. So in order to build cJSON with CMake on a Unix platform, make a `build` directory and run CMake inside it.\n\n```\nmkdir build\ncd build\ncmake ..\n```\n\nThis will create a Makefile and a bunch of other files. You can then compile it:\n\n```\nmake\n```\n\nAnd install it with `make install` if you want. By default it installs the headers `/usr/local/include/cjson` and the libraries to `/usr/local/lib`. It also installs files for pkg-config to make it easier to detect and use an existing installation of CMake. And it installs CMake config files, that can be used by other CMake based projects to discover the library.\n\nYou can change the build process with a list of different options that you can pass to CMake. Turn them on with `On` and off with `Off`:\n\n* `-DENABLE_CJSON_TEST=On`: Enable building the tests. (on by default)\n* `-DENABLE_CJSON_UTILS=On`: Enable building cJSON_Utils. (off by default)\n* `-DENABLE_TARGET_EXPORT=On`: Enable the export of CMake targets. Turn off if it makes problems. (on by default)\n* `-DENABLE_CUSTOM_COMPILER_FLAGS=On`: Enable custom compiler flags (currently for Clang, GCC and MSVC). Turn off if it makes problems. (on by default)\n* `-DENABLE_VALGRIND=On`: Run tests with [valgrind](http://valgrind.org). (off by default)\n* `-DENABLE_SANITIZERS=On`: Compile cJSON with [AddressSanitizer](https://github.com/google/sanitizers/wiki/AddressSanitizer) and [UndefinedBehaviorSanitizer](https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html) enabled (if possible). (off by default)\n* `-DENABLE_SAFE_STACK`: Enable the [SafeStack](https://clang.llvm.org/docs/SafeStack.html) instrumentation pass. Currently only works with the Clang compiler. (off by default)\n* `-DBUILD_SHARED_LIBS=On`: Build the shared libraries. (on by default)\n* `-DBUILD_SHARED_AND_STATIC_LIBS=On`: Build both shared and static libraries. (off by default)\n* `-DCMAKE_INSTALL_PREFIX=/usr`: Set a prefix for the installation.\n* `-DENABLE_LOCALES=On`: Enable the usage of localeconv method. ( on by default )\n* `-DCJSON_OVERRIDE_BUILD_SHARED_LIBS=On`: Enable overriding the value of `BUILD_SHARED_LIBS` with `-DCJSON_BUILD_SHARED_LIBS`.\n* `-DENABLE_CJSON_VERSION_SO`: Enable cJSON so version. ( on by default )\n\nIf you are packaging cJSON for a distribution of Linux, you would probably take these steps for example:\n```\nmkdir build\ncd build\ncmake .. -DENABLE_CJSON_UTILS=On -DENABLE_CJSON_TEST=Off -DCMAKE_INSTALL_PREFIX=/usr\nmake\nmake DESTDIR=$pkgdir install\n```\n\nOn Windows CMake is usually used to create a Visual Studio solution file by running it inside the Developer Command Prompt for Visual Studio, for exact steps follow the official documentation from CMake and Microsoft and use the online search engine of your choice. The descriptions of the the options above still generally apply, although not all of them work on Windows.\n\n#### Makefile\n\n**NOTE:** This Method is deprecated. Use CMake if at all possible. Makefile support is limited to fixing bugs.\n\nIf you don't have CMake available, but still have GNU make. You can use the makefile to build cJSON:\n\nRun this command in the directory with the source code and it will automatically compile static and shared libraries and a little test program (not the full test suite).\n\n```\nmake all\n```\n\nIf you want, you can install the compiled library to your system using `make install`. By default it will install the headers in `/usr/local/include/cjson` and the libraries in `/usr/local/lib`. But you can change this behavior by setting the `PREFIX` and `DESTDIR` variables: `make PREFIX=/usr DESTDIR=temp install`. And uninstall them with: `make PREFIX=/usr DESTDIR=temp uninstall`.\n\n#### Meson\n\nTo make cjson work in a project using meson, the libcjson dependency has to be included:\n\n```meson\nproject('c-json-example', 'c')\n\ncjson = dependency('libcjson')\n\nexample = executable(\n    'example',\n    'example.c',\n    dependencies: [cjson],\n)\n```\n\n\n#### Vcpkg\n\nYou can download and install cJSON using the [vcpkg](https://github.com/Microsoft/vcpkg) dependency manager:\n```\ngit clone https://github.com/Microsoft/vcpkg.git\ncd vcpkg\n./bootstrap-vcpkg.sh\n./vcpkg integrate install\nvcpkg install cjson\n```\n\nThe cJSON port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.\n\n### Including cJSON\n\nIf you installed it via CMake or the Makefile, you can include cJSON like this:\n\n```c\n#include <cjson/cJSON.h>\n```\n\n### Data Structure\n\ncJSON represents JSON data using the `cJSON` struct data type:\n\n```c\n/* The cJSON structure: */\ntypedef struct cJSON\n{\n    struct cJSON *next;\n    struct cJSON *prev;\n    struct cJSON *child;\n    int type;\n    char *valuestring;\n    /* writing to valueint is DEPRECATED, use cJSON_SetNumberValue instead */\n    int valueint;\n    double valuedouble;\n    char *string;\n} cJSON;\n```\n\nAn item of this type represents a JSON value. The type is stored in `type` as a bit-flag (**this means that you cannot find out the type by just comparing the value of `type`**).\n\nTo check the type of an item, use the corresponding `cJSON_Is...` function. It does a `NULL` check followed by a type check and returns a boolean value if the item is of this type.\n\nThe type can be one of the following:\n\n* `cJSON_Invalid` (check with `cJSON_IsInvalid`): Represents an invalid item that doesn't contain any value. You automatically have this type if you set the item to all zero bytes.\n* `cJSON_False` (check with `cJSON_IsFalse`): Represents a `false` boolean value. You can also check for boolean values in general with `cJSON_IsBool`.\n* `cJSON_True` (check with `cJSON_IsTrue`): Represents a `true` boolean value. You can also check for boolean values in general with `cJSON_IsBool`.\n* `cJSON_NULL` (check with `cJSON_IsNull`): Represents a `null` value.\n* `cJSON_Number` (check with `cJSON_IsNumber`): Represents a number value. The value is stored as a double in `valuedouble` and also in `valueint`. If the number is outside of the range of an integer, `INT_MAX` or `INT_MIN` are used for `valueint`.\n* `cJSON_String` (check with `cJSON_IsString`): Represents a string value. It is stored in the form of a zero terminated string in `valuestring`.\n* `cJSON_Array` (check with `cJSON_IsArray`): Represent an array value. This is implemented by pointing `child` to a linked list of `cJSON` items that represent the values in the array. The elements are linked together using `next` and `prev`, where the first element has `prev.next == NULL` and the last element `next == NULL`.\n* `cJSON_Object` (check with `cJSON_IsObject`): Represents an object value. Objects are stored same way as an array, the only difference is that the items in the object store their keys in `string`.\n* `cJSON_Raw` (check with `cJSON_IsRaw`): Represents any kind of JSON that is stored as a zero terminated array of characters in `valuestring`. This can be used, for example, to avoid printing the same static JSON over and over again to save performance. cJSON will never create this type when parsing. Also note that cJSON doesn't check if it is valid JSON.\n\nAdditionally there are the following two flags:\n\n* `cJSON_IsReference`: Specifies that the item that `child` points to and/or `valuestring` is not owned by this item, it is only a reference. So `cJSON_Delete` and other functions will only deallocate this item, not its `child`/`valuestring`.\n* `cJSON_StringIsConst`: This means that `string` points to a constant string. This means that `cJSON_Delete` and other functions will not try to deallocate `string`.\n\n### Working with the data structure\n\nFor every value type there is a `cJSON_Create...` function that can be used to create an item of that type.\nAll of these will allocate a `cJSON` struct that can later be deleted with `cJSON_Delete`.\nNote that you have to delete them at some point, otherwise you will get a memory leak.  \n**Important**: If you have added an item to an array or an object already, you **mustn't** delete it with `cJSON_Delete`. Adding it to an array or object transfers its ownership so that when that array or object is deleted, \nit gets deleted as well. You also could use `cJSON_SetValuestring` to change a `cJSON_String`'s `valuestring`, and you needn't to free the previous `valuestring` manually.\n\n#### Basic types\n\n* **null** is created with `cJSON_CreateNull`\n* **booleans** are created with `cJSON_CreateTrue`, `cJSON_CreateFalse` or `cJSON_CreateBool`\n* **numbers** are created with `cJSON_CreateNumber`. This will set both `valuedouble` and `valueint`. If the number is outside of the range of an integer, `INT_MAX` or `INT_MIN` are used for `valueint`\n* **strings** are created with `cJSON_CreateString` (copies the string) or with `cJSON_CreateStringReference` (directly points to the string. This means that `valuestring` won't be deleted by `cJSON_Delete` and you are responsible for its lifetime, useful for constants)\n\n#### Arrays\n\nYou can create an empty array with `cJSON_CreateArray`. `cJSON_CreateArrayReference` can be used to create an array that doesn't \"own\" its content, so its content doesn't get deleted by `cJSON_Delete`.\n\nTo add items to an array, use `cJSON_AddItemToArray` to append items to the end.\nUsing `cJSON_AddItemReferenceToArray` an element can be added as a reference to another item, array or string. This means that `cJSON_Delete` will not delete that items `child` or `valuestring` properties, so no double frees are occurring if they are already used elsewhere.\nTo insert items in the middle, use `cJSON_InsertItemInArray`. It will insert an item at the given 0 based index and shift all the existing items to the right.\n\nIf you want to take an item out of an array at a given index and continue using it, use `cJSON_DetachItemFromArray`, it will return the detached item, so be sure to assign it to a pointer, otherwise you will have a memory leak.\n\nDeleting items is done with `cJSON_DeleteItemFromArray`. It works like `cJSON_DetachItemFromArray`, but deletes the detached item via `cJSON_Delete`.\n\nYou can also replace an item in an array in place. Either with `cJSON_ReplaceItemInArray` using an index or with `cJSON_ReplaceItemViaPointer` given a pointer to an element. `cJSON_ReplaceItemViaPointer` will return `0` if it fails. What this does internally is to detach the old item, delete it and insert the new item in its place.\n\nTo get the size of an array, use `cJSON_GetArraySize`. Use `cJSON_GetArrayItem` to get an element at a given index.\n\nBecause an array is stored as a linked list, iterating it via index is inefficient (`O(nÂ²)`), so you can iterate over an array using the `cJSON_ArrayForEach` macro in `O(n)` time complexity.\n\n#### Objects\n\nYou can create an empty object with `cJSON_CreateObject`. `cJSON_CreateObjectReference` can be used to create an object that doesn't \"own\" its content, so its content doesn't get deleted by `cJSON_Delete`.\n\nTo add items to an object, use `cJSON_AddItemToObject`. Use `cJSON_AddItemToObjectCS` to add an item to an object with a name that is a constant or reference (key of the item, `string` in the `cJSON` struct), so that it doesn't get freed by `cJSON_Delete`.\nUsing `cJSON_AddItemReferenceToArray` an element can be added as a reference to another object, array or string. This means that `cJSON_Delete` will not delete that items `child` or `valuestring` properties, so no double frees are occurring if they are already used elsewhere.\n\nIf you want to take an item out of an object, use `cJSON_DetachItemFromObjectCaseSensitive`, it will return the detached item, so be sure to assign it to a pointer, otherwise you will have a memory leak.\n\nDeleting items is done with `cJSON_DeleteItemFromObjectCaseSensitive`. It works like `cJSON_DetachItemFromObjectCaseSensitive` followed by `cJSON_Delete`.\n\nYou can also replace an item in an object in place. Either with `cJSON_ReplaceItemInObjectCaseSensitive` using a key or with `cJSON_ReplaceItemViaPointer` given a pointer to an element. `cJSON_ReplaceItemViaPointer` will return `0` if it fails. What this does internally is to detach the old item, delete it and insert the new item in its place.\n\nTo get the size of an object, you can use `cJSON_GetArraySize`, this works because internally objects are stored as arrays.\n\nIf you want to access an item in an object, use `cJSON_GetObjectItemCaseSensitive`.\n\nTo iterate over an object, you can use the `cJSON_ArrayForEach` macro the same way as for arrays.\n\ncJSON also provides convenient helper functions for quickly creating a new item and adding it to an object, like `cJSON_AddNullToObject`. They return a pointer to the new item or `NULL` if they failed.\n\n### Parsing JSON\n\nGiven some JSON in a zero terminated string, you can parse it with `cJSON_Parse`.\n\n```c\ncJSON *json = cJSON_Parse(string);\n```\n\nGiven some JSON in a string (whether zero terminated or not), you can parse it with `cJSON_ParseWithLength`.\n\n```c\ncJSON *json = cJSON_ParseWithLength(string, buffer_length);\n```\n\nIt will parse the JSON and allocate a tree of `cJSON` items that represents it. Once it returns, you are fully responsible for deallocating it after use with `cJSON_Delete`.\n\nThe allocator used by `cJSON_Parse` is `malloc` and `free` by default but can be changed (globally) with `cJSON_InitHooks`.\n\nIf an error occurs a pointer to the position of the error in the input string can be accessed using `cJSON_GetErrorPtr`. Note though that this can produce race conditions in multithreading scenarios, in that case it is better to use `cJSON_ParseWithOpts` with `return_parse_end`.\nBy default, characters in the input string that follow the parsed JSON will not be considered as an error.\n\nIf you want more options, use `cJSON_ParseWithOpts(const char *value, const char **return_parse_end, cJSON_bool require_null_terminated)`.\n`return_parse_end` returns a pointer to the end of the JSON in the input string or the position that an error occurs at (thereby replacing `cJSON_GetErrorPtr` in a thread safe way). `require_null_terminated`, if set to `1` will make it an error if the input string contains data after the JSON.\n\nIf you want more options giving buffer length, use `cJSON_ParseWithLengthOpts(const char *value, size_t buffer_length, const char **return_parse_end, cJSON_bool require_null_terminated)`.\n\n### Printing JSON\n\nGiven a tree of `cJSON` items, you can print them as a string using `cJSON_Print`.\n\n```c\nchar *string = cJSON_Print(json);\n```\n\nIt will allocate a string and print a JSON representation of the tree into it. Once it returns, you are fully responsible for deallocating it after use with your allocator. (usually `free`, depends on what has been set with `cJSON_InitHooks`).\n\n`cJSON_Print` will print with whitespace for formatting. If you want to print without formatting, use `cJSON_PrintUnformatted`.\n\nIf you have a rough idea of how big your resulting string will be, you can use `cJSON_PrintBuffered(const cJSON *item, int prebuffer, cJSON_bool fmt)`. `fmt` is a boolean to turn formatting with whitespace on and off. `prebuffer` specifies the first buffer size to use for printing. `cJSON_Print` currently uses 256 bytes for its first buffer size. Once printing runs out of space, a new buffer is allocated and the old gets copied over before printing is continued.\n\nThese dynamic buffer allocations can be completely avoided by using `cJSON_PrintPreallocated(cJSON *item, char *buffer, const int length, const cJSON_bool format)`. It takes a buffer to a pointer to print to and its length. If the length is reached, printing will fail and it returns `0`. In case of success, `1` is returned. Note that you should provide 5 bytes more than is actually needed, because cJSON is not 100% accurate in estimating if the provided memory is enough.\n\n### Example\n\nIn this example we want to build and parse the following JSON:\n\n```json\n{\n    \"name\": \"Awesome 4K\",\n    \"resolutions\": [\n        {\n            \"width\": 1280,\n            \"height\": 720\n        },\n        {\n            \"width\": 1920,\n            \"height\": 1080\n        },\n        {\n            \"width\": 3840,\n            \"height\": 2160\n        }\n    ]\n}\n```\n\n#### Printing\n\nLet's build the above JSON and print it to a string:\n\n```c\n//create a monitor with a list of supported resolutions\n//NOTE: Returns a heap allocated string, you are required to free it after use.\nchar *create_monitor(void)\n{\n    const unsigned int resolution_numbers[3][2] = {\n        {1280, 720},\n        {1920, 1080},\n        {3840, 2160}\n    };\n    char *string = NULL;\n    cJSON *name = NULL;\n    cJSON *resolutions = NULL;\n    cJSON *resolution = NULL;\n    cJSON *width = NULL;\n    cJSON *height = NULL;\n    size_t index = 0;\n\n    cJSON *monitor = cJSON_CreateObject();\n    if (monitor == NULL)\n    {\n        goto end;\n    }\n\n    name = cJSON_CreateString(\"Awesome 4K\");\n    if (name == NULL)\n    {\n        goto end;\n    }\n    /* after creation was successful, immediately add it to the monitor,\n     * thereby transferring ownership of the pointer to it */\n    cJSON_AddItemToObject(monitor, \"name\", name);\n\n    resolutions = cJSON_CreateArray();\n    if (resolutions == NULL)\n    {\n        goto end;\n    }\n    cJSON_AddItemToObject(monitor, \"resolutions\", resolutions);\n\n    for (index = 0; index < (sizeof(resolution_numbers) / (2 * sizeof(int))); ++index)\n    {\n        resolution = cJSON_CreateObject();\n        if (resolution == NULL)\n        {\n            goto end;\n        }\n        cJSON_AddItemToArray(resolutions, resolution);\n\n        width = cJSON_CreateNumber(resolution_numbers[index][0]);\n        if (width == NULL)\n        {\n            goto end;\n        }\n        cJSON_AddItemToObject(resolution, \"width\", width);\n\n        height = cJSON_CreateNumber(resolution_numbers[index][1]);\n        if (height == NULL)\n        {\n            goto end;\n        }\n        cJSON_AddItemToObject(resolution, \"height\", height);\n    }\n\n    string = cJSON_Print(monitor);\n    if (string == NULL)\n    {\n        fprintf(stderr, \"Failed to print monitor.\\n\");\n    }\n\nend:\n    cJSON_Delete(monitor);\n    return string;\n}\n```\n\nAlternatively we can use the `cJSON_Add...ToObject` helper functions to make our lives a little easier:\n\n```c\n//NOTE: Returns a heap allocated string, you are required to free it after use.\nchar *create_monitor_with_helpers(void)\n{\n    const unsigned int resolution_numbers[3][2] = {\n        {1280, 720},\n        {1920, 1080},\n        {3840, 2160}\n    };\n    char *string = NULL;\n    cJSON *resolutions = NULL;\n    size_t index = 0;\n\n    cJSON *monitor = cJSON_CreateObject();\n\n    if (cJSON_AddStringToObject(monitor, \"name\", \"Awesome 4K\") == NULL)\n    {\n        goto end;\n    }\n\n    resolutions = cJSON_AddArrayToObject(monitor, \"resolutions\");\n    if (resolutions == NULL)\n    {\n        goto end;\n    }\n\n    for (index = 0; index < (sizeof(resolution_numbers) / (2 * sizeof(int))); ++index)\n    {\n        cJSON *resolution = cJSON_CreateObject();\n\n        if (cJSON_AddNumberToObject(resolution, \"width\", resolution_numbers[index][0]) == NULL)\n        {\n            goto end;\n        }\n\n        if (cJSON_AddNumberToObject(resolution, \"height\", resolution_numbers[index][1]) == NULL)\n        {\n            goto end;\n        }\n\n        cJSON_AddItemToArray(resolutions, resolution);\n    }\n\n    string = cJSON_Print(monitor);\n    if (string == NULL)\n    {\n        fprintf(stderr, \"Failed to print monitor.\\n\");\n    }\n\nend:\n    cJSON_Delete(monitor);\n    return string;\n}\n```\n\n#### Parsing\n\nIn this example we will parse a JSON in the above format and check if the monitor supports a Full HD resolution while printing some diagnostic output:\n\n```c\n/* return 1 if the monitor supports full hd, 0 otherwise */\nint supports_full_hd(const char * const monitor)\n{\n    const cJSON *resolution = NULL;\n    const cJSON *resolutions = NULL;\n    const cJSON *name = NULL;\n    int status = 0;\n    cJSON *monitor_json = cJSON_Parse(monitor);\n    if (monitor_json == NULL)\n    {\n        const char *error_ptr = cJSON_GetErrorPtr();\n        if (error_ptr != NULL)\n        {\n            fprintf(stderr, \"Error before: %s\\n\", error_ptr);\n        }\n        status = 0;\n        goto end;\n    }\n\n    name = cJSON_GetObjectItemCaseSensitive(monitor_json, \"name\");\n    if (cJSON_IsString(name) && (name->valuestring != NULL))\n    {\n        printf(\"Checking monitor \\\"%s\\\"\\n\", name->valuestring);\n    }\n\n    resolutions = cJSON_GetObjectItemCaseSensitive(monitor_json, \"resolutions\");\n    cJSON_ArrayForEach(resolution, resolutions)\n    {\n        cJSON *width = cJSON_GetObjectItemCaseSensitive(resolution, \"width\");\n        cJSON *height = cJSON_GetObjectItemCaseSensitive(resolution, \"height\");\n\n        if (!cJSON_IsNumber(width) || !cJSON_IsNumber(height))\n        {\n            status = 0;\n            goto end;\n        }\n\n        if ((width->valuedouble == 1920) && (height->valuedouble == 1080))\n        {\n            status = 1;\n            goto end;\n        }\n    }\n\nend:\n    cJSON_Delete(monitor_json);\n    return status;\n}\n```\n\nNote that there are no NULL checks except for the result of `cJSON_Parse` because `cJSON_GetObjectItemCaseSensitive` checks for `NULL` inputs already, so a `NULL` value is just propagated and `cJSON_IsNumber` and `cJSON_IsString` return `0` if the input is `NULL`.\n\n### Caveats\n\n#### Zero Character\n\ncJSON doesn't support strings that contain the zero character `'\\0'` or `\\u0000`. This is impossible with the current API because strings are zero terminated.\n\n#### Character Encoding\n\ncJSON only supports UTF-8 encoded input. In most cases it doesn't reject invalid UTF-8 as input though, it just propagates it through as is. As long as the input doesn't contain invalid UTF-8, the output will always be valid UTF-8.\n\n#### C Standard\n\ncJSON is written in ANSI C (or C89, C90). If your compiler or C library doesn't follow this standard, correct behavior is not guaranteed.\n\nNOTE: ANSI C is not C++ therefore it shouldn't be compiled with a C++ compiler. You can compile it with a C compiler and link it with your C++ code however. Although compiling with a C++ compiler might work, correct behavior is not guaranteed.\n\n#### Floating Point Numbers\n\ncJSON does not officially support any `double` implementations other than IEEE754 double precision floating point numbers. It might still work with other implementations but bugs with these will be considered invalid.\n\nThe maximum length of a floating point literal that cJSON supports is currently 63 characters.\n\n#### Deep Nesting Of Arrays And Objects\n\ncJSON doesn't support arrays and objects that are nested too deeply because this would result in a stack overflow. To prevent this cJSON limits the depth to `CJSON_NESTING_LIMIT` which is 1000 by default but can be changed at compile time.\n\n#### Thread Safety\n\nIn general cJSON is **not thread safe**.\n\nHowever it is thread safe under the following conditions:\n\n* `cJSON_GetErrorPtr` is never used (the `return_parse_end` parameter of `cJSON_ParseWithOpts` can be used instead)\n* `cJSON_InitHooks` is only ever called before using cJSON in any threads.\n* `setlocale` is never called before all calls to cJSON functions have returned.\n\n#### Case Sensitivity\n\nWhen cJSON was originally created, it didn't follow the JSON standard and didn't make a distinction between uppercase and lowercase letters. If you want the correct, standard compliant, behavior, you need to use the `CaseSensitive` functions where available.\n\n#### Duplicate Object Members\n\ncJSON supports parsing and printing JSON that contains objects that have multiple members with the same name. `cJSON_GetObjectItemCaseSensitive` however will always only return the first one.\n\n# Enjoy cJSON!\n\n- Dave Gamble (original author)\n- Max Bruckner and Alan Wang (current maintainer)\n- and the other [cJSON contributors](CONTRIBUTORS.md)\n",
      "stars_today": 2
    },
    {
      "id": 261039251,
      "name": "swift-composable-architecture",
      "full_name": "pointfreeco/swift-composable-architecture",
      "description": "A library for building applications in a consistent and understandable way, with composition, testing, and ergonomics in mind.",
      "html_url": "https://github.com/pointfreeco/swift-composable-architecture",
      "stars": 14281,
      "forks": 1628,
      "language": "Swift",
      "topics": [
        "architecture",
        "composition",
        "modularity",
        "swiftui",
        "testability",
        "uikit"
      ],
      "created_at": "2020-05-03T23:18:40Z",
      "updated_at": "2026-01-27T22:07:13Z",
      "pushed_at": "2025-12-30T21:38:44Z",
      "open_issues": 24,
      "owner": {
        "login": "pointfreeco",
        "avatar_url": "https://avatars.githubusercontent.com/u/29466629?v=4"
      },
      "readme": "# The Composable Architecture\n\n[![CI](https://github.com/pointfreeco/swift-composable-architecture/actions/workflows/ci.yml/badge.svg)](https://github.com/pointfreeco/swift-composable-architecture/actions/workflows/ci.yml)\n[![Slack](https://img.shields.io/badge/slack-chat-informational.svg?label=Slack&logo=slack)](https://www.pointfree.co/slack-invite)\n[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fpointfreeco%2Fswift-composable-architecture%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/pointfreeco/swift-composable-architecture)\n[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fpointfreeco%2Fswift-composable-architecture%2Fbadge%3Ftype%3Dplatforms)](https://swiftpackageindex.com/pointfreeco/swift-composable-architecture)\n\nThe Composable Architecture (TCA, for short) is a library for building applications in a consistent \nand understandable way, with composition, testing, and ergonomics in mind. It can be used in \nSwiftUI, UIKit, and more, and on any Apple platform (iOS, macOS, iPadOS, visionOS, tvOS, and watchOS).\n\n* [What is the Composable Architecture?](#what-is-the-composable-architecture)\n* [Learn more](#learn-more)\n* [Examples](#examples)\n* [Basic usage](#basic-usage)\n* [Documentation](#documentation)\n* [FAQ](#faq)\n* [Community](#community)\n* [Installation](#installation)\n* [Translations](#translations)\n\n## What is the Composable Architecture?\n\nThis library provides a few core tools that can be used to build applications of varying purpose and \ncomplexity. It provides compelling stories that you can follow to solve many problems you encounter \nday-to-day when building applications, such as:\n\n* **State management**\n  <br> How to manage the state of your application using simple value types, and share state across \n  many screens so that mutations in one screen can be immediately observed in another screen.\n\n* **Composition**\n  <br> How to break down large features into smaller components that can be extracted to their own, \n  isolated modules and be easily glued back together to form the feature.\n\n* **Side effects**\n  <br> How to let certain parts of the application talk to the outside world in the most testable \n  and understandable way possible.\n\n* **Testing**\n  <br> How to not only test a feature built in the architecture, but also write integration tests \n  for features that have been composed of many parts, and write end-to-end tests to understand how \n  side effects influence your application. This allows you to make strong guarantees that your \n  business logic is running in the way you expect.\n\n* **Ergonomics**\n  <br> How to accomplish all of the above in a simple API with as few concepts and moving parts as \n  possible.\n\n## Learn More\n\nThe Composable Architecture was designed over the course of many episodes on \n[Point-Free][pointfreeco], a video series exploring advanced programming topics in the Swift language, \nhosted by [Brandon Williams][mbrandonw] and [Stephen Celis][stephencelis].\n\nYou can watch all of the episodes [here][tca-episode-collection], as well as a dedicated, [multipart\ntour][tca-tour] of the architecture from scratch.\n\n<a href=\"https://www.pointfree.co/collections/tours/composable-architecture-1-0\">\n  <img alt=\"video poster image\" src=\"https://d3rccdn33rt8ze.cloudfront.net/episodes/0243.jpeg\" width=\"600\">\n</a>\n\n## Examples\n\n[![Screen shots of example applications](https://d3rccdn33rt8ze.cloudfront.net/composable-architecture/demos.png)](./Examples)\n\nThis repo comes with _lots_ of examples to demonstrate how to solve common and complex problems with \nthe Composable Architecture. Check out [this](./Examples) directory to see them all, including:\n\n* [Case Studies](./Examples/CaseStudies)\n  * Getting started\n  * Effects\n  * Navigation\n  * Higher-order reducers\n  * Reusable components\n* [Location manager](https://github.com/pointfreeco/composable-core-location/tree/main/Examples/LocationManager)\n* [Motion manager](https://github.com/pointfreeco/composable-core-motion/tree/main/Examples/MotionManager)\n* [Search](./Examples/Search)\n* [Speech Recognition](./Examples/SpeechRecognition)\n* [SyncUps app](./Examples/SyncUps)\n* [Tic-Tac-Toe](./Examples/TicTacToe)\n* [Todos](./Examples/Todos)\n* [Voice memos](./Examples/VoiceMemos)\n\nLooking for something more substantial? Check out the source code for [isowords][gh-isowords], an \niOS word search game built in SwiftUI and the Composable Architecture.\n\n## Basic Usage\n\n> [!Note] \n> For a step-by-step interactive tutorial, be sure to check out [Meet the Composable\n> Architecture][meet-tca].\n\nTo build a feature using the Composable Architecture you define some types and values that model \nyour domain:\n\n* **State**: A type that describes the data your feature needs to perform its logic and render its \nUI.\n* **Action**: A type that represents all of the actions that can happen in your feature, such as \nuser actions, notifications, event sources and more.\n* **Reducer**: A function that describes how to evolve the current state of the app to the next \nstate given an action. The reducer is also responsible for returning any effects that should be \nrun, such as API requests, which can be done by returning an `Effect` value.\n* **Store**: The runtime that actually drives your feature. You send all user actions to the store \nso that the store can run the reducer and effects, and you can observe state changes in the store \nso that you can update UI.\n\nThe benefits of doing this are that you will instantly unlock testability of your feature, and you \nwill be able to break large, complex features into smaller domains that can be glued together.\n\nAs a basic example, consider a UI that shows a number along with \"+\" and \"âˆ’\" buttons that increment \nand decrement the number. To make things interesting, suppose there is also a button that when \ntapped makes an API request to fetch a random fact about that number and displays it in the view.\n\nTo implement this feature we create a new type that will house the domain and behavior of the \nfeature, and it will be annotated with the `@Reducer` macro:\n\n```swift\nimport ComposableArchitecture\n\n@Reducer\nstruct Feature {\n}\n```\n\nIn here we need to define a type for the feature's state, which consists of an integer for the \ncurrent count, as well as an optional string that represents the fact being presented:\n\n```swift\n@Reducer\nstruct Feature {\n  @ObservableState\n  struct State: Equatable {\n    var count = 0\n    var numberFact: String?\n  }\n}\n```\n\n> [!Note] \n> We've applied the `@ObservableState` macro to `State` in order to take advantage of the\n> observation tools in the library.\n\nWe also need to define a type for the feature's actions. There are the obvious actions, such as \ntapping the decrement button, increment button, or fact button. But there are also some slightly \nnon-obvious ones, such as the action that occurs when we receive a response from the fact API \nrequest:\n\n```swift\n@Reducer\nstruct Feature {\n  @ObservableState\n  struct State: Equatable { /* ... */ }\n  enum Action {\n    case decrementButtonTapped\n    case incrementButtonTapped\n    case numberFactButtonTapped\n    case numberFactResponse(String)\n  }\n}\n```\n\nAnd then we implement the `body` property, which is responsible for composing the actual logic and \nbehavior for the feature. In it we can use the `Reduce` reducer to describe how to change the\ncurrent state to the next state, and what effects need to be executed. Some actions don't need to\nexecute effects, and they can return `.none` to represent that:\n\n```swift\n@Reducer\nstruct Feature {\n  @ObservableState\n  struct State: Equatable { /* ... */ }\n  enum Action { /* ... */ }\n\n  var body: some Reducer<State, Action> {\n    Reduce { state, action in\n      switch action {\n      case .decrementButtonTapped:\n        state.count -= 1\n        return .none\n\n      case .incrementButtonTapped:\n        state.count += 1\n        return .none\n\n      case .numberFactButtonTapped:\n        return .run { [count = state.count] send in\n          let (data, _) = try await URLSession.shared.data(\n            from: URL(string: \"http://numbersapi.com/\\(count)/trivia\")!\n          )\n          await send(\n            .numberFactResponse(String(decoding: data, as: UTF8.self))\n          )\n        }\n\n      case let .numberFactResponse(fact):\n        state.numberFact = fact\n        return .none\n      }\n    }\n  }\n}\n```\n\nAnd then finally we define the view that displays the feature. It holds onto a `StoreOf<Feature>` \nso that it can observe all changes to the state and re-render, and we can send all user actions to \nthe store so that state changes:\n\n```swift\nstruct FeatureView: View {\n  let store: StoreOf<Feature>\n\n  var body: some View {\n    Form {\n      Section {\n        Text(\"\\(store.count)\")\n        Button(\"Decrement\") { store.send(.decrementButtonTapped) }\n        Button(\"Increment\") { store.send(.incrementButtonTapped) }\n      }\n\n      Section {\n        Button(\"Number fact\") { store.send(.numberFactButtonTapped) }\n      }\n      \n      if let fact = store.numberFact {\n        Text(fact)\n      }\n    }\n  }\n}\n```\n\nIt is also straightforward to have a UIKit controller driven off of this store. You can observe\nstate changes in the store in `viewDidLoad`, and then populate the UI components with data from\nthe store. The code is a bit longer than the SwiftUI version, so we have collapsed it here:\n\n<details>\n  <summary>Click to expand!</summary>\n\n  ```swift\n  class FeatureViewController: UIViewController {\n    let store: StoreOf<Feature>\n\n    init(store: StoreOf<Feature>) {\n      self.store = store\n      super.init(nibName: nil, bundle: nil)\n    }\n\n    required init?(coder: NSCoder) {\n      fatalError(\"init(coder:) has not been implemented\")\n    }\n\n    override func viewDidLoad() {\n      super.viewDidLoad()\n\n      let countLabel = UILabel()\n      let decrementButton = UIButton()\n      let incrementButton = UIButton()\n      let factLabel = UILabel()\n      \n      // Omitted: Add subviews and set up constraints...\n      \n      observe { [weak self] in\n        guard let self \n        else { return }\n        \n        countLabel.text = \"\\(self.store.count)\"\n        factLabel.text = self.store.numberFact\n      }\n    }\n\n    @objc private func incrementButtonTapped() {\n      self.store.send(.incrementButtonTapped)\n    }\n    @objc private func decrementButtonTapped() {\n      self.store.send(.decrementButtonTapped)\n    }\n    @objc private func factButtonTapped() {\n      self.store.send(.numberFactButtonTapped)\n    }\n  }\n  ```\n</details>\n\nOnce we are ready to display this view, for example in the app's entry point, we can construct a \nstore. This can be done by specifying the initial state to start the application in, as well as \nthe reducer that will power the application:\n\n```swift\nimport ComposableArchitecture\n\n@main\nstruct MyApp: App {\n  var body: some Scene {\n    WindowGroup {\n      FeatureView(\n        store: Store(initialState: Feature.State()) {\n          Feature()\n        }\n      )\n    }\n  }\n}\n```\n\nAnd that is enough to get something on the screen to play around with. It's definitely a few more \nsteps than if you were to do this in a vanilla SwiftUI way, but there are a few benefits. It gives \nus a consistent manner to apply state mutations, instead of scattering logic in some observable \nobjects and in various action closures of UI components. It also gives us a concise way of \nexpressing side effects. And we can immediately test this logic, including the effects, without \ndoing much additional work.\n\n### Testing\n\n> [!Note] \n> For more in-depth information on testing, see the dedicated [testing][testing-article] article. \n\nTo test use a `TestStore`, which can be created with the same information as the `Store`, but it \ndoes extra work to allow you to assert how your feature evolves as actions are sent:\n\n```swift\n@Test\nfunc basics() async {\n  let store = TestStore(initialState: Feature.State()) {\n    Feature()\n  }\n}\n```\n\nOnce the test store is created we can use it to make an assertion of an entire user flow of steps. \nEach step of the way we need to prove that state changed how we expect. For example, we can \nsimulate the user flow of tapping on the increment and decrement buttons:\n\n```swift\n// Test that tapping on the increment/decrement buttons changes the count\nawait store.send(.incrementButtonTapped) {\n  $0.count = 1\n}\nawait store.send(.decrementButtonTapped) {\n  $0.count = 0\n}\n```\n\nFurther, if a step causes an effect to be executed, which feeds data back into the store, we must \nassert on that. For example, if we simulate the user tapping on the fact button we expect to \nreceive a fact response back with the fact, which then causes the `numberFact` state to be \npopulated:\n\n```swift\nawait store.send(.numberFactButtonTapped)\n\nawait store.receive(\\.numberFactResponse) {\n  $0.numberFact = ???\n}\n```\n\nHowever, how do we know what fact is going to be sent back to us?\n\nCurrently our reducer is using an effect that reaches out into the real world to hit an API server, \nand that means we have no way to control its behavior. We are at the whims of our internet \nconnectivity and the availability of the API server in order to write this test.\n\nIt would be better for this dependency to be passed to the reducer so that we can use a live \ndependency when running the application on a device, but use a mocked dependency for tests. We can \ndo this by adding a property to the `Feature` reducer:\n\n```swift\n@Reducer\nstruct Feature {\n  let numberFact: (Int) async throws -> String\n  // ...\n}\n```\n\nThen we can use it in the `reduce` implementation:\n\n```swift\ncase .numberFactButtonTapped:\n  return .run { [count = state.count] send in \n    let fact = try await self.numberFact(count)\n    await send(.numberFactResponse(fact))\n  }\n```\n\nAnd in the entry point of the application we can provide a version of the dependency that actually \ninteracts with the real world API server:\n\n```swift\n@main\nstruct MyApp: App {\n  var body: some Scene {\n    WindowGroup {\n      FeatureView(\n        store: Store(initialState: Feature.State()) {\n          Feature(\n            numberFact: { number in\n              let (data, _) = try await URLSession.shared.data(\n                from: URL(string: \"http://numbersapi.com/\\(number)\")!\n              )\n              return String(decoding: data, as: UTF8.self)\n            }\n          )\n        }\n      )\n    }\n  }\n}\n```\n\nBut in tests we can use a mock dependency that immediately returns a deterministic, predictable \nfact: \n\n```swift\n@Test\nfunc basics() async {\n  let store = TestStore(initialState: Feature.State()) {\n    Feature(numberFact: { \"\\($0) is a good number Brent\" })\n  }\n}\n```\n\nWith that little bit of upfront work we can finish the test by simulating the user tapping on the \nfact button, and then receiving the response from the dependency to present the fact:\n\n```swift\nawait store.send(.numberFactButtonTapped)\n\nawait store.receive(\\.numberFactResponse) {\n  $0.numberFact = \"0 is a good number Brent\"\n}\n```\n\nWe can also improve the ergonomics of using the `numberFact` dependency in our application. Over \ntime the application may evolve into many features, and some of those features may also want access \nto `numberFact`, and explicitly passing it through all layers can get annoying. There is a process \nyou can follow to â€œregisterâ€ dependencies with the library, making them instantly available to any \nlayer in the application.\n\n> [!Note] \n> For more in-depth information on dependency management, see the dedicated\n> [dependencies][dependencies-article] article. \n\nWe can start by wrapping the number fact functionality in a new type:\n\n```swift\nstruct NumberFactClient {\n  var fetch: (Int) async throws -> String\n}\n```\n\nAnd then registering that type with the dependency management system by conforming the client to\nthe `DependencyKey` protocol, which requires you to specify the live value to use when running the\napplication in simulators or devices:\n\n```swift\nextension NumberFactClient: DependencyKey {\n  static let liveValue = Self(\n    fetch: { number in\n      let (data, _) = try await URLSession.shared\n        .data(from: URL(string: \"http://numbersapi.com/\\(number)\")!\n      )\n      return String(decoding: data, as: UTF8.self)\n    }\n  )\n}\n\nextension DependencyValues {\n  var numberFact: NumberFactClient {\n    get { self[NumberFactClient.self] }\n    set { self[NumberFactClient.self] = newValue }\n  }\n}\n```\n\nWith that little bit of upfront work done you can instantly start making use of the dependency in \nany feature by using the `@Dependency` property wrapper:\n\n```diff\n @Reducer\n struct Feature {\n-  let numberFact: (Int) async throws -> String\n+  @Dependency(\\.numberFact) var numberFact\n   \n   â€¦\n\n-  try await self.numberFact(count)\n+  try await self.numberFact.fetch(count)\n }\n```\n\nThis code works exactly as it did before, but you no longer have to explicitly pass the dependency \nwhen constructing the feature's reducer. When running the app in previews, the simulator or on a \ndevice, the live dependency will be provided to the reducer, and in tests the test dependency will \nbe provided.\n\nThis means the entry point to the application no longer needs to construct dependencies:\n\n```swift\n@main\nstruct MyApp: App {\n  var body: some Scene {\n    WindowGroup {\n      FeatureView(\n        store: Store(initialState: Feature.State()) {\n          Feature()\n        }\n      )\n    }\n  }\n}\n```\n\nAnd the test store can be constructed without specifying any dependencies, but you can still \noverride any dependency you need to for the purpose of the test:\n\n```swift\nlet store = TestStore(initialState: Feature.State()) {\n  Feature()\n} withDependencies: {\n  $0.numberFact.fetch = { \"\\($0) is a good number Brent\" }\n}\n\n// ...\n```\n\nThat is the basics of building and testing a feature in the Composable Architecture. There are \n_a lot_ more things to be explored, such as composition, modularity, adaptability, and complex \neffects. The [Examples](./Examples) directory has a bunch of projects to explore to see more \nadvanced usages.\n\n## Documentation\n\nThe documentation for releases and `main` are available here:\n\n* [`main`](https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture)\n* [1.x.x](https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/~/documentation/composablearchitecture)\n\nThere are a number of articles in the documentation that you may find helpful as you become more \ncomfortable with the library:\n\n* [Getting started][getting-started-article]\n* [Dependencies][dependencies-article]\n* [Testing][testing-article]\n* [Navigation][navigation-article]\n* [Sharing state][sharing-state-article]\n* [Performance][performance-article]\n* [Concurrency][concurrency-article]\n* [Bindings][bindings-article]\n\n## FAQ\n\nWe have a [dedicated article][faq-article] for all of the most frequently asked questions and\ncomments people have concerning the library.\n\n## Community\n\nIf you want to discuss the Composable Architecture or have a question about how to use it to solve \na particular problem, there are a number of places you can discuss with fellow \n[Point-Free](http://www.pointfree.co) enthusiasts:\n\n* For long-form discussions, we recommend the [discussions][gh-discussions] tab of this repo.\n* For casual chat, we recommend the [Point-Free Community slack](http://pointfree.co/slack-invite).\n\n## Installation\n\nYou can add ComposableArchitecture to an Xcode project by adding it as a package dependency.\n\n  1. From the **File** menu, select **Add Package Dependencies...**\n  2. Enter \"https://github.com/pointfreeco/swift-composable-architecture\" into the package \n     repository URL text field\n  3. Depending on how your project is structured:\n      - If you have a single application target that needs access to the library, then add \n        **ComposableArchitecture** directly to your application.\n      - If you want to use this library from multiple Xcode targets, or mix Xcode targets and SPM \n        targets, you must create a shared framework that depends on **ComposableArchitecture** and \n        then depend on that framework in all of your targets. For an example of this, check out the \n        [Tic-Tac-Toe](./Examples/TicTacToe) demo application, which splits lots of features into \n        modules and consumes the static library in this fashion using the **tic-tac-toe** Swift \n        package.\n\n## Companion libraries\n\nThe Composable Architecture is built with extensibility in mind, and there are a number of\ncommunity-supported libraries available to enhance your applications:\n\n* [Composable Architecture Extras](https://github.com/Ryu0118/swift-composable-architecture-extras):\n  A companion library to the Composable Architecture.\n* [TCAComposer](https://github.com/mentalflux/tca-composer): A macro framework for generating\n  boiler-plate code in the Composable Architecture.\n* [TCACoordinators](https://github.com/johnpatrickmorgan/TCACoordinators): The coordinator pattern\n  in the Composable Architecture.\n\nIf you'd like to contribute a library, please [open a\nPR](https://github.com/pointfreeco/swift-composable-architecture/edit/main/README.md) with a link\nto it!\n\n## Translations\n\nThe following translations of this README have been contributed by members of the community:\n\n* [Arabic](https://gist.github.com/NorhanBoghdadi/1b98d55c02b683ddef7e05c2ebcccd47)\n* [French](https://gist.github.com/nikitamounier/0e93eb832cf389db12f9a69da030a2dc)\n* [Hindi](https://gist.github.com/akashsoni01/b358ee0b3b747167964ef6946123c88d)\n* [Indonesian](https://gist.github.com/wendyliga/792ea9ac5cc887f59de70a9e39cc7343)\n* [Italian](https://gist.github.com/Bellaposa/5114e6d4d55fdb1388e8186886d48958)\n* [Japanese](https://gist.github.com/Achoo-kr/2d0712deb77f78b3379551ac7baea3e4)\n* [Korean](https://gist.github.com/Achoo-kr/5d8936d12e71028fcc4a7c5e078ca038)\n* [Polish](https://gist.github.com/MarcelStarczyk/6b6153051f46912a665c32199f0d1d54)\n* [Portuguese](https://gist.github.com/SevioCorrea/2bbf337cd084a58c89f2f7f370626dc8)\n* [Russian](https://gist.github.com/SubvertDev/3317d0c3b35ed601be330d6fc0df5aba)\n* [Simplified Chinese](https://gist.github.com/sh3l6orrr/10c8f7c634a892a9c37214f3211242ad)\n* [Spanish](https://gist.github.com/pitt500/f5e32fccb575ce112ffea2827c7bf942)\n* [Turkish](https://gist.github.com/gokhanamal/93001244ef0c1cec58abeb1afc0de37c)\n* [Ukrainian](https://gist.github.com/barabashd/33b64676195ce41f4bb73c327ea512a8)\n\nIf you'd like to contribute a translation, please [open a\nPR](https://github.com/pointfreeco/swift-composable-architecture/edit/main/README.md) with a link \nto a [Gist](https://gist.github.com)!\n\n## Credits and thanks\n\nThe following people gave feedback on the library at its early stages and helped make the library \nwhat it is today:\n\nPaul Colton, Kaan Dedeoglu, Matt Diephouse, Josef DoleÅ¾al, Eimantas, Matthew Johnson, George \nKaimakas, Nikita Leonov, Christopher Liscio, Jeffrey Macko, Alejandro Martinez, Shai Mishali, Willis \nPlummer, Simon-Pierre Roy, Justin Price, Sven A. Schmidt, Kyle Sherman, Petr Å Ã­ma, Jasdev Singh, \nMaxim Smirnov, Ryan Stone, Daniel Hollis Tavares, and all of the [Point-Free][pointfreeco] \nsubscribers ğŸ˜.\n\nSpecial thanks to [Chris Liscio](https://twitter.com/liscio) who helped us work through many strange \nSwiftUI quirks and helped refine the final API.\n\nAnd thanks to [Shai Mishali](https://github.com/freak4pc) and the\n[CombineCommunity](https://github.com/CombineCommunity/CombineExt/) project, from which we took \ntheir implementation of `Publishers.Create`, which we use in `Effect` to help bridge delegate and \ncallback-based APIs, making it much easier to interface with 3rd party frameworks.\n\n## Other libraries\n\nThe Composable Architecture was built on a foundation of ideas started by other libraries, in \nparticular [Elm](https://elm-lang.org) and [Redux](https://redux.js.org/).\n\nThere are also many architecture libraries in the Swift and iOS community. Each one of these has \ntheir own set of priorities and trade-offs that differ from the Composable Architecture.\n\n* [RIBs](https://github.com/uber/RIBs)\n* [Loop](https://github.com/ReactiveCocoa/Loop)\n* [ReSwift](https://github.com/ReSwift/ReSwift)\n* [Workflow](https://github.com/square/workflow)\n* [ReactorKit](https://github.com/ReactorKit/ReactorKit)\n* [RxFeedback](https://github.com/NoTests/RxFeedback.swift)\n* [Mobius.swift](https://github.com/spotify/mobius.swift)\n* <details>\n  <summary>And more</summary>\n\n  * [Fluxor](https://github.com/FluxorOrg/Fluxor)\n  * [PromisedArchitectureKit](https://github.com/RPallas92/PromisedArchitectureKit)\n  </details>\n\n## License\n\nThis library is released under the MIT license. See [LICENSE](LICENSE) for details.\n\n[pointfreeco]: https://www.pointfree.co\n[mbrandonw]: https://twitter.com/mbrandonw\n[stephencelis]: https://twitter.com/stephencelis\n[tca-episode-collection]: https://www.pointfree.co/collections/composable-architecture\n[tca-tour]: https://www.pointfree.co/collections/tours/composable-architecture-1-0\n[gh-isowords]: https://github.com/pointfreeco/isowords\n[gh-discussions]: https://github.com/pointfreeco/swift-composable-architecture/discussions\n[swift-forum]: https://forums.swift.org/c/related-projects/swift-composable-architecture\n[testing-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/testingtca\n[faq-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/faq\n[dependencies-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/dependencymanagement\n[getting-started-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/gettingstarted\n[navigation-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/navigation\n[performance-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/performance\n[concurrency-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/swiftconcurrency\n[bindings-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/bindings\n[sharing-state-article]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/documentation/composablearchitecture/sharingstate\n[meet-tca]: https://swiftpackageindex.com/pointfreeco/swift-composable-architecture/main/tutorials/meetcomposablearchitecture\n",
      "stars_today": 2
    },
    {
      "id": 40136600,
      "name": "ktor",
      "full_name": "ktorio/ktor",
      "description": "Framework for quickly creating connected applications in Kotlin with minimal effort",
      "html_url": "https://github.com/ktorio/ktor",
      "stars": 14250,
      "forks": 1221,
      "language": "Kotlin",
      "topics": [
        "async",
        "asynchronous",
        "kotlin",
        "web",
        "web-framework"
      ],
      "created_at": "2015-08-03T16:49:36Z",
      "updated_at": "2026-01-27T23:52:39Z",
      "pushed_at": "2026-01-27T16:49:46Z",
      "open_issues": 199,
      "owner": {
        "login": "ktorio",
        "avatar_url": "https://avatars.githubusercontent.com/u/28214161?v=4"
      },
      "readme": "<div align=\"center\">\n\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/ktorio/ktor/main/.github/images/ktor-logo-for-dark.svg\">\n    <img alt=\"Ktor logo\" src=\"https://raw.githubusercontent.com/ktorio/ktor/main/.github/images/ktor-logo-for-light.svg\">\n  </picture>\n\n</div>\n\n[![Official JetBrains project](https://jb.gg/badges/official.svg)](https://confluence.jetbrains.com/display/ALL/JetBrains+on+GitHub)\n[![Maven Central](https://img.shields.io/maven-central/v/io.ktor/ktor-server)](https://central.sonatype.com/search?namespace=io.ktor)\n[![Kotlin](https://img.shields.io/badge/kotlin-2.3.0-blue.svg?logo=kotlin)](https://kotlinlang.org)\n[![Slack channel](https://img.shields.io/badge/chat-slack-green.svg?logo=slack)](https://kotlinlang.slack.com/messages/ktor/)\n[![GitHub License](https://img.shields.io/badge/license-Apache%20License%202.0-blue.svg?style=flat)](https://www.apache.org/licenses/LICENSE-2.0)\n[![Contribute with Gitpod](https://img.shields.io/badge/Contribute%20with-Gitpod-908a85?logo=gitpod)](https://gitpod.io/#https://github.com/ktorio/ktor)\n\nKtor is an asynchronous framework for creating microservices, web applications and more. Written in Kotlin from the\nground up.\n\nFirst add the dependency to your project:\n\n```kotlin\nrepositories {\n    mavenCentral()\n}\n\ndependencies {\n    implementation(\"io.ktor:ktor-server-netty:$ktor_version\")\n}\n```\n\nThen create an `Application` and install some features:\n\n```kotlin\nimport io.ktor.server.netty.*\nimport io.ktor.server.routing.*\nimport io.ktor.server.application.*\nimport io.ktor.http.*\nimport io.ktor.server.response.*\nimport io.ktor.server.engine.*\n\nfun main(args: Array<String>) {\n    embeddedServer(Netty, 8080) {\n        routing {\n            get(\"/\") {\n                call.respondText(\"Hello, world!\", ContentType.Text.Html)\n            }\n        }\n    }.start(wait = true)\n}\n```\n\nYou also can use [Ktor Gradle Plugin](https://github.com/ktorio/ktor-build-plugins) to configure bom, run tasks and deployment:\n```kotlin\nplugins {\n    id(\"io.ktor.plugin\") version \"3.1.1\"\n}\n\ndependencies {\n    implementation(\"io.ktor:ktor-server-netty\")\n}\n```\n\nTo run the created application, execute:\n```shell\n./gradlew run\n```\n\n* Runs embedded web server on `localhost:8080`\n* Installs routing and responds with `Hello, world!` when receiving a GET http request for the root path\n\n## Start using Ktor\n\nBuild your first Kotlin HTTP or RESTful application using Ktor: [start.ktor.io](https://start.ktor.io)\n\n## Principles\n\n#### Unopinionated\n\nKtor Framework doesn't impose a lot of constraints on what technology a project is going to use â€“ logging,\ntemplating, messaging, persistence, serialization, dependency injection, etc.\nSometimes it may be required to implement a simple interface, but usually it is a matter of writing a\ntransforming or intercepting function. Features are installed into the application using a unified *interception*\nmechanism\nwhich allows building arbitrary pipelines.\n\nKtor Applications can be hosted in any servlet container with Servlet 3.0+ API support such as Tomcat, or\nstandalone using Netty or Jetty. Support for other hosts can be added through the unified hosting API.\n\nKtor APIs are mostly functions calls with lambdas. Thanks to Kotlin DSL capabilities, the code looks declarative.\nApplication composition is entirely up to the developer's choice â€“ with functions or classes, using dependency injection\nframework or doing it all manually in the main function.\n\n#### Asynchronous\n\nThe Ktor pipeline machinery and API are utilising Kotlin coroutines to provide easy-to-use asynchronous\nprogramming model without making it too cumbersome. All host implementations are using asynchronous I/O facilities\nto avoid thread blocking.\n\n#### Testable\n\nKtor applications can be hosted in a special test environment, which emulates a web server to some\nextent without actually doing any networking. It provides easy way to test an application without mocking\ntoo much stuff, and still achieve good performance while validating application calls. Running integration tests with a\nreal\nembedded web server are of course possible, too.\n\n## JetBrains Product\n\nKtor is an official [JetBrains](https://jetbrains.com) product and is primarily developed by the team at JetBrains, with\ncontributions\nfrom the community.\n\n## Documentation\n\nPlease visit [ktor.io](https://ktor.io) for Quick Start and detailed explanations of features, usage and machinery.\n\n* Getting started with [Gradle](https://ktor.io/docs/gradle.html)\n* Getting started with [Maven](https://ktor.io/docs/maven.html)\n* Getting started with [IDEA](https://ktor.io/docs/intellij-idea.html)\n\n## Reporting Issues / Support\n\nPlease use [our issue tracker](https://youtrack.jetbrains.com/issues/KTOR) for filing feature requests and bugs. If\nyou'd like to ask a question, we recommend [StackOverflow](https://stackoverflow.com/questions/tagged/ktor) where\nmembers of the team monitor frequently.\n\nThere is also community support on the [Kotlin Slack Ktor channel](https://app.slack.com/client/T09229ZC6/C0A974TJ9)\n\n## Reporting Security Vulnerabilities\n\nIf you find a security vulnerability in Ktor, we kindly request that you reach out to the JetBrains security team via\nour [responsible disclosure process](https://www.jetbrains.com/legal/terms/responsible-disclosure.html).\n\n## Inspirations\n\nKotlin web frameworks such as Wasabi and Kara, which are currently deprecated.\n\n## Contributing\n\nPlease see [the contribution guide](CONTRIBUTING.md) and the [Code of conduct](CODE_OF_CONDUCT.md) before contributing.\n",
      "stars_today": 2
    },
    {
      "id": 17253131,
      "name": "renderdoc",
      "full_name": "baldurk/renderdoc",
      "description": "RenderDoc is a stand-alone graphics debugging tool.",
      "html_url": "https://github.com/baldurk/renderdoc",
      "stars": 10361,
      "forks": 1530,
      "language": "C++",
      "topics": [
        "d3d11",
        "d3d12",
        "debugger",
        "direct3d",
        "directx",
        "graphics",
        "graphics-programming",
        "opengl",
        "renderdoc",
        "vulkan",
        "vulkan-api"
      ],
      "created_at": "2014-02-27T15:16:30Z",
      "updated_at": "2026-01-28T00:58:11Z",
      "pushed_at": "2026-01-27T21:45:06Z",
      "open_issues": 54,
      "owner": {
        "login": "baldurk",
        "avatar_url": "https://avatars.githubusercontent.com/u/661798?v=4"
      },
      "readme": "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/661798/36482670-f81601c0-170b-11e8-8adb-2365b346ac27.png\" /></p>\n\n[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE.md)\n[![CI](https://github.com/baldurk/renderdoc/workflows/CI/badge.svg?branch=v1.x&event=push)](https://github.com/baldurk/renderdoc/actions)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](docs/CODE_OF_CONDUCT.md) \n\nRenderDoc is a frame-capture based graphics debugger, currently available for Vulkan, D3D11, D3D12, OpenGL, and OpenGL ES development on Windows, Linux, Android, and Nintendo Switch&trade;. It is completely open-source under the MIT license.\n\nRenderDoc is intended for debugging your own programs only. Any discussion of capturing programs that you did not create will not be allowed in any official public RenderDoc setting, including the issue tracker, discord, or via email. For example this includes capturing commercial games that you did not create, or capturing Google Maps or Google Earth. Note: Capturing projects you created that use a third party engine like Unreal or Unity, or open source and free projects is completely fine and supported.\n\nIf you have any questions, suggestions or problems or you can [create an issue](https://github.com/baldurk/renderdoc/issues/new/choose) here on github, [email me directly](mailto:baldurk@baldurk.org) or come into [IRC](https://webchat.oftc.net/?channels=renderdoc) or [Discord](https://discord.gg/ahq6yRB) to discuss it.\n\nTo install on windows run the appropriate installer for your OS ([64-bit](https://renderdoc.org/stable/latest/RenderDoc_latest_64.msi) | [32-bit](https://renderdoc.org/stable/latest/RenderDoc_latest_32.msi)) or download the portable zip from the [builds page](https://renderdoc.org/builds). The 64-bit windows build fully supports capturing from 32-bit programs. On linux only 64-bit x86 is supported - there is a precompiled [binary tarball](https://renderdoc.org/stable/latest/renderdoc_latest.tar.gz) available, or your distribution may package it. If not you can [build from source](docs/CONTRIBUTING/Compiling.md).\n\n* **Downloads**: Stable and nightly builds: https://renderdoc.org/builds ( [Symbol server](https://renderdoc.org/symbols) )\n* **Documentation**: [HTML online](https://renderdoc.org/docs), [CHM in builds](https://renderdoc.org/docs/renderdoc.chm), [Videos](https://www.youtube.com/user/baldurkarlsson)\n* **Contact**: [baldurk@baldurk.org](mailto:baldurk@baldurk.org), [#renderdoc on OFTC IRC](https://webchat.oftc.net/?channels=renderdoc), [Discord server](https://discord.gg/ahq6yRB)\n* **Code of Conduct**: [Contributor Covenant](docs/CODE_OF_CONDUCT.md)\n* **Information for contributors**: [All contribution information](docs/CONTRIBUTING.md), [Compilation instructions](docs/CONTRIBUTING/Compiling.md)\n* **Community extensions**: [Extensions repository](https://github.com/baldurk/renderdoc-contrib)\n\nScreenshots\n--------------\n\n| [ ![Texture view](https://renderdoc.org/fp/ts_screen1.jpg?2) ](https://renderdoc.org/fp/screen1.jpg) | [ ![Pixel history & shader debug](https://renderdoc.org/fp/ts_screen2.jpg?2) ](https://renderdoc.org/fp/screen2.png) |\n| --- | --- |\n| [ ![Mesh viewer](https://renderdoc.org/fp/ts_screen3.jpg?2) ](https://renderdoc.org/fp/screen3.png) | [ ![Pipeline viewer & constants](https://renderdoc.org/fp/ts_screen4.jpg?2) ](https://renderdoc.org/fp/screen4.png) |\n\nAPI Support\n--------------\n\n|                          | Windows                  | Linux                    | Android                   |\n| ------------------------ | ------------------------ | ------------------------ | ------------------------  |\n| Vulkan                   | :heavy_check_mark:       | :heavy_check_mark:       | :heavy_check_mark:        |\n| OpenGL ES 2.0 - 3.2      | :heavy_check_mark:       | :heavy_check_mark:       | :heavy_check_mark:        |\n| OpenGL 3.2 - 4.6 Core    | :heavy_check_mark:       | :heavy_check_mark:       |  N/A                      |\n| D3D11 & D3D12            | :heavy_check_mark:       |  N/A                     |  N/A                      |\n| OpenGL 1.0 - 2.0 Compat  | :heavy_multiplication_x: | :heavy_multiplication_x: |  N/A                      |\n| D3D9 & 10                | :heavy_multiplication_x: |  N/A                     |  N/A                      |\n| Metal                    |  N/A                     |  N/A                     |  N/A                      |\n\n* Nintendo Switch&trade; support is distributed separately for authorized developers as part of the NintendoSDK. For more information, consult the Nintendo Developer Portal.\n\nDownloads\n--------------\n\nThere are [binary releases](https://renderdoc.org/builds) available, built from the release targets. If you just want to use the program and you ended up here, this is what you want :).\n\nIt's recommended that if you're new you start with the stable builds. Nightly builds are available every day from the [v1.x branch here](https://renderdoc.org/builds#nightly) if you need it, but correspondingly may be less stable.\n\nDocumentation\n--------------\n\nThe text documentation is available [online for the latest stable version](https://renderdoc.org/docs/), as well as in [renderdoc.chm](https://renderdoc.org/docs/renderdoc.chm) in any build. It's built from [restructured text with sphinx](docs).\n\nAs mentioned above there are some [youtube videos](https://www.youtube.com/user/baldurkarlsson) showing the use of some basic features and an introduction/overview.\n\nThere is also a great presentation by [@Icetigris](https://twitter.com/Icetigris) which goes into some details of how RenderDoc can be used in real world situations: [slides are up here](https://docs.google.com/presentation/d/1LQUMIld4SGoQVthnhT1scoA3k4Sg0as14G4NeSiSgFU/edit#slide=id.p).\n\nLicense\n--------------\n\nRenderDoc is released under the MIT license, see [LICENSE.md](LICENSE.md) for full text as well as 3rd party library acknowledgements.\n\nCompiling\n---------\n\nBuilding RenderDoc is fairly straight forward on most platforms. See [Compiling.md](docs/CONTRIBUTING/Compiling.md) for more details.\n\nContributing & Development\n--------------\n\nI've added some notes on how to contribute, as well as where to get started looking through the code in [Developing-Change.md](docs/CONTRIBUTING/Developing-Change.md). All contribution information is available under [CONTRIBUTING.md](docs/CONTRIBUTING.md).\n\n",
      "stars_today": 2
    },
    {
      "id": 6687936,
      "name": "mbedtls",
      "full_name": "Mbed-TLS/mbedtls",
      "description": "An open source, portable, easy to use, readable and flexible TLS library, and reference implementation of the PSA Cryptography API. Releases are on a varying cadence, typically around 3 - 6 months between releases.",
      "html_url": "https://github.com/Mbed-TLS/mbedtls",
      "stars": 6422,
      "forks": 2833,
      "language": "C",
      "topics": [
        "crypto",
        "cryptography-library",
        "psa",
        "ssl",
        "tls"
      ],
      "created_at": "2012-11-14T13:13:13Z",
      "updated_at": "2026-01-28T00:39:55Z",
      "pushed_at": "2026-01-22T16:28:25Z",
      "open_issues": 1568,
      "owner": {
        "login": "Mbed-TLS",
        "avatar_url": "https://avatars.githubusercontent.com/u/97226525?v=4"
      },
      "readme": "README for Mbed TLS\n===================\n\nMbed TLS is a C library that implements X.509 certificate manipulation and the TLS and DTLS protocols. Its small code footprint makes it suitable for embedded systems.\nMbed TLS includes the [TF-PSA-Crypto repository](https://github.com/Mbed-TLS/TF-PSA-Crypto) that provides an implementation of the [PSA Cryptography API](https://arm-software.github.io/psa-api).\n\nConfiguration\n-------------\nConfiguration options related to X.509 and TLS are available in `include/mbedtls/mbedtls_config.h`, while cryptography and platform options are located in the TF-PSA-Crypto configuration file `tf-psa-crypto/include/psa/crypto_config.h`.\n\nWith the default platform options, Mbed TLS should build out of the box on most systems.\n\nThese configuration files can be edited manually, or programmatically using the Python script `scripts/config.py` (run with --help for usage instructions).\n\nWe provide some non-standard configurations focused on specific use cases in the `configs/` directory. You can read more about those in `configs/README.txt`.\n\nDocumentation\n-------------\n\nThe main Mbed TLS documentation is available via [ReadTheDocs](https://mbed-tls.readthedocs.io/).\n\nTo generate a local copy of the library documentation in HTML format, tailored to your compile-time configuration:\n\n1. Make sure that [Doxygen](http://www.doxygen.nl/) is installed.\n1. Run `cmake -B /path/to/build_dir /path/to/mbedtls/source`\n1. Run `cmake --build /path/to/build_dir --target mbedtls-apidoc`\n1. Open one of the main generated HTML files:\n   * `apidoc/index.html`\n   * `apidoc/modules.html` or `apidoc/topics.html`\n\nFor other sources of documentation, see the [SUPPORT](SUPPORT.md) document.\n\nCompiling\n---------\n\nWe use CMake to configure and drive our build process. Three libraries are built: `libtfpsacrypto`, `libmbedx509`, and `libmbedtls`. Note that `libmbedtls` depends on `libmbedx509` and `libtfpsacrypto`, and `libmbedx509` depends on `libtfpsacrypto`. As a result, some linkers will expect flags to be in a specific order, for example the GNU linker wants `-lmbedtls -lmbedx509 -ltfpsacrypto`. The cryptographic library `libtfpsacrypto` is also provided under its legacy name, `libmbedcrypto`.\n\n### Tool versions\n\nYou need the following tools to build the library from the main branch with the provided CMake files. Mbed TLS minimum tool version requirements are set based on the versions shipped in the latest or penultimate (depending on the release cadence) long-term support releases of major Linux distributions, namely at time of writing: Ubuntu 22.04, RHEL 9, and SLES 15 SP4.\n\n* CMake 3.20.2 or later.\n* A build system like Make or Ninja for which CMake can generate build files.\n* A C99 toolchain (compiler, linker, archiver). We actively test with GCC 5.4, Clang 3.8, Arm Compiler 6, and Visual Studio 2017 Compiler. More recent versions should work. Slightly older versions may work.\n* Python 3.8 or later to generate the test code. Python is also needed to build the development branch (see next section).\n* Perl to run the tests, and to generate some source files in the development branch.\n* Doxygen 1.8.14 or later (if building the documentation; slightly older versions should work).\n\n### Git usage\n\nThe supported branches (see [`BRANCHES.md`](BRANCHES.md)) use [Git submodules](https://git-scm.com/book/en/v2/Git-Tools-Submodules#_cloning_submodules). They contain two submodules: the [framework](https://github.com/Mbed-TLS/mbedtls-framework) submodule and the [tf-psa-crypto](https://github.com/Mbed-TLS/TF-PSA-Crypto) submodule, except for the 3.6 LTS branch, which contains only the framework submodule. Release tags also use Git submodules.\n\nAfter cloning or checking out a branch or tag, run:\n    ```\n    git submodule update --init --recursive\n    ```\n to initialize and update the submodules before building.\n\nHowever, the official source release tarballs (e.g. [mbedtls-4.0.0.tar.bz2](https://github.com/Mbed-TLS/mbedtls/releases/download/mbedtls-4.0.0/mbedtls-4.0.0.tar.bz2)) include the contents of the submodules.\n\n### Generated source files in the development branch\n\nThe source code of Mbed TLS includes some files that are automatically generated by scripts and whose content depends only on the Mbed TLS source, not on the platform or on the library configuration. These files are not included in the development branch of Mbed TLS, but the generated files are included in official releases. This section explains how to generate the missing files in the development branch.\n\nThe following tools are required:\n\n* Perl, for some library source files.\n* Python 3 and some Python packages, for some library source files, sample programs and test data. To install the necessary packages, run:\n    ```\n    python3 -m pip install --user -r scripts/basic.requirements.txt\n    ```\n    Depending on your Python installation, you may need to invoke `python` instead of `python3`. To install the packages system-wide or in a virtual environment, omit the `--user` option.\n* A C compiler for the host platform, for some test data.\n\nThe scripts that generate the configuration-independent files will look for a host C compiler in the following places (in order of preference):\n\n1. The `HOSTCC` environment variable. This can be used if `CC` is pointing to a cross-compiler.\n2. The `CC` environment variable.\n3. An executable called `cc` in the current path.\n\nNote: If you have multiple toolchains installed, it is recommended to set `CC` or `HOSTCC` to the intended host compiler before generating the files.\n\nAny of the following methods are available to generate the configuration-independent files:\n\n* On non-Windows systems, when not cross-compiling, CMake generates the required files automatically.\n* Run `framework/scripts/make_generated_files.py` to generate all the configuration-independent files.\n\n### CMake\n\nIn order to build the libraries using CMake in a separate directory (recommended), just enter at the command line:\n\n    mkdir /path/to/build_dir && cd /path/to/build_dir\n    cmake /path/to/mbedtls_source\n    cmake --build .\n\nIn order to run the tests, enter:\n\n    ctest\n\nThe test suites need Python to be built. If you don't have Python installed, you'll want to disable the test suites with:\n\n    cmake -DENABLE_TESTING=Off /path/to/mbedtls_source\n\nTo configure CMake for building shared libraries, use:\n\n    cmake -DUSE_SHARED_MBEDTLS_LIBRARY=On /path/to/mbedtls_source\n\nThere are many different build types available with CMake. Most of them are available for gcc and clang, though some are compiler-specific:\n\n-   `Release`. This generates the default code without any unnecessary information in the binary files.\n-   `Debug`. This generates debug information and disables optimization of the code.\n-   `Coverage`. This generates code coverage information in addition to debug information.\n-   `ASan`. This instruments the code with AddressSanitizer to check for memory errors. (This includes LeakSanitizer, with recent version of gcc and clang.) (With recent version of clang, this mode also instruments the code with UndefinedSanitizer to check for undefined behaviour.)\n-   `ASanDbg`. Same as ASan but slower, with debug information and better stack traces.\n-   `MemSan`. This instruments the code with MemorySanitizer to check for uninitialised memory reads.\n-   `MemSanDbg`. Same as MemSan but slower, with debug information, better stack traces and origin tracking.\n-   `Check`. This activates the compiler warnings that depend on optimization and treats all warnings as errors.\n-   `TSan`. This instruments the code with ThreadSanitizer to detect data races and other threading-related concurrency issues at runtime.\n-   `TSanDbg`. Same as TSan but slower, with debug information, better stack traces and origin tracking.\n\nSwitching build types in CMake is simple. For debug mode, enter at the command line:\n\n    cmake -D CMAKE_BUILD_TYPE=Debug /path/to/mbedtls_source\n\nTo list other available CMake options, use:\n\n    cmake -LH\n\nNote that, with CMake, you can't adjust the compiler or its flags after the\ninitial invocation of cmake. This means that `CC=your_cc make` and `make\nCC=your_cc` will *not* work (similarly with `CFLAGS` and other variables).\nThese variables need to be adjusted when invoking cmake for the first time,\nfor example:\n\n    CC=your_cc cmake /path/to/mbedtls_source\n\nIf you already invoked cmake and want to change those settings, you need to\ninvoke the configuration phase of CMake again with the new settings.\n\nNote that it is possible to build in-place; this will however overwrite the\nlegacy Makefiles still used for testing purposes (see\n`scripts/tmp_ignore_makefiles.sh` if you want to prevent `git status` from\nshowing them as modified). In order to do so, from the Mbed TLS source\ndirectory, use:\n\n    cmake .\n    cmake --build .\n\nIf you want to change `CC` or `CFLAGS` afterwards, you will need to remove the\nCMake cache. This can be done with the following command using GNU find:\n\n    find . -iname '*cmake*' -not -name CMakeLists.txt -exec rm -rf {} +\n\nYou can now make the desired change:\n\n    CC=your_cc cmake .\n    cmake --build .\n\nRegarding variables, also note that if you set CFLAGS when invoking cmake,\nyour value of CFLAGS doesn't override the content provided by CMake (depending\non the build mode as seen above), it's merely prepended to it.\n\n#### Consuming Mbed TLS\n\nMbed TLS provides a CMake package configuration file for consumption as a\ndependency in other CMake projects. You can load its CMake targets with:\n\n    find_package(MbedTLS REQUIRED)\n\nYou can help CMake find the package:\n\n- By setting the variable `MbedTLS_DIR` to `${YOUR_MBEDTLS_BUILD_DIR}/cmake`,\n  as shown in `programs/test/cmake_package/CMakeLists.txt`, or\n- By adding the Mbed TLS installation prefix to `CMAKE_PREFIX_PATH`,\n  as shown in `programs/test/cmake_package_install/CMakeLists.txt`.\n\nAfter a successful `find_package(MbedTLS)`, the following imported targets are available:\n\n- `MbedTLS::tfpsacrypto`, the crypto library\n- `MbedTLS::mbedtls`, the TLS library\n- `MbedTLS::mbedx509`, the X.509 library\n\nYou can then use these directly through `target_link_libraries()`:\n\n    add_executable(xyz)\n\n    target_link_libraries(xyz\n        PUBLIC MbedTLS::mbedtls\n               MbedTLS::tfpsacrypto\n               MbedTLS::mbedx509)\n\nThis will link the Mbed TLS libraries to your library or application, and add\nits include directories to your target (transitively, in the case of `PUBLIC` or\n`INTERFACE` link libraries).\n\n#### Mbed TLS as a subproject\n\nMbed TLS supports being built as a CMake subproject. One can\nuse `add_subdirectory()` from a parent CMake project to include Mbed TLS as a\nsubproject.\n\nExample programs\n----------------\n\nWe've included example programs for a lot of different features and uses in [`programs/`](programs/README.md).\nPlease note that the goal of these sample programs is to demonstrate specific features of the library, and the code may need to be adapted to build a real-world application.\n\nTests\n-----\n\nMbed TLS includes an elaborate test suite in `tests/` that initially requires Python to generate the tests files (e.g. `test_suite_ssl.c`). These files are generated from a `function file` (e.g. `suites/test_suite_ssl.function`) and a `data file` (e.g. `suites/test_suite_ssl.data`). The `function file` contains the test functions. The `data file` contains the test cases, specified as parameters that will be passed to the test function.\n\nFor machines with a Unix shell and OpenSSL (and optionally GnuTLS) installed, additional test scripts are available:\n\n-   `tests/ssl-opt.sh` runs integration tests for various TLS options (renegotiation, resumption, etc.) and tests interoperability of these options with other implementations.\n-   `tests/compat.sh` tests interoperability of every ciphersuite with other implementations.\n-   `tests/scripts/depends.py` tests builds in configurations with a single curve, key exchange, hash, cipher, or pkalg on.\n-   `tests/scripts/all.sh` runs a combination of the above tests, plus some more, with various build options (such as ASan, full `mbedtls_config.h`, etc).\n\nInstead of manually installing the required versions of all tools required for testing, it is possible to use the Docker images from our CI systems, as explained in [our testing infrastructure repository](https://github.com/Mbed-TLS/mbedtls-test/blob/main/README.md#quick-start).\n\nPorting Mbed TLS\n----------------\n\nMbed TLS can be ported to many different architectures, OS's and platforms. Before starting a port, you may find the following Knowledge Base articles useful:\n\n-   [Porting Mbed TLS to a new environment or OS](https://mbed-tls.readthedocs.io/en/latest/kb/how-to/how-do-i-port-mbed-tls-to-a-new-environment-OS/)\n-   [What external dependencies does Mbed TLS rely on?](https://mbed-tls.readthedocs.io/en/latest/kb/development/what-external-dependencies-does-mbedtls-rely-on/)\n-   [How do I configure Mbed TLS](https://mbed-tls.readthedocs.io/en/latest/kb/compiling-and-building/how-do-i-configure-mbedtls/)\n\nMbed TLS is mostly written in portable C99; however, it has a few platform requirements that go beyond the standard, but are met by most modern architectures:\n\n- Bytes must be 8 bits.\n- All-bits-zero must be a valid representation of a null pointer.\n- Signed integers must be represented using two's complement.\n- `int` and `size_t` must be at least 32 bits wide.\n- The types `uint8_t`, `uint16_t`, `uint32_t` and their signed equivalents must be available.\n- Mixed-endian platforms are not supported.\n- SIZE_MAX must be at least as big as INT_MAX and UINT_MAX.\n\nLicense\n-------\n\nUnless specifically indicated otherwise in a file, Mbed TLS files are provided under a dual [Apache-2.0](https://spdx.org/licenses/Apache-2.0.html) OR [GPL-2.0-or-later](https://spdx.org/licenses/GPL-2.0-or-later.html) license. See the [LICENSE](LICENSE) file for the full text of these licenses, and [the 'License and Copyright' section in the contributing guidelines](CONTRIBUTING.md#License-and-Copyright) for more information.\n\nContributing\n------------\n\nWe gratefully accept bug reports and contributions from the community. Please see the [contributing guidelines](CONTRIBUTING.md) for details on how to do this.\n\nContact\n-------\n\n* To report a security vulnerability in Mbed TLS, please email <mbed-tls-security@lists.trustedfirmware.org>. For more information, see [`SECURITY.md`](SECURITY.md).\n* To report a bug or request a feature in Mbed TLS, please [file an issue on GitHub](https://github.com/Mbed-TLS/mbedtls/issues/new/choose).\n* Please see [`SUPPORT.md`](SUPPORT.md) for other channels for discussion and support about Mbed TLS.\n",
      "stars_today": 2
    },
    {
      "id": 26537135,
      "name": "u-boot",
      "full_name": "u-boot/u-boot",
      "description": "\"Das U-Boot\" Source Tree",
      "html_url": "https://github.com/u-boot/u-boot",
      "stars": 4902,
      "forks": 4263,
      "language": "C",
      "topics": [],
      "created_at": "2014-11-12T13:29:02Z",
      "updated_at": "2026-01-27T21:38:03Z",
      "pushed_at": "2026-01-27T15:31:22Z",
      "open_issues": 233,
      "owner": {
        "login": "u-boot",
        "avatar_url": "https://avatars.githubusercontent.com/u/9681997?v=4"
      },
      "readme": " # SPDX-License-Identifier: GPL-2.0+\n#\n# (C) Copyright 2000 - 2013\n# Wolfgang Denk, DENX Software Engineering, wd@denx.de.\n\nSummary:\n========\n\nThis directory contains the source code for U-Boot, a boot loader for\nEmbedded boards based on PowerPC, ARM, MIPS and several other\nprocessors, which can be installed in a boot ROM and used to\ninitialize and test the hardware or to download and run application\ncode.\n\nThe development of U-Boot is closely related to Linux: some parts of\nthe source code originate in the Linux source tree, we have some\nheader files in common, and special provision has been made to\nsupport booting of Linux images.\n\nSome attention has been paid to make this software easily\nconfigurable and extendable. For instance, all monitor commands are\nimplemented with the same call interface, so that it's very easy to\nadd new commands. Also, instead of permanently adding rarely used\ncode (for instance hardware test utilities) to the monitor, you can\nload and run it dynamically.\n\n\nStatus:\n=======\n\nIn general, all boards for which a default configuration file exists in the\nconfigs/ directory have been tested to some extent and can be considered\n\"working\". In fact, many of them are used in production systems.\n\nIn case of problems you can use\n\n     scripts/get_maintainer.pl <path>\n\nto identify the people or companies responsible for various boards and\nsubsystems. Or have a look at the git log.\n\n\nWhere to get help:\n==================\n\nIn case you have questions about, problems with or contributions for\nU-Boot, you should send a message to the U-Boot mailing list at\n<u-boot@lists.denx.de>. There is also an archive of previous traffic\non the mailing list - please search the archive before asking FAQ's.\nPlease see https://lists.denx.de/pipermail/u-boot and\nhttps://marc.info/?l=u-boot\n\nWhere to get source code:\n=========================\n\nThe U-Boot source code is maintained in the Git repository at\nhttps://source.denx.de/u-boot/u-boot.git ; you can browse it online at\nhttps://source.denx.de/u-boot/u-boot\n\nThe \"Tags\" links on this page allow you to download tarballs of\nany version you might be interested in. Official releases are also\navailable from the DENX file server through HTTPS or FTP.\nhttps://ftp.denx.de/pub/u-boot/\nftp://ftp.denx.de/pub/u-boot/\n\n\nWhere we come from:\n===================\n\n- start from 8xxrom sources\n- create PPCBoot project (https://sourceforge.net/projects/ppcboot)\n- clean up code\n- make it easier to add custom boards\n- make it possible to add other [PowerPC] CPUs\n- extend functions, especially:\n  * Provide extended interface to Linux boot loader\n  * S-Record download\n  * network boot\n  * ATA disk / SCSI ... boot\n- create ARMBoot project (https://sourceforge.net/projects/armboot)\n- add other CPU families (starting with ARM)\n- create U-Boot project (https://sourceforge.net/projects/u-boot)\n- current project page: see https://www.denx.de/wiki/U-Boot\n\n\nNames and Spelling:\n===================\n\nThe \"official\" name of this project is \"Das U-Boot\". The spelling\n\"U-Boot\" shall be used in all written text (documentation, comments\nin source files etc.). Example:\n\n\tThis is the README file for the U-Boot project.\n\nFile names etc. shall be based on the string \"u-boot\". Examples:\n\n\tinclude/asm-ppc/u-boot.h\n\n\t#include <asm/u-boot.h>\n\nVariable names, preprocessor constants etc. shall be either based on\nthe string \"u_boot\" or on \"U_BOOT\". Example:\n\n\tU_BOOT_VERSION\t\tu_boot_logo\n\tIH_OS_U_BOOT\t\tu_boot_hush_start\n\n\nSoftware Configuration:\n=======================\n\nSelection of Processor Architecture and Board Type:\n---------------------------------------------------\n\nFor all supported boards there are ready-to-use default\nconfigurations available; just type \"make <board_name>_defconfig\".\n\nExample: For a TQM823L module type:\n\n\tcd u-boot\n\tmake TQM823L_defconfig\n\nNote: If you're looking for the default configuration file for a board\nyou're sure used to be there but is now missing, check the file\ndoc/README.scrapyard for a list of no longer supported boards.\n\nSandbox Environment:\n--------------------\n\nU-Boot can be built natively to run on a Linux host using the 'sandbox'\nboard. This allows feature development which is not board- or architecture-\nspecific to be undertaken on a native platform. The sandbox is also used to\nrun some of U-Boot's tests.\n\nSee doc/arch/sandbox/sandbox.rst for more details.\n\nThe following options need to be configured:\n\n- CPU Type:\tDefine exactly one, e.g. CONFIG_MPC85XX.\n\n- Board Type:\tDefine exactly one, e.g. CONFIG_MPC8540ADS.\n\n- 85xx CPU Options:\n\t\tCONFIG_SYS_PPC64\n\n\t\tSpecifies that the core is a 64-bit PowerPC implementation (implements\n\t\tthe \"64\" category of the Power ISA). This is necessary for ePAPR\n\t\tcompliance, among other possible reasons.\n\n\t\tCONFIG_SYS_FSL_ERRATUM_A004510\n\n\t\tEnables a workaround for erratum A004510.  If set,\n\t\tthen CONFIG_SYS_FSL_ERRATUM_A004510_SVR_REV and\n\t\tCFG_SYS_FSL_CORENET_SNOOPVEC_COREONLY must be set.\n\n\t\tCONFIG_SYS_FSL_ERRATUM_A004510_SVR_REV\n\t\tCONFIG_SYS_FSL_ERRATUM_A004510_SVR_REV2 (optional)\n\n\t\tDefines one or two SoC revisions (low 8 bits of SVR)\n\t\tfor which the A004510 workaround should be applied.\n\n\t\tThe rest of SVR is either not relevant to the decision\n\t\tof whether the erratum is present (e.g. p2040 versus\n\t\tp2041) or is implied by the build target, which controls\n\t\twhether CONFIG_SYS_FSL_ERRATUM_A004510 is set.\n\n\t\tSee Freescale App Note 4493 for more information about\n\t\tthis erratum.\n\n\t\tCFG_SYS_FSL_CORENET_SNOOPVEC_COREONLY\n\n\t\tThis is the value to write into CCSR offset 0x18600\n\t\taccording to the A004510 workaround.\n\n\t\tCONFIG_SYS_FSL_SINGLE_SOURCE_CLK\n\t\tSingle Source Clock is clocking mode present in some of FSL SoC's.\n\t\tIn this mode, a single differential clock is used to supply\n\t\tclocks to the sysclock, ddrclock and usbclock.\n\n- Generic CPU options:\n\n\t\tCONFIG_SYS_FSL_DDR\n\t\tFreescale DDR driver in use. This type of DDR controller is\n\t\tfound in mpc83xx, mpc85xx as well as some ARM core SoCs.\n\n\t\tCFG_SYS_FSL_DDR_ADDR\n\t\tFreescale DDR memory-mapped register base.\n\n\t\tCONFIG_SYS_FSL_IFC_CLK_DIV\n\t\tDefines divider of platform clock(clock input to IFC controller).\n\n\t\tCONFIG_SYS_FSL_LBC_CLK_DIV\n\t\tDefines divider of platform clock(clock input to eLBC controller).\n\n\t\tCFG_SYS_FSL_DDR_SDRAM_BASE_PHY\n\t\tPhysical address from the view of DDR controllers. It is the\n\t\tsame as CFG_SYS_DDR_SDRAM_BASE for  all Power SoCs. But\n\t\tit could be different for ARM SoCs.\n\n- ARM options:\n\t\tCFG_SYS_EXCEPTION_VECTORS_HIGH\n\n\t\tSelect high exception vectors of the ARM core, e.g., do not\n\t\tclear the V bit of the c1 register of CP15.\n\n\t\tCOUNTER_FREQUENCY\n\t\tGeneric timer clock source frequency.\n\n\t\tCOUNTER_FREQUENCY_REAL\n\t\tGeneric timer clock source frequency if the real clock is\n\t\tdifferent from COUNTER_FREQUENCY, and can only be determined\n\t\tat run time.\n\n- Linux Kernel Interface:\n\t\tCONFIG_OF_LIBFDT\n\n\t\tNew kernel versions are expecting firmware settings to be\n\t\tpassed using flattened device trees (based on open firmware\n\t\tconcepts).\n\n\t\tCONFIG_OF_LIBFDT\n\t\t * New libfdt-based support\n\t\t * Adds the \"fdt\" command\n\t\t * The bootm command automatically updates the fdt\n\n\t\tOF_TBCLK - The timebase frequency.\n\n\t\tboards with QUICC Engines require OF_QE to set UCC MAC\n\t\taddresses\n\n\t\tCONFIG_OF_IDE_FIXUP\n\n\t\tU-Boot can detect if an IDE device is present or not.\n\t\tIf not, and this new config option is activated, U-Boot\n\t\tremoves the ATA node from the DTS before booting Linux,\n\t\tso the Linux IDE driver does not probe the device and\n\t\tcrash. This is needed for buggy hardware (uc101) where\n\t\tno pull down resistor is connected to the signal IDE5V_DD7.\n\n- vxWorks boot parameters:\n\n\t\tbootvx constructs a valid bootline using the following\n\t\tenvironments variables: bootdev, bootfile, ipaddr, netmask,\n\t\tserverip, gatewayip, hostname, othbootargs.\n\t\tIt loads the vxWorks image pointed bootfile.\n\n\t\tNote: If a \"bootargs\" environment is defined, it will override\n\t\tthe defaults discussed just above.\n\n- Cache Configuration for ARM:\n\t\tCFG_SYS_PL310_BASE - Physical base address of PL310\n\t\t\t\t\tcontroller register space\n\n- Serial Ports:\n\t\tCFG_PL011_CLOCK\n\n\t\tIf you have Amba PrimeCell PL011 UARTs, set this variable to\n\t\tthe clock speed of the UARTs.\n\n\t\tCFG_PL01x_PORTS\n\n\t\tIf you have Amba PrimeCell PL010 or PL011 UARTs on your board,\n\t\tdefine this to a list of base addresses for each (supported)\n\t\tport. See e.g. include/configs/versatile.h\n\n\t\tCONFIG_SERIAL_HW_FLOW_CONTROL\n\n\t\tDefine this variable to enable hw flow control in serial driver.\n\t\tCurrent user of this option is drivers/serial/nsl16550.c driver\n\n- Removal of commands\n\t\tIf no commands are needed to boot, you can disable\n\t\tCONFIG_CMDLINE to remove them. In this case, the command line\n\t\twill not be available, and when U-Boot wants to execute the\n\t\tboot command (on start-up) it will call board_run_command()\n\t\tinstead. This can reduce image size significantly for very\n\t\tsimple boot procedures.\n\n- Regular expression support:\n\t\tCONFIG_REGEX\n\t\tIf this variable is defined, U-Boot is linked against\n\t\tthe SLRE (Super Light Regular Expression) library,\n\t\twhich adds regex support to some commands, as for\n\t\texample \"env grep\" and \"setexpr\".\n\n- Watchdog:\n\t\tCFG_SYS_WATCHDOG_FREQ\n\t\tSome platforms automatically call WATCHDOG_RESET()\n\t\tfrom the timer interrupt handler every\n\t\tCFG_SYS_WATCHDOG_FREQ interrupts. If not set by the\n\t\tboard configuration file, a default of CONFIG_SYS_HZ/2\n\t\t(i.e. 500) is used. Setting CFG_SYS_WATCHDOG_FREQ\n\t\tto 0 disables calling WATCHDOG_RESET() from the timer\n\t\tinterrupt.\n\n- GPIO Support:\n\t\tThe CFG_SYS_I2C_PCA953X_WIDTH option specifies a list of\n\t\tchip-ngpio pairs that tell the PCA953X driver the number of\n\t\tpins supported by a particular chip.\n\n\t\tNote that if the GPIO device uses I2C, then the I2C interface\n\t\tmust also be configured. See I2C Support, below.\n\n- Timestamp Support:\n\n\t\tWhen CONFIG_TIMESTAMP is selected, the timestamp\n\t\t(date and time) of an image is printed by image\n\t\tcommands like bootm or iminfo. This option is\n\t\tautomatically enabled when you select CONFIG_CMD_DATE .\n\n- Partition Labels (disklabels) Supported:\n\t\tZero or more of the following:\n\t\tCONFIG_MAC_PARTITION   Apple's MacOS partition table.\n\t\tCONFIG_ISO_PARTITION   ISO partition table, used on CDROM etc.\n\t\tCONFIG_EFI_PARTITION   GPT partition table, common when EFI is the\n\t\t\t\t       bootloader.  Note 2TB partition limit; see\n\t\t\t\t       disk/part_efi.c\n\t\tCONFIG_SCSI) you must configure support for at\n\t\tleast one non-MTD partition type as well.\n\n- NETWORK Support (PCI):\n\t\tCONFIG_E1000_SPI\n\t\tUtility code for direct access to the SPI bus on Intel 8257x.\n\t\tThis does not do anything useful unless you set at least one\n\t\tof CONFIG_CMD_E1000 or CONFIG_E1000_SPI_GENERIC.\n\n\t\tCONFIG_NATSEMI\n\t\tSupport for National dp83815 chips.\n\n\t\tCONFIG_NS8382X\n\t\tSupport for National dp8382[01] gigabit chips.\n\n- NETWORK Support (other):\n\t\tCONFIG_CALXEDA_XGMAC\n\t\tSupport for the Calxeda XGMAC device\n\n\t\tCONFIG_LAN91C96\n\t\tSupport for SMSC's LAN91C96 chips.\n\n\t\t\tCONFIG_LAN91C96_USE_32_BIT\n\t\t\tDefine this to enable 32 bit addressing\n\n\t\t\tCFG_SYS_DAVINCI_EMAC_PHY_COUNT\n\t\t\tDefine this if you have more then 3 PHYs.\n\n\t\tCONFIG_FTGMAC100\n\t\tSupport for Faraday's FTGMAC100 Gigabit SoC Ethernet\n\n\t\t\tCONFIG_FTGMAC100_EGIGA\n\t\t\tDefine this to use GE link update with gigabit PHY.\n\t\t\tDefine this if FTGMAC100 is connected to gigabit PHY.\n\t\t\tIf your system has 10/100 PHY only, it might not occur\n\t\t\twrong behavior. Because PHY usually return timeout or\n\t\t\tuseless data when polling gigabit status and gigabit\n\t\t\tcontrol registers. This behavior won't affect the\n\t\t\tcorrectnessof 10/100 link speed update.\n\n\t\tCONFIG_SH_ETHER\n\t\tSupport for Renesas on-chip Ethernet controller\n\n- TPM Support:\n\t\tCONFIG_TPM\n\t\tSupport TPM devices.\n\n\t\tCONFIG_TPM_TIS_INFINEON\n\t\tSupport for Infineon i2c bus TPM devices. Only one device\n\t\tper system is supported at this time.\n\n\t\t\tCONFIG_TPM_TIS_I2C_BURST_LIMITATION\n\t\t\tDefine the burst count bytes upper limit\n\n\t\tCONFIG_TPM_ATMEL_TWI\n\t\tSupport for Atmel TWI TPM device. Requires I2C support.\n\n\t\tCONFIG_TPM_TIS_LPC\n\t\tSupport for generic parallel port TPM devices. Only one device\n\t\tper system is supported at this time.\n\n\t\tCONFIG_TPM\n\t\tDefine this to enable the TPM support library which provides\n\t\tfunctional interfaces to some TPM commands.\n\t\tRequires support for a TPM device.\n\n\t\tCONFIG_TPM_AUTH_SESSIONS\n\t\tDefine this to enable authorized functions in the TPM library.\n\t\tRequires CONFIG_TPM and CONFIG_SHA1.\n\n- USB Support:\n\t\tAt the moment only the UHCI host controller is\n\t\tsupported (PIP405, MIP405); define\n\t\tCONFIG_USB_UHCI to enable it.\n\t\tdefine CONFIG_USB_KEYBOARD to enable the USB Keyboard\n\t\tand define CONFIG_USB_STORAGE to enable the USB\n\t\tstorage devices.\n\t\tNote:\n\t\tSupported are USB Keyboards and USB Floppy drives\n\t\t(TEAC FD-05PUB).\n\n\t\tCONFIG_USB_DWC2_REG_ADDR the physical CPU address of the DWC2\n\t\tHW module registers.\n\n- USB Device:\n\t\tDefine the below if you wish to use the USB console.\n\t\tOnce firmware is rebuilt from a serial console issue the\n\t\tcommand \"setenv stdin usbtty; setenv stdout usbtty\" and\n\t\tattach your USB cable. The Unix command \"dmesg\" should print\n\t\tit has found a new device. The environment variable usbtty\n\t\tcan be set to gserial or cdc_acm to enable your device to\n\t\tappear to a USB host as a Linux gserial device or a\n\t\tCommon Device Class Abstract Control Model serial device.\n\t\tIf you select usbtty = gserial you should be able to enumerate\n\t\ta Linux host by\n\t\t# modprobe usbserial vendor=0xVendorID product=0xProductID\n\t\telse if using cdc_acm, simply setting the environment\n\t\tvariable usbtty to be cdc_acm should suffice. The following\n\t\tmight be defined in YourBoardName.h\n\n\t\tIf you have a USB-IF assigned VendorID then you may wish to\n\t\tdefine your own vendor specific values either in BoardName.h\n\t\tor directly in usbd_vendor_info.h. If you don't define\n\t\tCONFIG_USBD_MANUFACTURER, CONFIG_USBD_PRODUCT_NAME,\n\t\tCONFIG_USBD_VENDORID and CONFIG_USBD_PRODUCTID, then U-Boot\n\t\tshould pretend to be a Linux device to it's target host.\n\n\t\t\tCONFIG_USBD_MANUFACTURER\n\t\t\tDefine this string as the name of your company for\n\t\t\t- CONFIG_USBD_MANUFACTURER \"my company\"\n\n\t\t\tCONFIG_USBD_PRODUCT_NAME\n\t\t\tDefine this string as the name of your product\n\t\t\t- CONFIG_USBD_PRODUCT_NAME \"acme usb device\"\n\n\t\t\tCONFIG_USBD_VENDORID\n\t\t\tDefine this as your assigned Vendor ID from the USB\n\t\t\tImplementors Forum. This *must* be a genuine Vendor ID\n\t\t\tto avoid polluting the USB namespace.\n\t\t\t- CONFIG_USBD_VENDORID 0xFFFF\n\n\t\t\tCONFIG_USBD_PRODUCTID\n\t\t\tDefine this as the unique Product ID\n\t\t\tfor your device\n\t\t\t- CONFIG_USBD_PRODUCTID 0xFFFF\n\n- MMC Support:\n\t\tCONFIG_SH_MMCIF\n\t\tSupport for Renesas on-chip MMCIF controller\n\n\t\t\tCONFIG_SH_MMCIF_ADDR\n\t\t\tDefine the base address of MMCIF registers\n\n\t\t\tCONFIG_SH_MMCIF_CLK\n\t\t\tDefine the clock frequency for MMCIF\n\n- USB Device Firmware Update (DFU) class support:\n\t\tCONFIG_DFU_OVER_USB\n\t\tThis enables the USB portion of the DFU USB class\n\n\t\tCONFIG_DFU_NAND\n\t\tThis enables support for exposing NAND devices via DFU.\n\n\t\tCONFIG_DFU_RAM\n\t\tThis enables support for exposing RAM via DFU.\n\t\tNote: DFU spec refer to non-volatile memory usage, but\n\t\tallow usages beyond the scope of spec - here RAM usage,\n\t\tone that would help mostly the developer.\n\n\t\tCONFIG_SYS_DFU_DATA_BUF_SIZE\n\t\tDfu transfer uses a buffer before writing data to the\n\t\traw storage device. Make the size (in bytes) of this buffer\n\t\tconfigurable. The size of this buffer is also configurable\n\t\tthrough the \"dfu_bufsiz\" environment variable.\n\n\t\tCONFIG_SYS_DFU_MAX_FILE_SIZE\n\t\tWhen updating files rather than the raw storage device,\n\t\twe use a static buffer to copy the file into and then write\n\t\tthe buffer once we've been given the whole file.  Define\n\t\tthis to the maximum filesize (in bytes) for the buffer.\n\t\tDefault is 4 MiB if undefined.\n\n\t\tDFU_DEFAULT_POLL_TIMEOUT\n\t\tPoll timeout [ms], is the timeout a device can send to the\n\t\thost. The host must wait for this timeout before sending\n\t\ta subsequent DFU_GET_STATUS request to the device.\n\n\t\tDFU_MANIFEST_POLL_TIMEOUT\n\t\tPoll timeout [ms], which the device sends to the host when\n\t\tentering dfuMANIFEST state. Host waits this timeout, before\n\t\tsending again an USB request to the device.\n\n- Keyboard Support:\n\t\tSee Kconfig help for available keyboard drivers.\n\n- MII/PHY support:\n\t\tCONFIG_PHY_CLOCK_FREQ (ppc4xx)\n\n\t\tThe clock frequency of the MII bus\n\n\t\tCONFIG_PHY_CMD_DELAY (ppc4xx)\n\n\t\tSome PHY like Intel LXT971A need extra delay after\n\t\tcommand issued before MII status register can be read\n\n- BOOTP Recovery Mode:\n\t\tCONFIG_BOOTP_RANDOM_DELAY\n\n\t\tIf you have many targets in a network that try to\n\t\tboot using BOOTP, you may want to avoid that all\n\t\tsystems send out BOOTP requests at precisely the same\n\t\tmoment (which would happen for instance at recovery\n\t\tfrom a power failure, when all systems will try to\n\t\tboot, thus flooding the BOOTP server. Defining\n\t\tCONFIG_BOOTP_RANDOM_DELAY causes a random delay to be\n\t\tinserted before sending out BOOTP requests. The\n\t\tfollowing delays are inserted then:\n\n\t\t1st BOOTP request:\tdelay 0 ... 1 sec\n\t\t2nd BOOTP request:\tdelay 0 ... 2 sec\n\t\t3rd BOOTP request:\tdelay 0 ... 4 sec\n\t\t4th and following\n\t\tBOOTP requests:\t\tdelay 0 ... 8 sec\n\n\t\tCFG_BOOTP_ID_CACHE_SIZE\n\n\t\tBOOTP packets are uniquely identified using a 32-bit ID. The\n\t\tserver will copy the ID from client requests to responses and\n\t\tU-Boot will use this to determine if it is the destination of\n\t\tan incoming response. Some servers will check that addresses\n\t\taren't in use before handing them out (usually using an ARP\n\t\tping) and therefore take up to a few hundred milliseconds to\n\t\trespond. Network congestion may also influence the time it\n\t\ttakes for a response to make it back to the client. If that\n\t\ttime is too long, U-Boot will retransmit requests. In order\n\t\tto allow earlier responses to still be accepted after these\n\t\tretransmissions, U-Boot's BOOTP client keeps a small cache of\n\t\tIDs. The CFG_BOOTP_ID_CACHE_SIZE controls the size of this\n\t\tcache. The default is to keep IDs for up to four outstanding\n\t\trequests. Increasing this will allow U-Boot to accept offers\n\t\tfrom a BOOTP client in networks with unusually high latency.\n\n- DHCP Advanced Options:\n\n - Link-local IP address negotiation:\n\t\tNegotiate with other link-local clients on the local network\n\t\tfor an address that doesn't require explicit configuration.\n\t\tThis is especially useful if a DHCP server cannot be guaranteed\n\t\tto exist in all environments that the device must operate.\n\n\t\tSee doc/README.link-local for more information.\n\n - MAC address from environment variables\n\n\t\tFDT_SEQ_MACADDR_FROM_ENV\n\n\t\tFix-up device tree with MAC addresses fetched sequentially from\n\t\tenvironment variables. This config work on assumption that\n\t\tnon-usable ethernet node of device-tree are either not present\n\t\tor their status has been marked as \"disabled\".\n\n - CDP Options:\n\t\tCONFIG_CDP_DEVICE_ID\n\n\t\tThe device id used in CDP trigger frames.\n\n\t\tCONFIG_CDP_DEVICE_ID_PREFIX\n\n\t\tA two character string which is prefixed to the MAC address\n\t\tof the device.\n\n\t\tCONFIG_CDP_PORT_ID\n\n\t\tA printf format string which contains the ascii name of\n\t\tthe port. Normally is set to \"eth%d\" which sets\n\t\teth0 for the first Ethernet, eth1 for the second etc.\n\n\t\tCONFIG_CDP_CAPABILITIES\n\n\t\tA 32bit integer which indicates the device capabilities;\n\t\t0x00000010 for a normal host which does not forwards.\n\n\t\tCONFIG_CDP_VERSION\n\n\t\tAn ascii string containing the version of the software.\n\n\t\tCONFIG_CDP_PLATFORM\n\n\t\tAn ascii string containing the name of the platform.\n\n\t\tCONFIG_CDP_TRIGGER\n\n\t\tA 32bit integer sent on the trigger.\n\n\t\tCONFIG_CDP_POWER_CONSUMPTION\n\n\t\tA 16bit integer containing the power consumption of the\n\t\tdevice in .1 of milliwatts.\n\n\t\tCONFIG_CDP_APPLIANCE_VLAN_TYPE\n\n\t\tA byte containing the id of the VLAN.\n\n- Status LED:\tCONFIG_LED_STATUS\n\n\t\tSeveral configurations allow to display the current\n\t\tstatus using a LED. For instance, the LED will blink\n\t\tfast while running U-Boot code, stop blinking as\n\t\tsoon as a reply to a BOOTP request was received, and\n\t\tstart blinking slow once the Linux kernel is running\n\t\t(supported by a status LED driver in the Linux\n\t\tkernel). Defining CONFIG_LED_STATUS enables this\n\t\tfeature in U-Boot.\n\n\t\tAdditional options:\n\n\t\tCONFIG_LED_STATUS_GPIO\n\t\tThe status LED can be connected to a GPIO pin.\n\t\tIn such cases, the gpio_led driver can be used as a\n\t\tstatus LED backend implementation. Define CONFIG_LED_STATUS_GPIO\n\t\tto include the gpio_led driver in the U-Boot binary.\n\n\t\tCFG_GPIO_LED_INVERTED_TABLE\n\t\tSome GPIO connected LEDs may have inverted polarity in which\n\t\tcase the GPIO high value corresponds to LED off state and\n\t\tGPIO low value corresponds to LED on state.\n\t\tIn such cases CFG_GPIO_LED_INVERTED_TABLE may be defined\n\t\twith a list of GPIO LEDs that have inverted polarity.\n\n- I2C Support:\n\t\tCFG_SYS_NUM_I2C_BUSES\n\t\tHold the number of i2c buses you want to use.\n\n\t\tCFG_SYS_I2C_BUSES\n\t\thold a list of buses you want to use\n\n\t\t CFG_SYS_I2C_BUSES\t{{0, {I2C_NULL_HOP}}, \\\n\t\t\t\t\t{0, {{I2C_MUX_PCA9547, 0x70, 1}}}, \\\n\t\t\t\t\t{0, {{I2C_MUX_PCA9547, 0x70, 2}}}, \\\n\t\t\t\t\t{0, {{I2C_MUX_PCA9547, 0x70, 3}}}, \\\n\t\t\t\t\t{0, {{I2C_MUX_PCA9547, 0x70, 4}}}, \\\n\t\t\t\t\t{0, {{I2C_MUX_PCA9547, 0x70, 5}}}, \\\n\t\t\t\t\t{1, {I2C_NULL_HOP}}, \\\n\t\t\t\t\t{1, {{I2C_MUX_PCA9544, 0x72, 1}}}, \\\n\t\t\t\t\t{1, {{I2C_MUX_PCA9544, 0x72, 2}}}, \\\n\t\t\t\t\t}\n\n\t\twhich defines\n\t\t\tbus 0 on adapter 0 without a mux\n\t\t\tbus 1 on adapter 0 with a PCA9547 on address 0x70 port 1\n\t\t\tbus 2 on adapter 0 with a PCA9547 on address 0x70 port 2\n\t\t\tbus 3 on adapter 0 with a PCA9547 on address 0x70 port 3\n\t\t\tbus 4 on adapter 0 with a PCA9547 on address 0x70 port 4\n\t\t\tbus 5 on adapter 0 with a PCA9547 on address 0x70 port 5\n\t\t\tbus 6 on adapter 1 without a mux\n\t\t\tbus 7 on adapter 1 with a PCA9544 on address 0x72 port 1\n\t\t\tbus 8 on adapter 1 with a PCA9544 on address 0x72 port 2\n\n\t\tIf you do not have i2c muxes on your board, omit this define.\n\n- Legacy I2C Support:\n\t\tIf you use the software i2c interface (CONFIG_SYS_I2C_SOFT)\n\t\tthen the following macros need to be defined (examples are\n\t\tfrom include/configs/lwmon.h):\n\n\t\tI2C_INIT\n\n\t\t(Optional). Any commands necessary to enable the I2C\n\t\tcontroller or configure ports.\n\n\t\teg: #define I2C_INIT (immr->im_cpm.cp_pbdir |=\tPB_SCL)\n\n\t\tI2C_ACTIVE\n\n\t\tThe code necessary to make the I2C data line active\n\t\t(driven).  If the data line is open collector, this\n\t\tdefine can be null.\n\n\t\teg: #define I2C_ACTIVE (immr->im_cpm.cp_pbdir |=  PB_SDA)\n\n\t\tI2C_TRISTATE\n\n\t\tThe code necessary to make the I2C data line tri-stated\n\t\t(inactive).  If the data line is open collector, this\n\t\tdefine can be null.\n\n\t\teg: #define I2C_TRISTATE (immr->im_cpm.cp_pbdir &= ~PB_SDA)\n\n\t\tI2C_READ\n\n\t\tCode that returns true if the I2C data line is high,\n\t\tfalse if it is low.\n\n\t\teg: #define I2C_READ ((immr->im_cpm.cp_pbdat & PB_SDA) != 0)\n\n\t\tI2C_SDA(bit)\n\n\t\tIf <bit> is true, sets the I2C data line high. If it\n\t\tis false, it clears it (low).\n\n\t\teg: #define I2C_SDA(bit) \\\n\t\t\tif(bit) immr->im_cpm.cp_pbdat |=  PB_SDA; \\\n\t\t\telse\timmr->im_cpm.cp_pbdat &= ~PB_SDA\n\n\t\tI2C_SCL(bit)\n\n\t\tIf <bit> is true, sets the I2C clock line high. If it\n\t\tis false, it clears it (low).\n\n\t\teg: #define I2C_SCL(bit) \\\n\t\t\tif(bit) immr->im_cpm.cp_pbdat |=  PB_SCL; \\\n\t\t\telse\timmr->im_cpm.cp_pbdat &= ~PB_SCL\n\n\t\tI2C_DELAY\n\n\t\tThis delay is invoked four times per clock cycle so this\n\t\tcontrols the rate of data transfer.  The data rate thus\n\t\tis 1 / (I2C_DELAY * 4). Often defined to be something\n\t\tlike:\n\n\t\t#define I2C_DELAY  udelay(2)\n\n\t\tCONFIG_SOFT_I2C_GPIO_SCL / CONFIG_SOFT_I2C_GPIO_SDA\n\n\t\tIf your arch supports the generic GPIO framework (asm/gpio.h),\n\t\tthen you may alternatively define the two GPIOs that are to be\n\t\tused as SCL / SDA.  Any of the previous I2C_xxx macros will\n\t\thave GPIO-based defaults assigned to them as appropriate.\n\n\t\tYou should define these to the GPIO value as given directly to\n\t\tthe generic GPIO functions.\n\n\t\tCFG_SYS_I2C_NOPROBES\n\n\t\tThis option specifies a list of I2C devices that will be skipped\n\t\twhen the 'i2c probe' command is issued.\n\n\t\te.g.\n\t\t\t#define CFG_SYS_I2C_NOPROBES {0x50,0x68}\n\n\t\twill skip addresses 0x50 and 0x68 on a board with one I2C bus\n\n\t\tCONFIG_SOFT_I2C_READ_REPEATED_START\n\n\t\tdefining this will force the i2c_read() function in\n\t\tthe soft_i2c driver to perform an I2C repeated start\n\t\tbetween writing the address pointer and reading the\n\t\tdata.  If this define is omitted the default behaviour\n\t\tof doing a stop-start sequence will be used.  Most I2C\n\t\tdevices can use either method, but some require one or\n\t\tthe other.\n\n- SPI Support:\tCONFIG_SPI\n\n\t\tEnables SPI driver (so far only tested with\n\t\tSPI EEPROM, also an instance works with Crystal A/D and\n\t\tD/As on the SACSng board)\n\n\t\tCFG_SYS_SPI_MXC_WAIT\n\t\tTimeout for waiting until spi transfer completed.\n\t\tdefault: (CONFIG_SYS_HZ/100)     /* 10 ms */\n\n- FPGA Support: CONFIG_FPGA\n\n\t\tEnables FPGA subsystem.\n\n\t\tCONFIG_FPGA_<vendor>\n\n\t\tEnables support for specific chip vendors.\n\t\t(ALTERA, XILINX)\n\n\t\tCONFIG_FPGA_<family>\n\n\t\tEnables support for FPGA family.\n\t\t(SPARTAN2, SPARTAN3, VIRTEX2, CYCLONE2, ACEX1K, ACEX)\n\n\t\tCONFIG_SYS_FPGA_CHECK_BUSY\n\n\t\tEnable checks on FPGA configuration interface busy\n\t\tstatus by the configuration function. This option\n\t\twill require a board or device specific function to\n\t\tbe written.\n\n\t\tCFG_FPGA_DELAY\n\n\t\tIf defined, a function that provides delays in the FPGA\n\t\tconfiguration driver.\n\n\t\tCFG_SYS_FPGA_CHECK_ERROR\n\n\t\tCheck for configuration errors during FPGA bitfile\n\t\tloading. For example, abort during Virtex II\n\t\tconfiguration if the INIT_B line goes low (which\n\t\tindicated a CRC error).\n\n\t\tCFG_SYS_FPGA_WAIT_INIT\n\n\t\tMaximum time to wait for the INIT_B line to de-assert\n\t\tafter PROB_B has been de-asserted during a Virtex II\n\t\tFPGA configuration sequence. The default time is 500\n\t\tms.\n\n\t\tCFG_SYS_FPGA_WAIT_BUSY\n\n\t\tMaximum time to wait for BUSY to de-assert during\n\t\tVirtex II FPGA configuration. The default is 5 ms.\n\n\t\tCFG_SYS_FPGA_WAIT_CONFIG\n\n\t\tTime to wait after FPGA configuration. The default is\n\t\t200 ms.\n\n- Vendor Parameter Protection:\n\n\t\tU-Boot considers the values of the environment\n\t\tvariables \"serial#\" (Board Serial Number) and\n\t\t\"ethaddr\" (Ethernet Address) to be parameters that\n\t\tare set once by the board vendor / manufacturer, and\n\t\tprotects these variables from casual modification by\n\t\tthe user. Once set, these variables are read-only,\n\t\tand write or delete attempts are rejected. You can\n\t\tchange this behaviour:\n\n\t\tIf CONFIG_ENV_OVERWRITE is #defined in your config\n\t\tfile, the write protection for vendor parameters is\n\t\tcompletely disabled. Anybody can change or delete\n\t\tthese parameters.\n\n\t\tThe same can be accomplished in a more flexible way\n\t\tfor any variable by configuring the type of access\n\t\tto allow for those variables in the \".flags\" variable\n\t\tor define CFG_ENV_FLAGS_LIST_STATIC.\n\n- Protected RAM:\n\t\tCFG_PRAM\n\n\t\tDefine this variable to enable the reservation of\n\t\t\"protected RAM\", i. e. RAM which is not overwritten\n\t\tby U-Boot. Define CFG_PRAM to hold the number of\n\t\tkB you want to reserve for pRAM. You can overwrite\n\t\tthis default value by defining an environment\n\t\tvariable \"pram\" to the number of kB you want to\n\t\treserve. Note that the board info structure will\n\t\tstill show the full amount of RAM. If pRAM is\n\t\treserved, a new environment variable \"mem\" will\n\t\tautomatically be defined to hold the amount of\n\t\tremaining RAM in a form that can be passed as boot\n\t\targument to Linux, for instance like that:\n\n\t\t\tsetenv bootargs ... mem=\\${mem}\n\t\t\tsaveenv\n\n\t\tThis way you can tell Linux not to use this memory,\n\t\teither, which results in a memory region that will\n\t\tnot be affected by reboots.\n\n\t\t*WARNING* If your board configuration uses automatic\n\t\tdetection of the RAM size, you must make sure that\n\t\tthis memory test is non-destructive. So far, the\n\t\tfollowing board configurations are known to be\n\t\t\"pRAM-clean\":\n\n\t\t\tIVMS8, IVML24, SPD8xx,\n\t\t\tHERMES, IP860, RPXlite, LWMON,\n\t\t\tFLAGADM\n\n- Error Recovery:\n\tNote:\n\n\t\tIn the current implementation, the local variables\n\t\tspace and global environment variables space are\n\t\tseparated. Local variables are those you define by\n\t\tsimply typing `name=value'. To access a local\n\t\tvariable later on, you have write `$name' or\n\t\t`${name}'; to execute the contents of a variable\n\t\tdirectly type `$name' at the command prompt.\n\n\t\tGlobal environment variables are those you use\n\t\tsetenv/printenv to work with. To run a command stored\n\t\tin such a variable, you need to use the run command,\n\t\tand you must not use the '$' sign to access them.\n\n\t\tTo store commands and special characters in a\n\t\tvariable, please use double quotation marks\n\t\tsurrounding the whole text of the variable, instead\n\t\tof the backslashes before semicolons and special\n\t\tsymbols.\n\n- Default Environment:\n\t\tCFG_EXTRA_ENV_SETTINGS\n\n\t\tDefine this to contain any number of null terminated\n\t\tstrings (variable = value pairs) that will be part of\n\t\tthe default environment compiled into the boot image.\n\n\t\tFor example, place something like this in your\n\t\tboard's config file:\n\n\t\t#define CFG_EXTRA_ENV_SETTINGS \\\n\t\t\t\"myvar1=value1\\0\" \\\n\t\t\t\"myvar2=value2\\0\"\n\n\t\tWarning: This method is based on knowledge about the\n\t\tinternal format how the environment is stored by the\n\t\tU-Boot code. This is NOT an official, exported\n\t\tinterface! Although it is unlikely that this format\n\t\twill change soon, there is no guarantee either.\n\t\tYou better know what you are doing here.\n\n\t\tNote: overly (ab)use of the default environment is\n\t\tdiscouraged. Make sure to check other ways to preset\n\t\tthe environment like the \"source\" command or the\n\t\tboot command first.\n\n- Automatic software updates via TFTP server\n\t\tCONFIG_UPDATE_TFTP\n\t\tCONFIG_UPDATE_TFTP_CNT_MAX\n\t\tCONFIG_UPDATE_TFTP_MSEC_MAX\n\n\t\tThese options enable and control the auto-update feature;\n\t\tfor a more detailed description refer to doc/README.update.\n\n- MTD Support (mtdparts command, UBI support)\n\t\tCONFIG_MTD_UBI_WL_THRESHOLD\n\t\tThis parameter defines the maximum difference between the highest\n\t\terase counter value and the lowest erase counter value of eraseblocks\n\t\tof UBI devices. When this threshold is exceeded, UBI starts performing\n\t\twear leveling by means of moving data from eraseblock with low erase\n\t\tcounter to eraseblocks with high erase counter.\n\n\t\tThe default value should be OK for SLC NAND flashes, NOR flashes and\n\t\tother flashes which have eraseblock life-cycle 100000 or more.\n\t\tHowever, in case of MLC NAND flashes which typically have eraseblock\n\t\tlife-cycle less than 10000, the threshold should be lessened (e.g.,\n\t\tto 128 or 256, although it does not have to be power of 2).\n\n\t\tdefault: 4096\n\n\t\tCONFIG_MTD_UBI_BEB_LIMIT\n\t\tThis option specifies the maximum bad physical eraseblocks UBI\n\t\texpects on the MTD device (per 1024 eraseblocks). If the\n\t\tunderlying flash does not admit of bad eraseblocks (e.g. NOR\n\t\tflash), this value is ignored.\n\n\t\tNAND datasheets often specify the minimum and maximum NVM\n\t\t(Number of Valid Blocks) for the flashes' endurance lifetime.\n\t\tThe maximum expected bad eraseblocks per 1024 eraseblocks\n\t\tthen can be calculated as \"1024 * (1 - MinNVB / MaxNVB)\",\n\t\twhich gives 20 for most NANDs (MaxNVB is basically the total\n\t\tcount of eraseblocks on the chip).\n\n\t\tTo put it differently, if this value is 20, UBI will try to\n\t\treserve about 1.9% of physical eraseblocks for bad blocks\n\t\thandling. And that will be 1.9% of eraseblocks on the entire\n\t\tNAND chip, not just the MTD partition UBI attaches. This means\n\t\tthat if you have, say, a NAND flash chip admits maximum 40 bad\n\t\teraseblocks, and it is split on two MTD partitions of the same\n\t\tsize, UBI will reserve 40 eraseblocks when attaching a\n\t\tpartition.\n\n\t\tdefault: 20\n\n\t\tCONFIG_MTD_UBI_FASTMAP\n\t\tFastmap is a mechanism which allows attaching an UBI device\n\t\tin nearly constant time. Instead of scanning the whole MTD device it\n\t\tonly has to locate a checkpoint (called fastmap) on the device.\n\t\tThe on-flash fastmap contains all information needed to attach\n\t\tthe device. Using fastmap makes only sense on large devices where\n\t\tattaching by scanning takes long. UBI will not automatically install\n\t\ta fastmap on old images, but you can set the UBI parameter\n\t\tCONFIG_MTD_UBI_FASTMAP_AUTOCONVERT to 1 if you want so. Please note\n\t\tthat fastmap-enabled images are still usable with UBI implementations\n\t\twithout\tfastmap support. On typical flash devices the whole fastmap\n\t\tfits into one PEB. UBI will reserve PEBs to hold two fastmaps.\n\n\t\tCONFIG_MTD_UBI_FASTMAP_AUTOCONVERT\n\t\tSet this parameter to enable fastmap automatically on images\n\t\twithout a fastmap.\n\t\tdefault: 0\n\n\t\tCONFIG_MTD_UBI_FM_DEBUG\n\t\tEnable UBI fastmap debug\n\t\tdefault: 0\n\n- SPL framework\n\t\tCONFIG_SPL\n\t\tEnable building of SPL globally.\n\n\t\tCONFIG_SPL_PANIC_ON_RAW_IMAGE\n\t\tWhen defined, SPL will panic() if the image it has\n\t\tloaded does not have a signature.\n\t\tDefining this is useful when code which loads images\n\t\tin SPL cannot guarantee that absolutely all read errors\n\t\twill be caught.\n\t\tAn example is the LPC32XX MLC NAND driver, which will\n\t\tconsider that a completely unreadable NAND block is bad,\n\t\tand thus should be skipped silently.\n\n\t\tCONFIG_SPL_DISPLAY_PRINT\n\t\tFor ARM, enable an optional function to print more information\n\t\tabout the running system.\n\n\t\tCONFIG_SPL_MPC83XX_WAIT_FOR_NAND\n\t\tSet this for NAND SPL on PPC mpc83xx targets, so that\n\t\tstart.S waits for the rest of the SPL to load before\n\t\tcontinuing (the hardware starts execution after just\n\t\tloading the first page rather than the full 4K).\n\n\t\tCONFIG_SPL_UBI\n\t\tSupport for a lightweight UBI (fastmap) scanner and\n\t\tloader\n\n\t\tCONFIG_SYS_NAND_5_ADDR_CYCLE, CONFIG_SYS_NAND_PAGE_SIZE,\n\t\tCONFIG_SYS_NAND_OOBSIZE, CONFIG_SYS_NAND_BLOCK_SIZE,\n\t\tCONFIG_SYS_NAND_BAD_BLOCK_POS, CFG_SYS_NAND_ECCPOS,\n\t\tCFG_SYS_NAND_ECCSIZE, CFG_SYS_NAND_ECCBYTES\n\t\tDefines the size and behavior of the NAND that SPL uses\n\t\tto read U-Boot\n\n\t\tCFG_SYS_NAND_U_BOOT_DST\n\t\tLocation in memory to load U-Boot to\n\n\t\tCFG_SYS_NAND_U_BOOT_SIZE\n\t\tSize of image to load\n\n\t\tCFG_SYS_NAND_U_BOOT_START\n\t\tEntry point in loaded image to jump to\n\n\t\tCONFIG_SPL_RAM_DEVICE\n\t\tSupport for running image already present in ram, in SPL binary\n\n\t\tCONFIG_SPL_FIT_PRINT\n\t\tPrinting information about a FIT image adds quite a bit of\n\t\tcode to SPL. So this is normally disabled in SPL. Use this\n\t\toption to re-enable it. This will affect the output of the\n\t\tbootm command when booting a FIT image.\n\n- Interrupt support (PPC):\n\n\t\tThere are common interrupt_init() and timer_interrupt()\n\t\tfor all PPC archs. interrupt_init() calls interrupt_init_cpu()\n\t\tfor CPU specific initialization. interrupt_init_cpu()\n\t\tshould set decrementer_count to appropriate value. If\n\t\tCPU resets decrementer automatically after interrupt\n\t\t(ppc4xx) it should set decrementer_count to zero.\n\t\ttimer_interrupt() calls timer_interrupt_cpu() for CPU\n\t\tspecific handling. If board has watchdog / status_led\n\t\t/ other_activity_monitor it works automatically from\n\t\tgeneral timer_interrupt().\n\n\nBoard initialization settings:\n------------------------------\n\nDuring Initialization u-boot calls a number of board specific functions\nto allow the preparation of board specific prerequisites, e.g. pin setup\nbefore drivers are initialized. To enable these callbacks the\nfollowing configuration macros have to be defined. Currently this is\narchitecture specific, so please check arch/your_architecture/lib/board.c\ntypically in board_init_f() and board_init_r().\n\n- CONFIG_BOARD_EARLY_INIT_F: Call board_early_init_f()\n- CONFIG_BOARD_EARLY_INIT_R: Call board_early_init_r()\n- CONFIG_BOARD_LATE_INIT: Call board_late_init()\n\nConfiguration Settings:\n-----------------------\n\n- CONFIG_SYS_LONGHELP: Defined when you want long help messages included;\n\t\tundefine this when you're short of memory.\n\n- CFG_SYS_HELP_CMD_WIDTH: Defined when you want to override the default\n\t\twidth of the commands listed in the 'help' command output.\n\n- CONFIG_SYS_PROMPT:\tThis is what U-Boot prints on the console to\n\t\tprompt for user input.\n\n- CFG_SYS_BAUDRATE_TABLE:\n\t\tList of legal baudrate settings for this board.\n\n- CFG_SYS_MEM_RESERVE_SECURE\n\t\tOnly implemented for ARMv8 for now.\n\t\tIf defined, the size of CFG_SYS_MEM_RESERVE_SECURE memory\n\t\tis substracted from total RAM and won't be reported to OS.\n\t\tThis memory can be used as secure memory. A variable\n\t\tgd->arch.secure_ram is used to track the location. In systems\n\t\tthe RAM base is not zero, or RAM is divided into banks,\n\t\tthis variable needs to be recalcuated to get the address.\n\n- CFG_SYS_SDRAM_BASE:\n\t\tPhysical start address of SDRAM. _Must_ be 0 here.\n\n- CFG_SYS_FLASH_BASE:\n\t\tPhysical start address of Flash memory.\n\n- CONFIG_SYS_MALLOC_LEN:\n\t\tSize of DRAM reserved for malloc() use.\n\n- CFG_SYS_BOOTMAPSZ:\n\t\tMaximum size of memory mapped by the startup code of\n\t\tthe Linux kernel; all data that must be processed by\n\t\tthe Linux kernel (bd_info, boot arguments, FDT blob if\n\t\tused) must be put below this limit, unless \"bootm_low\"\n\t\tenvironment variable is defined and non-zero. In such case\n\t\tall data for the Linux kernel must be between \"bootm_low\"\n\t\tand \"bootm_low\" + CFG_SYS_BOOTMAPSZ.\t The environment\n\t\tvariable \"bootm_mapsize\" will override the value of\n\t\tCFG_SYS_BOOTMAPSZ.  If CFG_SYS_BOOTMAPSZ is undefined,\n\t\tthen the value in \"bootm_size\" will be used instead.\n\n- CONFIG_SYS_BOOT_GET_CMDLINE:\n\t\tEnables allocating and saving kernel cmdline in space between\n\t\t\"bootm_low\" and \"bootm_low\" + BOOTMAPSZ.\n\n- CONFIG_SYS_BOOT_GET_KBD:\n\t\tEnables allocating and saving a kernel copy of the bd_info in\n\t\tspace between \"bootm_low\" and \"bootm_low\" + BOOTMAPSZ.\n\n- CONFIG_SYS_FLASH_PROTECTION\n\t\tIf defined, hardware flash sectors protection is used\n\t\tinstead of U-Boot software protection.\n\n- CONFIG_SYS_FLASH_CFI:\n\t\tDefine if the flash driver uses extra elements in the\n\t\tcommon flash structure for storing flash geometry.\n\n- CONFIG_FLASH_CFI_DRIVER\n\t\tThis option also enables the building of the cfi_flash driver\n\t\tin the drivers directory\n\n- CONFIG_FLASH_CFI_MTD\n\t\tThis option enables the building of the cfi_mtd driver\n\t\tin the drivers directory. The driver exports CFI flash\n\t\tto the MTD layer.\n\n- CONFIG_SYS_FLASH_USE_BUFFER_WRITE\n\t\tUse buffered writes to flash.\n\n- CONFIG_ENV_FLAGS_LIST_DEFAULT\n- CFG_ENV_FLAGS_LIST_STATIC\n\tEnable validation of the values given to environment variables when\n\tcalling env set.  Variables can be restricted to only decimal,\n\thexadecimal, or boolean.  If CONFIG_CMD_NET is also defined,\n\tthe variables can also be restricted to IP address or MAC address.\n\n\tThe format of the list is:\n\t\ttype_attribute = [s|d|x|b|i|m]\n\t\taccess_attribute = [a|r|o|c]\n\t\tattributes = type_attribute[access_attribute]\n\t\tentry = variable_name[:attributes]\n\t\tlist = entry[,list]\n\n\tThe type attributes are:\n\t\ts - String (default)\n\t\td - Decimal\n\t\tx - Hexadecimal\n\t\tb - Boolean ([1yYtT|0nNfF])\n\t\ti - IP address\n\t\tm - MAC address\n\n\tThe access attributes are:\n\t\ta - Any (default)\n\t\tr - Read-only\n\t\to - Write-once\n\t\tc - Change-default\n\n\t- CONFIG_ENV_FLAGS_LIST_DEFAULT\n\t\tDefine this to a list (string) to define the \".flags\"\n\t\tenvironment variable in the default or embedded environment.\n\n\t- CFG_ENV_FLAGS_LIST_STATIC\n\t\tDefine this to a list (string) to define validation that\n\t\tshould be done if an entry is not found in the \".flags\"\n\t\tenvironment variable.  To override a setting in the static\n\t\tlist, simply add an entry for the same variable name to the\n\t\t\".flags\" variable.\n\n\tIf CONFIG_REGEX is defined, the variable_name above is evaluated as a\n\tregular expression. This allows multiple variables to define the same\n\tflags without explicitly listing them for each variable.\n\nThe following definitions that deal with the placement and management\nof environment data (variable area); in general, we support the\nfollowing configurations:\n\nBE CAREFUL! The first access to the environment happens quite early\nin U-Boot initialization (when we try to get the setting of for the\nconsole baudrate). You *MUST* have mapped your NVRAM area then, or\nU-Boot will hang.\n\nPlease note that even with NVRAM we still use a copy of the\nenvironment in RAM: we could work on NVRAM directly, but we want to\nkeep settings there always unmodified except somebody uses \"saveenv\"\nto save the current settings.\n\nBE CAREFUL! For some special cases, the local device can not use\n\"saveenv\" command. For example, the local device will get the\nenvironment stored in a remote NOR flash by SRIO or PCIE link,\nbut it can not erase, write this NOR flash by SRIO or PCIE interface.\n\n- CONFIG_NAND_ENV_DST\n\n\tDefines address in RAM to which the nand_spl code should copy the\n\tenvironment. If redundant environment is used, it will be copied to\n\tCONFIG_NAND_ENV_DST + CONFIG_ENV_SIZE.\n\nPlease note that the environment is read-only until the monitor\nhas been relocated to RAM and a RAM copy of the environment has been\ncreated; also, when using EEPROM you will have to use env_get_f()\nuntil then to read environment variables.\n\nThe environment is protected by a CRC32 checksum. Before the monitor\nis relocated into RAM, as a result of a bad CRC you will be working\nwith the compiled-in default environment - *silently*!!! [This is\nnecessary, because the first environment variable we need is the\n\"baudrate\" setting for the console - if we have a bad CRC, we don't\nhave any device yet where we could complain.]\n\nNote: once the monitor has been relocated, then it will complain if\nthe default environment is used; a new CRC is computed as soon as you\nuse the \"saveenv\" command to store a valid environment.\n\n- CONFIG_DISPLAY_BOARDINFO\n\t\tDisplay information about the board that U-Boot is running on\n\t\twhen U-Boot starts up. The board function checkboard() is called\n\t\tto do this.\n\n- CONFIG_DISPLAY_BOARDINFO_LATE\n\t\tSimilar to the previous option, but display this information\n\t\tlater, once stdio is running and output goes to the LCD, if\n\t\tpresent.\n\nLow Level (hardware related) configuration options:\n---------------------------------------------------\n\n- CONFIG_SYS_CACHELINE_SIZE:\n\t\tCache Line Size of the CPU.\n\n- CONFIG_SYS_CCSRBAR_DEFAULT:\n\t\tDefault (power-on reset) physical address of CCSR on Freescale\n\t\tPowerPC SOCs.\n\n- CFG_SYS_CCSRBAR:\n\t\tVirtual address of CCSR.  On a 32-bit build, this is typically\n\t\tthe same value as CONFIG_SYS_CCSRBAR_DEFAULT.\n\n- CFG_SYS_CCSRBAR_PHYS:\n\t\tPhysical address of CCSR.  CCSR can be relocated to a new\n\t\tphysical address, if desired.  In this case, this macro should\n\t\tbe set to that address.\t Otherwise, it should be set to the\n\t\tsame value as CONFIG_SYS_CCSRBAR_DEFAULT.  For example, CCSR\n\t\tis typically relocated on 36-bit builds.  It is recommended\n\t\tthat this macro be defined via the _HIGH and _LOW macros:\n\n\t\t#define CFG_SYS_CCSRBAR_PHYS ((CFG_SYS_CCSRBAR_PHYS_HIGH\n\t\t\t* 1ull) << 32 | CFG_SYS_CCSRBAR_PHYS_LOW)\n\n- CFG_SYS_CCSRBAR_PHYS_HIGH:\n\t\tBits 33-36 of CFG_SYS_CCSRBAR_PHYS.\tThis value is typically\n\t\teither 0 (32-bit build) or 0xF (36-bit build).\tThis macro is\n\t\tused in assembly code, so it must not contain typecasts or\n\t\tinteger size suffixes (e.g. \"ULL\").\n\n- CFG_SYS_CCSRBAR_PHYS_LOW:\n\t\tLower 32-bits of CFG_SYS_CCSRBAR_PHYS.  This macro is\n\t\tused in assembly code, so it must not contain typecasts or\n\t\tinteger size suffixes (e.g. \"ULL\").\n\n- CONFIG_SYS_IMMR:\tPhysical address of the Internal Memory.\n\t\tDO NOT CHANGE unless you know exactly what you're\n\t\tdoing! (11-4) [MPC8xx systems only]\n\n- CFG_SYS_INIT_RAM_ADDR:\n\n\t\tStart address of memory area that can be used for\n\t\tinitial data and stack; please note that this must be\n\t\twritable memory that is working WITHOUT special\n\t\tinitialization, i. e. you CANNOT use normal RAM which\n\t\twill become available only after programming the\n\t\tmemory controller and running certain initialization\n\t\tsequences.\n\n\t\tU-Boot uses the following memory types:\n\t\t- MPC8xx: IMMR (internal memory of the CPU)\n\n- CONFIG_SYS_SCCR:\tSystem Clock and reset Control Register (15-27)\n\n- CONFIG_SYS_OR_TIMING_SDRAM:\n\t\tSDRAM timing\n\n- CONFIG_SYS_SRIOn_MEM_VIRT:\n\t\tVirtual Address of SRIO port 'n' memory region\n\n- CONFIG_SYS_SRIOn_MEM_PHYxS:\n\t\tPhysical Address of SRIO port 'n' memory region\n\n- CONFIG_SYS_SRIOn_MEM_SIZE:\n\t\tSize of SRIO port 'n' memory region\n\n- CONFIG_SYS_NAND_BUSWIDTH_16BIT\n\t\tDefined to tell the NAND controller that the NAND chip is using\n\t\ta 16 bit bus.\n\t\tNot all NAND drivers use this symbol.\n\t\tExample of drivers that use it:\n\t\t- drivers/mtd/nand/raw/ndfc.c\n\t\t- drivers/mtd/nand/raw/mxc_nand.c\n\n- CONFIG_SYS_NDFC_EBC0_CFG\n\t\tSets the EBC0_CFG register for the NDFC. If not defined\n\t\ta default value will be used.\n\n- CONFIG_SYS_SPD_BUS_NUM\n\t\tIf SPD EEPROM is on an I2C bus other than the first\n\t\tone, specify here. Note that the value must resolve\n\t\tto something your driver can deal with.\n\n- CONFIG_FSL_DDR_INTERACTIVE\n\t\tEnable interactive DDR debugging. See doc/README.fsl-ddr.\n\n- CONFIG_FSL_DDR_SYNC_REFRESH\n\t\tEnable sync of refresh for multiple controllers.\n\n- CONFIG_FSL_DDR_BIST\n\t\tEnable built-in memory test for Freescale DDR controllers.\n\n- CONFIG_RMII\n\t\tEnable RMII mode for all FECs.\n\t\tNote that this is a global option, we can't\n\t\thave one FEC in standard MII mode and another in RMII mode.\n\n- CONFIG_CRC32_VERIFY\n\t\tAdd a verify option to the crc32 command.\n\t\tThe syntax is:\n\n\t\t=> crc32 -v <address> <count> <crc32>\n\n\t\tWhere address/count indicate a memory area\n\t\tand crc32 is the correct crc32 which the\n\t\tarea should have.\n\n- CONFIG_LOOPW\n\t\tAdd the \"loopw\" memory command. This only takes effect if\n\t\tthe memory commands are activated globally (CONFIG_CMD_MEMORY).\n\n- CONFIG_CMD_MX_CYCLIC\n\t\tAdd the \"mdc\" and \"mwc\" memory commands. These are cyclic\n\t\t\"md/mw\" commands.\n\t\tExamples:\n\n\t\t=> mdc.b 10 4 500\n\t\tThis command will print 4 bytes (10,11,12,13) each 500 ms.\n\n\t\t=> mwc.l 100 12345678 10\n\t\tThis command will write 12345678 to address 100 all 10 ms.\n\n\t\tThis only takes effect if the memory commands are activated\n\t\tglobally (CONFIG_CMD_MEMORY).\n\n- CONFIG_XPL_BUILD\n\t\tSet when the currently running compilation is for an artifact\n\t\tthat will end up in one of the 'xPL' builds, i.e. SPL, TPL or\n\t\tVPL. Code that needs phase-specific behaviour can check this,\n\t\tor (where possible) use xpl_phase() instead.\n\n- CONFIG_TPL_BUILD\n\t\tSet when the currently running compilation is for an artifact\n\t\tthat will end up in the TPL build (as opposed to SPL, VPL or\n\t\tU-Boot proper). Code that needs phase-specific behaviour can\n\t\tcheck this, or (where possible) use xpl_phase() instead.\n\n- CONFIG_VPL_BUILD\n\t\tSet when the currently running compilation is for an artifact\n\t\tthat will end up in the VPL build (as opposed to the SPL, TPL\n\t\tor U-Boot proper). Code that needs phase-specific behaviour can\n\t\tcheck this, or (where possible) use xpl_phase() instead.\n\n- CONFIG_ARCH_MAP_SYSMEM\n\t\tGenerally U-Boot (and in particular the md command) uses\n\t\teffective address. It is therefore not necessary to regard\n\t\tU-Boot address as virtual addresses that need to be translated\n\t\tto physical addresses. However, sandbox requires this, since\n\t\tit maintains its own little RAM buffer which contains all\n\t\taddressable memory. This option causes some memory accesses\n\t\tto be mapped through map_sysmem() / unmap_sysmem().\n\n- CONFIG_X86_RESET_VECTOR\n\t\tIf defined, the x86 reset vector code is included. This is not\n\t\tneeded when U-Boot is running from Coreboot.\n\nFreescale QE/FMAN Firmware Support:\n-----------------------------------\n\nThe Freescale QUICCEngine (QE) and Frame Manager (FMAN) both support the\nloading of \"firmware\", which is encoded in the QE firmware binary format.\nThis firmware often needs to be loaded during U-Boot booting, so macros\nare used to identify the storage device (NOR flash, SPI, etc) and the address\nwithin that device.\n\n- CONFIG_SYS_FMAN_FW_ADDR\n\tThe address in the storage device where the FMAN microcode is located.  The\n\tmeaning of this address depends on which CONFIG_SYS_QE_FMAN_FW_IN_xxx macro\n\tis also specified.\n\n- CONFIG_SYS_QE_FW_ADDR\n\tThe address in the storage device where the QE microcode is located.  The\n\tmeaning of this address depends on which CONFIG_SYS_QE_FMAN_FW_IN_xxx macro\n\tis also specified.\n\n- CONFIG_SYS_QE_FMAN_FW_LENGTH\n\tThe maximum possible size of the firmware.  The firmware binary format\n\thas a field that specifies the actual size of the firmware, but it\n\tmight not be possible to read any part of the firmware unless some\n\tlocal storage is allocated to hold the entire firmware first.\n\n- CONFIG_SYS_QE_FMAN_FW_IN_NOR\n\tSpecifies that QE/FMAN firmware is located in NOR flash, mapped as\n\tnormal addressable memory via the LBC.  CONFIG_SYS_FMAN_FW_ADDR is the\n\tvirtual address in NOR flash.\n\n- CONFIG_SYS_QE_FMAN_FW_IN_NAND\n\tSpecifies that QE/FMAN firmware is located in NAND flash.\n\tCONFIG_SYS_FMAN_FW_ADDR is the offset within NAND flash.\n\n- CONFIG_SYS_QE_FMAN_FW_IN_MMC\n\tSpecifies that QE/FMAN firmware is located on the primary SD/MMC\n\tdevice.  CONFIG_SYS_FMAN_FW_ADDR is the byte offset on that device.\n\n- CONFIG_SYS_QE_FMAN_FW_IN_REMOTE\n\tSpecifies that QE/FMAN firmware is located in the remote (master)\n\tmemory space.\tCONFIG_SYS_FMAN_FW_ADDR is a virtual address which\n\tcan be mapped from slave TLB->slave LAW->slave SRIO or PCIE outbound\n\twindow->master inbound window->master LAW->the ucode address in\n\tmaster's memory space.\n\nFreescale Layerscape Management Complex Firmware Support:\n---------------------------------------------------------\nThe Freescale Layerscape Management Complex (MC) supports the loading of\n\"firmware\".\nThis firmware often needs to be loaded during U-Boot booting, so macros\nare used to identify the storage device (NOR flash, SPI, etc) and the address\nwithin that device.\n\n- CONFIG_FSL_MC_ENET\n\tEnable the MC driver for Layerscape SoCs.\n\nFreescale Layerscape Debug Server Support:\n-------------------------------------------\nThe Freescale Layerscape Debug Server Support supports the loading of\n\"Debug Server firmware\" and triggering SP boot-rom.\nThis firmware often needs to be loaded during U-Boot booting.\n\n- CONFIG_SYS_MC_RSV_MEM_ALIGN\n\tDefine alignment of reserved memory MC requires\n\n\nBuilding the Software:\n======================\n\nBuilding U-Boot has been tested in several native build environments\nand in many different cross environments. Of course we cannot support\nall possibly existing versions of cross development tools in all\n(potentially obsolete) versions. In case of tool chain problems we\nrecommend to use the ELDK (see https://www.denx.de/wiki/DULG/ELDK)\nwhich is extensively used to build and test U-Boot.\n\nIf you are not using a native environment, it is assumed that you\nhave GNU cross compiling tools available in your path. In this case,\nyou must set the environment variable CROSS_COMPILE in your shell.\nNote that no changes to the Makefile or any other source files are\nnecessary. For example using the ELDK on a 4xx CPU, please enter:\n\n\t$ CROSS_COMPILE=ppc_4xx-\n\t$ export CROSS_COMPILE\n\nU-Boot is intended to be simple to build. After installing the\nsources you must configure U-Boot for one specific board type. This\nis done by typing:\n\n\tmake NAME_defconfig\n\nwhere \"NAME_defconfig\" is the name of one of the existing configu-\nrations; see configs/*_defconfig for supported names.\n\nNote: for some boards special configuration names may exist; check if\n      additional information is available from the board vendor; for\n      instance, the TQM823L systems are available without (standard)\n      or with LCD support. You can select such additional \"features\"\n      when choosing the configuration, i. e.\n\n      make TQM823L_defconfig\n\t- will configure for a plain TQM823L, i. e. no LCD support\n\n      make TQM823L_LCD_defconfig\n\t- will configure for a TQM823L with U-Boot console on LCD\n\n      etc.\n\n\nFinally, type \"make all\", and you should get some working U-Boot\nimages ready for download to / installation on your system:\n\n- \"u-boot.bin\" is a raw binary image\n- \"u-boot\" is an image in ELF binary format\n- \"u-boot.srec\" is in Motorola S-Record format\n\nUser specific CPPFLAGS, AFLAGS and CFLAGS can be passed to the compiler by\nsetting the according environment variables KCPPFLAGS, KAFLAGS and KCFLAGS.\nFor example to treat all compiler warnings as errors:\n\n\tmake KCFLAGS=-Werror\n\nPlease be aware that the Makefiles assume you are using GNU make, so\nfor instance on NetBSD you might need to use \"gmake\" instead of\nnative \"make\".\n\n\nIf the system board that you have is not listed, then you will need\nto port U-Boot to your hardware platform. To do this, follow these\nsteps:\n\n1.  Create a new directory to hold your board specific code. Add any\n    files you need. In your board directory, you will need at least\n    the \"Makefile\" and a \"<board>.c\".\n2.  Create a new configuration file \"include/configs/<board>.h\" for\n    your board.\n3.  If you're porting U-Boot to a new CPU, then also create a new\n    directory to hold your CPU specific code. Add any files you need.\n4.  Run \"make <board>_defconfig\" with your new name.\n5.  Type \"make\", and you should get a working \"u-boot.srec\" file\n    to be installed on your target system.\n6.  Debug and solve any problems that might arise.\n    [Of course, this last step is much harder than it sounds.]\n\n\nTesting of U-Boot Modifications, Ports to New Hardware, etc.:\n==============================================================\n\nIf you have modified U-Boot sources (for instance added a new board\nor support for new devices, a new CPU, etc.) you are expected to\nprovide feedback to the other developers. The feedback normally takes\nthe form of a \"patch\", i.e. a context diff against a certain (latest\nofficial or latest in the git repository) version of U-Boot sources.\n\nBut before you submit such a patch, please verify that your modifi-\ncation did not break existing code. At least make sure that *ALL* of\nthe supported boards compile WITHOUT ANY compiler warnings. To do so,\njust run the buildman script (tools/buildman/buildman), which will\nconfigure and build U-Boot for ALL supported system. Be warned, this\nwill take a while. Please see the buildman README, or run 'buildman -H'\nfor documentation.\n\n\nSee also \"U-Boot Porting Guide\" below.\n\n\nMonitor Commands - Overview:\n============================\n\ngo\t- start application at address 'addr'\nrun\t- run commands in an environment variable\nbootm\t- boot application image from memory\nbootp\t- boot image via network using BootP/TFTP protocol\nbootz   - boot zImage from memory\ntftpboot- boot image via network using TFTP protocol\n\t       and env variables \"ipaddr\" and \"serverip\"\n\t       (and eventually \"gatewayip\")\ntftpput - upload a file via network using TFTP protocol\nrarpboot- boot image via network using RARP/TFTP protocol\ndiskboot- boot from IDE devicebootd   - boot default, i.e., run 'bootcmd'\nloads\t- load S-Record file over serial line\nloadb\t- load binary file over serial line (kermit mode)\nloadm   - load binary blob from source address to destination address\nmd\t- memory display\nmm\t- memory modify (auto-incrementing)\nnm\t- memory modify (constant address)\nmw\t- memory write (fill)\nms\t- memory search\ncp\t- memory copy\ncmp\t- memory compare\ncrc32\t- checksum calculation\ni2c\t- I2C sub-system\nsspi\t- SPI utility commands\nbase\t- print or set address offset\nprintenv- print environment variables\npwm\t- control pwm channels\nseama   - load SEAMA NAND image\nsetenv\t- set environment variables\nsaveenv - save environment variables to persistent storage\nprotect - enable or disable FLASH write protection\nerase\t- erase FLASH memory\nflinfo\t- print FLASH memory information\nnand\t- NAND memory operations (see doc/README.nand)\nbdinfo\t- print Board Info structure\niminfo\t- print header information for application image\nconinfo - print console devices and informations\nide\t- IDE sub-system\nloop\t- infinite loop on address range\nloopw\t- infinite write loop on address range\nmtest\t- simple RAM test\nicache\t- enable or disable instruction cache\ndcache\t- enable or disable data cache\nreset\t- Perform RESET of the CPU\necho\t- echo args to console\nversion - print monitor version\nhelp\t- print online help\n?\t- alias for 'help'\n\n\nMonitor Commands - Detailed Description:\n========================================\n\nTODO.\n\nFor now: just type \"help <command>\".\n\n\nNote for Redundant Ethernet Interfaces:\n=======================================\n\nSome boards come with redundant Ethernet interfaces; U-Boot supports\nsuch configurations and is capable of automatic selection of a\n\"working\" interface when needed. MAC assignment works as follows:\n\nNetwork interfaces are numbered eth0, eth1, eth2, ... Corresponding\nMAC addresses can be stored in the environment as \"ethaddr\" (=>eth0),\n\"eth1addr\" (=>eth1), \"eth2addr\", ...\n\nIf the network interface stores some valid MAC address (for instance\nin SROM), this is used as default address if there is NO correspon-\nding setting in the environment; if the corresponding environment\nvariable is set, this overrides the settings in the card; that means:\n\no If the SROM has a valid MAC address, and there is no address in the\n  environment, the SROM's address is used.\n\no If there is no valid address in the SROM, and a definition in the\n  environment exists, then the value from the environment variable is\n  used.\n\no If both the SROM and the environment contain a MAC address, and\n  both addresses are the same, this MAC address is used.\n\no If both the SROM and the environment contain a MAC address, and the\n  addresses differ, the value from the environment is used and a\n  warning is printed.\n\no If neither SROM nor the environment contain a MAC address, an error\n  is raised. If CONFIG_NET_RANDOM_ETHADDR is defined, then in this case\n  a random, locally-assigned MAC is used.\n\nIf Ethernet drivers implement the 'write_hwaddr' function, valid MAC addresses\nwill be programmed into hardware as part of the initialization process.\t This\nmay be skipped by setting the appropriate 'ethmacskip' environment variable.\nThe naming convention is as follows:\n\"ethmacskip\" (=>eth0), \"eth1macskip\" (=>eth1) etc.\n\nImage Formats:\n==============\n\nU-Boot is capable of booting (and performing other auxiliary operations on)\nimages in two formats:\n\nNew uImage format (FIT)\n-----------------------\n\nFlexible and powerful format based on Flattened Image Tree -- FIT (similar\nto Flattened Device Tree). It allows the use of images with multiple\ncomponents (several kernels, ramdisks, etc.), with contents protected by\nSHA1, MD5 or CRC32. More details are found in the doc/uImage.FIT directory.\n\n\nOld uImage format\n-----------------\n\nOld image format is based on binary files which can be basically anything,\npreceded by a special header; see the definitions in include/image.h for\ndetails; basically, the header defines the following image properties:\n\n* Target Operating System (Provisions for OpenBSD, NetBSD, FreeBSD,\n  4.4BSD, Linux, SVR4, Esix, Solaris, Irix, SCO, Dell, NCR, VxWorks,\n  LynxOS, pSOS, QNX, RTEMS, INTEGRITY;\n  Currently supported: Linux, NetBSD, VxWorks, QNX, RTEMS, INTEGRITY).\n* Target CPU Architecture (Provisions for Alpha, ARM, Intel x86,\n  IA64, MIPS, Nios II, PowerPC, IBM S390, SuperH, Sparc, Sparc 64 Bit;\n  Currently supported: ARM, Intel x86, MIPS, Nios II, PowerPC).\n* Compression Type (uncompressed, gzip, bzip2)\n* Load Address\n* Entry Point\n* Image Name\n* Image Timestamp\n\nThe header is marked by a special Magic Number, and both the header\nand the data portions of the image are secured against corruption by\nCRC32 checksums.\n\n\nLinux Support:\n==============\n\nAlthough U-Boot should support any OS or standalone application\neasily, the main focus has always been on Linux during the design of\nU-Boot.\n\nU-Boot includes many features that so far have been part of some\nspecial \"boot loader\" code within the Linux kernel. Also, any\n\"initrd\" images to be used are no longer part of one big Linux image;\ninstead, kernel and \"initrd\" are separate images. This implementation\nserves several purposes:\n\n- the same features can be used for other OS or standalone\n  applications (for instance: using compressed images to reduce the\n  Flash memory footprint)\n\n- it becomes much easier to port new Linux kernel versions because\n  lots of low-level, hardware dependent stuff are done by U-Boot\n\n- the same Linux kernel image can now be used with different \"initrd\"\n  images; of course this also means that different kernel images can\n  be run with the same \"initrd\". This makes testing easier (you don't\n  have to build a new \"zImage.initrd\" Linux image when you just\n  change a file in your \"initrd\"). Also, a field-upgrade of the\n  software is easier now.\n\n\nLinux HOWTO:\n============\n\nPorting Linux to U-Boot based systems:\n---------------------------------------\n\nU-Boot cannot save you from doing all the necessary modifications to\nconfigure the Linux device drivers for use with your target hardware\n(no, we don't intend to provide a full virtual machine interface to\nLinux :-).\n\nBut now you can ignore ALL boot loader code (in arch/powerpc/mbxboot).\n\nJust make sure your machine specific header file (for instance\ninclude/asm-ppc/tqm8xx.h) includes the same definition of the Board\nInformation structure as we define in include/asm-<arch>/u-boot.h,\nand make sure that your definition of IMAP_ADDR uses the same value\nas your U-Boot configuration in CONFIG_SYS_IMMR.\n\nNote that U-Boot now has a driver model, a unified model for drivers.\nIf you are adding a new driver, plumb it into driver model. If there\nis no uclass available, you are encouraged to create one. See\ndoc/driver-model.\n\n\nConfiguring the Linux kernel:\n-----------------------------\n\nNo specific requirements for U-Boot. Make sure you have some root\ndevice (initial ramdisk, NFS) for your target system.\n\n\nBuilding a Linux Image:\n-----------------------\n\nWith U-Boot, \"normal\" build targets like \"zImage\" or \"bzImage\" are\nnot used. If you use recent kernel source, a new build target\n\"uImage\" will exist which automatically builds an image usable by\nU-Boot. Most older kernels also have support for a \"pImage\" target,\nwhich was introduced for our predecessor project PPCBoot and uses a\n100% compatible format.\n\nExample:\n\n\tmake TQM850L_defconfig\n\tmake oldconfig\n\tmake dep\n\tmake uImage\n\nThe \"uImage\" build target uses a special tool (in 'tools/mkimage') to\nencapsulate a compressed Linux kernel image with header\t information,\nCRC32 checksum etc. for use with U-Boot. This is what we are doing:\n\n* build a standard \"vmlinux\" kernel image (in ELF binary format):\n\n* convert the kernel into a raw binary image:\n\n\t${CROSS_COMPILE}-objcopy -O binary \\\n\t\t\t\t -R .note -R .comment \\\n\t\t\t\t -S vmlinux linux.bin\n\n* compress the binary image:\n\n\tgzip -9 linux.bin\n\n* package compressed binary image for U-Boot:\n\n\tmkimage -A ppc -O linux -T kernel -C gzip \\\n\t\t-a 0 -e 0 -n \"Linux Kernel Image\" \\\n\t\t-d linux.bin.gz uImage\n\n\nThe \"mkimage\" tool can also be used to create ramdisk images for use\nwith U-Boot, either separated from the Linux kernel image, or\ncombined into one file. \"mkimage\" encapsulates the images with a 64\nbyte header containing information about target architecture,\noperating system, image type, compression method, entry points, time\nstamp, CRC32 checksums, etc.\n\n\"mkimage\" can be called in two ways: to verify existing images and\nprint the header information, or to build new images.\n\nIn the first form (with \"-l\" option) mkimage lists the information\ncontained in the header of an existing U-Boot image; this includes\nchecksum verification:\n\n\ttools/mkimage -l image\n\t  -l ==> list image header information\n\nThe second form (with \"-d\" option) is used to build a U-Boot image\nfrom a \"data file\" which is used as image payload:\n\n\ttools/mkimage -A arch -O os -T type -C comp -a addr -e ep \\\n\t\t      -n name -d data_file image\n\t  -A ==> set architecture to 'arch'\n\t  -O ==> set operating system to 'os'\n\t  -T ==> set image type to 'type'\n\t  -C ==> set compression type 'comp'\n\t  -a ==> set load address to 'addr' (hex)\n\t  -e ==> set entry point to 'ep' (hex)\n\t  -n ==> set image name to 'name'\n\t  -d ==> use image data from 'datafile'\n\nRight now, all Linux kernels for PowerPC systems use the same load\naddress (0x00000000), but the entry point address depends on the\nkernel version:\n\n- 2.2.x kernels have the entry point at 0x0000000C,\n- 2.3.x and later kernels have the entry point at 0x00000000.\n\nSo a typical call to build a U-Boot image would read:\n\n\t-> tools/mkimage -n '2.4.4 kernel for TQM850L' \\\n\t> -A ppc -O linux -T kernel -C gzip -a 0 -e 0 \\\n\t> -d /opt/elsk/ppc_8xx/usr/src/linux-2.4.4/arch/powerpc/coffboot/vmlinux.gz \\\n\t> examples/uImage.TQM850L\n\tImage Name:   2.4.4 kernel for TQM850L\n\tCreated:      Wed Jul 19 02:34:59 2000\n\tImage Type:   PowerPC Linux Kernel Image (gzip compressed)\n\tData Size:    335725 Bytes = 327.86 kB = 0.32 MB\n\tLoad Address: 0x00000000\n\tEntry Point:  0x00000000\n\nTo verify the contents of the image (or check for corruption):\n\n\t-> tools/mkimage -l examples/uImage.TQM850L\n\tImage Name:   2.4.4 kernel for TQM850L\n\tCreated:      Wed Jul 19 02:34:59 2000\n\tImage Type:   PowerPC Linux Kernel Image (gzip compressed)\n\tData Size:    335725 Bytes = 327.86 kB = 0.32 MB\n\tLoad Address: 0x00000000\n\tEntry Point:  0x00000000\n\nNOTE: for embedded systems where boot time is critical you can trade\nspeed for memory and install an UNCOMPRESSED image instead: this\nneeds more space in Flash, but boots much faster since it does not\nneed to be uncompressed:\n\n\t-> gunzip /opt/elsk/ppc_8xx/usr/src/linux-2.4.4/arch/powerpc/coffboot/vmlinux.gz\n\t-> tools/mkimage -n '2.4.4 kernel for TQM850L' \\\n\t> -A ppc -O linux -T kernel -C none -a 0 -e 0 \\\n\t> -d /opt/elsk/ppc_8xx/usr/src/linux-2.4.4/arch/powerpc/coffboot/vmlinux \\\n\t> examples/uImage.TQM850L-uncompressed\n\tImage Name:   2.4.4 kernel for TQM850L\n\tCreated:      Wed Jul 19 02:34:59 2000\n\tImage Type:   PowerPC Linux Kernel Image (uncompressed)\n\tData Size:    792160 Bytes = 773.59 kB = 0.76 MB\n\tLoad Address: 0x00000000\n\tEntry Point:  0x00000000\n\n\nSimilar you can build U-Boot images from a 'ramdisk.image.gz' file\nwhen your kernel is intended to use an initial ramdisk:\n\n\t-> tools/mkimage -n 'Simple Ramdisk Image' \\\n\t> -A ppc -O linux -T ramdisk -C gzip \\\n\t> -d /LinuxPPC/images/SIMPLE-ramdisk.image.gz examples/simple-initrd\n\tImage Name:   Simple Ramdisk Image\n\tCreated:      Wed Jan 12 14:01:50 2000\n\tImage Type:   PowerPC Linux RAMDisk Image (gzip compressed)\n\tData Size:    566530 Bytes = 553.25 kB = 0.54 MB\n\tLoad Address: 0x00000000\n\tEntry Point:  0x00000000\n\nThe \"dumpimage\" tool can be used to disassemble or list the contents of images\nbuilt by mkimage. See dumpimage's help output (-h) for details.\n\nInstalling a Linux Image:\n-------------------------\n\nTo downloading a U-Boot image over the serial (console) interface,\nyou must convert the image to S-Record format:\n\n\tobjcopy -I binary -O srec examples/image examples/image.srec\n\nThe 'objcopy' does not understand the information in the U-Boot\nimage header, so the resulting S-Record file will be relative to\naddress 0x00000000. To load it to a given address, you need to\nspecify the target address as 'offset' parameter with the 'loads'\ncommand.\n\nExample: install the image to address 0x40100000 (which on the\nTQM8xxL is in the first Flash bank):\n\n\t=> erase 40100000 401FFFFF\n\n\t.......... done\n\tErased 8 sectors\n\n\t=> loads 40100000\n\t## Ready for S-Record download ...\n\t~>examples/image.srec\n\t1 2 3 4 5 6 7 8 9 10 11 12 13 ...\n\t...\n\t15989 15990 15991 15992\n\t[file transfer complete]\n\t[connected]\n\t## Start Addr = 0x00000000\n\n\nYou can check the success of the download using the 'iminfo' command;\nthis includes a checksum verification so you can be sure no data\ncorruption happened:\n\n\t=> imi 40100000\n\n\t## Checking Image at 40100000 ...\n\t   Image Name:\t 2.2.13 for initrd on TQM850L\n\t   Image Type:\t PowerPC Linux Kernel Image (gzip compressed)\n\t   Data Size:\t 335725 Bytes = 327 kB = 0 MB\n\t   Load Address: 00000000\n\t   Entry Point:\t 0000000c\n\t   Verifying Checksum ... OK\n\n\nBoot Linux:\n-----------\n\nThe \"bootm\" command is used to boot an application that is stored in\nmemory (RAM or Flash). In case of a Linux kernel image, the contents\nof the \"bootargs\" environment variable is passed to the kernel as\nparameters. You can check and modify this variable using the\n\"printenv\" and \"setenv\" commands:\n\n\n\t=> printenv bootargs\n\tbootargs=root=/dev/ram\n\n\t=> setenv bootargs root=/dev/nfs rw nfsroot=10.0.0.2:/LinuxPPC nfsaddrs=10.0.0.99:10.0.0.2\n\n\t=> printenv bootargs\n\tbootargs=root=/dev/nfs rw nfsroot=10.0.0.2:/LinuxPPC nfsaddrs=10.0.0.99:10.0.0.2\n\n\t=> bootm 40020000\n\t## Booting Linux kernel at 40020000 ...\n\t   Image Name:\t 2.2.13 for NFS on TQM850L\n\t   Image Type:\t PowerPC Linux Kernel Image (gzip compressed)\n\t   Data Size:\t 381681 Bytes = 372 kB = 0 MB\n\t   Load Address: 00000000\n\t   Entry Point:\t 0000000c\n\t   Verifying Checksum ... OK\n\t   Uncompressing Kernel Image ... OK\n\tLinux version 2.2.13 (wd@denx.local.net) (gcc version 2.95.2 19991024 (release)) #1 Wed Jul 19 02:35:17 MEST 2000\n\tBoot arguments: root=/dev/nfs rw nfsroot=10.0.0.2:/LinuxPPC nfsaddrs=10.0.0.99:10.0.0.2\n\ttime_init: decrementer frequency = 187500000/60\n\tCalibrating delay loop... 49.77 BogoMIPS\n\tMemory: 15208k available (700k kernel code, 444k data, 32k init) [c0000000,c1000000]\n\t...\n\nIf you want to boot a Linux kernel with initial RAM disk, you pass\nthe memory addresses of both the kernel and the initrd image (PPBCOOT\nformat!) to the \"bootm\" command:\n\n\t=> imi 40100000 40200000\n\n\t## Checking Image at 40100000 ...\n\t   Image Name:\t 2.2.13 for initrd on TQM850L\n\t   Image Type:\t PowerPC Linux Kernel Image (gzip compressed)\n\t   Data Size:\t 335725 Bytes = 327 kB = 0 MB\n\t   Load Address: 00000000\n\t   Entry Point:\t 0000000c\n\t   Verifying Checksum ... OK\n\n\t## Checking Image at 40200000 ...\n\t   Image Name:\t Simple Ramdisk Image\n\t   Image Type:\t PowerPC Linux RAMDisk Image (gzip compressed)\n\t   Data Size:\t 566530 Bytes = 553 kB = 0 MB\n\t   Load Address: 00000000\n\t   Entry Point:\t 00000000\n\t   Verifying Checksum ... OK\n\n\t=> bootm 40100000 40200000\n\t## Booting Linux kernel at 40100000 ...\n\t   Image Name:\t 2.2.13 for initrd on TQM850L\n\t   Image Type:\t PowerPC Linux Kernel Image (gzip compressed)\n\t   Data Size:\t 335725 Bytes = 327 kB = 0 MB\n\t   Load Address: 00000000\n\t   Entry Point:\t 0000000c\n\t   Verifying Checksum ... OK\n\t   Uncompressing Kernel Image ... OK\n\t## Loading RAMDisk Image at 40200000 ...\n\t   Image Name:\t Simple Ramdisk Image\n\t   Image Type:\t PowerPC Linux RAMDisk Image (gzip compressed)\n\t   Data Size:\t 566530 Bytes = 553 kB = 0 MB\n\t   Load Address: 00000000\n\t   Entry Point:\t 00000000\n\t   Verifying Checksum ... OK\n\t   Loading Ramdisk ... OK\n\tLinux version 2.2.13 (wd@denx.local.net) (gcc version 2.95.2 19991024 (release)) #1 Wed Jul 19 02:32:08 MEST 2000\n\tBoot arguments: root=/dev/ram\n\ttime_init: decrementer frequency = 187500000/60\n\tCalibrating delay loop... 49.77 BogoMIPS\n\t...\n\tRAMDISK: Compressed image found at block 0\n\tVFS: Mounted root (ext2 filesystem).\n\n\tbash#\n\nBoot Linux and pass a flat device tree:\n-----------\n\nFirst, U-Boot must be compiled with the appropriate defines. See the section\ntitled \"Linux Kernel Interface\" above for a more in depth explanation. The\nfollowing is an example of how to start a kernel and pass an updated\nflat device tree:\n\n=> print oftaddr\noftaddr=0x300000\n=> print oft\noft=oftrees/mpc8540ads.dtb\n=> tftp $oftaddr $oft\nSpeed: 1000, full duplex\nUsing TSEC0 device\nTFTP from server 192.168.1.1; our IP address is 192.168.1.101\nFilename 'oftrees/mpc8540ads.dtb'.\nLoad address: 0x300000\nLoading: #\ndone\nBytes transferred = 4106 (100a hex)\n=> tftp $loadaddr $bootfile\nSpeed: 1000, full duplex\nUsing TSEC0 device\nTFTP from server 192.168.1.1; our IP address is 192.168.1.2\nFilename 'uImage'.\nLoad address: 0x200000\nLoading:############\ndone\nBytes transferred = 1029407 (fb51f hex)\n=> print loadaddr\nloadaddr=200000\n=> print oftaddr\noftaddr=0x300000\n=> bootm $loadaddr - $oftaddr\n## Booting image at 00200000 ...\n   Image Name:\t Linux-2.6.17-dirty\n   Image Type:\t PowerPC Linux Kernel Image (gzip compressed)\n   Data Size:\t 1029343 Bytes = 1005.2 kB\n   Load Address: 00000000\n   Entry Point:\t 00000000\n   Verifying Checksum ... OK\n   Uncompressing Kernel Image ... OK\nBooting using flat device tree at 0x300000\nUsing MPC85xx ADS machine description\nMemory CAM mapping: CAM0=256Mb, CAM1=256Mb, CAM2=0Mb residual: 0Mb\n[snip]\n\n\nMore About U-Boot Image Types:\n------------------------------\n\nU-Boot supports the following image types:\n\n   \"Standalone Programs\" are directly runnable in the environment\n\tprovided by U-Boot; it is expected that (if they behave\n\twell) you can continue to work in U-Boot after return from\n\tthe Standalone Program.\n   \"OS Kernel Images\" are usually images of some Embedded OS which\n\twill take over control completely. Usually these programs\n\twill install their own set of exception handlers, device\n\tdrivers, set up the MMU, etc. - this means, that you cannot\n\texpect to re-enter U-Boot except by resetting the CPU.\n   \"RAMDisk Images\" are more or less just data blocks, and their\n\tparameters (address, size) are passed to an OS kernel that is\n\tbeing started.\n   \"Multi-File Images\" contain several images, typically an OS\n\t(Linux) kernel image and one or more data images like\n\tRAMDisks. This construct is useful for instance when you want\n\tto boot over the network using BOOTP etc., where the boot\n\tserver provides just a single image file, but you want to get\n\tfor instance an OS kernel and a RAMDisk image.\n\n\t\"Multi-File Images\" start with a list of image sizes, each\n\timage size (in bytes) specified by an \"uint32_t\" in network\n\tbyte order. This list is terminated by an \"(uint32_t)0\".\n\tImmediately after the terminating 0 follow the images, one by\n\tone, all aligned on \"uint32_t\" boundaries (size rounded up to\n\ta multiple of 4 bytes).\n\n   \"Firmware Images\" are binary images containing firmware (like\n\tU-Boot or FPGA images) which usually will be programmed to\n\tflash memory.\n\n   \"Script files\" are command sequences that will be executed by\n\tU-Boot's command interpreter; this feature is especially\n\tuseful when you configure U-Boot to use a real shell (hush)\n\tas command interpreter.\n\nBooting the Linux zImage:\n-------------------------\n\nOn some platforms, it's possible to boot Linux zImage. This is done\nusing the \"bootz\" command. The syntax of \"bootz\" command is the same\nas the syntax of \"bootm\" command.\n\nNote, defining the CONFIG_SUPPORT_RAW_INITRD allows user to supply\nkernel with raw initrd images. The syntax is slightly different, the\naddress of the initrd must be augmented by it's size, in the following\nformat: \"<initrd addres>:<initrd size>\".\n\n\nStandalone HOWTO:\n=================\n\nOne of the features of U-Boot is that you can dynamically load and\nrun \"standalone\" applications, which can use some resources of\nU-Boot like console I/O functions or interrupt services.\n\nTwo simple examples are included with the sources:\n\n\"Hello World\" Demo:\n-------------------\n\n'examples/hello_world.c' contains a small \"Hello World\" Demo\napplication; it is automatically compiled when you build U-Boot.\nIt's configured to run at address 0x00040004, so you can play with it\nlike that:\n\n\t=> loads\n\t## Ready for S-Record download ...\n\t~>examples/hello_world.srec\n\t1 2 3 4 5 6 7 8 9 10 11 ...\n\t[file transfer complete]\n\t[connected]\n\t## Start Addr = 0x00040004\n\n\t=> go 40004 Hello World! This is a test.\n\t## Starting application at 0x00040004 ...\n\tHello World\n\targc = 7\n\targv[0] = \"40004\"\n\targv[1] = \"Hello\"\n\targv[2] = \"World!\"\n\targv[3] = \"This\"\n\targv[4] = \"is\"\n\targv[5] = \"a\"\n\targv[6] = \"test.\"\n\targv[7] = \"<NULL>\"\n\tHit any key to exit ...\n\n\t## Application terminated, rc = 0x0\n\nAnother example, which demonstrates how to register a CPM interrupt\nhandler with the U-Boot code, can be found in 'examples/timer.c'.\nHere, a CPM timer is set up to generate an interrupt every second.\nThe interrupt service routine is trivial, just printing a '.'\ncharacter, but this is just a demo program. The application can be\ncontrolled by the following keys:\n\n\t? - print current values og the CPM Timer registers\n\tb - enable interrupts and start timer\n\te - stop timer and disable interrupts\n\tq - quit application\n\n\t=> loads\n\t## Ready for S-Record download ...\n\t~>examples/timer.srec\n\t1 2 3 4 5 6 7 8 9 10 11 ...\n\t[file transfer complete]\n\t[connected]\n\t## Start Addr = 0x00040004\n\n\t=> go 40004\n\t## Starting application at 0x00040004 ...\n\tTIMERS=0xfff00980\n\tUsing timer 1\n\t  tgcr @ 0xfff00980, tmr @ 0xfff00990, trr @ 0xfff00994, tcr @ 0xfff00998, tcn @ 0xfff0099c, ter @ 0xfff009b0\n\nHit 'b':\n\t[q, b, e, ?] Set interval 1000000 us\n\tEnabling timer\nHit '?':\n\t[q, b, e, ?] ........\n\ttgcr=0x1, tmr=0xff1c, trr=0x3d09, tcr=0x0, tcn=0xef6, ter=0x0\nHit '?':\n\t[q, b, e, ?] .\n\ttgcr=0x1, tmr=0xff1c, trr=0x3d09, tcr=0x0, tcn=0x2ad4, ter=0x0\nHit '?':\n\t[q, b, e, ?] .\n\ttgcr=0x1, tmr=0xff1c, trr=0x3d09, tcr=0x0, tcn=0x1efc, ter=0x0\nHit '?':\n\t[q, b, e, ?] .\n\ttgcr=0x1, tmr=0xff1c, trr=0x3d09, tcr=0x0, tcn=0x169d, ter=0x0\nHit 'e':\n\t[q, b, e, ?] ...Stopping timer\nHit 'q':\n\t[q, b, e, ?] ## Application terminated, rc = 0x0\n\n\nImplementation Internals:\n=========================\n\nThe following is not intended to be a complete description of every\nimplementation detail. However, it should help to understand the\ninner workings of U-Boot and make it easier to port it to custom\nhardware.\n\n\nInitial Stack, Global Data:\n---------------------------\n\nThe implementation of U-Boot is complicated by the fact that U-Boot\nstarts running out of ROM (flash memory), usually without access to\nsystem RAM (because the memory controller is not initialized yet).\nThis means that we don't have writable Data or BSS segments, and BSS\nis not initialized as zero. To be able to get a C environment working\nat all, we have to allocate at least a minimal stack. Implementation\noptions for this are defined and restricted by the CPU used: Some CPU\nmodels provide on-chip memory (like the IMMR area on MPC8xx and\nMPC826x processors), on others (parts of) the data cache can be\nlocked as (mis-) used as memory, etc.\n\n\tChris Hallinan posted a good summary of these issues to the\n\tU-Boot mailing list:\n\n\tSubject: RE: [U-Boot-Users] RE: More On Memory Bank x (nothingness)?\n\tFrom: \"Chris Hallinan\" <clh@net1plus.com>\n\tDate: Mon, 10 Feb 2003 16:43:46 -0500 (22:43 MET)\n\t...\n\n\tCorrect me if I'm wrong, folks, but the way I understand it\n\tis this: Using DCACHE as initial RAM for Stack, etc, does not\n\trequire any physical RAM backing up the cache. The cleverness\n\tis that the cache is being used as a temporary supply of\n\tnecessary storage before the SDRAM controller is setup. It's\n\tbeyond the scope of this list to explain the details, but you\n\tcan see how this works by studying the cache architecture and\n\toperation in the architecture and processor-specific manuals.\n\n\tOCM is On Chip Memory, which I believe the 405GP has 4K. It\n\tis another option for the system designer to use as an\n\tinitial stack/RAM area prior to SDRAM being available. Either\n\toption should work for you. Using CS 4 should be fine if your\n\tboard designers haven't used it for something that would\n\tcause you grief during the initial boot! It is frequently not\n\tused.\n\n\tCFG_SYS_INIT_RAM_ADDR should be somewhere that won't interfere\n\twith your processor/board/system design. The default value\n\tyou will find in any recent u-boot distribution in\n\twalnut.h should work for you. I'd set it to a value larger\n\tthan your SDRAM module. If you have a 64MB SDRAM module, set\n\tit above 400_0000. Just make sure your board has no resources\n\tthat are supposed to respond to that address! That code in\n\tstart.S has been around a while and should work as is when\n\tyou get the config right.\n\n\t-Chris Hallinan\n\tDS4.COM, Inc.\n\nIt is essential to remember this, since it has some impact on the C\ncode for the initialization procedures:\n\n* Initialized global data (data segment) is read-only. Do not attempt\n  to write it.\n\n* Do not use any uninitialized global data (or implicitly initialized\n  as zero data - BSS segment) at all - this is undefined, initiali-\n  zation is performed later (when relocating to RAM).\n\n* Stack space is very limited. Avoid big data buffers or things like\n  that.\n\nHaving only the stack as writable memory limits means we cannot use\nnormal global data to share information between the code. But it\nturned out that the implementation of U-Boot can be greatly\nsimplified by making a global data structure (gd_t) available to all\nfunctions. We could pass a pointer to this data as argument to _all_\nfunctions, but this would bloat the code. Instead we use a feature of\nthe GCC compiler (Global Register Variables) to share the data: we\nplace a pointer (gd) to the global data into a register which we\nreserve for this purpose.\n\nWhen choosing a register for such a purpose we are restricted by the\nrelevant  (E)ABI  specifications for the current architecture, and by\nGCC's implementation.\n\nFor PowerPC, the following registers have specific use:\n\tR1:\tstack pointer\n\tR2:\treserved for system use\n\tR3-R4:\tparameter passing and return values\n\tR5-R10: parameter passing\n\tR13:\tsmall data area pointer\n\tR30:\tGOT pointer\n\tR31:\tframe pointer\n\n\t(U-Boot also uses R12 as internal GOT pointer. r12\n\tis a volatile register so r12 needs to be reset when\n\tgoing back and forth between asm and C)\n\n    ==> U-Boot will use R2 to hold a pointer to the global data\n\n    Note: on PPC, we could use a static initializer (since the\n    address of the global data structure is known at compile time),\n    but it turned out that reserving a register results in somewhat\n    smaller code - although the code savings are not that big (on\n    average for all boards 752 bytes for the whole U-Boot image,\n    624 text + 127 data).\n\nOn ARM, the following registers are used:\n\n\tR0:\tfunction argument word/integer result\n\tR1-R3:\tfunction argument word\n\tR9:\tplatform specific\n\tR10:\tstack limit (used only if stack checking is enabled)\n\tR11:\targument (frame) pointer\n\tR12:\ttemporary workspace\n\tR13:\tstack pointer\n\tR14:\tlink register\n\tR15:\tprogram counter\n\n    ==> U-Boot will use R9 to hold a pointer to the global data\n\n    Note: on ARM, only R_ARM_RELATIVE relocations are supported.\n\nOn Nios II, the ABI is documented here:\n\thttps://www.altera.com/literature/hb/nios2/n2cpu_nii51016.pdf\n\n    ==> U-Boot will use gp to hold a pointer to the global data\n\n    Note: on Nios II, we give \"-G0\" option to gcc and don't use gp\n    to access small data sections, so gp is free.\n\nOn RISC-V, the following registers are used:\n\n\tx0: hard-wired zero (zero)\n\tx1: return address (ra)\n\tx2:\tstack pointer (sp)\n\tx3:\tglobal pointer (gp)\n\tx4:\tthread pointer (tp)\n\tx5:\tlink register (t0)\n\tx8:\tframe pointer (fp)\n\tx10-x11:\targuments/return values (a0-1)\n\tx12-x17:\targuments (a2-7)\n\tx28-31:\t temporaries (t3-6)\n\tpc:\tprogram counter (pc)\n\n    ==> U-Boot will use gp to hold a pointer to the global data\n\nSystem Initialization:\n----------------------\n\nIn the reset configuration, U-Boot starts at the reset entry point\n(on most PowerPC systems at address 0x00000100). Because of the reset\nconfiguration for CS0# this is a mirror of the on board Flash memory.\nTo be able to re-map memory U-Boot then jumps to its link address.\nTo be able to implement the initialization code in C, a (small!)\ninitial stack is set up in the internal Dual Ported RAM (in case CPUs\nwhich provide such a feature like), or in a locked part of the data\ncache. After that, U-Boot initializes the CPU core, the caches and\nthe SIU.\n\nNext, all (potentially) available memory banks are mapped using a\npreliminary mapping. For example, we put them on 512 MB boundaries\n(multiples of 0x20000000: SDRAM on 0x00000000 and 0x20000000, Flash\non 0x40000000 and 0x60000000, SRAM on 0x80000000). Then UPM A is\nprogrammed for SDRAM access. Using the temporary configuration, a\nsimple memory test is run that determines the size of the SDRAM\nbanks.\n\nWhen there is more than one SDRAM bank, and the banks are of\ndifferent size, the largest is mapped first. For equal size, the first\nbank (CS2#) is mapped first. The first mapping is always for address\n0x00000000, with any additional banks following immediately to create\ncontiguous memory starting from 0.\n\nThen, the monitor installs itself at the upper end of the SDRAM area\nand allocates memory for use by malloc() and for the global Board\nInfo data; also, the exception vector code is copied to the low RAM\npages, and the final stack is set up.\n\nOnly after this relocation will you have a \"normal\" C environment;\nuntil that you are restricted in several ways, mostly because you are\nrunning from ROM, and because the code will have to be relocated to a\nnew address in RAM.\n\n\nContributing\n============\n\nThe U-Boot projects depends on contributions from the user community.\nIf you want to participate, please, have a look at the 'General'\nsection of https://docs.u-boot.org/en/latest/develop/index.html\nwhere we describe coding standards and the patch submission process.\n",
      "stars_today": 2
    },
    {
      "id": 89033556,
      "name": "firebase-ios-sdk",
      "full_name": "firebase/firebase-ios-sdk",
      "description": "Firebase SDK for Apple App Development",
      "html_url": "https://github.com/firebase/firebase-ios-sdk",
      "stars": 6516,
      "forks": 1718,
      "language": "C++",
      "topics": [
        "ai",
        "analytics",
        "authentication",
        "crash-reporting",
        "database",
        "database-as-a-service",
        "firebase",
        "firebase-auth",
        "firebase-authentication",
        "firebase-database",
        "firebase-messaging",
        "firebase-storage",
        "gemini",
        "ios-sdk",
        "objective-c",
        "push-notifications",
        "storage-service"
      ],
      "created_at": "2017-04-22T00:26:50Z",
      "updated_at": "2026-01-27T23:02:35Z",
      "pushed_at": "2026-01-27T22:33:10Z",
      "open_issues": 429,
      "owner": {
        "login": "firebase",
        "avatar_url": "https://avatars.githubusercontent.com/u/1335026?v=4"
      },
      "readme": "<p align=\"center\">\n  <a href=\"https://cocoapods.org/pods/Firebase\">\n    <img src=\"https://img.shields.io/github/v/release/Firebase/firebase-ios-sdk?style=flat&label=CocoaPods\"/>\n  </a>\n  <a href=\"https://swiftpackageindex.com/firebase/firebase-ios-sdk\">\n    <img src=\"https://img.shields.io/github/v/release/Firebase/firebase-ios-sdk?style=flat&label=Swift%20Package%20Index&color=red\"/>\n  </a>\n  <a href=\"https://cocoapods.org/pods/Firebase\">\n    <img src=\"https://img.shields.io/github/license/Firebase/firebase-ios-sdk?style=flat\"/>\n  </a><br/>\n  <a href=\"https://swiftpackageindex.com/firebase/firebase-ios-sdk\">\n    <img src=\"https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Ffirebase%2Ffirebase-ios-sdk%2Fbadge%3Ftype%3Dplatforms\"/>\n  </a>\n  <a href=\"https://swiftpackageindex.com/firebase/firebase-ios-sdk\">\n    <img src=\"https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Ffirebase%2Ffirebase-ios-sdk%2Fbadge%3Ftype%3Dswift-versions\"/>\n  </a>\n</p>\n\n# Firebase Apple Open Source Development\n\nThis repository contains the source code for all Apple platform Firebase SDKs except FirebaseAnalytics.\n\nFirebase is an app development platform with tools to help you build, grow, and\nmonetize your app. More information about Firebase can be found on the\n[official Firebase website](https://firebase.google.com).\n\n## Installation\n\nSee the subsections below for details about the different installation methods. Where\navailable, it's recommended to install any libraries with a `Swift` suffix to get the\nbest experience when writing your app in Swift.\n\n1. [Standard pod install](#standard-pod-install)\n2. [Swift Package Manager](#swift-package-manager)\n3. [Installing from the GitHub repo](#installing-from-github)\n4. [Experimental Carthage](#carthage-ios-only)\n\n### Standard pod install\n\nFor instructions on the standard pod install, visit:\n[https://firebase.google.com/docs/ios/setup](https://firebase.google.com/docs/ios/setup).\n\n### Swift Package Manager\n\nInstructions for [Swift Package Manager](https://swift.org/package-manager/) support can be\nfound in the [SwiftPackageManager.md](SwiftPackageManager.md) Markdown file.\n\n### Installing from GitHub\n\nThese instructions can be used to access the Firebase repo at other branches,\ntags, or commits.\n\n#### Background\n\nSee [the Podfile Syntax Reference](https://guides.cocoapods.org/syntax/podfile.html#pod)\nfor instructions and options about overriding pod source locations.\n\n#### Accessing Firebase Source Snapshots\n\nAll official releases are tagged in this repo and available via CocoaPods. To access a local\nsource snapshot or unreleased branch, use Podfile directives like the following:\n\nTo access FirebaseFirestore via a branch:\n```ruby\npod 'FirebaseCore', :git => 'https://github.com/firebase/firebase-ios-sdk.git', :branch => 'main'\npod 'FirebaseFirestore', :git => 'https://github.com/firebase/firebase-ios-sdk.git', :branch => 'main'\n```\n\nTo access FirebaseMessaging via a checked-out version of the firebase-ios-sdk repo:\n```ruby\npod 'FirebaseCore', :path => '/path/to/firebase-ios-sdk'\npod 'FirebaseMessaging', :path => '/path/to/firebase-ios-sdk'\n```\n\n### Carthage (iOS only)\n\nInstructions for the experimental Carthage distribution can be found at\n[Carthage.md](Carthage.md).\n\n### Using Firebase from a Framework or a library\n\nFor details on using Firebase from a Framework or a library, refer to [firebase_in_libraries.md](docs/firebase_in_libraries.md).\n\n## Development\n\nTo develop Firebase software in this repository, ensure that you have at least\nthe following software:\n\n* Xcode 16.2 (or later)\n\nCocoaPods is still the canonical way to develop, but much of the repo now supports\ndevelopment with Swift Package Manager.\n\n### CocoaPods\n\nInstall the following:\n* CocoaPods 1.12.0 (or later)\n* [CocoaPods generate](https://github.com/square/cocoapods-generate)\n\nFor the pod that you want to develop:\n\n```ruby\npod gen Firebase{name here}.podspec --local-sources=./ --auto-open --platforms=ios\n```\n\nNote: If the CocoaPods cache is out of date, you may need to run\n`pod repo update` before the `pod gen` command.\n\nNote: Set the `--platforms` option to `macos` or `tvos` to develop/test for\nthose platforms. Since 10.2, Xcode does not properly handle multi-platform\nCocoaPods workspaces.\n\nFirestore has a self-contained Xcode project. See\n[Firestore/README](Firestore/README.md) Markdown file.\n\n#### Development for Catalyst\n* `pod gen {name here}.podspec --local-sources=./ --auto-open --platforms=ios`\n* Check the Mac box in the App-iOS Build Settings\n* Sign the App in the Settings Signing & Capabilities tab\n* Click Pods in the Project Manager\n* Add Signing to the iOS host app and unit test targets\n* Select the Unit-unit scheme\n* Run it to build and test\n\nAlternatively, disable signing in each target:\n* Go to Build Settings tab\n* Click `+`\n* Select `Add User-Defined Setting`\n* Add `CODE_SIGNING_REQUIRED` setting with a value of `NO`\n\n### Swift Package Manager\n* To enable test schemes: `./scripts/setup_spm_tests.sh`\n* `open Package.swift` or double click `Package.swift` in Finder.\n* Xcode will open the project\n  * Choose a scheme for a library to build or test suite to run\n  * Choose a target platform by selecting the run destination along with the scheme\n\n### Adding a New Firebase Pod\n\nRefer to [AddNewPod](docs/AddNewPod.md) Markdown file for details.\n\n### Managing Headers and Imports\n\nFor information about managing headers and imports, see [HeadersImports](HeadersImports.md) Markdown file.\n\n### Code Formatting\n\nTo ensure that the code is formatted consistently, run the script\n[./scripts/check.sh](https://github.com/firebase/firebase-ios-sdk/blob/main/scripts/check.sh)\nbefore creating a pull request (PR).\n\nGitHub Actions will verify that any code changes are done in a style-compliant\nway. Install `clang-format` and `mint`:\n\n```console\nbrew install clang-format@21\nbrew install mint\n```\n\n### Running Unit Tests\n\nSelect a scheme and press Command-u to build a component and run its unit tests.\n\n### Running Sample Apps\nTo run the sample apps and integration tests, you'll need a valid\n`GoogleService-Info.plist\n` file. The Firebase Xcode project contains dummy plist\nfiles without real values, but they can be replaced with real plist files. To get your own\n`GoogleService-Info.plist` files:\n\n1. Go to the [Firebase Console](https://console.firebase.google.com/)\n2. Create a new Firebase project, if you don't already have one\n3. For each sample app you want to test, create a new Firebase app with the sample app's bundle\nidentifier (e.g., `com.google.Database-Example`)\n4. Download the resulting `GoogleService-Info.plist` and add it to the Xcode project.\n\n### Coverage Report Generation\n\nFor coverage report generation instructions, see [scripts/code_coverage_report/README](scripts/code_coverage_report/README.md) Markdown file.\n\n## Specific Component Instructions\nSee the sections below for any special instructions for those components.\n\n### Firebase AI Logic\n\nSee the [Firebase AI Logic README](FirebaseAI#development) for instructions\nabout building and testing the SDK.\n\n### Firebase Auth\n\nFor specific Firebase Auth development, refer to the [Auth Sample README](FirebaseAuth/Tests/Sample/README.md) for instructions about\nbuilding and running the FirebaseAuth pod along with various samples and tests.\n\n### Firebase Database\n\nThe Firebase Database Integration tests can be run against a locally running Database Emulator\nor against a production instance.\n\nTo run against a local emulator instance, invoke `./scripts/run_database_emulator.sh start` before\nrunning the integration test.\n\nTo run against a production instance, provide a valid `GoogleServices-Info.plist` and copy it to\n`FirebaseDatabase/Tests/Resources/GoogleService-Info.plist`. Your Security Rule must be set to\n[public](https://firebase.google.com/docs/database/security/quickstart) while your tests are\nrunning.\n\n### Firebase Dynamic Links\n\nFirebase Dynamic Links is **deprecated** and should not be used in new projects. The service will shut down on August 25, 2025.\n\nPlease see our [Dynamic Links Deprecation FAQ documentation](https://firebase.google.com/support/dynamic-links-faq) for more guidance.\n\n### Firebase Performance Monitoring\n\nFor specific Firebase Performance Monitoring development, see\n[the Performance README](FirebasePerformance/README.md) for instructions about building the SDK\nand [the Performance TestApp README](FirebasePerformance/Tests/TestApp/README.md) for instructions about\nintegrating Performance with the dev test App.\n\n### Firebase Storage\n\nTo run the Storage Integration tests, follow the instructions in\n[StorageIntegration.swift](FirebaseStorage/Tests/Integration/StorageIntegration.swift).\n\n#### Push Notifications\n\nPush notifications can only be delivered to specially provisioned App IDs in the developer portal.\nIn order to test receiving push notifications, you will need to:\n\n1. Change the bundle identifier of the sample app to something you own in your Apple Developer\naccount and enable that App ID for push notifications.\n2. You'll also need to\n[upload your APNs Provider Authentication Key or certificate to the\nFirebase Console](https://firebase.google.com/docs/cloud-messaging/ios/certs)\nat **Project Settings > Cloud Messaging > [Your Firebase App]**.\n3. Ensure your iOS device is added to your Apple Developer portal as a test device.\n\n#### iOS Simulator\n\nThe iOS Simulator cannot register for remote notifications and will not receive push notifications.\nTo receive push notifications, follow the steps above and run the app on a physical device.\n\n## Building with Firebase on Apple platforms\n\nFirebase provides official beta support for macOS, Catalyst, and tvOS. visionOS and watchOS\nare community supported. Thanks to community contributions for many of the multi-platform PRs.\n\nAt this time, most of Firebase's products are available across Apple platforms. There are still\na few gaps, especially on visionOS and watchOS. For details about the current support matrix, see\n[this chart](https://firebase.google.com/docs/ios/learn-more#firebase_library_support_by_platform)\nin Firebase's documentation.\n\n### visionOS\n\nWhere supported, visionOS works as expected with the exception of Firestore via Swift Package\nManager where it is required to use the source distribution.\n\nTo enable the Firestore source distribution, quit Xcode and open the desired\nproject from the command line with the `FIREBASE_SOURCE_FIRESTORE` environment\nvariable: `open --env FIREBASE_SOURCE_FIRESTORE /path/to/project.xcodeproj`.\nTo go back to using the binary distribution of Firestore, quit Xcode and open\nXcode like normal, without the environment variable.\n\n### watchOS\nThanks to contributions from the community, many of Firebase SDKs now compile, run unit tests, and\nwork on watchOS. See the [Independent Watch App Sample](Example/watchOSSample).\n\nKeep in mind that watchOS is not officially supported by Firebase. While we can catch basic unit\ntest issues with GitHub Actions, there may be some changes where the SDK no longer works as expected\non watchOS. If you encounter this, please\n[file an issue](https://github.com/firebase/firebase-ios-sdk/issues).\n\nDuring app setup in the console, you may get to a step that mentions something like \"Checking if the\napp has communicated with our servers\". This relies on Analytics and will not work on watchOS.\n**It's safe to ignore the message and continue**, the rest of the SDKs will work as expected.\n\n#### Additional Crashlytics Notes\n* watchOS has limited support. Due to watchOS restrictions, mach exceptions and signal crashes are\nnot recorded. (Crashes in SwiftUI are generated as mach exceptions, so will not be recorded)\n\n## Combine\nThanks to contributions from the community, _FirebaseCombineSwift_ contains support for Apple's Combine\nframework. This module is currently under development and not yet supported for use in production\nenvironments. For more details, please refer to the [docs](FirebaseCombineSwift/README.md).\n\n## Roadmap\n\nSee [Roadmap](ROADMAP.md) for more about the Firebase Apple SDK Open Source\nplans and directions.\n\n## Contributing\n\nSee [Contributing](CONTRIBUTING.md) for more information on contributing to the Firebase\nApple SDK.\n\n## License\n\nThe contents of this repository are licensed under the\n[Apache License, version 2.0](http://www.apache.org/licenses/LICENSE-2.0).\n\nYour use of Firebase is governed by the\n[Terms of Service for Firebase Services](https://firebase.google.com/terms/).\n",
      "stars_today": 2
    },
    {
      "id": 2019791,
      "name": "liquibase",
      "full_name": "liquibase/liquibase",
      "description": "Main Liquibase Source",
      "html_url": "https://github.com/liquibase/liquibase",
      "stars": 5403,
      "forks": 1938,
      "language": "Java",
      "topics": [
        "continuous-delivery",
        "continuous-deployment",
        "database",
        "database-administration",
        "database-management",
        "database-migrations",
        "db2",
        "devops",
        "hacktoberfest",
        "java",
        "java-library",
        "liquibase",
        "mariadb",
        "mysql",
        "oracle",
        "sql",
        "sqlserver"
      ],
      "created_at": "2011-07-08T20:08:20Z",
      "updated_at": "2026-01-26T22:21:34Z",
      "pushed_at": "2026-01-27T19:27:03Z",
      "open_issues": 686,
      "owner": {
        "login": "liquibase",
        "avatar_url": "https://avatars.githubusercontent.com/u/438548?v=4"
      },
      "readme": "# Liquibase [![Build and Test](https://github.com/liquibase/liquibase/actions/workflows/run-tests.yml/badge.svg)](https://github.com/liquibase/liquibase/actions/workflows/run-tests.yml) [![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=liquibase&metric=alert_status)](https://sonarcloud.io/summary/new_code?id=liquibase)\n<p align=\"center\"><img src=\"https://github.com/liquibase/liquibase/blob/master/Liquibase.png\" width=\"30%\" height=\"30%\"></p>\n\nLiquibase helps millions of developers track, version, and deploy database schema changes. It will help you to:\n- Control database schema changes for specific versions\n- Eliminate errors and delays when releasing databases\n- Automatically order scripts for deployment\n- Easily rollback changes\n- Collaborate with tools you already use\n\nThis repository contains the main source code for Liquibase Community. For more information about the product, see the [Liquibase website](https://www.liquibase.com/).\n\n## Liquibase Automation and Integrations\n\nLiquibase Community has built-in support for a variety of databases. Databases that are not part of Liquibase Community require extensions that you can download for free. Here is the full list of [supported databases](https://www.liquibase.com/supported-databases).\n\nLiquibase can be integrated with Maven, Ant, Gradle, Spring Boot, and other CI/CD tools. For a full list, see [Liquibase Tools & Integrations](https://docs.liquibase.com/tools-integrations/home.html). You can use Liquibase with [GitHub Actions](https://github.com/liquibase/liquibase-github-action-example), [Spinnaker](https://github.com/liquibase/liquibase-spinnaker-plugin), and many different [workflows](https://docs.liquibase.com/workflows/home.html).\n\n\n## Install and Run Liquibase\n\n### System Requirements\nLiquibase system requirements can be found on the [Download Liquibase](https://www.liquibase.com/download) page.\n\n### An H2 in-memory database example for CLI\n1. [Download and run the appropriate installer](https://www.liquibase.com/download). \n2. Make sure to add Liquibase to your PATH.\n3. Copy the included `examples` directory to the needed location.\n4. Open your CLI and navigate to your `examples/sql` or `examples/xml` directory.\n5. Start the included H2 database with the `liquibase init start-h2` command.\n6. Run the `liquibase update` command.\n7. Run the `liquibase history` command to see what has executed!\n\nSee also how to [get started with Liquibase in minutes](https://docs.liquibase.com/start/home.html) or refer to our [Installing Liquibase](https://docs.liquibase.com/start/install/home.html) documentation page for more details.\n\n## Documentation\n\nVisit the [Liquibase Documentation](https://docs.liquibase.com/home.html) website to find the information on how Liquibase works.\n\n## Courses\n\nLearn all about Liquibase by taking our free online courses at [Liquibase University](https://learn.liquibase.com/).\n\n## Want to help?\n\nWant to file a bug or improve documentation? Excellent! Read up on our guidelines for [contributing](https://contribute.liquibase.com/)!\n\n### Contribute code \n\nUse our [step-by-step instructions](https://contribute.liquibase.com/code/) for contributing code to the Liquibase project. \n\n### Join the Liquibase Community\n\nEarn points for your achievements and contributions, collect and show off your badges, add accreditations to your LinkedIn. [Learn more about the pathway to Legend and benefits](https://www.liquibase.com/community/liquibase-legends). Enjoy being part of the community!\n\n## Liquibase Extensions\n\n[Provide more database support and features for Liquibase](https://contribute.liquibase.com/extensions-integrations/directory/).\n\n## License\n\nLiquibase Community is [licensed under the Functional Source License (FSL)](https://fsl.software/FSL-1.1-ALv2.template.md).\n\n[Liquibase Secure](https://www.liquibase.com/liquibase-secure) has additional features and support and is commercially licensed.\n\nLIQUIBASE is a registered trademark of [Liquibase Inc.](https://www.liquibase.com/company)\n\n## [Contact us](https://www.liquibase.com/contact)\n\n[Liquibase Forum](https://forum.liquibase.org/) \n\n[Liquibase Blog](https://www.liquibase.com/blog)\n\n[Get Support & Advanced Features](https://www.liquibase.com/pricing)\n\n## Publish Release Manual Trigger to Sonatype \n\n1. When a PO (Product Owner) or a Team Leader navigates to Publish a release from here -> https://github.com/liquibase/liquibase/releases/, the workflow from /workflow/release-published.yml job is triggered. \n2. When a release is triggered, the workflow file will stop after `Setup` step and an email will be sent out to the list of `approvers` mentioned in job `manual_trigger_deployment`. You can click on the link and perform anyone of the options mentioned in description. \n3. A minimum of 2 approvers are needed in order for the other jobs such as `deploy_maven`, `deploy_javadocs`, `publish_to_github_packages`, etc to be executed.\n4. When you view the GitHub PR, make sure to verify the version which is being published. It should say something like `Deploying v4.20.0 to sonatype`\n\n\n\n",
      "stars_today": 2
    },
    {
      "id": 96496250,
      "name": "swift-snapshot-testing",
      "full_name": "pointfreeco/swift-snapshot-testing",
      "description": "ğŸ“¸ Delightful Swift snapshot testing.",
      "html_url": "https://github.com/pointfreeco/swift-snapshot-testing",
      "stars": 4117,
      "forks": 643,
      "language": "Swift",
      "topics": [
        "screenshot-testing",
        "snapshot-testing",
        "swift",
        "testing"
      ],
      "created_at": "2017-07-07T03:38:51Z",
      "updated_at": "2026-01-27T18:33:18Z",
      "pushed_at": "2025-11-17T17:51:33Z",
      "open_issues": 196,
      "owner": {
        "login": "pointfreeco",
        "avatar_url": "https://avatars.githubusercontent.com/u/29466629?v=4"
      },
      "readme": "# ğŸ“¸ SnapshotTesting\n\n[![CI](https://github.com/pointfreeco/swift-snapshot-testing/workflows/CI/badge.svg)](https://actions-badge.atrox.dev/pointfreeco/swift-snapshot-testing/goto)\n[![Slack](https://img.shields.io/badge/slack-chat-informational.svg?label=Slack&logo=slack)](http://pointfree.co/slack-invite)\n[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fpointfreeco%2Fswift-snapshot-testing%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/pointfreeco/swift-snapshot-testing)\n[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fpointfreeco%2Fswift-snapshot-testing%2Fbadge%3Ftype%3Dplatforms)](https://swiftpackageindex.com/pointfreeco/swift-snapshot-testing)\n\nDelightful Swift snapshot testing.\n\n## Usage\n\nOnce [installed](#installation), _no additional configuration is required_. You can import the\n`SnapshotTesting` module and call the `assertSnapshot` function.\n\n``` swift\nimport SnapshotTesting\nimport Testing\n\n@MainActor\nstruct MyViewControllerTests {\n  @Test func myViewController() {\n    let vc = MyViewController()\n\n    assertSnapshot(of: vc, as: .image)\n  }\n}\n```\n\nWhen an assertion first runs, a snapshot is automatically recorded to disk and the test will fail,\nprinting out the file path of any newly-recorded reference.\n\n> âŒ failed - No reference was found on disk. Automatically recorded snapshot: â€¦\n>\n> open \"â€¦/MyAppTests/\\_\\_Snapshots\\_\\_/MyViewControllerTests/testMyViewController.png\"\n>\n> Re-run \"testMyViewController\" to test against the newly-recorded snapshot.\n\nRepeat test runs will load this reference and compare it with the runtime value. If they don't\nmatch, the test will fail and describe the difference. Failures can be inspected from Xcode's Report\nNavigator or by inspecting the file URLs of the failure.\n\nYou can record a new reference by customizing snapshots inline with the assertion, or using the\n`withSnapshotTesting` tool:\n\n```swift\n// Record just this one snapshot\nassertSnapshot(of: vc, as: .image, record: .all)\n\n// Record all snapshots in a scope:\nwithSnapshotTesting(record: .all) {\n  assertSnapshot(of: vc1, as: .image)\n  assertSnapshot(of: vc2, as: .image)\n  assertSnapshot(of: vc3, as: .image)\n}\n\n// Record all snapshot failures in a Swift Testing suite:\n@Suite(.snapshots(record: .failed))\nstruct FeatureTests {}\n\n// Record all snapshot failures in an 'XCTestCase' subclass:\nclass FeatureTests: XCTestCase {\n  override func invokeTest() {\n    withSnapshotTesting(record: .failed) {\n      super.invokeTest()\n    }\n  }\n}\n```\n\n## Snapshot Anything\n\nWhile most snapshot testing libraries in the Swift community are limited to `UIImage`s of `UIView`s,\nSnapshotTesting can work with _any_ format of _any_ value on _any_ Swift platform!\n\nThe `assertSnapshot` function accepts a value and any snapshot strategy that value supports. This\nmeans that a view or view controller can be tested against an image representation _and_ against a\ntextual representation of its properties and subview hierarchy.\n\n``` swift\nassertSnapshot(of: vc, as: .image)\nassertSnapshot(of: vc, as: .recursiveDescription)\n```\n\nView testing is highly configurable. You can override trait collections (for specific size classes\nand content size categories) and generate device-agnostic snapshots, all from a single simulator.\n\n``` swift\nassertSnapshot(of: vc, as: .image(on: .iPhoneSe))\nassertSnapshot(of: vc, as: .recursiveDescription(on: .iPhoneSe))\n\nassertSnapshot(of: vc, as: .image(on: .iPhoneSe(.landscape)))\nassertSnapshot(of: vc, as: .recursiveDescription(on: .iPhoneSe(.landscape)))\n\nassertSnapshot(of: vc, as: .image(on: .iPhoneX))\nassertSnapshot(of: vc, as: .recursiveDescription(on: .iPhoneX))\n\nassertSnapshot(of: vc, as: .image(on: .iPadMini(.portrait)))\nassertSnapshot(of: vc, as: .recursiveDescription(on: .iPadMini(.portrait)))\n```\n\n> **Warning**\n> Snapshots must be compared using the exact same simulator that originally took the reference to\n> avoid discrepancies between images.\n\nBetter yet, SnapshotTesting isn't limited to views and view controllers! There are a number of\navailable snapshot strategies to choose from.\n\nFor example, you can snapshot test URL requests (_e.g._, those that your API client prepares).\n\n``` swift\nassertSnapshot(of: urlRequest, as: .raw)\n// POST http://localhost:8080/account\n// Cookie: pf_session={\"userId\":\"1\"}\n//\n// email=blob%40pointfree.co&name=Blob\n```\n\nAnd you can snapshot test `Encodable` values against their JSON _and_ property list representations.\n\n``` swift\nassertSnapshot(of: user, as: .json)\n// {\n//   \"bio\" : \"Blobbed around the world.\",\n//   \"id\" : 1,\n//   \"name\" : \"Blobby\"\n// }\n\nassertSnapshot(of: user, as: .plist)\n// <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n// <!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\"\n//  \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n// <plist version=\"1.0\">\n// <dict>\n//   <key>bio</key>\n//   <string>Blobbed around the world.</string>\n//   <key>id</key>\n//   <integer>1</integer>\n//   <key>name</key>\n//   <string>Blobby</string>\n// </dict>\n// </plist>\n```\n\nIn fact, _any_ value can be snapshot-tested by default using its\n[mirror](https://developer.apple.com/documentation/swift/mirror)!\n\n``` swift\nassertSnapshot(of: user, as: .dump)\n// â–¿ User\n//   - bio: \"Blobbed around the world.\"\n//   - id: 1\n//   - name: \"Blobby\"\n```\n\nIf your data can be represented as an image, text, or data, you can write a snapshot test for it!\n\n## Documentation\n\nThe latest documentation is available\n[here](https://swiftpackageindex.com/pointfreeco/swift-snapshot-testing/main/documentation/snapshottesting).\n\n## Installation\n\n### Xcode\n\n> **Warning**\n> By default, Xcode will try to add the SnapshotTesting package to your project's main\n> application/framework target. Please ensure that SnapshotTesting is added to a _test_ target\n> instead, as documented in the last step, below.\n\n 1. From the **File** menu, navigate through **Swift Packages** and select\n    **Add Package Dependencyâ€¦**.\n 2. Enter package repository URL: `https://github.com/pointfreeco/swift-snapshot-testing`.\n 3. Confirm the version and let Xcode resolve the package.\n 4. On the final dialog, update SnapshotTesting's **Add to Target** column to a test target that\n    will contain snapshot tests (if you have more than one test target, you can later add\n    SnapshotTesting to them by manually linking the library in its build phase).\n\n### Swift Package Manager\n\nIf you want to use SnapshotTesting in any other project that uses\n[SwiftPM](https://swift.org/package-manager/), add the package as a dependency in `Package.swift`:\n\n```swift\ndependencies: [\n  .package(\n    url: \"https://github.com/pointfreeco/swift-snapshot-testing\",\n    from: \"1.12.0\"\n  ),\n]\n```\n\nNext, add `SnapshotTesting` as a dependency of your test target:\n\n```swift\ntargets: [\n  .target(name: \"MyApp\"),\n  .testTarget(\n    name: \"MyAppTests\",\n    dependencies: [\n      \"MyApp\",\n      .product(name: \"SnapshotTesting\", package: \"swift-snapshot-testing\"),\n    ]\n  )\n]\n```\n\n## Features\n\n  - [**Dozens of snapshot strategies**][available-strategies]. Snapshot\n    testing isn't just for `UIView`s and `CALayer`s. Write snapshots against _any_ value.\n  - [**Write your own snapshot strategies**][defining-strategies].\n    If you can convert it to an image, string, data, or your own diffable format, you can snapshot\n    test it! Build your own snapshot strategies from scratch or transform existing ones.\n  - **No configuration required.** Don't fuss with scheme settings and environment variables.\n    Snapshots are automatically saved alongside your tests.\n  - **More hands-off.** New snapshots are recorded whether `isRecording` mode is `true` or not.\n  - **Subclass-free.** Assert from any XCTest case or Quick spec.\n  - **Device-agnostic snapshots.** Render views and view controllers for specific devices and trait\n    collections from a single simulator.\n  - **First-class Xcode support.** Image differences are captured as XCTest attachments. Text\n    differences are rendered in inline error messages.\n  - **Supports any platform that supports Swift.** Write snapshot tests for iOS, Linux, macOS, and\n    tvOS.\n  - **SceneKit, SpriteKit, and WebKit support.** Most snapshot testing libraries don't support these\n    view subclasses.\n  - **`Codable` support**. Snapshot encodable data structures into their JSON and property list\n    representations.\n  - **Custom diff tool integration**. Configure failure messages to print diff commands for\n    [Kaleidoscope](https://kaleidoscope.app) or your diff tool of choice.\n    ``` swift\n    SnapshotTesting.diffToolCommand = { \"ksdiff \\($0) \\($1)\" }\n    ```\n\n[available-strategies]: https://swiftpackageindex.com/pointfreeco/swift-snapshot-testing/main/documentation/snapshottesting/snapshotting\n[defining-strategies]: https://swiftpackageindex.com/pointfreeco/swift-snapshot-testing/main/documentation/snapshottesting/customstrategies\n\n## Plug-ins\n\n  - [AccessibilitySnapshot](https://github.com/cashapp/AccessibilitySnapshot) adds easy regression\n    testing for iOS accessibility.\n    \n  - [AccessibilitySnapshotColorBlindness](https://github.com/Sherlouk/AccessibilitySnapshotColorBlindness)\n    adds snapshot strategies for color blindness simulation on iOS views, view controllers and images.\n\n  - [GRDBSnapshotTesting](https://github.com/SebastianOsinski/GRDBSnapshotTesting) adds snapshot\n    strategy for testing SQLite database migrations made with [GRDB](https://github.com/groue/GRDB.swift).\n\n  - [Nimble-SnapshotTesting](https://github.com/tahirmt/Nimble-SnapshotTesting) adds \n    [Nimble](https://github.com/Quick/Nimble) matchers for SnapshotTesting to be used by Swift\n    Package Manager.\n\n  - [Prefire](https://github.com/BarredEwe/Prefire) generating Snapshot Tests via\n    [Swift Package Plugins](https://github.com/apple/swift-package-manager/blob/main/Documentation/Plugins.md)\n    using SwiftUI `Preview`\n  \n  - [PreviewSnapshots](https://github.com/doordash-oss/swiftui-preview-snapshots) share `View`\n    configurations between SwiftUI Previews and snapshot tests and generate several snapshots with a\n    single test assertion.\n\n  - [swift-html](https://github.com/pointfreeco/swift-html) is a Swift DSL for type-safe,\n    extensible, and transformable HTML documents and includes an `HtmlSnapshotTesting` module to\n    snapshot test its HTML documents.\n\n  - [swift-snapshot-testing-nimble](https://github.com/Killectro/swift-snapshot-testing-nimble) adds\n    [Nimble](https://github.com/Quick/Nimble) matchers for SnapshotTesting.\n\n  - [swift-snapshot-testing-stitch](https://github.com/Sherlouk/swift-snapshot-testing-stitch/) adds\n    the ability to stitch multiple UIView's or UIViewController's together in a single test.\n\n  - [SnapshotTestingDump](https://github.com/tahirmt/swift-snapshot-testing-dump) Adds support to\n    use [swift-custom-dump](https://github.com/pointfreeco/swift-custom-dump/) by using `customDump`\n    strategy for `Any`\n\n  - [SnapshotTestingHEIC](https://github.com/alexey1312/SnapshotTestingHEIC) adds image support\n  using the HEIC storage format which reduces file sizes in comparison to PNG.\n\n  - [SnapshotVision](https://github.com/gregersson/swift-snapshot-testing-vision) adds snapshot\n    strategy for text recognition on views and images. Uses Apples Vision framework.\n\nHave you written your own SnapshotTesting plug-in?\n[Add it here](https://github.com/pointfreeco/swift-snapshot-testing/edit/master/README.md) and\nsubmit a pull request!\n\n## Related Tools\n\n  - [`iOSSnapshotTestCase`](https://github.com/uber/ios-snapshot-test-case/) helped introduce screen\n    shot testing to a broad audience in the iOS community. Experience with it inspired the creation\n    of this library.\n\n  - [Jest](https://jestjs.io) brought generalized snapshot testing to the JavaScript community with\n    a polished user experience. Several features of this library (diffing, automatically capturing\n    new snapshots) were directly influenced.\n\n## Learn More\n\nSnapshotTesting was designed with [witness-oriented programming](https://www.pointfree.co/episodes/ep39-witness-oriented-library-design).\n\nThis concept (and more) are explored thoroughly in a series of episodes on\n[Point-Free](https://www.pointfree.co), a video series exploring functional programming and Swift\nhosted by [Brandon Williams](https://twitter.com/mbrandonw) and\n[Stephen Celis](https://twitter.com/stephencelis).\n\nWitness-oriented programming and the design of this library was explored in the following\n[Point-Free](https://www.pointfree.co) episodes:\n\n  - [Episode 33](https://www.pointfree.co/episodes/ep33-protocol-witnesses-part-1): Protocol Witnesses: Part 1\n  - [Episode 34](https://www.pointfree.co/episodes/ep34-protocol-witnesses-part-1): Protocol Witnesses: Part 2\n  - [Episode 35](https://www.pointfree.co/episodes/ep35-advanced-protocol-witnesses-part-1): Advanced Protocol Witnesses: Part 1\n  - [Episode 36](https://www.pointfree.co/episodes/ep36-advanced-protocol-witnesses-part-2): Advanced Protocol Witnesses: Part 2\n  - [Episode 37](https://www.pointfree.co/episodes/ep37-protocol-oriented-library-design-part-1): Protocol-Oriented Library Design: Part 1\n  - [Episode 38](https://www.pointfree.co/episodes/ep38-protocol-oriented-library-design-part-2): Protocol-Oriented Library Design: Part 2\n  - [Episode 39](https://www.pointfree.co/episodes/ep39-witness-oriented-library-design): Witness-Oriented Library Design\n  - [Episode 40](https://www.pointfree.co/episodes/ep40-async-functional-refactoring): Async Functional Refactoring\n  - [Episode 41](https://www.pointfree.co/episodes/ep41-a-tour-of-snapshot-testing): A Tour of Snapshot Testing ğŸ†“\n\n<a href=\"https://www.pointfree.co/episodes/ep41-a-tour-of-snapshot-testing\">\n  <img alt=\"video poster image\" src=\"https://d3rccdn33rt8ze.cloudfront.net/episodes/0041.jpeg\" width=\"480\">\n</a>\n\n## License\n\nThis library is released under the MIT license. See [LICENSE](LICENSE) for details.\n",
      "stars_today": 2
    },
    {
      "id": 30017750,
      "name": "ComplexHeatmap",
      "full_name": "jokergoo/ComplexHeatmap",
      "description": "Make Complex Heatmaps ",
      "html_url": "https://github.com/jokergoo/ComplexHeatmap",
      "stars": 1462,
      "forks": 242,
      "language": "R",
      "topics": [
        "clustering",
        "complex-heatmaps",
        "heatmap"
      ],
      "created_at": "2015-01-29T11:45:58Z",
      "updated_at": "2026-01-27T11:11:38Z",
      "pushed_at": "2025-06-23T15:51:50Z",
      "open_issues": 227,
      "owner": {
        "login": "jokergoo",
        "avatar_url": "https://avatars.githubusercontent.com/u/449218?v=4"
      },
      "readme": "# Make Complex Heatmaps <a href=\"https://jokergoo.github.io/ComplexHeatmap-reference/book/\"><img src=\"https://jokergoo.github.io/ComplexHeatmap-reference/book/complexheatmap-cover.jpg\" width=240 align=\"right\" style=\"border:2px solid black;\" ></a>\n\n[![R-CMD-check](https://github.com/jokergoo/ComplexHeatmap/workflows/R-CMD-check/badge.svg)](https://github.com/jokergoo/ComplexHeatmap/actions)\n[![codecov](https://img.shields.io/codecov/c/github/jokergoo/ComplexHeatmap.svg)](https://codecov.io/github/jokergoo/ComplexHeatmap) \n[![bioc](http://www.bioconductor.org/shields/downloads/devel/ComplexHeatmap.svg)](https://bioconductor.org/packages/stats/bioc/ComplexHeatmap/) \n[![bioc](http://www.bioconductor.org/shields/years-in-bioc/ComplexHeatmap.svg)](http://bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html)\n\n<img src=\"http://jokergoo.github.io/complexheatmap_logo.svg\" width=\"550\">\n\n\nComplex heatmaps are efficient to visualize associations between different\nsources of data sets and reveal potential patterns. Here the\n**ComplexHeatmap** package provides a highly flexible way to arrange multiple\nheatmaps and supports various annotation graphics.\n\nThe [**InteractiveComplexHeatmap**](https://github.com/jokergoo/InteractiveComplexHeatmap) package can directly export static complex heatmaps into an interactive Shiny app. Have a try!\n\n## Citation\n\nZuguang Gu, et al., [Complex heatmaps reveal patterns and correlations in multidimensional genomic data](http://bioinformatics.oxfordjournals.org/content/early/2016/05/20/bioinformatics.btw313.abstract), Bioinformatics, 2016.\n\nZuguang Gu. [Complex Heatmap Visualization](https://doi.org/10.1002/imt2.43), iMeta, 2022. \n\n\n## Install\n\n`ComplexHeatmap` is available on [Bioconductor](http://www.bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html), you can install it by:\n\n```r\nif (!requireNamespace(\"BiocManager\", quietly=TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"ComplexHeatmap\")\n```\n\nIf you want the latest version, install it directly from GitHub:\n\n```r\nlibrary(devtools)\ninstall_github(\"jokergoo/ComplexHeatmap\")\n```\n\n## Usage\n\nMake a single heatmap:\n\n```r\nHeatmap(mat, ...)\n```\n\nA single Heatmap with column annotations:\n\n```r\nha = HeatmapAnnotation(df = anno1, anno_fun = anno2, ...)\nHeatmap(mat, ..., top_annotation = ha)\n```\n\nMake a list of heatmaps:\n\n```r\nHeatmap(mat1, ...) + Heatmap(mat2, ...)\n```\n\nMake a list of heatmaps and row annotations:\n\n```r\nha = HeatmapAnnotation(df = anno1, anno_fun = anno2, ..., which = \"row\")\nHeatmap(mat1, ...) + Heatmap(mat2, ...) + ha\n```\n\n## Documentation\n\nThe full documentations are available at https://jokergoo.github.io/ComplexHeatmap-reference/book/ and the website is at https://jokergoo.github.io/ComplexHeatmap.\n\n## Blog posts\n\nThere are following blog posts focusing on specific topics:\n\n- [Make 3D heatmap](https://jokergoo.github.io/2021/03/24/3d-heatmap/)\n- [Translate from pheatmap to ComplexHeatmap](https://jokergoo.github.io/2020/05/06/translate-from-pheatmap-to-complexheatmap/)\n- [Set cell width/height in the heatmap](https://jokergoo.github.io/2020/05/11/set-cell-width/height-in-the-heatmap/)\n- [Interactive ComplexHeatmap](https://jokergoo.github.io/2020/05/15/interactive-complexheatmap/)\n- [Word cloud as heatmap annotation](https://jokergoo.github.io/2020/05/31/word-cloud-as-heatmap-annotation/)\n- [Which heatmap function is faster?](https://jokergoo.github.io/2020/06/19/which-heatmap-function-is-faster/)\n- [Rasterization in ComplexHeatmap](https://jokergoo.github.io/2020/06/30/rasterization-in-complexheatmap/)\n- [Block annotation over several slices](https://jokergoo.github.io/2020/07/06/block-annotation-over-several-slices/)\n- [Integrate ComplexHeatmap with cowplot package](https://jokergoo.github.io/2020/07/14/integrate-complexheatmap-with-cowplot-package/)\n\n\n## Examples\n\n### Visualize Methylation Profile with Complex Annotations\n\n![complexheatmap_example4](https://user-images.githubusercontent.com/449218/47718635-2ec22980-dc49-11e8-9f01-37becb19e0d5.png)\n\n### Correlations between methylation, expression and other genomic features\n\n![complexheatmap_example3](https://user-images.githubusercontent.com/449218/47718636-2ec22980-dc49-11e8-8db0-1659c27dcf40.png)\n\n### Visualize Cell Heterogeneity from Single Cell RNASeq\n\n![complexheatmap_example2](https://user-images.githubusercontent.com/449218/47718637-2ec22980-dc49-11e8-925e-955c16cfa982.png)\n\n### Making Enhanced OncoPrint\n\n![complexheatmap_example1](https://user-images.githubusercontent.com/449218/47718638-2ec22980-dc49-11e8-845e-21e51d3b8e73.png)\n\n### UpSet plot\n\n<img src=\"https://user-images.githubusercontent.com/449218/102615477-48c76a80-4136-11eb-98d9-3c528844fbe8.png\" width=500 />\n\n### 3D heatmap\n\n![image](https://user-images.githubusercontent.com/449218/112284448-8c77c600-8c89-11eb-8d38-c5538900df20.png)\n\n\n\n## License\n\nMIT @ Zuguang Gu\n\n",
      "stars_today": 2
    },
    {
      "id": 346465436,
      "name": "vineflower",
      "full_name": "Vineflower/vineflower",
      "description": "Modern Java decompiler aiming to be as accurate as possible, with an emphasis on output quality. Fork of the Fernflower decompiler.",
      "html_url": "https://github.com/Vineflower/vineflower",
      "stars": 1940,
      "forks": 122,
      "language": "Java",
      "topics": [
        "decompiler",
        "fernflower",
        "hacktoberfest",
        "java"
      ],
      "created_at": "2021-03-10T19:15:51Z",
      "updated_at": "2026-01-27T20:30:38Z",
      "pushed_at": "2026-01-19T05:47:07Z",
      "open_issues": 86,
      "owner": {
        "login": "Vineflower",
        "avatar_url": "https://avatars.githubusercontent.com/u/138941471?v=4"
      },
      "readme": "# Vineflower\n\nVineflower is a modern general purpose Java & JVM language decompiler focused on providing the best quality, speed, and usability.\n\nVineflower's features include:\n- Java 21+ support, including records, sealed classes, switch expressions, pattern matching, and more\n- Clean code generation and output, with automatic output formatting\n- Multithreaded decompilation\n\nExamples of Vineflower's output compared to other decompilers can be found on [the website](https://vineflower.org/output-comparison/).\n\n## Use\n\nVineflower can be used from the console or as a library. To run Vineflower from the command line, download the latest release from the [Releases tab](https://github.com/Vineflower/vineflower/releases).\nYou can then run Vineflower with `java -jar vineflower.jar <arguments> <source> <destination>`.\n`<arguments>` is the list of [commandline arguments](https://vineflower.org/usage/) that you want to pass to the decompiler.\n`<source>` can be a jar, zip, folder, or class file, and `<destination>` can be a folder, zip, jar, or omitted to print to the console.\n\nTo use Vineflower as a library, you can find distributions on maven central. Vineflower 1.9+ requires Java 11 or higher to run, and Vineflower 1.11+ requires Java 17 or higher to run.\nVineflower can be addded as a dependency in gradle with:\n```groovy\ndependencies {\n    implementation 'org.vineflower:vineflower:<version>'\n}\n```\n\nMore instructions on how to interface with Vineflower can be found on [the website](https://vineflower.org/usage-code/).\n\nFor IDE use, the [Vineflower Intellij IDEA plugin](https://plugins.jetbrains.com/plugin/18032-quiltflower) replaces Fernflower in IDEA with Vineflower.\n\nPlease report any issues to the [Issues tab!](https://github.com/Vineflower/vineflower/issues)\n\n### Building\nVineflower can be built simply with `./gradlew build`.\n\n### Support\nFor support or questions, please join one of the listed [social platforms](https://vineflower.org/socials/).\n\n## Contributing\nContributions are always welcome! [The website](https://vineflower.org/development/) has detailed instructions on how to set up Vineflower development, as well as information on debugging.\nWhen submitting pull requests, please target the latest `develop/1.xx.y` branch.\n\n### Special Thanks\nVineflower is a fork of Jetbrains' Fernflower, MinecraftForge's ForgeFlower, FabricMC's fork of Fernflower, and a direct continuation of work on Quiltflower. Special thanks to:\n\n* [Stiver](https://blog.jetbrains.com/idea/2024/11/in-memory-of-stiver/), for creating Fernflower\n* JetBrains, for maintaining Fernflower\n* MinecraftForge Team, for maintaining ForgeFlower\n* FabricMC Team, for maintaining Fabric's fork of Fernflower\n* CFR, for its large suite of very useful tests",
      "stars_today": 2
    },
    {
      "id": 881501535,
      "name": "openai-java",
      "full_name": "openai/openai-java",
      "description": "The official Java library for the OpenAI API",
      "html_url": "https://github.com/openai/openai-java",
      "stars": 1322,
      "forks": 196,
      "language": "Kotlin",
      "topics": [],
      "created_at": "2024-10-31T17:42:57Z",
      "updated_at": "2026-01-28T01:23:43Z",
      "pushed_at": "2026-01-27T23:22:04Z",
      "open_issues": 51,
      "owner": {
        "login": "openai",
        "avatar_url": "https://avatars.githubusercontent.com/u/14957082?v=4"
      },
      "readme": "# OpenAI Java API Library\n\n<!-- x-release-please-start-version -->\n\n[![Maven Central](https://img.shields.io/maven-central/v/com.openai/openai-java)](https://central.sonatype.com/artifact/com.openai/openai-java/4.16.1)\n[![javadoc](https://javadoc.io/badge2/com.openai/openai-java/4.16.1/javadoc.svg)](https://javadoc.io/doc/com.openai/openai-java/4.16.1)\n\n<!-- x-release-please-end -->\n\nThe OpenAI Java SDK provides convenient access to the [OpenAI REST API](https://platform.openai.com/docs) from applications written in Java.\n\n<!-- x-release-please-start-version -->\n\nThe REST API documentation can be found on [platform.openai.com](https://platform.openai.com/docs). Javadocs are available on [javadoc.io](https://javadoc.io/doc/com.openai/openai-java/4.16.1).\n\n<!-- x-release-please-end -->\n\n## Installation\n\n<!-- x-release-please-start-version -->\n\n[_Try `openai-java-spring-boot-starter` if you're using Spring Boot!_](#spring-boot)\n\n### Gradle\n\n```kotlin\nimplementation(\"com.openai:openai-java:4.16.1\")\n```\n\n### Maven\n\n```xml\n<dependency>\n  <groupId>com.openai</groupId>\n  <artifactId>openai-java</artifactId>\n  <version>4.16.1</version>\n</dependency>\n```\n\n<!-- x-release-please-end -->\n\n## Requirements\n\nThis library requires Java 8 or later.\n\n## Usage\n\n> [!TIP]\n> See the [`openai-java-example`](openai-java-example/src/main/java/com/openai/example) directory for complete and runnable examples!\n\nThe primary API for interacting with OpenAI models is the [Responses API](https://platform.openai.com/docs/api-reference/responses). You can generate text from the model with the code below.\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport com.openai.models.ChatModel;\nimport com.openai.models.responses.Response;\nimport com.openai.models.responses.ResponseCreateParams;\n\n// Configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID` and `OPENAI_PROJECT_ID` environment variables\nOpenAIClient client = OpenAIOkHttpClient.fromEnv();\n\nResponseCreateParams params = ResponseCreateParams.builder()\n        .input(\"Say this is a test\")\n        .model(ChatModel.GPT_4_1)\n        .build();\nResponse response = client.responses().create(params);\n```\n\nThe previous standard (supported indefinitely) for generating text is the [Chat Completions API](https://platform.openai.com/docs/api-reference/chat). You can use that API to generate text from the model with the code below.\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport com.openai.models.ChatModel;\nimport com.openai.models.chat.completions.ChatCompletion;\nimport com.openai.models.chat.completions.ChatCompletionCreateParams;\n\n// Configures using the `openai.apiKey`, `openai.orgId`, `openai.projectId`, `openai.webhookSecret` and `openai.baseUrl` system properties\n// Or configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables\nOpenAIClient client = OpenAIOkHttpClient.fromEnv();\n\nChatCompletionCreateParams params = ChatCompletionCreateParams.builder()\n    .addUserMessage(\"Say this is a test\")\n    .model(ChatModel.GPT_4O)\n    .build();\nChatCompletion chatCompletion = client.chat().completions().create(params);\n```\n\n## Client configuration\n\nConfigure the client using system properties or environment variables:\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\n\n// Configures using the `openai.apiKey`, `openai.orgId`, `openai.projectId`, `openai.webhookSecret` and `openai.baseUrl` system properties\n// Or configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables\nOpenAIClient client = OpenAIOkHttpClient.fromEnv();\n```\n\nOr manually:\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\n\nOpenAIClient client = OpenAIOkHttpClient.builder()\n    .apiKey(\"My API Key\")\n    .build();\n```\n\nOr using a combination of the two approaches:\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\n\nOpenAIClient client = OpenAIOkHttpClient.builder()\n    // Configures using the `openai.apiKey`, `openai.orgId`, `openai.projectId`, `openai.webhookSecret` and `openai.baseUrl` system properties\n    // Or configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables\n    .fromEnv()\n    .apiKey(\"My API Key\")\n    .build();\n```\n\nSee this table for the available options:\n\n| Setter          | System property        | Environment variable    | Required | Default value                 |\n| --------------- | ---------------------- | ----------------------- | -------- | ----------------------------- |\n| `apiKey`        | `openai.apiKey`        | `OPENAI_API_KEY`        | true     | -                             |\n| `organization`  | `openai.orgId`         | `OPENAI_ORG_ID`         | false    | -                             |\n| `project`       | `openai.projectId`     | `OPENAI_PROJECT_ID`     | false    | -                             |\n| `webhookSecret` | `openai.webhookSecret` | `OPENAI_WEBHOOK_SECRET` | false    | -                             |\n| `baseUrl`       | `openai.baseUrl`       | `OPENAI_BASE_URL`       | true     | `\"https://api.openai.com/v1\"` |\n\nSystem properties take precedence over environment variables.\n\n> [!TIP]\n> Don't create more than one client in the same application. Each client has a connection pool and\n> thread pools, which are more efficient to share between requests.\n\n### Modifying configuration\n\nTo temporarily use a modified client configuration, while reusing the same connection and thread pools, call `withOptions()` on any client or service:\n\n```java\nimport com.openai.client.OpenAIClient;\n\nOpenAIClient clientWithOptions = client.withOptions(optionsBuilder -> {\n    optionsBuilder.baseUrl(\"https://example.com\");\n    optionsBuilder.maxRetries(42);\n});\n```\n\nThe `withOptions()` method does not affect the original client or service.\n\n## Requests and responses\n\nTo send a request to the OpenAI API, build an instance of some `Params` class and pass it to the corresponding client method. When the response is received, it will be deserialized into an instance of a Java class.\n\nFor example, `client.chat().completions().create(...)` should be called with an instance of `ChatCompletionCreateParams`, and it will return an instance of `ChatCompletion`.\n\n## Immutability\n\nEach class in the SDK has an associated [builder](https://blogs.oracle.com/javamagazine/post/exploring-joshua-blochs-builder-design-pattern-in-java) or factory method for constructing it.\n\nEach class is [immutable](https://docs.oracle.com/javase/tutorial/essential/concurrency/immutable.html) once constructed. If the class has an associated builder, then it has a `toBuilder()` method, which can be used to convert it back to a builder for making a modified copy.\n\nBecause each class is immutable, builder modification will _never_ affect already built class instances.\n\n## Asynchronous execution\n\nThe default client is synchronous. To switch to asynchronous execution, call the `async()` method:\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport com.openai.models.ChatModel;\nimport com.openai.models.chat.completions.ChatCompletion;\nimport com.openai.models.chat.completions.ChatCompletionCreateParams;\nimport java.util.concurrent.CompletableFuture;\n\n// Configures using the `openai.apiKey`, `openai.orgId`, `openai.projectId`, `openai.webhookSecret` and `openai.baseUrl` system properties\n// Or configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables\nOpenAIClient client = OpenAIOkHttpClient.fromEnv();\n\nChatCompletionCreateParams params = ChatCompletionCreateParams.builder()\n    .addUserMessage(\"Say this is a test\")\n    .model(ChatModel.GPT_4O)\n    .build();\nCompletableFuture<ChatCompletion> chatCompletion = client.async().chat().completions().create(params);\n```\n\nOr create an asynchronous client from the beginning:\n\n```java\nimport com.openai.client.OpenAIClientAsync;\nimport com.openai.client.okhttp.OpenAIOkHttpClientAsync;\nimport com.openai.models.ChatModel;\nimport com.openai.models.chat.completions.ChatCompletion;\nimport com.openai.models.chat.completions.ChatCompletionCreateParams;\nimport java.util.concurrent.CompletableFuture;\n\n// Configures using the `openai.apiKey`, `openai.orgId`, `openai.projectId`, `openai.webhookSecret` and `openai.baseUrl` system properties\n// Or configures using the `OPENAI_API_KEY`, `OPENAI_ORG_ID`, `OPENAI_PROJECT_ID`, `OPENAI_WEBHOOK_SECRET` and `OPENAI_BASE_URL` environment variables\nOpenAIClientAsync client = OpenAIOkHttpClientAsync.fromEnv();\n\nChatCompletionCreateParams params = ChatCompletionCreateParams.builder()\n    .addUserMessage(\"Say this is a test\")\n    .model(ChatModel.GPT_4O)\n    .build();\nCompletableFuture<ChatCompletion> chatCompletion = client.chat().completions().create(params);\n```\n\nThe asynchronous client supports the same options as the synchronous one, except most methods return `CompletableFuture`s.\n\n## Streaming\n\nThe SDK defines methods that return response \"chunk\" streams, where each chunk can be individually processed as soon as it arrives instead of waiting on the full response. Streaming methods generally correspond to [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events) or [JSONL](https://jsonlines.org) responses.\n\nSome of these methods may have streaming and non-streaming variants, but a streaming method will always have a `Streaming` suffix in its name, even if it doesn't have a non-streaming variant.\n\nThese streaming methods return [`StreamResponse`](openai-java-core/src/main/kotlin/com/openai/core/http/StreamResponse.kt) for synchronous clients:\n\n```java\nimport com.openai.core.http.StreamResponse;\nimport com.openai.models.chat.completions.ChatCompletionChunk;\n\ntry (StreamResponse<ChatCompletionChunk> streamResponse = client.chat().completions().createStreaming(params)) {\n    streamResponse.stream().forEach(chunk -> {\n        System.out.println(chunk);\n    });\n    System.out.println(\"No more chunks!\");\n}\n```\n\nOr [`AsyncStreamResponse`](openai-java-core/src/main/kotlin/com/openai/core/http/AsyncStreamResponse.kt) for asynchronous clients:\n\n```java\nimport com.openai.core.http.AsyncStreamResponse;\nimport com.openai.models.chat.completions.ChatCompletionChunk;\nimport java.util.Optional;\n\nclient.async().chat().completions().createStreaming(params).subscribe(chunk -> {\n    System.out.println(chunk);\n});\n\n// If you need to handle errors or completion of the stream\nclient.async().chat().completions().createStreaming(params).subscribe(new AsyncStreamResponse.Handler<>() {\n    @Override\n    public void onNext(ChatCompletionChunk chunk) {\n        System.out.println(chunk);\n    }\n\n    @Override\n    public void onComplete(Optional<Throwable> error) {\n        if (error.isPresent()) {\n            System.out.println(\"Something went wrong!\");\n            throw new RuntimeException(error.get());\n        } else {\n            System.out.println(\"No more chunks!\");\n        }\n    }\n});\n\n// Or use futures\nclient.async().chat().completions().createStreaming(params)\n    .subscribe(chunk -> {\n        System.out.println(chunk);\n    })\n    .onCompleteFuture();\n    .whenComplete((unused, error) -> {\n        if (error != null) {\n            System.out.println(\"Something went wrong!\");\n            throw new RuntimeException(error);\n        } else {\n            System.out.println(\"No more chunks!\");\n        }\n    });\n```\n\nAsync streaming uses a dedicated per-client cached thread pool [`Executor`](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Executor.html) to stream without blocking the current thread. This default is suitable for most purposes.\n\nTo use a different `Executor`, configure the subscription using the `executor` parameter:\n\n```java\nimport java.util.concurrent.Executor;\nimport java.util.concurrent.Executors;\n\nExecutor executor = Executors.newFixedThreadPool(4);\nclient.async().chat().completions().createStreaming(params).subscribe(\n    chunk -> System.out.println(chunk), executor\n);\n```\n\nOr configure the client globally using the `streamHandlerExecutor` method:\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport java.util.concurrent.Executors;\n\nOpenAIClient client = OpenAIOkHttpClient.builder()\n    .fromEnv()\n    .streamHandlerExecutor(Executors.newFixedThreadPool(4))\n    .build();\n```\n\n### Streaming helpers\n\nThe SDK provides conveniences for streamed chat completions. A\n[`ChatCompletionAccumulator`](openai-java-core/src/main/kotlin/com/openai/helpers/ChatCompletionAccumulator.kt)\ncan record the stream of chat completion chunks in the response as they are processed and accumulate\na [`ChatCompletion`](openai-java-core/src/main/kotlin/com/openai/models/chat/completions/ChatCompletion.kt)\nobject similar to that which would have been returned by the non-streaming API.\n\nFor a synchronous response add a\n[`Stream.peek()`](https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html#peek-java.util.function.Consumer-)\ncall to the stream pipeline to accumulate each chunk:\n\n```java\nimport com.openai.core.http.StreamResponse;\nimport com.openai.helpers.ChatCompletionAccumulator;\nimport com.openai.models.chat.completions.ChatCompletion;\nimport com.openai.models.chat.completions.ChatCompletionChunk;\n\nChatCompletionAccumulator chatCompletionAccumulator = ChatCompletionAccumulator.create();\n\ntry (StreamResponse<ChatCompletionChunk> streamResponse =\n        client.chat().completions().createStreaming(createParams)) {\n    streamResponse.stream()\n            .peek(chatCompletionAccumulator::accumulate)\n            .flatMap(completion -> completion.choices().stream())\n            .flatMap(choice -> choice.delta().content().stream())\n            .forEach(System.out::print);\n}\n\nChatCompletion chatCompletion = chatCompletionAccumulator.chatCompletion();\n```\n\nFor an asynchronous response, add the `ChatCompletionAccumulator` to the `subscribe()` call:\n\n```java\nimport com.openai.helpers.ChatCompletionAccumulator;\nimport com.openai.models.chat.completions.ChatCompletion;\n\nChatCompletionAccumulator chatCompletionAccumulator = ChatCompletionAccumulator.create();\n\nclient.chat()\n        .completions()\n        .createStreaming(createParams)\n        .subscribe(chunk -> chatCompletionAccumulator.accumulate(chunk).choices().stream()\n                .flatMap(choice -> choice.delta().content().stream())\n                .forEach(System.out::print))\n        .onCompleteFuture()\n        .join();\n\nChatCompletion chatCompletion = chatCompletionAccumulator.chatCompletion();\n```\n\nThe SDK provides conveniences for streamed responses. A\n[`ResponseAccumulator`](openai-java-core/src/main/kotlin/com/openai/helpers/ResponseAccumulator.kt)\ncan record the stream of response events as they are processed and accumulate a\n[`Response`](openai-java-core/src/main/kotlin/com/openai/models/responses/Response.kt)\nobject similar to that which would have been returned by the non-streaming API.\n\nFor a synchronous response add a\n[`Stream.peek()`](https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html#peek-java.util.function.Consumer-)\ncall to the stream pipeline to accumulate each event:\n\n```java\nimport com.openai.core.http.StreamResponse;\nimport com.openai.helpers.ResponseAccumulator;\nimport com.openai.models.responses.Response;\nimport com.openai.models.responses.ResponseStreamEvent;\n\nResponseAccumulator responseAccumulator = ResponseAccumulator.create();\n\ntry (StreamResponse<ResponseStreamEvent> streamResponse =\n        client.responses().createStreaming(createParams)) {\n    streamResponse.stream()\n            .peek(responseAccumulator::accumulate)\n            .flatMap(event -> event.outputTextDelta().stream())\n            .forEach(textEvent -> System.out.print(textEvent.delta()));\n}\n\nResponse response = responseAccumulator.response();\n```\n\nFor an asynchronous response, add the `ResponseAccumulator` to the `subscribe()` call:\n\n```java\nimport com.openai.helpers.ResponseAccumulator;\nimport com.openai.models.responses.Response;\n\nResponseAccumulator responseAccumulator = ResponseAccumulator.create();\n\nclient.responses()\n        .createStreaming(createParams)\n        .subscribe(event -> responseAccumulator.accumulate(event)\n                .outputTextDelta().ifPresent(textEvent -> System.out.print(textEvent.delta())))\n        .onCompleteFuture()\n        .join();\n\nResponse response = responseAccumulator.response();\n```\n\n## Structured outputs with JSON schemas\n\nOpen AI [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs?api-mode=chat)\nis a feature that ensures that the model will always generate responses that adhere to a supplied\n[JSON schema](https://json-schema.org/overview/what-is-jsonschema).\n\nA JSON schema can be defined by creating a\n[`ResponseFormatJsonSchema`](openai-java-core/src/main/kotlin/com/openai/models/ResponseFormatJsonSchema.kt)\nand setting it on the input parameters. However, for greater convenience, a JSON schema can instead\nbe derived automatically from the structure of an arbitrary Java class. The JSON content from the\nresponse will then be converted automatically to an instance of that Java class. A full, working\nexample of the use of Structured Outputs with arbitrary Java classes can be seen in\n[`StructuredOutputsExample`](openai-java-example/src/main/java/com/openai/example/StructuredOutputsExample.java).\n\nJava classes can contain fields declared to be instances of other classes and can use collections\n(see [Defining JSON schema properties](#defining-json-schema-properties) for more details):\n\n```java\nclass Person {\n    public String name;\n    public int birthYear;\n}\n\nclass Book {\n    public String title;\n    public Person author;\n    public int publicationYear;\n}\n\nclass BookList {\n    public List<Book> books;\n}\n```\n\nPass the top-level classâ€”`BookList` in this exampleâ€”to `responseFormat(Class<T>)` when building the\nparameters and then access an instance of `BookList` from the generated message content in the\nresponse:\n\n```java\nimport com.openai.models.ChatModel;\nimport com.openai.models.chat.completions.ChatCompletionCreateParams;\nimport com.openai.models.chat.completions.StructuredChatCompletionCreateParams;\n\nStructuredChatCompletionCreateParams<BookList> params = ChatCompletionCreateParams.builder()\n        .addUserMessage(\"List some famous late twentieth century novels.\")\n        .model(ChatModel.GPT_4_1)\n        .responseFormat(BookList.class)\n        .build();\n\nclient.chat().completions().create(params).choices().stream()\n        .flatMap(choice -> choice.message().content().stream())\n        .flatMap(bookList -> bookList.books.stream())\n        .forEach(book -> System.out.println(book.title + \" by \" + book.author.name));\n```\n\nYou can start building the parameters with an instance of\n[`ChatCompletionCreateParams.Builder`](openai-java-core/src/main/kotlin/com/openai/models/chat/completions/ChatCompletionCreateParams.kt)\nor\n[`StructuredChatCompletionCreateParams.Builder`](openai-java-core/src/main/kotlin/com/openai/models/chat/completions/StructuredChatCompletionCreateParams.kt).\nIf you start with the former (which allows for more compact code) the builder type will change to\nthe latter when `ChatCompletionCreateParams.Builder.responseFormat(Class<T>)` is called.\n\nIf a field in a class is optional and does not require a defined value, you can represent this using\nthe [`java.util.Optional`](https://docs.oracle.com/javase/8/docs/api/java/util/Optional.html) class.\nIt is up to the AI model to decide whether to provide a value for that field or leave it empty.\n\n```java\nimport java.util.Optional;\n\nclass Book {\n    public String title;\n    public Person author;\n    public int publicationYear;\n    public Optional<String> isbn;\n}\n```\n\nGeneric type information for fields is retained in the class's metadata, but _generic type erasure_\napplies in other scopes. While, for example, a JSON schema defining an array of books can be derived\nfrom the `BookList.books` field with type `List<Book>`, a valid JSON schema cannot be derived from a\nlocal variable of that same type, so the following will _not_ work:\n\n```java\nList<Book> books = new ArrayList<>();\n\nStructuredChatCompletionCreateParams<List<Book>> params = ChatCompletionCreateParams.builder()\n        .responseFormat(books.getClass())\n        // ...\n        .build();\n```\n\nIf an error occurs while converting a JSON response to an instance of a Java class, the error\nmessage will include the JSON response to assist in diagnosis. For instance, if the response is\ntruncated, the JSON data will be incomplete and cannot be converted to a class instance. If your\nJSON response may contain sensitive information, avoid logging it directly, or ensure that you\nredact any sensitive details from the error message.\n\n### Local JSON schema validation\n\nStructured Outputs supports a\n[subset](https://platform.openai.com/docs/guides/structured-outputs#supported-schemas) of the JSON\nSchema language. Schemas are generated automatically from classes to align with this subset.\nHowever, due to the inherent structure of the classes, the generated schema may still violate\ncertain OpenAI schema restrictions, such as exceeding the maximum nesting depth or utilizing\nunsupported data types.\n\nTo facilitate compliance, the method `responseFormat(Class<T>)` performs a validation check on the\nschema derived from the specified class. This validation ensures that all restrictions are adhered\nto. If any issues are detected, an exception will be thrown, providing a detailed message outlining\nthe reasons for the validation failure.\n\n- **Local Validation**: The validation process occurs locally, meaning no requests are sent to the\n  remote AI model. If the schema passes local validation, it is likely to pass remote validation as\n  well.\n- **Remote Validation**: The remote AI model will conduct its own validation upon receiving the JSON\n  schema in the request.\n- **Version Compatibility**: There may be instances where local validation fails while remote\n  validation succeeds. This can occur if the SDK version is outdated compared to the restrictions\n  enforced by the remote AI model.\n- **Disabling Local Validation**: If you encounter compatibility issues and wish to bypass local\n  validation, you can disable it by passing\n  [`JsonSchemaLocalValidation.NO`](openai-java-core/src/main/kotlin/com/openai/core/JsonSchemaLocalValidation.kt)\n  to the `responseFormat(Class<T>, JsonSchemaLocalValidation)` method when building the parameters.\n  (The default value for this parameter is `JsonSchemaLocalValidation.YES`.)\n\n```java\nimport com.openai.core.JsonSchemaLocalValidation;\nimport com.openai.models.ChatModel;\nimport com.openai.models.chat.completions.ChatCompletionCreateParams;\nimport com.openai.models.chat.completions.StructuredChatCompletionCreateParams;\n\nStructuredChatCompletionCreateParams<BookList> params = ChatCompletionCreateParams.builder()\n        .addUserMessage(\"List some famous late twentieth century novels.\")\n        .model(ChatModel.GPT_4_1)\n        .responseFormat(BookList.class, JsonSchemaLocalValidation.NO)\n        .build();\n```\n\nBy following these guidelines, you can ensure that your structured outputs conform to the necessary\nschema requirements and minimize the risk of remote validation errors.\n\n### Usage with the Responses API\n\n_Structured Outputs_ are also supported for the Responses API. The usage is the same as described\nexcept where the Responses API differs slightly from the Chat Completions API. Pass the top-level\nclass to `text(Class<T>)` when building the parameters and then access an instance of the class from\nthe generated message content in the response.\n\nYou can start building the parameters with an instance of\n[`ResponseCreateParams.Builder`](openai-java-core/src/main/kotlin/com/openai/models/responses/ResponseCreateParams.kt)\nor\n[`StructuredResponseCreateParams.Builder`](openai-java-core/src/main/kotlin/com/openai/models/responses/StructuredResponseCreateParams.kt).\nIf you start with the former (which allows for more compact code) the builder type will change to\nthe latter when `ResponseCreateParams.Builder.text(Class<T>)` is called.\n\nFor a full example of the usage of _Structured Outputs_ with the Responses API, see\n[`ResponsesStructuredOutputsExample`](openai-java-example/src/main/java/com/openai/example/ResponsesStructuredOutputsExample.java).\n\nInstead of using `ResponseCreateParams.text(Class<T>)`, you can build a\n[`StructuredResponseTextConfig`](openai-java-core/src/main/kotlin/com/openai/models/responses/StructuredResponseTextConfig.kt)\nand set it on the `ResponseCreateParams` using the `text(StructuredResponseTextConfig)` method.\nSimilar to using `ResponseCreateParams`, you can start with a `ResponseTextConfig.Builder` and its\n`format(Class<T>)` method will change it to a `StructuredResponseTextConfig.Builder`. This also\nallows you to set the `verbosity` configuration parameter on the text configuration before adding it\nto the `ResponseCreateParams`.\n\nFor a full example of the usage of _Structured Outputs_ with the `ResponseTextConfig` and its\n`verbosity` parameter, see\n[`ResponsesStructuredOutputsVerbosityExample`](openai-java-example/src/main/java/com/openai/example/ResponsesStructuredOutputsVerbosityExample.java).\n\n### Usage with streaming\n\n_Structured Outputs_ can also be used with [Streaming](#streaming) and the Chat Completions API. As\nresponses are returned in \"chunks\", the full response must first be accumulated to concatenate the\nJSON strings that can then be converted into instances of the arbitrary Java class. Normal streaming\noperations can be performed while accumulating the JSON strings.\n\nUse the [`ChatCompletionAccumulator`](openai-java-core/src/main/kotlin/com/openai/helpers/ChatCompletionAccumulator.kt)\nas described in the section on [Streaming helpers](#streaming-helpers) to accumulate the JSON\nstrings. Once accumulated, use `ChatCompletionAccumulator.chatCompletion(Class<T>)` to convert the\naccumulated `ChatCompletion` into a\n[`StructuredChatCompletion`](openai-java-core/src/main/kotlin/com/openai/models/chat/completions/StructuredChatCompletion.kt).\nThe `StructuredChatCompletion` can then automatically deserialize the JSON strings into instances of\nyour Java class.\n\nFor a full example of the usage of _Structured Outputs_ with Streaming and the Chat Completions API,\nsee\n[`StructuredOutputsStreamingExample`](openai-java-example/src/main/java/com/openai/example/StructuredOutputsStreamingExample.java).\n\nWith the Responses API, accumulate events while streaming using the\n[`ResponseAccumulator`](openai-java-core/src/main/kotlin/com/openai/helpers/ResponseAccumulator.kt).\nOnce accumulated, use `ResponseAccumulator.response(Class<T>)` to convert the accumulated `Response`\ninto a\n[`StructuredResponse`](openai-java-core/src/main/kotlin/com/openai/models/responses/StructuredResponse.kt).\nThe [`StructuredResponse`] can then automatically deserialize the JSON strings into instances of\nyour Java class.\n\nFor a full example of the usage of _Structured Outputs_ with Streaming and the Responses API, see\n[`ResponsesStructuredOutputsStreamingExample`](openai-java-example/src/main/java/com/openai/example/ResponsesStructuredOutputsStreamingExample.java).\n\n### Defining JSON schema properties\n\nWhen a JSON schema is derived from your Java classes, all properties represented by `public` fields\nor `public` getter methods are included in the schema by default. Non-`public` fields and getter\nmethods are _not_ included by default. You can exclude `public`, or include non-`public` fields or\ngetter methods, by using the `@JsonIgnore` or `@JsonProperty` annotations respectively (see\n[Annotating classes and JSON schemas](#annotating-classes-and-json-schemas) for details).\n\nIf you do not want to define `public` fields, you can define `private` fields and corresponding\n`public` getter methods. For example, a `private` field `myValue` with a `public` getter method\n`getMyValue()` will result in a `\"myValue\"` property being included in the JSON schema. If you\nprefer not to use the conventional Java \"get\" prefix for the name of the getter method, then you\n_must_ annotate the getter method with the `@JsonProperty` annotation and the full method name will\nbe used as the property name. You do not have to define any corresponding setter methods if you do\nnot need them.\n\nEach of your classes _must_ define at least one property to be included in the JSON schema. A\nvalidation error will occur if any class contains no fields or getter methods from which schema\nproperties can be derived. This may occur if, for example:\n\n- There are no fields or getter methods in the class.\n- All fields and getter methods are `public`, but all are annotated with `@JsonIgnore`.\n- All fields and getter methods are non-`public`, but none are annotated with `@JsonProperty`.\n- A field or getter method is declared with a `Map` type. A `Map` is treated like a separate class\n  with no named properties, so it will result in an empty `\"properties\"` field in the JSON schema.\n\n### Annotating classes and JSON schemas\n\nYou can use annotations to add further information to the JSON schema derived from your Java\nclasses, or to control which fields or getter methods will be included in the schema. Details from\nannotations captured in the JSON schema may be used by the AI model to improve its response. The SDK\nsupports the use of [Jackson Databind](https://github.com/FasterXML/jackson-databind) annotations.\n\n```java\nimport com.fasterxml.jackson.annotation.JsonClassDescription;\nimport com.fasterxml.jackson.annotation.JsonIgnore;\nimport com.fasterxml.jackson.annotation.JsonPropertyDescription;\n\nclass Person {\n    @JsonPropertyDescription(\"The first name and surname of the person\")\n    public String name;\n    public int birthYear;\n    @JsonPropertyDescription(\"The year the person died, or 'present' if the person is living.\")\n    public String deathYear;\n}\n\n@JsonClassDescription(\"The details of one published book\")\nclass Book {\n    public String title;\n    public Person author;\n    @JsonPropertyDescription(\"The year in which the book was first published.\")\n    public int publicationYear;\n    @JsonIgnore public String genre;\n}\n\nclass BookList {\n    public List<Book> books;\n}\n```\n\n- Use `@JsonClassDescription` to add a detailed description to a class.\n- Use `@JsonPropertyDescription` to add a detailed description to a field or getter method of a\n  class.\n- Use `@JsonIgnore` to exclude a `public` field or getter method of a class from the generated JSON\n  schema.\n- Use `@JsonProperty` to include a non-`public` field or getter method of a class in the generated\n  JSON schema.\n\nIf you use `@JsonProperty(required = false)`, the `false` value will be ignored. OpenAI JSON schemas\nmust mark all properties as _required_, so the schema generated from your Java classes will respect\nthat restriction and ignore any annotation that would violate it.\n\nYou can also use [OpenAPI Swagger 2](https://swagger.io/specification/v2/)\n[`@Schema`](https://github.com/swagger-api/swagger-core/wiki/Swagger-2.X---Annotations#schema) and\n[`@ArraySchema`](https://github.com/swagger-api/swagger-core/wiki/Swagger-2.X---Annotations#arrayschema)\nannotations. These allow type-specific constraints to be added to your schema properties. You can\nlearn more about the supported constraints in the OpenAI documentation on\n[Supported properties](https://platform.openai.com/docs/guides/structured-outputs#supported-properties).\n\n```java\nimport io.swagger.v3.oas.annotations.media.Schema;\nimport io.swagger.v3.oas.annotations.media.ArraySchema;\n\nclass Article {\n    @ArraySchema(minItems = 1, maxItems = 10)\n    public List<String> authors;\n\n    @Schema(pattern = \"^[A-Za-z ]+$\")\n    public String title;\n\n    @Schema(format = \"date\")\n    public String publicationDate;\n\n    @Schema(minimum = \"1\")\n    public int pageCount;\n}\n```\n\nLocal validation will check that you have not used any unsupported constraint keywords. However, the\nvalues of the constraints are _not_ validated locally. For example, if you use a value for the\n`\"format\"` constraint of a string property that is not in the list of\n[supported format names](https://platform.openai.com/docs/guides/structured-outputs#supported-properties),\nthen local validation will pass, but the AI model may report an error.\n\nIf you use both Jackson and Swagger annotations to set the same schema field, the Jackson annotation\nwill take precedence. In the following example, the description of `myProperty` will be set to\n\"Jackson description\"; \"Swagger description\" will be ignored:\n\n```java\nimport com.fasterxml.jackson.annotation.JsonPropertyDescription;\nimport io.swagger.v3.oas.annotations.media.Schema;\n\nclass MyObject {\n    @Schema(description = \"Swagger description\")\n    @JsonPropertyDescription(\"Jackson description\")\n    public String myProperty;\n}\n```\n\n## Function calling with JSON schemas\n\nOpenAI [Function Calling](https://platform.openai.com/docs/guides/function-calling?api-mode=chat)\nlets you integrate external functions directly into the language model's responses. Instead of\nproducing plain text, the model can output instructions (with parameters) for calling a function\nwhen appropriate. You define a [JSON schema](https://json-schema.org/overview/what-is-jsonschema)\nfor functions, and the model uses it to decide when and how to trigger these calls, enabling more\ninteractive, data-driven applications.\n\nA JSON schema describing a function's parameters can be defined via the API by building a\n[`ChatCompletionTool`](openai-java-core/src/main/kotlin/com/openai/models/chat/completions/ChatCompletionTool.kt)\ncontaining a\n[`FunctionDefinition`](openai-java-core/src/main/kotlin/com/openai/models/FunctionDefinition.kt)\nand then using `addTool` to set it on the input parameters. The response from the AI model may then\ncontain requests to call your functions, detailing the functions' names and their parameter values\nas JSON data that conforms to the JSON schema from the function definition. You can then parse the\nparameter values from this JSON, invoke your functions, and pass your functions' results back to the\nAI model. A full, working example of _Function Calling_ using the low-level API can be seen in\n[`FunctionCallingRawExample`](openai-java-example/src/main/java/com/openai/example/FunctionCallingRawExample.java).\n\nHowever, for greater convenience, the SDK can derive a function and its parameters automatically\nfrom the structure of an arbitrary Java class: the class's name provides the function name, and the\nclass's fields define the function's parameters. When the AI model responds with the parameter\nvalues in JSON form, you can then easily convert that JSON to an instance of your Java class and\nuse the parameter values to invoke your custom function. A full, working example of the use of\n_Function Calling_ with Java classes to define function parameters can be seen in\n[`FunctionCallingExample`](openai-java-example/src/main/java/com/openai/example/FunctionCallingExample.java).\n\nLike for [Structured Outputs](#structured-outputs-with-json-schemas), Java classes can contain\nfields declared to be instances of other classes and can use collections (see\n[Defining JSON schema properties](#defining-json-schema-properties) for more details). Optionally,\nannotations can be used to set the descriptions of the function (class) and its parameters (fields)\nto assist the AI model in understanding the purpose of the function and the possible values of its\nparameters.\n\n```java\nimport com.fasterxml.jackson.annotation.JsonClassDescription;\nimport com.fasterxml.jackson.annotation.JsonPropertyDescription;\n\n@JsonClassDescription(\"Gets the quality of the given SDK.\")\nstatic class GetSdkQuality {\n    @JsonPropertyDescription(\"The name of the SDK.\")\n    public String name;\n\n    public SdkQuality execute() {\n        return new SdkQuality(\n                name, name.contains(\"OpenAI\") ? \"It's robust and polished!\" : \"*shrug*\");\n    }\n}\n\nstatic class SdkQuality {\n    public String quality;\n\n    public SdkQuality(String name, String evaluation) {\n        quality = name + \": \" + evaluation;\n    }\n}\n\n@JsonClassDescription(\"Gets the review score (out of 10) for the named SDK.\")\nstatic class GetSdkScore {\n  public String name;\n\n  public int execute() {\n    return name.contains(\"OpenAI\") ? 10 : 3;\n  }\n}\n```\n\nWhen your functions are defined, add them to the input parameters using `addTool(Class<T>)` and then\ncall them if requested to do so in the AI model's response. `Function.argments(Class<T>)` can be\nused to parse a function's parameters in JSON form to an instance of your function-defining class.\nThe fields of that instance will be set to the values of the parameters to the function call.\n\nAfter calling the function, use `ChatCompletionToolMessageParam.Builder.contentAsJson(Object)` to\npass the function's result back to the AI model. The method will convert the result to JSON form\nfor consumption by the model. The `Object` can be any object, including simple `String` instances\nand boxed primitive types.\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport com.openai.models.ChatModel;\nimport com.openai.models.chat.completions.*;\nimport java.util.Collection;\n\nOpenAIClient client = OpenAIOkHttpClient.fromEnv();\n\nChatCompletionCreateParams.Builder createParamsBuilder = ChatCompletionCreateParams.builder()\n        .model(ChatModel.GPT_3_5_TURBO)\n        .maxCompletionTokens(2048)\n        .addTool(GetSdkQuality.class)\n        .addTool(GetSdkScore.class)\n        .addUserMessage(\"How good are the following SDKs and what do reviewers say: \"\n                + \"OpenAI Java SDK, Unknown Company SDK.\");\n\nclient.chat().completions().create(createParamsBuilder.build()).choices().stream()\n        .map(ChatCompletion.Choice::message)\n        // Add each assistant message onto the builder so that we keep track of the\n        // conversation for asking a follow-up question later.\n        .peek(createParamsBuilder::addMessage)\n        .flatMap(message -> {\n            message.content().ifPresent(System.out::println);\n            return message.toolCalls().stream().flatMap(Collection::stream);\n        })\n        .forEach(toolCall -> {\n            Object result = callFunction(toolCall.function());\n            // Add the tool call result to the conversation.\n            createParamsBuilder.addMessage(ChatCompletionToolMessageParam.builder()\n                    .toolCallId(toolCall.id())\n                    .contentAsJson(result)\n                    .build());\n        });\n\n// Ask a follow-up question about the function call result.\ncreateParamsBuilder.addUserMessage(\"Why do you say that?\");\nclient.chat().completions().create(createParamsBuilder.build()).choices().stream()\n        .flatMap(choice -> choice.message().content().stream())\n        .forEach(System.out::println);\n\nstatic Object callFunction(ChatCompletionMessageToolCall.Function function) {\n  switch (function.name()) {\n    case \"GetSdkQuality\":\n      return function.arguments(GetSdkQuality.class).execute();\n    case \"GetSdkScore\":\n      return function.arguments(GetSdkScore.class).execute();\n    default:\n      throw new IllegalArgumentException(\"Unknown function: \" + function.name());\n  }\n}\n```\n\nIn the code above, an `execute()` method encapsulates each function's logic. However, there is no\nrequirement to follow that pattern. You are free to implement your function's logic in any way that\nbest suits your use case. The pattern above is only intended to _suggest_ that a suitable pattern\nmay make the process of function calling simpler to understand and implement.\n\n### Usage with the Responses API\n\n_Function Calling_ is also supported for the Responses API. The usage is the same as described\nexcept where the Responses API differs slightly from the Chat Completions API. Pass the top-level\nclass to `addTool(Class<T>)` when building the parameters. In the response, look for\n[`RepoonseOutputItem`](openai-java-core/src/main/kotlin/com/openai/models/responses/ResponseOutputItem.kt)\ninstances that are function calls. Parse the parameters to each function call to an instance of the\nclass using\n[`ResponseFunctionToolCall.arguments(Class<T>)`](openai-java-core/src/main/kotlin/com/openai/models/responses/ResponseFunctionToolCall.kt).\nFinally, pass the result of each call back to the model.\n\nFor a full example of the usage of _Function Calling_ with the Responses API using the low-level\nAPI to define and parse function parameters, see\n[`ResponsesFunctionCallingRawExample`](openai-java-example/src/main/java/com/openai/example/ResponsesFunctionCallingRawExample.java).\n\nFor a full example of the usage of _Function Calling_ with the Responses API using Java classes to\ndefine and parse function parameters, see\n[`ResponsesFunctionCallingExample`](openai-java-example/src/main/java/com/openai/example/ResponsesFunctionCallingExample.java).\n\n### Local function JSON schema validation\n\nLike for _Structured Outputs_, you can perform local validation to check that the JSON schema\nderived from your function class respects the restrictions imposed by OpenAI on such schemas. Local\nvalidation is enabled by default, but it can be disabled by adding `JsonSchemaLocalValidation.NO` to\nthe call to `addTool`.\n\n```java\nChatCompletionCreateParams.Builder createParamsBuilder = ChatCompletionCreateParams.builder()\n        .model(ChatModel.GPT_3_5_TURBO)\n        .maxCompletionTokens(2048)\n        .addTool(GetSdkQuality.class, JsonSchemaLocalValidation.NO)\n        .addTool(GetSdkScore.class, JsonSchemaLocalValidation.NO)\n        .addUserMessage(\"How good are the following SDKs and what do reviewers say: \"\n                + \"OpenAI Java SDK, Unknown Company SDK.\");\n```\n\nSee [Local JSON schema validation](#local-json-schema-validation) for more details on local schema\nvalidation and under what circumstances you might want to disable it.\n\n### Annotating function classes\n\nYou can use annotations to add further information about functions to the JSON schemas that are\nderived from your function classes, or to control which fields or getter methods will be used as\nparameters to the function. Details from annotations captured in the JSON schema may be used by the\nAI model to improve its response. The SDK supports the use of\n[Jackson Databind](https://github.com/FasterXML/jackson-databind) annotations.\n\n- Use `@JsonClassDescription` to add a description to a function class detailing when and how to use\n  that function.\n- Use `@JsonTypeName` to set the function name to something other than the simple name of the class,\n  which is used by default.\n- Use `@JsonPropertyDescription` to add a detailed description to function parameter (a field or\n  getter method of a function class).\n- Use `@JsonIgnore` to exclude a `public` field or getter method of a class from the generated JSON\n  schema for a function's parameters.\n- Use `@JsonProperty` to include a non-`public` field or getter method of a class in the generated\n  JSON schema for a function's parameters.\n\nOpenAI provides some\n[Best practices for defining functions](https://platform.openai.com/docs/guides/function-calling#best-practices-for-defining-functions)\nthat may help you to understand how to use the above annotations effectively for your functions.\n\nSee also [Defining JSON schema properties](#defining-json-schema-properties) for more details on how\nto use fields and getter methods and combine access modifiers and annotations to define the\nparameters of your functions. The same rules apply to function classes and to the structured output\nclasses described in that section.\n\n## File uploads\n\nThe SDK defines methods that accept files.\n\nTo upload a file, pass a [`Path`](https://docs.oracle.com/javase/8/docs/api/java/nio/file/Path.html):\n\n```java\nimport com.openai.models.files.FileCreateParams;\nimport com.openai.models.files.FileObject;\nimport com.openai.models.files.FilePurpose;\nimport java.nio.file.Paths;\n\nFileCreateParams params = FileCreateParams.builder()\n    .purpose(FilePurpose.FINE_TUNE)\n    .file(Paths.get(\"input.jsonl\"))\n    .build();\nFileObject fileObject = client.files().create(params);\n```\n\nOr an arbitrary [`InputStream`](https://docs.oracle.com/javase/8/docs/api/java/io/InputStream.html):\n\n```java\nimport com.openai.models.files.FileCreateParams;\nimport com.openai.models.files.FileObject;\nimport com.openai.models.files.FilePurpose;\nimport java.net.URL;\n\nFileCreateParams params = FileCreateParams.builder()\n    .purpose(FilePurpose.FINE_TUNE)\n    .file(new URL(\"https://example.com/input.jsonl\").openStream())\n    .build();\nFileObject fileObject = client.files().create(params);\n```\n\nOr a `byte[]` array:\n\n```java\nimport com.openai.models.files.FileCreateParams;\nimport com.openai.models.files.FileObject;\nimport com.openai.models.files.FilePurpose;\n\nFileCreateParams params = FileCreateParams.builder()\n    .purpose(FilePurpose.FINE_TUNE)\n    .file(\"content\".getBytes())\n    .build();\nFileObject fileObject = client.files().create(params);\n```\n\nNote that when passing a non-`Path` its filename is unknown so it will not be included in the request. To manually set a filename, pass a [`MultipartField`](openai-java-core/src/main/kotlin/com/openai/core/Values.kt):\n\n```java\nimport com.openai.core.MultipartField;\nimport com.openai.models.files.FileCreateParams;\nimport com.openai.models.files.FileObject;\nimport com.openai.models.files.FilePurpose;\nimport java.io.InputStream;\nimport java.net.URL;\n\nFileCreateParams params = FileCreateParams.builder()\n    .purpose(FilePurpose.FINE_TUNE)\n    .file(MultipartField.<InputStream>builder()\n        .value(new URL(\"https://example.com/input.jsonl\").openStream())\n        .filename(\"input.jsonl\")\n        .build())\n    .build();\nFileObject fileObject = client.files().create(params);\n```\n\n## Webhook Verification\n\nVerifying webhook signatures is _optional but encouraged_.\n\nFor more information about webhooks, see [the API docs](https://platform.openai.com/docs/guides/webhooks).\n\n### Parsing webhook payloads\n\nFor most use cases, you will likely want to verify the webhook and parse the payload at the same time. To achieve this, we provide the method `client.webhooks().unwrap()`, which parses a webhook request and verifies that it was sent by OpenAI. This method will throw an exception if the signature is invalid.\n\nNote that the `body` parameter must be the raw JSON string sent from the server (do not parse it first). The `.unwrap()` method will parse this JSON for you into an event object after verifying the webhook was sent from OpenAI.\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport com.openai.core.http.Headers;\nimport com.openai.models.webhooks.UnwrapWebhookEvent;\nimport java.util.Optional;\n\nOpenAIClient client = OpenAIOkHttpClient.fromEnv(); // OPENAI_WEBHOOK_SECRET env var used by default\n\npublic void handleWebhook(String body, Map<String, String> headers) {\n    try {\n        Headers headersList = Headers.builder()\n                .putAll(headers)\n                .build();\n\n        UnwrapWebhookEvent event = client.webhooks().unwrap(body, headersList, Optional.empty());\n\n        if (event.isResponseCompletedWebhookEvent()) {\n            System.out.println(\"Response completed: \" + event.asResponseCompletedWebhookEvent().data());\n        } else if (event.isResponseFailed()) {\n            System.out.println(\"Response failed: \" + event.asResponseFailed().data());\n        } else {\n            System.out.println(\"Unhandled event type: \" + event.getClass().getSimpleName());\n        }\n    } catch (Exception e) {\n        System.err.println(\"Invalid webhook signature: \" + e.getMessage());\n        // Handle invalid signature\n    }\n}\n```\n\n### Verifying webhook payloads directly\n\nIn some cases, you may want to verify the webhook separately from parsing the payload. If you prefer to handle these steps separately, we provide the method `client.webhooks().verifySignature()` to _only verify_ the signature of a webhook request. Like `.unwrap()`, this method will throw an exception if the signature is invalid.\n\nNote that the `body` parameter must be the raw JSON string sent from the server (do not parse it first). You will then need to parse the body after verifying the signature.\n\n```java\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport com.openai.core.http.Headers;\nimport com.openai.models.webhooks.WebhookVerificationParams;\nimport java.util.Optional;\n\nOpenAIClient client = OpenAIOkHttpClient.fromEnv(); // OPENAI_WEBHOOK_SECRET env var used by default\nObjectMapper objectMapper = new ObjectMapper();\n\npublic void handleWebhook(String body, Map<String, String> headers) {\n    try {\n        Headers headersList = Headers.builder()\n                .putAll(headers)\n                .build();\n\n        client.webhooks().verifySignature(\n            WebhookVerificationParams.builder()\n                .payload(body)\n                .headers(headersList)\n                .build()\n        );\n\n        // Parse the body after verification\n        Map<String, Object> event = objectMapper.readValue(body, Map.class);\n        System.out.println(\"Verified event: \" + event);\n    } catch (Exception e) {\n        System.err.println(\"Invalid webhook signature: \" + e.getMessage());\n        // Handle invalid signature\n    }\n}\n```\n\n## Binary responses\n\nThe SDK defines methods that return binary responses, which are used for API responses that shouldn't necessarily be parsed, like non-JSON data.\n\nThese methods return [`HttpResponse`](openai-java-core/src/main/kotlin/com/openai/core/http/HttpResponse.kt):\n\n```java\nimport com.openai.core.http.HttpResponse;\nimport com.openai.models.files.FileContentParams;\n\nHttpResponse response = client.files().content(\"file_id\");\n```\n\nTo save the response content to a file, use the [`Files.copy(...)`](https://docs.oracle.com/javase/8/docs/api/java/nio/file/Files.html#copy-java.io.InputStream-java.nio.file.Path-java.nio.file.CopyOption...-) method:\n\n```java\nimport com.openai.core.http.HttpResponse;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\nimport java.nio.file.StandardCopyOption;\n\ntry (HttpResponse response = client.files().content(params)) {\n    Files.copy(\n        response.body(),\n        Paths.get(path),\n        StandardCopyOption.REPLACE_EXISTING\n    );\n} catch (Exception e) {\n    System.out.println(\"Something went wrong!\");\n    throw new RuntimeException(e);\n}\n```\n\nOr transfer the response content to any [`OutputStream`](https://docs.oracle.com/javase/8/docs/api/java/io/OutputStream.html):\n\n```java\nimport com.openai.core.http.HttpResponse;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\ntry (HttpResponse response = client.files().content(params)) {\n    response.body().transferTo(Files.newOutputStream(Paths.get(path)));\n} catch (Exception e) {\n    System.out.println(\"Something went wrong!\");\n    throw new RuntimeException(e);\n}\n```\n\n## Raw responses\n\nThe SDK defines methods that deserialize responses into instances of Java classes. However, these methods don't provide access to the response headers, status code, or the raw response body.\n\nTo access this data, prefix any HTTP method call on a client or service with `withRawResponse()`:\n\n```java\nimport com.openai.core.http.Headers;\nimport com.openai.core.http.HttpResponseFor;\nimport com.openai.models.ChatModel;\nimport com.openai.models.chat.completions.ChatCompletion;\nimport com.openai.models.chat.completions.ChatCompletionCreateParams;\n\nChatCompletionCreateParams params = ChatCompletionCreateParams.builder()\n    .addUserMessage(\"Say this is a test\")\n    .model(ChatModel.GPT_4O)\n    .build();\nHttpResponseFor<ChatCompletion> chatCompletion = client.chat().completions().withRawResponse().create(params);\n\nint statusCode = chatCompletion.statusCode();\nHeaders headers = chatCompletion.headers();\n```\n\nYou can still deserialize the response into an instance of a Java class if needed:\n\n```java\nimport com.openai.models.chat.completions.ChatCompletion;\n\nChatCompletion parsedChatCompletion = chatCompletion.parse();\n```\n\n### Request IDs\n\n> For more information on debugging requests, see [the API docs](https://platform.openai.com/docs/api-reference/debugging-requests).\n\nWhen using raw responses, you can access the `x-request-id` response header using the `requestId()` method:\n\n```java\nimport com.openai.core.http.HttpResponseFor;\nimport com.openai.models.chat.completions.ChatCompletion;\nimport java.util.Optional;\n\nHttpResponseFor<ChatCompletion> chatCompletion = client.chat().completions().withRawResponse().create(params);\nOptional<String> requestId = chatCompletion.requestId();\n```\n\nThis can be used to quickly log failing requests and report them back to OpenAI.\n\n## Error handling\n\nThe SDK throws custom unchecked exception types:\n\n- [`OpenAIServiceException`](openai-java-core/src/main/kotlin/com/openai/errors/OpenAIServiceException.kt): Base class for HTTP errors. See this table for which exception subclass is thrown for each HTTP status code:\n\n  | Status | Exception                                                                                                              |\n  | ------ | ---------------------------------------------------------------------------------------------------------------------- |\n  | 400    | [`BadRequestException`](openai-java-core/src/main/kotlin/com/openai/errors/BadRequestException.kt)                     |\n  | 401    | [`UnauthorizedException`](openai-java-core/src/main/kotlin/com/openai/errors/UnauthorizedException.kt)                 |\n  | 403    | [`PermissionDeniedException`](openai-java-core/src/main/kotlin/com/openai/errors/PermissionDeniedException.kt)         |\n  | 404    | [`NotFoundException`](openai-java-core/src/main/kotlin/com/openai/errors/NotFoundException.kt)                         |\n  | 422    | [`UnprocessableEntityException`](openai-java-core/src/main/kotlin/com/openai/errors/UnprocessableEntityException.kt)   |\n  | 429    | [`RateLimitException`](openai-java-core/src/main/kotlin/com/openai/errors/RateLimitException.kt)                       |\n  | 5xx    | [`InternalServerException`](openai-java-core/src/main/kotlin/com/openai/errors/InternalServerException.kt)             |\n  | others | [`UnexpectedStatusCodeException`](openai-java-core/src/main/kotlin/com/openai/errors/UnexpectedStatusCodeException.kt) |\n\n  [`SseException`](openai-java-core/src/main/kotlin/com/openai/errors/SseException.kt) is thrown for errors encountered during [SSE streaming](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events) after a successful initial HTTP response.\n\n- [`OpenAIIoException`](openai-java-core/src/main/kotlin/com/openai/errors/OpenAIIoException.kt): I/O networking errors.\n\n- [`OpenAIRetryableException`](openai-java-core/src/main/kotlin/com/openai/errors/OpenAIRetryableException.kt): Generic error indicating a failure that could be retried by the client.\n\n- [`OpenAIInvalidDataException`](openai-java-core/src/main/kotlin/com/openai/errors/OpenAIInvalidDataException.kt): Failure to interpret successfully parsed data. For example, when accessing a property that's supposed to be required, but the API unexpectedly omitted it from the response.\n\n- [`OpenAIException`](openai-java-core/src/main/kotlin/com/openai/errors/OpenAIException.kt): Base class for all exceptions. Most errors will result in one of the previously mentioned ones, but completely generic errors may be thrown using the base class.\n\n## Pagination\n\nThe SDK defines methods that return a paginated lists of results. It provides convenient ways to access the results either one page at a time or item-by-item across all pages.\n\n### Auto-pagination\n\nTo iterate through all results across all pages, use the `autoPager()` method, which automatically fetches more pages as needed.\n\nWhen using the synchronous client, the method returns an [`Iterable`](https://docs.oracle.com/javase/8/docs/api/java/lang/Iterable.html)\n\n```java\nimport com.openai.models.finetuning.jobs.FineTuningJob;\nimport com.openai.models.finetuning.jobs.JobListPage;\n\nJobListPage page = client.fineTuning().jobs().list();\n\n// Process as an Iterable\nfor (FineTuningJob job : page.autoPager()) {\n    System.out.println(job);\n}\n\n// Process as a Stream\npage.autoPager()\n    .stream()\n    .limit(50)\n    .forEach(job -> System.out.println(job));\n```\n\nWhen using the asynchronous client, the method returns an [`AsyncStreamResponse`](openai-java-core/src/main/kotlin/com/openai/core/http/AsyncStreamResponse.kt):\n\n```java\nimport com.openai.core.http.AsyncStreamResponse;\nimport com.openai.models.finetuning.jobs.FineTuningJob;\nimport com.openai.models.finetuning.jobs.JobListPageAsync;\nimport java.util.Optional;\nimport java.util.concurrent.CompletableFuture;\n\nCompletableFuture<JobListPageAsync> pageFuture = client.async().fineTuning().jobs().list();\n\npageFuture.thenRun(page -> page.autoPager().subscribe(job -> {\n    System.out.println(job);\n}));\n\n// If you need to handle errors or completion of the stream\npageFuture.thenRun(page -> page.autoPager().subscribe(new AsyncStreamResponse.Handler<>() {\n    @Override\n    public void onNext(FineTuningJob job) {\n        System.out.println(job);\n    }\n\n    @Override\n    public void onComplete(Optional<Throwable> error) {\n        if (error.isPresent()) {\n            System.out.println(\"Something went wrong!\");\n            throw new RuntimeException(error.get());\n        } else {\n            System.out.println(\"No more!\");\n        }\n    }\n}));\n\n// Or use futures\npageFuture.thenRun(page -> page.autoPager()\n    .subscribe(job -> {\n        System.out.println(job);\n    })\n    .onCompleteFuture()\n    .whenComplete((unused, error) -> {\n        if (error != null) {\n            System.out.println(\"Something went wrong!\");\n            throw new RuntimeException(error);\n        } else {\n            System.out.println(\"No more!\");\n        }\n    }));\n```\n\n### Manual pagination\n\nTo access individual page items and manually request the next page, use the `items()`,\n`hasNextPage()`, and `nextPage()` methods:\n\n```java\nimport com.openai.models.finetuning.jobs.FineTuningJob;\nimport com.openai.models.finetuning.jobs.JobListPage;\n\nJobListPage page = client.fineTuning().jobs().list();\nwhile (true) {\n    for (FineTuningJob job : page.items()) {\n        System.out.println(job);\n    }\n\n    if (!page.hasNextPage()) {\n        break;\n    }\n\n    page = page.nextPage();\n}\n```\n\n## Logging\n\nThe SDK uses the standard [OkHttp logging interceptor](https://github.com/square/okhttp/tree/master/okhttp-logging-interceptor).\n\nEnable logging by setting the `OPENAI_LOG` environment variable to `info`:\n\n```sh\nexport OPENAI_LOG=info\n```\n\nOr to `debug` for more verbose logging:\n\n```sh\nexport OPENAI_LOG=debug\n```\n\n## ProGuard and R8\n\nAlthough the SDK uses reflection, it is still usable with [ProGuard](https://github.com/Guardsquare/proguard) and [R8](https://developer.android.com/topic/performance/app-optimization/enable-app-optimization) because `openai-java-core` is published with a [configuration file](openai-java-core/src/main/resources/META-INF/proguard/openai-java-core.pro) containing [keep rules](https://www.guardsquare.com/manual/configuration/usage).\n\nProGuard and R8 should automatically detect and use the published rules, but you can also manually copy the keep rules if necessary.\n\n## GraalVM\n\nAlthough the SDK uses reflection, it is still usable in [GraalVM](https://www.graalvm.org) because `openai-java-core` is published with [reachability metadata](https://www.graalvm.org/latest/reference-manual/native-image/metadata/).\n\nGraalVM should automatically detect and use the published metadata, but [manual configuration](https://www.graalvm.org/jdk24/reference-manual/native-image/overview/BuildConfiguration/) is also available.\n\n## Spring Boot\n\nIf you're using Spring Boot, then you can use the SDK's [Spring Boot starter](https://docs.spring.io/spring-boot/docs/2.7.18/reference/htmlsingle/#using.build-systems.starters) to simplify configuration and get set up quickly.\n\n### Installation\n\n<!-- x-release-please-start-version -->\n\n#### Gradle\n\n```kotlin\nimplementation(\"com.openai:openai-java-spring-boot-starter:4.16.1\")\n```\n\n#### Maven\n\n```xml\n<dependency>\n  <groupId>com.openai</groupId>\n  <artifactId>openai-java-spring-boot-starter</artifactId>\n  <version>4.16.1</version>\n</dependency>\n```\n\n<!-- x-release-please-end -->\n\n### Configuration\n\nThe [client's environment variable options](#client-configuration) can be configured in [`application.properties` or `application.yml`](https://docs.spring.io/spring-boot/how-to/properties-and-configuration.html).\n\n#### `application.properties`\n\n```properties\nopenai.base-url=https://api.openai.com/v1\nopenai.api-key=My API Key\nopenai.org-id=My Organization\nopenai.project-id=My Project\nopenai.webhook-secret=My Webhook Secret\n```\n\n#### `application.yml`\n\n```yaml\nopenai:\n  base-url: https://api.openai.com/v1\n  api-key: My API Key\n  org-id: My Organization\n  project-id: My Project\n  webhook-secret: My Webhook Secret\n```\n\n#### Other configuration\n\nConfigure any other client option by providing one or more instances of [`OpenAIClientCustomizer`](openai-java-spring-boot-starter/src/main/kotlin/com/openai/springboot/OpenAIClientCustomizer.kt). For example, here's how you'd set [`maxRetries`](#retries):\n\n```java\nimport com.openai.springboot.OpenAIClientCustomizer;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration\npublic class OpenAIConfig {\n    @Bean\n    public OpenAIClientCustomizer customizer() {\n        return builder -> builder.maxRetries(3);\n    }\n}\n```\n\n### Usage\n\n[Inject](https://docs.spring.io/spring-framework/reference/core/beans/dependencies/factory-collaborators.html) [`OpenAIClient`](openai-java-core/src/main/kotlin/com/openai/client/OpenAIClient.kt) anywhere and start using it!\n\n## Jackson\n\nThe SDK depends on [Jackson](https://github.com/FasterXML/jackson) for JSON serialization/deserialization. It is compatible with version 2.13.4 or higher, but depends on version 2.18.2 by default.\n\nThe SDK throws an exception if it detects an incompatible Jackson version at runtime (e.g. if the default version was overridden in your Maven or Gradle config).\n\nIf the SDK threw an exception, but you're _certain_ the version is compatible, then disable the version check using the `checkJacksonVersionCompatibility` on [`OpenAIOkHttpClient`](openai-java-client-okhttp/src/main/kotlin/com/openai/client/okhttp/OpenAIOkHttpClient.kt) or [`OpenAIOkHttpClientAsync`](openai-java-client-okhttp/src/main/kotlin/com/openai/client/okhttp/OpenAIOkHttpClientAsync.kt).\n\n> [!CAUTION]\n> We make no guarantee that the SDK works correctly when the Jackson version check is disabled.\n\nAlso note that there are bugs in older Jackson versions that can affect the SDK. We don't work around all Jackson bugs ([example](https://github.com/FasterXML/jackson-databind/issues/3240)) and expect users to upgrade Jackson for those instead.\n\n## Microsoft Azure\n\nTo use this library with [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview), use the same\nOpenAI client builder but with the Azure-specific configuration.\n\n```java\nOpenAIClient client = OpenAIOkHttpClient.builder()\n        // Gets the API key and endpoint from the `AZURE_OPENAI_KEY` and `OPENAI_BASE_URL` environment variables, respectively\n        .fromEnv()\n        // Set the Azure Entra ID\n        .credential(BearerTokenCredential.create(AuthenticationUtil.getBearerTokenSupplier(\n                new DefaultAzureCredentialBuilder().build(), \"https://cognitiveservices.azure.com/.default\")))\n        .build();\n```\n\nSee the complete Azure OpenAI example in the [`openai-java-example`](openai-java-example/src/main/java/com/openai/example/AzureEntraIdExample.java) directory. The other examples in the directory also work with Azure as long as the client is configured to use it.\n\n### Optional: URL path mode configuration\n\nThe [`ClientOptions`](openai-java-core/src/main/kotlin/com/openai/core/ClientOptions.kt) can be configured to treat Azure OpenAI endpoint URLs differently, depending on your service setup. The default value is [`AzureUrlPathMode.AUTO`](openai-java-core/src/main/kotlin/com/openai/azure/AzureUrlPathMode.kt). To customize the SDK behavior, each value does the following:\n- `AzureUrlPathMode.LEGACY`: forces the deployment or model name into the path.\n- `AzureUrlPathMode.UNIFIED`: for newer endpoints ending in `/openai/v1` the service behaviour matches OpenAI's, therefore [`AzureOpenAIServiceVersion`](openai-java-core/src/main/kotlin/com/openai/azure/AzureOpenAIServiceVersion.kt) becomes optional and the model is passed in the request object.\n- `AzureUrlPathMode.AUTO`: automatically detects the path mode based on the base URL. Default value.\n\n## Network options\n\n### Retries\n\nThe SDK automatically retries 2 times by default, with a short exponential backoff between requests.\n\nOnly the following error types are retried:\n\n- Connection errors (for example, due to a network connectivity problem)\n- 408 Request Timeout\n- 409 Conflict\n- 429 Rate Limit\n- 5xx Internal\n\nThe API may also explicitly instruct the SDK to retry or not retry a request.\n\nTo set a custom number of retries, configure the client using the `maxRetries` method:\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\n\nOpenAIClient client = OpenAIOkHttpClient.builder()\n    .fromEnv()\n    .maxRetries(4)\n    .build();\n```\n\n### Timeouts\n\nRequests time out after 10 minutes by default.\n\nTo set a custom timeout, configure the method call using the `timeout` method:\n\n```java\nimport com.openai.models.chat.completions.ChatCompletion;\n\nChatCompletion chatCompletion = client.chat().completions().create(\n  params, RequestOptions.builder().timeout(Duration.ofSeconds(30)).build()\n);\n```\n\nOr configure the default for all method calls at the client level:\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport java.time.Duration;\n\nOpenAIClient client = OpenAIOkHttpClient.builder()\n    .fromEnv()\n    .timeout(Duration.ofSeconds(30))\n    .build();\n```\n\n### Proxies\n\nTo route requests through a proxy, configure the client using the `proxy` method:\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\nimport java.net.InetSocketAddress;\nimport java.net.Proxy;\n\nOpenAIClient client = OpenAIOkHttpClient.builder()\n    .fromEnv()\n    .proxy(new Proxy(\n      Proxy.Type.HTTP, new InetSocketAddress(\n        \"https://example.com\", 8080\n      )\n    ))\n    .build();\n```\n\n### HTTPS\n\n> [!NOTE]\n> Most applications should not call these methods, and instead use the system defaults. The defaults include\n> special optimizations that can be lost if the implementations are modified.\n\nTo configure how HTTPS connections are secured, configure the client using the `sslSocketFactory`, `trustManager`, and `hostnameVerifier` methods:\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\n\nOpenAIClient client = OpenAIOkHttpClient.builder()\n    .fromEnv()\n    // If `sslSocketFactory` is set, then `trustManager` must be set, and vice versa.\n    .sslSocketFactory(yourSSLSocketFactory)\n    .trustManager(yourTrustManager)\n    .hostnameVerifier(yourHostnameVerifier)\n    .build();\n```\n\n### Custom HTTP client\n\nThe SDK consists of three artifacts:\n\n- `openai-java-core`\n  - Contains core SDK logic\n  - Does not depend on [OkHttp](https://square.github.io/okhttp)\n  - Exposes [`OpenAIClient`](openai-java-core/src/main/kotlin/com/openai/client/OpenAIClient.kt), [`OpenAIClientAsync`](openai-java-core/src/main/kotlin/com/openai/client/OpenAIClientAsync.kt), [`OpenAIClientImpl`](openai-java-core/src/main/kotlin/com/openai/client/OpenAIClientImpl.kt), and [`OpenAIClientAsyncImpl`](openai-java-core/src/main/kotlin/com/openai/client/OpenAIClientAsyncImpl.kt), all of which can work with any HTTP client\n- `openai-java-client-okhttp`\n  - Depends on [OkHttp](https://square.github.io/okhttp)\n  - Exposes [`OpenAIOkHttpClient`](openai-java-client-okhttp/src/main/kotlin/com/openai/client/okhttp/OpenAIOkHttpClient.kt) and [`OpenAIOkHttpClientAsync`](openai-java-client-okhttp/src/main/kotlin/com/openai/client/okhttp/OpenAIOkHttpClientAsync.kt), which provide a way to construct [`OpenAIClientImpl`](openai-java-core/src/main/kotlin/com/openai/client/OpenAIClientImpl.kt) and [`OpenAIClientAsyncImpl`](openai-java-core/src/main/kotlin/com/openai/client/OpenAIClientAsyncImpl.kt), respectively, using OkHttp\n- `openai-java`\n  - Depends on and exposes the APIs of both `openai-java-core` and `openai-java-client-okhttp`\n  - Does not have its own logic\n\nThis structure allows replacing the SDK's default HTTP client without pulling in unnecessary dependencies.\n\n#### Customized [`OkHttpClient`](https://square.github.io/okhttp/3.x/okhttp/okhttp3/OkHttpClient.html)\n\n> [!TIP]\n> Try the available [network options](#network-options) before replacing the default client.\n\nTo use a customized `OkHttpClient`:\n\n1. Replace your [`openai-java` dependency](#installation) with `openai-java-core`\n2. Copy `openai-java-client-okhttp`'s [`OkHttpClient`](openai-java-client-okhttp/src/main/kotlin/com/openai/client/okhttp/OkHttpClient.kt) class into your code and customize it\n3. Construct [`OpenAIClientImpl`](openai-java-core/src/main/kotlin/com/openai/client/OpenAIClientImpl.kt) or [`OpenAIClientAsyncImpl`](openai-java-core/src/main/kotlin/com/openai/client/OpenAIClientAsyncImpl.kt), similarly to [`OpenAIOkHttpClient`](openai-java-client-okhttp/src/main/kotlin/com/openai/client/okhttp/OpenAIOkHttpClient.kt) or [`OpenAIOkHttpClientAsync`](openai-java-client-okhttp/src/main/kotlin/com/openai/client/okhttp/OpenAIOkHttpClientAsync.kt), using your customized client\n\n### Completely custom HTTP client\n\nTo use a completely custom HTTP client:\n\n1. Replace your [`openai-java` dependency](#installation) with `openai-java-core`\n2. Write a class that implements the [`HttpClient`](openai-java-core/src/main/kotlin/com/openai/core/http/HttpClient.kt) interface\n3. Construct [`OpenAIClientImpl`](openai-java-core/src/main/kotlin/com/openai/client/OpenAIClientImpl.kt) or [`OpenAIClientAsyncImpl`](openai-java-core/src/main/kotlin/com/openai/client/OpenAIClientAsyncImpl.kt), similarly to [`OpenAIOkHttpClient`](openai-java-client-okhttp/src/main/kotlin/com/openai/client/okhttp/OpenAIOkHttpClient.kt) or [`OpenAIOkHttpClientAsync`](openai-java-client-okhttp/src/main/kotlin/com/openai/client/okhttp/OpenAIOkHttpClientAsync.kt), using your new client class\n\n## Undocumented API functionality\n\nThe SDK is typed for convenient usage of the documented API. However, it also supports working with undocumented or not yet supported parts of the API.\n\n### Parameters\n\nTo set undocumented parameters, call the `putAdditionalHeader`, `putAdditionalQueryParam`, or `putAdditionalBodyProperty` methods on any `Params` class:\n\n```java\nimport com.openai.core.JsonValue;\nimport com.openai.models.chat.completions.ChatCompletionCreateParams;\n\nChatCompletionCreateParams params = ChatCompletionCreateParams.builder()\n    .putAdditionalHeader(\"Secret-Header\", \"42\")\n    .putAdditionalQueryParam(\"secret_query_param\", \"42\")\n    .putAdditionalBodyProperty(\"secretProperty\", JsonValue.from(\"42\"))\n    .build();\n```\n\nThese can be accessed on the built object later using the `_additionalHeaders()`, `_additionalQueryParams()`, and `_additionalBodyProperties()` methods.\n\nTo set undocumented parameters on _nested_ headers, query params, or body classes, call the `putAdditionalProperty` method on the nested class:\n\n```java\nimport com.openai.core.JsonValue;\nimport com.openai.models.chat.completions.ChatCompletionCreateParams;\n\nChatCompletionCreateParams params = ChatCompletionCreateParams.builder()\n    .responseFormat(ChatCompletionCreateParams.ResponseFormat.builder()\n        .putAdditionalProperty(\"secretProperty\", JsonValue.from(\"42\"))\n        .build())\n    .build();\n```\n\nThese properties can be accessed on the nested built object later using the `_additionalProperties()` method.\n\nTo set a documented parameter or property to an undocumented or not yet supported _value_, pass a [`JsonValue`](openai-java-core/src/main/kotlin/com/openai/core/Values.kt) object to its setter:\n\n```java\nimport com.openai.core.JsonValue;\nimport com.openai.models.ChatModel;\nimport com.openai.models.chat.completions.ChatCompletionCreateParams;\n\nChatCompletionCreateParams params = ChatCompletionCreateParams.builder()\n    .messages(JsonValue.from(42))\n    .model(ChatModel.GPT_4O)\n    .build();\n```\n\nThe most straightforward way to create a [`JsonValue`](openai-java-core/src/main/kotlin/com/openai/core/Values.kt) is using its `from(...)` method:\n\n```java\nimport com.openai.core.JsonValue;\nimport java.util.List;\nimport java.util.Map;\n\n// Create primitive JSON values\nJsonValue nullValue = JsonValue.from(null);\nJsonValue booleanValue = JsonValue.from(true);\nJsonValue numberValue = JsonValue.from(42);\nJsonValue stringValue = JsonValue.from(\"Hello World!\");\n\n// Create a JSON array value equivalent to `[\"Hello\", \"World\"]`\nJsonValue arrayValue = JsonValue.from(List.of(\n  \"Hello\", \"World\"\n));\n\n// Create a JSON object value equivalent to `{ \"a\": 1, \"b\": 2 }`\nJsonValue objectValue = JsonValue.from(Map.of(\n  \"a\", 1,\n  \"b\", 2\n));\n\n// Create an arbitrarily nested JSON equivalent to:\n// {\n//   \"a\": [1, 2],\n//   \"b\": [3, 4]\n// }\nJsonValue complexValue = JsonValue.from(Map.of(\n  \"a\", List.of(\n    1, 2\n  ),\n  \"b\", List.of(\n    3, 4\n  )\n));\n```\n\nNormally a `Builder` class's `build` method will throw [`IllegalStateException`](https://docs.oracle.com/javase/8/docs/api/java/lang/IllegalStateException.html) if any required parameter or property is unset.\n\nTo forcibly omit a required parameter or property, pass [`JsonMissing`](openai-java-core/src/main/kotlin/com/openai/core/Values.kt):\n\n```java\nimport com.openai.core.JsonMissing;\nimport com.openai.models.ChatModel;\nimport com.openai.models.chat.completions.ChatCompletionCreateParams;\n\nChatCompletionCreateParams params = ChatCompletionCreateParams.builder()\n    .model(ChatModel.GPT_4O)\n    .messages(JsonMissing.of())\n    .build();\n```\n\n### Response properties\n\nTo access undocumented response properties, call the `_additionalProperties()` method:\n\n```java\nimport com.openai.core.JsonValue;\nimport java.util.Map;\n\nMap<String, JsonValue> additionalProperties = client.chat().completions().create(params)._additionalProperties();\nJsonValue secretPropertyValue = additionalProperties.get(\"secretProperty\");\n\nString result = secretPropertyValue.accept(new JsonValue.Visitor<>() {\n    @Override\n    public String visitNull() {\n        return \"It's null!\";\n    }\n\n    @Override\n    public String visitBoolean(boolean value) {\n        return \"It's a boolean!\";\n    }\n\n    @Override\n    public String visitNumber(Number value) {\n        return \"It's a number!\";\n    }\n\n    // Other methods include `visitMissing`, `visitString`, `visitArray`, and `visitObject`\n    // The default implementation of each unimplemented method delegates to `visitDefault`, which throws by default, but can also be overridden\n});\n```\n\nTo access a property's raw JSON value, which may be undocumented, call its `_` prefixed method:\n\n```java\nimport com.openai.core.JsonField;\nimport com.openai.models.chat.completions.ChatCompletionMessageParam;\nimport java.util.Optional;\n\nJsonField<List<ChatCompletionMessageParam>> messages = client.chat().completions().create(params)._messages();\n\nif (messages.isMissing()) {\n  // The property is absent from the JSON response\n} else if (messages.isNull()) {\n  // The property was set to literal null\n} else {\n  // Check if value was provided as a string\n  // Other methods include `asNumber()`, `asBoolean()`, etc.\n  Optional<String> jsonString = messages.asString();\n\n  // Try to deserialize into a custom type\n  MyClass myObject = messages.asUnknown().orElseThrow().convert(MyClass.class);\n}\n```\n\n### Response validation\n\nIn rare cases, the API may return a response that doesn't match the expected type. For example, the SDK may expect a property to contain a `String`, but the API could return something else.\n\nBy default, the SDK will not throw an exception in this case. It will throw [`OpenAIInvalidDataException`](openai-java-core/src/main/kotlin/com/openai/errors/OpenAIInvalidDataException.kt) only if you directly access the property.\n\nIf you would prefer to check that the response is completely well-typed upfront, then either call `validate()`:\n\n```java\nimport com.openai.models.chat.completions.ChatCompletion;\n\nChatCompletion chatCompletion = client.chat().completions().create(params).validate();\n```\n\nOr configure the method call to validate the response using the `responseValidation` method:\n\n```java\nimport com.openai.models.chat.completions.ChatCompletion;\n\nChatCompletion chatCompletion = client.chat().completions().create(\n  params, RequestOptions.builder().responseValidation(true).build()\n);\n```\n\nOr configure the default for all method calls at the client level:\n\n```java\nimport com.openai.client.OpenAIClient;\nimport com.openai.client.okhttp.OpenAIOkHttpClient;\n\nOpenAIClient client = OpenAIOkHttpClient.builder()\n    .fromEnv()\n    .responseValidation(true)\n    .build();\n```\n\n## FAQ\n\n### Why don't you use plain `enum` classes?\n\nJava `enum` classes are not trivially [forwards compatible](https://www.stainless.com/blog/making-java-enums-forwards-compatible). Using them in the SDK could cause runtime exceptions if the API is updated to respond with a new enum value.\n\n### Why do you represent fields using `JsonField<T>` instead of just plain `T`?\n\nUsing `JsonField<T>` enables a few features:\n\n- Allowing usage of [undocumented API functionality](#undocumented-api-functionality)\n- Lazily [validating the API response against the expected shape](#response-validation)\n- Representing absent vs explicitly null values\n\n### Why don't you use [`data` classes](https://kotlinlang.org/docs/data-classes.html)?\n\nIt is not [backwards compatible to add new fields to a data class](https://kotlinlang.org/docs/api-guidelines-backward-compatibility.html#avoid-using-data-classes-in-your-api) and we don't want to introduce a breaking change every time we add a field to a class.\n\n### Why don't you use checked exceptions?\n\nChecked exceptions are widely considered a mistake in the Java programming language. In fact, they were omitted from Kotlin for this reason.\n\nChecked exceptions:\n\n- Are verbose to handle\n- Encourage error handling at the wrong level of abstraction, where nothing can be done about the error\n- Are tedious to propagate due to the [function coloring problem](https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function)\n- Don't play well with lambdas (also due to the function coloring problem)\n\n## Semantic versioning\n\nThis package generally follows [SemVer](https://semver.org/spec/v2.0.0.html) conventions, though certain backwards-incompatible changes may be released as minor versions:\n\n1. Changes to library internals which are technically public but not intended or documented for external use. _(Please open a GitHub issue to let us know if you are relying on such internals.)_\n2. Changes that we do not expect to impact the vast majority of users in practice.\n\nWe take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.\n\nWe are keen for your feedback; please open an [issue](https://www.github.com/openai/openai-java/issues) with questions, bugs, or suggestions.\n",
      "stars_today": 2
    },
    {
      "id": 138660553,
      "name": "DoubletFinder",
      "full_name": "chris-mcginnis-ucsf/DoubletFinder",
      "description": "R package for detecting doublets in single-cell RNA sequencing data",
      "html_url": "https://github.com/chris-mcginnis-ucsf/DoubletFinder",
      "stars": 521,
      "forks": 123,
      "language": "R",
      "topics": [],
      "created_at": "2018-06-25T23:32:45Z",
      "updated_at": "2026-01-28T02:01:52Z",
      "pushed_at": "2025-03-21T11:20:52Z",
      "open_issues": 22,
      "owner": {
        "login": "chris-mcginnis-ucsf",
        "avatar_url": "https://avatars.githubusercontent.com/u/40582930?v=4"
      },
      "readme": "~~ Announcement (11/24/21) ~~\nI'm now a postdoc at Stanford and my UCSF email will be decommissioned soon. I also only check my github repos about once per month, so please reach out directly at cmcginni@stanford[dot]edu if you run into any issues. \n\n# DoubletFinder\n\nDoubletFinder is an R package that predicts doublets in single-cell RNA sequencing data. \n\nDoubletFinder is implemented to interface with Seurat >= 2.0 (https://satijalab.org/seurat/) \n\nDoubletFinder was published by Cell Systems in April, 2019: https://www.cell.com/cell-systems/fulltext/S2405-4712(19)30073-0\n\n## Updates\n\n(02/02/2025) Haibo Liu (Senior Bioinformatician at UMass, @haibol2016) added as maintainer after his much-needed improvement updates to the package.\n\n(11/21/2023) Made compatible with Seurat v5 and removed '_v3' flag from relevant function names.\n\n(03/31/2020) Internalized functions normally in 'modes' package to enable compatibility with R v3.6 and highger.\n\n(06/21/2019) Added parallelization to paramSweep_v3 (thanks NathanSkeen!) -- Note: progress no longer updated, but the process is much faster! Fixed bug with smaller datasets. Updated readme.\n\n(04/12/2019) Added SCTransform compatibilities to 'paramSweep_v3' and 'doubletFinder_v3'\n\n(04/08/2019) Added 'PCs' argument to 'doubletFinder', 'doubletFinder_v3', 'paramSweep', and 'paramSweep_v3' to avoid conflicts with dimension reduction preferences. Updated readme.\n\n(01/12/2019) Seurat V3 compatibility: 'doubletFinder_v3' and 'paramSweep_v3' functions added, other functions for parameter estimation remain compatible.  \n\n## DoubletFinder V2.0 (11/28/2018) \n\nNew Features:\n1. Increased computational efficiency during pANN computation\n2. Implemented strategy for determining optimal pK values for any scRNA-seq data using pN-pK parameter sweeps and mean-variance-normalized bimodality coefficient (BCmvn)\n3. Included vignette describing 'best-practices' for applying DoubletFinder to scRNA-seq data generated without sample multiplexing\n\n## Installation (in R/RStudio)\n\n```{r}\nremotes::install_github('chris-mcginnis-ucsf/DoubletFinder', force = TRUE)\n```\n\n## Dependencies\n\nDoubletFinder requires the following R packages: \n* Seurat (>= 2.0) \n* Matrix (1.2.14) \n* fields (9.6) \n* KernSmooth (2.23-15)\n* ROCR (1.0-7)\n* parallel (3.5.1)\n* NOTE: These package versions were used in the bioRxiv paper, but other versions may work, as well.\n\n## Frequently Asked Questions\n\nQuestion: What is my anticipated doublet rate? \nAnswer: This is dependent on your platform (10x, parse, etc.) and will vary with the number of input cells. It will not always be 7.5% as is used in the tutorial. This information is available in the user guides for each technology. See https://github.com/chris-mcginnis-ucsf/DoubletFinder/issues/76 and https://github.com/chris-mcginnis-ucsf/DoubletFinder/issues/156\n\nQuestion: Can I run DoubletFinder on merged data from multiple 10x lanes?\nAnswer: Technically yes but I would only do this if you were splitting the same sample across multiple lanes. You want to avoid instances where DoubletFinder is attempting to find doublets that do not actually exist in the data. I would also not advise running DF on integrated Seurat objects. See https://github.com/chris-mcginnis-ucsf/DoubletFinder/issues/101 \n\nQuestion: I see multiple potential pK values when visualizing BCmvn -- what should I do?\nAnswer: I would spot check the results in GEX space to see what makes the most sense given your understanding of the data. See https://github.com/chris-mcginnis-ucsf/DoubletFinder/issues/62 and https://github.com/chris-mcginnis-ucsf/DoubletFinder/issues/40\n\n# DoubletFinder Overview\n\nDoubletFinder can be broken up into 4 steps:\n\n(1) Generate artificial doublets from existing scRNA-seq data \n\n(2) Pre-process merged real-artificial data\n\n(3) Perform PCA and use the PC distance matrix to find each cell's proportion of artificial k nearest neighbors (pANN)\n\n(4) Rank order and threshold pANN values according to the expected number of doublets\n\n![alternativetext](DF.screenshots/DoubletFinderOverview.png)\n\nDoubletFinder takes the following arguments:\n\nseu ~ This is a fully-processed Seurat object (i.e., after NormalizeData, FindVariableGenes, ScaleData, RunPCA, and RunTSNE have all been run).\n\nPCs ~ The number of statistically-significant principal components, specified as a range (e.g., PCs = 1:10)\n\npN ~ This defines the number of generated artificial doublets, expressed as a proportion of the merged real-artificial data. Default is set to 25%, based on observation that DoubletFinder performance is largely pN-invariant (see McGinnis, Murrow and Gartner 2019, Cell Systems).\n\npK ~ This defines the PC neighborhood size used to compute pANN, expressed as a proportion of the merged real-artificial data. No default is set, as pK should be adjusted for each scRNA-seq dataset. Optimal pK values should be estimated using the strategy described below.\n\nnExp ~ This defines the pANN threshold used to make final doublet/singlet predictions. This value can best be estimated from cell loading densities into the 10X/Drop-Seq device, and adjusted according to the estimated proportion of homotypic doublets.\n\n## Application to Cell Hashing and Demuxlet data\n\nDoubletFinder successfully recapitulates ground-truth doublet classifications determined using antibody-barcode sample multiplexing (Cell Hashing) and SNP deconvolution (Demuxlet). DoubletFinder identifies false-negative Demuxlet classifications caused by doublets formed from cells with identical SNP profiles. DoubletFinder is insensitive to homotypic doublets -- i.e., doublets dervied from transcriptionally-similar cell states. \n\n![alternativetext](DF.screenshots/Results_Demux.png)\n![alternativetext](DF.screenshots/Results_Hashing.png)\n\n# 'Best-Practices' for scRNA-seq data generated without sample multiplexing\n\n## Input scRNA-seq Data\n\n* Do not apply DoubletFinder to aggregated scRNA-seq data representing multiple *distinct* samples (e.g., multiple 10X lanes). For example, if you run DoubletFinder on aggregated data representing WT and mutant cell lines sequenced across different 10X lanes, artificial doublets will be generated from WT and mutant cells, which cannot exist in your data. These artificial doublets will skew results. Notably, it is okay to run DoubletFinder on data generated by splitting a single sample across multiple 10X lanes. \n\n* Ensure that input data is cleared of low-quality cell clusters. There are a variety of ways to do this, but I usually use the following workflow:\n1. Manually threshold raw gene expression matrices according to RNA nUMIs (especially important when dealing with super-loaded 10X data because of the way CellRanger threholds data -- See Lun et al., 2019, Genome Biology.\n2. Pre-process data using standard workflow.\n3. Identify clusters with (A) low RNA UMIs, (B) High % mitochondrial reads, and/or (C) Uninformative marker genes.\n4. Remove clusters, pre-process again, and run DoubletFinder.\n\n## pK Selection\n\nROC analysis across pN-pK parameter sweeps for Cell Hashing and Demuxlet datasets demonstrate that DoubletFinder performance is largely invariant of pN value selection:\n\n![alternativetext](DF.screenshots/ParamSweep_Schematic.png)\n![alternativetext](DF.screenshots/ParamSweep_HeatMap.png)\n\nROC analysis across pN-pK parameter sweeps for simulated scRNA-seq data with (I) Variable numbers of cell states and (II) Variable magnitudes of transcriptional heterogeneity demonstrates that (I) Optimal pK value selection depends on the total number of cell states and (II) DoubletFinder performance suffers when applied to transcriptionally-homogenous data. Simulated data was generated using a strategy similar to as described in Wolock, Lopex & Klein 2019, Cell Systems.\n\n![alternativetext](DF.screenshots/Simulation_Schematic.png)\n![alternativetext](DF.screenshots/Results_Simulation.png)\n\nSimulated and sample-multiplexed data are unique in that ground-truth doublet classifications can be leveraged to characterize how DoubletFinder parameters must be 'fit' to distinct scRNA-seq datasets. However, doublets remain unknown in real-world contexts -- which is likely why you are interested in DoubletFinder, at all!\n\nTo maximize the accuracy of DoubletFinder predictions, we sought a ground-truth-agnostic metric that coincides with pK values that maximize AUC in Cell Hashing and Demuxlet data. Mean-variance normalized bimodality coefficient (BCmvn) achieves this goal, featuring a single, easily-discernible maximum at pK values that optimize AUC. \n\n![alternativetext](DF.screenshots/BCmvn.png)\n\nBCmvn distributions also feature a single maximum for scRNA-seq datasets generated without sample-multiplexing (e.g., Mouse pancreas, Byrnes et al., 2018, Nature Communcations; Mouse kidney, Park et al., 2018, Science), enabling pK selection.\n\n## Doublet Number Estimation\n\nDoubletFinder is sensitive to heterotypic doublets -- i.e., doublets formed from transcriptionally-distinct cell states -- but is insensitive to homotypic doublets -- i.e., doublets formed from transcriptionally-similar cell states. In our original manuscript, we suggested using DoubletFinder to predict the number of doublets expected from Poisson statistical estimates realting to the droplet microfluidics cell loading density. However, Poisson estimates are agnostic of homotypic doublets, and will thus invariably overestimate the number of *detectable* doublets.\n\nTo address this issue, we suggest users utilize literature-supported cell type annotations to model the proportion of homotypic doublets present in their data. As an example, we present an analysis of mouse kidney scRNA-seq data (Park et al., 2018, Science):\n\n![alternativetext](DF.screenshots/HomotypicAdjustment.png)\n\nNotably, it is conceivable that literature-suppoted cell type annotations may not accurately recapitulate the magnitude of transcriptional divergence necessary for DoubletFinder sensitivity. For example, nominally-homogenous cells (e.g., CD4+ T-cells) may exist along a spectrum of gene expression states (e.g., distinct anatomical locations, disease states, naive/Tregs/Th17 cells, etc.), and doublets formed by cell sub-types may be detectable by DoubletFinder. Thus, we consider doublet number estimates based on Poisson statistics with and without homotypic doublet proportion adjustment to 'bookend' the real detectable doublet rate. \n\n## Example code for 'real-world' applications\n\n```R\n## Pre-process Seurat object (standard) --------------------------------------------------------------------------------------\nseu_kidney <- CreateSeuratObject(kidney.data)\nseu_kidney <- NormalizeData(seu_kidney)\nseu_kidney <- FindVariableFeatures(seu_kidney, selection.method = \"vst\", nfeatures = 2000)\nseu_kidney <- ScaleData(seu_kidney)\nseu_kidney <- RunPCA(seu_kidney)\nseu_kidney <- RunUMAP(seu_kidney, dims = 1:10)\n\n## Pre-process Seurat object (sctransform) -----------------------------------------------------------------------------------\nseu_kidney <- CreateSeuratObject(kidney.data)\nseu_kidney <- SCTransform(seu_kidney)\nseu_kidney <- RunPCA(seu_kidney)\nseu_kidney <- RunUMAP(seu_kidney, dims = 1:10)\n\n## pK Identification (no ground-truth) ---------------------------------------------------------------------------------------\nsweep.res.list_kidney <- paramSweep(seu_kidney, PCs = 1:10, sct = FALSE)\nsweep.stats_kidney <- summarizeSweep(sweep.res.list_kidney, GT = FALSE)\nbcmvn_kidney <- find.pK(sweep.stats_kidney)\n\n## pK Identification (ground-truth) ------------------------------------------------------------------------------------------\nsweep.res.list_kidney <- paramSweep(seu_kidney, PCs = 1:10, sct = FALSE)\ngt.calls <- seu_kidney@meta.data[rownames(sweep.res.list_kidney[[1]]), \"GT\"].   ## GT is a vector containing \"Singlet\" and \"Doublet\" calls recorded using sample multiplexing classification and/or in silico geneotyping results \nsweep.stats_kidney <- summarizeSweep(sweep.res.list_kidney, GT = TRUE, GT.calls = gt.calls)\nbcmvn_kidney <- find.pK(sweep.stats_kidney)\n\n## Homotypic Doublet Proportion Estimate -------------------------------------------------------------------------------------\nhomotypic.prop <- modelHomotypic(annotations)           ## ex: annotations <- seu_kidney@meta.data$ClusteringResults\nnExp_poi <- round(0.075*nrow(seu_kidney@meta.data))  ## Assuming 7.5% doublet formation rate - tailor for your dataset\nnExp_poi.adj <- round(nExp_poi*(1-homotypic.prop))\n\n## Run DoubletFinder with varying classification stringencies ----------------------------------------------------------------\nseu_kidney <- doubletFinder(seu_kidney, PCs = 1:10, pN = 0.25, pK = 0.09, nExp = nExp_poi, reuse.pANN = NULL, sct = FALSE)\nseu_kidney <- doubletFinder(seu_kidney, PCs = 1:10, pN = 0.25, pK = 0.09, nExp = nExp_poi.adj, reuse.pANN = \"pANN_0.25_0.09_913\", sct = FALSE)\n```\n\n![alternativetext](DF.screenshots/DFkidney_low.vs.high.png)\n\n## Other Doublet Detection Methods\n[Scrublet (Py)](https://github.com/AllonKleinLab/scrublet)\n[DoubletDecon (R)](https://github.com/EDePasquale/DoubletDecon)\n[DoubletDetection (Py)](https://github.com/JonathanShor/DoubletDetection)\n[Solo (Py)](https://github.com/calico/solo)\n[scds (R)](https://github.com/kostkalab/scds)\n[scDblFinder (R)](https://github.com/plger/scDblFinder)\n\n## References\n\n1.\tStoeckius M, Zheng S, Houck-Loomis B, Hao S, Yeung BZ, Smibert P, Satija R. Cell Hashing with barcoded antibodies enables multiplexing and doublet detection for single cell genomics. Genome Biology. 2018. 19:224.\n\n2.  Kang HM, Subramaniam M, Targ S, Nguyen M, Maliskova L, McCarthy E, Wan E, Wong S, Byrnes L, Lanata CM, Gate RE, Mostafavi S, Marson A, Zaitlen N, Criswell LA, Ye JC. Multiplexed droplet single-cell RNA-sequencing using natural genetic variation. Nature Biotechnology. 2018. 36(1):89-94. \n\n3.  Wolock SL, Lopez R, Klein AM. Scrublet: Computational Identification of Cell Doublets in Single-Cell Transcriptomic Data. Cell Systems. 2019. 8(4):281-291.e9.\n\n4.  Park J, Shrestha R, Qiu C, Kondo A, Huang S, Werth M, Li M, Barasch J, SusztÃ¡k K. Single-cell transcriptomics of the mouse kidney reveals potential cellular targets of kidney disease. Science. 2018. 360(6390):758-63.\n\n5.  Byrnes LE, Wong DM, Subramaniam M, Meyer NP, Gilchrist CL, Knox SM, Tward AD, Ye CJ, Sneddon JB. Lineage dynamics of murine pancreatic development at single-cell resolution. Nature Communications. 2018; 9:3922.\n\n6.  Bais AS, Kostka D. scds: computational annotation of doublets in single-cell RNA sequencing data. Bioinformatics. 2020. 36(4):1150-8.\n\n7.  Bernstein NJ, Fong NL, Lam I, Roy MA, Hendrickson DG, Kelley DR. Solo: Doublet Identification in Single-Cell RNA-Seq via Semi-Supervised Deep Learning. Cell Systems. 2020. S2405-4712(20)30195-2.\n\n8.  DePasquale EAK, Schnell DJ, Van Camp PJ, Valiente-Alandi I, Blaxall BC, Grimes HL, Singh H, Salomonis N. DoubletDecon: Deconvoluting Doublets from Single-Cell RNA-Sequencing Data. Cell Reports. 2019. 29(6):1718-27.e8.\n",
      "stars_today": 2
    },
    {
      "id": 13628513,
      "name": "logrus",
      "full_name": "sirupsen/logrus",
      "description": "Structured, pluggable logging for Go.",
      "html_url": "https://github.com/sirupsen/logrus",
      "stars": 25672,
      "forks": 2276,
      "language": "Go",
      "topics": [
        "go",
        "logging",
        "logrus"
      ],
      "created_at": "2013-10-16T19:08:55Z",
      "updated_at": "2026-01-28T00:27:24Z",
      "pushed_at": "2026-01-27T10:41:01Z",
      "open_issues": 72,
      "owner": {
        "login": "sirupsen",
        "avatar_url": "https://avatars.githubusercontent.com/u/97400?v=4"
      },
      "readme": "# Logrus <img src=\"http://i.imgur.com/hTeVwmJ.png\" width=\"40\" height=\"40\" alt=\":walrus:\" class=\"emoji\" title=\":walrus:\"/> [![Build Status](https://github.com/sirupsen/logrus/workflows/CI/badge.svg)](https://github.com/sirupsen/logrus/actions?query=workflow%3ACI) [![Go Reference](https://pkg.go.dev/badge/github.com/sirupsen/logrus.svg)](https://pkg.go.dev/github.com/sirupsen/logrus)\n\nLogrus is a structured logger for Go (golang), completely API compatible with\nthe standard library logger.\n\n**Logrus is in maintenance-mode.** We will not be introducing new features. It's\nsimply too hard to do in a way that won't break many people's projects, which is\nthe last thing you want from your Logging library (again...).\n\nThis does not mean Logrus is dead. Logrus will continue to be maintained for\nsecurity, (backwards compatible) bug fixes, and performance (where we are\nlimited by the interface).\n\nI believe Logrus' biggest contribution is to have played a part in today's\nwidespread use of structured logging in Golang. There doesn't seem to be a\nreason to do a major, breaking iteration into Logrus V2, since the fantastic Go\ncommunity has built those independently. Many fantastic alternatives have sprung\nup. Logrus would look like those, had it been re-designed with what we know\nabout structured logging in Go today. Check out, for example,\n[Zerolog][zerolog], [Zap][zap], and [Apex][apex].\n\n[zerolog]: https://github.com/rs/zerolog\n[zap]: https://github.com/uber-go/zap\n[apex]: https://github.com/apex/log\n\n**Seeing weird case-sensitive problems?** It's in the past been possible to\nimport Logrus as both upper- and lower-case. Due to the Go package environment,\nthis caused issues in the community and we needed a standard. Some environments\nexperienced problems with the upper-case variant, so the lower-case was decided.\nEverything using `logrus` will need to use the lower-case:\n`github.com/sirupsen/logrus`. Any package that isn't, should be changed.\n\nTo fix Glide, see [these\ncomments](https://github.com/sirupsen/logrus/issues/553#issuecomment-306591437).\nFor an in-depth explanation of the casing issue, see [this\ncomment](https://github.com/sirupsen/logrus/issues/570#issuecomment-313933276).\n\nNicely color-coded in development (when a TTY is attached, otherwise just\nplain text):\n\n![Colored](http://i.imgur.com/PY7qMwd.png)\n\nWith `logrus.SetFormatter(&logrus.JSONFormatter{})`, for easy parsing by logstash\nor Splunk:\n\n```text\n{\"animal\":\"walrus\",\"level\":\"info\",\"msg\":\"A group of walrus emerges from the\nocean\",\"size\":10,\"time\":\"2014-03-10 19:57:38.562264131 -0400 EDT\"}\n\n{\"level\":\"warning\",\"msg\":\"The group's number increased tremendously!\",\n\"number\":122,\"omg\":true,\"time\":\"2014-03-10 19:57:38.562471297 -0400 EDT\"}\n\n{\"animal\":\"walrus\",\"level\":\"info\",\"msg\":\"A giant walrus appears!\",\n\"size\":10,\"time\":\"2014-03-10 19:57:38.562500591 -0400 EDT\"}\n\n{\"animal\":\"walrus\",\"level\":\"info\",\"msg\":\"Tremendously sized cow enters the ocean.\",\n\"size\":9,\"time\":\"2014-03-10 19:57:38.562527896 -0400 EDT\"}\n\n{\"level\":\"fatal\",\"msg\":\"The ice breaks!\",\"number\":100,\"omg\":true,\n\"time\":\"2014-03-10 19:57:38.562543128 -0400 EDT\"}\n```\n\nWith the default `logrus.SetFormatter(&logrus.TextFormatter{})` when a TTY is not\nattached, the output is compatible with the\n[logfmt](https://pkg.go.dev/github.com/kr/logfmt) format:\n\n```text\ntime=\"2015-03-26T01:27:38-04:00\" level=debug msg=\"Started observing beach\" animal=walrus number=8\ntime=\"2015-03-26T01:27:38-04:00\" level=info msg=\"A group of walrus emerges from the ocean\" animal=walrus size=10\ntime=\"2015-03-26T01:27:38-04:00\" level=warning msg=\"The group's number increased tremendously!\" number=122 omg=true\ntime=\"2015-03-26T01:27:38-04:00\" level=debug msg=\"Temperature changes\" temperature=-4\ntime=\"2015-03-26T01:27:38-04:00\" level=panic msg=\"It's over 9000!\" animal=orca size=9009\ntime=\"2015-03-26T01:27:38-04:00\" level=fatal msg=\"The ice breaks!\" err=&{0x2082280c0 map[animal:orca size:9009] 2015-03-26 01:27:38.441574009 -0400 EDT panic It's over 9000!} number=100 omg=true\n```\nTo ensure this behaviour even if a TTY is attached, set your formatter as follows:\n\n```go\nlogrus.SetFormatter(&logrus.TextFormatter{\n    DisableColors: true,\n    FullTimestamp: true,\n})\n```\n\n#### Logging Method Name\n\nIf you wish to add the calling method as a field, instruct the logger via:\n\n```go\nlogrus.SetReportCaller(true)\n```\nThis adds the caller as 'method' like so:\n\n```json\n{\"animal\":\"penguin\",\"level\":\"fatal\",\"method\":\"github.com/sirupsen/arcticcreatures.migrate\",\"msg\":\"a penguin swims by\",\n\"time\":\"2014-03-10 19:57:38.562543129 -0400 EDT\"}\n```\n\n```text\ntime=\"2015-03-26T01:27:38-04:00\" level=fatal method=github.com/sirupsen/arcticcreatures.migrate msg=\"a penguin swims by\" animal=penguin\n```\nNote that this does add measurable overhead - the cost will depend on the version of Go, but is\nbetween 20 and 40% in recent tests with 1.6 and 1.7.  You can validate this in your\nenvironment via benchmarks:\n\n```bash\ngo test -bench=.*CallerTracing\n```\n\n#### Case-sensitivity\n\nThe organization's name was changed to lower-case--and this will not be changed\nback. If you are getting import conflicts due to case sensitivity, please use\nthe lower-case import: `github.com/sirupsen/logrus`.\n\n#### Example\n\nThe simplest way to use Logrus is simply the package-level exported logger:\n\n```go\npackage main\n\nimport \"github.com/sirupsen/logrus\"\n\nfunc main() {\n  logrus.WithFields(logrus.Fields{\n    \"animal\": \"walrus\",\n  }).Info(\"A walrus appears\")\n}\n```\n\nNote that it's completely api-compatible with the stdlib logger, so you can\nreplace your `log` imports everywhere with `log \"github.com/sirupsen/logrus\"`\nand you'll now have the flexibility of Logrus. You can customize it all you\nwant:\n\n```go\npackage main\n\nimport (\n  \"os\"\n\n  log \"github.com/sirupsen/logrus\"\n)\n\nfunc init() {\n  // Log as JSON instead of the default ASCII formatter.\n  log.SetFormatter(&log.JSONFormatter{})\n\n  // Output to stdout instead of the default stderr\n  // Can be any io.Writer, see below for File example\n  log.SetOutput(os.Stdout)\n\n  // Only log the warning severity or above.\n  log.SetLevel(log.WarnLevel)\n}\n\nfunc main() {\n  log.WithFields(log.Fields{\n    \"animal\": \"walrus\",\n    \"size\":   10,\n  }).Info(\"A group of walrus emerges from the ocean\")\n\n  log.WithFields(log.Fields{\n    \"omg\":    true,\n    \"number\": 122,\n  }).Warn(\"The group's number increased tremendously!\")\n\n  log.WithFields(log.Fields{\n    \"omg\":    true,\n    \"number\": 100,\n  }).Fatal(\"The ice breaks!\")\n\n  // A common pattern is to re-use fields between logging statements by re-using\n  // the logrus.Entry returned from WithFields()\n  contextLogger := log.WithFields(log.Fields{\n    \"common\": \"this is a common field\",\n    \"other\": \"I also should be logged always\",\n  })\n\n  contextLogger.Info(\"I'll be logged with common and other field\")\n  contextLogger.Info(\"Me too\")\n}\n```\n\nFor more advanced usage such as logging to multiple locations from the same\napplication, you can also create an instance of the `logrus` Logger:\n\n```go\npackage main\n\nimport (\n  \"os\"\n\n  \"github.com/sirupsen/logrus\"\n)\n\n// Create a new instance of the logger. You can have any number of instances.\nvar logger = logrus.New()\n\nfunc main() {\n  // The API for setting attributes is a little different than the package level\n  // exported logger. See Godoc. \n  logger.Out = os.Stdout\n\n  // You could set this to any `io.Writer` such as a file\n  // file, err := os.OpenFile(\"logrus.log\", os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666)\n  // if err == nil {\n  //  logger.Out = file\n  // } else {\n  //  logger.Info(\"Failed to log to file, using default stderr\")\n  // }\n\n  logger.WithFields(logrus.Fields{\n    \"animal\": \"walrus\",\n    \"size\":   10,\n  }).Info(\"A group of walrus emerges from the ocean\")\n}\n```\n\n#### Fields\n\nLogrus encourages careful, structured logging through logging fields instead of\nlong, unparseable error messages. For example, instead of: `logrus.Fatalf(\"Failed\nto send event %s to topic %s with key %d\")`, you should log the much more\ndiscoverable:\n\n```go\nlogrus.WithFields(logrus.Fields{\n  \"event\": event,\n  \"topic\": topic,\n  \"key\": key,\n}).Fatal(\"Failed to send event\")\n```\n\nWe've found this API forces you to think about logging in a way that produces\nmuch more useful logging messages. We've been in countless situations where just\na single added field to a log statement that was already there would've saved us\nhours. The `WithFields` call is optional.\n\nIn general, with Logrus using any of the `printf`-family functions should be\nseen as a hint you should add a field, however, you can still use the\n`printf`-family functions with Logrus.\n\n#### Default Fields\n\nOften it's helpful to have fields _always_ attached to log statements in an\napplication or parts of one. For example, you may want to always log the\n`request_id` and `user_ip` in the context of a request. Instead of writing\n`logger.WithFields(logrus.Fields{\"request_id\": request_id, \"user_ip\": user_ip})` on\nevery line, you can create a `logrus.Entry` to pass around instead:\n\n```go\nrequestLogger := logger.WithFields(logrus.Fields{\"request_id\": request_id, \"user_ip\": user_ip})\nrequestLogger.Info(\"something happened on that request\") // will log request_id and user_ip\nrequestLogger.Warn(\"something not great happened\")\n```\n\n#### Hooks\n\nYou can add hooks for logging levels. For example to send errors to an exception\ntracking service on `Error`, `Fatal` and `Panic`, info to StatsD or log to\nmultiple places simultaneously, e.g. syslog.\n\nLogrus comes with [built-in hooks](hooks/). Add those, or your custom hook, in\n`init`:\n\n```go\npackage main\n\nimport (\n  \"log/syslog\"\n\n  \"github.com/sirupsen/logrus\"\n  airbrake \"gopkg.in/gemnasium/logrus-airbrake-hook.v2\"\n  logrus_syslog \"github.com/sirupsen/logrus/hooks/syslog\"\n)\n\nfunc init() {\n\n  // Use the Airbrake hook to report errors that have Error severity or above to\n  // an exception tracker. You can create custom hooks, see the Hooks section.\n  logrus.AddHook(airbrake.NewHook(123, \"xyz\", \"production\"))\n\n  hook, err := logrus_syslog.NewSyslogHook(\"udp\", \"localhost:514\", syslog.LOG_INFO, \"\")\n  if err != nil {\n    logrus.Error(\"Unable to connect to local syslog daemon\")\n  } else {\n    logrus.AddHook(hook)\n  }\n}\n```\nNote: Syslog hooks also support connecting to local syslog (Ex. \"/dev/log\" or \"/var/run/syslog\" or \"/var/run/log\"). For the detail, please check the [syslog hook README](hooks/syslog/README.md).\n\nA list of currently known service hooks can be found in this wiki [page](https://github.com/sirupsen/logrus/wiki/Hooks)\n\n\n#### Level logging\n\nLogrus has seven logging levels: Trace, Debug, Info, Warning, Error, Fatal and Panic.\n\n```go\nlogrus.Trace(\"Something very low level.\")\nlogrus.Debug(\"Useful debugging information.\")\nlogrus.Info(\"Something noteworthy happened!\")\nlogrus.Warn(\"You should probably take a look at this.\")\nlogrus.Error(\"Something failed but I'm not quitting.\")\n// Calls os.Exit(1) after logging\nlogrus.Fatal(\"Bye.\")\n// Calls panic() after logging\nlogrus.Panic(\"I'm bailing.\")\n```\n\nYou can set the logging level on a `Logger`, then it will only log entries with\nthat severity or anything above it:\n\n```go\n// Will log anything that is info or above (warn, error, fatal, panic). Default.\nlogrus.SetLevel(logrus.InfoLevel)\n```\n\nIt may be useful to set `logrus.Level = logrus.DebugLevel` in a debug or verbose\nenvironment if your application has that.\n\nNote: If you want different log levels for global (`logrus.SetLevel(...)`) and syslog logging, please check the [syslog hook README](hooks/syslog/README.md#different-log-levels-for-local-and-remote-logging).\n\n#### Entries\n\nBesides the fields added with `WithField` or `WithFields` some fields are\nautomatically added to all logging events:\n\n1. `time`. The timestamp when the entry was created.\n2. `msg`. The logging message passed to `{Info,Warn,Error,Fatal,Panic}` after\n   the `AddFields` call. E.g. `Failed to send event.`\n3. `level`. The logging level. E.g. `info`.\n\n#### Environments\n\nLogrus has no notion of environment.\n\nIf you wish for hooks and formatters to only be used in specific environments,\nyou should handle that yourself. For example, if your application has a global\nvariable `Environment`, which is a string representation of the environment you\ncould do:\n\n```go\nimport (\n  \"github.com/sirupsen/logrus\"\n)\n\nfunc init() {\n  // do something here to set environment depending on an environment variable\n  // or command-line flag\n  if Environment == \"production\" {\n    logrus.SetFormatter(&logrus.JSONFormatter{})\n  } else {\n    // The TextFormatter is default, you don't actually have to do this.\n    logrus.SetFormatter(&logrus.TextFormatter{})\n  }\n}\n```\n\nThis configuration is how `logrus` was intended to be used, but JSON in\nproduction is mostly only useful if you do log aggregation with tools like\nSplunk or Logstash.\n\n#### Formatters\n\nThe built-in logging formatters are:\n\n* `logrus.TextFormatter`. Logs the event in colors if stdout is a tty, otherwise\n  without colors.\n  * *Note:* to force colored output when there is no TTY, set the `ForceColors`\n    field to `true`.  To force no colored output even if there is a TTY  set the\n    `DisableColors` field to `true`. For Windows, see\n    [github.com/mattn/go-colorable](https://github.com/mattn/go-colorable).\n  * When colors are enabled, levels are truncated to 4 characters by default. To disable\n    truncation set the `DisableLevelTruncation` field to `true`.\n  * When outputting to a TTY, it's often helpful to visually scan down a column where all the levels are the same width. Setting the `PadLevelText` field to `true` enables this behavior, by adding padding to the level text.\n  * All options are listed in the [generated docs](https://pkg.go.dev/github.com/sirupsen/logrus#TextFormatter).\n* `logrus.JSONFormatter`. Logs fields as JSON.\n  * All options are listed in the [generated docs](https://pkg.go.dev/github.com/sirupsen/logrus#JSONFormatter).\n\nThird-party logging formatters:\n\n* [`FluentdFormatter`](https://github.com/joonix/log). Formats entries that can be parsed by Kubernetes and Google Container Engine.\n* [`GELF`](https://github.com/fabienm/go-logrus-formatters). Formats entries so they comply to Graylog's [GELF 1.1 specification](http://docs.graylog.org/en/2.4/pages/gelf.html).\n* [`logstash`](https://github.com/bshuster-repo/logrus-logstash-hook). Logs fields as [Logstash](http://logstash.net) Events.\n* [`prefixed`](https://github.com/x-cray/logrus-prefixed-formatter). Displays log entry source along with alternative layout.\n* [`zalgo`](https://github.com/aybabtme/logzalgo). Invoking the Power of Zalgo.\n* [`nested-logrus-formatter`](https://github.com/antonfisher/nested-logrus-formatter). Converts logrus fields to a nested structure.\n* [`powerful-logrus-formatter`](https://github.com/zput/zxcTool). get fileName, log's line number and the latest function's name when print log; Save log to files.\n* [`caption-json-formatter`](https://github.com/nolleh/caption_json_formatter). logrus's message json formatter with human-readable caption added.\n\nYou can define your formatter by implementing the `Formatter` interface,\nrequiring a `Format` method. `Format` takes an `*Entry`. `entry.Data` is a\n`Fields` type (`map[string]interface{}`) with all your fields as well as the\ndefault ones (see Entries section above):\n\n```go\ntype MyJSONFormatter struct{}\n\nlogrus.SetFormatter(new(MyJSONFormatter))\n\nfunc (f *MyJSONFormatter) Format(entry *Entry) ([]byte, error) {\n  // Note this doesn't include Time, Level and Message which are available on\n  // the Entry. Consult `godoc` on information about those fields or read the\n  // source of the official loggers.\n  serialized, err := json.Marshal(entry.Data)\n    if err != nil {\n      return nil, fmt.Errorf(\"Failed to marshal fields to JSON, %w\", err)\n    }\n  return append(serialized, '\\n'), nil\n}\n```\n\n#### Logger as an `io.Writer`\n\nLogrus can be transformed into an `io.Writer`. That writer is the end of an `io.Pipe` and it is your responsibility to close it.\n\n```go\nw := logger.Writer()\ndefer w.Close()\n\nsrv := http.Server{\n    // create a stdlib log.Logger that writes to\n    // logrus.Logger.\n    ErrorLog: log.New(w, \"\", 0),\n}\n```\n\nEach line written to that writer will be printed the usual way, using formatters\nand hooks. The level for those entries is `info`.\n\nThis means that we can override the standard library logger easily:\n\n```go\nlogger := logrus.New()\nlogger.Formatter = &logrus.JSONFormatter{}\n\n// Use logrus for standard log output\n// Note that `log` here references stdlib's log\n// Not logrus imported under the name `log`.\nlog.SetOutput(logger.Writer())\n```\n\n#### Rotation\n\nLog rotation is not provided with Logrus. Log rotation should be done by an\nexternal program (like `logrotate(8)`) that can compress and delete old log\nentries. It should not be a feature of the application-level logger.\n\n#### Tools\n\n| Tool | Description |\n| ---- | ----------- |\n|[Logrus Mate](https://github.com/gogap/logrus_mate)|Logrus mate is a tool for Logrus to manage loggers, you can initial logger's level, hook and formatter by config file, the logger will be generated with different configs in different environments.|\n|[Logrus Viper Helper](https://github.com/heirko/go-contrib/tree/master/logrusHelper)|An Helper around Logrus to wrap with spf13/Viper to load configuration with fangs! And to simplify Logrus configuration use some behavior of [Logrus Mate](https://github.com/gogap/logrus_mate). [sample](https://github.com/heirko/iris-contrib/blob/master/middleware/logrus-logger/example) |\n\n#### Testing\n\nLogrus has a built-in facility for asserting the presence of log messages. This is implemented through the `test` hook and provides:\n\n* decorators for existing logger (`test.NewLocal` and `test.NewGlobal`) which basically just adds the `test` hook\n* a test logger (`test.NewNullLogger`) that just records log messages (and does not output any):\n\n```go\nimport(\n  \"testing\"\n\n  \"github.com/sirupsen/logrus\"\n  \"github.com/sirupsen/logrus/hooks/test\"\n  \"github.com/stretchr/testify/assert\"\n)\n\nfunc TestSomething(t*testing.T){\n  logger, hook := test.NewNullLogger()\n  logger.Error(\"Helloerror\")\n\n  assert.Equal(t, 1, len(hook.Entries))\n  assert.Equal(t, logrus.ErrorLevel, hook.LastEntry().Level)\n  assert.Equal(t, \"Helloerror\", hook.LastEntry().Message)\n\n  hook.Reset()\n  assert.Nil(t, hook.LastEntry())\n}\n```\n\n#### Fatal handlers\n\nLogrus can register one or more functions that will be called when any `fatal`\nlevel message is logged. The registered handlers will be executed before\nlogrus performs an `os.Exit(1)`. This behavior may be helpful if callers need\nto gracefully shut down. Unlike a `panic(\"Something went wrong...\")` call which can be intercepted with a deferred `recover` a call to `os.Exit(1)` can not be intercepted.\n\n```go\n// ...\nhandler := func() {\n  // gracefully shut down something...\n}\nlogrus.RegisterExitHandler(handler)\n// ...\n```\n\n#### Thread safety\n\nBy default, Logger is protected by a mutex for concurrent writes. The mutex is held when calling hooks and writing logs.\nIf you are sure such locking is not needed, you can call logger.SetNoLock() to disable the locking.\n\nSituations when locking is not needed include:\n\n* You have no hooks registered, or hooks calling is already thread-safe.\n\n* Writing to logger.Out is already thread-safe, for example:\n\n  1) logger.Out is protected by locks.\n\n  2) logger.Out is an os.File handler opened with `O_APPEND` flag, and every write is smaller than 4k. (This allows multi-thread/multi-process writing)\n\n     (Refer to http://www.notthewizard.com/2014/06/17/are-files-appends-really-atomic/)\n",
      "stars_today": 1
    },
    {
      "id": 35732214,
      "name": "SwiftLint",
      "full_name": "realm/SwiftLint",
      "description": "A tool to enforce Swift style and conventions.",
      "html_url": "https://github.com/realm/SwiftLint",
      "stars": 19415,
      "forks": 2279,
      "language": "Swift",
      "topics": [
        "code-quality",
        "hacktoberfest",
        "linter",
        "linting",
        "static-analysis",
        "swift"
      ],
      "created_at": "2015-05-16T16:59:31Z",
      "updated_at": "2026-01-27T21:24:05Z",
      "pushed_at": "2026-01-26T21:06:40Z",
      "open_issues": 473,
      "owner": {
        "login": "realm",
        "avatar_url": "https://avatars.githubusercontent.com/u/7575099?v=4"
      },
      "readme": "# SwiftLint\n\nA tool to enforce Swift style and conventions, loosely based on the now\narchived [GitHub Swift Style Guide](https://github.com/github/swift-style-guide).\nSwiftLint enforces the style guide rules that are generally accepted by the\nSwift community. These rules are well described in popular style guides like\n[Kodeco's Swift Style Guide](https://github.com/kodecocodes/swift-style-guide).\n\nSwiftLint rules are predominantly based on [SwiftSyntax](https://github.com/swiftlang/swift-syntax).\nSome rules still hook into [Clang](http://clang.llvm.org) and\n[SourceKit](http://www.jpsim.com/uncovering-sourcekit) to access type information.\n\n[![Supported Swift Versions](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Frealm%2FSwiftLint%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/realm/SwiftLint)\n[![Supported Platforms](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Frealm%2FSwiftLint%2Fbadge%3Ftype%3Dplatforms)](https://swiftpackageindex.com/realm/SwiftLint)\n[![Buildkite Build Status](https://badge.buildkite.com/e2a5bc32c347e76e2793e4c5764a5f42bcd42bbe32f79c3a53.svg?branch=main)](https://buildkite.com/swiftlint/swiftlint)\n\n![SwiftLint violations highlighted in the Xcode editor](https://raw.githubusercontent.com/realm/SwiftLint/main/assets/screenshot.png)\n\nThis project adheres to the\n[Contributor Covenant Code of Conduct](https://realm.io/conduct).\nBy participating, you are expected to uphold this code.\n\n> Switch Language:\n> [ä¸­æ–‡](https://github.com/realm/SwiftLint/blob/main/README_CN.md),\n> [í•œêµ­ì–´](https://github.com/realm/SwiftLint/blob/main/README_KR.md)\n\n## Video Introduction\n\nTo get a high-level overview of SwiftLint, we encourage you to watch this\npresentation recorded January 9th, 2017 by JP Simard (transcript provided):\n\n[![Presentation](https://raw.githubusercontent.com/realm/SwiftLint/main/assets/presentation.svg)](https://youtu.be/9Z1nTMTejqU)\n\n## Installation\n\n### [Swift Package Manager](https://github.com/apple/swift-package-manager)\n\nSwiftLint can be used as a [command plugin](#swift-package-command-plugin)\nor a [build tool plugin](#build-tool-plugins).\n\nAdd\n\n```swift\n.package(url: \"https://github.com/SimplyDanny/SwiftLintPlugins\", from: \"<version>\")\n```\n\nto your `Package.swift` file to consume the latest release of SwiftLint\nautomatically or pin the dependency to a specific version:\n\n```swift\n.package(url: \"https://github.com/SimplyDanny/SwiftLintPlugins\", exact: \"<version>\")\n```\n\nTherein, replace `<version>` with the desired minimum or exact version.\n\n> [!NOTE]\n> Consuming the plugins directly from the SwiftLint repository comes\n> with several drawbacks. To avoid them and reduce the overhead imposed, it's\n> highly recommended to consume the plugins from the dedicated\n> [SwiftLintPlugins repository](https://github.com/SimplyDanny/SwiftLintPlugins),\n> even though plugins from the SwiftLint repository are also absolutely\n> functional. If the plugins from SwiftLint are preferred, just use the URL\n> `https://github.com/realm/SwiftLint` in the package declarations above.\n>\n> However, [SwiftLintPlugins](https://github.com/SimplyDanny/SwiftLintPlugins)\n> facilitates plugin adoption massively. It lists some of the reasons that\n> drive the plugins as provided by SwiftLint itself very troublesome. Since\n> the plugin code and the releases are kept in sync, there is no difference\n> in functionality between the two, but you spare yourself a lot of time and\n> trouble using the dedicated plugins repository.\n>\n> This document assumes you're relying on SwiftLintPlugins.\n\n### [Xcode Package Dependency](https://developer.apple.com/documentation/xcode/adding-package-dependencies-to-your-app)\n\nUse the following link to add SwiftLint as a Package Dependency to an Xcode\nproject:\n\n```bash\nhttps://github.com/SimplyDanny/SwiftLintPlugins\n```\n\n### [Homebrew](http://brew.sh)\n\n```bash\nbrew install swiftlint\n```\n\n### [CocoaPods](https://cocoapods.org)\n\nAdd the following to your `Podfile`:\n\n```ruby\npod 'SwiftLint'\n```\n\nThis will download the SwiftLint binaries and dependencies in `Pods/` during\nyour next `pod install` execution and will allow you to invoke it via\n`${PODS_ROOT}/SwiftLint/swiftlint` in your Script Build Phases.\n\nInstalling via Cocoapods also enables pinning to a specific version of\nSwiftLint rather than simply the latest (which is the case with\n[Homebrew](#homebrew)).\n\nNote that this will add the SwiftLint binaries, its dependencies' binaries, and\nthe Swift binary library distribution to the `Pods/` directory, so checking in\nthis directory to SCM such as Git is discouraged.\n\n### [Mint](https://github.com/yonaskolb/mint)\n\n```bash\nmint install realm/SwiftLint\n```\n\n### [Bazel](https://bazel.build)\n\nPut this in your `MODULE.bazel`:\n\n```bzl\nbazel_dep(name = \"swiftlint\", version = \"0.52.4\", repo_name = \"SwiftLint\")\n```\n\nOr put this in your `WORKSPACE`:\n\n<details>\n\n<summary>WORKSPACE</summary>\n\n```bzl\nload(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\n\nhttp_archive(\n    name = \"build_bazel_rules_apple\",\n    sha256 = \"390841dd5f8a85fc25776684f4793d56e21b098dfd7243cd145b9831e6ef8be6\",\n    url = \"https://github.com/bazelbuild/rules_apple/releases/download/2.4.1/rules_apple.2.4.1.tar.gz\",\n)\n\nload(\n    \"@build_bazel_rules_apple//apple:repositories.bzl\",\n    \"apple_rules_dependencies\",\n)\n\napple_rules_dependencies()\n\nload(\n    \"@build_bazel_rules_swift//swift:repositories.bzl\",\n    \"swift_rules_dependencies\",\n)\n\nswift_rules_dependencies()\n\nload(\n    \"@build_bazel_rules_swift//swift:extras.bzl\",\n    \"swift_rules_extra_dependencies\",\n)\n\nswift_rules_extra_dependencies()\n\nhttp_archive(\n    name = \"SwiftLint\",\n    sha256 = \"c6ea58b9c72082cdc1ada4a2d48273ecc355896ed72204cedcc586b6ccb8aca6\",\n    url = \"https://github.com/realm/SwiftLint/releases/download/0.52.4/bazel.tar.gz\",\n)\n\nload(\"@SwiftLint//bazel:repos.bzl\", \"swiftlint_repos\")\n\nswiftlint_repos()\n\nload(\"@SwiftLint//bazel:deps.bzl\", \"swiftlint_deps\")\n\nswiftlint_deps()\n```\n\n</details>\n\nThen you can run SwiftLint in the current directory with this command:\n\n```console\nbazel run -c opt @SwiftLint//:swiftlint\n```\n\n### Pre-Built Package\n\nDownload `SwiftLint.pkg` from the\n[latest GitHub release](https://github.com/realm/SwiftLint/releases/latest) and\nrun it.\n\n### From Source\n\nMake sure the build tool [Bazel](https://bazel.build) and a\nrecent [Swift toolchain](https://www.swift.org/download/) are\ninstalled and all tools are discoverable in your `PATH`.\n\nTo build SwiftLint, clone this repository and run `make install`.\n\n## Setup\n\n> [!IMPORTANT]\n> While it may seem intuitive to run SwiftLint before compiling Swift source\n> files to exit a build early when there are lint violations, it is important\n> to understand that SwiftLint is designed to analyze valid source code that\n> is compilable. Non-compiling code can very easily lead to unexpected and\n> confusing results, especially when executing with `--fix`/`--autocorrect`\n> command line arguments.\n\n### Build Tool Plugins\n\nSwiftLint can be used as a build tool plugin for both\n[Swift Package projects](#swift-package-projects)\nand [Xcode projects](#xcode-projects).\n\nThe build tool plugin determines the SwiftLint working directory by locating\nthe topmost config file within the package/project directory. If a config file\nis not found therein, the package/project directory is used as the working\ndirectory.\n\nThe plugin throws an error when it is unable to resolve the SwiftLint working\ndirectory. For example, this will occur in Xcode projects where the target's\nSwift files are not located within the project directory.\n\nTo maximize compatibility with the plugin, avoid project structures that require\nthe use of the `--config` option.\n\n### Swift Package Projects\n\n> [!NOTE]\n> Requires installing via [Swift Package Manager](#swift-package-manager).\n\nBuild tool plugins run when building each target. When a project has multiple\ntargets, the plugin must be added to the desired targets individually.\n\nTo do this, add the plugin to the target(s) to be linted as follows:\n\n```swift\n.target(\n    ...\n    plugins: [.plugin(name: \"SwiftLintBuildToolPlugin\", package: \"SwiftLintPlugins\")]\n),\n```\n\n### Swift Package Command Plugin\n\n> [!NOTE]\n> Requires installing via [Swift Package Manager](#swift-package-manager).\n\nThe command plugin enables running SwiftLint from the command line as follows:\n\n```shell\nswift package plugin swiftlint\n```\n\n### Xcode Projects\n\n> [!NOTE]\n> Requires installing via [Xcode Package Dependency](#xcode-package-dependency).\n\nBuild tool plugins run as a build phase of each target. When a project has\nmultiple targets, the plugin must be added to the desired targets individually.\n\nTo do this, add the `SwiftLintBuildToolPlugin` to the `Run Build Tool Plug-ins`\nphase of the `Build Phases` for the target(s) to be linted.\n\n> [!TIP]\n> When using the plugin for the first time, be sure to trust and enable\n> it when prompted. If a macros build warning exists, select it to trust\n> and enable the macros as well.\n\nFor unattended use (e.g. on CI), package plugin and macro\nvalidations can be disabled with either of the following:\n\n* Using `xcodebuild` options:\n\n  ```bash\n  -skipPackagePluginValidation\n  -skipMacroValidation\n  ```\n\n* Setting Xcode defaults:\n\n  ```bash\n  defaults write com.apple.dt.Xcode IDESkipPackagePluginFingerprintValidatation -bool YES\n  defaults write com.apple.dt.Xcode IDESkipMacroFingerprintValidation -bool YES\n  ```\n\n> [!IMPORTANT]\n> The unattended use options bypass Xcode's validation dialogs\n> and implicitly trust all plugins and macros, which has security implications.\n\n#### Unexpected Xcode Project Structures\n\nProject structures where SwiftLint's configuration file is located\noutside of the package/project directory are not directly supported\nby the build tool plugin. This is because it isn't possible to pass\narguments to build tool plugins (e.g., passing the config file path).\n\nIf your project structure doesn't work directly with the build tool\nplugin, please consider one of the following options:\n\n* To use a config file located outside the package/project directory, a config\n  file may be added to that directory specifying a parent config path to the\n  other config file, e.g., `parent_config: path/to/.swiftlint.yml`.\n* You can also consider the use of a\n  [Run Script Build Phase](#xcode-run-script-build-phase) in place of the build\n  tool plugin.\n\n### Xcode Run Script Build Phase\n\n> [!NOTE]\n> Based upon the installation method used, the shell command syntax in the\n> Run Script Build Phase may be different or additional configuration could\n> be required. Refer to the [installation](#installation) instructions for\n> more information.\n\nIf the build tool plugin does not work for your project setup or when\nadditional custom setup is required, SwiftLint can be added as a Run Script\nBuild Phase. This is useful when a project setup relies on the `--config`\nSwiftLint option; or to lint all targets together in a single `swiftlint`\ninvocation. File inclusions and exclusions can be configured in the\n[`.swiftlint.yml` configuration](#configuration).\n\nTo do this, add a custom script to a `Run Script` phase of the `Build Phases`\nof the primary app target, after the `Compile Sources` phase. Use the\nfollowing script implementation:\n\n```bash\nif command -v swiftlint >/dev/null 2>&1\nthen\n    swiftlint\nelse\n    echo \"warning: `swiftlint` command not found - See https://github.com/realm/SwiftLint#installation for installation instructions.\"\nfi\n```\n\nIf you're using the SwiftLintPlugin in a Swift package,\nyou may refer to the `swiftlint` executable in the\nfollowing way:\n\n```bash\nSWIFT_PACKAGE_DIR=\"${BUILD_DIR%Build/*}SourcePackages/artifacts\"\nSWIFTLINT_CMD=\"$SWIFT_PACKAGE_DIR/swiftlintplugins/SwiftLintBinary/SwiftLintBinary.artifactbundle/macos/swiftlint\"\n\nif test -f \"$SWIFTLINT_CMD\" 2>&1\nthen\n    \"$SWIFTLINT_CMD\"\nelse\n    echo \"warning: `swiftlint` command not found - See https://github.com/realm/SwiftLint#xcode-run-script-build-phase for installation instructions.\"\nfi\n```\n\n> [!NOTE]\n> The `SWIFTLINT_CMD` path uses the default Xcode configuration and has been\n> tested on Xcode 15/16. In case of another configuration (e.g. a custom\n> Swift package path), please adapt the values accordingly.\n<!-- markdownlint-disable MD028 -->\n> [!TIP]\n> Uncheck `Based on dependency analysis` to run `swiftlint` on all incremental\n> builds, suppressing the unspecified outputs warning.\n\n#### Consideration for Xcode 15.0\n\nXcode 15 made a significant change by setting the default value of the\n`ENABLE_USER_SCRIPT_SANDBOXING` build setting from `NO` to `YES`.\nAs a result, SwiftLint encounters an error related to missing file permissions,\nwhich typically manifests as\n`error: Sandbox: swiftlint(19427) deny(1) file-read-data.`\n\nTo resolve this issue, it is necessary to manually set the\n`ENABLE_USER_SCRIPT_SANDBOXING` setting to `NO` for the specific target that\nSwiftLint is being configured for.\n\n#### Consideration for Apple Silicon\n\nIf you installed SwiftLint via Homebrew on Apple Silicon, you might experience\nthis warning:\n\n```bash\nwarning: SwiftLint not installed, download from https://github.com/realm/SwiftLint\n```\n\nThat is because Homebrew on Apple Silicon installs the binaries into the\n`/opt/homebrew/bin` folder by default. To instruct Xcode where to find\nSwiftLint, you can either add `/opt/homebrew/bin` to the `PATH` environment\nvariable in your build phase:\n\n```bash\nif [[ \"$(uname -m)\" == arm64 ]]\nthen\n    export PATH=\"/opt/homebrew/bin:$PATH\"\nfi\n\nif command -v swiftlint >/dev/null 2>&1\nthen\n    swiftlint\nelse\n    echo \"warning: `swiftlint` command not found - See https://github.com/realm/SwiftLint#installation for installation instructions.\"\nfi\n```\n\nor you can create a symbolic link in `/usr/local/bin` pointing to the actual\nbinary:\n\n```bash\nln -s /opt/homebrew/bin/swiftlint /usr/local/bin/swiftlint\n```\n\n#### Additional Considerations\n\nIf you wish to fix violations as well, your script could run\n`swiftlint --fix && swiftlint` instead of just `swiftlint`. This will mean\nthat all correctable violations are fixed while ensuring warnings show up in\nyour project for remaining violations.\n\nIf you've installed SwiftLint via CocoaPods the script should look like this:\n\n```bash\n\"${PODS_ROOT}/SwiftLint/swiftlint\"\n```\n\n### Visual Studio Code\n\nTo integrate SwiftLint with [Visual Studio Code](https://code.visualstudio.com), install the\n[`vscode-swiftlint`](https://marketplace.visualstudio.com/items?itemName=vknabel.vscode-swiftlint)\nextension from the marketplace.\n\n### Fastlane\n\nYou can use the official\n[`swiftlint` fastlane action](https://docs.fastlane.tools/actions/swiftlint)\nto run SwiftLint as part of your fastlane process.\n\n```ruby\nswiftlint(\n    mode: :lint,                            # SwiftLint mode: :lint (default) or :autocorrect\n    executable: \"Pods/SwiftLint/swiftlint\", # The SwiftLint binary path (optional). Important if you've installed it via CocoaPods\n    path: \"/path/to/lint\",                  # Specify path to lint (optional)\n    output_file: \"swiftlint.result.json\",   # The path of the output file (optional)\n    reporter: \"json\",                       # The custom reporter to use (optional)\n    config_file: \".swiftlint-ci.yml\",       # The path of the configuration file (optional)\n    files: [                                # List of files to process (optional)\n        \"AppDelegate.swift\",\n        \"path/to/project/Model.swift\"\n    ],\n    ignore_exit_status: true,               # Allow fastlane to continue even if SwiftLint returns a non-zero exit status (Default: false)\n    quiet: true,                            # Don't print status logs like 'Linting ' & 'Done linting' (Default: false)\n    strict: true                            # Fail on warnings? (Default: false)\n)\n```\n\n### Docker\n\nSwiftLint is also available as a [Docker](https://www.docker.com/) image using\n`Ubuntu`. So just the first time you need to pull the docker image using the\nnext command:\n\n```bash\ndocker pull ghcr.io/realm/swiftlint:latest\n```\n\nThen following times, you just run `swiftlint` inside of the docker like:\n\n```bash\ndocker run -it -v `pwd`:`pwd` -w `pwd` ghcr.io/realm/swiftlint:latest\n```\n\nThis will execute `swiftlint` in the folder where you are right now (`pwd`),\nshowing an output like:\n\n```bash\n$ docker run -it -v `pwd`:`pwd` -w `pwd` ghcr.io/realm/swiftlint:latest\nLinting Swift files in current working directory\nLinting 'RuleDocumentation.swift' (1/490)\n...\nLinting 'YamlSwiftLintTests.swift' (490/490)\nDone linting! Found 0 violations, 0 serious in 490 files.\n```\n\nHere you have more documentation about the usage of\n[Docker Images](https://docs.docker.com/).\n\n## Command Line Usage\n\n```txt\n$ swiftlint help\nOVERVIEW: A tool to enforce Swift style and conventions.\n\nUSAGE: swiftlint <subcommand>\n\nOPTIONS:\n  --version               Show the version.\n  -h, --help              Show help information.\n\nSUBCOMMANDS:\n  analyze                 Run analysis rules\n  docs                    Open SwiftLint documentation website in the default web browser\n  generate-docs           Generates markdown documentation for selected group of rules\n  lint (default)          Print lint warnings and errors\n  baseline                Operations on existing baselines\n  reporters               Display the list of reporters and their identifiers\n  rules                   Display the list of rules and their identifiers\n  version                 Display the current version of SwiftLint\n\n  See 'swiftlint help <subcommand>' for detailed help.\n```\n\nRun `swiftlint` in the directory containing the Swift files to lint. Directories\nwill be searched recursively.\n\nTo specify a list of files when using `lint` or `analyze`\n(like the list of files modified by Xcode specified by the\n[`ExtraBuildPhase`](https://github.com/norio-nomura/ExtraBuildPhase) Xcode\nplugin, or modified files in the working tree based on `git ls-files -m`), you\ncan do so by passing the option `--use-script-input-files` and setting the\nfollowing instance variables: `SCRIPT_INPUT_FILE_COUNT`\nand `SCRIPT_INPUT_FILE_0`, `SCRIPT_INPUT_FILE_1`, ...,\n`SCRIPT_INPUT_FILE_{SCRIPT_INPUT_FILE_COUNT - 1}`.\nSimilarly, files can be read from file lists by passing\nthe option `--use-script-input-file-lists` and setting the\nfollowing instance variables: `SCRIPT_INPUT_FILE_LIST_COUNT`\nand `SCRIPT_INPUT_FILE_LIST_0`, `SCRIPT_INPUT_FILE_LIST_1`, ...,\n`SCRIPT_INPUT_FILE_LIST_{SCRIPT_INPUT_FILE_LIST_COUNT - 1}`.\n\nThese are same environment variables set for input files to\n[custom Xcode script phases](http://indiestack.com/2014/12/speeding-up-custom-script-phases/).\n\n## Working With Multiple Swift Versions\n\nSwiftLint hooks into SourceKit so it continues working even as Swift evolves!\n\nThis also keeps SwiftLint lean, as it doesn't need to ship with a full Swift\ncompiler, it just communicates with the official one you already have installed\non your machine.\n\nYou should always run SwiftLint with the same toolchain you use to compile your\ncode.\n\nYou may want to override SwiftLint's default Swift toolchain if you have\nmultiple toolchains or Xcodes installed.\n\nHere's the order in which SwiftLint determines which Swift toolchain to use:\n\n* `$XCODE_DEFAULT_TOOLCHAIN_OVERRIDE`\n* `$TOOLCHAIN_DIR` or `$TOOLCHAINS`\n* `xcrun -find swift`\n* `/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain`\n* `/Applications/Xcode-beta.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain`\n* `~/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain`\n* `~/Applications/Xcode-beta.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain`\n\n`sourcekitd.framework` is expected to be found in the `usr/lib/` subdirectory of\nthe value passed in the paths above.\n\nYou may also set the `TOOLCHAINS` environment variable to the reverse-DNS\nnotation that identifies a Swift toolchain version:\n\n```shell\nTOOLCHAINS=com.apple.dt.toolchain.Swift_2_3 swiftlint --fix\n```\n\nOn Linux, SourceKit is expected to be located in\n`/usr/lib/libsourcekitdInProc.so` or specified by the `LINUX_SOURCEKIT_LIB_PATH`\nenvironment variable.\n\n## Git `pre-commit` Hook\n\nSwiftLint can be run as a [pre-commit](https://pre-commit.com/) hook.\nOnce [installed](https://pre-commit.com/#install), add this to the\n`.pre-commit-config.yaml` in the root of your repository:\n\n```yaml\nrepos:\n  - repo: https://github.com/realm/SwiftLint\n    rev: 0.57.1\n    hooks:\n      - id: swiftlint\n```\n\nAdjust `rev` to the SwiftLint version of your choice.  `pre-commit autoupdate`\ncan be used to update to the current version.\n\nSwiftLint can be configured using `entry` to apply fixes and fail on errors:\n\n```yaml\n- repo: https://github.com/realm/SwiftLint\n  rev: 0.57.1\n  hooks:\n    - id: swiftlint\n      entry: swiftlint --fix --strict\n```\n\n## Rules\n\nOver 200 rules are included in SwiftLint and the Swift community (that's you!)\ncontinues to contribute more over time.\n[Pull requests](https://github.com/realm/SwiftLint/blob/main/CONTRIBUTING.md)\nare encouraged.\n\nYou can find an updated list of rules and more information about them in the\n[Rule Directory](https://realm.github.io/SwiftLint/rule-directory.html).\n\nYou can also check the\n[Source/SwiftLintBuiltInRules/Rules](https://github.com/realm/SwiftLint/tree/main/Source/SwiftLintBuiltInRules/Rules)\ndirectory to see their implementation.\n\n### Opt-In Rules\n\n`opt_in_rules` are disabled by default (i.e., you have to explicitly enable them\nin your configuration file).\n\nGuidelines on when to mark a rule as opt-in:\n\n* A rule that can have many false positives (e.g. `empty_count`)\n* A rule that is too slow\n* A rule that is not general consensus or is only useful in some cases\n  (e.g. `force_unwrapping`)\n\n### Disable rules in code\n\nRules can be disabled with a comment inside a source file with the following\nformat:\n\n`// swiftlint:disable <rule1> [<rule2> <rule3>...]`\n\nThe rules will be disabled until the end of the file or until the linter sees a\nmatching enable comment:\n\n`// swiftlint:enable <rule1> [<rule2> <rule3>...]`\n\nFor example:\n\n```swift\n// swiftlint:disable colon\nlet noWarning :String = \"\" // No warning about colons immediately after variable names.\n// swiftlint:enable colon\nlet hasWarning :String = \"\" // Warning generated about colons immediately after variable names.\n```\n\nIncluding the `all` keyword will disable all rules until the linter sees a\nmatching enable comment:\n\n`// swiftlint:disable all`\n`// swiftlint:enable all`\n\nFor example:\n\n```swift\n// swiftlint:disable all\nlet noWarning :String = \"\" // No warning about colons immediately after variable names.\nlet i = \"\" // Also no warning about short identifier names.\n// swiftlint:enable all\nlet hasWarning :String = \"\" // Warning generated about colons immediately after variable names.\nlet y = \"\" // Warning generated about short identifier names.\n```\n\nIt's also possible to modify a `disable` or `enable` command by appending\n`:previous`, `:this` or `:next` for only applying the command to the previous,\nthis (current) or next line respectively.\n\nFor example:\n\n```swift\n// swiftlint:disable:next force_cast\nlet noWarning = NSNumber() as! Int\nlet hasWarning = NSNumber() as! Int\nlet noWarning2 = NSNumber() as! Int // swiftlint:disable:this force_cast\nlet noWarning3 = NSNumber() as! Int\n// swiftlint:disable:previous force_cast\n```\n\nRun `swiftlint rules` to print a list of all available rules and their\nidentifiers.\n\n### Configuration\n\nConfigure SwiftLint by adding a `.swiftlint.yml` file from the directory you'll\nrun SwiftLint from. The following parameters can be configured:\n\nRule inclusion:\n\n* `disabled_rules`: Disable rules from the default enabled set.\n* `opt_in_rules`: Enable rules that are not part of the default set. The\n   special `all` identifier will enable all opt in linter rules, except the ones\n   listed in `disabled_rules`.\n* `only_rules`: Only the rules specified in this list will be enabled.\n   Cannot be specified alongside `disabled_rules` or `opt_in_rules`.\n* `analyzer_rules`: This is an entirely separate list of rules that are only\n  run by the `analyze` command. All analyzer rules are opt-in, so this is the\n  only configurable rule list, there are no equivalents for `disabled_rules`\n  and `only_rules`. The special `all` identifier can also be used here to enable\n  all analyzer rules, except the ones listed in `disabled_rules`.\n\n```yaml\n# By default, SwiftLint uses a set of sensible default rules you can adjust. Find all the available rules\n# by running `swiftlint rules` or visiting https://realm.github.io/SwiftLint/rule-directory.html.\n\n# Rules turned on by default can be disabled.\ndisabled_rules:\n  - colon\n  - comma\n  - control_statement\n  \n# Rules turned off by default can be enabled.\nopt_in_rules:\n  - empty_count\n\n# Alternatively, specify all rules explicitly by uncommenting this option and removing the above two.\n# only_rules:\n#   - empty_parameters\n#   - vertical_whitespace\n\n# Rules only run by `swiftlint analyze`. These are all opt-in.\nanalyzer_rules:\n  - explicit_self\n\n# Case-sensitive paths to include during linting. Directory paths supplied on the\n# command line will be ignored. Wildcards are supported.\nincluded: \n  - Sources\n\n# Case-sensitive paths to ignore during linting. Takes precedence over `included`. Wildcards\n# are supported.\nexcluded: \n  - Carthage\n  - Pods\n  - Sources/ExcludedFolder\n  - Sources/ExcludedFile.swift\n  - Sources/*/ExcludedFile.swift\n\n# If true, SwiftLint will not fail if no lintable files are found.\nallow_zero_lintable_files: false\n\n# If true, SwiftLint will treat all warnings as errors.\nstrict: false\n\n# If true, SwiftLint will treat all errors as warnings.\nlenient: false\n\n# The path to a baseline file, which will be used to filter out detected violations.\nbaseline: Baseline.json\n\n# The path to save detected violations to as a new baseline.\nwrite_baseline: Baseline.json\n\n# If true, SwiftLint will check for updates after linting or analyzing.\ncheck_for_updates: true\n\n# Configurable rules can be customized. All rules support setting their severity level.\nforce_cast: warning # implicitly\nforce_try:\n  severity: warning # explicitly\n  \n# Rules that have both warning and error levels can set just the warning level implicitly.\nline_length: 110\n\n# To set both levels implicitly, use an array.\ntype_body_length:\n  - 300 # warning\n  - 400 # error\n\n# To set both levels explicitly, use a dictionary.\nfile_length:\n  warning: 500\n  error: 1200\n  \n# Naming rules can set warnings/errors for `min_length` and `max_length`. Additionally, they can\n# set excluded names and allowed symbols.\ntype_name:\n  min_length: 4 # warning\n  max_length: # warning and error\n    warning: 40\n    error: 50\n  excluded: i(Phone|Pad|Pod) # regex pattern\n  allowed_symbols: [\"_\"]\nidentifier_name:\n  min_length:\n    error: 4 # only error\n  excluded: # excluded via string array\n    - id\n    - URL\n    - GlobalAPIKey\n    \n# The default reporter (SwiftLint's output format) can be configured as `checkstyle`, `codeclimate`, `csv`,\n# `emoji`, `github-actions-logging`, `gitlab`, `html`, `json`, `junit`, `markdown`, `relative-path`, `sarif`,\n# `sonarqube`, `summary`, or `xcode` (default).\nreporter: \"xcode\"\n```\n\nYou can also use environment variables in your configuration file,\nby using `${SOME_VARIABLE}` in a string.\n\n### Defining Custom Rules\n\nIn addition to the rules that the main SwiftLint project ships with, SwiftLint\ncan also run two types of custom rules that you can define yourself in your own\nprojects:\n\n#### 1. Swift Custom Rules\n\nThese rules are written the same way as the Swift-based rules that ship with\nSwiftLint so they're fast, accurate, can leverage SwiftSyntax, can be unit\ntested, and more.\n\nUsing these requires building SwiftLint with Bazel as described in\n[this video](https://vimeo.com/820572803) or its associated code in\n[github.com/jpsim/swiftlint-bazel-example](https://github.com/jpsim/swiftlint-bazel-example).\n\n#### 2. Regex Custom Rules\n\nYou can define custom regex-based rules in your configuration file using the\nfollowing syntax:\n\n```yaml\ncustom_rules:\n  # Rule identifier.\n  pirates_beat_ninjas:\n    # Optional regex that defines paths to include during linting.\n    included:\n      - \".*\\\\.swift\"\n    # Optional regex that defines paths to exclude during linting.\n    excluded:\n      - \".*Test\\\\.swift\"\n    # Optional rule name.\n    name: \"Pirates Beat Ninjas\"\n    # Matching pattern.\n    regex: \"([nN]inja)\"\n    # Number of regex capture group to highlight the rule violation at. Optional, defaults to 0 (the whole match).\n    capture_group: 0\n    # SyntaxKinds to match. optional.\n    match_kinds:\n      - comment\n      - identifier\n    # Optional violation message.\n    message: \"Pirates are better than ninjas.\"\n    # Optional violation severity.\n    severity: error\n  no_hiding_in_strings:\n    regex: \"([nN]inja)\"\n    # Syntax kinds to match. optional.\n    match_kinds: string\n```\n\nThis is what the output would look like:\n\n![Custom violations highlighted in the Xcode editor](https://raw.githubusercontent.com/realm/SwiftLint/main/assets/custom-rule.png)\n\nIt is important to note that the regular expression pattern is used with the\nflags `s` and `m` enabled, that is `.`\n[matches newlines](https://developer.apple.com/documentation/foundation/nsregularexpression/options/1412529-dotmatcheslineseparators)\nand `^`/`$`\n[match the start and end of lines](https://developer.apple.com/documentation/foundation/nsregularexpression/options/1408263-anchorsmatchlines),\nrespectively. If you do not want to have `.` match newlines, for example, the\nregex can be prepended by `(?-s)`.\n\nYou can filter the matches by providing one or more `match_kinds`, which will\nreject matches that include syntax kinds that are not present in this list. Here\nare all the possible syntax kinds:\n\n* `argument`\n* `attribute.builtin`\n* `attribute.id`\n* `buildconfig.id`\n* `buildconfig.keyword`\n* `comment`\n* `comment.mark`\n* `comment.url`\n* `doccomment`\n* `doccomment.field`\n* `identifier`\n* `keyword`\n* `number`\n* `objectliteral`\n* `parameter`\n* `placeholder`\n* `string`\n* `string_interpolation_anchor`\n* `typeidentifier`\n\nAll syntax kinds used in a snippet of Swift code can be extracted asking\n[SourceKitten](https://github.com/jpsim/SourceKitten). For example,\n`sourcekitten syntax --text \"struct S {}\"` delivers\n\n* `source.lang.swift.syntaxtype.keyword` for the `struct` keyword and\n* `source.lang.swift.syntaxtype.identifier` for its name `S`\n\nwhich match to `keyword` and `identifier` in the above list.\n\nIf using custom rules in combination with `only_rules`, you must include the\nliteral string `custom_rules` in the `only_rules` list:\n\n```yaml\nonly_rules:\n  - custom_rules\n\ncustom_rules:\n  no_hiding_in_strings:\n    regex: \"([nN]inja)\"\n    match_kinds: string\n```\n\nUnlike Swift custom rules, you can use official SwiftLint builds\n(e.g. from Homebrew) to run regex custom rules.\n\n### Auto-correct\n\nSwiftLint can automatically correct certain violations. Files on disk are\noverwritten with a corrected version.\n\nPlease make sure to have backups of these files before running\n`swiftlint --fix`, otherwise important data may be lost.\n\nStandard linting is disabled while correcting because of the high likelihood of\nviolations (or their offsets) being incorrect after modifying a file while\napplying corrections.\n\n### Analyze\n\nThe `swiftlint analyze` command can lint Swift files using the\nfull type-checked AST. The compiler log path containing the clean `swiftc` build\ncommand invocation (incremental builds will fail) must be passed to `analyze`\nvia the `--compiler-log-path` flag.\ne.g. `--compiler-log-path /path/to/xcodebuild.log`\n\nThis can be obtained by\n\n1. Cleaning DerivedData (incremental builds won't work with analyze)\n2. Running `xcodebuild -workspace {WORKSPACE}.xcworkspace -scheme {SCHEME} > xcodebuild.log`\n3. Running `swiftlint analyze --compiler-log-path xcodebuild.log`\n\nAnalyzer rules tend to be considerably slower than lint rules.\n\n## Using Multiple Configuration Files\n\nSwiftLint offers a variety of ways to include multiple configuration files.\nMultiple configuration files get merged into one single configuration that is\nthen applied just as a single configuration file would get applied.\n\nThere are quite a lot of use cases where using multiple configuration files\ncould be helpful:\n\nFor instance, one could use a team-wide shared SwiftLint configuration while\nallowing overrides in each project via a child configuration file.\n\nTeam-Wide Configuration:\n\n```yaml\ndisabled_rules:\n  - force_cast\n```\n\nProject-Specific Configuration:\n\n```yaml\nopt_in_rules:\n  - force_cast\n```\n\n### Child/Parent Configs (Locally)\n\nYou can specify a `child_config` and/or a `parent_config` reference within a\nconfiguration file. These references should be local paths relative to the\nfolder of the configuration file they are specified in. This even works\nrecursively, as long as there are no cycles and no ambiguities.\n\n**A child config is treated as a refinement and thus has a higher priority**,\nwhile a parent config is considered a base with lower priority in case of\nconflicts.\n\nHere's an example, assuming you have the following file structure:\n\n```txt\nProjectRoot\n    |_ .swiftlint.yml\n    |_ .swiftlint_refinement.yml\n    |_ Base\n        |_ .swiftlint_base.yml\n```\n\nTo include both the refinement and the base file, your `.swiftlint.yml` should\nlook like this:\n\n```yaml\nchild_config: .swiftlint_refinement.yml\nparent_config: Base/.swiftlint_base.yml\n```\n\nWhen merging parent and child configs, `included` and `excluded` configurations\nare processed carefully to account for differences in the directory location\nof the containing configuration files.\n\n### Child/Parent Configs (Remote)\n\nJust as you can provide local `child_config`/`parent_config` references,\ninstead of referencing local paths, you can just put urls that lead to\nconfiguration files. In order for SwiftLint to detect these remote references,\nthey must start with `http://` or `https://`.\n\nThe referenced remote configuration files may even recursively reference other\nremote configuration files, but aren't allowed to include local references.\n\nUsing a remote reference, your `.swiftlint.yml` could look like this:\n\n```yaml\nparent_config: https://myteamserver.com/our-base-swiftlint-config.yml\n```\n\nEvery time you run SwiftLint and have an Internet connection, SwiftLint tries\nto get a new version of every remote configuration that is referenced. If this\nrequest times out, a cached version is used if available. If there is no cached\nversion available, SwiftLint fails â€“ but no worries, a cached version should be\nthere once SwiftLint has run successfully at least once.\n\nIf needed, the timeouts for the remote configuration fetching can be specified\nmanually via the configuration file(s) using the\n`remote_timeout`/`remote_timeout_if_cached` specifiers. These values default\nto 2 seconds or 1 second, respectively.\n\n### Command Line\n\nInstead of just providing one configuration file when running SwiftLint via the\ncommand line, you can also pass a hierarchy, where the first configuration is\ntreated as a parent, while the last one is treated as the highest-priority\nchild.\n\nA simple example including just two configuration files looks like this:\n\n`swiftlint --config .swiftlint.yml --config .swiftlint_child.yml`\n\n### Nested Configurations\n\nIn addition to a main configuration (the `.swiftlint.yml` file in the root\nfolder), you can put other configuration files named `.swiftlint.yml` into the\ndirectory structure that then get merged as a child config, but only with an\neffect for those files that are within the same directory as the config or in a\ndeeper directory where there isn't another configuration file. In other words:\nNested configurations don't work recursively â€“ there's a maximum number of one\nnested configuration per file that may be applied in addition to the main\nconfiguration.\n\n`.swiftlint.yml` files are only considered as a nested configuration if they\nhave not been used to build the main configuration already (e. g. by having\nbeen referenced via something like `child_config: Folder/.swiftlint.yml`).\nAlso, `parent_config`/`child_config` specifications of nested configurations\nare getting ignored because there's no sense to that.\n\nIf one (or more) SwiftLint file(s) are explicitly specified via the `--config`\nparameter, that configuration will be treated as an override, no matter whether\nthere exist other `.swiftlint.yml` files somewhere within the directory.\n**So if you want to use nested configurations, you can't use the `--config`\nparameter.**\n\n## License\n\n[MIT licensed.](https://github.com/realm/SwiftLint/blob/main/LICENSE)\n\n## About\n\nSwiftLint is utterly maintained by volunteers contributing to its success\nentirely in their free time. As such, SwiftLint isn't a commercial product\nin any way.\n\nBe kind to the people maintaining SwiftLint as a hobby and accept that their\ntime is limited. Support them by contributing to the project, reporting issues,\nand helping others in the community.\n\nSpecial thanks go to [MacStadium](https://www.macstadium.com) for providing\nphysical Mac mini machines to run our performance tests.\n\n![MacStadium](https://raw.githubusercontent.com/realm/SwiftLint/main/assets/macstadium.png)\n\nWe also thank Realm (now MongoDB) for their initial contributions and setup of\nthe project.\n",
      "stars_today": 1
    },
    {
      "id": 66302557,
      "name": "SwiftFormat",
      "full_name": "nicklockwood/SwiftFormat",
      "description": "A command-line tool and Xcode Extension for formatting Swift code",
      "html_url": "https://github.com/nicklockwood/SwiftFormat",
      "stars": 8673,
      "forks": 671,
      "language": "Swift",
      "topics": [],
      "created_at": "2016-08-22T19:39:05Z",
      "updated_at": "2026-01-28T00:21:37Z",
      "pushed_at": "2026-01-27T04:20:56Z",
      "open_issues": 329,
      "owner": {
        "login": "nicklockwood",
        "avatar_url": "https://avatars.githubusercontent.com/u/546885?v=4"
      },
      "readme": "![](EditorExtension/Application/Assets.xcassets/AppIcon.appiconset/icon_256x256.png)\n\n[![PayPal](https://img.shields.io/badge/paypal-donate-blue.svg)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=9ZGWNK5FEZFF6&source=url)\n[![Build](https://github.com/nicklockwood/SwiftFormat/actions/workflows/build.yml/badge.svg)](https://github.com/nicklockwood/SwiftFormat/actions/workflows/build.yml)\n[![Codecov](https://codecov.io/gh/nicklockwood/SwiftFormat/graphs/badge.svg)](https://codecov.io/gh/nicklockwood/SwiftFormat)\n[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fnicklockwood%2FSwiftFormat%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/nicklockwood/swiftformat)\n[![License](https://img.shields.io/badge/license-MIT-lightgrey.svg)](https://opensource.org/licenses/MIT)\n[![Mastodon](https://img.shields.io/badge/mastodon-@nicklockwood@mastodon.social-636dff.svg)](https://mastodon.social/@nicklockwood)\n\nTable of Contents\n-----------------\n\n- [What?](#what-is-this)\n- [Why?](#why-would-i-want-to-do-that)\n- [How?](#how-do-i-install-it)\n    - [Command-line tool](#command-line-tool)\n    - [Xcode source editor extension](#xcode-source-editor-extension)\n    - [Xcode build phase](#xcode-build-phase)\n    - [Swift Package Manager plugin](#swift-package-manager-plugin)\n    - [Via Applescript](#via-applescript)\n    - [VSCode plugin](#vscode-plugin)\n    - [Sublime Text plugin](#sublime-text-plugin)\n    - [Nova plugin](nova-plugin)\n    - [Git pre-commit hook](#git-pre-commit-hook)\n    - [GitHub Actions](#github-actions)\n    - [On CI using Danger](#on-ci-using-danger)\n    - [Bazel build](#bazel-build)\n    - [Docker](#docker)\n    - [Prerelease Builds](#prerelease-builds)\n- [Configuration](#configuration)\n    - [Options](#options)\n    - [Rules](#rules)\n    - [Swift version](#swift-version)\n    - [Config file](#config-file)\n    - [Globs](#globs)\n    - [Linting](#linting)\n    - [Error codes](#error-codes)\n    - [Cache](#cache)\n    - [File headers](#file-headers)\n    - [Markdown formatting](#markdown-formatting)\n- [FAQ](#faq)\n- [Known issues](#known-issues)\n- [Tip Jar](#tip-jar)\n- [Credits](#credits)\n\n\nWhat is this?\n----------------\n\nSwiftFormat is a code library and command-line tool for reformatting Swift code on macOS, Linux or Windows.\n\nSwiftFormat goes above and beyond what you might expect from a code formatter. In addition to adjusting white space it can insert or remove implicit `self`, remove redundant parentheses, and correct many other deviations from the standard Swift idioms.\n\n\nWhy would I want to do that?\n-----------------------------\n\nMany programmers have a preferred style for formatting their code, and others seem entirely blind to the existing formatting conventions of a project (to the enragement of their colleagues).\n\nWhen collaborating on a project, it can be helpful to agree on a common coding style, but enforcing that manually is tedious and error-prone, and can lead to arguments if some participants take it more seriously than others.\n\nHaving a tool to automatically enforce a common style eliminates those issues, and lets you focus on the behavior of the code, not its presentation.\n\n\nHow do I install it?\n---------------------\n\nThat depends - There are several ways you can use SwiftFormat:\n\n1. As a command-line tool that you run manually, or as part of some other toolchain\n2. As a Source Editor Extension that you can invoke via the Editor > SwiftFormat menu within Xcode\n3. As a build phase in your Xcode project, so that it runs every time you press Cmd-R or Cmd-B, or\n4. As a Git pre-commit hook, so that it runs on any files you've changed before you check them in\n\n\nCommand-line tool\n-------------------\n\n**Installation:**\n\nYou can install the `swiftformat` command-line tool on macOS or Linux using [Homebrew](http://brew.sh/). Assuming you already have Homebrew installed, just type:\n\n```bash\n$ brew install swiftformat\n```\n\nTo update to the latest version once installed:\n\n```bash\n$ brew upgrade swiftformat\n```\n\nAlternatively, you can install the tool on macOS or Linux by using [Mint](https://github.com/yonaskolb/Mint) as follows:\n\n```bash\n$ mint install nicklockwood/SwiftFormat\n```\n\nOr if you prefer, you can check out and build SwiftFormat manually on macOS, Linux or Windows as follows:\n\n```bash\n$ git clone https://github.com/nicklockwood/SwiftFormat\n$ cd SwiftFormat\n$ swift build -c release\n```\n\nIf you are installing SwiftFormat into your project directory, you can use [CocoaPods](https://cocoapods.org/) on macOS to automatically install the swiftformat binary along with your other pods - see the Xcode build phase instructions below for details.\n\nAnother option is to include the binary artifactbundle in your `Package.swift`:\n\n```swift\n.binaryTarget(\n    name: \"swiftformat\",\n    url: \"https://github.com/nicklockwood/SwiftFormat/releases/download/0.55.0/swiftformat-macos.artifactbundle.zip\",\n    checksum: \"CHECKSUM\"\n),\n``` \n\nIf you would prefer not to use a package manager, you can build the command-line app manually:\n\n1. open `SwiftFormat.xcodeproj` and build the `SwiftFormat (Application)` scheme.\n\n2. Drag the `swiftformat` binary into `/usr/local/bin/` (this is a hidden folder, but you can use the Finder's `Go > Go to Folder...` menu to open it).\n\n3. Open `~/.bash_profile` in your favorite text editor (this is a hidden file, but you can type `open ~/.bash_profile` in the terminal to open it).\n\n4. Add the following line to the file: `alias swiftformat=\"/usr/local/bin/swiftformat --indent 4\"` (you can omit the `--indent 4`, or replace it with something else. Run `swiftformat --help` to see the available options).\n\n5. Save the `.bash_profile` file and run the command `source ~/.bash_profile` for the changes to take effect.\n\n**Usage:**\n\nIf you followed the installation instructions above, you can now just type\n\n```bash\n$ swiftformat .\n```\n\n(that's a space and then a period after the command) in the terminal to format any Swift files in the current directory. In place of the `.`, you can instead type an absolute or relative path to the file or directory that you want to format.\n\n**WARNING:** `swiftformat .` will overwrite any Swift files it finds in the current directory, and any subfolders therein. If you run it in your home directory, it will probably reformat every Swift file on your hard drive.\n\nTo use it safely, do the following:\n\n1. Choose a file or directory that you want to apply the changes to.\n\n2. Make sure that you have committed all your changes to that code safely in git (or whatever source control system you use).\n\n3. (Optional) In Terminal, type `swiftformat --infer-options \"/path/to/your/code/\"`. This will suggest a set of formatting options to use that match your existing project style (but you are free to ignore these and use the defaults, or your own settings if you prefer).\n\n    The path can point to either a single Swift file or a directory of files. It can be either be absolute, or relative to the current directory. The `\"\"` quotes around the path are optional, but if the path contains spaces then you either need to use quotes, or escape each space with `\\`. You may include multiple paths separated by spaces.\n\n4. In Terminal, type `swiftformat \"/path/to/your/code/\"`. The same rules apply as above with respect to paths, and multiple space-delimited paths are allowed.\n\n    If you used `--infer-options` to generate a suggested set of options in step 3, you should copy and paste them into the command, either before or after the path(s) to your source files.\n\n    If you have created a [config file](#config-file), you can specify its path using `--config \"/path/to/your/config-file/\"`. Alternatively, if you name the file `.swiftformat` and place it inside the project you are formatting, it will be picked up automatically.\n\n5. Press enter to begin formatting. Once the formatting is complete, use your source control system to check the changes, and verify that no undesirable changes have been introduced. If they have, revert the changes, tweak the options and try again.\n\n6. (Optional) commit the changes.\n\nFollowing these instructions *should* ensure that you avoid catastrophic data loss, but in the unlikely event that it wipes your hard drive, **please note that I accept no responsibility**.\n\n**Using Standard Input/Output:**\n\nIf you prefer, you can use unix pipes to include SwiftFormat as part of a command chain. For example, this is an alternative way to format a file:\n\n```bash\n$ cat /path/to/file.swift | swiftformat --output /path/to/file.swift\n```\n\nOmitting the `--output /path/to/file.swift` will print the formatted file to Standard Output (stdout). You can also pass \"stdout\" explicitly as the output path:\n\n```bash\n$ cat /path/to/file.swift | swiftformat --output stdout\n```\n\nOr you can use `>` to specify the output path as follows:\n\n```bash\n$ cat /path/to/file.swift | swiftformat > /path/to/file.swift\n```\n\nIf you do not supply an input file, SwiftFormat will automatically take its input from Standard Input (stdin), but will time-out if no input is received immediately and display the help screen. To make it explicit, pass \"stdin\" as the input path:\n\n```bash\n$ cat /path/to/file.swift | swiftformat stdin\n```\n\nWhen using stdin, SwiftFormat does not have access to the file path of the input, so features that rely on the file location (such as inserting the creation date into header comments, or detecting `.swiftformat` configuration files in the file path) will not work. To solve this, you can provide the file path using the `--stdin-path` argument:\n\n```bash\n$ cat /path/to/file.swift | swiftformat stdin --stdinpath /path/to/file.swift\n```\n\n\nXcode source editor extension\n-----------------------------\n\n**Installation:**\n\nLike the command-line tool, you can install the SwiftFormat for Xcode extension application via [Homebrew](http://brew.sh/). Assuming you already have Homebrew installed, type:\n\n```bash\n$ brew install --cask swiftformat-for-xcode\n```\n\nThis will install SwiftFormat for Xcode in your Applications folder. Double-click the app to launch it, and then follow the on-screen instructions.\n\n**NOTE:** The app should be correctly signed, but if you get a Gatekeeper warning when trying to open it you can bypass this by right-clicking (or control-clicking) the app and selecting `Open`.\n\nTo update to the latest version once installed use:\n\n```bash\n$ brew upgrade --cask swiftformat-for-xcode\n```\n\nAlternatively, if you prefer not to use Homebrew, you'll find the latest version of the SwiftFormat for Xcode application on the [GitHub Releases](https://github.com/nicklockwood/SwiftFormat/releases) page. Download and unpack the zip archive, then drag `SwiftFormat for Xcode.app` into your `Applications` folder.\n\n**Usage:**\n\nOnce you have launched the app and restarted Xcode, you'll find a SwiftFormat option under Xcode's Editor menu. If the SwiftFormat menu does not appear [this thread](https://github.com/nicklockwood/SwiftFormat/issues/494) may help. \n\nYou can configure the formatting [rules](#rules) and [options](#options) using the SwiftFormat for Xcode host application. There is currently no way to override these per-project, however, you can import and export different configurations using the File menu. You will need to do this again each time you switch projects.\n\nThe format of the configuration file is described in the [Config section](#config-file) below.\n\n**Note:** SwiftFormat for Xcode cannot automatically detect changes to an imported configuration file. If you update the `.swiftformat` file for your project, you will need to manually re-import that file into SwiftFormat for Xcode in order for the Xcode source editor extension to use the new configuration.\n\n\nXcode build phase\n-------------------\n\n**NOTE:** Adding this script will overwrite your source files as you work on them, which has the annoying side-effect of clearing the undo history. You may wish to add the script to your test target rather than your main target, so that it is invoked only when you run the unit tests, and not every time you build the app.\n\nAlternatively, you might want to consider running SwiftFormat in [lint](#linting) mode as part of your normal build, and then running a formatting pass manually, or as part of a less-frequent build target (such as the tests).\n\n### Using Swift Package Manager\n\nTo set up SwiftFormat as an Xcode build phase, do the following:\n\n#### 1) Create a BuildTools folder and Package.swift\n\n1. Create a folder called `BuildTools` in the same folder as your xcodeproj file\n2. In this folder, create a file called `Package.swift`, with the following contents:\n```swift\n// swift-tools-version:5.1\nimport PackageDescription\n\nlet package = Package(\n    name: \"BuildTools\",\n    platforms: [.macOS(.v10_11)],\n    dependencies: [\n        .package(url: \"https://github.com/nicklockwood/SwiftFormat\", from: \"0.59.0\"),\n    ],\n    targets: [.target(name: \"BuildTools\", path: \"\")]\n)\n```\n3. If you are running Xcode 11.4 or later, in the `BuildTools` folder create a file called `Empty.swift` with nothing in it. This is to satisfy a change in Swift Package Manager.\n\n#### 2) Add a Build phase to your app target\n\n1. Click on your project in the file list, choose your target under `TARGETS`, click the `Build Phases` tab\n2. Add a `New Run Script Phase` by clicking the little plus icon in the top left\n3. Uncheck the `Based on dependency analysis` checkbox\n4. Drag the new `Run Script` phase **above** the `Compile Sources` phase, expand it and paste the following script:\n\n    ```bash\n    cd BuildTools\n    SDKROOT=(xcrun --sdk macosx --show-sdk-path)\n    #swift package update #Uncomment this line temporarily to update the version used to the latest matching your BuildTools/Package.swift file\n    swift run -c release swiftformat \"$SRCROOT\"\n    ```\n\nYou can also use `swift run -c release --package-path BuildTools swiftformat \"$SRCROOT\"` if you need a more complex script and `cd BuildTools` breaks stuff.\n\n**NOTE:** You may wish to check BuildTools/Package.swift into your source control so that the version used by your run-script phase is kept in version control. It is recommended to add the following to your .gitignore file: `BuildTools/.build` and `BuildTools/.swiftpm`.\n\n**NOTE (2):** If you are using Xcode 15 or later, make sure that the `ENABLE_USER_SCRIPT_SANDBOXING` (aka \"User Script Sandboxing\") option is set to NO, otherwise SwiftFormat won't be able to run correctly.\n\n### Using CocoaPods\n\n#### 1) Add the SwiftFormat CLI to your Podfile\n\n1. Add the `swiftformat` binary to your project directory via [CocoaPods](https://cocoapods.org/), by adding the following line to your Podfile then running `pod install`:\n\n    ```ruby\n    pod 'SwiftFormat/CLI', '~> 0.59.0'\n    ```\n\n**NOTE:** This will only install the pre-built command-line app, not the source code for the SwiftFormat framework.\n\n**NOTE (2):** When installing this way, GateKeeper may block swiftformat from running until you open it manually the first time by right-clicking in the Finder and selecting \"Open\".\n\n#### 2) Add a Build phase to your app target\n\n1. Click on your project in the file list, choose your target under `TARGETS`, click the `Build Phases` tab\n2. Add a `New Run Script Phase` by clicking the little plus icon in the top left\n3. Uncheck the `Based on dependency analysis` checkbox\n4. Drag the new `Run Script` phase **above** the `Compile Sources` phase, expand it and paste the following script:\n\n    ```bash\n    \"${PODS_ROOT}/SwiftFormat/CommandLineTool/swiftformat\" \"$SRCROOT\"\n    ```\n\n### Alternative: Locally installed SwiftFormat\n\nAlternatively, you could use a locally installed swiftformat command-line tool instead by putting the following in your Run Script build phase:\n\n```bash\nif which swiftformat >/dev/null; then\n  swiftformat .\nelse\n  echo \"warning: SwiftFormat not installed, download from https://github.com/nicklockwood/SwiftFormat\"\nfi\n```\n\nThis is not recommended for shared projects however, as different team members using different versions of SwiftFormat may result in noise in the commit history as code gets reformatted inconsistently.\n\nIf you installed SwiftFormat via Homebrew on Apple Silicon, you might experience this warning:\n\n> warning: SwiftFormat not installed, download from https://github.com/nicklockwood/SwiftFormat\n\nThat is because Homebrew on Apple Silicon installs the binaries into the `/opt/homebrew/bin`\nfolder by default. To instruct Xcode where to find SwiftFormat, you can either add\n`/opt/homebrew/bin` to the `PATH` environment variable in your build phase\n\n```bash\nif [[ \"$(uname -m)\" == arm64 ]]; then\n    export PATH=\"/opt/homebrew/bin:$PATH\"\nfi\n\nif which swiftformat > /dev/null; then\n  swiftformat .\nelse\n  echo \"warning: SwiftFormat not installed, download from https://github.com/nicklockwood/SwiftFormat\"\nfi\n```\n\nor you can create a symbolic link in `/usr/local/bin` pointing to the actual binary:\n\n```bash\nln -s /opt/homebrew/bin/swiftformat /usr/local/bin/swiftformat\n```\n\nSwift Package Manager plugin\n-----------------------------\n\nYou can use `SwiftFormat` as a SwiftPM command plugin.\n\n**NOTE:** Swift 5.6 or higher is required. Add the package to your dependencies in your `Package.swift` file.\n\n```swift\ndependencies: [\n    // ...\n    .package(url: \"https://github.com/nicklockwood/SwiftFormat\", from: \"0.59.0\"),\n]\n```\n\nThe plugin will find an existing `.swiftformat` in your package root folder and honor it automatically.\n\n### Trigger Plugin From Command-Line\n\n```bash\nswift package plugin --allow-writing-to-package-directory swiftformat\n```\n\nYou can limit the formatting to a particular target with `--target` option.\n\nYou can also specify `SwiftFormat` arguments, e.g. `--swift-version`.\n\nExample\n\n```bash\nswift package plugin --allow-writing-to-package-directory swiftformat --target MyLibrary --swift-version 5.6 --verbose\n```\n\n### Trigger Plugin From Xcode\n\nIn Xcode 14 you can trigger the command plugin execution for a Swift package or an Xcode project.\n\nFor an Xcode project the project's main directory will be processed and the `--target` option will be ignored.\n\nYou can also specify `SwiftFormat` arguments, e.g. `--swift-version`.\n\n![Run plugin in Xcode 14](https://user-images.githubusercontent.com/4176826/179352584-db7f7f42-452c-4a42-a329-01b115a237a7.gif)\n\nVia AppleScript\n----------------\n\nTo run SwiftFormat on the frontmost Xcode document (project or workspace) you can use the following AppleScript:\n\n```applescript\ntell application \"Xcode\"\n    set frontWindow to the first window\n    set myPath to path of document of frontWindow\n    do shell script \"cd \" & myPath & \";cd ..; /usr/local/bin/swiftformat .\"\nend tell\n```\n\nSome Apps you can trigger this from are [BetterTouchTool](https://folivora.ai), [Alfred](https://www.alfredapp.com) or [Keyboard Maestro](https://www.keyboardmaestro.com/main/). Another option is to define a QuickAction for Xcode via Automator and then assign a keyboard shortcut for it in the System Preferences.\n\n\nVSCode plugin\n--------------\n\nIf you prefer to use Microsoft's [VSCode](https://code.visualstudio.com) editor for writing Swift, [Valentin Knabel](https://github.com/vknabel) has created a [VSCode plugin](https://marketplace.visualstudio.com/items?itemName=vknabel.vscode-swiftformat) for SwiftFormat.\n\n\nSublime Text plugin\n--------------------\n\nIf you prefer to use the [Sublime Text](https://www.sublimetext.com) editor, try the [Sublime-Swift-Format plugin](https://github.com/aerobounce/Sublime-Swift-Format) by [Aerobounce](https://github.com/aerobounce).\n\n\nNova plugin\n-----------\n\nIf you prefer to use the [Nova](https://panic.com/nova) editor, try the [SwiftFormat extension](https://extensions.panic.com/extensions/org.padraig/org.padraig.SwiftFormat/) by [PÃ¡draig Ã“ CinnÃ©ide](https://mastodon.social/@PadraigOCinneide).\n\n\nGit pre-commit hook\n---------------------\n\n1. Follow the instructions for installing the SwiftFormat command-line tool.\n\n2. Install [git-format-staged](https://github.com/hallettj/git-format-staged).\n\n3. Edit or create a `.git/hooks/pre-commit` file in your project folder. The .git folder is hidden but should already exist if you are using Git with your project, so open it with the terminal, or the Finder's `Go > Go to Folder...` menu.\n\n4. Add the following line in the pre-commit file. The `{}` will be replaced automatically by the path to the Swift file being formatted:\n\n    ```bash\n    #!/bin/bash\n    git-format-staged --formatter \"swiftformat stdin --stdin-path '{}'\" \"*.swift\"\n    ```\n    \n    (Note that this example uses your locally installed version of SwiftFormat, not a separate copy in your project repository. You can replace `swiftformat` with the path to a copy inside your project if you prefer.)\n    \n5. enable the hook by typing `chmod +x .git/hooks/pre-commit` in the terminal.\n \nThe pre-commit hook will now run whenever you run `git commit`. Running `git commit --no-verify` will skip the pre-commit hook.\n\n**NOTE:** If you are using Git via a GUI client such as [Tower](https://www.git-tower.com), [additional steps](https://www.git-tower.com/help/mac/faq-and-tips/faq/hook-scripts) may be needed.\n\n**NOTE (2):** Unlike the Xcode build phase approach, git pre-commit hook won't be checked in to source control, and there's no way to guarantee that all users of the project are using the same version of SwiftFormat. For a collaborative project, you might want to consider a *post*-commit hook instead, which would run on your continuous integration server.\n\nGitHub Actions\n---------------------\n\n1. SwiftFormat comes preinstalled on all macOS GitHub-hosted runners. If you are self hosting you will need to ensure SwiftFormat is installed on your runner.\n2. Create a GitHub Actions workflow using SwiftFormat, passing the `--reporter github-actions-log` command line option. The following example action lints pull requests using SwiftFormat, reporting warnings using the GitHub Actions log.\n```yaml\n# Lint.yml\nname: Lint\non: pull_request\n\njobs:\n  Lint:\n    runs-on: macos-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: SwiftFormat\n        run: swiftformat --lint . --reporter github-actions-log\n```\n\nOn CI using Danger\n-------------------\n\nTo setup SwiftFormat to be used by your continuous integration system using [Danger](http://danger.systems/ruby/), do the following:\n\n1. Follow the [`instructions`](http://danger.systems/guides/getting_started.html) to setup Danger.\n2. Add the [`danger-swiftformat`](https://github.com/garriguv/danger-ruby-swiftformat) plugin to your `Gemfile`.\n3. Add the following to your `Dangerfile`:\n\n    ```ruby\n    swiftformat.binary_path = \"/path/to/swiftformat\" # optional\n    swiftformat.additional_args = \"--indent tab --self insert\" # optional\n    swiftformat.check_format(fail_on_error: true)\n    ```\n\n    **NOTE:** It is recommended to add the `swiftformat` binary to your project directory to ensure the same version is used each time (see the [Xcode build phase](#xcode-build-phase) instructions above).\n\n\nBazel Build\n-----------\n\nIf you use [Bazel](https://bazel.build/) to build your Swift projects and want to ensure that only properly formatted code is merged to your main branch, try [rules_swiftformat](https://github.com/cgrindel/rules_swiftformat). The repository contains Bazel rules and macros that format Swift source files using SwiftFormat, test that the formatted files exist in the workspace directory, and copy the formatted files to the workspace directory.\n\n\nDocker\n-----------\n\nSwiftFormat publishes releases into [GitHub Packages](https://github.com/features/packages) Docker registry. To pull the image call:\n\n```bash\n$ docker pull ghcr.io/nicklockwood/swiftformat:latest\n```\n\nBy default, the container runs `swiftformat .` Therefore, you need to provide a path either via an argument:\n\n```bash\ndocker run --rm -v /local/source/path:/work ghcr.io/nicklockwood/swiftformat:latest /work\n```\n\nor by changing the working dir:\n\n```bash\ndocker run --rm -v /local/source/path:/work -w /work ghcr.io/nicklockwood/swiftformat:latest\n```\n\nTo check the installed SwiftFormat version:\n\n```bash\ndocker run --rm ghcr.io/nicklockwood/swiftformat:latest --version\n```\n\nLinting example:\n\n```bash\ndocker run --rm -v /local/source/path:/work ghcr.io/nicklockwood/swiftformat:latest /work --lint\n```\n\nPrerelease Builds\n-----------------\n\n***Prerelease builds are subject to breaking changes.***\n\nNew rules, options, and fixes are merged to the [`develop`](https://github.com/nicklockwood/SwiftFormat/commits/develop/) branch before being incorporated into an official release. You may want to use a prerelease version of SwiftFormat that includes the latest unreleased changes.\n\n**Homebrew:**\n\nThe [Homebrew](http://brew.sh/) `--HEAD` option downloads, builds, and installs the latest changes from the `develop` branch. \n\nYou can install a prerelease build via Homebrew by running:\n\n```bash\n$ brew install swiftformat --HEAD\n```\n\n**Nightly Builds:**\n\nNightly builds of the `develop` branch are available in the [calda/SwiftFormat-nightly](https://github.com/calda/SwiftFormat-nightly) repo. A new release is published every day, unless there have been no changes to `develop` since the last release. You can download executables for the latest nightly release [here](https://github.com/calda/SwiftFormat-nightly/releases/latest).\n\nCommit SHAs on `develop` are unstable since that branch is occasionally rebased, but artifact URLs and tags in [calda/SwiftFormat-nightly](https://github.com/calda/SwiftFormat-nightly) are stable references that can be used from other repos or tools.\n\nConfiguration\n-------------\n\nSwiftFormat's configuration is split between **rules** and **options**. Rules are functions in the SwiftFormat library that apply changes to the code. Options are settings that control the behavior of the rules. \n\n\nOptions\n-------\n\nThe options available in SwiftFormat can be displayed using the `--options` command-line argument. The default value for each option is indicated in the help text.\n\nRules are configured by adding `--[option_name] [value]` to your command-line arguments, or by creating a `.swiftformat` [config file](#config-file) and placing it in your project directory.\n\nA given option may affect multiple rules. Use `--rule-info [rule_name]` command for details about which options affect a given rule, or see the [Rules.md](https://github.com/nicklockwood/SwiftFormat/blob/main/Rules.md) file.\n\nYou can configure options for specific files or code ranges by using `swiftformat:options` directive in comments inside your Swift file. To temporarily set one or more options inside a source file, use:\n\n```swift\n// swiftformat:options --indent 2 --allman true\n```\n\nTo apply an options override only to a particular line, use the `:this`, `:next` or `:previous` modifiers:\n\n```swift\nlet indexUrl: URL // swiftformat:options:this --preserve-acronyms url \n\n// swiftformat:options:next --semicolons inline\ndoTheThing(); print(\"Did the thing\")\n```\n\n\nRules\n-----\n\nSwiftFormat includes over 50 rules, and new ones are added all the time. An up-to-date list can be found in [Rules.md](https://github.com/nicklockwood/SwiftFormat/blob/main/Rules.md) along with documentation for how they are used.\n\nThe list of available rules can be displayed within the command-line app using the `--rules` argument. Rules can be either enabled or disabled. Most are enabled by default. Disabled rules are marked with \"(disabled)\" when displayed using `--rules`.\n\nYou can use the `--rule-info [rule_name]` command to get information about a specific rule. Pass a comma-delimited list of rule names to get information for multiple rules at once, or use `--rule-info` with no argument for info on all rules.\n\nYou can disable rules individually using `--disable` followed by a list of one or more comma-delimited rule names, or enable opt-in rules using `--enable` followed by the rule names:\n\n```bash\n--disable redundantSelf,trailingClosures\n--enable isEmpty\n```\n\nIf you prefer, you can use multiple `--enable`/`--disable` arguments instead of using commas:\n\n```bash\n--disable indent\n--disable linebreaks\n--disable redundantSelf\n```\n\nAlternatively, you can use the line continuation character `\\` to wrap a single argument over multiple line:\n\n```bash         \n--disable          \\\n    indent,        \\\n    linebreaks,    \\\n    redundantSelf\n```\n\nTo avoid automatically opting-in to new rules when SwiftFormat is updated, you can disable all rules using:\n\n```bash\n--disable all\n```\n\nAnd then individually enable just the rules you want. Alternatively, use the`--rules` argument to *only* enable the rules you specify:\n\n```bash\n--rules indent,linebreaks\n```\n\nAs above, you may include multiple `--rules` arguments, or use the line continuation character `\\` to wrap the rules onto separate lines:\n\n```bash\n--rules redundantSelf\n--rules         \\\n    indent,     \\\n    linebreaks\n```\n\nTo see exactly which rules were applied to a given file, you can use the `--verbose` command-line option to force SwiftFormat to print a more detailed log as it applies the formatting. **NOTE:** running in verbose mode is slower than the default mode.\n\nYou can disable rules for specific files or code ranges by using `swiftformat:` directives in comments inside your Swift file. To temporarily disable one or more rules inside a source file, use:\n\n```swift\n// swiftformat:disable <rule1> [<rule2> [rule<3> ...]]\n```\n\nTo enable the rule(s) again, use:\n\n```swift\n// swiftformat:enable <rule1> [<rule2> [rule<3> ...]]\n```\n\nTo disable all rules use:\n\n```swift\n// swiftformat:disable all\n```\n\nAnd to enable them all again, use:\n\n```swift\n// swiftformat:enable all\n```\n\nTo temporarily prevent one or more rules being applied to just the next line, use:\n\n```swift\n// swiftformat:disable:next <rule1> [<rule2> [rule<3> ...]]\nlet foo = bar // rule(s) will be disabled for this line\nlet bar = baz // rule(s) will be re-enabled for this line\n```\n\nYou can also use `this` or `previous` to enable or disable rules for the current or previous line. There is no need to manually re-enable a rule after using the `next`, `this` or `previous` directives.\n\n**NOTE:** The `swiftformat:enable` directive only serves to counter a previous `swiftformat:disable` directive in the same file. It is not possible to use `swiftformat:enable` to enable a rule that was not already enabled when formatting started.\n\n\nSwift version\n-------------\n\nMost SwiftFormat rules are version-agnostic, but some are applicable only to newer Swift versions. These rules will be disabled automatically if the Swift version is not specified, so to make sure that the full functionality is available you should specify the version of Swift that is used by your project.\n\nYou can specify the Swift compiler version in one of two ways:\n\nYou can specify your project's Swift compiler version using the `--swift-version` command line argument. You can also add the `--swift-version` option to your `.swiftformat` file.\n\nAnother option is to add a `.swift-version` file to your project directory. This is a text file that should contain the minimum Swift version supported by your project, and is also supported by some other tools. Any `.swift-version` files always take precedence over the `--swift-version` argument.\n\nBoth the `.swift-version` file and the `--swift-version` option in a `.swiftformat` file are applied hierarchically; If you have submodules in your project that use a different Swift version, you can add separate swift version configurations for those directories.\n\nSwift language mode\n-------------------\n\nSwiftFormat also allows you to specify the Swift _language mode_ used by your project. This is distinct from the Swift compiler version. For example, you can use the Swift 6.0 compiler with either the Swift 5 language mode or the Swift 6 language mode. Some SwiftFormat rules will behave differently under different Swift language modes.\n\nYou can specify your project's Swift language mode using the `--language-mode` command line argument. You can also add the `--language-mode` option to your `.swiftformat` file.\n\nIf not specified, SwiftFormat uses the default language mode of the specified Swift compiler version. The default language mode in Swift 5.x and Swift 6.x is the Swift 5 language mode. If your project uses the Swift 6 language mode, you should specify `--language-mode 6`.\n\n\nConfig file\n-----------\n\nAlthough it is possible to configure SwiftFormat directly by using the command-line [options](#options) and [rules](#rules) detailed above, it is sometimes more convenient to create a configuration file, which can be added to your project and shared with other developers.\n\nA SwiftFormat configuration file consists of one or more command-line options, split onto separate lines, e.g:\n\n```\n--allman true\n--indent tab\n--disable elseOnSameLine,semicolons\n```\n\nWhile formatting, SwiftFormat will automatically check inside each subdirectory for the presence of a `.swiftformat` file and will apply any options that it finds there to the files in that directory.\n\nThis allows you to override certain rules or formatting options just for a particular directory of files. You can also specify excluded files relative to that directory using `--exclude`, which may be more convenient than specifying them at the top-level:\n\n```\n--exclude Pods,Generated\n```\n\nThe `--exclude` option takes a comma-delimited list of file or directory paths to exclude from formatting. Excluded paths are relative to the config file containing the `--exclude` command. The excluded paths can include wildcards, specified using Unix \"Glob\" syntax, as [documented below](#globs).\n\nConfig files named \".swiftformat\" will be processed automatically, however, you can select an additional configuration file to use for formatting using the `--config \"path/to/config/file\"` command-line argument. A configuration file selected using `--config` does not need to be named \".swiftformat\", and can be located outside of the project directory.\n\nThe config file format is designed to be edited by hand. You may include blank lines for readability, and can also add comments using a hash prefix (#), e.g.\n\n```\n# format options\n--allman true\n--indent tab # tabs FTW!\n\n# file options\n--exclude Pods\n\n# rules\n--disable elseOnSameLine,semicolons\n```\n\nYou can create multiple configuration sections within a single `.swiftformat` file to apply different formatting options to different parts of your project. Each section should specify a `--filter` glob pattern to determine which files the configuration applies to. Options in that section are used when formatting files that match `--filter` glob, in addition to the base options in the file.\n\n```\n--enable indent\n--indent 4\n\n[Tests]\n--filter **/Tests/**\n--enable noForceUnwrapInTests\n--enable noForceTryInTests\n--indent 2\n```\n\nIf you would prefer not to edit the configuration file by hand, you can use the [SwiftFormat for Xcode](#xcode-source-editor-extension) app to edit the configuration and export a configuration file. You can also use the swiftformat command-line-tool's `--infer-options` command to generate a config file from your existing project, like this:\n\n```bash\n$ cd /path/to/project\n$ swiftformat --infer-options . --output .swiftformat\n```\n\nGlobs\n-----\n\nWhen excluding files from formatting using the `--exclude` option, you may wish to make use of wildcard paths (aka \"Globs\") to match all files that match a particular naming convention without having to manually list them all.\n\nSwiftFormat's glob syntax is based on Ruby's implementation, which varies slightly from the Unix standard. The following patterns are supported:\n\n* `*` - A single star matches zero or more characters in a filename, but *not* a `/`.\n\n* `**` - A double star will match anything, including one or more `/`.\n\n* `?` - A question mark will match any single character except `/`.\n\n* `[abc]` - Matches any single character inside the brackets.\n\n* `[a-z]` - Matches a single character in the specified range in the brackets.\n\n* `{foo,bar}` - Matches any one of the comma-delimited strings inside the braces.\n\nExamples:\n\n* `foo.swift` - Matches the file \"foo.swift\" in the same directory as the config file.\n\n* `*.swift` - Matches any Swift file in the same directory as the config file.\n\n* `foo/bar.swift` - Matches the file \"bar.swift\" in the directory \"foo\".\n\n* `**/foo.swift` - Matches any file named \"foo.swift\" in the project.\n\n* `**/*.swift` - Matches any Swift file in the project.\n\n* `**/Generated` - Matches any folder called `Generated` in the project.\n\n* `**/*_generated.swift` - Matches any Swift file with the suffix \"_generated\" in the project.\n\n\nLinting\n-------\n\nSwiftFormat is primarily designed as a formatter rather than a linter, i.e. it is designed to fix your code rather than tell you what's wrong with it. However, sometimes it can be useful to verify that code has been formatted in a context where it is not desirable to actually change it.\n\nA typical example would be as part of a CI (Continuous Integration) process, where you may wish to have an automated script that checks committed code for style violations. While you can use a separate tool such as [SwiftLint](https://github.com/realm/SwiftLint) for this, it makes sense to be able to validate the formatting against the exact same rules as you are using to apply it.\n\nIn order to run SwiftFormat as a linter, you can use the `--lint` command-line option:\n\n```bash\n$ swiftformat --lint path/to/project\n```\n\nThis runs the same rules as format mode, and all the same configuration options apply, however, no files will be modified. Instead, SwiftFormat will format each file in memory and then compare the result against the input and report the lines that required changes.\n\nThe `--lint` option is similar to `--dry-run`, but `--lint` returns warnings for every line that required changes, and will return a nonzero error code (see [Error codes](#error-codes) below) if any changes are detected, which is useful if you want it to fail a build step on your CI server.\n\nIf you would prefer `--lint` not to fail your build, you can use the `--lenient` option to force SwiftFormat to return success in `--lint` mode even when formatting issues were detected.\n\n```bash\n$ swiftformat --lint --lenient path/to/project\n```\n\nBy default, `--lint` will only report lines that require formatting, but you can use the additional `--verbose` flag to display additional info about which files were checked, even if there were no changes needed.\n\nIf you would prefer not to see a warning for each and every formatting change, you can use the `--quiet` flag to suppress all output except errors.\n\nSometimes you may wish to autoformat some rules, but only lint others. To do that, use the `--lintonly` option in your config file to specify rules that should only be applied in `--lint` mode:\n\n```\n--rules braces,indent\n--lint-only trailingClosures,unusedArguments\n```\n\n\nError codes\n-----------\n\nThe swiftformat command-line tool will always exit with one of the following codes:\n\n* 0 - Success. This code will be returned in the event of a successful formatting run or if `--lint` detects no violations.\n* 1 - Lint failure. This code will be returned when running in `--lint` mode, or when autocorrecting in `--strict` mode, if the input requires formatting.\n* 70 - Program error. This code will be returned if there is a problem with the input or configuration arguments.\n\n\nCache\n------\n\nSwiftFormat uses a cache file to avoid reformatting files that haven't changed. For a large project, this can significantly reduce processing time.\n\nBy default, the cache is stored in `~/Library/Caches/com.charcoaldesign.swiftformat` on macOS, or `/tmp/com.charcoaldesign.swiftformat` on Linux. Use the command-line option `--cache ignore` to ignore the cached version and re-apply formatting to all files. Alternatively, you can use `--cache clear` to delete the cache (or you can just manually delete the cache file).\n\nThe cache is shared between all projects. The file is fairly small, as it only stores the path and size for each file, not the contents. If you do start experiencing slowdown due to the cache growing too large, you might want to consider using a separate cache file for each project.\n\nYou can specify a custom cache file location by passing a path as the `--cache` option value. For example, you might want to store the cache file inside your project directory. It is fine to check in the cache file if you want to share it between different users of your project, as the paths stored in the cache are relative to the location of the formatted files.\n\n\nFile headers\n-------------\n\nSwiftFormat can be configured to strip or replace the header comments in every file with a template. The \"header comment\" is defined as a comment block that begins on the first nonblank line in the file, and is followed by at least one blank line. This may consist of a single comment body, or multiple comments on consecutive lines:\n\n```swift\n// This is a header comment\n```\n\n```swift\n// This is a regular comment\nfunc foo(bar: Int) -> Void { ... }\n```\n\nThe header template is a string that you provide using the `--header` command-line option. Passing a value of `ignore` (the default) will leave the header comments unmodified. Passing `strip` or an empty string `\"\"` will remove them. If you wish to provide a custom header template, the format is as follows:\n\nFor a single-line template: `--header \"Copyright (c) 2017 Foobar Industries\"`\n\nFor a multiline comment, mark linebreaks with `\\n`: `--header \"First line\\nSecond line\"`\n\nYou can optionally include Swift comment markup in the template if you wish: `--header \"/*--- Header comment ---*/\"`\n\nIf you do not include comment markup, each line in the template will be prepended with `//` and a single space.\n\nIt is common practice to include the file name, creation date and/or the current year in a comment header copyright notice. To do that, you can use the following placeholders:\n\n* `{file}` - the name of the file\n* `{year}` - the current year\n* `{created}` - the date on which the file was created\n* `{created.year}` - the year in which the file was created\n* `{author.name}` - the name of the user who first committed the file\n* `{author.email}` - the email of the user who first committed the file \n\nFor example, a header template of:\n\n```bash\n--header \"{file}\\nCopyright (c) {year} Foobar Industries\\nCreated by John Smith on {created}.\"\n```\n\nWill be formatted as:\n\n```swift\n// SomeFile.swift\n// Copyright (c) 2019 Foobar Industries\n// Created by John Smith on 01/02/2016.\n```\n\n**NOTE:** the `{year}` value and `{created}` date format are determined from the current locale and timezone of the machine running the script. `{author.name}` and `{author.email}` requires the project to be version controlled by git.\n\n\nMarkdown formatting\n-------------------\n\nSwiftFormat can format Swift code blocks inside Markdown files (`.md`). This is useful for keeping code examples in documentation, README files, and other markdown content properly formatted.\n\n````diff\n  ### Sample README\n  \n  This is a nice project with lots of cool APIs to know about, including:\n  \n  ```swift\n  func foo(\n- bar: Bar,\n- baaz: Baaz\n+     bar: Bar,\n+     baaz: Baaz\n  ) -> Foo { ... }\n  ```\n````\n\nTo format Swift code blocks in markdown files, use the `--markdown-files` option with either `strict` or `lenient`:\n\n```bash\n$ swiftformat . --markdown-files strict\n$ swiftformat . --markdown-files lenient\n```\n\nOr add it to your `.swiftformat` config file:\n\n```\n--markdown-files strict\n```\n\n**Formatting modes:**\n\nSwiftFormat supports two modes for handling markdown files:\n\n- `lenient` (default): Ignores parsing errors in code blocks and continues formatting\n- `strict`: Fails if any code blocks contain parsing errors\n\nSwiftFormat's tokenizer is more permissive than the Swift compiler and typically only emits errors when encountering unbalanced scope tokens like `(` or `{`.\n\n**Code block options:**\n\nYou can specify options for options for individual code blocks by adding them after the opening delimiter. For example, you can use `no-format` to prevent a code block from being parsed or formatted:\n\n````md\n```swift no-format\nfunc example()\n{\n    doSomething()\n}\n```\n````\n\nYou can also specify SwiftFormat command line options to configure the behavior of individual rules:\n\n````md\n```swift --indent 2\nfunc example() {\n  doSomething()\n}\n```\n\n```swift --disable redundantSelf\nfunc example() {\n    self.doSomething()\n}\n```\n````\n\nFAQ\n-----\n\n*Q. How is this different from SwiftLint?*\n\n> A. SwiftLint is primarily designed to find and report code smells and style violations in your code. SwiftFormat is designed to fix them. While SwiftLint can autocorrect some issues, and SwiftFormat has some support for [linting](#linting), their primary functions are different.\n\n\n*Q. Can SwiftFormat and SwiftLint be used together?*\n\n> A. Absolutely! The style rules encouraged by both tools are quite similar, and SwiftFormat even fixes some style violations that SwiftLint warns about but can't currently autocorrect.\n\n\n*Q. What platforms does SwiftFormat support?*\n\n> A. SwiftFormat works on macOS 10.13 (High Sierra) and above, and also runs on Ubuntu Linux and Windows.\n\n\n*Q. What versions of Swift are supported?*\n\n> A. The SwiftFormat framework and command-line tool can be compiled using Swift 5.3 and above, and can format programs written in Swift 4.x or 5. Swift 3.x is no longer actively supported. If you are still using Swift 3.x or earlier and find that SwiftFormat breaks your code, the best solution is probably to revert to an earlier SwiftFormat release, or enable only a small subset of rules. Use the `--swift-version` argument to enable additional rules specific to later Swift versions.\n\n\n*Q. SwiftFormat made changes I didn't want it to. How can I find out which rules to disable?*\n\n> A. If you run SwiftFormat using the `--verbose` option, it will tell you which rules were applied to each file. You can then selectively disable certain rules using the `--disable` argument (see below).\n\n\n*Q. People on my team have different SwiftFormat versions installed. How can we ensure consistent formatting?\n\n> A. You can specify a `--min-version` argument in your project's .swiftformat` file to fail the build if developers attempt to use an older SwiftFormat version.\n\n\n*Q. How can I modify the formatting rules?*\n\n> A. Many configuration options are exposed in the command-line interface or `.swiftformat` configuration file. You can either set these manually, or use the `--infer-options` argument to automatically generate the configuration from your existing project.\n\n> If there is a rule that you don't like, and which cannot be configured to your liking via the command-line options, you can disable one or more rules by using the `--disable` argument, followed by the name of the rules, separated by commas. You can display a list of all supported rules using the `--rules` argument, and their behaviors are documented above this section in the README.\n\n> If you are using the Xcode source editor extension, rules and options can be configured using the [SwiftFormat for Xcode](#xcode-source-editor-extension) host application. Unfortunately, due to limitation of the Extensions API, there is no way to configure these on a per-project basis.\n\n> If the options you want aren't exposed, and disabling the rule doesn't solve the problem, the rules are implemented in the file `Rules.swift`, so you can modify them and build a new version of the command-line tool. If you think your changes might be generally useful, make a pull request.\n\n\nQ. I don't want to be surprised by new rules added when I upgrade SwiftFormat. How can I prevent this?\n\n> A. You can use the `--rules` argument to specify an exclusive list of rules to run. If new rules are added, they won't be enabled if you have specified a `--rules` list in your SwiftFormat configuration.\n\n\n*Q. Why can't I set the indent width or choose between tabs/spaces in the [SwiftFormat for Xcode](#xcode-source-editor-extension) options?*\n\n> Indent width and tabs/spaces can be configured in Xcode on a per project-basis. You'll find the option under \"Text Settings\" in the Files inspector of the right-hand sidebar.\n\n\n*Q. After applying SwiftFormat, my code won't compile. Is that a bug?*\n\n> A. SwiftFormat should ideally never break your code. Check the [known issues](#known-issues), and if it's not already listed there, or the suggested workaround doesn't solve your problem, please [open an issue on GitHub](https://github.com/nicklockwood/SwiftFormat/issues).\n\n\n*Q. Can I use SwiftFormat to lint my code without changing it?*\n\n> A. Yes, see the [linting](#linting) section above for details.\n\n\n*Q. Can I use the `SwiftFormat.framework` inside another app?*\n\n> A. Yes, the SwiftFormat framework can be included in an app or test target, and used for many kinds of parsing and processing of Swift source code besides formatting. The SwiftFormat framework is available as a [CocoaPod](https://cocoapods.org/pods/SwiftFormat) for easy integration.\n\n*Q. How to create own rule?*\n\n> A. 1) Open `SwiftFormat.xcodeproj`; 2) Add a rule in `Sources/Rules/..`; 3) Add a test in `Tests/Rules/..`; 4) Add an example in `Sources/Examples.swift`; 5) Run all tests.\n\n*Q. How do I run and debug the command line tool in Xcode while developing a new rule?*\n\n> A. You can run the `swiftformat` command line tool via the `Swift Format (Command Line Tool)` scheme, and you can pass in arguments like `/path/to/my/code --config /path/to/my/config` as the `Arguments Passed On Launch` in Xcode's scheme editor. More instructions are available [here](https://github.com/nicklockwood/SwiftFormat/pull/1804#issuecomment-2263079432).\n\nKnown issues\n---------------\n\n* When using the Xcode Source Editor Extension, the SwiftFormat menu sometimes disappears from Xcode. If this happens, try moving or renaming Xcode temporarily and then changing it back. Failing that, the suggestions in [this thread](https://github.com/nicklockwood/SwiftFormat/issues/494) may help.\n\n* The `enumNamespaces` rule replaces classes that have only static members with an `enum`. If the class is subclassed, or if there is code that depends on the class exposing certain runtime behaviors, this may break the program. To solve this you can either fix it on a per-case basis by adding a `// swiftformat:disable:next enumNamespaces` comment directive above the class declaration, or you can add `--enum-namespaces structs-only` to prevent the rule being applied to classes, or you can just disable the `enumNamespaces` rule completely.\n\n* The `redundantVoidReturnType` rule can inadvertently alter the type signature for closures, for example in cases where the closure calls a `@discardableResult` function. To solve this you can either fix it on a per-case basis by adding a `// swiftformat:disable:next redundantVoidReturnType` comment directive to disable the rule for a specific call site, or you can add `--closure-void preserve` to your [configuration](#configuration) to disable the rule completely for closures (regular functions or methods aren't affected).\n\n* The `redundantType` rule can introduce ambiguous code in certain cases when using the default mode of `--redundant-type inferred`. This can be worked around by by using `--redundant-type explicit`, or by manually removing the redundant type reference on the affected line, or by using the `// swiftformat:disable:next redundantType` comment directive to disable the rule at the call site (or just disable the `redundantType` rule completely).\n\n* If a type initializer or factory method returns an implicitly unwrapped optional value then the `redundantType` rule may remove the explicit type in a situation where it's actually required. To work around this you can either use `--redundant-type explicit`, or use the `// swiftformat:disable:next redundantType` comment directive to disable the rule at the call site (or just disable the `redundantType` rule completely).\n\n* When using the `initCoderUnavailable` rule, if an `init` that is marked as unavailable is overridden elsewhere in the program then it will cause a compilation error. The recommended workaround is to remove the override (which shouldn't affect the program behavior if the init was really unused) or use the `// swiftformat:disable:next initCoderUnavailable` comment directive to disable the rule for the overridden init (or just disable the `initCoderUnavailable` rule completely).\n\n* When using the `extensionAccessControl` rule with the `--extension-acl on-extension` option, if you have public methods defined on an internal type defined in another file, the resultant public extension will no longer compile. The recommended solution is to manually remove the `public` modifier (this won't change the program behavior) or disable the `extensionAccessControl` rule.\n\n* When using the `preferKeyPath` rule, conversion of `compactMap { $0.foo }` to `compactMap(\\.foo)` or `flatMap { $0.foo }` to `flatMap(\\.foo)` will result in code that fails to compile if `foo` is not an `Optional` property. This is due to a difference in the way that Swift handles type inference for closures vs keyPaths, as discussed [here](https://bugs.swift.org/browse/SR-13347). The recommended workaround is to replace `compactMap()` or `flatMap()` with `map()` in these cases, which will not change the behavior of the code.\n\n* When using the `--self remove` option, the `redundantSelf` rule will remove references to `self` in autoclosure arguments, which may change the meaning of the code, or cause it not to compile. To work around this issue, use the `--self-required` option to provide a comma-delimited list of methods to be excluded from the rule. The `expect()` function from the popular [Nimble](https://github.com/Quick/Nimble) unit testing framework is already excluded by default. If you are using the `--self insert` option then this is not an issue.\n\n* If you assign `SomeClass.self` to a variable and then instantiate an instance of the class using that variable, Swift requires that you use an explicit `.init()`, however, the `redundantInit` rule is not currently capable of detecting this situation in all cases, and may remove the `.init`. To work around this issue, use the `// swiftformat:disable:next redundantInit` comment directive to disable the rule for any affected lines of code (or just disable the `redundantInit` rule completely).\n\n* The `--self insert` option can only recognize locally declared member variables, not ones inherited from superclasses or extensions in other files, so it cannot insert missing `self` references for those. Note that the reverse is not true: `--self remove` should remove *all* redundant `self` references.\n\n* The `trailingClosures` rule can generate ambiguous code if a function has multiple optional closure arguments, or if multiple functions have signatures differing only by the name of the closure argument. For this reason, the rule is limited to anonymous closure arguments by default. You can use the `--trailing-closures` and `--never-trailing` arguments to explicitly opt in or out of trailing closure support for specific functions.\n\n* The `isEmpty` rule will convert `count == 0` to `isEmpty` even for types that do not have an `isEmpty` method, such as `NSArray`/`NSDictionary`/etc. Use of Foundation collections in Swift code is pretty rare, but just in case, the rule is disabled by default.\n\n* The `preferForLoop` rule will convert `foo.forEach` to `for item in foo` even for types that do not conform to the `Sequence` protocol and cannot be used with a `for ... in` loop. There are no such types built in, but custom types may have this issue.\n\n* If a file begins with a comment, the `stripHeaders` rule will remove it if it is followed by a blank line. To avoid this, make sure that the first comment is directly followed by a line of code.\n\n* When running a version of SwiftFormat built using Xcode 10.2 on macOS 10.14.3 or earlier, you may experience a crash with the error \"dyld: Library not loaded: @rpath/libswiftCore.dylib\". To fix this, you need to install the [Swift 5 Runtime Support for Command Line Tools](https://support.apple.com/kb/DL1998). These tools are included by default in macOS 10.14.4 and later.\n\n* If you have a generic typealias that defines a closure (e.g. `typealias ResultCompletion<T> = (Result<T, Error>) -> Void`) and use this closure as an argument in a generic function (e.g. `func handle<T: Decodable>(_ completion: ResultCompletion<T>)`), the `opaqueGenericParameters` rule may update the function definition to use `some` syntax (e.g. `func handle(_ completion: ResultCompletion<some Decodable>)`). `some` syntax is not permitted in closure parameters, so this will no longer compile. Workarounds include spelling out the closure explicitly in the generic function (instead of using a `typealias`) or disabling the `opaqueGenericParameters` rule (e.g. with `// swiftformat:disable:next opaqueGenericParameters`).\n\n* If compiling for macOS with Xcode 14.0 and configuring SwiftFormat with `--swift-version 5.7`, the `genericExtensions` rule may cause a build failure by updating extensions of the format `extension Collection where Element == Foo` to `extension Collection<Foo>`. This fails to compile for macOS in Xcode 14.0, because the macOS SDK in that version of Xcode [does not include](https://forums.swift.org/t/xcode-14-rc-cannot-specialize-protocol-type/60171) the Swift 5.7 standard library. Workarounds include using `--swift-version 5.6` instead, updating to Xcode 14.1+, or disabling the `genericExtensions` rule (e.g. with `// swiftformat:disable:next genericExtensions`).\n\n* The `propertyTypes` rule can cause a build failure in cases where there are multiple static overloads with the same name but different return types. As a workaround you can rename the overloads to no longer conflict, or exclude the property name with `--preserve-symbols propertyName,otherPropertyName,etc`.\n\n* The `propertyTypes` rule can cause a build failure in cases where the property's type is a protocol / existential like `let shapeStyle: ShapeStyle = .myShapeStyle`, and the value used on the right-hand side is defined in an extension like `extension ShapeStyle where Self == MyShapeStyle { static var myShapeStyle: MyShapeStyle { ... } }`. As a workaround you can use the existential `any` syntax (`let shapeStyle: any ShapeStyle = .myShapeStyle`), which the rule will preserve as-is, or exclude the type name and/or property name with `--preserve-symbols ShapeStyle,myShapeStyle,etc`.\n\n* The `propertyTypes` rule can cause a build failure in cases like `let foo = Foo.bar` where the value is a static member that doesn't return the same time. For example, `let foo: Foo = .bar` would be invalid if the `bar` property was defined as `static var bar: Bar`. As a workaround you can write the name of the type explicitly, like `let foo: Bar = Foo.bar`, or exclude the type name and/or property name with `--preserve-symbols Bar,bar,etc`.\n\n\nTip Jar\n-----------\n\nSwiftFormat is not a commercially-funded product, it's a labor of love given freely to the community. If you find it useful, please consider making a donation.\n\n[![Donate via PayPal](https://www.paypalobjects.com/en_GB/i/btn/btn_donate_LG.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=9ZGWNK5FEZFF6&source=url)\n\n\nCredits\n------------\n\n* [Cal Stephens](https://github.com/calda) - Numerous new formatting rules, options and bug fixes\n* [Tony Arnold](https://github.com/tonyarnold) - SwiftFormat for Xcode\n* [Vincent Bernier](https://github.com/vinceburn) - SwiftFormat for Xcode settings UI\n* [Vikram Kriplaney](https://github.com/markiv) - SwiftFormat for Xcode icon and search feature\n* [Hyperphonic](https://github.com/hyperphonic0) - Xcode 12 compatibility for SwiftFormat\n* [Maxime Marinel](https://github.com/bourvill) - Git pre-commit hook script\n* [Romain Pouclet](https://github.com/palleas) - Homebrew formula\n* [Aerobounce](https://github.com/aerobounce) - Homebrew cask and Sublime Text plugin\n* [Facundo Menzella](https://github.com/facumenzella) - Several new formatting rules and options\n* [Ali Akhtarzada](https://github.com/aliak00) - Several path-related CLI enhancements\n* [Yonas Kolb](https://github.com/yonaskolb) - Swift Package Manager integration\n* [Wolfgang Lutz](https://github.com/Lutzifer) - AppleScript integration instructions\n* [BalÃ¡zs KilvÃ¡dy](https://github.com/balitm) - Xcode lint warning integration\n* [Anthony Miller](https://github.com/AnthonyMDev) - Improvements to wrap/indent logic\n* [Shingo Takagi](https://github.com/zizi4n5) - Several brace-related bug fixes\n* [Benedek Kozma](https://github.com/cyberbeni) - Lint-only rules option\n* [Juri Pakaste](https://github.com/juri) - Filelist feature\n* [Jim Puls](https://github.com/puls) - Big Sur icon update\n* [Daniele Formichelli](https://github.com/danyf90) - JSON reporter\n* [Jonas Boberg](https://github.com/bobergj) - Github actions log reporter\n* [Mahdi Bchatnia](https://github.com/inket) - Linux build workflow\n* [Saleem Abdulrasool](https://github.com/compnerd) - Windows build workflow\n* [Arthur Semenyutin](https://github.com/vox-humana) - Docker image\n* [Marco Eidinger](https://github.com/MarcoEidinger) - Swift Package Manager plugin\n* [Hampus TaÌŠgerud](https://github.com/hampustagerud) - Git integration for fileHeader rule\n* [Nick Lockwood](https://github.com/nicklockwood) - Everything else\n\n([Full list of contributors](https://github.com/nicklockwood/SwiftFormat/graphs/contributors))\n",
      "stars_today": 1
    },
    {
      "id": 60561834,
      "name": "android",
      "full_name": "nextcloud/android",
      "description": "ğŸ“± Nextcloud Android app",
      "html_url": "https://github.com/nextcloud/android",
      "stars": 5099,
      "forks": 1912,
      "language": "Kotlin",
      "topics": [
        "android",
        "hacktoberfest",
        "java",
        "kotlin",
        "mobile",
        "mobile-app",
        "nextcloud",
        "open-source",
        "opensource"
      ],
      "created_at": "2016-06-06T21:23:36Z",
      "updated_at": "2026-01-27T18:36:24Z",
      "pushed_at": "2026-01-28T01:45:40Z",
      "open_issues": 1441,
      "owner": {
        "login": "nextcloud",
        "avatar_url": "https://avatars.githubusercontent.com/u/19211038?v=4"
      },
      "readme": "<!--\n ~ SPDX-FileCopyrightText: 2016-2024 Nextcloud GmbH and Nextcloud contributors\n ~ SPDX-License-Identifier: AGPL-3.0-or-later OR GPL-2.0-only\n-->\n# [Nextcloud](https://nextcloud.com) Android app :iphone:\n\n[![REUSE status](https://api.reuse.software/badge/github.com/nextcloud/android)](https://api.reuse.software/info/github.com/nextcloud/android) [![Build Status](https://drone.nextcloud.com/api/badges/nextcloud/android/status.svg)](https://drone.nextcloud.com/nextcloud/android) [![Codacy Badge](https://app.codacy.com/project/badge/Grade/fb4cf26336774ee3a5c9adfe829c41aa)](https://app.codacy.com/gh/nextcloud/android/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade) [![Releases](https://img.shields.io/github/release/nextcloud/android.svg)](https://github.com/nextcloud/android/releases/latest)\n\n[<img src=\"https://play.google.com/intl/en_us/badges/images/generic/en_badge_web_generic.png\" \nalt=\"Download from Google Play\" \nheight=\"80\">](https://play.google.com/store/apps/details?id=com.nextcloud.client)\n[<img src=\"https://f-droid.org/badge/get-it-on.png\"\nalt=\"Get it on F-Droid\"\nheight=\"80\">](https://f-droid.org/packages/com.nextcloud.client/)\n[<img src=\"https://github.com/user-attachments/assets/713d71c5-3dec-4ec4-a3f2-8d28d025a9c6\"\nalt=\"Get it with Obtainium\"\nheight=\"80\">](https://apps.obtainium.imranr.dev/redirect?r=obtainium://app/%7B%22id%22%3A%22com.nextcloud.client%22%2C%22url%22%3A%22https%3A%2F%2Fgithub.com%2Fnextcloud%2Fandroid%22%2C%22author%22%3A%22nextcloud%22%2C%22name%22%3A%22Nextcloud%22%2C%22preferredApkIndex%22%3A0%2C%22additionalSettings%22%3A%22%7B%5C%22includePrereleases%5C%22%3Afalse%2C%5C%22fallbackToOlderReleases%5C%22%3Atrue%2C%5C%22filterReleaseTitlesByRegEx%5C%22%3A%5C%22%5C%22%2C%5C%22filterReleaseNotesByRegEx%5C%22%3A%5C%22%5C%22%2C%5C%22verifyLatestTag%5C%22%3Afalse%2C%5C%22sortMethodChoice%5C%22%3A%5C%22date%5C%22%2C%5C%22useLatestAssetDateAsReleaseDate%5C%22%3Afalse%2C%5C%22releaseTitleAsVersion%5C%22%3Afalse%2C%5C%22trackOnly%5C%22%3Afalse%2C%5C%22versionExtractionRegEx%5C%22%3A%5C%22%5C%22%2C%5C%22matchGroupToUse%5C%22%3A%5C%22%5C%22%2C%5C%22versionDetection%5C%22%3Atrue%2C%5C%22releaseDateAsVersion%5C%22%3Afalse%2C%5C%22useVersionCodeAsOSVersion%5C%22%3Afalse%2C%5C%22apkFilterRegEx%5C%22%3A%5C%22%5Enextcloud.*%5C%22%2C%5C%22invertAPKFilter%5C%22%3Afalse%2C%5C%22autoApkFilterByArch%5C%22%3Atrue%2C%5C%22appName%5C%22%3A%5C%22%5C%22%2C%5C%22appAuthor%5C%22%3A%5C%22%5C%22%2C%5C%22shizukuPretendToBeGooglePlay%5C%22%3Afalse%2C%5C%22allowInsecure%5C%22%3Afalse%2C%5C%22exemptFromBackgroundUpdates%5C%22%3Afalse%2C%5C%22skipUpdateNotifications%5C%22%3Afalse%2C%5C%22about%5C%22%3A%5C%22Nextcloud%20ist%20eine%20Cloudanwendung%2C%20die%20selbst%20gehostet%20werden%20kann.%5C%22%2C%5C%22refreshBeforeDownload%5C%22%3Atrue%7D%22%2C%22overrideSource%22%3Anull%7D)\n\nSigning certificate fingerprint to [verify](https://developer.android.com/studio/command-line/apksigner#usage-verify) the APK using the official Android documentation.\n- APK with \"gplay\" name, found [here](https://github.com/nextcloud/android/releases) or distributed via Google Play Store\n- APK with \"nextcloud\", found [here](https://github.com/nextcloud/android/releases)\n- not suitable for Fdroid downloads, as Fdroid is signing it on their own\n```\nSHA-256: fb009522f65e25802261b67b10a45fd70e610031976f40b28a649e152ded0373   \nSHA-1: 74aa1702e714941be481e1f7ce4a8f779c19dcea\n```\n\n**The Android client for [Nextcloud](https://nextcloud.com). Easily work with your data on your Nextcloud.**\n\n![App screenshots](/doc/Nextcloud_Android_Screenshots.png \"App screenshots\")\n\n## Getting help :rescue\\_worker\\_helmet:\n\nNote: The section *Known Problems / FAQs* below may already document your situation.\n\nIf you need assistance or want to ask a question about the Android app, you are welcome to [ask for support](https://help.nextcloud.com/c/clients/android) in the [Nextcloud Help Forum](https://help.nextcloud.com). If you have found a probable bug or have an enhancement idea, feel free to [open a new Issue on GitHub](https://github.com/nextcloud/android/issues).\n\nIf you're not sure if something is a bug or a configuration matter (with your client, server, proxy, etc.), the [Nextcloud Help Forum](https://help.nextcloud.com) is probably the best place to start so that you can get feedback (you can always return here, after getting feedback there, to report a suspected bug). \n\nKeep in mind, that this repository only manages the Android app. If you find bugs or have problems with the server/backend, you should use the Nextcloud Help Forum to ask for help or report the bug to the [Nextcloud server team](https://github.com/nextcloud/server)!\n\n## How to contribute :rocket:\n\nIf you want to [contribute](https://nextcloud.com/contribute/) to the Nextcloud Android client app, there are many ways to help whether or not you are a coder: \n\n*   helping out other users on our forum at https://help.nextcloud.com\n*   providing translations of the app on [Transifex](https://app.transifex.com/nextcloud/nextcloud/android/)\n*   reporting problems / suggesting enhancements by [opening new issues](https://github.com/nextcloud/android/issues/new/choose)\n*   implementing proposed bug fixes and enhancement ideas by submitting PRs (associated with a corresponding issue preferably)\n*   reviewing [pull requests](https://github.com/nextcloud/android/pulls) and providing feedback on code, implementation, and functionality\n*   Add [automated tests](CONTRIBUTING.md#testing) for existing functionality\n*   installing and testing [pull request builds](https://github.com/nextcloud/android/pulls), [daily/dev builds](https://github.com/nextcloud/android#development-version-hammer), or [RCs/release candidate builds](https://github.com/nextcloud/android/releases) \n*   enhancing Admin, User, or Developer [documentation](https://github.com/nextcloud/documentation/)\n*   hitting hard on the latest stable release by testing fundamental features and evaluating the user experience\n*   proactively getting familiar with [how to gather debug logs](https://github.com/nextcloud/android#getting-debug-info-via-logcat-mag) from your devices (so that you are prepared to provide a detailed report if you encounter a problem with the app in the future)\n\n## Contribution Guidelines & License :scroll:\n\n[GPLv2](https://github.com/nextcloud/android/blob/master/LICENSE.txt). All contributions to this repository from June, 16 2016 on are considered to be licensed under the AGPLv3 or any later version.\n\nNextcloud doesn't require a CLA (Contributor License Agreement). The copyright belongs to all the individual contributors. Therefore we recommend that every contributor adds following line to the header of a file, if they changed it substantially:\n\n\tSPDX-FileCopyrightText: <year> <your name> <your email address>\n\nPlease read the [Code of Conduct](https://nextcloud.com/community/code-of-conduct/). This document offers some guidance to ensure Nextcloud participants can cooperate effectively in a positive and inspiring atmosphere, and to explain how together we can strengthen and support each other.\n\nPlease review the [guidelines for contributing](https://github.com/nextcloud/android/blob/master/CONTRIBUTING.md) to this repository.\n\nMore information on how to contribute: <https://nextcloud.com/contribute/>\n\n## Start contributing :hammer\\_and\\_wrench:\n\nMake sure you read [SETUP.md](https://github.com/nextcloud/android/blob/master/SETUP.md) and [CONTRIBUTING.md](https://github.com/nextcloud/android/blob/master/CONTRIBUTING.md) before you start working on this project. But basically: fork this repository and contribute back using pull requests to the master branch.\nEasy starting points are also reviewing [pull requests](https://github.com/nextcloud/android/pulls) and working on [starter issues](https://github.com/nextcloud/android/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22).\n\n## Logs\n\n### Getting debug info via logcat :mag:\n\n#### With a linux computer:\n\n*   enable USB-Debugging in your smartphones developer settings and connect it via USB\n*   open command prompt/terminal\n*   enter `adb logcat --pid=$(adb shell pidof -s 'com.nextcloud.client') > logcatOutput.txt` to save the output to this file\n\n**Note:** You must have [adb](https://developer.android.com/studio/releases/platform-tools.html) installed first!\n\n#### On Windows:\n\n*   download and install [Minimal ADB and fastboot](https://forum.xda-developers.com/t/tool-minimal-adb-and-fastboot-2-9-18.2317790/#post-42407269)\n*   enable USB-Debugging in your smartphones developer settings and connect it via USB\n*   launch Minimal ADB and fastboot\n*   enter `adb shell pidof -s 'com.nextcloud.client'` and use the output as `<processID>` in the following command:\n*   `adb logcat --pid=<processID> > \"%USERPROFILE%\\Downloads\\logcatOutput.txt\"` (This will produce a `logcatOutput.txt` file in your downloads)\n*   if the processID is `18841`, an example command is: `adb logcat --pid=18841 > \"%USERPROFILE%\\Downloads\\logcatOutput.txt\"` (You might cancel the process after a while manually: it will not be exited automatically.)\n*   For a PowerShell terminal, replace `%USERPROFILE%` with `$env:USERPROFILE` in the commands above.\n\n#### On a device (with root) :wrench:\n\n*   open terminal app *(can be enabled in developer options)*\n*   get root access via \"su\"\n*   enter `logcat -d --pid $(pidof -s com.nextcloud.client) -f /sdcard/logcatOutput.txt`\n\nor\n\n*   use [CatLog](https://play.google.com/store/apps/details?id=com.nolanlawson.logcat) or [aLogcat](https://play.google.com/store/apps/details?id=org.jtb.alogcat)\n\n**Note:** Your device needs to be rooted for this approach!\n\n## Development version :hammer:\n\n*   [APK (direct download)](https://download.nextcloud.com/android/dev/latest.apk)\n*   [F-Droid](https://f-droid.org/en/packages/com.nextcloud.android.beta/)\n\n## Known Problems and FAQs\n\n### Push notifications do not work on F-Droid editions\n\nPush Notifications are not currently supported in the F-Droid builds due to dependencies on Google Play services.\n\n## Remarks :scroll:\n\nGoogle Play and the Google Play logo are trademarks of Google Inc.\n",
      "stars_today": 1
    },
    {
      "id": 229030654,
      "name": "go-git",
      "full_name": "go-git/go-git",
      "description": "A highly extensible Git implementation in pure Go.",
      "html_url": "https://github.com/go-git/go-git",
      "stars": 7150,
      "forks": 876,
      "language": "Go",
      "topics": [
        "git",
        "git-client",
        "git-library",
        "git-server",
        "go-git",
        "golang"
      ],
      "created_at": "2019-12-19T10:27:02Z",
      "updated_at": "2026-01-28T02:03:25Z",
      "pushed_at": "2026-01-27T17:53:47Z",
      "open_issues": 171,
      "owner": {
        "login": "go-git",
        "avatar_url": "https://avatars.githubusercontent.com/u/57653224?v=4"
      },
      "readme": "![go-git logo](https://cdn.rawgit.com/src-d/artwork/02036484/go-git/files/go-git-github-readme-header.png)\n[![GoDoc](https://godoc.org/github.com/go-git/go-git/v6?status.svg)](https://pkg.go.dev/github.com/go-git/go-git/v6) [![Build Status](https://github.com/go-git/go-git/workflows/Test/badge.svg)](https://github.com/go-git/go-git/actions) [![Go Report Card](https://goreportcard.com/badge/github.com/go-git/go-git)](https://goreportcard.com/report/github.com/go-git/go-git) [![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/go-git/go-git/badge)](https://scorecard.dev/viewer/?uri=github.com/go-git/go-git)\n\n*go-git* is a highly extensible git implementation library written in **pure Go**.\n\nIt can be used to manipulate git repositories at low level *(plumbing)* or high level *(porcelain)*, through an idiomatic Go API. It also supports several types of storage, such as in-memory filesystems, or custom implementations, thanks to the [`Storer`](https://pkg.go.dev/github.com/go-git/go-git/v6/plumbing/storer) interface.\n\nIt's being actively developed since 2015 and is being used extensively by [Keybase](https://keybase.io/blog/encrypted-git-for-everyone), [Gitea](https://gitea.io/en-us/) or [Pulumi](https://github.com/search?q=org%3Apulumi+go-git&type=Code), and by many other libraries and tools.\n\nProject Status\n--------------\n\nAfter the [legal issues](https://github.com/src-d/go-git/issues/1295#issuecomment-592965250) with the [`src-d`](https://github.com/src-d) organization, the lack of update for four months and the requirement to make a hard fork, the project is **now back to normality**.\n\nThe project is currently actively maintained by individual contributors, including several of the original authors, but also backed by a new company, [gitsight](https://github.com/gitsight), where `go-git` is a critical component used at scale.\n\n\nComparison with git\n-------------------\n\n*go-git* aims to be fully compatible with [git](https://github.com/git/git), all the *porcelain* operations are implemented to work exactly as *git* does.\n\n*git* is a humongous project with years of development by thousands of contributors, making it challenging for *go-git* to implement all the features. You can find a comparison of *go-git* vs *git* in the [compatibility documentation](COMPATIBILITY.md).\n\n\nInstallation\n------------\n\nThe recommended way to install *go-git* is:\n\n```go\nimport \"github.com/go-git/go-git/v6\" // with go modules enabled (GO111MODULE=on or outside GOPATH)\nimport \"github.com/go-git/go-git\" // with go modules disabled\n```\n\n\nExamples\n--------\n\n> Please note that the `CheckIfError` and `Info` functions  used in the examples are from the [examples package](https://github.com/go-git/go-git/blob/master/_examples/common.go#L19) just to be used in the examples.\n\n\n### Basic example\n\nA basic example that mimics the standard `git clone` command\n\n```go\n// Clone the given repository to the given directory\nInfo(\"git clone https://github.com/go-git/go-git\")\n\n_, err := git.PlainClone(\"/tmp/foo\", &git.CloneOptions{\n    URL:      \"https://github.com/go-git/go-git\",\n    Progress: os.Stdout,\n})\n\nCheckIfError(err)\n```\n\nOutputs:\n```\nCounting objects: 4924, done.\nCompressing objects: 100% (1333/1333), done.\nTotal 4924 (delta 530), reused 6 (delta 6), pack-reused 3533\n```\n\n### In-memory example\n\nCloning a repository into memory and printing the history of HEAD, just like `git log` does\n\n\n```go\n// Clones the given repository in memory, creating the remote, the local\n// branches and fetching the objects, exactly as:\nInfo(\"git clone https://github.com/go-git/go-billy\")\n\nr, err := git.Clone(memory.NewStorage(), nil, &git.CloneOptions{\n    URL: \"https://github.com/go-git/go-billy\",\n})\n\nCheckIfError(err)\n\n// Gets the HEAD history from HEAD, just like this command:\nInfo(\"git log\")\n\n// ... retrieves the branch pointed by HEAD\nref, err := r.Head()\nCheckIfError(err)\n\n\n// ... retrieves the commit history\ncIter, err := r.Log(&git.LogOptions{From: ref.Hash()})\nCheckIfError(err)\n\n// ... just iterates over the commits, printing it\nerr = cIter.ForEach(func(c *object.Commit) error {\n\tfmt.Println(c)\n\treturn nil\n})\nCheckIfError(err)\n```\n\nOutputs:\n```\ncommit ded8054fd0c3994453e9c8aacaf48d118d42991e\nAuthor: Santiago M. Mola <santi@mola.io>\nDate:   Sat Nov 12 21:18:41 2016 +0100\n\n    index: ReadFrom/WriteTo returns IndexReadError/IndexWriteError. (#9)\n\ncommit df707095626f384ce2dc1a83b30f9a21d69b9dfc\nAuthor: Santiago M. Mola <santi@mola.io>\nDate:   Fri Nov 11 13:23:22 2016 +0100\n\n    readwriter: fix bug when writing index. (#10)\n\n    When using ReadWriter on an existing siva file, absolute offset for\n    index entries was not being calculated correctly.\n...\n```\n\nYou can find this [example](_examples/log/main.go) and many others in the [examples](_examples) folder.\n\nContribute\n----------\n\n[Contributions](https://github.com/go-git/go-git/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22) are more than welcome, if you are interested please take a look to\nour [Contributing Guidelines](CONTRIBUTING.md).\n\nLicense\n-------\nApache License Version 2.0, see [LICENSE](LICENSE)\n",
      "stars_today": 1
    },
    {
      "id": 35927665,
      "name": "seurat",
      "full_name": "satijalab/seurat",
      "description": "R toolkit for single cell genomics",
      "html_url": "https://github.com/satijalab/seurat",
      "stars": 2625,
      "forks": 979,
      "language": "R",
      "topics": [
        "cran",
        "human-cell-atlas",
        "single-cell-genomics",
        "single-cell-rna-seq"
      ],
      "created_at": "2015-05-20T05:23:02Z",
      "updated_at": "2026-01-28T01:52:10Z",
      "pushed_at": "2026-01-21T19:54:02Z",
      "open_issues": 310,
      "owner": {
        "login": "satijalab",
        "avatar_url": "https://avatars.githubusercontent.com/u/8411851?v=4"
      },
      "readme": "[![CRAN Version](https://www.r-pkg.org/badges/version/Seurat)](https://cran.r-project.org/package=Seurat)\n[![CRAN Downloads](https://cranlogs.r-pkg.org/badges/Seurat)](https://cran.r-project.org/package=Seurat)\n\n\n# Seurat v5\n\nSeurat is an R toolkit for single cell genomics, developed and maintained by the Satija Lab at NYGC.\n\nWe are excited to release Seurat v5! This updates introduces new functionality for spatial, multimodal, and scalable single-cell analysis.\n\nSeurat v5 is backwards-compatible with previous versions, so that users will continue to be able to re-run existing workflows. \n\nInstructions, documentation, and tutorials can be found at:\n\n* https://satijalab.org/seurat\n\nSeurat is also hosted on GitHub, you can view and clone the repository at\n\n* https://github.com/satijalab/seurat\n\nSeurat has been successfully installed on Mac OS X, Linux, and Windows, using the devtools package to install directly from GitHub\n\nImprovements and new features will be added on a regular basis, please post on the [github page](https://github.com/satijalab/seurat) with any questions or if you would like to contribute\n\nFor a version history/changelog, please see the [NEWS file](https://github.com/satijalab/seurat/blob/master/NEWS.md).\n",
      "stars_today": 1
    },
    {
      "id": 171964196,
      "name": "xcodes",
      "full_name": "XcodesOrg/xcodes",
      "description": "The best command-line tool to install and switch between multiple versions of Xcode.",
      "html_url": "https://github.com/XcodesOrg/xcodes",
      "stars": 4516,
      "forks": 164,
      "language": "Swift",
      "topics": [
        "hacktoberfest",
        "macos",
        "swift",
        "xcode"
      ],
      "created_at": "2019-02-22T00:00:25Z",
      "updated_at": "2026-01-27T16:42:31Z",
      "pushed_at": "2025-09-26T23:22:33Z",
      "open_issues": 157,
      "owner": {
        "login": "XcodesOrg",
        "avatar_url": "https://avatars.githubusercontent.com/u/96437613?v=4"
      },
      "readme": "# xcodes\n\nThe best command-line tool to install and switch between multiple versions of Xcode.\n\n_If you're looking for an app version of `xcodes`, try [Xcodes.app](https://github.com/XcodesOrg/XcodesApp)._\n\n![CI Status](https://github.com/XcodesOrg/xcodes/workflows/CI/badge.svg)\n\n![Header Image](Header.png)\n\n### :tada: Announcment\n\nXcodes is now part of the `XcodesOrg` - [read more here](nextstep.md)\n\n## Installation\n\n### Homebrew (Preferred)\n\n```sh\nbrew install xcodesorg/made/xcodes\n```\n\nThese are Developer ID-signed and notarized release builds and don't require Xcode to already be installed in order to use.\n\n**Other methods:**\n\n<details>\n<summary>Download a release</summary>\n\nDownload the latest release from the [Releases](https://github.com/XcodesOrg/xcodes/releases) page. These are Developer ID-signed release builds and don't require Xcode to already be installed in order to use.\n</details>\n\n<details>\n<summary>Using <a href=\"https://github.com/yonaskolb/Mint\">Mint</a></summary>\n\n```sh\nmint install XcodesOrg/xcodes\n```\n\n</details>\n\n<details>\n<summary>Build from source</summary>\n\nBuilding from source requires Xcode 12.0 or later, so it's not an option for setting up a computer from scratch.\n\n```sh\ngit clone https://github.com/XcodesOrg/xcodes\ncd xcodes\nmake install\n# or, if /usr/local/ isn't in your PATH\nPREFIX=/your/install/directory make install\n```\n\nWhile installing, you may get the following output:\n\n```sh\nswift build\nerror: terminated(72): xcrun --sdk macosx --find xctest output:\n```\n\nIf that occurs, it means you need to select a version of Xcode. You can do this with `xcode-select` or by choosing a Command Line Tools option in Xcode's preferences Locations tab.\n</details>\n\n## Usage\n\n### Install Xcode : \n\nInstall a specific version of Xcode using a command like one of these:\n\n```sh\nxcodes install 10.2.1\nxcodes install 11 Beta 7\nxcodes install 11.2 GM seed\nxcodes install 9.0 --path ~/Archive/Xcode_9.xip\nxcodes install --latest-prerelease\nxcodes install --latest --directory \"/Volumes/Bag Of Holding/\"\nxcodes install --latest --experimental-unxip\n```\n\nYou'll then be prompted to enter your Apple ID username and password. You can also provide these with the `XCODES_USERNAME` and `XCODES_PASSWORD` environment variables.\n\nAfter successfully authenticating, xcodes will save your Apple ID password into the keychain and will remember your Apple ID for future use. If you need to use a different Apple ID than the one that's remembered, set the `XCODES_USERNAME` environment variable.\n\nxcodes will download and install the version you asked for so that it's ready to use.\n\n```sh\n(1/6) Downloading Xcode 11.2.0: 100%\n(2/6) Unarchiving Xcode (This can take a while)\n(3/6) Moving Xcode to /Applications/Xcode-11.2.0.app\n(4/6) Moving Xcode archive Xcode-11.2.0.xip to the Trash\n(5/6) Checking security assessment and code signing\n(6/6) Finishing installation\nxcodes requires superuser privileges in order to finish installation.\nmacOS User Password:\n\nXcode 11.2.0 has been installed to /Applications/Xcode-11.2.0.app\n```\n\nIf you have [aria2](https://aria2.github.io) installed (it's available in Homebrew, `brew install aria2`), then xcodes will default to use it for downloads. It uses up to 16 connections to download Xcode 3-5x faster than URLSession.\n\nXcode will be installed to /Applications by default, but you can provide the path to a different directory with the `--directory` option or the `XCODES_DIRECTORY` environment variable. All of the xcodes commands support this option, like `select` and `uninstall`, so you can manage Xcode versions that aren't in /Applications. xcodes supports having all of your Xcode versions installed in _one_ directory, wherever that may be.\n\n### Install Runtimes : \n\nRun this command line to display the available runtimes \n\n```sh\nxcodes runtimes --include-betas\n```\n\nInstall the wanted Runtime (ex. iOS 17.0-beta1)\n\n```sh\nxcodes runtimes install \"iOS 17.0-beta1\"\n```\n\n### `.xcode-version`\n\nWe recommend the creation of a `.xcode-version` file to explicitly declare and store the Xcode version to be used by your CI environment as well as your team.\n\n```txt\n13.4.1\n```\n\nRead [the proposal](/XCODE_VERSION.md) of `.xcode-version`.\n\n### Commands\n\n- `download <version>`: Download a specific version of Xcode\n- `install <version>`: Download and install a specific version of Xcode\n- `installed`: List the versions of Xcode that are installed\n- `list`: List all versions of Xcode that are available to install\n- `select`: Change the selected Xcode\n- `uninstall`: Uninstall a specific version of Xcode\n- `update`: Update the list of available versions of Xcode\n- `version`: Print the version number of xcodes itself\n- `signout`: Clears the stored username and password\n\n### Experimental Unxip - for faster unxipping\n\nThanks to the amazing work by [saagarjhi](https://github.com/saagarjha/unxip) - Xcodes now includes the ability to unxip up to 70% faster on some systems.\n\n```sh\nxcodes install --latest --experimental-unxip\n```\n\n### Shell Completion Scripts\n\nxcodes can generate completion scripts which allow you to press the tab key on your keyboard to autocomplete commands and arguments when typing an xcodes command. The steps to install a completion script depend on the shell that you use. More information about installation instructions for different shells and the underlying implementation is available in the [swift-argument-parser repo](https://github.com/apple/swift-argument-parser/blob/main/Sources/ArgumentParser/Documentation.docc/Articles/InstallingCompletionScripts.md).\n\n<details>\n<summary>Zsh, with oh-my-zsh:</summary>\n\nRun the following commands:\n\n```sh\nmkdir ~/.oh-my-zsh/completions\nxcodes --generate-completion-script > ~/.oh-my-zsh/completions/_xcodes\n```\n\n</details>\n\n## Development\n\nYou'll need Xcode 13 in order to build and run xcodes.\n\n<details>\n<summary>Using Xcode</summary>\nEven though xcodes is a command-line app, all of the normal functionality works in Xcode, like building, running, and running tests. You can even type text into Xcode's console when it prompts you for input like your Apple ID or 2FA code.\n\nWhen running xcodes from Xcode, if you want to run a particular command or pass some arguments, you can hold the option key to present a sheet with more options. This means you'd use <kbd>Option</kbd> + <kbd>Command</kbd> + <kbd>R</kbd> or hold <kbd>Option</kbd> while clicking the Run button. Here you can add, remove, and toggle arguments that will be passed to xcodes when it's launched.\n\n![Xcode Edit Scheme Screen](XcodeRunSheet.png)\n</details>\n\n<details>\n<summary>Using Swift command line tools</summary>\nYou can also use the Swift command line tools once you have Xcode installed:\n\n- Build: `swift build`\n- Run: `swift run`, or commands like `swift run xcodes list`\n- Run tests: `swift test`\n\n</details>\n\nThere's a Makefile to help build xcodes for distribution. We already do this for you in order to provide Developer ID-signed and notarized release builds via Homebrew (see [Installation](#installation)).\n\n<details>\n<summary>Releasing a new version of xcodes</summary>\n\n```sh\n# Bump the version number in Version.swift, commit the change, and tag it\nvim Sources/XcodesKit/Version.swift\ngit add Sources/XcodesKit/Version.swift\ngit commit -m \"Bump version to $VERSION\"\ngit tag -asm \"$VERSION\" \"$VERSION\"\n\n# Clean first\nmake clean\n\n# Make a release build of xcodes, sign it, and zip it\nmake zip\n# Create a Homebrew bottle\nmake bottle VERSION=\"$VERSION\"\n\n# Notarize the release build\n# This can take a while\nmake notarize \\\n    TEAMID=\"ABC123\"\n\n# Push the new version bump commit and tag\ngit push --follow-tags\n\n# Edit the draft release created by Release Drafter to point at the new tag\n# Set the release title to the new version\n# Duplicate xcodes-$VERSION.mojave.tar.gz and rename to xcodes-$VERSION.arm64_mojave.tar.gz, also create `xcodes-$VERSION.macos.i386.tar.gz` and `xcodes-$VERSION.macos.arm64.tar.gz`\n# Add the xcodes.zip, xcodes-$VERSION.mojave.tar.gz, xcodes-$VERSION.arm64_mojave.tar.gz files to the release\n# Publish the release\n\n# Update the Homebrew Bottle: https://github.com/XcodesOrg/homebrew-made/blob/master/Formula/xcodes.rb\n```\n\n</details>\n\nNotable design decisions are recorded in [DECISIONS.md](./DECISIONS.md). The Apple authentication flow is described in [Apple.paw](./Apple.paw), which will allow you to play with the API endpoints that are involved using the [Paw](https://paw.cloud) app.\n\n[`xcode-install`](https://github.com/xcpretty/xcode-install) and [fastlane/spaceship](https://github.com/fastlane/fastlane/tree/master/spaceship) both deserve credit for figuring out the hard parts of what makes this possible.\n\n## Maintainers\n\n[Matt Kiazyk](https://github.com/mattkiazyk) - [Twitter](https://www.twitter.com/mattkiazyk)\n",
      "stars_today": 1
    },
    {
      "id": 20360040,
      "name": "clusterProfiler",
      "full_name": "YuLab-SMU/clusterProfiler",
      "description": ":bar_chart: A universal enrichment tool for interpreting omics data",
      "html_url": "https://github.com/YuLab-SMU/clusterProfiler",
      "stars": 1160,
      "forks": 263,
      "language": "R",
      "topics": [
        "enrichment-analysis",
        "go",
        "gsea",
        "kegg",
        "rstats",
        "visualization"
      ],
      "created_at": "2014-05-31T16:34:32Z",
      "updated_at": "2026-01-27T17:12:23Z",
      "pushed_at": "2026-01-25T00:20:22Z",
      "open_issues": 363,
      "owner": {
        "login": "YuLab-SMU",
        "avatar_url": "https://avatars.githubusercontent.com/u/40430016?v=4"
      },
      "readme": "# clusterProfiler\n\n<img src=\"inst/sticker/clusterProfiler_hex.png\" height=\"200\" align=\"right\" />\n\n[![Project Status: Active - The project has reached a stable, usable\nstate and is being actively\ndeveloped.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)\n[![](https://img.shields.io/badge/release%20version-4.18.4-green.svg)](https://www.bioconductor.org/packages/clusterProfiler)\n[![](https://img.shields.io/badge/devel%20version-4.19.4.006-green.svg)](https://github.com/guangchuangyu/clusterProfiler)\n[![Bioc](http://www.bioconductor.org/shields/years-in-bioc/clusterProfiler.svg)](https://www.bioconductor.org/packages/devel/bioc/html/clusterProfiler.html#since)\n\n[![platform](http://www.bioconductor.org/shields/availability/devel/clusterProfiler.svg)](https://www.bioconductor.org/packages/devel/bioc/html/clusterProfiler.html#archives)\n[![Build\nStatus](http://www.bioconductor.org/shields/build/devel/bioc/clusterProfiler.svg)](https://bioconductor.org/checkResults/devel/bioc-LATEST/clusterProfiler/)\n[![codecov](https://codecov.io/gh/GuangchuangYu/clusterProfiler/branch/master/graph/badge.svg)](https://codecov.io/gh/GuangchuangYu/clusterProfiler/)\n\n<!--\n[![Last-changedate](https://img.shields.io/badge/last%20change-2026--01--21-green.svg)](https://github.com/GuangchuangYu/clusterProfiler/commits/master)\n-->\n\n- [clusterProfiler](http://bioconductor.org/packages/clusterProfiler)\n  supports exploring functional characteristics of both coding and\n  non-coding genomics data for thousands of species with up-to-date gene\n  annotation.\n- It provides a universal interface for gene functional annotation from\n  a variety of sources and thus can be applied in diverse scenarios.\n- It provides a tidy interface to access, manipulate, and visualize\n  enrichment results to help users achieve efficient data interpretation\n- Datasets obtained from multiple treatments and time points can be\n  analyzed and compared in a single run, easily revealing functional\n  consensus and differences among distinct conditions\n\nFor details, please visit:\n\n- <https://yulab-smu.top/contribution-knowledge-mining/>\n- <https://yulab-smu.top/biomedical-knowledge-mining-book/>\n\n<img src=\"graphic-abstract-The-Innovation-2021.jpg\" width=\"890\"/>\n\n## :writing_hand: Authors\n\nGuangchuang YU <https://yulab-smu.top>\n\nSchool of Basic Medical Sciences, Southern Medical University\n\n------------------------------------------------------------------------\n\nIf you use\n[clusterProfiler](http://bioconductor.org/packages/clusterProfiler) in\npublished research, please cite the most appropriate paper(s) from this\nlist:\n\n1.  S Xu<sup>\\#</sup>, E Hu<sup>\\#</sup>, Y Cai<sup>\\#</sup>, Z\n    Xie<sup>\\#</sup>, X Luo<sup>\\#</sup>, L Zhan, W Tang, Q Wang, B Liu,\n    R Wang, W Xie, T Wu, L Xie, **G Yu**<sup>\\*</sup>. Using\n    clusterProfiler to characterise Multi-Omics Data. ***Nature\n    Protocols***. 2024, accepted. doi:\n    [10.1038/s41596-024-01020-z](https://doi.org/10.1038/s41596-024-01020-z)\n2.  T Wu<sup>\\#</sup>, E Hu<sup>\\#</sup>, S Xu, M Chen, P Guo, Z Dai, T\n    Feng, L Zhou, W Tang, L Zhan, X Fu, S Liu, X Bo<sup>\\*</sup>, **G\n    Yu**<sup>\\*</sup>. clusterProfiler 4.0: A universal enrichment tool\n    for interpreting omics data. ***The Innovation***. 2021,\n    2(3):100141. doi:\n    [10.1016/j.xinn.2021.100141](https://doi.org/10.1016/j.xinn.2021.100141)\n3.  **G Yu**<sup>\\*</sup>. Gene Ontology Semantic Similarity Analysis\n    Using GOSemSim. In: Kidder B. (eds) Stem Cell Transcriptional\n    Networks. ***Methods in Molecular Biology***. 2020, 2117:207-215.\n    Humana, New York, NY. doi:\n    [10.1007/978-1-0716-0301-7_11](https://doi.org/10.1007/978-1-0716-0301-7_11)\n4.  **G Yu**<sup>\\*</sup>. Using meshes for MeSH term enrichment and\n    semantic analyses. ***Bioinformatics***. 2018, 34(21):3766â€“3767.\n    doi:\n    [10.1093/bioinformatics/bty410](https://doi.org/10.1093/bioinformatics/bty410)\n5.  **G Yu**, QY He<sup>\\*</sup>. ReactomePA: an R/Bioconductor package\n    for reactome pathway analysis and visualization. ***Molecular\n    BioSystems***. 2016, 12(2):477-479. doi:\n    [10.1039/C5MB00663E](https://doi.org/10.1039/C5MB00663E)\n6.  **G Yu**<sup>\\*</sup>, LG Wang, and QY He<sup>\\*</sup>. ChIPseeker:\n    an R/Bioconductor package for ChIP peak annotation, comparison and\n    visualization. ***Bioinformatics***. 2015, 31(14):2382-2383. doi:\n    [10.1093/bioinformatics/btv145](https://doi.org/10.1093/bioinformatics/btv145)\n7.  **G Yu**<sup>\\*</sup>, LG Wang, GR Yan, QY He<sup>\\*</sup>. DOSE: an\n    R/Bioconductor package for Disease Ontology Semantic and Enrichment\n    analysis. ***Bioinformatics***. 2015, 31(4):608-609. doi:\n    [10.1093/bioinformatics/btu684](https://doi.org/10.1093/bioinformatics/btu684)\n8.  **G Yu**, LG Wang, Y Han and QY He<sup>\\*</sup>. clusterProfiler: an\n    R package for comparing biological themes among gene clusters.\n    ***OMICS: A Journal of Integrative Biology***. 2012, 16(5):284-287.\n    doi: [10.1089/omi.2011.0118](https://doi.org/10.1089/omi.2011.0118)\n9.  **G Yu**, F Li, Y Qin, X Bo<sup>\\*</sup>, Y Wu, S Wang<sup>\\*</sup>.\n    GOSemSim: an R package for measuring semantic similarity among GO\n    terms and gene products. ***Bioinformatics***. 2010, 26(7):976-978.\n    doi:\n    [10.1093/bioinformatics/btq064](https://doi.org/10.1093/bioinformatics/btq064)\n\n<!--\n&#10;\n&#10; r badge_custom(\"1st most cited paper\", \"in OMICS\", \"green\",\n  \"http://online.liebertpub.com/action/showMostCitedArticles?journalCode=omi\")`\n r badge_custom(\"ESI\", \"Highly Cited Paper\", \"green\")`\n r badge_doi(\"10.1089/omi.2011.0118\", \"green\")`\n&#10;\n------------------------------------------------------------------------\n&#10;### Citation\n&#10;\n&#10;\n<img src=\"https://guangchuangyu.github.io/software/citation_trend/clusterProfiler.png\" width=\"890\"/>\n&#10;\n### Download stats\n&#10;r badge_download_bioc(\"clusterProfiler\")\nr badge_bioc_download(\"clusterProfiler\", \"total\", \"blue\")\nr badge_bioc_download(\"clusterProfiler\", \"month\", \"blue\")\n&#10;\n<img src=\"https://guangchuangyu.github.io/software/dlstats/clusterProfiler.png\" width=\"890\"/>\n&#10;-->\n",
      "stars_today": 1
    },
    {
      "id": 17101828,
      "name": "swirl_courses",
      "full_name": "swirldev/swirl_courses",
      "description": ":mortar_board: A collection of interactive courses for the swirl R package.",
      "html_url": "https://github.com/swirldev/swirl_courses",
      "stars": 4523,
      "forks": 7239,
      "language": "R",
      "topics": [],
      "created_at": "2014-02-23T04:48:04Z",
      "updated_at": "2026-01-23T10:12:20Z",
      "pushed_at": "2024-01-10T17:38:19Z",
      "open_issues": 206,
      "owner": {
        "login": "swirldev",
        "avatar_url": "https://avatars.githubusercontent.com/u/5671732?v=4"
      },
      "readme": "# swirl courses\n\nThis is a collection of interactive courses for use with the [swirl R package](http://swirlstats.com). You'll find instructions for installing courses further down on this page. Some courses are still in development and we'd love to hear any [suggestions](https://github.com/swirldev/swirl_courses/issues/new) you have as you work through them.\n\nFor more information regarding swirl, visit [swirlstats.com](http://swirlstats.com) or the [swirl GitHub repository](https://github.com/swirldev/swirl). If you'd like to write your own interactive content, please visit the [Instructors page](http://swirlstats.com/instructors.html) of our website.\n\nHere are our current offerings, organized by level of difficulty:\n\n#### Beginner\n\n- **R Programming**: The basics of programming in R\n- [**R Programming E**](https://github.com/swirldev/R_Programming_E): Same as the original, but modified slightly for in-class use (see below ***)\n- [**The R Programming Environment**](https://swirlstats.com/scn/rpe.html)\n<!-- - **Data Analysis**: Basic ideas in statistics and data visualization -->\n<!-- - **Mathematical Biostatistics Boot Camp**: One- and two-sample t-tests, power, and sample size -->\n<!-- - **Open Intro**: A very basic introduction to statistics, data analysis, and data visualization -->\n\n\\*\\*\\* *R Programming E is identical to R Programming, except we've eliminated the prompts for Coursera credentials at the end of each lesson and instead give students the option to send an email to their instructor notifying them of completion. Admittedly, it's sort of a hack until we come up with a more robust solution for in-class use (i.e. an instructor \"dashboard\").*\n\n#### Intermediate\n\n- **Regression Models**: The basics of regression modeling in R\n- **Getting and Cleaning Data**: dplyr, tidyr, lubridate, oh my!\n\n#### Advanced\n\n- **Statistical Inference**: This intermediate to advanced level course closely follows the\n[Statistical Inference course](https://www.coursera.org/course/statinference) of the Johns Hopkins \n[Data Science Specialization](https://www.coursera.org/specialization/jhudatascience/1) on Coursera. It\nintroduces the student to basic concepts of statistical inference\nincluding probability, hypothesis testing, confidence intervals and\np-values. It concludes with an initiation to topics of particular\nrelevance to big data, issues of multiple testing and resampling.\n- [**Advanced R Programming**](https://swirlstats.com/scn/arp.html)\n\nSince our users come from a variety backgrounds, it's very hard to label material as **Beginner**, **Intermediate**, or **Advanced**. If you find something that is labelled **Beginner** to be challenging, please don't be discouraged. The first step of learning anything is to acknowledge that you are capable of understanding it. True understanding will come with time and practice.\n\n#### Course Authors\n\n- **Writing swirl Courses**: An interactive guides and example \n  for swirl course authors. The first group of lessons cover basics. The rest cover \n  special topics useful primarily as samples--points of departure for one's own material.\n  For more comprehensive documentation about writing your own swirl courses see http://swirlstats.com/swirlify/.\n\n## Install and run a course automatically from swirl\n\n**This is the preferred method of installing courses.** It automates the process by allowing you to do everything right from the R console.\n\n1) Make sure you have a recent version version of swirl:\n\n```\ninstall.packages(\"swirl\")\n```\n\n2) Enter the following from the R console, **substituting the name of the course** that you wish to install:\n\n```\nlibrary(swirl)\ninstall_course(\"Course Name Here\")\nswirl()\n```\n\nFor example, `install_course(\"R Programming\")` will install the R Programming course. **Please note that course names are case sensitive!**\n\nIf that doesn't work for you...\n\n## Install and run a course manually\n\nIf the automatic course installation method outlined above does not work for you, then there's a simple alternative.\n\n1. Find the course you want to install on the [Swirl Course network website](https://swirlstats.com/scn/title.html).\n2. Follow the manual installation instructions on the course page.\n\nIf that does not work for you, consider taking a look at the \n[legacy manual install instructions](https://github.com/swirldev/swirl_courses/wiki/Legacy-Manual-Install-Instructions-for-Swirl-Courses).\n\n## Uninstall a course\n\nIf you'd like to remove a course at any time, you can use `uninstall_course(\"Course Name Here\")`.\n\n## Using swirl in the classroom\n\nInstructors around the world are using swirl in their classrooms. We think this is awesome. If you're an instructor, please feel free to do the same -- free of charge. While your students may be paying to take your course or attend your institution, we simply ask that you don't charge people *directly* for the use of our software or instructional content.\n\nIf you are not sure about a particular use case, don't hesitate to post a\nquestion to our [Google Group](https://groups.google.com/forum/#!forum/swirl-discuss).\n",
      "stars_today": 0
    },
    {
      "id": 66645293,
      "name": "gantt",
      "full_name": "frappe/gantt",
      "description": "Open Source Javascript Gantt",
      "html_url": "https://github.com/frappe/gantt",
      "stars": 5821,
      "forks": 1255,
      "language": "JavaScript",
      "topics": [
        "frappe-gantt",
        "gantt",
        "gantt-chart",
        "ganttjs",
        "javascript-gantt"
      ],
      "created_at": "2016-08-26T12:17:58Z",
      "updated_at": "2026-01-27T17:17:43Z",
      "pushed_at": "2025-10-31T09:32:00Z",
      "open_issues": 71,
      "owner": {
        "login": "frappe",
        "avatar_url": "https://avatars.githubusercontent.com/u/836974?v=4"
      },
      "readme": "<div align=\"center\" markdown=\"1\">\n    <img src=\".github/gantt-logo.jpg\" width=\"80\">\n    <h1>Frappe Gantt</h1>\n\n**A modern, configurable, Gantt library for the web.**\n\n</div>\n\n![Hero Image](.github/hero-image.png)\n\n## Frappe Gantt\n\nGantt charts are bar charts that visually illustrate a project's tasks, schedule, and dependencies. With Frappe Gantt, you can build beautiful, customizable, Gantt charts with ease.\n\nYou can use it anywhere from hobby projects to tracking the goals of your team at the worksplace.\n\n[ERPNext](https://erpnext.com/) uses Frappe Gantt.\n\n### Motivation\n\nWe needed a Gantt View for ERPNext. Surprisingly, we couldn't find a visually appealing Gantt library that was open source - so we decided to build it. Initially, the design was heavily inspired by Google Gantt and DHTMLX.\n\n### Key Features\n\n-   **Customizable Views**: customize the timeline based on various time periods - day, hour, or year, you have it. You can also create your own views.\n-   **Ignore Periods**: exclude weekends and other holidays from your tasks' progress calculation.\n-   **Configure Anything**: spacing, edit access, labels, you can control it all. Change both the style and functionality to meet your needs.\n-   **Multi-lingual Support**: suitable for companies with an international base.\n\n## Usage\n\nInstall with:\n\n```bash\nnpm install frappe-gantt\n```\n\nInclude it in your HTML:\n\n```html\n<script src=\"frappe-gantt.umd.js\"></script>\n<link rel=\"stylesheet\" href=\"frappe-gantt.css\" />\n```\n\nOr from the CDN:\n\n```html\n<script src=\"https://cdn.jsdelivr.net/npm/frappe-gantt/dist/frappe-gantt.umd.js\"></script>\n<link\n    rel=\"stylesheet\"\n    href=\"https://cdn.jsdelivr.net/npm/frappe-gantt/dist/frappe-gantt.css\"\n/>\n```\n\nStart using Gantt:\n\n```js\nlet tasks = [\n  {\n    id: '1',\n    name: 'Redesign website',\n    start: '2016-12-28',\n    end: '2016-12-31',\n    progress: 20\n  },\n  ...\n]\nlet gantt = new Gantt(\"#gantt\", tasks);\n\n// Use .refresh to update the chart\ngantt.tasks.append(...)\ngantt.tasks.refresh()\n```\n\n### Configuration\n\nFrappe Gantt offers a wide range of options to customize your chart.\n\n| **Option**               | **Description**                                               | **Possible Values**                                                                                                                                                           | **Default**                                         |\n| ------------------------ | ------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\n| `arrow_curve`            | Curve radius of arrows connecting dependencies.               | Any positive integer.                                                                                                                                                         | `5`                                                 |\n| `auto_move_label`        | Move task labels when user scrolls horizontally.              | `true`, `false`                                                                                                                                                               | `false`                                             |\n| `bar_corner_radius`      | Radius of the task bar corners (in pixels).                   | Any positive integer.                                                                                                                                                         | `3`                                                 |\n| `bar_height`             | Height of task bars (in pixels).                              | Any positive integer.                                                                                                                                                         | `30`                                                |\n| `container_height`       | Height of the container.                                      | `auto` - dynamic container height to fit all tasks - _or_ any positive integer (for pixels).                                                                                  | `auto`                                              |\n| `column_width`           | Width of each column in the timeline.                         | Any positive integer.                                                                                                                                                         | 45                                                  |\n| `date_format`            | Format for displaying dates.                                  | Any valid JS date format string.                                                                                                                                              | `YYYY-MM-DD`                                        |\n| `upper_header_height`    | Height of the upper header in the timeline (in pixels).       | Any positive integer.                                                                                                                                                         | `45`                                                |\n| `lower_header_height`    | Height of the lower header in the timeline (in pixels).       | Any positive integer.                                                                                                                                                         | `30`                                                |\n| `snap_at`                | Snap tasks at particular intervel while resizing or dragging. | Any _interval_ (see below)                                                                                                                                                    | `1d`                                                |\n| `infinite_padding`       | Whether to extend timeline infinitely when user scrolls.      | `true`, `false`                                                                                                                                                               | `true`                                              |\n| `holidays`               | Highlighted holidays on the timeline.                         | Object mapping CSS colors to holiday types. Types can either be a) 'weekend', or b) array of _strings_ or _date objects_ or _objects_ in the format `{date: ..., label: ...}` | `{ 'var(--g-weekend-highlight-color)': 'weekend' }` |\n| `is_weekend`             | Determines whether a day is a weekend                         | Function                                                                                                                                                                      | `(d) => d.getDay() === 0 \\|\\| d.getDay() === 6`     |\n| `ignore`                 | Ignored areas in the rendering                                | `weekend` _or_ Array of strings or date objects (`weekend` can be present to the array also).                                                                                 | `[]`                                                |\n| `language`               | Language for localization.                                    | ISO 639-1 codes like `en`, `fr`, `es`.                                                                                                                                        | `en`                                                |\n| `lines`                  | Determines which grid lines to display.                       | `none` for no lines, `vertical` for only vertical lines, `horizontal` for only horizontal lines, `both` for complete grid.                                                    | `both`                                              |\n| `move_dependencies`      | Whether moving a task automatically moves its dependencies.   | `true`, `false`                                                                                                                                                               | `true`                                              |\n| `padding`                | Padding around task bars (in pixels).                         | Any positive integer.                                                                                                                                                         | `18`                                                |\n| `popup_on`               | Event to trigger the popup display.                           | `click` _or_ `hover`                                                                                                                                                          | `click`                                             |\n| `readonly_progress`      | Disables editing task progress.                               | `true`, `false`                                                                                                                                                               | `false`                                             |\n| `readonly_dates`         | Disables editing task dates.                                  | `true`, `false`                                                                                                                                                               | `false`                                             |\n| `readonly`               | Disables all editing features.                                | `true`, `false`                                                                                                                                                               | `false`                                             |\n| `scroll_to`              | Determines the starting point when chart is rendered.         | `today`, `start`, `end`, or a date string.                                                                                                                                    | `today`                                             |\n| `show_expected_progress` | Shows expected progress for tasks.                            | `true`, `false`                                                                                                                                                               | `false`                                             |\n| `today_button`           | Adds a button to navigate to todayâ€™s date.                    | `true`, `false`                                                                                                                                                               | `true`                                              |\n| `view_mode`              | The initial view mode of the Gantt chart.                     | `Day`, `Week`, `Month`, `Year`.                                                                                                                                               | `Day`                                               |\n| `view_mode_select`       | Allows selecting the view mode from a dropdown.               | `true`, `false`                                                                                                                                                               | `false`                                             |\n\nApart from these ones, two options - `popup` and `view_modes` (plural, not singular) - are available. They have \"sub\"-APIs, and thus are listed separately.\n\n#### View Mode Configuration\n\nThe `view_modes` option determines all the available view modes for the chart. It should be an array of objects.\n\nEach object can have the following properties:\n\n-   `name` (string) - the name of view mode.\n-   `padding` (interval) - the time above.\n-   `step` - the interval of each column\n-   `lower_text` (date format string _or_ function) - the format for text in lower header. Blank string for none. The function takes in `currentDate`, `previousDate`, and `lang`, and should return a string.\n-   `upper_text` (date format string _or_ function) - the format for text in upper header. Blank string for none. The function takes in `currentDate`, `previousDate`, and `lang`, and should return a string.\n-   `upper_text_frequency` (number) - how often the upper text has a value. Utilized in internal calculation to improve performance.\n-   `thick_line` (function) - takes in `currentDate`, returns Boolean determining whether the line for that date should be thicker than the others.\n\nThree other options allow you to override general configuration for this view mode alone:\n\n-   `date_format`\n-   `column_width`\n-   `snap_at`\n    For details, see the above table.\n\n#### Popup Configuration\n\n`popup` is a function. If it returns\n\n-   `false`, there will be no popup.\n-   `undefined`, the popup will be rendered based on manipulation within the function\n-   a HTML string, the popup will be that string.\n\nThe function receives one object as an argument, containing:\n\n-   `task` - the task as an object\n-   `chart` - the entire Gantt chart\n-   `get_title`, `get_subtitle`, `get_details` (functions) - get the relevant section as a HTML node.\n-   `set_title`, `set_subtitle`, `set_details` (functions) - take in the HTML of the relevant section\n-   `add_action` (function) - accepts two parameters, `html` and `func` - respectively determining the HTML of the action and the callback when the action is pressed.\n\n### API\n\nFrappe Gantt exposes a few helpful methods for you to interact with the chart:\n\n| **Name**            | **Description**                                       | **Parameters**                                                                                                                                                               |\n| ------------------- | ----------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `.update_options`   | Re-renders the chart after updating specific options. | `new_options` - object containing new options.                                                                                                                               |\n| `.change_view_mode` | Updates the view mode.                                | `view_mode` - Name of view mode _or_ view mode object (see above) and `maintain_pos` - whether to go back to current scroll position after rerendering, defaults to `false`. |\n| `.scroll_current`   | Scrolls to the current date                           | No parameters.                                                                                                                                                               |\n| `.update_task`      | Re-renders a specific task bar alone                  | `task_id` - id of task and `new_details` - object containing the task properties to be updated.                                                                              |\n\n## Development Setup\n\nIf you want to contribute enhancements or fixes:\n\n1. Clone this repo.\n2. `cd` into project directory.\n3. Run `pnpm i` to install dependencies.\n4. `pnpm run build` to build files - or `pnpm run build-dev` to build and watch for changes.\n5. Open `index.html` in your browser.\n6. Make your code changes and test them.\n\n<br />\n<br />\n<div align=\"center\" style=\"padding-top: 0.75rem;\">\n\t<a href=\"https://frappe.io\" target=\"_blank\">\n\t\t<picture>\n\t\t\t<source media=\"(prefers-color-scheme: dark)\" srcset=\"https://frappe.io/files/Frappe-white.png\">\n\t\t\t<img src=\"https://frappe.io/files/Frappe-black.png\" alt=\"Frappe Technologies\" height=\"28\"/>\n\t\t</picture>\n\t</a>\n</div>\n",
      "stars_today": 0
    },
    {
      "id": 14579179,
      "name": "plotly.R",
      "full_name": "plotly/plotly.R",
      "description": "An interactive graphing library for R",
      "html_url": "https://github.com/plotly/plotly.R",
      "stars": 2658,
      "forks": 639,
      "language": "R",
      "topics": [
        "d3js",
        "data-visualization",
        "ggplot2",
        "javascript",
        "plotly",
        "r",
        "r-package",
        "rstats",
        "shiny",
        "webgl"
      ],
      "created_at": "2013-11-21T05:56:51Z",
      "updated_at": "2026-01-26T06:11:04Z",
      "pushed_at": "2026-01-24T15:54:03Z",
      "open_issues": 750,
      "owner": {
        "login": "plotly",
        "avatar_url": "https://avatars.githubusercontent.com/u/5997976?v=4"
      },
      "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n<img src=\"man/figures/plotly.png\" width=\"200\" />\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/ropensci/plotly/workflows/R-CMD-check/badge.svg)](https://github.com/plotly/plotly.R/actions)\n[![CRAN Status](https://www.r-pkg.org/badges/version/plotly)](https://cran.r-project.org/package=plotly)\n[![CRAN Downloads](https://cranlogs.r-pkg.org/badges/grand-total/plotly)](https://cranlogs.r-pkg.org/badges/grand-total/plotly)\n[![monthly](https://cranlogs.r-pkg.org/badges/plotly)](https://cranlogs.r-pkg.org/badges/plotly)\n<!-- badges: end -->\n\nAn R package for creating interactive web graphics via the open source\nJavaScript graphing library\n[plotly.js](https://github.com/plotly/plotly.js).\n\n<div align=\"center\">\n  <a href=\"https://dash.plotly.com/project-maintenance\">\n    <img src=\"https://dash.plotly.com/assets/images/maintained-by-community.png\" width=\"400px\" alt=\"Maintained by the Plotly Community\">\n  </a>\n</div>\n\n## Installation\n\nInstall from CRAN:\n\n``` r\ninstall.packages(\"plotly\")\n```\n\nOr install the latest development version (on GitHub) via `{remotes}`:\n\n``` r\nremotes::install_github(\"plotly/plotly\")\n```\n\n## Getting started\n\n### Web-based ggplot2 graphics\n\nIf you use [ggplot2](https://github.com/tidyverse/ggplot2), `ggplotly()`\nconverts your static plots to an interactive web-based version\\!\n\n``` r\nlibrary(plotly)\ng <- ggplot(faithful, aes(x = eruptions, y = waiting)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") + \n  xlim(1, 6) + ylim(40, 100)\nggplotly(g)\n```\n\n![<https://i.imgur.com/G1rSArP.gifv>](https://i.imgur.com/G1rSArP.gif)\n\nBy default, `ggplotly()` tries to replicate the static ggplot2 version\nexactly (before any interaction occurs), but sometimes you need greater\ncontrol over the interactive behavior. The `ggplotly()` function itself\nhas some convenient â€œhigh-levelâ€ arguments, such as `dynamicTicks`,\nwhich tells plotly.js to dynamically recompute axes, when appropriate.\nThe `style()` function also comes in handy for *modifying* the\nunderlying trace\nattributes (e.g. [hoveron](https://plotly.com/r/reference/#scatter-hoveron)) used to generate the plot:\n\n``` r\ngg <- ggplotly(g, dynamicTicks = \"y\")\nstyle(gg, hoveron = \"points\", hoverinfo = \"x+y+text\", hoverlabel = list(bgcolor = \"white\"))\n```\n\n![<https://i.imgur.com/qRvLgea.gifv>](https://imgur.com/qRvLgea.gif)\n\nMoreover, since `ggplotly()` returns a plotly object, you can apply\nessentially any function from the R package on that object. Some useful\nones include `layout()` (for [customizing the\nlayout](https://plotly-r.com/improving-ggplotly.html#modifying-layout)),\n`add_traces()` (and its higher-level `add_*()` siblings, for example\n`add_polygons()`, for [adding new\ntraces/data](https://plotly-r.com/improving-ggplotly.html#leveraging-statistical-output)),\n`subplot()` (for [combining multiple plotly\nobjects](https://plotly-r.com/arranging-views.html#arranging-plotly-objects)),\nand `plotly_json()` (for inspecting the underlying JSON sent to\nplotly.js).\n\nThe `ggplotly()` function will also respect some â€œunofficialâ€\n**ggplot2** aesthetics, namely `text` (for [customizing the\ntooltip](https://plotly-r.com/controlling-tooltips.html#tooltip-text-ggplotly)),\n`frame` (for [creating\nanimations](https://plotly-r.com/animating-views.html)),\nand `ids` (for ensuring sensible smooth transitions).\n\n### Using plotly without ggplot2\n\nThe `plot_ly()` function provides a more direct interface to plotly.js\nso you can leverage more specialized chart types (e.g., [parallel\ncoordinates](https://plotly.com/r/parallel-coordinates-plot/) or\n[maps](https://plotly.com/r/maps/)) or even some visualization that the\nggplot2 API wonâ€™t ever support (e.g., surface,\n[mesh](https://plotly.com/r/3d-mesh/),\n[trisurf](https://plotly.com/r/trisurf/), etc).\n\n``` r\nplot_ly(z = ~volcano, type = \"surface\")\n```\n\n![<https://plotly.com/~brnvg/1134>](https://plotly.com/~brnvg/1134.png)\n\n## Learn more\n\nTo learn more about special features that the plotly R package provides (e.g., [client-side linking](https://plotly-r.com/client-side-linking.html), [**shiny** integration](https://plotly-r.com/linking-views-with-shiny.html), [editing and generating static images](https://plotly-r.com/publish.html), [custom events in JavaScript](https://plotly-r.com/javascript.html), and more), see <https://plotly-r.com>. You may already be familiar with existing plotly documentation (e.g., <https://plotly.com/r/>), which is essentially a language-agnostic how-to guide for learning plotly.js, whereas <https://plotly-r.com> is meant to be more wholistic tutorial written by and for the R user. The package itself ships with a number of demos (list them by running `demo(package = \"plotly\")`) and shiny/rmarkdown examples (list them by running `plotly_example(\"shiny\")` or `plotly_example(\"rmd\")`). [Carson](https://cpsievert.me) also keeps numerous [slide decks](https://talks.cpsievert.me) with useful examples and concepts.\n\n## Contributing\n\nPlease read through our [contributing\nguidelines](https://github.com/plotly/plotly.R/blob/master/CONTRIBUTING.md).\nIncluded are directions for opening issues, asking questions,\ncontributing changes to plotly, and our code of\nconduct.\n",
      "stars_today": 0
    },
    {
      "id": 159560389,
      "name": "renv",
      "full_name": "rstudio/renv",
      "description": "renv: Project environments for R.",
      "html_url": "https://github.com/rstudio/renv",
      "stars": 1127,
      "forks": 162,
      "language": "R",
      "topics": [],
      "created_at": "2018-11-28T20:25:39Z",
      "updated_at": "2026-01-27T00:44:54Z",
      "pushed_at": "2026-01-26T18:31:37Z",
      "open_issues": 212,
      "owner": {
        "login": "rstudio",
        "avatar_url": "https://avatars.githubusercontent.com/u/513560?v=4"
      },
      "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# renv <img src=\"man/figures/logo.svg\" align=\"right\" height=\"115\"/>\n\n<!-- badges: start -->\n\n[![Lifecycle:\nstable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html)\n[![CRAN\nstatus](https://www.r-pkg.org/badges/version/renv)](https://CRAN.R-project.org/package=renv)\n[![R-CMD-check](https://github.com/rstudio/renv/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/rstudio/renv/actions/workflows/R-CMD-check.yaml)\n\n<!-- badges: end -->\n\n## Overview\n\nThe renv package[^1] helps you create **r**eproducible **env**ironments\nfor your R projects. Use renv to make your R projects more isolated,\nportable and reproducible.\n\n- **Isolated**: Installing a new or updated package for one project\n  wonâ€™t break your other projects, and vice versa. Thatâ€™s because renv\n  gives each project its own private library.\n- **Portable**: Easily transport your projects from one computer to\n  another, even across different platforms. renv makes it easy to\n  install the packages your project depends on.\n- **Reproducible**: renv records the exact package versions you depend\n  on, and ensures those exact versions are the ones that get installed\n  wherever you go.\n\n## Installation\n\nInstall the latest version of renv from CRAN with:\n\n``` r\ninstall.packages(\"renv\")\n```\n\nAlternatively, install the development version from\n[r-universe](https://rstudio.r-universe.dev/renv) with:\n\n``` r\ninstall.packages(\"renv\", repos = \"https://rstudio.r-universe.dev\")\n```\n\n## Workflow\n\n<img src=\"vignettes/renv.png\" alt=\"A diagram showing the most important verbs and nouns of renv. Projects start with init(), which creates a project library using packages from the system library. snapshot() updates the lockfile using the packages installed in the project library, where restore() installs packages into the project library using the metadata from the lockfile, and status() compares the lockfile to the project library. You install and update packages from CRAN and GitHub using install() and update(), but because you'll need to do this for multiple projects, renv uses cache to make this fast.\" width=\"408\" style=\"display: block; margin: auto;\" />\n\nUse `renv::init()` to initialize renv in a new or existing project. This\nwill set up a **project library**, containing all the packages youâ€™re\ncurrently using. The packages (and all the metadata needed to reinstall\nthem) are recorded into a **lockfile**, `renv.lock`, and a `.Rprofile`\nensures that the library is used every time you open that project.\n\nAs you continue to work on your project, you will install and upgrade\npackages, either using `install.packages()` and `update.packages()` or\n`renv::install()` and `renv::update()`. After youâ€™ve confirmed your code\nworks as expected, use `renv::snapshot()` to record the packages and\ntheir sources in the lockfile.\n\nLater, if you need to share your code with someone else or run your code\non new machine, your collaborator (or you) can call `renv::restore()` to\nreinstall the specific package versions recorded in the lockfile.\n\n## Learning more\n\nIf this is your first time using renv, we strongly recommend starting\nwith the [Introduction to\nrenv](https://rstudio.github.io/renv/articles/renv.html) vignette: this\nwill help you understand the most important verbs and nouns of renv.\n\nIf you have a question about renv, please first check the\n[FAQ](https://rstudio.github.io/renv/articles/faq.html) to see whether\nyour question has already been addressed. If it hasnâ€™t, please feel free\nto ask on the [Posit Forum](https://forum.posit.co).\n\nIf you believe youâ€™ve found a bug in renv, please file a bug (and, if\npossible, a [reproducible example](https://reprex.tidyverse.org)) at\n<https://github.com/rstudio/renv/issues>.\n\n[^1]: Pronounced â€œRâ€ â€œenvâ€\n",
      "stars_today": 0
    },
    {
      "id": 259225467,
      "name": "CellChat",
      "full_name": "sqjin/CellChat",
      "description": "R toolkit for inference, visualization and analysis of cell-cell communication from single-cell data",
      "html_url": "https://github.com/sqjin/CellChat",
      "stars": 759,
      "forks": 167,
      "language": "R",
      "topics": [
        "cell-cell-communication",
        "cell-cell-interaction",
        "microenvironment",
        "single-cell-analysis"
      ],
      "created_at": "2020-04-27T06:28:33Z",
      "updated_at": "2026-01-24T22:48:40Z",
      "pushed_at": "2024-01-06T18:23:14Z",
      "open_issues": 455,
      "owner": {
        "login": "sqjin",
        "avatar_url": "https://avatars.githubusercontent.com/u/32399212?v=4"
      },
      "readme": "\n<p align=\"center\">\n  <img width=\"200\"  src=\"https://github.com/sqjin/CellChat/blob/master/CellChat_Logo.png\">\n</p>\n\n# CAUTION\nWe have updated CellChat to v2 and migrated CellChat to a new repository. This repository will be NOT updated and maintained any more. Please check the new repository [jinworks/CellChat](https://github.com/jinworks/CellChat) for the new updates, and the [CellChat v2 paper](https://biorxiv.org/cgi/content/short/2023.11.05.565674v1) for a comprehensive protocol of CellChat.  \n\n# About CellChat and CellChatDB\nCellChat is an R package designed for inference, analysis, and visualization of cell-cell communication from single-cell data. CellChat aims to enable users to identify and interpret cell-cell communication within an easily interpretable framework, with the emphasis of clear, attractive, and interpretable visualizations.  \n\nCellChatDB is a manually curated database of literature-supported ligand-receptor interactions in mutiple species, leading to a comprehensive recapitulation of known molecular interaction mechanisms including multi-subunit structure of ligand-receptor complexes and co-factors.\n\nIf you use CellChat in your research, please considering citing our papers: \n- [Suoqin Jin et al., CellChat for systematic analysis of cell-cell communication from single-cell and spatially resolved transcriptomics, bioRxiv 2023](https://biorxiv.org/cgi/content/short/2023.11.05.565674v1) [CellChat v2]\n- [Suoqin Jin et al., Inference and analysis of cell-cell communication using CellChat, Nature Communications 2021](https://www.nature.com/articles/s41467-021-21246-9) [CellChat v1]\n\n# Capabilities\nIn addition to infer the intercellular communication from any given single-cell data, CellChat provides functionality for further data exploration, analysis, and visualization. \n\n- It can quantitatively characterize and compare the inferred cell-cell communication networks using a systems approach by combining social network analysis, pattern recognition, and manifold learning approaches.\n- It provides an easy-to-use tool for extracting and visualizing high-order information of the inferred networks. For example, it allows ready prediction of major signaling inputs and outputs for all cell populations and how these populations and signals coordinate together for functions.\n- It enables comparative analysis of cell-cell communication across different conditions and identification of altered signaling and cell populations. \n- It provides several visualization outputs to facilitate intuitive user-guided data interpretation.\n\n<p align=\"center\">\n  <img width=\"700\"  src=\"https://github.com/sqjin/CellChat/blob/master/overview_CellChat.png\">\n</p>\n\n\n<p align=\"center\">\n  <a href=\"https://clustrmaps.com/site/1bpq2\">\n     <img width=\"200\"  src=\"https://clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=n&d=42WqeykSXznN_NSaBlpf6CtSXQxhqmIs6QusUsguFdY\" />\n   </a>\n</p>\n<p align=\"center\">\n  <a href=\"#\">\n     <img src=\"https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2Fsqjin%2FCellChat&labelColor=%233499cc&countColor=%2370c168\" />\n   </a>\n</p>\n\n\n\n\n\n",
      "stars_today": 0
    },
    {
      "id": 2188402,
      "name": "phyloseq",
      "full_name": "joey711/phyloseq",
      "description": "phyloseq is a set of classes, wrappers, and tools (in R) to make it easier to import, store, and analyze phylogenetic sequencing data; and to reproducibly share that data and analysis with others. See the phyloseq front page:",
      "html_url": "https://github.com/joey711/phyloseq",
      "stars": 635,
      "forks": 193,
      "language": "R",
      "topics": [],
      "created_at": "2011-08-11T00:16:34Z",
      "updated_at": "2026-01-12T16:12:10Z",
      "pushed_at": "2024-04-29T20:03:19Z",
      "open_issues": 765,
      "owner": {
        "login": "joey711",
        "avatar_url": "https://avatars.githubusercontent.com/u/841437?v=4"
      },
      "readme": "<link href=\"http://joey711.github.com/phyloseq/markdown.css\" rel=\"stylesheet\"></link>\n\n# [phyloseq](http://joey711.github.com/phyloseq/)\n\n[![Travis-CI Build Status](https://travis-ci.org/joey711/phyloseq.svg?branch=master)](https://travis-ci.org/joey711/phyloseq)\n\n![phyloseq](inst/extdata/phyloseq.png)\n\n## Quick Install\n\nIn R terminal:\n\n```\nif(!requireNamespace(\"BiocManager\")){\n  install.packages(\"BiocManager\")\n}\nBiocManager::install(\"phyloseq\")\n```\n\nSee [the phyloseq installation page](http://joey711.github.io/phyloseq/install.html)\nfor further details, examples.\n\n## Article on Improved Microbiome Analysis\n\nMcMurdie and Holmes (2014)\n[Waste Not, Want Not: Why Rarefying Microbiome Data is Statistically Inadmissible](http://dx.plos.org/10.1371/journal.pcbi.1003531)\n*PLoS Computational Biology*\n10(4): e1003531\n\nPresubmission versions ahead of acceptance (2013):\n[PDF version 2](http://arxiv.org/pdf/1310.0424v2.pdf),\n[PDF version 1](http://arxiv.org/pdf/1310.0424v1.pdf)\n\n\n## Peer-reviewed articles about phyloseq\n\nMcMurdie and Holmes (2014) [Shiny-phyloseq: Web Application for Interactive Microbiome Analysis with Provenance Tracking](http://bioinformatics.oxfordjournals.org/content/early/2014/10/02/bioinformatics.btu616).\n*Bioinformatics (Oxford, England)*\n31(2), 282â€“283.\n\nMcMurdie and Holmes (2013)\n[phyloseq: An R package for reproducible interactive analysis and graphics of microbiome census data](http://dx.plos.org/10.1371/journal.pone.0061217)\n*PLoS ONE* \n8(4):e61217\n\n## Other resources\n\nThe phyloseq project also has a number of supporting online resources,\nincluding (but probably not limited to)\n\n### [the phyloseq home page](http://joey711.github.com/phyloseq/)\n\n### [the phyloseq FAQ](https://www.bioconductor.org/packages/release/bioc/vignettes/phyloseq/inst/doc/phyloseq-FAQ.html)\nI recommend checking this page, and the issues tracker,\nbefore posting new issues.\n\n### [Bioconductor stable release](http://bioconductor.org/packages/release/bioc/html/phyloseq.html).\n\n### [the phyloseq Issue Tracker](https://github.com/joey711/phyloseq/issues)\nThis is the recommended location to post\n\n(1) feature requests\n(2) bug reports\n(3) theoretical considerations\n(4) other issues, feedback\n(5) ask for help\n\nSearch previous posts,\nand check [the phyloseq FAQ](https://www.bioconductor.org/packages/release/bioc/vignettes/phyloseq/inst/doc/phyloseq-FAQ.html)\nbefore posting a new issue.\n",
      "stars_today": 0
    },
    {
      "id": 53894616,
      "name": "infercnv",
      "full_name": "broadinstitute/infercnv",
      "description": "Inferring CNV from Single-Cell RNA-Seq",
      "html_url": "https://github.com/broadinstitute/infercnv",
      "stars": 650,
      "forks": 177,
      "language": "R",
      "topics": [],
      "created_at": "2016-03-14T21:53:54Z",
      "updated_at": "2026-01-20T07:04:11Z",
      "pushed_at": "2025-11-14T17:35:17Z",
      "open_issues": 236,
      "owner": {
        "login": "broadinstitute",
        "avatar_url": "https://avatars.githubusercontent.com/u/393552?v=4"
      },
      "readme": "\n***********************************************\n>InferCNV is no longer supported. Please explore alternatives such as InferCNA (https://jlaffy.github.io/infercna/)[https://jlaffy.github.io/infercna/], CopyKAT (https://github.com/navinlabcode/copykat)[https://github.com/navinlabcode/copykat], and Numbat (https://github.com/kharchenkolab/numbat)[https://github.com/kharchenkolab/numbat]\n***********************************************\n\n# Subclustering\n\nSubclustering resolution is one of the primary settings that will need to be adjusted in most runs to avoid oversplitting. The tutorial below explains how it works and details about it can also be found on the [wiki](https://github.com/broadinstitute/infercnv/wiki/infercnv-tumor-subclusters#tumor-subclustering-by-leiden-clustering-preferred).\n\n# Documentation\n### Full documentation\n\nVisit project [wiki](https://github.com/broadinstitute/inferCNV/wiki) for InferCNV documentation.\n\n\n### Infercnv video tutorial\n\nA **video** tutorial giving on overview of infercnv features and how to run an analysis can be found below **(click on the image)**:\n\n[![Tutorial: Running infercnv](http://img.youtube.com/vi/-qOcHAavZT8/0.jpg)](http://www.youtube.com/watch?v=-qOcHAavZT8 \"Tutorial: Running infercnv\")\n\n\n\n",
      "stars_today": 0
    },
    {
      "id": 216123064,
      "name": "ArchR",
      "full_name": "GreenleafLab/ArchR",
      "description": "ArchR : Analysis of Regulatory Chromatin in R (www.ArchRProject.com)",
      "html_url": "https://github.com/GreenleafLab/ArchR",
      "stars": 444,
      "forks": 153,
      "language": "R",
      "topics": [],
      "created_at": "2019-10-18T23:35:41Z",
      "updated_at": "2026-01-21T06:31:12Z",
      "pushed_at": "2025-02-18T21:19:00Z",
      "open_issues": 177,
      "owner": {
        "login": "GreenleafLab",
        "avatar_url": "https://avatars.githubusercontent.com/u/8398169?v=4"
      },
      "readme": "<p align=\"center\"><a href =\"https://www.archrproject.com\"><img src=\"Figures/ArchR_Logo_Integrated.png\" alt=\"\" width=\"350\"></a></p>\n<hr>\n\n[![Lifecycle: maturing](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://www.tidyverse.org/lifecycle/#maturing)\n\n### ArchR has new features available for scATAC-seq Analysis\n\n**Paired scATAC-seq and scRNA-seq Analysis**\n\nArchR now supports paired scATAC-seq and scRNA-seq Analysis! <br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See updates with importFeatureMatrix, addGeneExpressionMatrix, addIterativeLSI, addCombinedDims <br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For a brief tutorial of these features : https://greenleaflab.github.io/ArchR_2020/Ex-Analyze-Multiome.html\n\n**Trajectory Analysis**\n\nArchR now directly supports both monocle3 and Slingshot based trajectory analysis! <br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See updates with getMonocleTrajectories, addMonocleTrajectory, addSlingShotTrajectories <br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For a brief tutorial of these features : https://greenleaflab.github.io/ArchR_2020/Ex-Analyze-Trajectory.html\n\nAdditionally ArchR now enables export of a peak matrix that is compatible with STREAM!<br />\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;See updates with exportPeakMatrixForSTREAM <br />\n\n### ArchR is currently in Beta and will be in active development through the peer review process.\n\nArchR is a full-featured R package for processing and analyzing single-cell ATAC-seq data. ArchR provides the most extensive suite of scATAC-seq analysis tools of any software available. Additionally, ArchR excels in both speed and resource usage, making it possible to analyze 1 million cells in 8 hours on a MacBook Pro laptop.\n\n### For installation instructions and full documentation, visit www.ArchRProject.com.\n\n<hr>\n\n![](Figures/ArchR_Workflow_Horizontal.png)\n\n# Quick Installation of ArchR\nFor a full walk through of installation and frequently related issues please visit www.ArchRProject.com.\n\n**First, install devtools (for installing GitHub packages) if it isn't already installed:**\n``` r\nif (!requireNamespace(\"devtools\", quietly = TRUE)) install.packages(\"devtools\")\n```\n\n**Then, install BiocManager (for installing bioconductor packages) if it isn't already installed:**\n``` r\nif (!requireNamespace(\"BiocManager\", quietly = TRUE)) install.packages(\"BiocManager\")\n```\n\n**Then, install ArchR:**\n``` r\ndevtools::install_github(\"GreenleafLab/ArchR\", ref=\"master\", repos = BiocManager::repositories())\n```\n\n**Lastly, install all of the ArchR dependencies that aren't installed by default:**\n``` r\nlibrary(ArchR)\nArchR::installExtraPackages()\n```\nIf any of these steps fails, you should identify the offending package and troubleshoot that individual installation before proceeding. Additionally, please see the ArchR website (www.ArchRProject.com) where we have installation troubleshooting tips.\n\n# Pre-compiled ArchR environment\nWe provide two methods in which a user can manage R dependencies.  \n\n### Using renv to manage dependencies\nThe first is by using renv to manage a project's dependencies. To utilize this, make sure that the renv package is installed and loaded.  Before you are ready to use `renv`, you must ensure that you are working on the same R version that we used for the provided renv environment. \nThe R versions we currently support are:\n```\n- R 4.4\n- R 4.1\n```\nSecondly, make sure that the renv package is installed and loaded.\n```\ninstall.packages(\"renv\")\nlibrary(renv)\n```\nSet a working directory for your project\n```\ndir.create(path = \"./<project_name>\", showWarnings = FALSE)\nsetwd(\"./<project_name>\")\n```\nThen, lets download the lock file for the current master branch of ArchR.\nFor R 4.4:\n```\ndownload.file(url = \"https://pub-9ae435458ecc412abbbc9420a502ec38.r2.dev/renv.lock\", destfile = \"./renv.lock\")\n```\nFor R 4.1:\n```\ndownload.file(url = \"https://pub-9ae435458ecc412abbbc9420a502ec38.r2.dev/renv_4_1.lock\", destfile = \"./renv.lock\")\n```\n\nNow, we can initiate our renv project environment, utilizing the renv.lock to bootstrap a new renv environment.\n```\nrenv::init()\n```\n\n### Using Docker to manage dependencies\nWe also provide Docker images, built off of `rocker/rstudio`, that already have ArchR and all dependencies pre-loaded.\n\nThe latest version can be found at:\n```\ngreenleaflab/archr:latest\n```\nand other versions, including images built with differing R versions, can be found at:\n```\nhttps://hub.docker.com/r/greenleaflab/archr/tags\n```\n\nTo utilize these images, the user can first install Docker as mentioned in their [documentation](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository)\n\nFollowing, create a container using the following command:\n```\ndocker image pull greenleaflab/archr:latest\ndocker run -it --rm -v <your_workspace>:/workspace -p <your_port_of_interest>:8787\n```\nThis will spin up a container that has Rstudio turned on by default. Rstudio can be accessed through:\n```\nlocalhost:<your_port_of_interest>\n```\nIf you would like an interactive bash console instead, the following command can instead be called:\n```\ndocker run -it --rm -v <your_workspace>:/workspace -p <your_port_of_interest>:8787 bash\n```\n\n# Issues using ArchR?\n\nArchR is currently in __beta__. We expect there to be bumps in the road. If you think you have found a bug, please first install the latest version of ArchR via\n``` r\ndevtools::install_github(\"GreenleafLab/ArchR\", ref=\"master\", repos = BiocManager::repositories())\n```\nIf this does not fix your problem, please [report an issue on Github](https://github.com/GreenleafLab/ArchR/issues) with the __Bug Report__ form.\n\nIf you have questions about ArchR usage, please refer to the [the searchable full user's manual](https://www.archrproject.com/bookdown/index.html), [the FAQ section](https://www.archrproject.com/articles/Articles/faq.html), and the [publication](https://greenleaf.stanford.edu/assets/pdf/). If you think the documentation on this website or in the function annotations is unclear, please [submit an issue on Github](https://github.com/GreenleafLab/ArchR/issues) with the __Documentation Request__ form. If there is a feature that you think is missing from ArchR _and you have already searched the user's manual_, [submit an issue on Github](https://github.com/GreenleafLab/ArchR/issues) with the __Feature Request__ form. If none of these options help, [send us an email](mailto:archr.devs@gmail.com). We will do our best to respond to questions that are not otherwise answered in the documentation.\n\n\n",
      "stars_today": 0
    },
    {
      "id": 50672978,
      "name": "gcam-core",
      "full_name": "JGCRI/gcam-core",
      "description": "GCAM -- The Global Change Analysis Model",
      "html_url": "https://github.com/JGCRI/gcam-core",
      "stars": 379,
      "forks": 203,
      "language": "R",
      "topics": [
        "climate",
        "coupled-human-natural-systems",
        "economics",
        "energy",
        "gcam",
        "human-earth-system",
        "integrated-assessment",
        "land",
        "water"
      ],
      "created_at": "2016-01-29T15:57:28Z",
      "updated_at": "2026-01-25T12:56:21Z",
      "pushed_at": "2026-01-22T23:19:22Z",
      "open_issues": 253,
      "owner": {
        "login": "JGCRI",
        "avatar_url": "https://avatars.githubusercontent.com/u/8431983?v=4"
      },
      "readme": "# Global Change Analysis Model (GCAM)\n\nThe Joint Global Change Research Institute (JGCRI) of the Pacific \nNorthwest National Laboratory (PNNL) is the home and primary \ndevelopment institution for GCAM, a multisector model for exploring \nconsequences of and responses to global to local changes and stressors. \nRegional energy, water, land, and economic systems are connected to the\nrest of the globe through trade and interactions with environmental systems.\nThese systems are also connected with each other.\nMultisector models such as GCAM capture these \ninterconnected impacts in an economic framework in order to explore \nthese dynamic interactions and feedbacks between regions and sectors.\n\nGCAM has been developed at PNNL-JGCRI for over 20 years and is a freely\navailable community model and documented online (See below). The team\nat JGCRI is comprised of physical scientists, engineers, economists, energy\nexperts, forest ecologists, agricultural scientists, and environmental system\nscientists who develop the model and apply it to a range of research questions.\nThe JGCRI team works closely with the developers of other Earth system and\necosystem models to integrate the effects of human actions modeled in GCAM\ninto their research.\n\n## Model Overview\n\nGCAM is a dynamic-recursive model with technology-rich representations\nof the economy, energy sector, land use, and water linked to a reduced complexity\nEarth system model that can be used to explore many science and decision-relevant\nquestions including the effects of changes in trade patterns, critical minerals\n& materials availability, and deployment of energy technologies on human and\nEarth systems. Regional population and labor productivity growth assumptions\ndrive the energy and land-use systems, employing numerous technology options to\nproduce, transform, and provide energy services, as well as to produce\nagriculture and forest products, and to determine land use and land cover.\nUsing a run period extending from 1990 â€“ 2100 (historical years through 2021)\nwith annual results computed at 1-5 year intervals, GCAM has been used to\nexplore the potential role of emerging energy supply technologies and\nthe consequences of specific measures or energy technology adoption, including\nbioenergy; critical minerals & materials; hydrogen systems; nuclear energy;\nrenewable energy technologies; carbon capture, storage, and utilization and\nenergy use technology in buildings, industry, and the transportation\nsectors. GCAM outputs include projections of future energy and critical mineral\nsupply, trade, and demand and the resulting radiative forcing and other effects\nof 16 greenhouse gases, aerosols, and short-lived species at 0.5Ã—0.5 degree\nresolution, contingent on assumptions about future population, economy, technology,\ntrade and other polices.\n\n## Community guidelines for peer-reviewed journal articles using GCAM\n\nThis section outlines some suggested language which the GCAM user community \ncan employ to describe GCAM in papers in peer-reviewed journal articles,\nreports, or other public documents using GCAM or versions of GCAM. GCAM is\nunder continuous development. The suggested language for the opening paragraphs\nof a methodology or introduction section of a paper describing GCAM is as\nfollows:\n\n\"The Global Change Analysis Model (GCAM) is a multisector model developed and maintained at the Pacific Northwest National Laboratoryâ€™s Joint Global Change Research Institute (JGCRI, 2023) _\\<include additional citations to previous GCAM studies as relevant\\>_. GCAM is an open-source community model. In this study, we use GCAM v NN. The documentation of the model is available at the GCAM documentation page ([http://jgcri.github.io/gcam-doc](http://jgcri.github.io/gcam-doc)) and the description below is a summary. GCAM includes representations of: economy, energy, agriculture, and water supply in 32 geopolitical regions across the globe; their GHG and air pollutant emissions and global GHG concentrations, radiative forcing, and temperature change; and the associated land allocation, water use, and agriculture production across 396 land sub-regions and 235 water basins.  _\\<If using GCAM-USA, include without quotes: \"This study uses a U.S.-focused version of GCAM called GCAM-USA that includes representation of energy, economy, and water systems for the fifty states and the District of Columbia in addition to 31 regions outside of the United States.â€\\>_. The version of GCAM used in this study is available â€“ along with full source code and instructions for use â€“ in a public repository _\\<include citation including link to the GCAM repository with doi used in paper\\>_. \n\nSubsequent paragraphs of the description might expound on particular capabilities, systems, or sectors of focus in the paper. Details in the GCAM documentation page can be used as a reference to develop these paragraphs.\n\nCommunity users of GCAM might also undertake their own model developments and/or assumptions for papers. It is recommended that these departures from the publicly available version of the model be clearly described. In addition, if these developments are substantial, we suggest making this clear by including an additional phrase (e.g. region name or name of institution) in the name of the model and explicitly calling it out in place of or immediately following the italicized portion in the above paragraphs. For example: _\"This study uses a modified version of GCAM/GCAM-USA called GCAM-\\<institution name\\>/GCAM-USA-\\<institution name\\>. GCAM-\\<institution name\\>/GCAM-USA-\\<institution name\\> incorporates additional details and modified assumptions from GCAM v NN as described subsequently\"_. \n\n## Documentation\n\n* [GCAM Documentation](http://jgcri.github.io/gcam-doc/)\n* [Getting Started with GCAM](http://jgcri.github.io/gcam-doc/user-guide.html)\n* [GCAM Community](https://gcims.pnnl.gov/community)\n* [GCAM Videos and Tutorial Slides](https://gcims.pnnl.gov/community)\n* [GCAM Citation and Co-authorship Guidelines](http://jgcri.github.io/gcam-doc/community-guide.html)\n\n## Selected Publications\n\nCalvin, K., Patel, P., Clarke, L., Asrar, G., Bond-Lamberty, B., Cui, R. Y., Di Vittorio, A., Dorheim, K., Edmonds, J., Hartin, C., Hejazi, M., Horowitz, R., Iyer, G., Kyle, P., Kim, S., Link, R., McJeon, H., Smith, S. J., Snyder, A., Waldhoff, S., and Wise, M.: GCAM v5.1: representing the linkages between energy, water, land, climate, and economic systems, Geosci. Model Dev., 12, 677â€“698, https://doi.org/10.5194/gmd-12-677-2019, 2019.\n\nEdmonds, J., and J. Reilly (1985)Global Energy: Assessing the Future (Oxford University Press, New York) pp.317.\n\nEdmonds, J., M. Wise, H. Pitcher, R. Richels, T. Wigley, and C. MacCracken. (1997) â€œAn Integrated Assessment of Climate Change and the Accelerated Introduction of Advanced Energy Technologiesâ€, Mitigation and Adaptation Strategies for Global Change, 1, pp. 311-39\n\nKim, S.H., J. Edmonds, J. Lurz, S. J. Smith, and M. Wise (2006) â€œThe ObjECTS Framework for Integrated Assessment: Hybrid Modeling of Transportation â€ Energy Journal (Special Issue #2) pp 51-80.\n\n[Full list of GCAM publications](http://jgcri.github.io/gcam-doc/references.html)\n",
      "stars_today": 0
    },
    {
      "id": 167440342,
      "name": "monocle3",
      "full_name": "cole-trapnell-lab/monocle3",
      "description": null,
      "html_url": "https://github.com/cole-trapnell-lab/monocle3",
      "stars": 434,
      "forks": 116,
      "language": "R",
      "topics": [
        "single-cell-rna-seq"
      ],
      "created_at": "2019-01-24T21:26:18Z",
      "updated_at": "2026-01-24T03:39:16Z",
      "pushed_at": "2026-01-08T18:32:17Z",
      "open_issues": 261,
      "owner": {
        "login": "cole-trapnell-lab",
        "avatar_url": "https://avatars.githubusercontent.com/u/8060918?v=4"
      },
      "readme": "MONOCLE 3\n=======================\n\nMonocle 3 is an analysis toolkit for single-cell RNA-Seq experiments.  To use this package, you will need the R statistical computing environment (version 3.0 or later) and several packages available through Bioconductor and CRAN.\n\nDetails on how to install and use Monocle 3 are available on our website:\n\nhttp://cole-trapnell-lab.github.io/monocle3/\n\n## Monocle3 with BPCells counts matrix support\n\nThis development branch version of Monocle3 adds the ability to store the counts matrix on-disk using the BPCells package. By default, Monocle3 stores the counts matrix in-memory as a sparse matrix, as in previous versions. In order to store the matrix on-disk, you must set the matrix_control list value `matrix_class=\"BPCells\"` in the affected commands. For example, to load a MatrixMarket file as an on-disk matrix, use the command\n\n```\ncds <- load_mm_data(mat_path=<path_to_mtx_file>,\n                    feature_anno_path=<path_to_feature_anno_file>,\n                    cell_anno_path=<path_to_cell_anno_file>,\n                    matrix_control=list(matrix_class='BPCells'))\n```\n\n### Install Monocle3 with BPCells\n\nYou must install BPCells from Github before you can install this Monocle3 version, and BPCells requires an HDF5 object library for installation. After installing the HDF5 library, you install BPCells using the command\n\n```\nremotes::install_github(\"bnprks/BPCells/r\")\n```\n\nThe [BPCells Github site](https://github.com/bnprks/BPCells)  has additional information.\n\nSome Linux distributions provide the HDF5 library as an option. The\nBPCells site has information about installing the HDF5 library on various operating systems.\n\nI used Homebrew to install an HDF5 library on MacOS. I seemed to need to install the pkg-config package as well, and add a pkg-config configuration file for HDF5. Homebrew installed pkg-config in '/opt/homebrew' so I added the hdf5.pc file in\n\n> /opt/homebrew/lib/pkgconfig/hdf5.pc\n\nwith the contents\n\n```\nprefix=/opt/homebrew/Cellar/hdf5/1.12.2_2\nexec_prefix=${prefix}\nincludedir=${prefix}/include\nlibdir=/opt/homebrew/Cellar/hdf5/1.12.2_2/lib\n  \nName: hdf5\nDescription: HDF5\nURL: xx\nVersion: 1.12.2_2\nCflags: -I${includedir}\nLibs: -L${libdir} -lhdf5\n```\n\nYou may need to update the version strings in your hdf5.pc file.\n\nMonocle3 no longer uses the terra package, so it does not need to be installed.\n\n### Notes\n\n- Monocle3 can use the BPCells package to store the feature-cell counts matrix on-disk rather than in-memory, which enables analysis of considerably larger data sets than before. By default, Monocle3 stores the counts matrix in-memory as a sparse matrix, as it has in the past. To store the counts matrix on-disk, use the parameter `matrix_control=list(matrix_class=\"BPCells\")` when you make the CDS or convert the counts matrix using one of the functions\n  - `load_mm_data()`\n  - `load_mtx_data()`\n  - `load_cellranger_data()`\n  - `load_a549()`\n  - `load_worm_embryo()`\n  - `load_worm_l2()`\n  - `convert_counts_matrix()`\n\n  For example, to convert a dgCMatrix counts matrix to a BPCells on-disk matrix in an existing CDS, use the command `cds <- convert_counts_matrix(cds, matrix_control=list(matrix_class=\"BPCells\"))`.\n- BPCells stores the count matrix information in directories with names similar to `monocle.bpcells.20230830.4c4b1bebe4b4.tmp`. Monocle3 tries to remove those directories when you quit R. Please do not remove them while Monocle3 is running because doing so eliminates the count matrix data. You *can* remove them after quitting R if Monocle3 fails to remove them.\n- The method `new_cell_data_set()` accepts a BPCells on-disk counts matrix.\n- The functions `save_monocle_objects()` and `load_monocle_objects()` store and load BPCells on-disk matrices when the CDS counts matrix is an on-disk BPCells matrix.\n- The Monocle3 `saveRDS()` function warns the user to use `save_monocle_objects()` when saving a CDS with a BPCells on-disk counts matrix. If you insist on using the `saveRDS()` function, the BPCells on-disk matrix directory will not be stored and you will be unable to load it with the `readRDS()` function.\n- The function `combine_cds()` combines CDSes with mixes of dgCMatrix and BPCells on-disk counts matrices into a BPCells on-disk counts matrix. When called with the `matrix_control=list(matrix_class=\"BPCells\")` parameter, `combine_cds()` combines CDSes with all dgCMatrix counts matrices into a BPCells on-disk counts matrix.\n- Note that when the counts matrix is stored as a BPCells on-disk matrix, the `new_cell_data_set()` method stores a second BPCells on-disk copy of the matrix in the CDS assays slot with the name `counts_row_order`. The `counts_row_order` matrix is used by Monocle3 when the counts matrix is accessed intensively by row. The reason is that, by default, BPCells stores the matrix as a one-dimensional vector in column-major order, as does R. As a result, column access is fast and row access is slow. We use BPCell's ability to also store and access matrices in row-major order, which gives fast row access. However, this means that the two copies of the counts matrix must have the same count values. If you replace or change the CDS's counts matrix, you must also update the `counts_row_order` matrix, which you can do using the function `set_cds_row_order_matrix()`.\n  - The CDS assays slot is a named list where the standard, column-major order, matrix is called `counts` and the BPCells row-major order matrix is called `counts_row_order`.\n  - The `counts` matrix getter and setter methods are `counts(cds)` and `counts(cds)<-`.\n  - The Monocle3 setter warns about re-setting the BPCells `counts_row_order` matrix, unless called with the parameter `bpcells_warn=FALSE`.\n  - The `counts_row_order` getter method is called `counts_row_order`.\n  - There is no corresponding `counts_row_order` setter method\n- By default, the BPCells on-disk matrix is stored in a directory that is created where R is started. You can change the directory location using the `matrix_path` value in the `matrix_control` parameter.\n- For more information about the `matrix_control` values, see the help document for the function `set_matrix_control()`.\n- I tested this version using BPCells counts matrices on the examples in the Monocle3 documentation although I did not try all of the plotting functions.\n\n",
      "stars_today": 0
    },
    {
      "id": 185880527,
      "name": "signac",
      "full_name": "stuart-lab/signac",
      "description": "R toolkit for the analysis of single-cell chromatin data",
      "html_url": "https://github.com/stuart-lab/signac",
      "stars": 403,
      "forks": 100,
      "language": "R",
      "topics": [
        "atac",
        "bioinformatics",
        "single-cell"
      ],
      "created_at": "2019-05-09T22:32:26Z",
      "updated_at": "2026-01-28T01:52:36Z",
      "pushed_at": "2026-01-20T08:57:03Z",
      "open_issues": 19,
      "owner": {
        "login": "stuart-lab",
        "avatar_url": "https://avatars.githubusercontent.com/u/102445397?v=4"
      },
      "readme": "# Signac <img align=\"right\" src=\"man/figures/logo.svg\" style=\"height:100px;\" />\n\n[![R-CMD-check](https://github.com/stuart-lab/signac/workflows/R-CMD-check/badge.svg)](https://github.com/stuart-lab/signac/actions)\n[![CRAN\nVersion](https://www.r-pkg.org/badges/version/Signac)](https://cran.r-project.org/package=Signac)\n[![CRAN\nDownloads](https://cranlogs.r-pkg.org/badges/Signac)](https://cran.r-project.org/package=Signac)\n\n## Overview\n\nSignac is a comprehensive R package for the analysis of single-cell\nchromatin data. Signac includes functions for quality control,\nnormalization, dimension reduction, clustering, differential activity,\nand more.\n\nDocumentation and tutorials can be found at\n<https://stuartlab.org/signac/>\n\n## Installation\n\nTo install the latest release of Signac from CRAN:\n\n``` r\nsetRepositories(ind=1:3) # needed to automatically install Bioconductor dependencies\ninstall.packages(\"Signac\")\n```\n\nTo release the latest develop version from GitHub:\n\n``` r\nif (!requireNamespace(\"remotes\", quietly = TRUE))\n    install.packages(\"remotes\")\nremotes::install_github(\"stuart-lab/signac\", ref = \"develop\")\n```\n\n## Release notes\n\nFor a changelog please see the [NEWS\nfile](https://github.com/stuart-lab/signac/blob/develop/NEWS.md), also\navailable on the [Signac\nwebsite](https://stuartlab.org/signac/news/index.html).\n\n## Contributing\n\nWe welcome contributions to the Signac package. Please see the\n[contribution guide](https://github.com/stuart-lab/signac/blob/develop/CONTRIBUTING.md)\nfor more information.\n\n## Getting help\n\nIf you encounter a bug or have a feature request, please open an\n[issue](https://github.com/stuart-lab/signac/issues).\n\nIf you would like to discuss questions related to single-cell analysis,\nyou can open a\n[discussion](https://github.com/stuart-lab/signac/discussions).\n\n## Citing Signac\n\nIf you use the Signac package in your work please cite [Stuart et\nal. 2021](https://doi.org/10.1038/s41592-021-01282-5)\n\n```\n@ARTICLE{signac,\n  title     = \"Single-cell chromatin state analysis with Signac\",\n  author    = \"Stuart, Tim and Srivastava, Avi and Madad, Shaista and Lareau,\n               Caleb A and Satija, Rahul\",\n  journal   = \"Nat. Methods\",\n  publisher = \"Nature Publishing Group\",\n  pages     = \"1--9\",\n  month     =  nov,\n  year      =  2021,\n  url       = \"https://www.nature.com/articles/s41592-021-01282-5\",\n  language  = \"en\"\n}\n```\n\n## Related packages\n\n-   [Seurat](https://github.com/satijalab/seurat)\n-   [SeuratObject](https://github.com/satijalab/seurat-object)\n-   [SeuratDisk](https://github.com/mojaveazure/seurat-disk)\n-   [SeuratData](https://github.com/satijalab/seurat-data)\n-   [SeuratWrappers](https://github.com/satijalab/seurat-wrappers)\n-   [Azimuth](https://github.com/satijalab/azimuth)\n",
      "stars_today": 0
    },
    {
      "id": 452908115,
      "name": "nflverse-data",
      "full_name": "nflverse/nflverse-data",
      "description": "Automated nflverse data repository",
      "html_url": "https://github.com/nflverse/nflverse-data",
      "stars": 330,
      "forks": 34,
      "language": "R",
      "topics": [],
      "created_at": "2022-01-28T02:01:18Z",
      "updated_at": "2026-01-25T05:32:52Z",
      "pushed_at": "2026-01-18T10:33:20Z",
      "open_issues": 6,
      "owner": {
        "login": "nflverse",
        "avatar_url": "https://avatars.githubusercontent.com/u/79467114?v=4"
      },
      "readme": "\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# nflverse-data\n\n<!-- badges: start -->\n<!-- badges: end -->\n\nThis repository holds automated data releases for nflverse projects\n(i.e.Â all of the data powered/scraped via GitHub Actions).\n\n## Usage\n\nYou can download data hosted here with the `{nflreadr}` package, or\nmanually download and access the\n[releases](https://github.com/nflverse/nflverse-data/releases) page.\nReleases are roughly organized along the [main\nfunctions](https://nflreadr.nflverse.com/reference/) of nflreadr.\n\n## Automation Status\n\nPlease see https://nflreadr.nflverse.com/articles/nflverse_data_schedule.html#automation-status for the status table. \n",
      "stars_today": 0
    },
    {
      "id": 248836530,
      "name": "scTenifoldKnk",
      "full_name": "cailab-tamu/scTenifoldKnk",
      "description": " R/MATLAB package to perform virtual knockout experiments on single-cell gene regulatory networks.",
      "html_url": "https://github.com/cailab-tamu/scTenifoldKnk",
      "stars": 142,
      "forks": 14,
      "language": "R",
      "topics": [
        "functional-genomics",
        "gene-function",
        "gene-knockout",
        "gene-regulatory-network",
        "virtual-knockout-experiments"
      ],
      "created_at": "2020-03-20T19:31:03Z",
      "updated_at": "2026-01-26T15:37:01Z",
      "pushed_at": "2026-01-26T15:09:47Z",
      "open_issues": 24,
      "owner": {
        "login": "cailab-tamu",
        "avatar_url": "https://avatars.githubusercontent.com/u/31963060?v=4"
      },
      "readme": "scTenifoldKnk\n=============\n\nA R/MATLAB/Python package to perform virtual knockout experiments on single-cell gene regulatory networks. **scTenifoldKnk** is a machine learning workflow that performs virtual knockout experiments using single-cell RNA sequencing (scRNAseq) data from wild-type (WT) control samples as input. Constructs a single-cell gene regulatory network (scGRN) and knocks out a target gene from the adjacency matrix of the WT scGRN by setting the geneâ€™s outdegree edges to zero. **scTenifoldKnk** then compares the knocked out scGRN with the WT scGRN to identify differentially regulated genes, called virtual-knockout perturbed genes, which are used to assess the impact of the gene knockout and reveal the geneâ€™s function in the analyzed cells.\n\nPython version of scTenifoldKnk is available at: https://github.com/qwerty239qwe/scTenifoldpy\n\nMATLAB version is available at: https://github.com/jamesjcai/scGEAToolbox\n\nInstall:\n-------\nYou can install **scTenifoldKnk/R** using the following command:\n\n```{R}\nlibrary(remotes)\ninstall_github('cailab-tamu/scTenifoldKnk')\nlibrary(scTenifoldKnk)\n```\n\nAvailable functions:\n--------------------\n\n|Code| Function |\n|:-|:-|\n|scTenifoldKnk|Perform virtual knockout experiments on single-cell gene regulatory networks|\n\nInput:\n--------\nThe required input for **scTenifoldKnk** is an expression matrix with genes in the rows and cells (barcodes) in the columns. Data is expected to be previously normalized or _not normalized_ if `QC = TRUE`.\n\nRunning time:\n--------\nThe running time of scTenifoldKnk is largely dependent on how long it takes to construct scGRNs from subsampled expression matrices. Time increases proportional to the number of cells and genes in the dataset used as input. Below is a table of running times under different scenarios:\n\n| Number of Cells | Number of Genes | Running Time |\n|-----------------|-----------------|--------------|\n| 300             | 1000            | 3.45 min     |\n| 1000            | 1000            | 4.25 min     |\n| 1000            | 5000            | 171.88 min (2 h 51.6 min) |\n| 2500            | 5000            | 175.29 min (2 h 55.3 min) |\n| 5000            | 5000            | 188.88 min (3 h 8.9 min) |\n| 5000            | 7500            | 189.51 min (3 h 9.5 min)  |\n| 7500            | 5000            | 615.45 min (10 h 15.5 min) |\n| 7500            | 7500            | 616.12 min (10 h 16.1 min)  |\n\n\nOutput:\n--------\nThe output of **scTenifoldKnk** is a list with 3 slots as follows: \n  * **tensorNetworks**: The computed weight-averaged denoised gene regulatory networks after CANDECOMP/PARAFAC (CP) tensor decomposition. It includes two slots with:\n    * **X**: The constructed network for the _X_ sample.\n    * **Y**: The constructed network for the _Y_ sample.\n  * **manifoldAlignment**: The generated low-dimensional features result of the non-linear manifold alignment. It is a data frame with _2 times the number of genes_ in the rows and _d_ (default= 2) dimensions in the columns\n  * **diffRegulation**: The results of the differential regulation analysis. It is a data frame with 6 columns as follows:\n    * **gene**: A character vector with the gene id identified from the manifoldAlignment output.\n    * **distance**: A numeric vector of the Euclidean distance computed between the coordinates of the same gene in both conditions.\n    * **Z**: A numeric vector of the Z-scores computed after Box-Cox power transformation.\n    * **FC**: A numeric vector of the FC computed with respect to the expectation.\n    * **p.value**: A numeric vector of the p-values associated to the fold-changes, probabilities are asigned as P[X > x] using the Chi-square distribution with one degree of freedom.\n    * **p.adj**: A numeric vector of adjusted p-values using Benjamini & Hochberg (1995) FDR correction.\n\n---\nThe function to plot the egocentric KO, the code is available at [https://github.com/dosorio/utilities/blob/master/singleCell/plotKO.R](https://github.com/dosorio/utilities/blob/master/singleCell/plotKO.R), it requires: The object out of Knk (as X), the gene to knockout (gKO).\n\n\nÂ©ï¸ The Texas A & M University System. All rights reserved.\n",
      "stars_today": 0
    }
  ],
  "created_at": "2026-01-28T02:14:44.730071817Z"
}